7636	def expand_filepaths ( base_dir , rel_paths ) : return [ os . path . join ( base_dir , os . path . normpath ( rp ) ) for rp in rel_paths ]
6591	def receive ( self ) : ret = [ ] while True : if self . runid_pkgidx_map : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret . extend ( self . _collect_all_finished_pkgidx_result_pairs ( ) ) if not self . runid_pkgidx_map : break time . sleep ( self . sleep ) ret = sorted ( ret , key = itemgetter ( 0 ) ) return ret
9456	def cancel_scheduled_play ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledPlay/' method = 'POST' return self . request ( path , method , call_params )
3765	def Joule_Thomson ( T , V , Cp , dV_dT = None , beta = None ) : r if dV_dT : return ( T * dV_dT - V ) / Cp elif beta : return V / Cp * ( beta * T - 1. ) else : raise Exception ( 'Either dV_dT or beta is needed' )
8640	def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2185	def tryload ( self , cfgstr = None , on_error = 'raise' ) : cfgstr = self . _rectify_cfgstr ( cfgstr ) if self . enabled : try : if self . verbose > 1 : self . log ( '[cacher] tryload fname={}' . format ( self . fname ) ) return self . load ( cfgstr ) except IOError : if self . verbose > 0 : self . log ( '[cacher] ... {} cache miss' . format ( self . fname ) ) except Exception : if self . verbose > 0 : self . log ( '[cacher] ... failed to load' ) if on_error == 'raise' : raise elif on_error == 'clear' : self . clear ( cfgstr ) return None else : raise KeyError ( 'Unknown method on_error={}' . format ( on_error ) ) else : if self . verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) return None
412	def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
8225	def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
76	def normalize_shape ( shape ) : if isinstance ( shape , tuple ) : return shape assert ia . is_np_array ( shape ) , ( "Expected tuple of ints or array, got %s." % ( type ( shape ) , ) ) return shape . shape
4326	def delay ( self , positions ) : if not isinstance ( positions , list ) : raise ValueError ( "positions must be a a list of numbers" ) if not all ( ( is_number ( p ) and p >= 0 ) for p in positions ) : raise ValueError ( "positions must be positive nubmers" ) effect_args = [ 'delay' ] effect_args . extend ( [ '{:f}' . format ( p ) for p in positions ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'delay' ) return self
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
10499	def waitFor ( self , timeout , notification , ** kwargs ) : return self . _waitFor ( timeout , notification , ** kwargs )
2852	def mpsse_set_clock ( self , clock_hz , adaptive = False , three_phase = False ) : self . _write ( '\x8A' ) if adaptive : self . _write ( '\x96' ) else : self . _write ( '\x97' ) if three_phase : self . _write ( '\x8C' ) else : self . _write ( '\x8D' ) divisor = int ( math . ceil ( ( 30000000.0 - float ( clock_hz ) ) / float ( clock_hz ) ) ) & 0xFFFF if three_phase : divisor = int ( divisor * ( 2.0 / 3.0 ) ) logger . debug ( 'Setting clockspeed with divisor value {0}' . format ( divisor ) ) self . _write ( str ( bytearray ( ( 0x86 , divisor & 0xFF , ( divisor >> 8 ) & 0xFF ) ) ) )
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
6297	def release ( self , buffer = True ) : for key , vao in self . vaos : vao . release ( ) if buffer : for buff in self . buffers : buff . buffer . release ( ) if self . _index_buffer : self . _index_buffer . release ( )
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
4635	def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
335	def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] return 100 - np . abs ( 50 - np . mean ( q ) ) / .5
698	def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )
5433	def split_pair ( pair_string , separator , nullable_idx = 1 ) : pair = pair_string . split ( separator , 1 ) if len ( pair ) == 1 : if nullable_idx == 0 : return [ None , pair [ 0 ] ] elif nullable_idx == 1 : return [ pair [ 0 ] , None ] else : raise IndexError ( 'nullable_idx should be either 0 or 1.' ) else : return pair
5616	def write_vector_window ( in_data = None , out_schema = None , out_tile = None , out_path = None , bucket_resource = None ) : try : os . remove ( out_path ) except OSError : pass out_features = [ ] for feature in in_data : try : for out_geom in multipart_to_singleparts ( clean_geometry_type ( to_shape ( feature [ "geometry" ] ) . intersection ( out_tile . bbox ) , out_schema [ "geometry" ] ) ) : out_features . append ( { "geometry" : mapping ( out_geom ) , "properties" : feature [ "properties" ] } ) except Exception as e : logger . warning ( "failed to prepare geometry for writing: %s" , e ) continue if out_features : try : if out_path . startswith ( "s3://" ) : with VectorWindowMemoryFile ( tile = out_tile , features = out_features , schema = out_schema , driver = "GeoJSON" ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with fiona . open ( out_path , 'w' , schema = out_schema , driver = "GeoJSON" , crs = out_tile . crs . to_dict ( ) ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . writerecords ( out_features ) except Exception as e : logger . error ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "nothing to write" , out_path ) )
2041	def SELFDESTRUCT ( self , recipient ) : recipient = Operators . EXTRACT ( recipient , 0 , 160 ) address = self . address if issymbolic ( recipient ) : logger . info ( "Symbolic recipient on self destruct" ) recipient = solver . get_value ( self . constraints , recipient ) if recipient not in self . world : self . world . create_account ( address = recipient ) self . world . send_funds ( address , recipient , self . world . get_balance ( address ) ) self . world . delete_account ( address ) raise EndTx ( 'SELFDESTRUCT' )
13773	def init_logs ( path = None , target = None , logger_name = 'root' , level = logging . DEBUG , maxBytes = 1 * 1024 * 1024 , backupCount = 5 , application_name = 'default' , server_hostname = None , fields = None ) : log_file = os . path . abspath ( os . path . join ( path , target ) ) logger = logging . getLogger ( logger_name ) logger . setLevel ( level ) handler = logging . handlers . RotatingFileHandler ( log_file , maxBytes = maxBytes , backupCount = backupCount ) handler . setLevel ( level ) handler . setFormatter ( JsonFormatter ( application_name = application_name , server_hostname = server_hostname , fields = fields ) ) logger . addHandler ( handler )
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
1532	def get_execution_state ( self , topologyName , callback = None ) : if callback : self . execution_state_watchers [ topologyName ] . append ( callback ) else : execution_state_path = self . get_execution_state_path ( topologyName ) with open ( execution_state_path ) as f : data = f . read ( ) executionState = ExecutionState ( ) executionState . ParseFromString ( data ) return executionState
7407	def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
339	def log_every_n ( level , msg , n , * args ) : count = _GetNextLogCountPerToken ( _GetFileAndLine ( ) ) log_if ( level , msg , not ( count % n ) , * args )
1491	def get_serializer ( context ) : cluster_config = context . get_cluster_config ( ) serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) if serializer_clsname is None : return PythonSerializer ( ) else : try : topo_pex_path = context . get_topology_pex_path ( ) pex_loader . load_pex ( topo_pex_path ) serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) serializer = serializer_cls ( ) return serializer except Exception as e : raise RuntimeError ( "Error with loading custom serializer class: %s, with error message: %s" % ( serializer_clsname , str ( e ) ) )
4930	def transform_courserun_title ( self , content_metadata_item ) : title = content_metadata_item . get ( 'title' ) or '' course_run_start = content_metadata_item . get ( 'start' ) if course_run_start : if course_available_for_enrollment ( content_metadata_item ) : title += ' ({starts}: {:%B %Y})' . format ( parse_lms_api_datetime ( course_run_start ) , starts = _ ( 'Starts' ) ) else : title += ' ({:%B %Y} - {enrollment_closed})' . format ( parse_lms_api_datetime ( course_run_start ) , enrollment_closed = _ ( 'Enrollment Closed' ) ) title_with_locales = [ ] content_metadata_language_code = transform_language_code ( content_metadata_item . get ( 'content_language' , '' ) ) for locale in self . enterprise_configuration . get_locales ( default_locale = content_metadata_language_code ) : title_with_locales . append ( { 'locale' : locale , 'value' : title } ) return title_with_locales
2024	def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
9463	def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
10808	def delete ( self ) : with db . session . begin_nested ( ) : Membership . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_group ( self ) . delete ( ) GroupAdmin . query_by_admin ( self ) . delete ( ) db . session . delete ( self )
6654	def pruneCache ( ) : cache_dir = folders . cacheDirectory ( ) def fullpath ( f ) : return os . path . join ( cache_dir , f ) def getMTimeSafe ( f ) : try : return os . stat ( f ) . st_mtime except FileNotFoundError : import time return time . clock ( ) fsutils . mkDirP ( cache_dir ) max_cached_modules = getMaxCachedModules ( ) for f in sorted ( [ f for f in os . listdir ( cache_dir ) if os . path . isfile ( fullpath ( f ) ) and not f . endswith ( '.json' ) and not f . endswith ( '.locked' ) ] , key = lambda f : getMTimeSafe ( fullpath ( f ) ) , reverse = True ) [ max_cached_modules : ] : cache_logger . debug ( 'cleaning up cache file %s' , f ) removeFromCache ( f ) cache_logger . debug ( 'cache pruned to %s items' , max_cached_modules )
7606	def search_clans ( self , ** params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , ** params )
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
12406	def serialize ( self , data = None ) : if data is not None and self . response is not None : self . response [ 'Content-Type' ] = self . media_types [ 0 ] self . response . write ( data ) return data
11432	def _check_field_validity ( field ) : if type ( field ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Field of type '%s' should be either " "a list or a tuple." % type ( field ) ) if len ( field ) != 5 : raise InvenioBibRecordFieldError ( "Field of length '%d' should have 5 " "elements." % len ( field ) ) if type ( field [ 0 ] ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Subfields of type '%s' should be " "either a list or a tuple." % type ( field [ 0 ] ) ) if type ( field [ 1 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 1 of type '%s' should be " "a string." % type ( field [ 1 ] ) ) if type ( field [ 2 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 2 of type '%s' should be " "a string." % type ( field [ 2 ] ) ) if type ( field [ 3 ] ) is not str : raise InvenioBibRecordFieldError ( "Controlfield value of type '%s' " "should be a string." % type ( field [ 3 ] ) ) if type ( field [ 4 ] ) is not int : raise InvenioBibRecordFieldError ( "Global position of type '%s' should " "be an int." % type ( field [ 4 ] ) ) for subfield in field [ 0 ] : if ( type ( subfield ) not in ( list , tuple ) or len ( subfield ) != 2 or type ( subfield [ 0 ] ) is not str or type ( subfield [ 1 ] ) is not str ) : raise InvenioBibRecordFieldError ( "Subfields are malformed. " "Should a list of tuples of 2 strings." )
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
2531	def parse_doc_fields ( self , doc_term ) : try : self . builder . set_doc_spdx_id ( self . doc , doc_term ) except SPDXValueError : self . value_error ( 'DOC_SPDX_ID_VALUE' , doc_term ) try : if doc_term . count ( '#' , 0 , len ( doc_term ) ) <= 1 : doc_namespace = doc_term . split ( '#' ) [ 0 ] self . builder . set_doc_namespace ( self . doc , doc_namespace ) else : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) except SPDXValueError : self . value_error ( 'DOC_NAMESPACE_VALUE' , doc_term ) for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'specVersion' ] , None ) ) : try : self . builder . set_doc_version ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_VERS_VALUE' , o ) except CardinalityError : self . more_than_one_error ( 'specVersion' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'dataLicense' ] , None ) ) : try : self . builder . set_doc_data_lic ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'DOC_D_LICS' , o ) except CardinalityError : self . more_than_one_error ( 'dataLicense' ) break for _s , _p , o in self . graph . triples ( ( doc_term , self . spdx_namespace [ 'name' ] , None ) ) : try : self . builder . set_doc_name ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'name' ) break for _s , _p , o in self . graph . triples ( ( doc_term , RDFS . comment , None ) ) : try : self . builder . set_doc_comment ( self . doc , six . text_type ( o ) ) except CardinalityError : self . more_than_one_error ( 'Document comment' ) break
10960	def scramble_positions ( p , delete_frac = 0.1 ) : probs = [ 1 - delete_frac , delete_frac ] m = np . random . choice ( [ True , False ] , p . shape [ 0 ] , p = probs ) jumble = np . random . randn ( m . sum ( ) , 3 ) return p [ m ] + jumble
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
2360	def t_stringdollar_rbrace ( self , t ) : r'\}' t . lexer . braces -= 1 if t . lexer . braces == 0 : t . lexer . begin ( 'string' )
175	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_lines = color_lines if color_lines is not None else np . float32 ( color ) color_points = color_points if color_points is not None else np . float32 ( color ) * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else np . float32 ( alpha ) alpha_points = alpha_points if alpha_points is not None else np . float32 ( alpha ) size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 image = self . draw_lines_on_image ( image , color = np . array ( color_lines ) . astype ( np . uint8 ) , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) image = self . draw_points_on_image ( image , color = np . array ( color_points ) . astype ( np . uint8 ) , alpha = alpha_points , size = size_points , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image
3943	async def _on_push_data ( self , data_bytes ) : logger . debug ( 'Received chunk:\n{}' . format ( data_bytes ) ) for chunk in self . _chunk_parser . get_chunks ( data_bytes ) : if not self . _is_connected : if self . _on_connect_called : self . _is_connected = True await self . on_reconnect . fire ( ) else : self . _on_connect_called = True self . _is_connected = True await self . on_connect . fire ( ) container_array = json . loads ( chunk ) for inner_array in container_array : array_id , data_array = inner_array logger . debug ( 'Chunk contains data array with id %r:\n%r' , array_id , data_array ) await self . on_receive_array . fire ( data_array )
1894	def _send ( self , cmd : str ) : logger . debug ( '>%s' , cmd ) try : self . _proc . stdout . flush ( ) self . _proc . stdin . write ( f'{cmd}\n' ) except IOError as e : raise SolverError ( str ( e ) )
11627	def make_present_participles ( verbs ) : res = [ ] for verb in verbs : parts = verb . split ( ) if parts [ 0 ] . endswith ( "e" ) : parts [ 0 ] = parts [ 0 ] [ : - 1 ] + "ing" else : parts [ 0 ] = parts [ 0 ] + "ing" res . append ( " " . join ( parts ) ) return res
1190	def filter ( names , pat ) : import os result = [ ] try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) match = re_pat . match if 1 : for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result
5033	def get ( self , request , enterprise_customer_uuid ) : context = self . _build_context ( request , enterprise_customer_uuid ) transmit_courses_metadata_form = TransmitEnterpriseCoursesForm ( ) context . update ( { self . ContextParameters . TRANSMIT_COURSES_METADATA_FORM : transmit_courses_metadata_form } ) return render ( request , self . template , context )
5205	def proc_elms ( ** kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
7659	def append_columns ( self , columns ) : self . append_records ( [ dict ( time = t , duration = d , value = v , confidence = c ) for ( t , d , v , c ) in six . moves . zip ( columns [ 'time' ] , columns [ 'duration' ] , columns [ 'value' ] , columns [ 'confidence' ] ) ] )
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : raise
11497	def create_community ( self , token , name , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name optional_keys = [ 'description' , 'uuid' , 'privacy' , 'can_join' ] for key in optional_keys : if key in kwargs : if key == 'can_join' : parameters [ 'canjoin' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.community.create' , parameters ) return response
8601	def delete_share ( self , group_id , resource_id ) : response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'DELETE' ) return response
7103	def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) initcoeffs = np . zeros ( 22 ) leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict else : LOGERROR ( 'EPD fit did not converge' ) return None
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
1720	def trans ( ele , standard = False ) : try : node = globals ( ) . get ( ele [ 'type' ] ) if not node : raise NotImplementedError ( '%s is not supported!' % ele [ 'type' ] ) if standard : node = node . __dict__ [ 'standard' ] if 'standard' in node . __dict__ else node return node ( ** ele ) except : raise
724	def getDataRowCount ( self ) : inputRowCountAfterAggregation = 0 while True : record = self . getNextRecord ( ) if record is None : return inputRowCountAfterAggregation inputRowCountAfterAggregation += 1 if inputRowCountAfterAggregation > 10000 : raise RuntimeError ( 'No end of datastream found.' )
2215	def _join_itemstrs ( itemstrs , itemsep , newlines , _leaf_info , nobraces , trailing_sep , compact_brace , lbr , rbr ) : use_newline = newlines > 0 if newlines < 0 : use_newline = ( - newlines ) < _leaf_info [ 'max_height' ] if use_newline : sep = ',\n' if nobraces : body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = body_str else : if compact_brace : indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body_str = sep . join ( indented ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' if compact_brace : braced_body_str = ( lbr + body_str . replace ( '\n' , '\n ' ) + rbr ) else : braced_body_str = ( lbr + '\n' + body_str + '\n' + rbr ) retstr = braced_body_str else : sep = ',' + itemsep body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = ( lbr + body_str + rbr ) return retstr
12278	def run_executable ( repo , args , includes ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) tmpdir = tempfile . mkdtemp ( ) print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) files = extract_files ( strace_filename , includes ) execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
7222	def get ( self , recipe_id ) : self . logger . debug ( 'Retrieving recipe by id: ' + recipe_id ) url = '%(base_url)s/recipe/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
6055	def sparse_grid_from_unmasked_sparse_grid ( unmasked_sparse_grid , sparse_to_unmasked_sparse ) : total_pix_pixels = sparse_to_unmasked_sparse . shape [ 0 ] pix_grid = np . zeros ( ( total_pix_pixels , 2 ) ) for pixel_index in range ( total_pix_pixels ) : pix_grid [ pixel_index , 0 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 0 ] pix_grid [ pixel_index , 1 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 1 ] return pix_grid
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
7744	def _timeout_cb ( self , method ) : self . _anything_done = True logger . debug ( "_timeout_cb() called for: {0!r}" . format ( method ) ) result = method ( ) rec = method . _pyxmpp_recurring if rec : self . _prepare_pending ( ) return True if rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) tag = glib . timeout_add ( int ( result * 1000 ) , self . _timeout_cb , method ) self . _timer_sources [ method ] = tag else : self . _timer_sources . pop ( method , None ) self . _prepare_pending ( ) return False
10417	def variants_of ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Set [ Protein ] : if modifications : return _get_filtered_variants_of ( graph , node , modifications ) return { v for u , v , key , data in graph . edges ( keys = True , data = True ) if ( u == node and data [ RELATION ] == HAS_VARIANT and pybel . struct . has_protein_modification ( v ) ) }
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : root = '.' else : root = dirname ( __file__ ) for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
12748	def load_skel ( self , source , ** kwargs ) : logging . info ( '%s: parsing skeleton configuration' , source ) if hasattr ( source , 'read' ) : p = parser . parse ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
10624	def _calculate_Hfr ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) Hfr = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr = Hfr + dHfr return Hfr
3920	def set_focus ( self , position ) : self . _focus_position = position self . _modified ( ) try : self . next_position ( position ) except IndexError : self . _is_scrolling = False else : self . _is_scrolling = True
11966	def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
5348	def compose_bugzilla ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'bugzilla' ] ) > 0 ] : if 'bugzilla' not in projects [ p ] : projects [ p ] [ 'bugzilla' ] = [ ] urls = [ url [ 'query_url' ] for url in data [ p ] [ 'bugzilla' ] if url [ 'query_url' ] not in projects [ p ] [ 'bugzilla' ] ] projects [ p ] [ 'bugzilla' ] += urls return projects
6254	def root_path ( ) : module_dir = os . path . dirname ( globals ( ) [ '__file__' ] ) return os . path . dirname ( os . path . dirname ( module_dir ) )
6133	def from_string ( cls , content , position = 1 , file_id = None ) : if file_id is None : file_id = 'inlined_input' return cls ( FileMetadata ( file_id , position ) , content )
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
10142	def upload_files ( selected_file , selected_host , only_link , file_name ) : try : answer = requests . post ( url = selected_host [ 0 ] + "upload.php" , files = { 'files[]' : selected_file } ) file_name_1 = re . findall ( r'"url": *"((h.+\/){0,1}(.+?))"[,\}]' , answer . text . replace ( "\\" , "" ) ) [ 0 ] [ 2 ] if only_link : return [ selected_host [ 1 ] + file_name_1 , "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) ] else : return "{}: {}{}" . format ( file_name , selected_host [ 1 ] , file_name_1 ) except requests . exceptions . ConnectionError : print ( file_name + ' couldn\'t be uploaded to ' + selected_host [ 0 ] )
7848	def get_features ( self ) : l = self . xpath_ctxt . xpathEval ( "d:feature" ) ret = [ ] for f in l : if f . hasProp ( "var" ) : ret . append ( f . prop ( "var" ) . decode ( "utf-8" ) ) return ret
5273	def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
13594	def print_information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest_stable or 'N/A' ) , fg = 'yellow' if handler . latest_stable else 'magenta' ) ) ) if label is not None : latest_revision = handler . latest_revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest_revision or 'N/A' ) , fg = 'yellow' if latest_revision else 'magenta' ) ) )
25	def store_args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg_names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional_args , ** keyword_args ) : self = positional_args [ 0 ] args = defaults . copy ( ) for name , value in zip ( arg_names , positional_args [ 1 : ] ) : args [ name ] = value args . update ( keyword_args ) self . __dict__ . update ( args ) return method ( * positional_args , ** keyword_args ) return wrapper
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
6352	def _remove_dupes ( self , phonetic ) : alt_string = phonetic alt_array = alt_string . split ( '|' ) result = '|' for i in range ( len ( alt_array ) ) : alt = alt_array [ i ] if alt and '|' + alt + '|' not in result : result += alt + '|' return result [ 1 : - 1 ]
4707	def power_on ( self , interval = 200 ) : if self . __power_on_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_ON" ) return 1 return self . __press ( self . __power_on_port , interval = interval )
3047	def _do_retrieve_scopes ( self , http , token ) : logger . info ( 'Refreshing scopes' ) query_params = { 'access_token' : token , 'fields' : 'scope' } token_info_uri = _helpers . update_query_params ( self . token_info_uri , query_params ) resp , content = transport . request ( http , token_info_uri ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . scopes = set ( _helpers . string_to_scopes ( d . get ( 'scope' , '' ) ) ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error_description' in d : error_msg = d [ 'error_description' ] except ( TypeError , ValueError ) : pass raise Error ( error_msg )
1271	def setup_components_and_tf_funcs ( self , custom_getter = None ) : self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( "Invalid internal initialization value." ) custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . distributions = self . create_distributions ( ) self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter
8299	def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
11780	def ContinuousXor ( n ) : "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints." examples = [ ] for i in range ( n ) : x , y = [ random . uniform ( 0.0 , 2.0 ) for i in '12' ] examples . append ( [ x , y , int ( x ) != int ( y ) ] ) return DataSet ( name = "continuous xor" , examples = examples )
5134	def generate_pagerank_graph ( num_vertices = 250 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) r = np . zeros ( num_vertices ) for k , pr in nx . pagerank ( g ) . items ( ) : r [ k ] = pr g = set_types_rank ( g , rank = r , ** kwargs ) return g
6718	def virtualenv_exists ( self , virtualenv_dir = None ) : r = self . local_renderer ret = True with self . settings ( warn_only = True ) : ret = r . run_or_local ( 'ls {virtualenv_dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
13693	def register ( self , service , name = '' ) : try : is_model = issubclass ( service , orb . Model ) except StandardError : is_model = False if is_model : self . services [ service . schema ( ) . dbname ( ) ] = ( ModelService , service ) else : super ( OrbApiFactory , self ) . register ( service , name = name )
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
12535	def copy_files_to_other_folder ( self , output_folder , rename_files = True , mkdir = True , verbose = False ) : import shutil if not os . path . exists ( output_folder ) : os . mkdir ( output_folder ) if not rename_files : for dcmf in self . items : outf = os . path . join ( output_folder , os . path . basename ( dcmf ) ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf ) else : n_pad = len ( self . items ) + 2 for idx , dcmf in enumerate ( self . items ) : outf = '{number:0{width}d}.dcm' . format ( width = n_pad , number = idx ) outf = os . path . join ( output_folder , outf ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf )
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
9392	def calc_key_stats ( self , metric_store ) : stats_to_calculate = [ 'mean' , 'std' , 'min' , 'max' ] percentiles_to_calculate = range ( 0 , 100 , 1 ) for column , groups_store in metric_store . items ( ) : for group , time_store in groups_store . items ( ) : data = metric_store [ column ] [ group ] . values ( ) if self . groupby : column_name = group + '.' + column else : column_name = column if column . startswith ( 'qps' ) : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( data , stats_to_calculate , percentiles_to_calculate ) else : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( list ( heapq . merge ( * data ) ) , stats_to_calculate , percentiles_to_calculate ) self . update_summary_stats ( column_name )
1044	def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) if Q >> BITS : raise ValueError ( "input out of range" ) sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : break else : return NO_MATCH elif them is None : return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
13426	def update_message ( self , message ) : url = "/2/messages/%s" % message . message_id data = self . _put_resource ( url , message . json_data ( ) ) return self . message_from_json ( data )
3241	def _get_base ( server_certificate , ** conn ) : server_certificate [ '_version' ] = 1 cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , ** conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
11848	def things_near ( self , location , radius = None ) : "Return all things within radius of location." if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
3383	def __build_problem ( self ) : prob = constraint_matrices ( self . model , zero_tol = self . feasibility_tol ) equalities = prob . equalities b = prob . b bounds = np . atleast_2d ( prob . bounds ) . T var_bounds = np . atleast_2d ( prob . variable_bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility_tol ) fixed_non_zero = np . abs ( prob . variable_bounds [ : , 1 ] ) > self . feasibility_tol fixed_non_zero &= prob . variable_fixed if any ( fixed_non_zero ) : n_fixed = fixed_non_zero . sum ( ) rows = np . zeros ( ( n_fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n_fixed ) , np . where ( fixed_non_zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var_b = prob . variable_bounds [ : , 1 ] b = np . hstack ( [ b , var_b [ fixed_non_zero ] ] ) homogeneous = False nulls = nullspace ( equalities ) return Problem ( equalities = shared_np_array ( equalities . shape , equalities ) , b = shared_np_array ( b . shape , b ) , inequalities = shared_np_array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared_np_array ( bounds . shape , bounds ) , variable_fixed = shared_np_array ( prob . variable_fixed . shape , prob . variable_fixed , integer = True ) , variable_bounds = shared_np_array ( var_bounds . shape , var_bounds ) , nullspace = shared_np_array ( nulls . shape , nulls ) , homogeneous = homogeneous )
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) self . _creationTracebackString = traceback . format_stack ( ) if self . _clsNumOutstanding >= g_max_concurrency : errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
1982	def sync ( f ) : def new_function ( self , * args , ** kw ) : self . _lock . acquire ( ) try : return f ( self , * args , ** kw ) finally : self . _lock . release ( ) return new_function
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
2577	def _gather_all_deps ( self , args , kwargs ) : depends = [ ] count = 0 for dep in args : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for key in kwargs : dep = kwargs [ key ] if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for dep in kwargs . get ( 'inputs' , [ ] ) : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) return count , depends
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
1739	def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
10796	def translate_fourier ( image , dx ) : N = image . shape [ 0 ] f = 2 * np . pi * np . fft . fftfreq ( N ) kx , ky , kz = np . meshgrid ( * ( f , ) * 3 , indexing = 'ij' ) kv = np . array ( [ kx , ky , kz ] ) . T q = np . fft . fftn ( image ) * np . exp ( - 1.j * ( kv * dx ) . sum ( axis = - 1 ) ) . T return np . real ( np . fft . ifftn ( q ) )
410	def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] inputs = self . _to_bc_h_w ( inputs , input_shape ) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) coords = tf . expand_dims ( grid_offset , 0 ) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals
9131	def ls ( cls , session : Optional [ Session ] = None ) -> List [ 'Action' ] : if session is None : session = _make_session ( ) actions = session . query ( cls ) . order_by ( cls . created . desc ( ) ) . all ( ) session . close ( ) return actions
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
8006	def handle_read ( self ) : with self . _lock : logger . debug ( "handle_read()" ) if self . _socket is None : return while True : try : sock , address = self . _socket . accept ( ) except socket . error , err : if err . args [ 0 ] in BLOCKING_ERRORS : break else : raise logger . debug ( "Accepted connection from: {0!r}" . format ( address ) ) self . _target ( sock , address )
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
4767	def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
579	def dictDiff ( da , db ) : different = False resultDict = dict ( ) resultDict [ 'inAButNotInB' ] = set ( da ) - set ( db ) if resultDict [ 'inAButNotInB' ] : different = True resultDict [ 'inBButNotInA' ] = set ( db ) - set ( da ) if resultDict [ 'inBButNotInA' ] : different = True resultDict [ 'differentValues' ] = [ ] for key in ( set ( da ) - resultDict [ 'inAButNotInB' ] ) : comparisonResult = da [ key ] == db [ key ] if isinstance ( comparisonResult , bool ) : isEqual = comparisonResult else : isEqual = comparisonResult . all ( ) if not isEqual : resultDict [ 'differentValues' ] . append ( key ) different = True assert ( ( ( resultDict [ 'inAButNotInB' ] or resultDict [ 'inBButNotInA' ] or resultDict [ 'differentValues' ] ) and different ) or not different ) return resultDict if different else None
7474	def write_to_fullarr ( data , sample , sidx ) : LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) with h5py . File ( data . clust_database , 'r+' ) as io5 : chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : newcatg = indat [ "icatg" ] onall = indat [ "inall" ] for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand_dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand_dims ( onall , axis = 1 )
11305	def autodiscover ( self , url ) : headers , response = fetch_url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider_data = json . loads ( response ) return self . store_providers ( provider_data )
8478	def install ( ) : cmd = CommandHelper ( ) cmd . install ( "npm" ) cmd = CommandHelper ( ) cmd . install ( "nodejs-legacy" ) cmd = CommandHelper ( ) cmd . command = "npm install -g retire" cmd . execute ( ) if cmd . errors : from termcolor import colored print colored ( cmd . errors , "red" ) else : print cmd . output
5864	def remove_organization_course ( organization , course_key ) : _validate_organization_data ( organization ) _validate_course_key ( course_key ) return data . delete_organization_course ( course_key = course_key , organization = organization )
6331	def encode ( self , word , terminator = '\0' ) : r if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
8458	def shell ( cmd , check = True , stdin = None , stdout = None , stderr = None ) : return subprocess . run ( cmd , shell = True , check = check , stdin = stdin , stdout = stdout , stderr = stderr )
7043	def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : series_median = npmedian ( fmags ) series_wmean = ( npsum ( fmags * ( 1.0 / ( ferrs * ferrs ) ) ) / npsum ( 1.0 / ( ferrs * ferrs ) ) ) series_mad = npmedian ( npabs ( fmags - series_median ) ) series_stdev = 1.483 * series_mad series_skew = spskew ( fmags ) series_kurtosis = spkurtosis ( fmags ) series_above1std = len ( fmags [ fmags > ( series_median + series_stdev ) ] ) series_below1std = len ( fmags [ fmags < ( series_median - series_stdev ) ] ) series_beyond1std = ( series_above1std + series_below1std ) / float ( ndet ) series_mag_percentiles = nppercentile ( fmags , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) return { 'median' : series_median , 'wmean' : series_wmean , 'mad' : series_mad , 'stdev' : series_stdev , 'skew' : series_skew , 'kurtosis' : series_kurtosis , 'beyond1std' : series_beyond1std , 'mag_percentiles' : series_mag_percentiles , 'mag_iqr' : series_mag_percentiles [ 8 ] - series_mag_percentiles [ 3 ] , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate light curve moments' ) return None
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
1776	def AND ( cpu , dest , src ) : if src . size == 64 and src . type == 'immediate' and dest . size == 64 : arg1 = Operators . SEXTEND ( src . read ( ) , 32 , 64 ) else : arg1 = src . read ( ) res = dest . write ( dest . read ( ) & arg1 ) cpu . _calculate_logic_flags ( dest . size , res )
11416	def record_add_subfield_into ( rec , tag , subfield_code , value , subfield_position = None , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) if subfield_position is None : subfields . append ( ( subfield_code , value ) ) else : subfields . insert ( subfield_position , ( subfield_code , value ) )
11626	def build ( self , pre = None , shortest = False ) : res = super ( Q , self ) . build ( pre , shortest = shortest ) if self . escape : return repr ( res ) elif self . html_js_escape : return ( "'" + res . encode ( "string_escape" ) . replace ( "<" , "\\x3c" ) . replace ( ">" , "\\x3e" ) + "'" ) else : return "" . join ( [ self . quote , res , self . quote ] )
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
13360	def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
13802	def _auth ( self , client_id , key , method , callback ) : available = auth_methods . keys ( ) if method not in available : raise Proauth2Error ( 'invalid_request' , 'unsupported authentication method: %s' 'available methods: %s' % ( method , '\n' . join ( available ) ) ) client = yield Task ( self . data_store . fetch , 'applications' , client_id = client_id ) if not client : raise Proauth2Error ( 'access_denied' ) if not auth_methods [ method ] ( key , client [ 'client_secret' ] ) : raise Proauth2Error ( 'access_denied' ) callback ( )
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
3773	def set_user_methods ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . T_cached = None
10981	def accept ( group_id ) : membership = Membership . query . get_or_404 ( ( current_user . get_id ( ) , group_id ) ) try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) ) flash ( _ ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) )
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
6326	def corpus_importer ( self , corpus , n_val = 1 , bos = '_START_' , eos = '_END_' ) : r if not corpus or not isinstance ( corpus , Corpus ) : raise TypeError ( 'Corpus argument of the Corpus class required.' ) sentences = corpus . sents ( ) for sent in sentences : ngs = Counter ( sent ) for key in ngs . keys ( ) : self . _add_to_ngcorpus ( self . ngcorpus , [ key ] , ngs [ key ] ) if n_val > 1 : if bos and bos != '' : sent = [ bos ] + sent if eos and eos != '' : sent += [ eos ] for i in range ( 2 , n_val + 1 ) : for j in range ( len ( sent ) - i + 1 ) : self . _add_to_ngcorpus ( self . ngcorpus , sent [ j : j + i ] , 1 )
4843	def get_common_course_modes ( self , course_run_ids ) : available_course_modes = None for course_run_id in course_run_ids : course_run = self . get_course_run ( course_run_id ) or { } course_run_modes = { seat . get ( 'type' ) for seat in course_run . get ( 'seats' , [ ] ) } if available_course_modes is None : available_course_modes = course_run_modes else : available_course_modes &= course_run_modes if not available_course_modes : return available_course_modes return available_course_modes
7021	def parallel_gen_binnedlc_pkls ( binnedpkldir , textlcdir , timebinsec , binnedpklglob = '*binned*sec*.pkl' , textlcglob = '*.tfalc.TF1*' ) : binnedpkls = sorted ( glob . glob ( os . path . join ( binnedpkldir , binnedpklglob ) ) ) textlcs = [ ] for bpkl in binnedpkls : objectid = HATIDREGEX . findall ( bpkl ) if objectid is not None : objectid = objectid [ 0 ] searchpath = os . path . join ( textlcdir , '%s-%s' % ( objectid , textlcglob ) ) textlcf = glob . glob ( searchpath ) if textlcf : textlcs . append ( textlcf ) else : textlcs . append ( None )
6876	def _gzip_sqlitecurve ( sqlitecurve , force = False ) : if force : cmd = 'gzip -k -f %s' % sqlitecurve else : cmd = 'gzip -k %s' % sqlitecurve try : outfile = '%s.gz' % sqlitecurve if os . path . exists ( outfile ) and not force : os . remove ( sqlitecurve ) return outfile else : subprocess . check_output ( cmd , shell = True ) if os . path . exists ( outfile ) : return outfile else : return None except subprocess . CalledProcessError : return None
3678	def legal_status ( self ) : r if self . __legal_status : return self . __legal_status else : self . __legal_status = legal_status ( self . CAS , Method = 'COMBINED' ) return self . __legal_status
3789	def property_derivative_T ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_T ( T , P , zs , ws , method , order ) except : pass return None
11764	def compute_utility ( self , board , move , player ) : "If X wins with this move, return 1; if O return -1; else return 0." if ( self . k_in_row ( board , move , player , ( 0 , 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 0 ) ) or self . k_in_row ( board , move , player , ( 1 , - 1 ) ) or self . k_in_row ( board , move , player , ( 1 , 1 ) ) ) : return if_ ( player == 'X' , + 1 , - 1 ) else : return 0
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
6740	def get_packager ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
11235	def get ( ) : config = { } try : config = _load_config ( ) except IOError : try : _create_default_config ( ) config = _load_config ( ) except IOError as e : raise ConfigError ( _FILE_CREATION_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( e . args [ 0 ] ) ) except Exception : raise ConfigError ( _JSON_SYNTAX_ERROR . format ( 'Yaml syntax error..' ) ) try : _validate ( config ) except KeyError as e : raise ConfigError ( _MANDATORY_KEY_ERROR . format ( e . args [ 0 ] ) ) except SyntaxError as e : raise ConfigError ( _INVALID_KEY_ERROR . format ( e . args [ 0 ] ) ) except ValueError as e : raise ConfigError ( _INVALID_VALUE_ERROR . format ( e . args [ 0 ] ) ) config [ 'projects-path' ] = os . path . expanduser ( config [ 'projects-path' ] ) _complete_config ( config ) return config
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
6298	def draw ( self , mesh , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : self . program [ "m_proj" ] . write ( projection_matrix ) self . program [ "m_mv" ] . write ( view_matrix ) mesh . vao . render ( self . program )
8427	def brewer_pal ( type = 'seq' , palette = 1 ) : def full_type_name ( text ) : abbrevs = { 'seq' : 'Sequential' , 'qual' : 'Qualitative' , 'div' : 'Diverging' } text = abbrevs . get ( text , text ) return text . title ( ) def number_to_palette_name ( ctype , n ) : n -= 1 palettes = sorted ( colorbrewer . COLOR_MAPS [ ctype ] . keys ( ) ) if n < len ( palettes ) : return palettes [ n ] raise ValueError ( "There are only '{}' palettes of type {}. " "You requested palette no. {}" . format ( len ( palettes ) , ctype , n + 1 ) ) def max_palette_colors ( type , palette_name ) : if type == 'Sequential' : return 9 elif type == 'Diverging' : return 11 else : qlimit = { 'Accent' : 8 , 'Dark2' : 8 , 'Paired' : 12 , 'Pastel1' : 9 , 'Pastel2' : 8 , 'Set1' : 9 , 'Set2' : 8 , 'Set3' : 12 } return qlimit [ palette_name ] type = full_type_name ( type ) if isinstance ( palette , int ) : palette_name = number_to_palette_name ( type , palette ) else : palette_name = palette nmax = max_palette_colors ( type , palette_name ) def _brewer_pal ( n ) : _n = n if n <= nmax else nmax try : bmap = colorbrewer . get_map ( palette_name , type , _n ) except ValueError as err : if 0 <= _n < 3 : bmap = colorbrewer . get_map ( palette_name , type , 3 ) else : raise err hex_colors = bmap . hex_colors [ : n ] if n > nmax : msg = ( "Warning message:" "Brewer palette {} has a maximum of {} colors" "Returning the palette you asked for with" "that many colors" . format ( palette_name , nmax ) ) warnings . warn ( msg ) hex_colors = hex_colors + [ None ] * ( n - nmax ) return hex_colors return _brewer_pal
9340	def flatten_dtype ( dtype , _next = None ) : types = [ ] if _next is None : _next = [ 0 , '' ] primary = True else : primary = False prefix = _next [ 1 ] if dtype . names is None : for i in numpy . ndindex ( dtype . shape ) : if dtype . base == dtype : types . append ( ( '%s%s' % ( prefix , simplerepr ( i ) ) , dtype ) ) _next [ 0 ] += 1 else : _next [ 1 ] = '%s%s' % ( prefix , simplerepr ( i ) ) types . extend ( flatten_dtype ( dtype . base , _next ) ) else : for field in dtype . names : typ_fields = dtype . fields [ field ] if len ( prefix ) > 0 : _next [ 1 ] = prefix + '.' + field else : _next [ 1 ] = '' + field flat_dt = flatten_dtype ( typ_fields [ 0 ] , _next ) types . extend ( flat_dt ) _next [ 1 ] = prefix if primary : return numpy . dtype ( types ) else : return types
10109	def schema ( tg ) : tables = { } for tname , table in tg . tabledict . items ( ) : t = TableSpec . from_table_metadata ( table ) tables [ t . name ] = t for at in t . many_to_many . values ( ) : tables [ at . name ] = at ordered = OrderedDict ( ) i = 0 while tables and i < 100 : i += 1 for table in list ( tables . keys ( ) ) : if all ( ( ref [ 1 ] in ordered ) or ref [ 1 ] == table for ref in tables [ table ] . foreign_keys ) : ordered [ table ] = tables . pop ( table ) break if tables : raise ValueError ( 'there seem to be cyclic dependencies between the tables' ) return list ( ordered . values ( ) )
8035	def summarize ( text , char_limit , sentence_filter = None , debug = False ) : debug_info = { } sents = list ( tools . sent_splitter_ja ( text ) ) words_list = [ w . encode ( 'utf-8' ) for s in sents for w in tools . word_segmenter_ja ( s ) ] tf = collections . Counter ( ) for words in words_list : for w in words : tf [ w ] += 1.0 if sentence_filter is not None : valid_indices = [ i for i , s in enumerate ( sents ) if sentence_filter ( s ) ] sents = [ sents [ i ] for i in valid_indices ] words_list = [ words_list [ i ] for i in valid_indices ] sent_ids = [ str ( i ) for i in range ( len ( sents ) ) ] sent_id2len = dict ( ( id_ , len ( s ) ) for id_ , s in zip ( sent_ids , sents ) ) word_contain = dict ( ) for id_ , words in zip ( sent_ids , words_list ) : word_contain [ id_ ] = collections . defaultdict ( lambda : 0 ) for w in words : word_contain [ id_ ] [ w ] = 1 prob = pulp . LpProblem ( 'summarize' , pulp . LpMaximize ) sent_vars = pulp . LpVariable . dicts ( 'sents' , sent_ids , 0 , 1 , pulp . LpBinary ) word_vars = pulp . LpVariable . dicts ( 'words' , tf . keys ( ) , 0 , 1 , pulp . LpBinary ) prob += pulp . lpSum ( [ tf [ w ] * word_vars [ w ] for w in tf ] ) prob += pulp . lpSum ( [ sent_id2len [ id_ ] * sent_vars [ id_ ] for id_ in sent_ids ] ) <= char_limit , 'lengthRequirement' for w in tf : prob += pulp . lpSum ( [ word_contain [ id_ ] [ w ] * sent_vars [ id_ ] for id_ in sent_ids ] ) >= word_vars [ w ] , 'z:{}' . format ( w ) prob . solve ( ) sent_indices = [ ] for v in prob . variables ( ) : if v . name . startswith ( 'sents' ) and v . varValue == 1 : sent_indices . append ( int ( v . name . split ( '_' ) [ - 1 ] ) ) return [ sents [ i ] for i in sent_indices ] , debug_info
500	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
11140	def reset ( self ) : self . __path = None self . __repo = { 'repository_unique_name' : str ( uuid . uuid1 ( ) ) , 'create_utctime' : time . time ( ) , 'last_update_utctime' : None , 'pyrep_version' : str ( __version__ ) , 'repository_information' : '' , 'walk_repo' : [ ] }
10863	def add_particle ( self , pos , rad ) : rad = listify ( rad ) inds = np . arange ( self . N , self . N + len ( rad ) ) self . pos = np . vstack ( [ self . pos , pos ] ) self . rad = np . hstack ( [ self . rad , np . zeros ( len ( rad ) ) ] ) self . setup_variables ( ) self . trigger_parameter_change ( ) params = self . param_particle_rad ( inds ) self . trigger_update ( params , rad ) return inds
8462	def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , ** kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , ** kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
3318	def get ( self , token ) : self . _lock . acquire_read ( ) try : lock = self . _dict . get ( token ) if lock is None : _logger . debug ( "Lock purged dangling: {}" . format ( token ) ) self . delete ( token ) return None expire = float ( lock [ "expire" ] ) if expire >= 0 and expire < time . time ( ) : _logger . debug ( "Lock timed-out({}): {}" . format ( expire , lock_string ( lock ) ) ) self . delete ( token ) return None return lock finally : self . _lock . release ( )
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
4712	def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
12151	def html_single_basic ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_basic.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDDD;">' html += '<span class="title">summary of data from: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' catOrder = [ "experiment" , "plot" , "tif" , "other" ] categories = cm . list_order_by ( filesByType . keys ( ) , catOrder ) for category in [ x for x in categories if len ( filesByType [ x ] ) ] : if category == 'experiment' : html += "<h3>Experimental Data:</h3>" elif category == 'plot' : html += "<h3>Intrinsic Properties:</h3>" elif category == 'tif' : html += "<h3>Micrographs:</h3>" elif category == 'other' : html += "<h3>Additional Files:</h3>" else : html += "<h3>????:</h3>" for fname in filesByType [ category ] : html += self . htmlFor ( fname ) html += '<br>' * 3 print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" del context del frames return modname , funcname , lineno
9950	def get_node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , _bind_args ( obj , args , kwargs )
8016	async def receive_json ( self , content , ** kwargs ) : if isinstance ( content , dict ) and "stream" in content and "payload" in content : steam_name = content [ "stream" ] payload = content [ "payload" ] if steam_name not in self . applications_accepting_frames : raise ValueError ( "Invalid multiplexed frame received (stream not mapped)" ) await self . send_upstream ( message = { "type" : "websocket.receive" , "text" : await self . encode_json ( payload ) } , stream_name = steam_name ) return else : raise ValueError ( "Invalid multiplexed **frame received (no channel/payload key)" )
13832	def _SkipFieldValue ( tokenizer ) : if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
12346	def compress ( self , delete_tif = False , folder = None ) : return compress ( self . images , delete_tif , folder )
12909	def from_json ( cls , fh ) : if isinstance ( fh , str ) : return cls ( json . loads ( fh ) ) else : return cls ( json . load ( fh ) )
5966	def get_lipid_vdwradii ( outdir = os . path . curdir , libdir = None ) : vdwradii_dat = os . path . join ( outdir , "vdwradii.dat" ) if libdir is not None : filename = os . path . join ( libdir , 'vdwradii.dat' ) if not os . path . exists ( filename ) : msg = 'No VDW database file found in {filename!r}.' . format ( ** vars ( ) ) logger . exception ( msg ) raise OSError ( msg , errno . ENOENT ) else : try : filename = os . path . join ( os . environ [ 'GMXLIB' ] , 'vdwradii.dat' ) except KeyError : try : filename = os . path . join ( os . environ [ 'GMXDATA' ] , 'top' , 'vdwradii.dat' ) except KeyError : msg = "Cannot find vdwradii.dat. Set GMXLIB (point to 'top') or GMXDATA ('share/gromacs')." logger . exception ( msg ) raise OSError ( msg , errno . ENOENT ) if not os . path . exists ( filename ) : msg = "Cannot find {filename!r}; something is wrong with the Gromacs installation." . format ( ** vars ( ) ) logger . exception ( msg , errno . ENOENT ) raise OSError ( msg ) patterns = vdw_lipid_resnames + list ( { x [ : 3 ] for x in vdw_lipid_resnames } ) with open ( vdwradii_dat , 'w' ) as outfile : outfile . write ( '; Special larger vdw radii for solvating lipid membranes\n' ) for resname in patterns : for atom , radius in vdw_lipid_atom_radii . items ( ) : outfile . write ( '{resname:4!s} {atom:<5!s} {radius:5.3f}\n' . format ( ** vars ( ) ) ) with open ( filename , 'r' ) as infile : for line in infile : outfile . write ( line ) logger . debug ( 'Created lipid vdW radii file {vdwradii_dat!r}.' . format ( ** vars ( ) ) ) return realpath ( vdwradii_dat )
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
2919	def Serializable ( o ) : if isinstance ( o , ( str , dict , int ) ) : return o else : try : json . dumps ( o ) return o except Exception : LOG . debug ( "Got a non-serilizeable object: %s" % o ) return o . __repr__ ( )
4424	def get ( self , guild_id ) : if guild_id not in self . _players : p = self . _player ( lavalink = self . lavalink , guild_id = guild_id ) self . _players [ guild_id ] = p return self . _players [ guild_id ]
4972	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
11655	def transform ( self , X , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . transform ( X . stacked_features , ** params ) return self . _gather_outputs ( X , X_new )
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
700	def getOrphanParticleInfos ( self , swarmId , genIdx ) : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if not entry [ 'hidden' ] : continue modelParams = entry [ 'modelParams' ] if modelParams [ 'particleState' ] [ 'swarmId' ] != swarmId : continue isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
12060	def TK_ask ( title , msg ) : root = tkinter . Tk ( ) root . attributes ( "-topmost" , True ) root . withdraw ( ) result = tkinter . messagebox . askyesno ( title , msg ) root . destroy ( ) return result
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
8975	def walk ( knitting_pattern ) : rows_before = { } free_rows = [ ] walk = [ ] for row in knitting_pattern . rows : rows_before_ = row . rows_before [ : ] if rows_before_ : rows_before [ row ] = rows_before_ else : free_rows . append ( row ) assert free_rows while free_rows : row = free_rows . pop ( 0 ) walk . append ( row ) assert row not in rows_before for freed_row in reversed ( row . rows_after ) : todo = rows_before [ freed_row ] todo . remove ( row ) if not todo : del rows_before [ freed_row ] free_rows . insert ( 0 , freed_row ) assert not rows_before , "everything is walked" return walk
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
4940	def link_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) self . get_or_create ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) except User . DoesNotExist : PendingEnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_email = user_email )
2870	def setup ( self , pin , mode ) : self . mraa_gpio . Gpio . dir ( self . mraa_gpio . Gpio ( pin ) , self . _dir_mapping [ mode ] )
11004	def psffunc ( self , x , y , z , ** kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_pinhole_psf else : func = psfcalc . calculate_pinhole_psf x0 , y0 = [ psfcalc . vec_to_halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap_and_calc_psf ( x0 , y0 , z , func , ** kwargs ) return vls / vls . sum ( )
3951	def _read_comparator ( self , mux , gain , data_rate , mode , high_threshold , low_threshold , active_low , traditional , latching , num_readings ) : assert num_readings == 1 or num_readings == 2 or num_readings == 4 , 'Num readings must be 1, 2, or 4!' self . _device . writeList ( ADS1x15_POINTER_HIGH_THRESHOLD , [ ( high_threshold >> 8 ) & 0xFF , high_threshold & 0xFF ] ) self . _device . writeList ( ADS1x15_POINTER_LOW_THRESHOLD , [ ( low_threshold >> 8 ) & 0xFF , low_threshold & 0xFF ] ) config = ADS1x15_CONFIG_OS_SINGLE config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] config |= mode if data_rate is None : data_rate = self . _data_rate_default ( ) config |= self . _data_rate_config ( data_rate ) if not traditional : config |= ADS1x15_CONFIG_COMP_WINDOW if not active_low : config |= ADS1x15_CONFIG_COMP_ACTIVE_HIGH if latching : config |= ADS1x15_CONFIG_COMP_LATCHING config |= ADS1x15_CONFIG_COMP_QUE [ num_readings ] self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) time . sleep ( 1.0 / data_rate + 0.0001 ) result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
7509	def _save ( self ) : fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
11289	def strip_xml_namespace ( root ) : try : root . tag = root . tag . split ( '}' ) [ 1 ] except IndexError : pass for element in root . getchildren ( ) : strip_xml_namespace ( element )
11789	def sample ( self ) : "Return a random sample from the distribution." if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
7633	def __get_dtype ( typespec ) : if 'type' in typespec : return __TYPE_MAP__ . get ( typespec [ 'type' ] , np . object_ ) elif 'enum' in typespec : return np . object_ elif 'oneOf' in typespec : types = [ __get_dtype ( v ) for v in typespec [ 'oneOf' ] ] if all ( [ t == types [ 0 ] for t in types ] ) : return types [ 0 ] return np . object_
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
6454	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _accents ) wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
4400	def _generate_ranges ( start_date , end_date ) : range_start = start_date while range_start < end_date : range_end = range_start + timedelta ( days = 60 ) yield ( range_start . strftime ( "%d/%m/%Y" ) , range_end . strftime ( "%d/%m/%Y" ) ) range_start += timedelta ( days = 30 )
13629	def put ( self , metrics ) : if type ( metrics ) == list : for metric in metrics : self . c . put_metric_data ( ** metric ) else : self . c . put_metric_data ( ** metrics )
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
9291	def db_value ( self , value ) : if not isinstance ( value , UUID ) : value = UUID ( value ) parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( OrderedUUIDField , self ) . db_value ( value )
7891	def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler . user_left ( self . me , None ) self . joined = False
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
6747	def get_hosts_for_site ( site = None ) : site = site or env . SITE hosts = set ( ) for hostname , _sites in six . iteritems ( env . available_sites_by_host ) : for _site in _sites : if _site == site : host_ip = get_host_ip ( hostname ) if host_ip : hosts . add ( host_ip ) break return list ( hosts )
3357	def _extend_nocheck ( self , iterable ) : current_length = len ( self ) list . extend ( self , iterable ) _dict = self . _dict if current_length is 0 : self . _generate_index ( ) return for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : _dict [ obj . id ] = i
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
690	def encodeValue ( self , value , toBeAdded = True ) : encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) if toBeAdded : self . encodings . append ( encodedValue ) self . numEncodings += 1 return encodedValue
3461	def single_gene_deletion ( model , gene_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'gene' , element_lists = _element_lists ( model . genes , gene_list ) , method = method , solution = solution , processes = processes , ** kwargs )
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
9779	def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( ** user . to_dict ( ) ) )
13081	def register_filters ( self ) : for _filter , instance in self . _filters : if not instance : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( flask_nemo . filters , _filter ) else : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( instance , _filter . replace ( "_{}" . format ( instance . name ) , "" ) )
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
5084	def unlink_learners ( self ) : sap_inactive_learners = self . client . get_inactive_sap_learners ( ) enterprise_customer = self . enterprise_configuration . enterprise_customer if not sap_inactive_learners : LOGGER . info ( 'Enterprise customer {%s} has no SAPSF inactive learners' , enterprise_customer . name ) return provider_id = enterprise_customer . identity_provider tpa_provider = get_identity_provider ( provider_id ) if not tpa_provider : LOGGER . info ( 'Enterprise customer {%s} has no associated identity provider' , enterprise_customer . name ) return None for sap_inactive_learner in sap_inactive_learners : social_auth_user = get_user_from_social_auth ( tpa_provider , sap_inactive_learner [ 'studentID' ] ) if not social_auth_user : continue try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = social_auth_user . email , ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : LOGGER . info ( 'Learner with email {%s} is not associated with Enterprise Customer {%s}' , social_auth_user . email , enterprise_customer . name )
2707	def top_sentences ( kernel , path ) : key_sent = { } i = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : graf = meta [ "graf" ] tagged_sent = [ WordNode . _make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged_sent ] ) m_sent = mh_digest ( [ str ( w . word_id ) for w in tagged_sent ] ) dist = sum ( [ m_sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key_sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key_sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield SummarySent ( dist = dist , idx = i , text = text )
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
896	def read ( cls , proto ) : tm = object . __new__ ( cls ) tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) tm . _random = Random ( ) tm . _random . read ( proto . random ) tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm
784	def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
9823	def delete ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not click . confirm ( "Are sure you want to delete project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without deleting project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . delete_project ( user , project_name ) local_project = ProjectManager . get_config ( ) if local_project and ( user , project_name ) == ( local_project . user , local_project . name ) : ProjectManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete project `{}/{}`.' . format ( user , project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Project `{}/{}` was delete successfully" . format ( user , project_name ) )
10788	def add_subtract_locally ( st , region_depth = 3 , filter_size = 5 , sigma_cutoff = 8 , ** kwargs ) : tiles = identify_misfeatured_regions ( st , filter_size = filter_size , sigma_cutoff = sigma_cutoff ) n_empty = 0 n_added = 0 new_poses = [ ] for t in tiles : curn , curinds = add_subtract_misfeatured_tile ( st , t , ** kwargs ) if curn == 0 : n_empty += 1 else : n_added += curn new_poses . extend ( st . obj_get_positions ( ) [ curinds ] ) if n_empty > region_depth : break else : pass return n_added , new_poses
12508	def get_3D_coordmap ( img ) : if isinstance ( img , nib . Nifti1Image ) : img = nifti2nipy ( img ) if img . ndim == 4 : from nipy . core . reference . coordinate_map import drop_io_dim cm = drop_io_dim ( img . coordmap , 3 ) else : cm = img . coordmap return cm
8644	def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13191	def json_struct_to_xml ( json_obj , root , custom_namespace = None ) : if isinstance ( root , ( str , unicode ) ) : if root . startswith ( '!' ) : root = etree . Element ( '{%s}%s' % ( NS_PROTECTED , root [ 1 : ] ) ) elif root . startswith ( '+' ) : if not custom_namespace : raise Exception ( "JSON fields starts with +, but no custom namespace provided" ) root = etree . Element ( '{%s}%s' % ( custom_namespace , root [ 1 : ] ) ) else : root = etree . Element ( root ) if root . tag in ( 'attachments' , 'grouped_events' , 'media_files' ) : for link in json_obj : root . append ( json_link_to_xml ( link ) ) elif isinstance ( json_obj , ( str , unicode ) ) : root . text = json_obj elif isinstance ( json_obj , ( int , float ) ) : root . text = unicode ( json_obj ) elif isinstance ( json_obj , dict ) : if frozenset ( json_obj . keys ( ) ) == frozenset ( ( 'type' , 'coordinates' ) ) : root . append ( geojson_to_gml ( json_obj ) ) else : for key , val in json_obj . items ( ) : if key == 'url' or key . endswith ( '_url' ) : el = json_link_to_xml ( val , json_link_key_to_xml_rel ( key ) ) else : el = json_struct_to_xml ( val , key , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif isinstance ( json_obj , list ) : tag_name = root . tag if tag_name . endswith ( 'ies' ) : tag_name = tag_name [ : - 3 ] + 'y' elif tag_name . endswith ( 's' ) : tag_name = tag_name [ : - 1 ] for val in json_obj : el = json_struct_to_xml ( val , tag_name , custom_namespace = custom_namespace ) if el is not None : root . append ( el ) elif json_obj is None : return None else : raise NotImplementedError return root
10865	def _tile ( self , n ) : zsc = np . array ( [ 1.0 / self . zscale , 1 , 1 ] ) pos , rad = self . pos [ n ] , self . rad [ n ] pos = self . _trans ( pos ) return Tile ( pos - zsc * rad , pos + zsc * rad ) . pad ( self . support_pad )
10669	def _get_default_data_path_ ( ) : module_path = os . path . dirname ( sys . modules [ __name__ ] . __file__ ) data_path = os . path . join ( module_path , r'data/rao' ) data_path = os . path . abspath ( data_path ) return data_path
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
7518	def maxind_numba ( block ) : inds = 0 for row in xrange ( block . shape [ 0 ] ) : where = np . where ( block [ row ] != 45 ) [ 0 ] if len ( where ) == 0 : obs = 100 else : left = np . min ( where ) right = np . max ( where ) obs = np . sum ( block [ row , left : right ] == 45 ) if obs > inds : inds = obs return inds
11689	def get_metadata ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content ) . getchildren ( ) [ 0 ]
12789	def get ( self , q = None , page = None ) : etag = generate_etag ( current_ext . content_version . encode ( 'utf8' ) ) self . check_etag ( etag , weak = True ) res = jsonify ( current_ext . styles ) res . set_etag ( etag ) return res
5791	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : certs_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustCopyAnchorCertificates ( certs_pointer_pointer ) handle_sec_error ( res ) certs_pointer = unwrap ( certs_pointer_pointer ) certificates = { } trust_info = { } all_purposes = '2.5.29.37.0' default_trust = ( set ( ) , set ( ) ) length = CoreFoundation . CFArrayGetCount ( certs_pointer ) for index in range ( 0 , length ) : cert_pointer = CoreFoundation . CFArrayGetValueAtIndex ( certs_pointer , index ) der_cert , cert_hash = _cert_details ( cert_pointer ) certificates [ cert_hash ] = der_cert CoreFoundation . CFRelease ( certs_pointer ) for domain in [ SecurityConst . kSecTrustSettingsDomainUser , SecurityConst . kSecTrustSettingsDomainAdmin ] : cert_trust_settings_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustSettingsCopyCertificates ( domain , cert_trust_settings_pointer_pointer ) if res == SecurityConst . errSecNoTrustSettings : continue handle_sec_error ( res ) cert_trust_settings_pointer = unwrap ( cert_trust_settings_pointer_pointer ) length = CoreFoundation . CFArrayGetCount ( cert_trust_settings_pointer ) for index in range ( 0 , length ) : cert_pointer = CoreFoundation . CFArrayGetValueAtIndex ( cert_trust_settings_pointer , index ) trust_settings_pointer_pointer = new ( CoreFoundation , 'CFArrayRef *' ) res = Security . SecTrustSettingsCopyTrustSettings ( cert_pointer , domain , trust_settings_pointer_pointer ) if res == SecurityConst . errSecItemNotFound : continue if res == SecurityConst . errSecInvalidTrustSettings : der_cert , cert_hash = _cert_details ( cert_pointer ) if cert_hash in certificates : _cert_callback ( cert_callback , certificates [ cert_hash ] , 'invalid trust settings' ) del certificates [ cert_hash ] continue handle_sec_error ( res ) trust_settings_pointer = unwrap ( trust_settings_pointer_pointer ) trust_oids = set ( ) reject_oids = set ( ) settings_length = CoreFoundation . CFArrayGetCount ( trust_settings_pointer ) for settings_index in range ( 0 , settings_length ) : settings_dict_entry = CoreFoundation . CFArrayGetValueAtIndex ( trust_settings_pointer , settings_index ) settings_dict = CFHelpers . cf_dictionary_to_dict ( settings_dict_entry ) policy_oid = settings_dict . get ( 'kSecTrustSettingsPolicy' , { } ) . get ( 'SecPolicyOid' , all_purposes ) trust_result = settings_dict . get ( 'kSecTrustSettingsResult' , 1 ) should_trust = trust_result != 0 and trust_result != 3 if should_trust : trust_oids . add ( policy_oid ) else : reject_oids . add ( policy_oid ) der_cert , cert_hash = _cert_details ( cert_pointer ) if all_purposes in reject_oids : if cert_hash in certificates : _cert_callback ( cert_callback , certificates [ cert_hash ] , 'explicitly distrusted' ) del certificates [ cert_hash ] else : if all_purposes in trust_oids : trust_oids = set ( [ all_purposes ] ) trust_info [ cert_hash ] = ( trust_oids , reject_oids ) CoreFoundation . CFRelease ( trust_settings_pointer ) CoreFoundation . CFRelease ( cert_trust_settings_pointer ) output = [ ] for cert_hash in certificates : if not callback_only_on_failure : _cert_callback ( cert_callback , certificates [ cert_hash ] , None ) cert_trust_info = trust_info . get ( cert_hash , default_trust ) output . append ( ( certificates [ cert_hash ] , cert_trust_info [ 0 ] , cert_trust_info [ 1 ] ) ) return output
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
8893	def calculate_uuid ( self ) : if self . uuid_input_fields is None : raise NotImplementedError ( ) if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) if not hashable_input : return uuid . uuid4 ( ) . hex return sha2_uuid ( hashable_input )
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( ** params or { } )
7915	def list_all ( cls , basic = None ) : if basic is None : return [ s for s in cls . _defs ] else : return [ s . name for s in cls . _defs . values ( ) if s . basic == basic ]
13525	def error ( code : int , * args , ** kwargs ) -> HedgehogCommandError : if code == FAILED_COMMAND and len ( args ) >= 1 and args [ 0 ] == "Emergency Shutdown activated" : return EmergencyShutdown ( * args , ** kwargs ) return _errors [ code ] ( * args , ** kwargs )
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
2477	def add_lic_xref ( self , doc , ref ) : if self . has_extr_lic ( doc ) : self . extr_lic ( doc ) . add_xref ( ref ) return True else : raise OrderError ( 'ExtractedLicense::CrossRef' )
8734	def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( )
9145	def clear ( skip ) : for name in sorted ( MODULES ) : if name in skip : continue click . secho ( f'clearing cache for {name}' , fg = 'cyan' , bold = True ) clear_cache ( name )
1445	def register_metric ( self , name , metric , time_bucket_in_sec ) : if name in self . metrics_map : raise RuntimeError ( "Another metric has already been registered with name: %s" % name ) Log . debug ( "Register metric: %s, with interval: %s" , name , str ( time_bucket_in_sec ) ) self . metrics_map [ name ] = metric if time_bucket_in_sec in self . time_bucket_in_sec_to_metrics_name : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] . append ( name ) else : self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] = [ name ] self . _register_timer_task ( time_bucket_in_sec )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , ** kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( ** params ) else : params [ 'Body' ] = file resp = await self . put_object ( ** params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
8049	def run ( self ) : if self . err is not None : assert self . source is None msg = "%s%03i %s" % ( rst_prefix , rst_fail_load , "Failed to load file: %s" % self . err , ) yield 0 , 0 , msg , type ( self ) module = [ ] try : module = parse ( StringIO ( self . source ) , self . filename ) except SyntaxError as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_parse , "Failed to parse file: %s" % err , ) yield 0 , 0 , msg , type ( self ) module = [ ] except AllError : msg = "%s%03i %s" % ( rst_prefix , rst_fail_all , "Failed to parse __all__ entry." , ) yield 0 , 0 , msg , type ( self ) module = [ ] for definition in module : if not definition . docstring : continue try : unindented = trim ( dequote_docstring ( definition . docstring ) ) rst_errors = list ( rst_lint . lint ( unindented ) ) except Exception as err : msg = "%s%03i %s" % ( rst_prefix , rst_fail_lint , "Failed to lint docstring: %s - %s" % ( definition . name , err ) , ) yield definition . start , 0 , msg , type ( self ) continue for rst_error in rst_errors : if rst_error . level <= 1 : continue msg = rst_error . message . split ( "\n" , 1 ) [ 0 ] code = code_mapping ( rst_error . level , msg ) assert code < 100 , code code += 100 * rst_error . level msg = "%s%03i %s" % ( rst_prefix , code , msg ) yield definition . start + rst_error . line , 0 , msg , type ( self )
5577	def load_input_reader ( input_params , readonly = False ) : logger . debug ( "find input reader with params %s" , input_params ) if not isinstance ( input_params , dict ) : raise TypeError ( "input_params must be a dictionary" ) if "abstract" in input_params : driver_name = input_params [ "abstract" ] [ "format" ] elif "path" in input_params : if os . path . splitext ( input_params [ "path" ] ) [ 1 ] : input_file = input_params [ "path" ] driver_name = driver_from_file ( input_file ) else : logger . debug ( "%s is a directory" , input_params [ "path" ] ) driver_name = "TileDirectory" else : raise MapcheteDriverError ( "invalid input parameters %s" % input_params ) for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "driver_name" ] == driver_name ) : return v . load ( ) . InputData ( input_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , ** kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
4212	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = CommandLineTool ( ) return cli . run ( argv )
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
6863	def tic_single_object_crossmatch ( ra , dec , radius ) : for val in ra , dec , radius : if not isinstance ( val , float ) : raise AssertionError ( 'plz input ra,dec,radius in decimal degrees' ) crossmatchInput = { "fields" : [ { "name" : "ra" , "type" : "float" } , { "name" : "dec" , "type" : "float" } ] , "data" : [ { "ra" : ra , "dec" : dec } ] } request = { "service" : "Mast.Tic.Crossmatch" , "data" : crossmatchInput , "params" : { "raColumn" : "ra" , "decColumn" : "dec" , "radius" : radius } , "format" : "json" , 'removecache' : True } headers , out_string = _mast_query ( request ) out_data = json . loads ( out_string ) return out_data
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
5760	def configure_ci_jobs ( config_url , rosdistro_name , ci_build_name , groovy_script = None , dry_run = False ) : config = get_config_index ( config_url ) build_files = get_ci_build_files ( config , rosdistro_name ) build_file = build_files [ ci_build_name ] index = get_index ( config . rosdistro_index_url ) targets = [ ] for os_name in build_file . targets . keys ( ) : for os_code_name in build_file . targets [ os_name ] . keys ( ) : for arch in build_file . targets [ os_name ] [ os_code_name ] : targets . append ( ( os_name , os_code_name , arch ) ) print ( 'The build file contains the following targets:' ) for os_name , os_code_name , arch in targets : print ( ' -' , os_name , os_code_name , arch ) dist_file = get_distribution_file ( index , rosdistro_name , build_file ) if not dist_file : print ( 'No distribution file matches the build file' ) return ci_view_name = get_ci_view_name ( rosdistro_name ) from ros_buildfarm . jenkins import connect jenkins = connect ( config . jenkins_url ) if groovy_script is None else False view_configs = { } views = { ci_view_name : configure_ci_view ( jenkins , ci_view_name , dry_run = dry_run ) } if not jenkins : view_configs . update ( views ) groovy_data = { 'dry_run' : dry_run , 'expected_num_views' : len ( view_configs ) , } ci_job_names = [ ] job_configs = OrderedDict ( ) is_disabled = False for os_name , os_code_name , arch in targets : try : job_name , job_config = configure_ci_job ( config_url , rosdistro_name , ci_build_name , os_name , os_code_name , arch , config = config , build_file = build_file , index = index , dist_file = dist_file , jenkins = jenkins , views = views , is_disabled = is_disabled , groovy_script = groovy_script , dry_run = dry_run , trigger_timer = build_file . jenkins_job_schedule ) ci_job_names . append ( job_name ) if groovy_script is not None : print ( "Configuration for job '%s'" % job_name ) job_configs [ job_name ] = job_config except JobValidationError as e : print ( e . message , file = sys . stderr ) groovy_data [ 'expected_num_jobs' ] = len ( job_configs ) groovy_data [ 'job_prefixes_and_names' ] = { } if groovy_script is not None : print ( "Writing groovy script '%s' to reconfigure %d jobs" % ( groovy_script , len ( job_configs ) ) ) content = expand_template ( 'snippet/reconfigure_jobs.groovy.em' , groovy_data ) write_groovy_script_and_configs ( groovy_script , content , job_configs , view_configs )
8565	def update_loadbalancer ( self , datacenter_id , loadbalancer_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
13708	def check_ip ( self , ip ) : self . _last_result = None if is_valid_ipv4 ( ip ) : key = None if self . _use_cache : key = self . _make_cache_key ( ip ) self . _last_result = self . _cache . get ( key , version = self . _cache_version ) if self . _last_result is None : error , age , threat , type = self . _request_httpbl ( ip ) if error == 127 or error == 0 : self . _last_result = { 'error' : error , 'age' : age , 'threat' : threat , 'type' : type } if self . _use_cache : self . _cache . set ( key , self . _last_result , timeout = self . _api_timeout , version = self . _cache_version ) if self . _last_result is not None and settings . CACHED_HTTPBL_USE_LOGGING : logger . info ( 'httpBL check ip: {0}; ' 'httpBL result: error: {1}, age: {2}, threat: {3}, type: {4}' . format ( ip , self . _last_result [ 'error' ] , self . _last_result [ 'age' ] , self . _last_result [ 'threat' ] , self . _last_result [ 'type' ] ) ) return self . _last_result
1900	def get_all_values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , ConstraintSet ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp_cs . new_array ( index_max = expression . index_max , value_bits = expression . value_bits , taint = expression . taint ) . array else : raise NotImplementedError ( f"get_all_values only implemented for {type(expression)} expression type." ) temp_cs . add ( var == expression ) self . _reset ( temp_cs . to_string ( related_to = var ) ) result = [ ] while self . _is_sat ( ) : value = self . _getvalue ( var ) result . append ( value ) self . _assert ( var != value ) if len ( result ) >= maxcnt : if silent : break else : raise TooManySolutions ( result ) return result
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
4935	def chunks ( dictionary , chunk_size ) : iterable = iter ( dictionary ) for __ in range ( 0 , len ( dictionary ) , chunk_size ) : yield { key : dictionary [ key ] for key in islice ( iterable , chunk_size ) }
4830	def get_course_grade ( self , course_id , username ) : results = self . client . courses ( course_id ) . get ( username = username ) for row in results : if row . get ( 'username' ) == username : return row raise HttpNotFoundError ( 'No grade record found for course={}, username={}' . format ( course_id , username ) )
10652	def prepare_to_run ( self , clock , period_count ) : for c in self . components : c . prepare_to_run ( clock , period_count ) for a in self . activities : a . prepare_to_run ( clock , period_count )
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
5940	def _build_arg_list ( self , ** kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] if not flag . startswith ( '-' ) : flag = '-' + flag if value is True : arglist . append ( flag ) elif value is False : if flag . startswith ( '-no' ) : arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) elif value is None : pass else : try : arglist . extend ( [ flag ] + value ) except TypeError : arglist . extend ( [ flag , value ] ) return list ( map ( str , arglist ) )
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
9300	def list ( self , filters , cursor , count ) : assert isinstance ( filters , dict ) , "expected filters type 'dict'" assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" query = self . get_query ( ) assert isinstance ( query , peewee . Query ) paginator = self . get_paginator ( ) assert isinstance ( paginator , Pagination ) count += 1 pquery = paginator . filter_query ( query , cursor , count ) items = [ item for item in pquery ] next_item = items . pop ( 1 ) next_cursor = next_item . to_cursor_ref ( ) return items , next_cursor
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice print ( self . _chunksize ) rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break fillsets [ : ] = tmpr del tmpr
2030	def CODECOPY ( self , mem_offset , code_offset , size ) : self . _allocate ( mem_offset , size ) GCOPY = 3 copyfee = self . safe_mul ( GCOPY , Operators . UDIV ( self . safe_add ( size , 31 ) , 32 ) ) self . _consume ( copyfee ) if issymbolic ( size ) : max_size = solver . max ( self . constraints , size ) else : max_size = size for i in range ( max_size ) : if issymbolic ( i < size ) : default = Operators . ITEBV ( 8 , i < size , 0 , self . _load ( mem_offset + i , 1 ) ) else : if i < size : default = 0 else : default = self . _load ( mem_offset + i , 1 ) if issymbolic ( code_offset ) : value = Operators . ITEBV ( 8 , code_offset + i >= len ( self . bytecode ) , default , self . bytecode [ code_offset + i ] ) else : if code_offset + i >= len ( self . bytecode ) : value = default else : value = self . bytecode [ code_offset + i ] self . _store ( mem_offset + i , value ) self . _publish ( 'did_evm_read_code' , code_offset , size )
892	def columnForCell ( self , cell ) : self . _validateCell ( cell ) return int ( cell / self . cellsPerColumn )
4526	def run ( self , next_task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next_task . event . set ( )
8053	def listener ( self , sock , * args ) : conn , addr = sock . accept ( ) f = conn . makefile ( conn ) self . shell = ShoebotCmd ( self . bot , stdin = f , stdout = f , intro = INTRO ) print ( _ ( "Connected" ) ) GObject . io_add_watch ( conn , GObject . IO_IN , self . handler ) if self . shell . intro : self . shell . stdout . write ( str ( self . shell . intro ) + "\n" ) self . shell . stdout . flush ( ) return True
7427	def refmap_init ( data , sample , force ) : sample . files . unmapped_reads = os . path . join ( data . dirs . edits , "{}-refmap_derep.fastq" . format ( sample . name ) ) sample . files . mapped_reads = os . path . join ( data . dirs . refmapping , "{}-mapped-sorted.bam" . format ( sample . name ) )
883	def activateDendrites ( self , learn = True ) : ( numActiveConnected , numActivePotential ) = self . connections . computeActivity ( self . activeCells , self . connectedPermanence ) activeSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActiveConnected ) ) if numActiveConnected [ i ] >= self . activationThreshold ) matchingSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActivePotential ) ) if numActivePotential [ i ] >= self . minThreshold ) self . activeSegments = sorted ( activeSegments , key = self . connections . segmentPositionSortKey ) self . matchingSegments = sorted ( matchingSegments , key = self . connections . segmentPositionSortKey ) self . numActiveConnectedSynapsesForSegment = numActiveConnected self . numActivePotentialSynapsesForSegment = numActivePotential if learn : for segment in self . activeSegments : self . lastUsedIterationForSegment [ segment . flatIdx ] = self . iteration self . iteration += 1
3175	def create ( self , campaign_id , data , ** queryparams ) : self . campaign_id = campaign_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) response = self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'feedback' ) , data = data , ** queryparams ) if response is not None : self . feedback_id = response [ 'feedback_id' ] else : self . feedback_id = None return response
2221	def _rectify_hasher ( hasher ) : if xxhash is not None : if hasher in { 'xxh32' , 'xx32' , 'xxhash' } : return xxhash . xxh32 if hasher in { 'xxh64' , 'xx64' } : return xxhash . xxh64 if hasher is NoParam or hasher == 'default' : hasher = DEFAULT_HASHER elif isinstance ( hasher , six . string_types ) : if hasher not in hashlib . algorithms_available : raise KeyError ( 'unknown hasher: {}' . format ( hasher ) ) else : hasher = getattr ( hashlib , hasher ) elif isinstance ( hasher , HASH ) : return lambda : hasher return hasher
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) packet += HeronProtocol . pack_int ( datasize ) packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename packet += reqid . pack ( ) packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
8655	def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
8413	def to_numeric ( self , td ) : if self . package == 'pandas' : return td . value / NANOSECONDS [ self . units ] else : return td . total_seconds ( ) / SECONDS [ self . units ]
3361	def shadow_price ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . _model . constraints [ self . id ] . dual except AttributeError : raise RuntimeError ( "metabolite '{}' is not part of a model" . format ( self . id ) ) except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
3118	def _create_file_if_needed ( self ) : if not os . path . exists ( self . _filename ) : old_umask = os . umask ( 0o177 ) try : open ( self . _filename , 'a+b' ) . close ( ) finally : os . umask ( old_umask )
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
9091	def _make_namespace ( self ) -> Namespace : namespace = Namespace ( name = self . _get_namespace_name ( ) , keyword = self . _get_namespace_keyword ( ) , url = self . _get_namespace_url ( ) , version = str ( time . asctime ( ) ) , ) self . session . add ( namespace ) entries = self . _get_namespace_entries ( namespace ) self . session . add_all ( entries ) t = time . time ( ) log . info ( 'committing models' ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t ) return namespace
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) links = self . _parse_link_headers ( request [ 'Link' ] ) for link in links : self . relate ( target , read ( self , link [ 'uri' ] ) ) self . response . status = http . client . NO_CONTENT self . make_response ( )
4149	def onesided_gen ( self ) : if self . N % 2 == 0 : for n in range ( 0 , self . N // 2 + 1 ) : yield n * self . df else : for n in range ( 0 , ( self . N + 1 ) // 2 ) : yield n * self . df
4286	def write ( self , album ) : page = self . template . render ( ** self . generate_context ( album ) ) output_file = os . path . join ( album . dst_path , album . output_file ) with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
6851	def get_public_ip ( self ) : r = self . local_renderer ret = r . run ( r . env . get_public_ip_command ) or '' ret = ret . strip ( ) print ( 'ip:' , ret ) return ret
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
200	def draw_on_image ( self , image , alpha = 0.75 , resize = "segmentation_map" , background_threshold = 0.01 , background_class_id = None , colors = None , draw_background = False ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "segmentation_map" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr . shape [ 0 : 2 ] , interpolation = "cubic" ) segmap_drawn , foreground_mask = self . draw ( background_threshold = background_threshold , background_class_id = background_class_id , size = image . shape [ 0 : 2 ] if resize == "segmentation_map" else None , colors = colors , return_foreground_mask = True ) if draw_background : mix = np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) else : foreground_mask = foreground_mask [ ... , np . newaxis ] mix = np . zeros_like ( image ) mix += ( ~ foreground_mask ) . astype ( np . uint8 ) * image mix += foreground_mask . astype ( np . uint8 ) * np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) return mix
9601	def wait_for_element ( self , using , value , timeout = 10000 , interval = 1000 , asserter = is_displayed ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for_element ( ctx , using , value ) : el = ctx . element ( using , value ) asserter ( el ) return el return _wait_for_element ( self , using , value )
9833	def initialize ( self ) : return self . DXclasses [ self . type ] ( self . id , ** self . args )
4304	def _get_valid_formats ( ) : if NO_SOX : return [ ] so = subprocess . check_output ( [ 'sox' , '-h' ] ) if type ( so ) is not str : so = str ( so , encoding = 'UTF-8' ) so = so . split ( '\n' ) idx = [ i for i in range ( len ( so ) ) if 'AUDIO FILE FORMATS:' in so [ i ] ] [ 0 ] formats = so [ idx ] . split ( ' ' ) [ 3 : ] return formats
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) obj . _origData [ thisField ] = newValue
8694	def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main ( )
7720	def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : child . unlinkNode ( ) child . freeNode ( ) break if parameters . maxchars and parameters . maxchars < 0 : raise ValueError ( "History parameter maxchars must be positive" ) if parameters . maxstanzas and parameters . maxstanzas < 0 : raise ValueError ( "History parameter maxstanzas must be positive" ) if parameters . maxseconds and parameters . maxseconds < 0 : raise ValueError ( "History parameter maxseconds must be positive" ) hnode = self . xmlnode . newChild ( self . xmlnode . ns ( ) , "history" , None ) if parameters . maxchars is not None : hnode . setProp ( "maxchars" , str ( parameters . maxchars ) ) if parameters . maxstanzas is not None : hnode . setProp ( "maxstanzas" , str ( parameters . maxstanzas ) ) if parameters . maxseconds is not None : hnode . setProp ( "maxseconds" , str ( parameters . maxseconds ) ) if parameters . since is not None : hnode . setProp ( "since" , parameters . since . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) )
3419	def save_matlab_model ( model , file_name , varname = None ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) if varname is None : varname = str ( model . id ) if model . id is not None and len ( model . id ) > 0 else "exported_model" mat = create_mat_dict ( model ) scipy_io . savemat ( file_name , { varname : mat } , appendmat = True , oned_as = "column" )
6128	def _build_backend ( ) : backend_path = os . environ . get ( 'PEP517_BACKEND_PATH' ) if backend_path : extra_pathitems = backend_path . split ( os . pathsep ) sys . path [ : 0 ] = extra_pathitems ep = os . environ [ 'PEP517_BUILD_BACKEND' ] mod_path , _ , obj_path = ep . partition ( ':' ) try : obj = import_module ( mod_path ) except ImportError : raise BackendUnavailable ( traceback . format_exc ( ) ) if backend_path : if not any ( contained_in ( obj . __file__ , path ) for path in extra_pathitems ) : raise BackendInvalid ( "Backend was not loaded from backend-path" ) if obj_path : for path_part in obj_path . split ( '.' ) : obj = getattr ( obj , path_part ) return obj
5484	def execute ( api ) : try : return api . execute ( ) except Exception as exception : now = datetime . now ( ) . strftime ( '%Y-%m-%d %H:%M:%S.%f' ) _print_error ( '%s: Exception %s: %s' % ( now , type ( exception ) . __name__ , str ( exception ) ) ) raise exception
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
6897	def _periodicfeatures_worker ( task ) : pfpickle , lcbasedir , outdir , starfeatures , kwargs = task try : return get_periodicfeatures ( pfpickle , lcbasedir , outdir , starfeatures = starfeatures , ** kwargs ) except Exception as e : LOGEXCEPTION ( 'failed to get periodicfeatures for %s' % pfpickle )
2507	def get_extr_lics_xref ( self , extr_lic ) : xrefs = list ( self . graph . triples ( ( extr_lic , RDFS . seeAlso , None ) ) ) return map ( lambda xref_triple : xref_triple [ 2 ] , xrefs )
3342	def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
7004	def train_rf_classifier ( collected_features , test_fraction = 0.25 , n_crossval_iterations = 20 , n_kfolds = 5 , crossval_scoring_metric = 'f1' , classifier_to_pickle = None , nworkers = - 1 , ) : if ( isinstance ( collected_features , str ) and os . path . exists ( collected_features ) ) : with open ( collected_features , 'rb' ) as infd : fdict = pickle . load ( infd ) elif isinstance ( collected_features , dict ) : fdict = collected_features else : LOGERROR ( "can't figure out the input collected_features arg" ) return None tfeatures = fdict [ 'features_array' ] tlabels = fdict [ 'labels_array' ] tfeaturenames = fdict [ 'availablefeatures' ] tmagcol = fdict [ 'magcol' ] tobjectids = fdict [ 'objectids' ] training_features , testing_features , training_labels , testing_labels = ( train_test_split ( tfeatures , tlabels , test_size = test_fraction , random_state = RANDSEED , stratify = tlabels ) ) clf = RandomForestClassifier ( n_jobs = nworkers , random_state = RANDSEED ) rf_hyperparams = { "max_depth" : [ 3 , 4 , 5 , None ] , "n_estimators" : sp_randint ( 100 , 2000 ) , "max_features" : sp_randint ( 1 , 5 ) , "min_samples_split" : sp_randint ( 2 , 11 ) , "min_samples_leaf" : sp_randint ( 2 , 11 ) , } cvsearch = RandomizedSearchCV ( clf , param_distributions = rf_hyperparams , n_iter = n_crossval_iterations , scoring = crossval_scoring_metric , cv = StratifiedKFold ( n_splits = n_kfolds , shuffle = True , random_state = RANDSEED ) , random_state = RANDSEED ) LOGINFO ( 'running grid-search CV to optimize RF hyperparameters...' ) cvsearch_classifiers = cvsearch . fit ( training_features , training_labels ) _gridsearch_report ( cvsearch_classifiers . cv_results_ ) bestclf = cvsearch_classifiers . best_estimator_ bestclf_score = cvsearch_classifiers . best_score_ bestclf_hyperparams = cvsearch_classifiers . best_params_ test_predicted_labels = bestclf . predict ( testing_features ) recscore = recall_score ( testing_labels , test_predicted_labels ) precscore = precision_score ( testing_labels , test_predicted_labels ) f1score = f1_score ( testing_labels , test_predicted_labels ) confmatrix = confusion_matrix ( testing_labels , test_predicted_labels ) outdict = { 'features' : tfeatures , 'labels' : tlabels , 'feature_names' : tfeaturenames , 'magcol' : tmagcol , 'objectids' : tobjectids , 'kwargs' : { 'test_fraction' : test_fraction , 'n_crossval_iterations' : n_crossval_iterations , 'n_kfolds' : n_kfolds , 'crossval_scoring_metric' : crossval_scoring_metric , 'nworkers' : nworkers } , 'collect_kwargs' : fdict [ 'kwargs' ] , 'testing_features' : testing_features , 'testing_labels' : testing_labels , 'training_features' : training_features , 'training_labels' : training_labels , 'best_classifier' : bestclf , 'best_score' : bestclf_score , 'best_hyperparams' : bestclf_hyperparams , 'best_recall' : recscore , 'best_precision' : precscore , 'best_f1' : f1score , 'best_confmatrix' : confmatrix } if classifier_to_pickle : with open ( classifier_to_pickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
9566	def pack_into ( fmt , buf , offset , * args , ** kwargs ) : return CompiledFormat ( fmt ) . pack_into ( buf , offset , * args , ** kwargs )
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , ** kwargs ) : if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , ** kwargs ) else : return func ( profile , grid , * args , ** kwargs ) return wrapper
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : continue if "formula" not in sakefile [ target ] : for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , ** data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , ** sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
10269	def node_is_upstream_leaf ( graph : BELGraph , node : BaseEntity ) -> bool : return 0 == len ( graph . predecessors ( node ) ) and 1 == len ( graph . successors ( node ) )
10924	def fit_comp ( new_comp , old_comp , ** kwargs ) : new_cat = new_comp . category new_comp . category = 'ilm' fake_s = states . ImageState ( Image ( old_comp . get ( ) . copy ( ) ) , [ new_comp ] , pad = 0 , mdl = mdl . SmoothFieldModel ( ) ) do_levmarq ( fake_s , new_comp . params , ** kwargs ) new_comp . category = new_cat
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
11894	def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
1400	def extract_logical_plan ( self , topology ) : logicalPlan = { "spouts" : { } , "bolts" : { } , } for spout in topology . spouts ( ) : spoutName = spout . comp . name spoutType = "default" spoutSource = "NA" spoutVersion = "NA" spoutConfigs = spout . comp . config . kvs for kvs in spoutConfigs : if kvs . key == "spout.type" : spoutType = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.source" : spoutSource = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.version" : spoutVersion = javaobj . loads ( kvs . serialized_value ) spoutPlan = { "config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , "type" : spoutType , "source" : spoutSource , "version" : spoutVersion , "outputs" : [ ] } for outputStream in list ( spout . outputs ) : spoutPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan for bolt in topology . bolts ( ) : boltName = bolt . comp . name boltPlan = { "config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , "outputs" : [ ] , "inputs" : [ ] } for outputStream in list ( bolt . outputs ) : boltPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) for inputStream in list ( bolt . inputs ) : boltPlan [ "inputs" ] . append ( { "stream_name" : inputStream . stream . id , "component_name" : inputStream . stream . component_name , "grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) } ) logicalPlan [ "bolts" ] [ boltName ] = boltPlan return logicalPlan
2352	def root ( self ) : if self . _root is None and self . _root_locator is not None : return self . page . find_element ( * self . _root_locator ) return self . _root
4468	def deserialize ( encoded , ** kwargs ) : params = jsonpickle . decode ( encoded , ** kwargs ) return __reconstruct ( params )
11487	def _search_folder_for_item_or_folder ( name , folder_id ) : session . token = verify_credentials ( ) children = session . communicator . folder_children ( session . token , folder_id ) for folder in children [ 'folders' ] : if folder [ 'name' ] == name : return False , folder [ 'folder_id' ] for item in children [ 'items' ] : if item [ 'name' ] == name : return True , item [ 'item_id' ] return False , - 1
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
2080	def callback ( self , pk = None , host_config_key = '' , extra_vars = None ) : url = self . endpoint + '%s/callback/' % pk if not host_config_key : host_config_key = client . get ( url ) . json ( ) [ 'host_config_key' ] post_data = { 'host_config_key' : host_config_key } if extra_vars : post_data [ 'extra_vars' ] = parser . process_extra_vars ( list ( extra_vars ) , force_json = True ) r = client . post ( url , data = post_data , auth = None ) if r . status_code == 201 : return { 'changed' : True }
10776	def finalize ( self , result = None ) : if not self . settings_path : return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
6398	def dist_monge_elkan ( src , tar , sim_func = sim_levenshtein , symmetric = False ) : return MongeElkan ( ) . dist ( src , tar , sim_func , symmetric )
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
9292	def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
9852	def _export_dx ( self , filename , type = None , typequote = '"' , ** kwargs ) : root , ext = os . path . splitext ( filename ) filename = root + '.dx' comments = [ 'OpenDX density file written by gridDataFormats.Grid.export()' , 'File format: http://opendx.sdsc.edu/docs/html/pages/usrgu068.htm#HDREDF' , 'Data are embedded in the header and tied to the grid positions.' , 'Data is written in C array order: In grid[x,y,z] the axis z is fastest' , 'varying, then y, then finally x, i.e. z is the innermost loop.' ] if self . metadata : comments . append ( 'Meta data stored with the python Grid object:' ) for k in self . metadata : comments . append ( ' ' + str ( k ) + ' = ' + str ( self . metadata [ k ] ) ) comments . append ( '(Note: the VMD dx-reader chokes on comments below this line)' ) components = dict ( positions = OpenDX . gridpositions ( 1 , self . grid . shape , self . origin , self . delta ) , connections = OpenDX . gridconnections ( 2 , self . grid . shape ) , data = OpenDX . array ( 3 , self . grid , type = type , typequote = typequote ) , ) dx = OpenDX . field ( 'density' , components = components , comments = comments ) dx . write ( filename )
1409	def to_table ( components , topo_info ) : inputs , outputs = defaultdict ( list ) , defaultdict ( list ) for ctype , component in components . items ( ) : if ctype == 'bolts' : for component_name , component_info in component . items ( ) : for input_stream in component_info [ 'inputs' ] : input_name = input_stream [ 'component_name' ] inputs [ component_name ] . append ( input_name ) outputs [ input_name ] . append ( component_name ) info = [ ] spouts_instance = topo_info [ 'physical_plan' ] [ 'spouts' ] bolts_instance = topo_info [ 'physical_plan' ] [ 'bolts' ] for ctype , component in components . items ( ) : if ctype == "stages" : continue for component_name , component_info in component . items ( ) : row = [ ctype [ : - 1 ] , component_name ] if ctype == 'spouts' : row . append ( len ( spouts_instance [ component_name ] ) ) else : row . append ( len ( bolts_instance [ component_name ] ) ) row . append ( ',' . join ( inputs . get ( component_name , [ '-' ] ) ) ) row . append ( ',' . join ( outputs . get ( component_name , [ '-' ] ) ) ) info . append ( row ) header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] return info , header
7178	def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driver ( grammar , pytree . convert ) if src_txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src_txt [ : 1024 ] else '\n' src_txt += nl try : result = drv . parse_string ( src_txt , True ) except ParseError as pe : lineno , column = pe . context [ 1 ] lines = src_txt . splitlines ( ) try : faulty_line = lines [ lineno - 1 ] except IndexError : faulty_line = "<line number missing in source>" raise ValueError ( f"Cannot parse: {lineno}:{column}: {faulty_line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file_input , [ result ] ) return result
13638	def settings ( path = None , with_path = None ) : if path : Settings . bind ( path , with_path = with_path ) return Settings . _wrapped
6415	def median ( nums ) : nums = sorted ( nums ) mag = len ( nums ) if mag % 2 : mag = int ( ( mag - 1 ) / 2 ) return nums [ mag ] mag = int ( mag / 2 ) med = ( nums [ mag - 1 ] + nums [ mag ] ) / 2 return med if not med . is_integer ( ) else int ( med )
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
1546	def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
8759	def get_subnets ( context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker = None , filters = None , fields = None ) : LOG . info ( "get_subnets for tenant %s with filters %s fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } subnets = db_api . subnet_find ( context , limit = limit , page_reverse = page_reverse , sorts = sorts , marker_obj = marker , join_dns = True , join_routes = True , join_pool = True , ** filters ) for subnet in subnets : cache = subnet . get ( "_allocation_pool_cache" ) if not cache : db_api . subnet_update_set_alloc_pool_cache ( context , subnet , subnet . allocation_pools ) return v . _make_subnets_list ( subnets , fields = fields )
4957	def parse_csv ( file_stream , expected_columns = None ) : reader = unicodecsv . DictReader ( file_stream , encoding = "utf-8" ) if expected_columns and set ( expected_columns ) - set ( reader . fieldnames ) : raise ValidationError ( ValidationMessages . MISSING_EXPECTED_COLUMNS . format ( expected_columns = ", " . join ( expected_columns ) , actual_columns = ", " . join ( reader . fieldnames ) ) ) for row in reader : yield row
2848	def _check ( self , command , * args ) : ret = command ( self . _ctx , * args ) logger . debug ( 'Called ftdi_{0} and got response {1}.' . format ( command . __name__ , ret ) ) if ret != 0 : raise RuntimeError ( 'ftdi_{0} failed with error {1}: {2}' . format ( command . __name__ , ret , ftdi . get_error_string ( self . _ctx ) ) )
2003	def function_call ( type_spec , * args ) : m = re . match ( r"(?P<name>[a-zA-Z_][a-zA-Z_0-9]*)(?P<type>\(.*\))" , type_spec ) if not m : raise EthereumError ( "Function signature expected" ) ABI . _check_and_warn_num_args ( type_spec , * args ) result = ABI . function_selector ( type_spec ) result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , ** kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
859	def isTemporal ( inferenceElement ) : if InferenceElement . __temporalInferenceElements is None : InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) return inferenceElement in InferenceElement . __temporalInferenceElements
1021	def buildSequencePool ( numSequences = 10 , seqLen = [ 2 , 3 , 4 ] , numPatterns = 5 , numOnBitsPerPattern = 3 , patternOverlap = 0 , ** kwargs ) : patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] for _ in xrange ( numSequences ) : sequence = [ ] length = random . choice ( seqLen ) for _ in xrange ( length ) : patIdx = random . choice ( xrange ( numPatterns ) ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print "\nTraining sequences" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )
3345	def parse_if_header_dict ( environ ) : if "wsgidav.conditions.if" in environ : return if "HTTP_IF" not in environ : environ [ "wsgidav.conditions.if" ] = None environ [ "wsgidav.ifLockTokenList" ] = [ ] return iftext = environ [ "HTTP_IF" ] . strip ( ) if not iftext . startswith ( "<" ) : iftext = "<*>" + iftext ifDict = dict ( [ ] ) ifLockList = [ ] resource1 = "*" for ( tmpURLVar , URLVar , _tmpContentVar , contentVar ) in reIfSeparator . findall ( iftext ) : if tmpURLVar != "" : resource1 = URLVar else : listTagContents = [ ] testflag = True for listitem in reIfTagListContents . findall ( contentVar ) : if listitem . upper ( ) != "NOT" : if listitem . startswith ( "[" ) : listTagContents . append ( ( testflag , "entity" , listitem . strip ( '"[]' ) ) ) else : listTagContents . append ( ( testflag , "locktoken" , listitem . strip ( "<>" ) ) ) ifLockList . append ( listitem . strip ( "<>" ) ) testflag = listitem . upper ( ) != "NOT" if resource1 in ifDict : listTag = ifDict [ resource1 ] else : listTag = [ ] ifDict [ resource1 ] = listTag listTag . append ( listTagContents ) environ [ "wsgidav.conditions.if" ] = ifDict environ [ "wsgidav.ifLockTokenList" ] = ifLockList _logger . debug ( "parse_if_header_dict\n{}" . format ( pformat ( ifDict ) ) ) return
9933	def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
2910	def _find_ancestor ( self , task_spec ) : if self . parent is None : return self if self . parent . task_spec == task_spec : return self . parent return self . parent . _find_ancestor ( task_spec )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( ** filter_kwargs ) : yield integrated_channel
2341	def forward ( self , x ) : self . noise . normal_ ( ) return self . layers ( th . cat ( [ x , self . noise ] , 1 ) )
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
786	def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
6165	def sqrt_rc_imp ( Ns , alpha , M = 6 ) : n = np . arange ( - M * Ns , M * Ns + 1 ) b = np . zeros ( len ( n ) ) Ns *= 1.0 a = alpha for i in range ( len ( n ) ) : if abs ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) <= np . finfo ( np . float ) . eps / 2 : b [ i ] = 1 / 2. * ( ( 1 + a ) * np . sin ( ( 1 + a ) * np . pi / ( 4. * a ) ) - ( 1 - a ) * np . cos ( ( 1 - a ) * np . pi / ( 4. * a ) ) + ( 4 * a ) / np . pi * np . sin ( ( 1 - a ) * np . pi / ( 4. * a ) ) ) else : b [ i ] = 4 * a / ( np . pi * ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) ) b [ i ] = b [ i ] * ( np . cos ( ( 1 + a ) * np . pi * n [ i ] / Ns ) + np . sinc ( ( 1 - a ) * n [ i ] / Ns ) * ( 1 - a ) * np . pi / ( 4. * a ) ) return b
7992	def _send_stream_error ( self , condition ) : if self . _output_state is "closed" : return if self . _output_state in ( None , "restart" ) : self . _send_stream_start ( ) element = StreamErrorElement ( condition ) . as_xml ( ) self . transport . send_element ( element ) self . transport . disconnect ( ) self . _output_state = "closed"
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
13701	def _before ( self ) : if request . path in self . excluded_routes : request . _tracy_exclude = True return request . _tracy_start_time = monotonic ( ) client = request . headers . get ( trace_header_client , None ) require_client = current_app . config . get ( "TRACY_REQUIRE_CLIENT" , False ) if client is None and require_client : abort ( 400 , "Missing %s header" % trace_header_client ) request . _tracy_client = client request . _tracy_id = request . headers . get ( trace_header_id , new_id ( ) )
70	def remove_out_of_image ( self , fully = True , partly = False ) : bbs_clean = [ bb for bb in self . bounding_boxes if not bb . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return BoundingBoxesOnImage ( bbs_clean , shape = self . shape )
612	def _getExperimentDescriptionSchema ( ) : installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) schemaFilePath = os . path . join ( installPath , "experimentDescriptionSchema.json" ) return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) )
413	def find_top_model ( self , sess , sort = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) s = time . time ( ) d = self . db . Model . find_one ( filter = kwargs , sort = sort ) _temp_file_name = '_find_one_model_ztemp_file' if d is not None : params_id = d [ 'params_id' ] graphs = d [ 'architecture' ] _datetime = d [ 'time' ] exists_or_mkdir ( _temp_file_name , False ) with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) else : print ( "[Database] FAIL! Cannot find model: {}" . format ( kwargs ) ) return False try : params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) network = load_graph_and_params ( name = _temp_file_name , sess = sess ) del_folder ( _temp_file_name ) pc = self . db . Model . find ( kwargs ) print ( "[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s" . format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) ) for key in d : network . __dict__ . update ( { "_%s" % key : d [ key ] } ) params_id_list = pc . distinct ( 'params_id' ) n_params = len ( params_id_list ) if n_params != 1 : print ( " Note that there are {} models match the kwargs" . format ( n_params ) ) return network except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False
13439	def lock_file ( filename ) : lockfile = "%s.lock" % filename if isfile ( lockfile ) : return False else : with open ( lockfile , "w" ) : pass return True
11776	def WeightedMajority ( predictors , weights ) : "Return a predictor that takes a weighted vote." def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
7677	def hierarchy ( annotation , ** kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , ** kwargs )
13627	def Timestamp ( value , _divisor = 1. , tz = UTC , encoding = None ) : value = Float ( value , encoding ) if value is not None : value = value / _divisor return datetime . fromtimestamp ( value , tz ) return None
13066	def make_members ( self , collection , lang = None ) : objects = sorted ( [ self . expose_ancestors_or_children ( member , collection , lang = lang ) for member in collection . members if member . get_label ( ) ] , key = itemgetter ( "label" ) ) return objects
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
10114	def dump_grid ( grid ) : header = 'ver:%s' % dump_str ( str ( grid . _version ) , version = grid . _version ) if bool ( grid . metadata ) : header += ' ' + dump_meta ( grid . metadata , version = grid . _version ) columns = dump_columns ( grid . column , version = grid . _version ) rows = dump_rows ( grid ) return '\n' . join ( [ header , columns ] + rows + [ '' ] )
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : return None assert tx . depth == 0 return tx except IndexError : return None
7177	def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as src_buffer : src_encoding = src_buffer . encoding src_node = lib2to3_parse ( src_buffer . read ( ) ) try : with open ( ( pyi_dir / src . name ) . with_suffix ( '.pyi' ) ) as pyi_file : pyi_txt = pyi_file . read ( ) except FileNotFoundError : if not quiet : print ( f'warning: .pyi file for source {src} not found in {pyi_dir}' , file = sys . stderr , ) else : pyi_ast = ast3 . parse ( pyi_txt ) assert isinstance ( pyi_ast , ast3 . Module ) reapply_all ( pyi_ast . body , src_node ) fix_remaining_type_comments ( src_node ) targets . mkdir ( parents = True , exist_ok = True ) with open ( targets / src . name , 'w' , encoding = src_encoding ) as target_file : target_file . write ( lib2to3_unparse ( src_node , hg = hg ) ) return targets / src . name
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
9862	async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
2115	def status ( self , pk = None , detail = False , ** kwargs ) : job = self . last_job_data ( pk , ** kwargs ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
9467	def conference_play ( self , call_params ) : path = '/' + self . api_version + '/ConferencePlay/' method = 'POST' return self . request ( path , method , call_params )
7683	def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , ** fig_kw ) if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , ** kwargs ) return fig , axs
503	def _labelListToCategoryNumber ( self , labelList ) : categoryNumber = 0 for label in labelList : categoryNumber += self . _labelToCategoryNumber ( label ) return categoryNumber
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
6343	def raw ( self ) : r doc_list = [ ] for doc in self . corpus : sent_list = [ ] for sent in doc : sent_list . append ( ' ' . join ( sent ) ) doc_list . append ( self . sent_split . join ( sent_list ) ) del sent_list return self . doc_split . join ( doc_list )
9164	def includeme ( config ) : api_key_authn_policy = APIKeyAuthenticationPolicy ( ) config . include ( 'openstax_accounts' ) openstax_authn_policy = config . registry . getUtility ( IOpenstaxAccountsAuthenticationPolicy ) policies = [ api_key_authn_policy , openstax_authn_policy ] authn_policy = MultiAuthenticationPolicy ( policies ) config . set_authentication_policy ( authn_policy ) authz_policy = ACLAuthorizationPolicy ( ) config . set_authorization_policy ( authz_policy )
13295	def convert_lsstdoc_tex ( content , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : augmented_content = '\n' . join ( ( LSSTDOC_MACROS , content ) ) return convert_text ( augmented_content , 'latex' , to_fmt , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args )
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
357	def load_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , is_latest = True , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] if is_latest : ckpt_file = tf . train . latest_checkpoint ( save_dir ) else : ckpt_file = os . path . join ( save_dir , mode_name ) if not var_list : var_list = tf . global_variables ( ) logging . info ( "[*] load %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) try : saver = tf . train . Saver ( var_list ) saver . restore ( sess , ckpt_file ) except Exception as e : logging . info ( e ) logging . info ( "[*] load ckpt fail ..." )
2992	def mutualFundSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( mutualFundSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
9710	def heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem
2005	def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
10505	def stopEventLoop ( ) : stopper = PyObjCAppHelperRunLoopStopper_wrap . currentRunLoopStopper ( ) if stopper is None : if NSApp ( ) is not None : NSApp ( ) . terminate_ ( None ) return True return False NSTimer . scheduledTimerWithTimeInterval_target_selector_userInfo_repeats_ ( 0.0 , stopper , 'performStop:' , None , False ) return True
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : shutil . copy2 ( src , dst )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
4899	def parse_arguments ( * args , ** options ) : days = options . get ( 'days' , 1 ) enterprise_customer_uuid = options . get ( 'enterprise_customer_uuid' ) enterprise_customer = None if enterprise_customer_uuid : try : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( 'Enterprise customer with uuid "{enterprise_customer_uuid}" does not exist.' . format ( enterprise_customer_uuid = enterprise_customer_uuid ) ) return days , enterprise_customer
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
11728	def _assert_contains ( haystack , needle , invert , escape = False ) : myneedle = re . escape ( needle ) if escape else needle matched = re . search ( myneedle , haystack , re . M ) if ( invert and matched ) or ( not invert and not matched ) : raise AssertionError ( "'%s' %sfound in '%s'" % ( needle , "" if invert else "not " , haystack ) )
9675	def _calculate_float ( self , byte_array ) : if len ( byte_array ) != 4 : return None return struct . unpack ( 'f' , struct . pack ( '4B' , * byte_array ) ) [ 0 ]
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
2896	def is_completed ( self ) : mask = Task . NOT_FINISHED_MASK iter = Task . Iterator ( self . task_tree , mask ) try : next ( iter ) except StopIteration : return True return False
3520	def snapengage ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return SnapEngageNode ( )
11784	def check_example ( self , example ) : "Raise ValueError if example has any invalid values." if self . values : for a in self . attrs : if example [ a ] not in self . values [ a ] : raise ValueError ( 'Bad value %s for attribute %s in %s' % ( example [ a ] , self . attrnames [ a ] , example ) )
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
10893	def pad ( self , pad ) : tile = self . copy ( ) tile . l -= pad tile . r += pad return tile
1070	def getaddrlist ( self ) : result = [ ] ad = self . getaddress ( ) while ad : result += ad ad = self . getaddress ( ) return result
12275	def iso_reference_isvalid ( ref ) : ref = str ( ref ) cs_source = ref [ 4 : ] + ref [ : 4 ] return ( iso_reference_str2int ( cs_source ) % 97 ) == 1
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
2199	def platform_config_dir ( ) : if LINUX : dpath_ = os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) elif DARWIN : dpath_ = '~/Library/Application Support' elif WIN32 : dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : raise NotImplementedError ( 'Unknown Platform %r' % ( sys . platform , ) ) dpath = normpath ( expanduser ( dpath_ ) ) return dpath
12659	def import_pyfile ( filepath , mod_name = None ) : import sys if sys . version_info . major == 3 : import importlib . machinery loader = importlib . machinery . SourceFileLoader ( '' , filepath ) mod = loader . load_module ( mod_name ) else : import imp mod = imp . load_source ( mod_name , filepath ) return mod
3903	def _show_menu ( self ) : current_widget = self . _tabbed_window . get_current_widget ( ) if hasattr ( current_widget , 'get_menu_widget' ) : menu_widget = current_widget . get_menu_widget ( self . _hide_menu ) overlay = urwid . Overlay ( menu_widget , self . _tabbed_window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . _urwid_loop . widget = overlay
1186	def check_charset ( self , ctx , char ) : self . set_dispatcher . reset ( char ) save_position = ctx . code_position result = None while result is None : result = self . set_dispatcher . dispatch ( ctx . peek_code ( ) , ctx ) ctx . code_position = save_position return result
7321	def convert_markdown ( message ) : assert message [ 'Content-Type' ] . startswith ( "text/markdown" ) del message [ 'Content-Type' ] message = make_message_multipart ( message ) for payload_item in set ( message . get_payload ( ) ) : if payload_item [ 'Content-Type' ] . startswith ( 'text/plain' ) : original_text = payload_item . get_payload ( ) html_text = markdown . markdown ( original_text ) html_payload = future . backports . email . mime . text . MIMEText ( "<html><body>{}</body></html>" . format ( html_text ) , "html" , ) message . attach ( html_payload ) return message
7396	def get_publication_list ( context , list , template = 'publications/publications.html' ) : list = List . objects . filter ( list__iexact = list ) if not list : return '' list = list [ 0 ] publications = list . publication_set . all ( ) publications = publications . order_by ( '-year' , '-month' , '-id' ) if not publications : return '' populate ( publications ) return render_template ( template , context [ 'request' ] , { 'list' : list , 'publications' : publications } )
8067	def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
8648	def reject_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'reject' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotRejectedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2274	def _win32_rmtree ( path , verbose = 0 ) : def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) _rmjunctions ( path ) import shutil shutil . rmtree ( path )
3231	def get_cache_access_details ( key = None ) : from cloudaux . gcp . decorators import _GCP_CACHE return _GCP_CACHE . get_access_details ( key = key )
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
320	def get_max_drawdown_underwater ( underwater ) : valley = np . argmin ( underwater ) peak = underwater [ : valley ] [ underwater [ : valley ] == 0 ] . index [ - 1 ] try : recovery = underwater [ valley : ] [ underwater [ valley : ] == 0 ] . index [ 0 ] except IndexError : recovery = np . nan return peak , valley , recovery
78	def AdditivePoissonNoise ( lam = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : lam2 = iap . handle_continuous_param ( lam , "lam" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AddElementwise ( iap . RandomSign ( iap . Poisson ( lam = lam2 ) ) , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
3491	def _sbase_annotations ( sbase , annotation ) : if not annotation or len ( annotation ) == 0 : return annotation_data = deepcopy ( annotation ) for key , value in annotation_data . items ( ) : if isinstance ( value , ( float , int ) ) : value = str ( value ) if isinstance ( value , string_types ) : annotation_data [ key ] = [ ( "is" , value ) ] for key , value in annotation_data . items ( ) : for idx , item in enumerate ( value ) : if isinstance ( item , string_types ) : value [ idx ] = ( "is" , item ) meta_id = "meta_{}" . format ( sbase . getId ( ) ) sbase . setMetaId ( meta_id ) for provider , data in iteritems ( annotation_data ) : if provider in [ "SBO" , "sbo" ] : if provider == "SBO" : LOGGER . warning ( "'SBO' provider is deprecated, " "use 'sbo' provider instead" ) sbo_term = data [ 0 ] [ 1 ] _check ( sbase . setSBOTerm ( sbo_term ) , "Setting SBOTerm: {}" . format ( sbo_term ) ) continue for item in data : qualifier_str , entity = item [ 0 ] , item [ 1 ] qualifier = QUALIFIER_TYPES . get ( qualifier_str , None ) if qualifier is None : qualifier = libsbml . BQB_IS LOGGER . error ( "Qualifier type is not supported on " "annotation: '{}'" . format ( qualifier_str ) ) qualifier_type = libsbml . BIOLOGICAL_QUALIFIER if qualifier_str . startswith ( "bqm_" ) : qualifier_type = libsbml . MODEL_QUALIFIER cv = libsbml . CVTerm ( ) cv . setQualifierType ( qualifier_type ) if qualifier_type == libsbml . BIOLOGICAL_QUALIFIER : cv . setBiologicalQualifierType ( qualifier ) elif qualifier_type == libsbml . MODEL_QUALIFIER : cv . setModelQualifierType ( qualifier ) else : raise CobraSBMLError ( 'Unsupported qualifier: ' '%s' % qualifier ) resource = "%s/%s/%s" % ( URL_IDENTIFIERS_PREFIX , provider , entity ) cv . addResource ( resource ) _check ( sbase . addCVTerm ( cv ) , "Setting cvterm: {}, resource: {}" . format ( cv , resource ) )
4804	def when_called_with ( self , * some_args , ** some_kwargs ) : if not self . expected : raise TypeError ( 'expected exception not set, raises() must be called first' ) try : self . val ( * some_args , ** some_kwargs ) except BaseException as e : if issubclass ( type ( e ) , self . expected ) : return AssertionBuilder ( str ( e ) , self . description , self . kind ) else : self . _err ( 'Expected <%s> to raise <%s> when called with (%s), but raised <%s>.' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) , type ( e ) . __name__ ) ) self . _err ( 'Expected <%s> to raise <%s> when called with (%s).' % ( self . val . __name__ , self . expected . __name__ , self . _fmt_args_kwargs ( * some_args , ** some_kwargs ) ) )
7966	def end ( self , tag ) : self . _level -= 1 if self . _level < 0 : self . _handler . stream_parse_error ( u"Unexpected end tag for: {0!r}" . format ( tag ) ) return if self . _level == 0 : if tag != self . _root . tag : self . _handler . stream_parse_error ( u"Unexpected end tag for:" " {0!r} (stream end tag expected)" . format ( tag ) ) return self . _handler . stream_end ( ) return element = self . _builder . end ( tag ) if self . _level == 1 : self . _handler . stream_element ( element )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
13114	def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
10876	def calculate_linescan_ilm_psf ( y , z , polar_angle = 0. , nlpts = 1 , pinhole_width = 1 , use_laggauss = False , ** kwargs ) : if use_laggauss : x_vals , wts = calc_pts_lag ( ) else : x_vals , wts = calc_pts_hg ( ) xg , yg , zg = [ np . zeros ( list ( y . shape ) + [ x_vals . size ] ) for a in range ( 3 ) ] hilm = np . zeros ( xg . shape ) for a in range ( x_vals . size ) : xg [ ... , a ] = x_vals [ a ] yg [ ... , a ] = y . copy ( ) zg [ ... , a ] = z . copy ( ) y_pinhole , wts_pinhole = np . polynomial . hermite . hermgauss ( nlpts ) y_pinhole *= np . sqrt ( 2 ) * pinhole_width wts_pinhole /= np . sqrt ( np . pi ) for yp , wp in zip ( y_pinhole , wts_pinhole ) : rho = np . sqrt ( xg * xg + ( yg - yp ) * ( yg - yp ) ) phi = np . arctan2 ( yg , xg ) hsym , hasym = get_hsym_asym ( rho , zg , get_hdet = False , ** kwargs ) hilm += wp * ( hsym + np . cos ( 2 * ( phi - polar_angle ) ) * hasym ) for a in range ( x_vals . size ) : hilm [ ... , a ] *= wts [ a ] return hilm . sum ( axis = - 1 ) * 2.
10357	def random_by_edges ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 edges = graph . edges ( keys = True ) n = int ( graph . number_of_edges ( ) * percentage ) subedges = random . sample ( edges , n ) rv = graph . fresh_copy ( ) for u , v , k in subedges : safe_add_edge ( rv , u , v , k , graph [ u ] [ v ] [ k ] ) update_node_helper ( graph , rv ) return rv
2111	def empty ( organization = None , user = None , team = None , credential_type = None , credential = None , notification_template = None , inventory_script = None , inventory = None , project = None , job_template = None , workflow = None , all = None , no_color = False ) : from tower_cli . cli . transfer . cleaner import Cleaner destroyer = Cleaner ( no_color ) assets_to_export = { } for asset_type in SEND_ORDER : assets_to_export [ asset_type ] = locals ( ) [ asset_type ] destroyer . go_ham ( all = all , asset_input = assets_to_export )
1265	def sanity_check_actions ( actions_spec ) : actions = copy . deepcopy ( actions_spec ) is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) for name , action in actions . items ( ) : if 'type' not in action : action [ 'type' ] = 'int' if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( "Action requires value 'num_actions' set!" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( "Action requires both values 'min_value' and 'max_value' set!" ) if 'shape' not in action : action [ 'shape' ] = ( ) if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique
11248	def median ( data ) : ordered = sorted ( data ) length = len ( ordered ) if length % 2 == 0 : return ( ordered [ math . floor ( length / 2 ) - 1 ] + ordered [ math . floor ( length / 2 ) ] ) / 2.0 elif length % 2 != 0 : return ordered [ math . floor ( length / 2 ) ]
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , ** kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , ** kwargs )
9921	def validate_key ( self , key ) : if not models . PasswordResetToken . valid_tokens . filter ( key = key ) . exists ( ) : raise serializers . ValidationError ( _ ( "The provided reset token does not exist, or is expired." ) ) return key
5725	def write ( self , mi_cmd_to_write , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True , read_response = True , ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if type ( mi_cmd_to_write ) in [ str , unicode ] : pass elif type ( mi_cmd_to_write ) == list : mi_cmd_to_write = "\n" . join ( mi_cmd_to_write ) else : raise TypeError ( "The gdb mi command must a be str or list. Got " + str ( type ( mi_cmd_to_write ) ) ) self . logger . debug ( "writing: %s" , mi_cmd_to_write ) if not mi_cmd_to_write . endswith ( "\n" ) : mi_cmd_to_write_nl = mi_cmd_to_write + "\n" else : mi_cmd_to_write_nl = mi_cmd_to_write if USING_WINDOWS : outputready = [ self . stdin_fileno ] else : _ , outputready , _ = select . select ( [ ] , self . write_list , [ ] , timeout_sec ) for fileno in outputready : if fileno == self . stdin_fileno : self . gdb_process . stdin . write ( mi_cmd_to_write_nl . encode ( ) ) self . gdb_process . stdin . flush ( ) else : self . logger . error ( "got unexpected fileno %d" % fileno ) if read_response is True : return self . get_gdb_response ( timeout_sec = timeout_sec , raise_error_on_timeout = raise_error_on_timeout ) else : return [ ]
9851	def _export_python ( self , filename , ** kwargs ) : data = dict ( grid = self . grid , edges = self . edges , metadata = self . metadata ) with open ( filename , 'wb' ) as f : cPickle . dump ( data , f , cPickle . HIGHEST_PROTOCOL )
3559	def find_service ( self , uuid ) : for service in self . list_services ( ) : if service . uuid == uuid : return service return None
9591	def set_window_size ( self , width , height , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_SIZE , { 'width' : int ( width ) , 'height' : int ( height ) , 'window_handle' : window_handle } )
3004	def _get_oauth2_client_id_and_secret ( settings_instance ) : secret_json = getattr ( settings_instance , 'GOOGLE_OAUTH2_CLIENT_SECRETS_JSON' , None ) if secret_json is not None : return _load_client_secrets ( secret_json ) else : client_id = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_ID" , None ) client_secret = getattr ( settings_instance , "GOOGLE_OAUTH2_CLIENT_SECRET" , None ) if client_id is not None and client_secret is not None : return client_id , client_secret else : raise exceptions . ImproperlyConfigured ( "Must specify either GOOGLE_OAUTH2_CLIENT_SECRETS_JSON, or " "both GOOGLE_OAUTH2_CLIENT_ID and " "GOOGLE_OAUTH2_CLIENT_SECRET in settings.py" )
10902	def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) axis . add_artist ( at )
7735	def set_stringprep_cache_size ( size ) : global _stringprep_cache_size _stringprep_cache_size = size if len ( Profile . cache_items ) > size : remove = Profile . cache_items [ : - size ] for profile , key in remove : try : del profile . cache [ key ] except KeyError : pass Profile . cache_items = Profile . cache_items [ - size : ]
3587	def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
1979	def wait ( self , readfds , writefds , timeout ) : logger . info ( "WAIT:" ) logger . info ( "\tProcess %d is going to wait for [ %r %r %r ]" , self . _current , readfds , writefds , timeout ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout else : self . timers [ self . _current ] = None procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . info ( "\tTransfer control from process %d to %d" , procid , self . _current ) logger . info ( "\tREMOVING %r from %r. Current: %r" , procid , self . running , self . _current ) self . running . remove ( procid ) if self . _current not in self . running : logger . info ( "\tCurrent not running. Checking for timers..." ) self . _current = None if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . check_timers ( )
13274	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : if obj . flags [ 'C_CONTIGUOUS' ] : obj_data = obj . data else : cont_obj = np . ascontiguousarray ( obj ) assert ( cont_obj . flags [ 'C_CONTIGUOUS' ] ) obj_data = cont_obj . data data_b64 = base64 . b64encode ( obj_data ) return dict ( __ndarray__ = data_b64 , dtype = str ( obj . dtype ) , shape = obj . shape ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSONEncoder ( self , obj )
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
7805	def verify_jid_against_common_name ( self , jid ) : if not self . common_names : return False for name in self . common_names : try : cn_jid = JID ( name ) except ValueError : continue if jid == cn_jid : return True return False
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
5401	def _get_logging_env ( self , logging_uri , user_project ) : if not logging_uri . endswith ( '.log' ) : raise ValueError ( 'Logging URI must end in ".log": {}' . format ( logging_uri ) ) logging_prefix = logging_uri [ : - len ( '.log' ) ] return { 'LOGGING_PATH' : '{}.log' . format ( logging_prefix ) , 'STDOUT_PATH' : '{}-stdout.log' . format ( logging_prefix ) , 'STDERR_PATH' : '{}-stderr.log' . format ( logging_prefix ) , 'USER_PROJECT' : user_project , }
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
4593	def receive ( self , msg ) : if self . edit_queue : self . edit_queue . put_edit ( self . _set , msg ) else : self . _set ( msg )
11353	def make_user_agent ( component = None ) : packageinfo = pkg_resources . require ( "harvestingkit" ) [ 0 ] useragent = "{0}/{1}" . format ( packageinfo . project_name , packageinfo . version ) if component is not None : useragent += " {0}" . format ( component ) return useragent
5905	def _mdp_include_string ( dirs ) : include_paths = [ os . path . expanduser ( p ) for p in dirs ] return ' -I' . join ( [ '' ] + include_paths )
9306	def encode_body ( req ) : if isinstance ( req . body , text_type ) : split = req . headers . get ( 'content-type' , 'text/plain' ) . split ( ';' ) if len ( split ) == 2 : ct , cs = split cs = cs . split ( '=' ) [ 1 ] req . body = req . body . encode ( cs ) else : ct = split [ 0 ] if ( ct == 'application/x-www-form-urlencoded' or 'x-amz-' in ct ) : req . body = req . body . encode ( ) else : req . body = req . body . encode ( 'utf-8' ) req . headers [ 'content-type' ] = ct + '; charset=utf-8'
6087	def contribution_maps_1d_from_hyper_images_and_galaxies ( hyper_model_image_1d , hyper_galaxy_images_1d , hyper_galaxies , hyper_minimum_values ) : return list ( map ( lambda hyper_galaxy , hyper_galaxy_image_1d , hyper_minimum_value : hyper_galaxy . contributions_from_model_image_and_galaxy_image ( model_image = hyper_model_image_1d , galaxy_image = hyper_galaxy_image_1d , minimum_value = hyper_minimum_value ) , hyper_galaxies , hyper_galaxy_images_1d , hyper_minimum_values ) )
625	def _neighbors ( coordinate , radius ) : ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) return numpy . array ( list ( itertools . product ( * ranges ) ) )
6339	def lcsseq ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) for i , src_char in enumerate ( src ) : for j , tar_char in enumerate ( tar ) : if src_char == tar_char : lengths [ i + 1 , j + 1 ] = lengths [ i , j ] + 1 else : lengths [ i + 1 , j + 1 ] = max ( lengths [ i + 1 , j ] , lengths [ i , j + 1 ] ) result = '' i , j = len ( src ) , len ( tar ) while i != 0 and j != 0 : if lengths [ i , j ] == lengths [ i - 1 , j ] : i -= 1 elif lengths [ i , j ] == lengths [ i , j - 1 ] : j -= 1 else : result = src [ i - 1 ] + result i -= 1 j -= 1 return result
7344	def get_data ( self , response ) : if self . _response_list : return response elif self . _response_key is None : if hasattr ( response , "items" ) : for key , data in response . items ( ) : if ( hasattr ( data , "__getitem__" ) and not hasattr ( data , "items" ) and len ( data ) > 0 and 'id' in data [ 0 ] ) : self . _response_key = key return data else : self . _response_list = True return response else : return response [ self . _response_key ] raise NoDataFound ( response = response , url = self . request . get_url ( ) )
40	def add ( self , * args , ** kwargs ) : idx = self . _next_idx super ( ) . add ( * args , ** kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
4002	def init_yaml_constructor ( ) : def utf_encoding_string_constructor ( loader , node ) : return loader . construct_scalar ( node ) . encode ( 'utf-8' ) yaml . SafeLoader . add_constructor ( u'tag:yaml.org,2002:str' , utf_encoding_string_constructor )
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
8222	def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
5578	def driver_from_file ( input_file ) : file_ext = os . path . splitext ( input_file ) [ 1 ] . split ( "." ) [ 1 ] if file_ext not in _file_ext_to_driver ( ) : raise MapcheteDriverError ( "no driver could be found for file extension %s" % file_ext ) driver = _file_ext_to_driver ( ) [ file_ext ] if len ( driver ) > 1 : warnings . warn ( DeprecationWarning ( "more than one driver for file found, taking %s" % driver [ 0 ] ) ) return driver [ 0 ]
7019	def merge_hatpi_textlc_apertures ( lclist ) : lcaps = { } framekeys = [ ] for lc in lclist : lcd = read_hatpi_textlc ( lc ) for col in lcd [ 'columns' ] : if col . startswith ( 'itf' ) : lcaps [ col ] = lcd thisframekeys = lcd [ 'frk' ] . tolist ( ) framekeys . extend ( thisframekeys ) framekeys = sorted ( list ( set ( framekeys ) ) )
6226	def rot_state ( self , x , y ) : if self . last_x is None : self . last_x = x if self . last_y is None : self . last_y = y x_offset = self . last_x - x y_offset = self . last_y - y self . last_x = x self . last_y = y x_offset *= self . mouse_sensitivity y_offset *= self . mouse_sensitivity self . yaw -= x_offset self . pitch += y_offset if self . pitch > 85.0 : self . pitch = 85.0 if self . pitch < - 85.0 : self . pitch = - 85.0 self . _update_yaw_and_pitch ( )
12677	def escape ( to_escape , safe = SAFE , escape_char = ESCAPE_CHAR , allow_collisions = False ) : if isinstance ( to_escape , bytes ) : to_escape = to_escape . decode ( 'utf8' ) if not isinstance ( safe , set ) : safe = set ( safe ) if allow_collisions : safe . add ( escape_char ) elif escape_char in safe : safe . remove ( escape_char ) chars = [ ] for c in to_escape : if c in safe : chars . append ( c ) else : chars . append ( _escape_char ( c , escape_char ) ) return u'' . join ( chars )
13253	async def download_metadata_yaml ( session , github_url ) : metadata_yaml_url = _build_metadata_yaml_url ( github_url ) async with session . get ( metadata_yaml_url ) as response : response . raise_for_status ( ) yaml_data = await response . text ( ) return yaml . safe_load ( yaml_data )
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
1083	def combine ( cls , date , time ) : "Construct a datetime from a given date and a given time." if not isinstance ( date , _date_class ) : raise TypeError ( "date argument must be a date instance" ) if not isinstance ( time , _time_class ) : raise TypeError ( "time argument must be a time instance" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )
5421	def _get_job_metadata ( provider , user_id , job_name , script , task_ids , user_project , unique_job_id ) : create_time = dsub_util . replace_timezone ( datetime . datetime . now ( ) , tzlocal ( ) ) user_id = user_id or dsub_util . get_os_user ( ) job_metadata = provider . prepare_job_metadata ( script . name , job_name , user_id , create_time ) if unique_job_id : job_metadata [ 'job-id' ] = uuid . uuid4 ( ) . hex job_metadata [ 'create-time' ] = create_time job_metadata [ 'script' ] = script job_metadata [ 'user-project' ] = user_project if task_ids : job_metadata [ 'task-ids' ] = dsub_util . compact_interval_string ( list ( task_ids ) ) return job_metadata
12944	def save ( self , cascadeSave = True ) : saver = IndexedRedisSave ( self . __class__ ) return saver . save ( self , cascadeSave = cascadeSave )
2783	def save ( self ) : data = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = PUT , params = data )
10205	def extract_date ( self , date ) : if isinstance ( date , six . string_types ) : try : date = dateutil . parser . parse ( date ) except ValueError : raise ValueError ( 'Invalid date format for statistic {}.' ) . format ( self . query_name ) if not isinstance ( date , datetime ) : raise TypeError ( 'Invalid date type for statistic {}.' ) . format ( self . query_name ) return date
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
6150	def fir_remez_lpf ( f_pass , f_stop , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = lowpass_order ( f_pass , f_stop , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
7143	def new_address ( self , label = None ) : return self . _backend . new_address ( account = self . index , label = label )
11699	def serve ( self , sock , request_handler , error_handler , debug = False , request_timeout = 60 , ssl = None , request_max_size = None , reuse_port = False , loop = None , protocol = HttpProtocol , backlog = 100 , ** kwargs ) : if debug : loop . set_debug ( debug ) server = partial ( protocol , loop = loop , connections = self . connections , signal = self . signal , request_handler = request_handler , error_handler = error_handler , request_timeout = request_timeout , request_max_size = request_max_size , ) server_coroutine = loop . create_server ( server , host = None , port = None , ssl = ssl , reuse_port = reuse_port , sock = sock , backlog = backlog ) loop . call_soon ( partial ( update_current_time , loop ) ) return server_coroutine
11246	def future_value ( present_value , annual_rate , periods_per_year , years ) : rate_per_period = annual_rate / float ( periods_per_year ) periods = periods_per_year * years return present_value * ( 1 + rate_per_period ) ** periods
6956	def _transit_model ( times , t0 , per , rp , a , inc , ecc , w , u , limb_dark , exp_time_minutes = 2 , supersample_factor = 7 ) : params = batman . TransitParams ( ) params . t0 = t0 params . per = per params . rp = rp params . a = a params . inc = inc params . ecc = ecc params . w = w params . u = u params . limb_dark = limb_dark t = times m = batman . TransitModel ( params , t , exp_time = exp_time_minutes / 60. / 24. , supersample_factor = supersample_factor ) return params , m
10995	def _barnes ( self , pos ) : b_in = self . b_in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) sz = self . npts [ 1 ] coeffs = self . get_values ( self . barnes_params ) b = BarnesInterpolationND ( b_in , coeffs , filter_size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local_updates , clipsize = self . barnes_clip_size , blocksize = 100 ) return b ( pos )
1606	def run_containers ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] container_id = cl_args [ 'id' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False containers = result [ 'physical_plan' ] [ 'stmgrs' ] all_bolts , all_spouts = set ( ) , set ( ) for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : all_bolts = all_bolts | set ( bolts ) for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : all_spouts = all_spouts | set ( spouts ) stmgrs = containers . keys ( ) stmgrs . sort ( ) if container_id is not None : try : normalized_cid = container_id - 1 if normalized_cid < 0 : raise stmgrs = [ stmgrs [ normalized_cid ] ] except : Log . error ( 'Invalid container id: %d' % container_id ) return False table = [ ] for sid , name in enumerate ( stmgrs ) : cid = sid + 1 host = containers [ name ] [ "host" ] port = containers [ name ] [ "port" ] pid = containers [ name ] [ "pid" ] instances = containers [ name ] [ "instance_ids" ] bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] sys . stdout . flush ( ) print ( tabulate ( table , headers = headers ) ) return True
11969	def _bits_to_dec ( nm , check = True ) : if check and not is_bits_nm ( nm ) : raise ValueError ( '_bits_to_dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID_NETMASKS [ bits ]
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
7162	def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
610	def _generateFileFromTemplates ( templateFileNames , outputFilePath , replacementDict ) : installPath = os . path . dirname ( __file__ ) outputFile = open ( outputFilePath , "w" ) outputLines = [ ] inputLines = [ ] firstFile = True for templateFileName in templateFileNames : if not firstFile : inputLines . extend ( [ os . linesep ] * 2 ) firstFile = False inputFilePath = os . path . join ( installPath , templateFileName ) inputFile = open ( inputFilePath ) inputLines . extend ( inputFile . readlines ( ) ) inputFile . close ( ) print "Writing " , len ( inputLines ) , "lines..." for line in inputLines : tempLine = line for k , v in replacementDict . iteritems ( ) : if v is None : v = "None" tempLine = re . sub ( k , v , tempLine ) outputFile . write ( tempLine ) outputFile . close ( )
629	def _bitForCoordinate ( cls , coordinate , n ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getUInt32 ( n )
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
8219	def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
12385	def parse ( text , encoding = 'utf8' ) : if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) return Query ( text , split_segments ( text ) )
2613	def pack_apply_message ( f , args , kwargs , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : arg_bufs = list ( chain . from_iterable ( serialize_object ( arg , buffer_threshold , item_threshold ) for arg in args ) ) kw_keys = sorted ( kwargs . keys ( ) ) kwarg_bufs = list ( chain . from_iterable ( serialize_object ( kwargs [ key ] , buffer_threshold , item_threshold ) for key in kw_keys ) ) info = dict ( nargs = len ( args ) , narg_bufs = len ( arg_bufs ) , kw_keys = kw_keys ) msg = [ pickle . dumps ( can ( f ) , PICKLE_PROTOCOL ) ] msg . append ( pickle . dumps ( info , PICKLE_PROTOCOL ) ) msg . extend ( arg_bufs ) msg . extend ( kwarg_bufs ) return msg
6421	def encode ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = word . translate ( { 198 : 'AE' , 338 : 'OE' } ) word = '' . join ( c for c in word if c in self . _uc_set ) for rule in self . _rule_order : regex , repl = self . _rule_table [ rule ] if isinstance ( regex , text_type ) : word = word . replace ( regex , repl ) else : word = regex . sub ( repl , word ) return word
12871	def chain ( * args ) : def chain_block ( * args , ** kwargs ) : v = args [ 0 ] ( * args , ** kwargs ) for p in args [ 1 : ] : v = p ( v ) return v return chain_block
9267	def get_time_of_tag ( self , tag ) : if not tag : raise ChangelogGeneratorError ( "tag is nil" ) name_of_tag = tag [ "name" ] time_for_name = self . tag_times_dict . get ( name_of_tag , None ) if time_for_name : return time_for_name else : time_string = self . fetcher . fetch_date_of_tag ( tag ) try : self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) except UnicodeWarning : print ( "ERROR ERROR:" , tag ) self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) return self . tag_times_dict [ name_of_tag ]
207	def draw ( self , size = None , cmap = "jet" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = "nearest" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn
3575	def peripheral_didReadRSSI_error_ ( self , peripheral , rssi , error ) : logger . debug ( 'peripheral_didReadRSSI_error called' ) if error is not None : return device = device_list ( ) . get ( peripheral ) if device is not None : device . _rssi_changed ( rssi )
7315	def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
4185	def window_flattop ( N , mode = 'symmetric' , precision = None ) : r assert mode in [ 'periodic' , 'symmetric' ] t = arange ( 0 , N ) if mode == 'periodic' : x = 2 * pi * t / float ( N ) else : if N == 1 : return ones ( 1 ) x = 2 * pi * t / float ( N - 1 ) a0 = 0.21557895 a1 = 0.41663158 a2 = 0.277263158 a3 = 0.083578947 a4 = 0.006947368 if precision == 'octave' : d = 4.6402 a0 = 1. / d a1 = 1.93 / d a2 = 1.29 / d a3 = 0.388 / d a4 = 0.0322 / d w = a0 - a1 * cos ( x ) + a2 * cos ( 2 * x ) - a3 * cos ( 3 * x ) + a4 * cos ( 4 * x ) return w
1278	def markdown ( text , escape = True , ** kwargs ) : return Markdown ( escape = escape , ** kwargs ) ( text )
5246	def update_missing ( ** kwargs ) : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return if len ( kwargs ) == 0 : return log_path = f'{data_path}/Logs/{missing_info(**kwargs)}' cnt = len ( files . all_files ( log_path ) ) + 1 files . create_folder ( log_path ) open ( f'{log_path}/{cnt}.log' , 'a' ) . close ( )
13661	def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : load_overrides ( args . config ) defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) config_cli_args = get_group ( 'cli' ) for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
4905	def create_course_completion ( self , user_id , payload ) : return self . _post ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
8363	def get_key_map ( self ) : kdict = { } for gdk_name in dir ( Gdk ) : nb_name = gdk_name . upper ( ) kdict [ nb_name ] = getattr ( Gdk , gdk_name ) return kdict
10525	def get_google_playlist ( self , playlist ) : logger . info ( "Loading playlist {0}" . format ( playlist ) ) for google_playlist in self . api . get_all_user_playlist_contents ( ) : if google_playlist [ 'name' ] == playlist or google_playlist [ 'id' ] == playlist : return google_playlist else : logger . warning ( "Playlist {0} does not exist." . format ( playlist ) ) return { }
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
10132	def parse_grid ( grid_data ) : try : grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
831	def decode ( self , encoded , parentFieldName = '' ) : fieldsDict = dict ( ) fieldsOrder = [ ] if parentFieldName == '' : parentName = self . name else : parentName = "%s.%s" % ( parentFieldName , self . name ) if self . encoders is not None : for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] ( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , parentFieldName = parentName ) fieldsDict . update ( subFieldsDict ) fieldsOrder . extend ( subFieldsOrder ) return ( fieldsDict , fieldsOrder )
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13534	def can_remove ( self ) : if self . children . count ( ) == 0 : return True ancestors = set ( self . ancestors_root ( ) ) children = set ( self . children . all ( ) ) return children . issubset ( ancestors )
1413	def _get_packing_plan_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_packing_plan_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_packing_plan ( data , stats ) : if data : packing_plan = PackingPlan ( ) packing_plan . ParseFromString ( data ) callback ( packing_plan ) else : callback ( None ) return isWatching
8412	def numeric_to_timedelta ( self , numerics ) : if self . package == 'pandas' : return [ self . type ( int ( x * self . factor ) , units = 'ns' ) for x in numerics ] else : return [ self . type ( seconds = x * self . factor ) for x in numerics ]
6308	def load_resource_module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
10286	def get_subgraph_peripheral_nodes ( graph : BELGraph , subgraph : Iterable [ BaseEntity ] , node_predicates : NodePredicates = None , edge_predicates : EdgePredicates = None , ) : node_filter = concatenate_node_predicates ( node_predicates = node_predicates ) edge_filter = and_edge_predicates ( edge_predicates = edge_predicates ) result = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for u , v , k , d in get_peripheral_successor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ v ] [ 'predecessor' ] [ u ] . append ( ( k , d ) ) for u , v , k , d in get_peripheral_predecessor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ u ] [ 'successor' ] [ v ] . append ( ( k , d ) ) return result
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
3525	def uservoice ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return UserVoiceNode ( )
11845	def list_things_at ( self , location , tclass = Thing ) : "Return all things exactly at a given location." return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) return len ( errors ) == 0 , errors
2698	def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
4052	def dump ( self , itemkey , filename = None , path = None ) : if not filename : filename = self . item ( itemkey ) [ "data" ] [ "filename" ] if path : pth = os . path . join ( path , filename ) else : pth = filename file = self . file ( itemkey ) if self . snapshot : self . snapshot = False pth = pth + ".zip" with open ( pth , "wb" ) as f : f . write ( file )
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
1587	def check_output_schema ( self , stream_id , tup ) : size = self . _output_schema . get ( stream_id , None ) if size is None : raise RuntimeError ( "%s emitting to stream %s but was not declared in output fields" % ( self . my_component_name , stream_id ) ) elif size != len ( tup ) : raise RuntimeError ( "Number of fields emitted in stream %s does not match what's expected. " "Expected: %s, Observed: %s" % ( stream_id , size , len ( tup ) ) )
6182	def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
13423	def line ( self , line ) : return [ x for x in re . split ( self . delimiter , line . rstrip ( ) ) if x != '' ]
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
3234	def list_targets_by_rule ( client = None , ** kwargs ) : result = client . list_targets_by_rule ( ** kwargs ) if not result . get ( "Targets" ) : result . update ( { "Targets" : [ ] } ) return result
9470	def conference_list_members ( self , call_params ) : path = '/' + self . api_version + '/ConferenceListMembers/' method = 'POST' return self . request ( path , method , call_params )
7651	def query_pop ( query , prefix , sep = '.' ) : terms = query . split ( sep ) if terms [ 0 ] == prefix : terms = terms [ 1 : ] return sep . join ( terms )
4299	def create_project ( config_data ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) kwargs = { } args = [ ] if config_data . template : kwargs [ 'template' ] = config_data . template args . append ( config_data . project_name ) if config_data . project_directory : args . append ( config_data . project_directory ) if not os . path . exists ( config_data . project_directory ) : os . makedirs ( config_data . project_directory ) base_cmd = 'django-admin.py' start_cmds = [ os . path . join ( os . path . dirname ( sys . executable ) , base_cmd ) ] start_cmd_pnodes = [ 'Scripts' ] start_cmds . extend ( [ os . path . join ( os . path . dirname ( sys . executable ) , pnode , base_cmd ) for pnode in start_cmd_pnodes ] ) start_cmd = [ base_cmd ] for p in start_cmds : if os . path . exists ( p ) : start_cmd = [ sys . executable , p ] break cmd_args = start_cmd + [ 'startproject' ] + args if config_data . verbose : sys . stdout . write ( 'Project creation command: {0}\n' . format ( ' ' . join ( cmd_args ) ) ) try : output = subprocess . check_output ( cmd_args , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
11710	def request ( self , path , data = None , headers = None , method = None ) : if isinstance ( data , str ) : data = data . encode ( 'utf-8' ) response = urlopen ( self . _request ( path , data = data , headers = headers , method = method ) ) self . _set_session_cookie ( response ) return response
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , ** kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , ** fix_kw ( kw ) ) as r : for item in r : yield item
7292	def set_post_data ( self ) : self . form . data = self . post_data_dict for field_key , field in self . form . fields . items ( ) : if has_digit ( field_key ) : base_key = make_key ( field_key , exclude_last_string = True ) for key in self . post_data_dict . keys ( ) : if base_key in key : self . form . fields . update ( { key : field } )
13030	def make_server ( host , port , app = None , server_class = AsyncWsgiServer , handler_class = AsyncWsgiHandler , ws_handler_class = None , ws_path = '/ws' ) : handler_class . ws_handler_class = ws_handler_class handler_class . ws_path = ws_path httpd = server_class ( ( host , port ) , RequestHandlerClass = handler_class ) httpd . set_app ( app ) return httpd
11447	def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False return ( session , req ) if get_request else session
3319	def create ( self , path , lock ) : self . _lock . acquire_write ( ) try : assert lock . get ( "token" ) is None assert lock . get ( "expire" ) is None , "Use timeout instead of expire" assert path and "/" in path org_path = path path = normalize_lock_root ( path ) lock [ "root" ] = path timeout = float ( lock . get ( "timeout" ) ) if timeout is None : timeout = LockStorageDict . LOCK_TIME_OUT_DEFAULT elif timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout validate_lock ( lock ) token = generate_lock_token ( ) lock [ "token" ] = token self . _dict [ token ] = lock key = "URL2TOKEN:{}" . format ( path ) if key not in self . _dict : self . _dict [ key ] = [ token ] else : tokList = self . _dict [ key ] tokList . append ( token ) self . _dict [ key ] = tokList self . _flush ( ) _logger . debug ( "LockStorageDict.set({!r}): {}" . format ( org_path , lock_string ( lock ) ) ) return lock finally : self . _lock . release ( )
12224	def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
2474	def set_lic_text ( self , doc , text ) : if self . has_extr_lic ( doc ) : if not self . extr_text_set : self . extr_text_set = True if validations . validate_is_free_form_text ( text ) : self . extr_lic ( doc ) . text = str_from_text ( text ) return True else : raise SPDXValueError ( 'ExtractedLicense::text' ) else : raise CardinalityError ( 'ExtractedLicense::text' ) else : raise OrderError ( 'ExtractedLicense::text' )
9263	def get_filtered_pull_requests ( self , pull_requests ) : pull_requests = self . filter_by_labels ( pull_requests , "pull requests" ) pull_requests = self . filter_merged_pull_requests ( pull_requests ) if self . options . verbose > 1 : print ( "\tremaining pull requests: {}" . format ( len ( pull_requests ) ) ) return pull_requests
7494	def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
7652	def match_query ( string , query ) : if six . callable ( query ) : return query ( string ) elif ( isinstance ( query , six . string_types ) and isinstance ( string , six . string_types ) ) : return re . match ( query , string ) is not None else : return query == string
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) return added , updated , removed
9028	def instructions ( self ) : x = self . x y = self . y result = [ ] for instruction in self . _row . instructions : instruction_in_grid = InstructionInGrid ( instruction , Point ( x , y ) ) x += instruction_in_grid . width result . append ( instruction_in_grid ) return result
5943	def autoconvert ( s ) : if type ( s ) is not str : return s for converter in int , float , str : try : s = [ converter ( i ) for i in s . split ( ) ] if len ( s ) == 1 : return s [ 0 ] else : return numpy . array ( s ) except ( ValueError , AttributeError ) : pass raise ValueError ( "Failed to autoconvert {0!r}" . format ( s ) )
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) return ret
12583	def _safe_cache ( memory , func , ** kwargs ) : cachedir = memory . cachedir if cachedir is None or cachedir in __CACHE_CHECKED : return memory . cache ( func , ** kwargs ) version_file = os . path . join ( cachedir , 'module_versions.json' ) versions = dict ( ) if os . path . exists ( version_file ) : with open ( version_file , 'r' ) as _version_file : versions = json . load ( _version_file ) modules = ( nibabel , ) my_versions = dict ( ( m . __name__ , LooseVersion ( m . __version__ ) . version [ : 2 ] ) for m in modules ) commons = set ( versions . keys ( ) ) . intersection ( set ( my_versions . keys ( ) ) ) collisions = [ m for m in commons if versions [ m ] != my_versions [ m ] ] if len ( collisions ) > 0 : if nilearn . CHECK_CACHE_VERSION : warnings . warn ( "Incompatible cache in %s: " "different version of nibabel. Deleting " "the cache. Put nilearn.CHECK_CACHE_VERSION " "to false to avoid this behavior." % cachedir ) try : tmp_dir = ( os . path . split ( cachedir ) [ : - 1 ] + ( 'old_%i' % os . getpid ( ) , ) ) tmp_dir = os . path . join ( * tmp_dir ) os . rename ( cachedir , tmp_dir ) shutil . rmtree ( tmp_dir ) except OSError : pass try : os . makedirs ( cachedir ) except OSError : pass else : warnings . warn ( "Incompatible cache in %s: " "old version of nibabel." % cachedir ) if versions != my_versions : with open ( version_file , 'w' ) as _version_file : json . dump ( my_versions , _version_file ) __CACHE_CHECKED [ cachedir ] = True return memory . cache ( func , ** kwargs )
214	def change_normalization ( cls , arr , source , target ) : ia . do_assert ( ia . is_np_array ( arr ) ) if isinstance ( source , HeatmapsOnImage ) : source = ( source . min_value , source . max_value ) else : ia . do_assert ( isinstance ( source , tuple ) ) ia . do_assert ( len ( source ) == 2 ) ia . do_assert ( source [ 0 ] < source [ 1 ] ) if isinstance ( target , HeatmapsOnImage ) : target = ( target . min_value , target . max_value ) else : ia . do_assert ( isinstance ( target , tuple ) ) ia . do_assert ( len ( target ) == 2 ) ia . do_assert ( target [ 0 ] < target [ 1 ] ) eps = np . finfo ( arr . dtype ) . eps mins_same = source [ 0 ] - 10 * eps < target [ 0 ] < source [ 0 ] + 10 * eps maxs_same = source [ 1 ] - 10 * eps < target [ 1 ] < source [ 1 ] + 10 * eps if mins_same and maxs_same : return np . copy ( arr ) min_source , max_source = source min_target , max_target = target diff_source = max_source - min_source diff_target = max_target - min_target arr_0to1 = ( arr - min_source ) / diff_source arr_target = min_target + arr_0to1 * diff_target return arr_target
3969	def _conditional_links ( assembled_specs , app_name ) : link_to_apps = [ ] potential_links = assembled_specs [ 'apps' ] [ app_name ] [ 'conditional_links' ] for potential_link in potential_links [ 'apps' ] : if potential_link in assembled_specs [ 'apps' ] : link_to_apps . append ( potential_link ) for potential_link in potential_links [ 'services' ] : if potential_link in assembled_specs [ 'services' ] : link_to_apps . append ( potential_link ) return link_to_apps
4280	def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce_opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) return Image . composite ( layer , im , layer )
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
10637	def get_element_mfr_dictionary ( self ) : element_symbols = self . material . elements element_mfrs = self . get_element_mfrs ( ) result = dict ( ) for s , mfr in zip ( element_symbols , element_mfrs ) : result [ s ] = mfr return result
6568	def random_xorsat ( num_variables , num_clauses , vartype = dimod . BINARY , satisfiable = True ) : if num_variables < 3 : raise ValueError ( "a xor problem needs at least 3 variables" ) if num_clauses > 8 * _nchoosek ( num_variables , 3 ) : raise ValueError ( "too many clauses" ) csp = ConstraintSatisfactionProblem ( vartype ) variables = list ( range ( num_variables ) ) constraints = set ( ) if satisfiable : values = tuple ( vartype . value ) planted_solution = { v : choice ( values ) for v in variables } configurations = [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) config = choice ( configurations ) for idx , v in enumerate ( const . variables ) : if config [ idx ] != ( planted_solution [ v ] > 0 ) : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) else : while len ( constraints ) < num_clauses : x , y , z = sample ( variables , 3 ) if y > x : x , y = y , x const = xor_gate ( [ x , y , z ] , vartype = vartype ) for idx , v in enumerate ( const . variables ) : if random ( ) > .5 : const . flip_variable ( v ) assert const . check ( planted_solution ) constraints . add ( const ) for const in constraints : csp . add_constraint ( const ) for v in variables : csp . add_variable ( v ) return csp
2070	def basen_to_integer ( self , X , cols , base ) : out_cols = X . columns . values . tolist ( ) for col in cols : col_list = [ col0 for col0 in out_cols if str ( col0 ) . startswith ( str ( col ) ) ] insert_at = out_cols . index ( col_list [ 0 ] ) if base == 1 : value_array = np . array ( [ int ( col0 . split ( '_' ) [ - 1 ] ) for col0 in col_list ] ) else : len0 = len ( col_list ) value_array = np . array ( [ base ** ( len0 - 1 - i ) for i in range ( len0 ) ] ) X . insert ( insert_at , col , np . dot ( X [ col_list ] . values , value_array . T ) ) X . drop ( col_list , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
13501	def update_time ( sender , ** kwargs ) : comment = kwargs [ 'instance' ] if comment . content_type . app_label == "happenings" and comment . content_type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object_pk ) item . save ( )
5460	def get_file_environment_variables ( file_params ) : env = { } for param in file_params : env [ param . name ] = os . path . join ( DATA_MOUNT_POINT , param . docker_path . rstrip ( '/' ) ) if param . value else '' return env
4923	def courses ( self , request , enterprise_customer , pk = None ) : catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( pk , request . GET ) self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses from endpoint '{endpoint}'. " "The resource you are looking for does not exist." . format ( endpoint = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) serializer . update_enterprise_courses ( enterprise_customer , catalog_id = pk ) return get_paginated_response ( serializer . data , request )
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
13371	def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
10087	def update ( self , * args , ** kwargs ) : super ( Deposit , self ) . update ( * args , ** kwargs )
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
11171	def optionhelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : def makelabels ( option ) : labels = '%*s--%s' % ( indent , ' ' , option . name ) if option . abbreviation : labels += ', -' + option . abbreviation return labels + ': ' docs = [ ] helpindent = _autoindent ( [ makelabels ( o ) for o in self . options . values ( ) ] , indent , maxindent ) for name in self . option_order : option = self . options [ name ] labels = makelabels ( option ) helpstring = "%s(%s). %s" % ( option . formatname , option . strvalue , option . docs ) wrapped = self . _wrap_labelled ( labels , helpstring , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
748	def _anomalyCompute ( self ) : inferenceType = self . getInferenceType ( ) inferences = { } sp = self . _getSPRegion ( ) score = None if inferenceType == InferenceType . NontemporalAnomaly : score = sp . getOutputData ( "anomalyScore" ) [ 0 ] elif inferenceType == InferenceType . TemporalAnomaly : tm = self . _getTPRegion ( ) if sp is not None : activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] else : sensor = self . _getSensorRegion ( ) activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] if not self . _predictedFieldName in self . _input : raise ValueError ( "Expected predicted field '%s' in input row, but was not found!" % self . _predictedFieldName ) score = tm . getOutputData ( "anomalyScore" ) [ 0 ] if sp is not None : self . _getAnomalyClassifier ( ) . setParameter ( "activeColumnCount" , len ( activeColumns ) ) self . _getAnomalyClassifier ( ) . prepareInputs ( ) self . _getAnomalyClassifier ( ) . compute ( ) labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) inferences [ InferenceElement . anomalyLabel ] = "%s" % labels inferences [ InferenceElement . anomalyScore ] = score return inferences
10901	def check_inputs ( self , comps ) : error = False compcats = [ c . category for c in comps ] for k , v in iteritems ( self . varmap ) : if k not in self . modelstr [ 'full' ] : log . warn ( 'Component (%s : %s) not used in model.' % ( k , v ) ) if v not in compcats : log . error ( 'Map component (%s : %s) not found in list of components.' % ( k , v ) ) error = True if error : raise ModelError ( 'Component list incomplete or incorrect' )
8009	def execute ( self ) : self . executed_at = now ( ) self . save ( ) with transaction . atomic ( ) : ret = BillingAgreement . execute ( self . id ) ret . user = self . user ret . save ( ) self . executed_agreement = ret self . save ( ) return ret
10124	def flip_x ( self , center = None ) : if center is None : self . poly . flip ( ) else : self . poly . flip ( center [ 0 ] )
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , ** kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , ** kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , ** kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
1562	def get_component_tasks ( self , component_id ) : ret = [ ] for task_id , comp_id in self . task_to_component_map . items ( ) : if comp_id == component_id : ret . append ( task_id ) return ret
13187	def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
11893	def retrieve_document ( file_path , directory = 'sec_filings' ) : ftp = FTP ( 'ftp.sec.gov' , timeout = None ) ftp . login ( ) name = file_path . replace ( '/' , '_' ) if not os . path . exists ( directory ) : os . makedirs ( directory ) with tempfile . TemporaryFile ( ) as temp : ftp . retrbinary ( 'RETR %s' % file_path , temp . write ) temp . seek ( 0 ) with open ( '{}/{}' . format ( directory , name ) , 'w+' ) as f : f . write ( temp . read ( ) . decode ( "utf-8" ) ) f . closed records = temp retry = False ftp . close ( )
611	def _getPropertyValue ( schema , propertyName , options ) : if propertyName not in options : paramsSchema = schema [ 'properties' ] [ propertyName ] if 'default' in paramsSchema : options [ propertyName ] = paramsSchema [ 'default' ] else : options [ propertyName ] = None
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
787	def partitionAtIntervals ( data , intervals ) : assert sum ( intervals ) <= len ( data ) start = 0 for interval in intervals : end = start + interval yield data [ start : end ] start = end raise StopIteration
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
7587	def taxon_table ( self ) : if self . tests : keys = sorted ( self . tests [ 0 ] . keys ( ) ) if isinstance ( self . tests , list ) : ld = [ [ ( key , i [ key ] ) for key in keys ] for i in self . tests ] dd = [ dict ( i ) for i in ld ] df = pd . DataFrame ( dd ) return df else : return pd . DataFrame ( pd . Series ( self . tests ) ) . T else : return None
555	def getAllSwarms ( self , sprintIdx ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'sprintIdx' ] == sprintIdx : swarmIds . append ( swarmId ) return swarmIds
10436	def verifypartialtablecell ( self , window_name , object_name , row_index , column_index , row_text ) : try : value = getcellvalue ( window_name , object_name , row_index , column_index ) if re . searchmatch ( row_text , value ) : return 1 except LdtpServerException : pass return 0
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
9773	def statuses ( ctx , page ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) page = page or 1 try : response = PolyaxonClient ( ) . job . get_statuses ( user , project_name , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True )
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) return [ f for list_ in chopped_filenames for f in list_ ]
12461	def pip_cmd ( env , cmd , ignore_activated = False , ** kwargs ) : r cmd = tuple ( cmd ) dirname = safe_path ( env ) if not ignore_activated : activated_env = os . environ . get ( 'VIRTUAL_ENV' ) if hasattr ( sys , 'real_prefix' ) : dirname = sys . prefix elif activated_env : dirname = activated_env pip_path = os . path . join ( dirname , 'Scripts' if IS_WINDOWS else 'bin' , 'pip' ) if kwargs . pop ( 'return_path' , False ) : return pip_path if not os . path . isfile ( pip_path ) : raise OSError ( 'No pip found at {0!r}' . format ( pip_path ) ) if BOOTSTRAPPER_TEST_KEY in os . environ and cmd [ 0 ] == 'install' : cmd = list ( cmd ) cmd . insert ( 1 , '--disable-pip-version-check' ) cmd = tuple ( cmd ) with disable_error_handler ( ) : return run_cmd ( ( pip_path , ) + cmd , ** kwargs )
2622	def spin_up_instance ( self , command , job_name ) : command = Template ( template_string ) . substitute ( jobname = job_name , user_script = command , linger = str ( self . linger ) . lower ( ) , worker_init = self . worker_init ) instance_type = self . instance_type subnet = self . sn_ids [ 0 ] ami_id = self . image_id total_instances = len ( self . instances ) if float ( self . spot_max_bid ) > 0 : spot_options = { 'MarketType' : 'spot' , 'SpotOptions' : { 'MaxPrice' : str ( self . spot_max_bid ) , 'SpotInstanceType' : 'one-time' , 'InstanceInterruptionBehavior' : 'terminate' } } else : spot_options = { } if total_instances > self . max_nodes : logger . warn ( "Exceeded instance limit ({}). Cannot continue\n" . format ( self . max_nodes ) ) return [ None ] try : tag_spec = [ { "ResourceType" : "instance" , "Tags" : [ { 'Key' : 'Name' , 'Value' : job_name } ] } ] instance = self . ec2 . create_instances ( MinCount = 1 , MaxCount = 1 , InstanceType = instance_type , ImageId = ami_id , KeyName = self . key_name , SubnetId = subnet , SecurityGroupIds = [ self . sg_id ] , TagSpecifications = tag_spec , InstanceMarketOptions = spot_options , InstanceInitiatedShutdownBehavior = 'terminate' , IamInstanceProfile = { 'Arn' : self . iam_instance_profile_arn } , UserData = command ) except ClientError as e : print ( e ) logger . error ( e . response ) return [ None ] except Exception as e : logger . error ( "Request for EC2 resources failed : {0}" . format ( e ) ) return [ None ] self . instances . append ( instance [ 0 ] . id ) logger . info ( "Started up 1 instance {} . Instance type:{}" . format ( instance [ 0 ] . id , instance_type ) ) return instance
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
3343	def calc_base64 ( s ) : s = compat . to_bytes ( s ) s = compat . base64_encodebytes ( s ) . strip ( ) return compat . to_native ( s )
3316	def get_domain_realm ( self , path_info , environ ) : realm = self . _calc_realm_from_path_provider ( path_info , environ ) return realm
11889	def get_lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : for bulb in self . _bulbs : try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) return self . _bulbs
5567	def area_at_zoom ( self , zoom = None ) : if zoom is None : if not self . _cache_full_process_area : logger . debug ( "calculate process area ..." ) self . _cache_full_process_area = cascaded_union ( [ self . _area_at_zoom ( z ) for z in self . init_zoom_levels ] ) . buffer ( 0 ) return self . _cache_full_process_area else : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) return self . _area_at_zoom ( zoom )
11927	def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
9881	def _reliability_data_to_value_counts ( reliability_data , value_domain ) : return np . array ( [ [ sum ( 1 for rate in unit if rate == v ) for v in value_domain ] for unit in reliability_data . T ] )
6684	def check_for_change ( self ) : r = self . local_renderer lm = self . last_manifest last_fingerprint = lm . fingerprint current_fingerprint = self . get_target_geckodriver_version_number ( ) self . vprint ( 'last_fingerprint:' , last_fingerprint ) self . vprint ( 'current_fingerprint:' , current_fingerprint ) if last_fingerprint != current_fingerprint : print ( 'A new release is available. %s' % self . get_most_recent_version ( ) ) return True print ( 'No updates found.' ) return False
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
13234	def make_aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : return timezone . localize ( value , is_dst = None ) else : return value . replace ( tzinfo = timezone )
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
1352	def make_success_response ( self , result ) : response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) response [ constants . RESPONSE_KEY_RESULT ] = result return response
10453	def waittillguinotexist ( self , window_name , object_name = '' , guiTimeOut = 30 ) : timeout = 0 while timeout < guiTimeOut : if not self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
4190	def window_poisson_hanning ( N , alpha = 2 ) : r w1 = window_hann ( N ) w2 = window_poisson ( N , alpha = alpha ) return w1 * w2
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
4472	def transform ( self , jam ) : for state in self . states ( jam ) : yield self . _transform ( jam , state )
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) if not img_np . flags [ "WRITEABLE" ] : try : img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
9198	def pop ( self , key , default = _sentinel ) : if default is not _sentinel : tup = self . _data . pop ( key . lower ( ) , default ) else : tup = self . _data . pop ( key . lower ( ) ) if tup is not default : return tup [ 1 ] else : return default
11453	def convert_all ( cls , records ) : out = [ "<collection>" ] for rec in records : conversion = cls ( rec ) out . append ( conversion . convert ( ) ) out . append ( "</collection>" ) return "\n" . join ( out )
10017	def upload_archive ( self , filename , key , auto_create_bucket = True ) : try : bucket = self . s3 . get_bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get_location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get_location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get_location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3ResponseError : bucket = self . s3 . create_bucket ( self . aws . bucket , location = self . aws . region ) def __report_upload_progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) k = Key ( bucket ) k . key = self . aws . bucket_path + key k . set_metadata ( 'time' , str ( time ( ) ) ) k . set_contents_from_filename ( filename , cb = __report_upload_progress , num_cb = 10 )
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
5159	def _add_tc_script ( self ) : context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
1660	def CheckCStyleCast ( filename , clean_lines , linenum , cast_type , pattern , error ) : line = clean_lines . elided [ linenum ] match = Search ( pattern , line ) if not match : return False context = line [ 0 : match . start ( 1 ) - 1 ] if Match ( r'.*\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\s*$' , context ) : return False if linenum > 0 : for i in xrange ( linenum - 1 , max ( 0 , linenum - 5 ) , - 1 ) : context = clean_lines . elided [ i ] + context if Match ( r'.*\b[_A-Z][_A-Z0-9]*\s*\((?:\([^()]*\)|[^()])*$' , context ) : return False if context . endswith ( ' operator++' ) or context . endswith ( ' operator--' ) : return False remainder = line [ match . end ( 0 ) : ] if Match ( r'^\s*(?:;|const\b|throw\b|final\b|override\b|[=>{),]|->)' , remainder ) : return False error ( filename , linenum , 'readability/casting' , 4 , 'Using C-style cast. Use %s<%s>(...) instead' % ( cast_type , match . group ( 1 ) ) ) return True
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
2546	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True doc . annotations [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'AnnotationComment' ) else : raise OrderError ( 'AnnotationComment' )
4839	def get_course_details ( self , course_id ) : return self . _load_data ( self . COURSES_ENDPOINT , resource_id = course_id , many = False )
5406	def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
1262	def import_demonstrations ( self , demonstrations ) : if isinstance ( demonstrations , dict ) : if self . unique_state : demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) if self . unique_action : demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) self . model . import_demo_experience ( ** demonstrations ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for demonstration in demonstrations : if self . unique_state : states [ 'state' ] . append ( demonstration [ 'states' ] ) else : for name , state in states . items ( ) : state . append ( demonstration [ 'states' ] [ name ] ) for name , internal in internals . items ( ) : internal . append ( demonstration [ 'internals' ] [ name ] ) if self . unique_action : actions [ 'action' ] . append ( demonstration [ 'actions' ] ) else : for name , action in actions . items ( ) : action . append ( demonstration [ 'actions' ] [ name ] ) terminal . append ( demonstration [ 'terminal' ] ) reward . append ( demonstration [ 'reward' ] ) self . model . import_demo_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
5190	def send_direct_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . DirectOperate ( command , index , callback , config )
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
8604	def update_user ( self , user_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
1919	def linux ( cls , path , argv = None , envp = None , entry_symbol = None , symbolic_files = None , concrete_start = '' , pure_symbolic = False , stdin_size = None , ** kwargs ) : if stdin_size is None : stdin_size = consts . stdin_size try : return cls ( _make_linux ( path , argv , envp , entry_symbol , symbolic_files , concrete_start , pure_symbolic , stdin_size ) , ** kwargs ) except elftools . common . exceptions . ELFError : raise Exception ( f'Invalid binary: {path}' )
13735	def get_api_error ( response ) : error_class = _status_code_to_class . get ( response . status_code , APIError ) return error_class ( response )
1685	def RepositoryName ( self ) : r fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] return fullname
8619	def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server_info = [ ] servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) return server_info
5993	def get_normalization_min_max ( array , norm_min , norm_max ) : if norm_min is None : norm_min = array . min ( ) if norm_max is None : norm_max = array . max ( ) return norm_min , norm_max
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
2456	def set_pkg_license_from_file ( self , doc , lic ) : self . assert_package_exists ( ) if validations . validate_lics_from_file ( lic ) : doc . package . licenses_from_files . append ( lic ) return True else : raise SPDXValueError ( 'Package::LicensesFromFile' )
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
2023	def LT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . ULT ( a , b ) , 1 , 0 )
5925	def check_setup ( ) : if "GROMACSWRAPPER_SUPPRESS_SETUP_CHECK" in os . environ : return True missing = [ d for d in config_directories if not os . path . exists ( d ) ] if len ( missing ) > 0 : print ( "NOTE: Some configuration directories are not set up yet: " ) print ( "\t{0!s}" . format ( '\n\t' . join ( missing ) ) ) print ( "NOTE: You can create the configuration file and directories with:" ) print ( "\t>>> import gromacs" ) print ( "\t>>> gromacs.config.setup()" ) return False return True
4051	def file ( self , item , ** kwargs ) : query_string = "/{t}/{u}/items/{i}/file" . format ( u = self . library_id , t = self . library_type , i = item . upper ( ) ) return self . _build_query ( query_string , no_params = True )
2299	def predict_features ( self , df_features , df_target , nh = 20 , idx = 0 , dropout = 0. , activation_function = th . nn . ReLU , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = 1000 , test_epochs = 1000 , device = None , verbose = None , nb_runs = 3 ) : device , verbose = SETTINGS . get_default ( ( 'device' , device ) , ( 'verbose' , verbose ) ) x = th . FloatTensor ( scale ( df_features . values ) ) . to ( device ) y = th . FloatTensor ( scale ( df_target . values ) ) . to ( device ) out = [ ] for i in range ( nb_runs ) : model = FSGNN_model ( [ x . size ( ) [ 1 ] + 1 , nh , 1 ] , dropout = dropout , activation_function = activation_function ) . to ( device ) out . append ( model . train ( x , y , lr = 0.01 , l1 = 0.1 , batch_size = - 1 , train_epochs = train_epochs , test_epochs = test_epochs , device = device , verbose = verbose ) ) return list ( np . mean ( np . array ( out ) , axis = 0 ) )
11720	def get_directory ( self , path_to_directory , timeout = 30 , backoff = 0.4 , max_wait = 4 ) : response = None started_at = None time_elapsed = 0 i = 0 while time_elapsed < timeout : response = self . _get ( '{0}.zip' . format ( path_to_directory ) ) if response : break else : if started_at is None : started_at = time . time ( ) time . sleep ( min ( backoff * ( 2 ** i ) , max_wait ) ) i += 1 time_elapsed = time . time ( ) - started_at return response
11584	def background_image_finder ( pipeline_index , soup , finder_image_urls = [ ] , * args , ** kwargs ) : now_finder_image_urls = [ ] for tag in soup . find_all ( style = True ) : style_string = tag [ 'style' ] if 'background-image' in style_string . lower ( ) : style = cssutils . parseStyle ( style_string ) background_image = style . getProperty ( 'background-image' ) if background_image : for property_value in background_image . propertyValue : background_image_url = str ( property_value . value ) if background_image_url : if ( background_image_url not in finder_image_urls ) and ( background_image_url not in now_finder_image_urls ) : now_finder_image_urls . append ( background_image_url ) output = { } output [ 'finder_image_urls' ] = finder_image_urls + now_finder_image_urls return output
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
6260	def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
9063	def unfix ( self , param ) : if param == "delta" : self . _unfix ( "logistic" ) else : self . _fix [ param ] = False
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
3567	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
4282	def video_size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot_pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot_match = rot_pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot_match : x , y = y , x return x , y
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
9100	def write_bel_namespace_mappings ( self , file : TextIO , ** kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( ** kwargs ) , file , indent = 2 , sort_keys = True )
136	def change_first_point_by_index ( self , point_idx ) : ia . do_assert ( 0 <= point_idx < len ( self . exterior ) ) if point_idx == 0 : return self . deepcopy ( ) exterior = np . concatenate ( ( self . exterior [ point_idx : , : ] , self . exterior [ : point_idx , : ] ) , axis = 0 ) return self . deepcopy ( exterior = exterior )
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
1553	def _get_comp_config ( self ) : proto_config = topology_pb2 . Config ( ) key = proto_config . kvs . add ( ) key . key = TOPOLOGY_COMPONENT_PARALLELISM key . value = str ( self . parallelism ) key . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) if self . custom_config is not None : sanitized = self . _sanitize_config ( self . custom_config ) for key , value in sanitized . items ( ) : if isinstance ( value , str ) : kvs = proto_config . kvs . add ( ) kvs . key = key kvs . value = value kvs . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) else : kvs = proto_config . kvs . add ( ) kvs . key = key kvs . serialized_value = default_serializer . serialize ( value ) kvs . type = topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) return proto_config
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( ** mpl_kwargs ) lines = LineCollection ( ** line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( ** scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( ** animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( ** save_args )
2316	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_pc ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True activate ( current_language ) return sent
7183	def parse_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<type_comment>' , 'eval' ) except SyntaxError : raise ValueError ( f"invalid type comment: {type_comment!r}" ) from None assert isinstance ( result , ast3 . Expression ) return result . body
11375	def _crawl_elsevier_and_find_issue_xml ( self ) : self . _found_issues = [ ] if not self . path and not self . package_name : for issue in self . conn . _get_issues ( ) : dirname = issue . rstrip ( '/issue.xml' ) try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) else : def visit ( dummy , dirname , names ) : if "issue.xml" in names : try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) walk ( self . path , visit , None )
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
6200	def _sim_timestamps ( self , max_rate , bg_rate , emission , i_start , rs , ip_start = 0 , scale = 10 , sort = True ) : counts_chunk = sim_timetrace_bg ( emission , max_rate , bg_rate , self . t_step , rs = rs ) nrows = emission . shape [ 0 ] if bg_rate is not None : nrows += 1 assert counts_chunk . shape == ( nrows , emission . shape [ 1 ] ) max_counts = counts_chunk . max ( ) if max_counts == 0 : return np . array ( [ ] , dtype = np . int64 ) , np . array ( [ ] , dtype = np . int64 ) time_start = i_start * scale time_stop = time_start + counts_chunk . shape [ 1 ] * scale ts_range = np . arange ( time_start , time_stop , scale , dtype = 'int64' ) times_chunk_p = [ ] par_index_chunk_p = [ ] for ip , counts_chunk_ip in enumerate ( counts_chunk ) : times_c_ip = [ ] for v in range ( 1 , max_counts + 1 ) : times_c_ip . append ( ts_range [ counts_chunk_ip >= v ] ) t = np . hstack ( times_c_ip ) times_chunk_p . append ( t ) par_index_chunk_p . append ( np . full ( t . size , ip + ip_start , dtype = 'u1' ) ) times_chunk = np . hstack ( times_chunk_p ) par_index_chunk = np . hstack ( par_index_chunk_p ) if sort : index_sort = times_chunk . argsort ( kind = 'mergesort' ) times_chunk = times_chunk [ index_sort ] par_index_chunk = par_index_chunk [ index_sort ] return times_chunk , par_index_chunk
7174	def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
1162	def set ( self ) : with self . __cond : self . __flag = True self . __cond . notify_all ( )
9800	def bookmark ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : PolyaxonClient ( ) . experiment_group . bookmark ( user , project_name , _group ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments group is bookmarked." )
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
10778	def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
10847	def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
4588	def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
10819	def get ( cls , group , user ) : try : m = cls . query . filter_by ( user_id = user . get_id ( ) , group = group ) . one ( ) return m except Exception : return None
12488	def _import_config ( filepath ) : if not op . isfile ( filepath ) : raise IOError ( 'Data config file not found. ' 'Got: {0}' . format ( filepath ) ) cfg = import_pyfile ( filepath ) if not hasattr ( cfg , 'root_path' ) : raise KeyError ( 'Config file root_path key not found.' ) if not hasattr ( cfg , 'filetree' ) : raise KeyError ( 'Config file filetree key not found.' ) return cfg . root_path , cfg . filetree
10656	def count_with_multiplier ( groups , multiplier ) : counts = collections . defaultdict ( float ) for group in groups : for element , count in group . count ( ) . items ( ) : counts [ element ] += count * multiplier return counts
11940	def mark_read ( user , message ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_delete ( user , message )
4532	def clone ( self ) : args = { k : getattr ( self , k ) for k in self . CLONE_ATTRS } args [ 'color_list' ] = copy . copy ( self . color_list ) return self . __class__ ( [ ] , ** args )
12531	def rename_file_group_to_serial_nums ( file_lst ) : file_lst . sort ( ) c = 1 for f in file_lst : dirname = get_abspath ( f . dirname ( ) ) fdest = f . joinpath ( dirname , "{0:04d}" . format ( c ) + OUTPUT_DICOM_EXTENSION ) log . info ( 'Renaming {0} to {1}' . format ( f , fdest ) ) f . rename ( fdest ) c += 1
7283	def trim_field_key ( document , field_key ) : trimming = True left_over_key_values = [ ] current_key = field_key while trimming and current_key : if hasattr ( document , current_key ) : trimming = False else : key_array = current_key . split ( "_" ) left_over_key_values . append ( key_array . pop ( ) ) current_key = u"_" . join ( key_array ) left_over_key_values . reverse ( ) return current_key , left_over_key_values
6442	def _cond_k ( self , word , suffix_len ) : return ( len ( word ) - suffix_len >= 3 ) and ( word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 ] == 'u' and word [ - suffix_len - 1 ] == 'e' ) )
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
7569	def fullcomp ( seq ) : seq = seq . replace ( "A" , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . replace ( 'G' , 'z' ) . replace ( 'u' , 'T' ) . replace ( 'v' , 'A' ) . replace ( 'p' , 'G' ) . replace ( 'z' , 'C' ) seq = seq . replace ( 'R' , 'u' ) . replace ( 'K' , 'v' ) . replace ( 'Y' , 'b' ) . replace ( 'M' , 'o' ) . replace ( 'u' , 'Y' ) . replace ( 'v' , 'M' ) . replace ( 'b' , 'R' ) . replace ( 'o' , 'K' ) seq = seq . replace ( 'r' , 'u' ) . replace ( 'k' , 'v' ) . replace ( 'y' , 'b' ) . replace ( 'm' , 'o' ) . replace ( 'u' , 'y' ) . replace ( 'v' , 'm' ) . replace ( 'b' , 'r' ) . replace ( 'o' , 'k' ) return seq
1250	def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward
10191	def get_anonymization_salt ( ts ) : salt_key = 'stats:salt:{}' . format ( ts . date ( ) . isoformat ( ) ) salt = current_cache . get ( salt_key ) if not salt : salt_bytes = os . urandom ( 32 ) salt = b64encode ( salt_bytes ) . decode ( 'utf-8' ) current_cache . set ( salt_key , salt , timeout = 60 * 60 * 24 ) return salt
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
1453	def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
9938	def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app_location = self . storages [ app ] . location if app_location not in searched_locations : searched_locations . append ( app_location ) match = self . find_in_app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
10800	def _newcall ( self , rvecs ) : sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
8188	def betweenness_centrality ( self , normalized = True ) : bc = proximity . brandes_betweenness_centrality ( self , normalized ) for id , w in bc . iteritems ( ) : self [ id ] . _betweenness = w return bc
7695	def timeout_handler ( interval , recurring = None ) : def decorator ( func ) : func . _pyxmpp_timeout = interval func . _pyxmpp_recurring = recurring return func return decorator
10927	def _run1 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) delta_vals = self . find_LM_updates ( self . calc_grad ( ) ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if not good_step : er0 = self . update_function ( self . param_vals ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '\t\t%f\t%f' % ( self . error , er1 ) ) grad = self . calc_grad ( ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if good_step : break else : er0 = self . update_function ( self . param_vals ) CLOG . warn ( 'Stuck!' ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) if good_step : self . _last_error = self . error self . error = er1 CLOG . debug ( 'Good step\t%f\t%f' % ( self . _last_error , self . error ) ) self . update_param_vals ( delta_vals , incremental = True ) self . decrease_damping ( )
1626	def ReverseCloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if line [ pos ] not in ')}]>' : return ( line , 0 , - 1 ) ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) if start_pos > - 1 : return ( line , linenum , start_pos ) while stack and linenum > 0 : linenum -= 1 line = clean_lines . elided [ linenum ] ( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) if start_pos > - 1 : return ( line , linenum , start_pos ) return ( line , 0 , - 1 )
4711	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env_to_dict ( PREFIX , REQUIRED ) block [ "DEV_PATH" ] = "/dev/%s" % block [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , block ) return 0
4410	async def disconnect ( self ) : if not self . is_connected : return await self . stop ( ) ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , None )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
9167	def _make_celery_app ( config ) : config . registry . celery_app . conf [ 'pyramid_config' ] = config return config . registry . celery_app
2105	def version ( ) : click . echo ( 'Tower CLI %s' % __version__ ) click . echo ( 'API %s' % CUR_API_VERSION ) try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
12780	def get_users ( self , sort = True ) : self . _load ( ) if sort : self . users . sort ( key = operator . itemgetter ( "name" ) ) return self . users
5524	def jenks_breaks ( values , nb_class ) : if not isinstance ( values , Iterable ) or isinstance ( values , ( str , bytes ) ) : raise TypeError ( "A sequence of numbers is expected" ) if isinstance ( nb_class , float ) and int ( nb_class ) == nb_class : nb_class = int ( nb_class ) if not isinstance ( nb_class , int ) : raise TypeError ( "Number of class have to be a positive integer: " "expected an instance of 'int' but found {}" . format ( type ( nb_class ) ) ) nb_values = len ( values ) if np and isinstance ( values , np . ndarray ) : values = values [ np . argwhere ( np . isfinite ( values ) ) . reshape ( - 1 ) ] else : values = [ i for i in values if isfinite ( i ) ] if len ( values ) != nb_values : warnings . warn ( 'Invalid values encountered (NaN or Inf) were ignored' ) nb_values = len ( values ) if nb_class >= nb_values or nb_class < 2 : raise ValueError ( "Number of class have to be an integer " "greater than 2 and " "smaller than the number of values to use" ) return jenks . _jenks_breaks ( values , nb_class )
6542	def on_tool_finish ( self , tool ) : with self . _lock : if tool in self . current_tools : self . current_tools . remove ( tool ) self . completed_tools . append ( tool )
3068	def wrap_http_for_auth ( credentials , http ) : orig_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if not credentials . access_token : _LOGGER . info ( 'Attempting refresh to obtain ' 'initial access_token' ) credentials . _refresh ( orig_request_method ) headers = _initialize_headers ( headers ) credentials . apply ( headers ) _apply_user_agent ( headers , credentials . user_agent ) body_stream_position = None if all ( getattr ( body , stream_prop , None ) for stream_prop in _STREAM_PROPERTIES ) : body_stream_position = body . tell ( ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) max_refresh_attempts = 2 for refresh_attempt in range ( max_refresh_attempts ) : if resp . status not in REFRESH_STATUS_CODES : break _LOGGER . info ( 'Refreshing due to a %s (attempt %s/%s)' , resp . status , refresh_attempt + 1 , max_refresh_attempts ) credentials . _refresh ( orig_request_method ) credentials . apply ( headers ) if body_stream_position is not None : body . seek ( body_stream_position ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) return resp , content http . request = new_request http . request . credentials = credentials
9372	def read_stream ( schema , stream , * , buffer_size = io . DEFAULT_BUFFER_SIZE ) : reader = _lancaster . Reader ( schema ) buf = stream . read ( buffer_size ) remainder = b'' while len ( buf ) > 0 : values , n = reader . read_seq ( buf ) yield from values remainder = buf [ n : ] buf = stream . read ( buffer_size ) if len ( buf ) > 0 and len ( remainder ) > 0 : ba = bytearray ( ) ba . extend ( remainder ) ba . extend ( buf ) buf = memoryview ( ba ) . tobytes ( ) if len ( remainder ) > 0 : raise EOFError ( '{} bytes remaining but could not continue reading ' 'from stream' . format ( len ( remainder ) ) )
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
13197	def open511_convert ( input_doc , output_format , serialize = True , ** kwargs ) : try : output_format_info = FORMATS [ output_format ] except KeyError : raise ValueError ( "Unrecognized output format %s" % output_format ) input_doc = ensure_format ( input_doc , output_format_info . input_format ) result = output_format_info . func ( input_doc , ** kwargs ) if serialize : result = output_format_info . serializer ( result ) return result
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
178	def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
5240	def market_close ( self , session , mins ) -> Session : if session not in self . exch : return SessNA end_time = self . exch [ session ] [ - 1 ] return Session ( shift_time ( end_time , - int ( mins ) + 1 ) , end_time )
4665	def id ( self ) : sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) self . data [ "signatures" ] = sigs return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
12327	def update ( globalvars ) : global config profileini = getprofileini ( ) config = configparser . ConfigParser ( ) config . read ( profileini ) defaults = { } if globalvars is not None : defaults = { a [ 0 ] : a [ 1 ] for a in globalvars } generic_configs = [ { 'name' : 'User' , 'nature' : 'generic' , 'description' : "General information" , 'variables' : [ 'user.email' , 'user.name' , 'user.fullname' ] , 'defaults' : { 'user.email' : { 'value' : defaults . get ( 'user.email' , '' ) , 'description' : "Email address" , 'validator' : EmailValidator ( ) } , 'user.fullname' : { 'value' : defaults . get ( 'user.fullname' , '' ) , 'description' : "Full Name" , 'validator' : NonEmptyValidator ( ) } , 'user.name' : { 'value' : defaults . get ( 'user.name' , getpass . getuser ( ) ) , 'description' : "Name" , 'validator' : NonEmptyValidator ( ) } , } } ] mgr = plugins_get_mgr ( ) extra_configs = mgr . gather_configs ( ) allconfigs = generic_configs + extra_configs for c in allconfigs : name = c [ 'name' ] for v in c [ 'variables' ] : try : c [ 'defaults' ] [ v ] [ 'value' ] = config [ name ] [ v ] except : continue for c in allconfigs : print ( "" ) print ( c [ 'description' ] ) print ( "==================" ) if len ( c [ 'variables' ] ) == 0 : print ( "Nothing to do. Enabled by default" ) continue name = c [ 'name' ] config [ name ] = { } config [ name ] [ 'nature' ] = c [ 'nature' ] for v in c [ 'variables' ] : value = '' description = v + " " helptext = "" validator = None if v in c [ 'defaults' ] : value = c [ 'defaults' ] [ v ] . get ( 'value' , '' ) helptext = c [ 'defaults' ] [ v ] . get ( "description" , "" ) validator = c [ 'defaults' ] [ v ] . get ( 'validator' , None ) if helptext != "" : description += "(" + helptext + ")" while True : choice = input_with_default ( description , value ) if validator is not None : if validator . is_valid ( choice ) : break else : print ( "Invalid input. Expected input is {}" . format ( validator . message ) ) else : break config [ name ] [ v ] = choice if v == 'enable' and choice == 'n' : break with open ( profileini , 'w' ) as fd : config . write ( fd ) print ( "Updated profile file:" , config )
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
7246	def launch ( self , workflow ) : try : r = self . gbdx_connection . post ( self . workflows_url , json = workflow ) try : r . raise_for_status ( ) except : print ( "GBDX API Status Code: %s" % r . status_code ) print ( "GBDX API Response: %s" % r . text ) r . raise_for_status ( ) workflow_id = r . json ( ) [ 'id' ] return workflow_id except TypeError : self . logger . debug ( 'Workflow not launched!' )
1802	def LEA ( cpu , dest , src ) : dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) )
8164	def inheritFromContext ( self , ignore = ( ) ) : for canvas_attr , grob_attr in STATES . items ( ) : if canvas_attr in ignore : continue setattr ( self , grob_attr , getattr ( self . _bot . _canvas , canvas_attr ) )
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
10446	def launchapp ( self , cmd , args = [ ] , delay = 0 , env = 1 , lang = "C" ) : try : atomac . NativeUIElement . launchAppByBundleId ( cmd ) return 1 except RuntimeError : if atomac . NativeUIElement . launchAppByBundlePath ( cmd , args ) : try : time . sleep ( int ( delay ) ) except ValueError : time . sleep ( 5 ) return 1 else : raise LdtpServerException ( u"Unable to find app '%s'" % cmd )
6759	def get_package_list ( self ) : os_version = self . os_version self . vprint ( 'os_version:' , os_version ) req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
912	def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
10287	def enrich_complexes ( graph : BELGraph ) -> None : nodes = list ( get_nodes_by_function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
3293	def remove_all_properties ( self , recursive ) : if self . provider . prop_manager : self . provider . prop_manager . remove_properties ( self . get_ref_url ( ) , self . environ )
9066	def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
4186	def window_taylor ( N , nbar = 4 , sll = - 30 ) : B = 10 ** ( - sll / 20 ) A = log ( B + sqrt ( B ** 2 - 1 ) ) / pi s2 = nbar ** 2 / ( A ** 2 + ( nbar - 0.5 ) ** 2 ) ma = arange ( 1 , nbar ) def calc_Fm ( m ) : numer = ( - 1 ) ** ( m + 1 ) * prod ( 1 - m ** 2 / s2 / ( A ** 2 + ( ma - 0.5 ) ** 2 ) ) denom = 2 * prod ( [ 1 - m ** 2 / j ** 2 for j in ma if j != m ] ) return numer / denom Fm = array ( [ calc_Fm ( m ) for m in ma ] ) def W ( n ) : return 2 * np . sum ( Fm * cos ( 2 * pi * ma * ( n - N / 2 + 1 / 2 ) / N ) ) + 1 w = array ( [ W ( n ) for n in range ( N ) ] ) scale = W ( ( N - 1 ) / 2 ) w /= scale return w
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
7323	def sendmail ( message , sender , recipients , config_filename ) : if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
11170	def read_docs ( self , docsfiles ) : updates = DocParser ( ) for docsfile in _list ( docsfiles ) : if os . path . isfile ( docsfile ) : updates . parse ( docsfile ) self . docs . update ( ( k , _docs ( updates [ k ] , self . docvars ) ) for k in self . docs if updates . blocks [ k ] ) for name , text in updates [ 'parameters' ] . items ( ) : if name in self : self . getparam ( name ) . docs = text [ 0 ] % self . docvars elif name not in self . ignore : raise ValueError ( "parameter %r does not exist" % name )
1385	def set_physical_plan ( self , physical_plan ) : if not physical_plan : self . physical_plan = None self . id = None else : self . physical_plan = physical_plan self . id = physical_plan . topology . id self . trigger_watches ( )
4594	def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
102	def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]
3600	def http_connection ( timeout ) : def wrapper ( f ) : def wrapped ( * args , ** kwargs ) : if not ( 'connection' in kwargs ) or not kwargs [ 'connection' ] : connection = requests . Session ( ) kwargs [ 'connection' ] = connection else : connection = kwargs [ 'connection' ] if not getattr ( connection , 'timeout' , False ) : connection . timeout = timeout connection . headers . update ( { 'Content-type' : 'application/json' } ) return f ( * args , ** kwargs ) return wraps ( f ) ( wrapped ) return wrapper
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
7549	def _set_debug_dict ( __loglevel__ ) : _lconfig . dictConfig ( { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : "%(asctime)s \t" + "pid=%(process)d \t" + "[%(filename)s]\t" + "%(levelname)s \t" + "%(message)s" } , } , 'handlers' : { __name__ : { 'level' : __loglevel__ , 'class' : 'logging.FileHandler' , 'filename' : __debugfile__ , 'formatter' : "standard" , 'mode' : 'a+' } } , 'loggers' : { __name__ : { 'handlers' : [ __name__ ] , 'level' : __loglevel__ , 'propogate' : True } } } )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
4311	def _build_input_args ( input_filepath_list , input_format_list ) : if len ( input_format_list ) != len ( input_filepath_list ) : raise ValueError ( "input_format_list & input_filepath_list are not the same size" ) input_args = [ ] zipped = zip ( input_filepath_list , input_format_list ) for input_file , input_fmt in zipped : input_args . extend ( input_fmt ) input_args . append ( input_file ) return input_args
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
13340	def rollaxis ( a , axis , start = 0 ) : if isinstance ( a , np . ndarray ) : return np . rollaxis ( a , axis , start ) if axis not in range ( a . ndim ) : raise ValueError ( 'rollaxis: axis (%d) must be >=0 and < %d' % ( axis , a . ndim ) ) if start not in range ( a . ndim + 1 ) : raise ValueError ( 'rollaxis: start (%d) must be >=0 and < %d' % ( axis , a . ndim + 1 ) ) axes = list ( range ( a . ndim ) ) axes . remove ( axis ) axes . insert ( start , axis ) return transpose ( a , axes )
12001	def _unsign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = '' if algorithm [ 'salt_size' ] : key_salt = data [ - algorithm [ 'salt_size' ] : ] data = data [ : - algorithm [ 'salt_size' ] ] key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _decode ( data , algorithm , key ) return data
532	def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( "getParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) return getter ( paramName )
9110	def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
3780	def load_all_methods ( self ) : r methods = [ ] Tmins , Tmaxs = [ ] , [ ] if self . CASRN in [ '7732-18-5' , '67-56-1' , '64-17-5' ] : methods . append ( TEST_METHOD_1 ) self . TEST_METHOD_1_Tmin = 200. self . TEST_METHOD_1_Tmax = 350 self . TEST_METHOD_1_coeffs = [ 1 , .002 ] Tmins . append ( self . TEST_METHOD_1_Tmin ) Tmaxs . append ( self . TEST_METHOD_1_Tmax ) if self . CASRN in [ '67-56-1' ] : methods . append ( TEST_METHOD_2 ) self . TEST_METHOD_2_Tmin = 300. self . TEST_METHOD_2_Tmax = 400 self . TEST_METHOD_2_coeffs = [ 1 , .003 ] Tmins . append ( self . TEST_METHOD_2_Tmin ) Tmaxs . append ( self . TEST_METHOD_2_Tmax ) self . all_methods = set ( methods ) if Tmins and Tmaxs : self . Tmin = min ( Tmins ) self . Tmax = max ( Tmaxs )
13437	def _cut_range ( self , line , start , current_position ) : result = [ ] try : for j in range ( start , len ( line ) ) : index = _setup_index ( j ) try : result . append ( line [ index ] ) except IndexError : result . append ( self . invalid_pos ) finally : result . append ( self . separator ) result . append ( line [ - 1 ] ) except IndexError : pass try : int ( self . positions [ current_position + 1 ] ) result . append ( self . separator ) except ( ValueError , IndexError ) : pass return result
229	def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure
9327	def valid_content_type ( self , content_type , accept ) : accept_tokens = accept . replace ( ' ' , '' ) . split ( ';' ) content_type_tokens = content_type . replace ( ' ' , '' ) . split ( ';' ) return ( all ( elem in content_type_tokens for elem in accept_tokens ) and ( content_type_tokens [ 0 ] == 'application/vnd.oasis.taxii+json' or content_type_tokens [ 0 ] == 'application/vnd.oasis.stix+json' ) )
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
6972	def _epd_residual2 ( coeffs , times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
9335	def copy ( a ) : shared = anonymousmemmap ( a . shape , dtype = a . dtype ) shared [ : ] = a [ : ] return shared
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
3245	def get_inline_policies ( group , ** conn ) : policy_list = list_group_policies ( group [ 'GroupName' ] ) policy_documents = { } for policy in policy_list : policy_documents [ policy ] = get_group_policy_document ( group [ 'GroupName' ] , policy , ** conn ) return policy_documents
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
10693	def rgb_to_hsv ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 _min = min ( r , g , b ) _max = max ( r , g , b ) v = _max delta = _max - _min if _max == 0 : return 0 , 0 , v s = delta / _max if delta == 0 : delta = 1 if r == _max : h = 60 * ( ( ( g - b ) / delta ) % 6 ) elif g == _max : h = 60 * ( ( ( b - r ) / delta ) + 2 ) else : h = 60 * ( ( ( r - g ) / delta ) + 4 ) return round ( h , 3 ) , round ( s , 3 ) , round ( v , 3 )
7977	def _post_connect ( self ) : if not self . initiator : if "plain" in self . auth_methods or "digest" in self . auth_methods : self . set_iq_get_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage1 ) self . set_iq_set_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage2 ) elif self . registration_callback : iq = Iq ( stanza_type = "get" ) iq . set_content ( Register ( ) ) self . set_response_handlers ( iq , self . registration_form_received , self . registration_error ) self . send ( iq ) return ClientStream . _post_connect ( self )
4987	def get_course_run_id ( user , enterprise_customer , course_key ) : try : course = CourseCatalogApiServiceClient ( enterprise_customer . site ) . get_course_details ( course_key ) except ImproperlyConfigured : raise Http404 users_all_enrolled_courses = EnrollmentApiClient ( ) . get_enrolled_courses ( user . username ) users_active_course_runs = get_active_course_runs ( course , users_all_enrolled_courses ) if users_all_enrolled_courses else [ ] course_run = get_current_course_run ( course , users_active_course_runs ) if course_run : course_run_id = course_run [ 'key' ] return course_run_id else : raise Http404
6794	def shell ( self ) : r = self . local_renderer if '@' in self . genv . host_string : r . env . shell_host_string = self . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' r . env . shell_default_dir = self . genv . shell_default_dir_template r . env . shell_interactive_djshell_str = self . genv . interactive_shell_template r . run_or_local ( 'ssh -t -i {key_filename} {shell_host_string} "{shell_interactive_djshell_str}"' )
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
4653	def add_required_fees ( self , ops , asset_id = "1.3.0" ) : ws = self . blockchain . rpc fees = ws . get_required_fees ( [ i . json ( ) for i in ops ] , asset_id ) for i , d in enumerate ( ops ) : if isinstance ( fees [ i ] , list ) : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 0 ] [ "amount" ] , asset_id = fees [ i ] [ 0 ] [ "asset_id" ] ) for j , _ in enumerate ( ops [ i ] . op . data [ "proposed_ops" ] . data ) : ops [ i ] . op . data [ "proposed_ops" ] . data [ j ] . data [ "op" ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 1 ] [ j ] [ "amount" ] , asset_id = fees [ i ] [ 1 ] [ j ] [ "asset_id" ] , ) else : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ "amount" ] , asset_id = fees [ i ] [ "asset_id" ] ) return ops
757	def _allow_new_attributes ( f ) : def decorated ( self , * args , ** kw ) : if not hasattr ( self , '_canAddAttributes' ) : self . __dict__ [ '_canAddAttributes' ] = 1 else : self . _canAddAttributes += 1 assert self . _canAddAttributes >= 1 count = self . _canAddAttributes f ( self , * args , ** kw ) if hasattr ( self , '_canAddAttributes' ) : self . _canAddAttributes -= 1 else : self . _canAddAttributes = count - 1 assert self . _canAddAttributes >= 0 if self . _canAddAttributes == 0 : del self . _canAddAttributes decorated . __doc__ = f . __doc__ decorated . __name__ = f . __name__ return decorated
6432	def encode ( self , word ) : word = word . upper ( ) word = self . _delete_consecutive_repeats ( word ) i = 0 while i < len ( word ) : for match_len in range ( 4 , 1 , - 1 ) : if word [ i : i + match_len ] in self . _rules [ match_len ] : repl = self . _rules [ match_len ] [ word [ i : i + match_len ] ] word = word [ : i ] + repl + word [ i + match_len : ] i += len ( repl ) break else : i += 1 word = word [ : 1 ] + word [ 1 : ] . translate ( self . _del_trans ) return word
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
8997	def url ( self , url , encoding = "UTF-8" ) : import urllib . request with urllib . request . urlopen ( url ) as file : webpage_content = file . read ( ) webpage_content = webpage_content . decode ( encoding ) return self . string ( webpage_content )
5299	def get_context_data ( self , ** kwargs ) : data = super ( BaseCalendarMonthView , self ) . get_context_data ( ** kwargs ) year = self . get_year ( ) month = self . get_month ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) cal = Calendar ( self . get_first_of_week ( ) ) month_calendar = [ ] now = datetime . datetime . utcnow ( ) date_lists = defaultdict ( list ) multidate_objs = [ ] for obj in data [ 'object_list' ] : obj_date = self . get_start_date ( obj ) end_date_field = self . get_end_date_field ( ) if end_date_field : end_date = self . get_end_date ( obj ) if end_date and end_date != obj_date : multidate_objs . append ( { 'obj' : obj , 'range' : [ x for x in daterange ( obj_date , end_date ) ] } ) continue date_lists [ obj_date ] . append ( obj ) for week in cal . monthdatescalendar ( date . year , date . month ) : week_range = set ( daterange ( week [ 0 ] , week [ 6 ] ) ) week_events = [ ] for val in multidate_objs : intersect_length = len ( week_range . intersection ( val [ 'range' ] ) ) if intersect_length : slot = 1 width = intersect_length nowrap_previous = True nowrap_next = True if val [ 'range' ] [ 0 ] >= week [ 0 ] : slot = 1 + ( val [ 'range' ] [ 0 ] - week [ 0 ] ) . days else : nowrap_previous = False if val [ 'range' ] [ - 1 ] > week [ 6 ] : nowrap_next = False week_events . append ( { 'event' : val [ 'obj' ] , 'slot' : slot , 'width' : width , 'nowrap_previous' : nowrap_previous , 'nowrap_next' : nowrap_next , } ) week_calendar = { 'events' : week_events , 'date_list' : [ ] , } for day in week : week_calendar [ 'date_list' ] . append ( { 'day' : day , 'events' : date_lists [ day ] , 'today' : day == now . date ( ) , 'is_current_month' : day . month == date . month , } ) month_calendar . append ( week_calendar ) data [ 'calendar' ] = month_calendar data [ 'weekdays' ] = [ DAYS [ x ] for x in cal . iterweekdays ( ) ] data [ 'month' ] = date data [ 'next_month' ] = self . get_next_month ( date ) data [ 'previous_month' ] = self . get_previous_month ( date ) return data
9561	def _apply_check_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'check' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
6466	def color ( self , index ) : if self . colors == 16 : if index >= 8 : return self . csi ( 'bold' ) + self . csi ( 'setaf' , index - 8 ) else : return self . csi ( 'sgr0' ) + self . csi ( 'setaf' , index ) else : return self . csi ( 'setaf' , index )
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
237	def compute_volume_exposures ( shares_held , volumes , percentile ) : shares_held = shares_held . replace ( 0 , np . nan ) shares_longed = shares_held [ shares_held > 0 ] shares_shorted = - 1 * shares_held [ shares_held < 0 ] shares_grossed = shares_held . abs ( ) longed_frac = shares_longed . divide ( volumes ) shorted_frac = shares_shorted . divide ( volumes ) grossed_frac = shares_grossed . divide ( volumes ) longed_threshold = 100 * longed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) shorted_threshold = 100 * shorted_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) grossed_threshold = 100 * grossed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) return longed_threshold , shorted_threshold , grossed_threshold
3530	def get_identity ( context , prefix = None , identity_func = None , user = None ) : if prefix is not None : try : return context [ '%s_identity' % prefix ] except KeyError : pass try : return context [ 'analytical_identity' ] except KeyError : pass if getattr ( settings , 'ANALYTICAL_AUTO_IDENTIFY' , True ) : try : if user is None : user = get_user_from_context ( context ) if get_user_is_authenticated ( user ) : if identity_func is not None : return identity_func ( user ) else : return user . get_username ( ) except ( KeyError , AttributeError ) : pass return None
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
8897	def _fsic_queuing_calc ( fsic1 , fsic2 ) : return { instance : fsic2 . get ( instance , 0 ) for instance , counter in six . iteritems ( fsic1 ) if fsic2 . get ( instance , 0 ) < counter }
11163	def mtime ( self ) : try : return self . _stat . st_mtime except : self . _stat = self . stat ( ) return self . mtime
6985	def parallel_timebin ( lclist , binsizesec , maxobjects = None , outdir = None , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , minbinelems = 7 , nworkers = NCPUS , maxworkertasks = 1000 ) : if outdir and not os . path . exists ( outdir ) : os . mkdir ( outdir ) if maxobjects is not None : lclist = lclist [ : maxobjects ] tasks = [ ( x , binsizesec , { 'outdir' : outdir , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'minbinelems' : minbinelems } ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( timebinlc_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
2635	def parent_callback ( self , executor_fu ) : with self . _update_lock : if not executor_fu . done ( ) : raise ValueError ( "done callback called, despite future not reporting itself as done" ) if executor_fu != self . parent : if executor_fu . exception ( ) is None and not isinstance ( executor_fu . result ( ) , RemoteExceptionWrapper ) : raise ValueError ( "internal consistency error: AppFuture done callback called without an exception, but parent has been changed since then" ) try : res = executor_fu . result ( ) if isinstance ( res , RemoteExceptionWrapper ) : res . reraise ( ) super ( ) . set_result ( executor_fu . result ( ) ) except Exception as e : if executor_fu . retries_left > 0 : pass else : super ( ) . set_exception ( e )
7182	def parse_signature_type_comment ( type_comment ) : try : result = ast3 . parse ( type_comment , '<func_type>' , 'func_type' ) except SyntaxError : raise ValueError ( f"invalid function signature type comment: {type_comment!r}" ) assert isinstance ( result , ast3 . FunctionType ) if len ( result . argtypes ) == 1 : argtypes = result . argtypes [ 0 ] else : argtypes = result . argtypes return argtypes , result . returns
10309	def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
5826	def _validate_search_query ( self , returning_query ) : start_index = returning_query . from_index or 0 size = returning_query . size or 0 if start_index < 0 : raise CitrinationClientError ( "start_index cannot be negative. Please enter a value greater than or equal to zero" ) if size < 0 : raise CitrinationClientError ( "Size cannot be negative. Please enter a value greater than or equal to zero" ) if start_index + size > MAX_QUERY_DEPTH : raise CitrinationClientError ( "Citrination does not support pagination past the {0}th result. Please reduce either the from_index and/or size such that their sum is below {0}" . format ( MAX_QUERY_DEPTH ) )
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
3822	async def easter_egg ( self , easter_egg_request ) : response = hangouts_pb2 . EasterEggResponse ( ) await self . _pb_request ( 'conversations/easteregg' , easter_egg_request , response ) return response
6163	def QPSK_rx ( fc , N_symb , Rs , EsN0 = 100 , fs = 125 , lfsr_len = 10 , phase = 0 , pulse = 'src' ) : Ns = int ( np . round ( fs / Rs ) ) print ( 'Ns = ' , Ns ) print ( 'Rs = ' , fs / float ( Ns ) ) print ( 'EsN0 = ' , EsN0 , 'dB' ) print ( 'phase = ' , phase , 'degrees' ) print ( 'pulse = ' , pulse ) x , b , data = QPSK_bb ( N_symb , Ns , lfsr_len , pulse ) x = cpx_AWGN ( x , EsN0 , Ns ) n = np . arange ( len ( x ) ) xc = x * np . exp ( 1j * 2 * np . pi * fc / float ( fs ) * n ) * np . exp ( 1j * phase ) return xc , b , data
11510	def get_item_metadata ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision response = self . request ( 'midas.item.getmetadata' , parameters ) return response
10194	def default_permission_factory ( query_name , params ) : from invenio_stats import current_stats if current_stats . queries [ query_name ] . permission_factory is None : return AllowAllPermission else : return current_stats . queries [ query_name ] . permission_factory ( query_name , params )
8452	def _get_current_branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )
3658	def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : ia . stop_image_acquisition ( ) ia . _release_data_streams ( ) id_ = ia . _device . id_ if ia . device . node_map : if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
6558	def add_constraint ( self , constraint , variables = tuple ( ) ) : if isinstance ( constraint , Constraint ) : if variables and ( tuple ( variables ) != constraint . variables ) : raise ValueError ( "mismatched variables and Constraint" ) elif isinstance ( constraint , Callable ) : constraint = Constraint . from_func ( constraint , variables , self . vartype ) elif isinstance ( constraint , Iterable ) : constraint = Constraint . from_configurations ( constraint , variables , self . vartype ) else : raise TypeError ( "Unknown constraint type given" ) self . constraints . append ( constraint ) for v in constraint . variables : self . variables [ v ] . append ( constraint )
12581	def setup_logging ( log_config_file = op . join ( op . dirname ( __file__ ) , 'logger.yml' ) , log_default_level = LOG_LEVEL , env_key = MODULE_NAME . upper ( ) + '_LOG_CFG' ) : path = log_config_file value = os . getenv ( env_key , None ) if value : path = value if op . exists ( path ) : log_cfg = yaml . load ( read ( path ) . format ( MODULE_NAME ) ) logging . config . dictConfig ( log_cfg ) else : logging . basicConfig ( level = log_default_level ) log = logging . getLogger ( __name__ ) log . debug ( 'Start logging.' )
9524	def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
1214	def _int_to_pos ( self , flat_position ) : return flat_position % self . env . action_space . screen_shape [ 0 ] , flat_position % self . env . action_space . screen_shape [ 1 ]
8536	def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last_timestamp
9736	def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
8544	def get_datacenter ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
13005	def bruteforce ( users , domain , password , host ) : cs = CredentialSearch ( use_pipe = False ) print_notification ( "Connecting to {}" . format ( host ) ) s = Server ( host ) c = Connection ( s ) for user in users : if c . rebind ( user = "{}\\{}" . format ( domain , user . username ) , password = password , authentication = NTLM ) : print_success ( 'Success for: {}:{}' . format ( user . username , password ) ) credential = cs . find_object ( user . username , password , domain = domain , host_ip = host ) if not credential : credential = Credential ( username = user . username , secret = password , domain = domain , host_ip = host , type = "plaintext" , port = 389 ) credential . add_tag ( tag ) credential . save ( ) user . add_tag ( tag ) user . save ( ) else : print_error ( "Fail for: {}:{}" . format ( user . username , password ) )
7917	def are_domains_equal ( domain1 , domain2 ) : domain1 = domain1 . encode ( "idna" ) domain2 = domain2 . encode ( "idna" ) return domain1 . lower ( ) == domain2 . lower ( )
3123	def _check_audience ( payload_dict , audience ) : if audience is None : return audience_in_payload = payload_dict . get ( 'aud' ) if audience_in_payload is None : raise AppIdentityError ( 'No aud field in token: {0}' . format ( payload_dict ) ) if audience_in_payload != audience : raise AppIdentityError ( 'Wrong recipient, {0} != {1}: {2}' . format ( audience_in_payload , audience , payload_dict ) )
8213	def export_svg ( self ) : d = "" if len ( self . _points ) > 0 : d += "M " + str ( self . _points [ 0 ] . x ) + " " + str ( self . _points [ 0 ] . y ) + " " for pt in self . _points : if pt . cmd == MOVETO : d += "M " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == LINETO : d += "L " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == CURVETO : d += "C " d += str ( pt . ctrl1 . x ) + " " + str ( pt . ctrl1 . y ) + " " d += str ( pt . ctrl2 . x ) + " " + str ( pt . ctrl2 . y ) + " " d += str ( pt . x ) + " " + str ( pt . y ) + " " c = "rgb(" c += str ( int ( self . path_color . r * 255 ) ) + "," c += str ( int ( self . path_color . g * 255 ) ) + "," c += str ( int ( self . path_color . b * 255 ) ) + ")" s = '<?xml version="1.0"?>\n' s += '<svg width="' + str ( _ctx . WIDTH ) + 'pt" height="' + str ( _ctx . HEIGHT ) + 'pt">\n' s += '<g>\n' s += '<path d="' + d + '" fill="none" stroke="' + c + '" stroke-width="' + str ( self . strokewidth ) + '" />\n' s += '</g>\n' s += '</svg>\n' f = open ( self . file + ".svg" , "w" ) f . write ( s ) f . close ( )
12025	def remove ( self , line_data , root_type = None ) : roots = [ ld for ld in self . ancestors ( line_data ) if ( root_type and ld [ 'line_type' ] == root_type ) or ( not root_type and not ld [ 'parents' ] ) ] or [ line_data ] for root in roots : root [ 'line_status' ] = 'removed' root_descendants = self . descendants ( root ) for root_descendant in root_descendants : root_descendant [ 'line_status' ] = 'removed' root_ancestors = self . ancestors ( root ) for root_ancestor in root_ancestors : if len ( [ ld for ld in root_ancestor [ 'children' ] if ld [ 'line_status' ] != 'removed' ] ) == 0 : root_ancestor [ 'line_status' ] = 'removed'
10073	def build_deposit_schema ( self , record ) : schema_path = current_jsonschemas . url_to_path ( record [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path : return current_jsonschemas . path_to_url ( schema_prefix + schema_path )
13641	def send ( self , use_open_peers = True , queue = True , ** kw ) : if not use_open_peers : ip = kw . get ( 'ip' ) port = kw . get ( 'port' ) peer = 'http://{}:{}' . format ( ip , port ) res = arky . rest . POST . peer . transactions ( peer = peer , transactions = [ self . tx . tx ] ) else : res = arky . core . sendPayload ( self . tx . tx ) if self . tx . success != '0.0%' : self . tx . error = None self . tx . success = True else : self . tx . error = res [ 'messages' ] self . tx . success = False self . tx . tries += 1 self . tx . res = res if queue : self . tx . send = True self . __save ( ) return res
7360	def _check_hla_alleles ( alleles , valid_alleles = None ) : require_iterable_of ( alleles , string_types , "HLA alleles" ) alleles = { normalize_allele_name ( allele . strip ( ) . upper ( ) ) for allele in alleles } if valid_alleles : missing_alleles = [ allele for allele in alleles if allele not in valid_alleles ] if len ( missing_alleles ) > 0 : raise UnsupportedAllele ( "Unsupported HLA alleles: %s" % missing_alleles ) return list ( alleles )
12011	def extractFile ( self , filename ) : files = [ x for x in self . tableOfContents if x [ 'filename' ] == filename ] if len ( files ) == 0 : raise FileNotFoundException ( ) fileRecord = files [ 0 ] metaheadroom = 1024 request = urllib2 . Request ( self . zipURI ) start = fileRecord [ 'filestart' ] end = fileRecord [ 'filestart' ] + fileRecord [ 'compressedsize' ] + metaheadroom request . headers [ 'Range' ] = "bytes=%s-%s" % ( start , end ) handle = urllib2 . urlopen ( request ) return_range = handle . headers . get ( 'Content-Range' ) if return_range != "bytes %d-%d/%s" % ( start , end , self . filesize ) : raise Exception ( "Ranged requests are not supported for this URI" ) filedata = handle . read ( ) zip_n = unpack ( "H" , filedata [ 26 : 28 ] ) [ 0 ] zip_m = unpack ( "H" , filedata [ 28 : 30 ] ) [ 0 ] has_data_descriptor = bool ( unpack ( "H" , filedata [ 6 : 8 ] ) [ 0 ] & 8 ) comp_size = unpack ( "I" , filedata [ 18 : 22 ] ) [ 0 ] if comp_size == 0 and has_data_descriptor : comp_size = fileRecord [ 'compressedsize' ] elif comp_size != fileRecord [ 'compressedsize' ] : raise Exception ( "Something went wrong. Directory and file header disagree of compressed file size" ) raw_zip_data = filedata [ 30 + zip_n + zip_m : 30 + zip_n + zip_m + comp_size ] uncompressed_data = "" compression_method = unpack ( "H" , filedata [ 8 : 10 ] ) [ 0 ] if compression_method == 0 : return raw_zip_data dec = zlib . decompressobj ( - zlib . MAX_WBITS ) for chunk in raw_zip_data : rv = dec . decompress ( chunk ) if rv : uncompressed_data = uncompressed_data + rv return uncompressed_data
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
7781	def rfc2426 ( self ) : ret = "begin:VCARD\r\n" ret += "version:3.0\r\n" for _unused , value in self . content . items ( ) : if value is None : continue if type ( value ) is list : for v in value : ret += v . rfc2426 ( ) else : v = value . rfc2426 ( ) ret += v return ret + "end:VCARD\r\n"
9307	def get_canonical_request ( self , req , cano_headers , signed_headers ) : url = urlparse ( req . url ) path = self . amz_cano_path ( url . path ) split = req . url . split ( '?' , 1 ) qs = split [ 1 ] if len ( split ) == 2 else '' qs = self . amz_cano_querystring ( qs ) payload_hash = req . headers [ 'x-amz-content-sha256' ] req_parts = [ req . method . upper ( ) , path , qs , cano_headers , signed_headers , payload_hash ] cano_req = '\n' . join ( req_parts ) return cano_req
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
7097	def on_map_fragment_created ( self , obj_id ) : self . fragment = MapFragment ( __id__ = obj_id ) self . map . onMapReady . connect ( self . on_map_ready ) self . fragment . getMapAsync ( self . map . getId ( ) ) context = self . get_context ( ) def on_transaction ( id ) : trans = FragmentTransaction ( __id__ = id ) trans . add ( self . widget . getId ( ) , self . fragment ) trans . commit ( ) def on_fragment_manager ( id ) : fm = FragmentManager ( __id__ = id ) fm . beginTransaction ( ) . then ( on_transaction ) context . widget . getSupportFragmentManager ( ) . then ( on_fragment_manager )
11129	def stats ( cls , traces ) : data = { } stats = { } for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
6857	def create_user ( name , password , host = 'localhost' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , ** kwargs ) puts ( "Created MySQL user '%s'." % name )
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
11467	def mkdir ( self , folder ) : current_folder = self . _ftp . pwd ( ) folders = folder . split ( '/' ) for fld in folders : try : self . cd ( fld ) except error_perm : self . _ftp . mkd ( fld ) self . cd ( fld ) self . cd ( current_folder )
11805	def record_conflict ( self , assignment , var , val , delta ) : "Record conflicts caused by addition or deletion of a Queen." n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta
6965	def get ( self ) : if 'reviewed' not in self . currentproject : self . currentproject [ 'reviewed' ] = { } self . write ( self . currentproject )
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) return self . current_time_format
10424	def infer_missing_two_way_edges ( graph ) : for u , v , k , d in graph . edges ( data = True , keys = True ) : if d [ RELATION ] in TWO_WAY_RELATIONS : infer_missing_backwards_edge ( graph , u , v , k )
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
818	def grow ( self , rows , cols ) : if not self . hist_ : self . hist_ = SparseMatrix ( rows , cols ) self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) self . colSums_ = numpy . zeros ( cols , dtype = dtype ) self . hack_ = None else : oldRows = self . hist_ . nRows ( ) oldCols = self . hist_ . nCols ( ) nextRows = max ( oldRows , rows ) nextCols = max ( oldCols , cols ) if ( oldRows < nextRows ) or ( oldCols < nextCols ) : self . hist_ . resize ( nextRows , nextCols ) if oldRows < nextRows : oldSums = self . rowSums_ self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None if oldCols < nextCols : oldSums = self . colSums_ self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) self . colSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None
2786	def create_from_snapshot ( self , * args , ** kwargs ) : data = self . get_data ( 'volumes/' , type = POST , params = { 'name' : self . name , 'snapshot_id' : self . snapshot_id , 'region' : self . region , 'size_gigabytes' : self . size_gigabytes , 'description' : self . description , 'filesystem_type' : self . filesystem_type , 'filesystem_label' : self . filesystem_label } ) if data : self . id = data [ 'volume' ] [ 'id' ] self . created_at = data [ 'volume' ] [ 'created_at' ] return self
13079	def render ( self , template , ** kwargs ) : kwargs [ "cache_key" ] = "%s" % kwargs [ "url" ] . values ( ) kwargs [ "lang" ] = self . get_locale ( ) kwargs [ "assets" ] = self . assets kwargs [ "main_collections" ] = self . main_collections ( kwargs [ "lang" ] ) kwargs [ "cache_active" ] = self . cache is not None kwargs [ "cache_time" ] = 0 kwargs [ "cache_key" ] , kwargs [ "cache_key_i18n" ] = self . make_cache_keys ( request . endpoint , kwargs [ "url" ] ) kwargs [ "template" ] = template for plugin in self . __plugins_render_views__ : kwargs . update ( plugin . render ( ** kwargs ) ) return render_template ( kwargs [ "template" ] , ** kwargs )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
9171	def declare_browsable_routes ( config ) : config . add_notfound_view ( default_exceptionresponse_view , append_slash = True ) add_route = config . add_route add_route ( 'admin-index' , '/a/' ) add_route ( 'admin-moderation' , '/a/moderation/' ) add_route ( 'admin-api-keys' , '/a/api-keys/' ) add_route ( 'admin-add-site-messages' , '/a/site-messages/' , request_method = 'GET' ) add_route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request_method = 'POST' ) add_route ( 'admin-delete-site-messages' , '/a/site-messages/' , request_method = 'DELETE' ) add_route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request_method = 'GET' ) add_route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request_method = 'POST' ) add_route ( 'admin-content-status' , '/a/content-status/' ) add_route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add_route ( 'admin-print-style' , '/a/print-style/' ) add_route ( 'admin-print-style-single' , '/a/print-style/{style}' )
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
5338	def __upload_title ( self , kibiter_major ) : if kibiter_major == "6" : resource = ".kibana/doc/projectname" data = { "projectname" : { "name" : self . project_name } } mapping_resource = ".kibana/_mapping/doc" mapping = { "dynamic" : "true" } url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , resource ) mapping_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , mapping_resource ) logger . debug ( "Adding mapping for dashboard title" ) res = self . grimoire_con . put ( mapping_url , data = json . dumps ( mapping ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create mapping for dashboard title." ) logger . error ( res . json ( ) ) logger . debug ( "Uploading dashboard title" ) res = self . grimoire_con . post ( url , data = json . dumps ( data ) , headers = ES6_HEADER ) try : res . raise_for_status ( ) except requests . exceptions . HTTPError : logger . error ( "Couldn't create dashboard title." ) logger . error ( res . json ( ) )
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
6544	def terminate ( self ) : if not self . is_terminated : log . debug ( "terminal client terminated" ) try : self . exec_command ( b"Quit" ) except BrokenPipeError : pass except socket . error as e : if e . errno != errno . ECONNRESET : raise self . app . close ( ) self . is_terminated = True
10002	def add_path ( self , nodes , ** attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , ** attr ) else : return nx . add_path ( self , nodes , ** attr )
9347	def argsort ( data , out = None , chunksize = None , baseargsort = None , argmerge = None , np = None ) : if baseargsort is None : baseargsort = lambda x : x . argsort ( ) if argmerge is None : argmerge = default_argmerge if chunksize is None : chunksize = 1024 * 1024 * 16 if out is None : arg1 = numpy . empty ( len ( data ) , dtype = 'intp' ) out = arg1 else : assert out . dtype == numpy . dtype ( 'intp' ) assert len ( out ) == len ( data ) arg1 = out if np is None : np = sharedmem . cpu_count ( ) if np <= 1 or len ( data ) < chunksize : out [ : ] = baseargsort ( data ) return out CHK = [ slice ( i , i + chunksize ) for i in range ( 0 , len ( data ) , chunksize ) ] DUMMY = slice ( len ( data ) , len ( data ) ) if len ( CHK ) % 2 : CHK . append ( DUMMY ) with sharedmem . TPool ( ) as pool : def work ( i ) : C = CHK [ i ] start , stop , step = C . indices ( len ( data ) ) arg1 [ C ] = baseargsort ( data [ C ] ) arg1 [ C ] += start pool . map ( work , range ( len ( CHK ) ) ) arg2 = numpy . empty_like ( arg1 ) flip = 0 while len ( CHK ) > 1 : with sharedmem . TPool ( ) as pool : def work ( i ) : C1 = CHK [ i ] C2 = CHK [ i + 1 ] start1 , stop1 , step1 = C1 . indices ( len ( data ) ) start2 , stop2 , step2 = C2 . indices ( len ( data ) ) assert start2 == stop1 argmerge ( data , arg1 [ C1 ] , arg1 [ C2 ] , arg2 [ start1 : stop2 ] ) return slice ( start1 , stop2 ) CHK = pool . map ( work , range ( 0 , len ( CHK ) , 2 ) ) arg1 , arg2 = arg2 , arg1 flip = flip + 1 if len ( CHK ) == 1 : break if len ( CHK ) % 2 : CHK . append ( DUMMY ) if flip % 2 != 0 : out [ : ] = arg1 return out
889	def _leastUsedCell ( cls , random , cells , connections ) : leastUsedCells = [ ] minNumSegments = float ( "inf" ) for cell in cells : numSegments = connections . numSegments ( cell ) if numSegments < minNumSegments : minNumSegments = numSegments leastUsedCells = [ ] if numSegments == minNumSegments : leastUsedCells . append ( cell ) i = random . getUInt32 ( len ( leastUsedCells ) ) return leastUsedCells [ i ]
5371	def load_file ( file_path , credentials = None ) : if file_path . startswith ( 'gs://' ) : return _load_file_from_gcs ( file_path , credentials ) else : return open ( file_path , 'r' )
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
10369	def summarize_edge_filter ( graph : BELGraph , edge_predicates : EdgePredicates ) -> None : passed = count_passed_edge_filter ( graph , edge_predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number_of_edges ( ) , ( ', ' . join ( edge_filter . __name__ for edge_filter in edge_predicates ) if isinstance ( edge_predicates , Iterable ) else edge_predicates . __name__ ) ) )
241	def create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , trade_daily_vol_limit = 0.05 , last_n_days = utils . APPROX_BDAYS_PER_MONTH * 6 , days_to_liquidate_limit = 1 , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) print ( "Max days to liquidation is computed for each traded name " "assuming a 20% limit on daily bar consumption \n" "and trailing 5 day mean volume as the available bar volume.\n\n" "Tickers with >1 day liquidation time at a" " constant $1m capital base:" ) max_days_by_ticker = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 ) max_days_by_ticker . index = ( max_days_by_ticker . index . map ( utils . format_asset ) ) print ( "Whole backtest:" ) utils . print_table ( max_days_by_ticker [ max_days_by_ticker . days_to_liquidate > days_to_liquidate_limit ] ) max_days_by_ticker_lnd = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 , last_n_days = last_n_days ) max_days_by_ticker_lnd . index = ( max_days_by_ticker_lnd . index . map ( utils . format_asset ) ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( max_days_by_ticker_lnd [ max_days_by_ticker_lnd . days_to_liquidate > 1 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data ) llt . index = llt . index . map ( utils . format_asset ) print ( 'Tickers with daily transactions consuming >{}% of daily bar \n' 'all backtest:' . format ( trade_daily_vol_limit * 100 ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data , last_n_days = last_n_days ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) bt_starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns . iloc [ 0 ] ) fig , ax_capacity_sweep = plt . subplots ( figsize = ( 14 , 6 ) ) plotting . plot_capacity_sweep ( returns , transactions , market_data , bt_starting_capital , min_pv = 100000 , max_pv = 300000000 , step_size = 1000000 , ax = ax_capacity_sweep )
2899	def get_tasks_from_spec_name ( self , name ) : return [ task for task in self . get_tasks ( ) if task . task_spec . name == name ]
5589	def hillshade ( elevation , tile , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : azimuth = float ( azimuth ) altitude = float ( altitude ) z = float ( z ) scale = float ( scale ) xres = tile . tile . pixel_x_size yres = - tile . tile . pixel_y_size slope , aspect = calculate_slope_aspect ( elevation , xres , yres , z = z , scale = scale ) deg2rad = math . pi / 180.0 shaded = np . sin ( altitude * deg2rad ) * np . sin ( slope ) + np . cos ( altitude * deg2rad ) * np . cos ( slope ) * np . cos ( ( azimuth - 90.0 ) * deg2rad - aspect ) shaded = ( ( ( shaded + 1.0 ) / 2 ) * - 255.0 ) . astype ( "uint8" ) return ma . masked_array ( data = np . pad ( shaded , 1 , mode = 'edge' ) , mask = elevation . mask )
4818	def parse_lms_api_datetime ( datetime_string , datetime_format = LMS_API_DATETIME_FORMAT ) : if isinstance ( datetime_string , datetime . datetime ) : date_time = datetime_string else : try : date_time = datetime . datetime . strptime ( datetime_string , datetime_format ) except ValueError : date_time = datetime . datetime . strptime ( datetime_string , LMS_API_DATETIME_FORMAT_WITHOUT_TIMEZONE ) if date_time . tzinfo is None : date_time = date_time . replace ( tzinfo = timezone . utc ) return date_time
11790	def revise ( csp , Xi , Xj , removals ) : "Return true if we remove a value." revised = False for x in csp . curr_domains [ Xi ] [ : ] : if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr_domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised
361	def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( "[*] creates %s ..." % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( "[!] %s exists ..." % path ) return True
2648	def make_rundir ( path ) : try : if not os . path . exists ( path ) : os . makedirs ( path ) prev_rundirs = glob ( os . path . join ( path , "[0-9]*" ) ) current_rundir = os . path . join ( path , '000' ) if prev_rundirs : x = sorted ( [ int ( os . path . basename ( x ) ) for x in prev_rundirs ] ) [ - 1 ] current_rundir = os . path . join ( path , '{0:03}' . format ( x + 1 ) ) os . makedirs ( current_rundir ) logger . debug ( "Parsl run initializing in rundir: {0}" . format ( current_rundir ) ) return os . path . abspath ( current_rundir ) except Exception as e : logger . error ( "Failed to create a run directory" ) logger . error ( "Error: {0}" . format ( e ) ) raise
13675	def add_path_object ( self , * args ) : for obj in args : obj . bundle = self self . files . append ( obj )
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
6806	def init_raspbian_vm ( self ) : r = self . local_renderer r . comment ( 'Installing system packages.' ) r . sudo ( 'add-apt-repository ppa:linaro-maintainers/tools' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install libsdl-dev qemu-system' ) r . comment ( 'Download image.' ) r . local ( 'wget https://downloads.raspberrypi.org/raspbian_lite_latest' ) r . local ( 'unzip raspbian_lite_latest.zip' ) r . comment ( 'Find start of the Linux ext4 partition.' ) r . local ( "parted -s 2016-03-18-raspbian-jessie-lite.img unit B print | " "awk '/^Number/{{p=1;next}}; p{{gsub(/[^[:digit:]]/, " ", $2); print $2}}' | sed -n 2p" , assign_to = 'START' ) r . local ( 'mkdir -p {raspbian_mount_point}' ) r . sudo ( 'mount -v -o offset=$START -t ext4 {raspbian_image} $MNT' ) r . comment ( 'Comment out everything in ld.so.preload' ) r . local ( "sed -i 's/^/#/g' {raspbian_mount_point}/etc/ld.so.preload" ) r . comment ( 'Comment out entries containing /dev/mmcblk in fstab.' ) r . local ( "sed -i '/mmcblk/ s?^?#?' /etc/fstab" ) r . sudo ( 'umount {raspbian_mount_point}' ) r . comment ( 'Download kernel.' ) r . local ( 'wget https://github.com/dhruvvyas90/qemu-rpi-kernel/blob/master/{raspbian_kernel}?raw=true' ) r . local ( 'mv {raspbian_kernel} {libvirt_images_dir}' ) r . comment ( 'Creating libvirt machine.' ) r . local ( 'virsh define libvirt-raspbian.xml' ) r . comment ( 'You should now be able to boot the VM by running:' ) r . comment ( '' ) r . comment ( ' qemu-system-arm -kernel {libvirt_boot_dir}/{raspbian_kernel} ' '-cpu arm1176 -m 256 -M versatilepb -serial stdio -append "root=/dev/sda2 rootfstype=ext4 rw" ' '-hda {libvirt_images_dir}/{raspbian_image}' ) r . comment ( '' ) r . comment ( 'Or by running virt-manager.' )
6969	def _old_epd_diffmags ( coeff , fsv , fdv , fkv , xcc , ycc , bgv , bge , mag ) : return - ( coeff [ 0 ] * fsv ** 2. + coeff [ 1 ] * fsv + coeff [ 2 ] * fdv ** 2. + coeff [ 3 ] * fdv + coeff [ 4 ] * fkv ** 2. + coeff [ 5 ] * fkv + coeff [ 6 ] + coeff [ 7 ] * fsv * fdv + coeff [ 8 ] * fsv * fkv + coeff [ 9 ] * fdv * fkv + coeff [ 10 ] * np . sin ( 2 * np . pi * xcc ) + coeff [ 11 ] * np . cos ( 2 * np . pi * xcc ) + coeff [ 12 ] * np . sin ( 2 * np . pi * ycc ) + coeff [ 13 ] * np . cos ( 2 * np . pi * ycc ) + coeff [ 14 ] * np . sin ( 4 * np . pi * xcc ) + coeff [ 15 ] * np . cos ( 4 * np . pi * xcc ) + coeff [ 16 ] * np . sin ( 4 * np . pi * ycc ) + coeff [ 17 ] * np . cos ( 4 * np . pi * ycc ) + coeff [ 18 ] * bgv + coeff [ 19 ] * bge - mag )
13392	def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
231	def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures
9948	def new_space_from_excel ( self , book , range_ , sheet = None , name = None , names_row = None , param_cols = None , space_param_order = None , cells_param_order = None , transpose = False , names_col = None , param_rows = None , ) : space = self . _impl . new_space_from_excel ( book , range_ , sheet , name , names_row , param_cols , space_param_order , cells_param_order , transpose , names_col , param_rows , ) return get_interfaces ( space )
11473	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : block_parser = TextBlockParser ( ) lines = text . splitlines ( ) parsed = [ ] for line in lines : if STANDALONE_URL_RE . match ( line ) : user_url = line . strip ( ) try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) except OEmbedException : if urlize_all_links : line = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) line = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) else : line = block_parser . parse ( line , maxwidth , maxheight , 'inline' , context , urlize_all_links ) parsed . append ( line ) return mark_safe ( '\n' . join ( parsed ) )
12007	def _get_algorithm_info ( self , algorithm_info ) : if algorithm_info [ 'algorithm' ] not in self . ALGORITHMS : raise Exception ( 'Algorithm not supported: %s' % algorithm_info [ 'algorithm' ] ) algorithm = self . ALGORITHMS [ algorithm_info [ 'algorithm' ] ] algorithm_info . update ( algorithm ) return algorithm_info
12454	def create_env ( env , args , recreate = False , ignore_activated = False , quiet = False ) : cmd = None result = True inside_env = hasattr ( sys , 'real_prefix' ) or os . environ . get ( 'VIRTUAL_ENV' ) env_exists = os . path . isdir ( env ) if not quiet : print_message ( '== Step 1. Create virtual environment ==' ) if ( recreate or ( not inside_env and not env_exists ) ) or ( ignore_activated and not env_exists ) : cmd = ( 'virtualenv' , ) + args + ( env , ) if not cmd and not quiet : if inside_env : message = 'Working inside of virtual environment, done...' else : message = 'Virtual environment {0!r} already created, done...' print_message ( message . format ( env ) ) if cmd : with disable_error_handler ( ) : result = not run_cmd ( cmd , echo = not quiet ) if not quiet : print_message ( ) return result
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
1248	def is_action_available ( self , action ) : temp_state = np . rot90 ( self . _state , action ) return self . _is_action_available_left ( temp_state )
3527	def piwik ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PiwikNode ( )
27	def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
4925	def get_missing_params_message ( self , parameter_state ) : params = ', ' . join ( name for name , present in parameter_state if not present ) return self . MISSING_REQUIRED_PARAMS_MSG . format ( params )
7790	def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) self . _items_list . sort ( ) if item . state == 'purged' : self . _purged += 1 if self . _purged > 0.25 * self . max_items : self . purge_items ( ) return state finally : self . _lock . release ( )
4581	def to_color ( c ) : if isinstance ( c , numbers . Number ) : return c , c , c if not c : raise ValueError ( 'Cannot create color from empty "%s"' % c ) if isinstance ( c , str ) : return name_to_color ( c ) if isinstance ( c , list ) : c = tuple ( c ) if isinstance ( c , tuple ) : if len ( c ) > 3 : return c [ : 3 ] while len ( c ) < 3 : c += ( c [ - 1 ] , ) return c raise ValueError ( 'Cannot create color from "%s"' % c )
11951	def _import_config ( config_file ) : jocker_lgr . debug ( 'config file is: {0}' . format ( config_file ) ) try : jocker_lgr . debug ( 'importing config...' ) with open ( config_file , 'r' ) as c : return yaml . safe_load ( c . read ( ) ) except IOError as ex : jocker_lgr . error ( str ( ex ) ) raise RuntimeError ( 'cannot access config file' ) except yaml . parser . ParserError as ex : jocker_lgr . error ( 'invalid yaml file: {0}' . format ( ex ) ) raise RuntimeError ( 'invalid yaml file' )
5690	def read_data_as_dataframe ( self , travel_impedance_measure , from_stop_I = None , to_stop_I = None , statistic = None ) : to_select = [ ] where_clauses = [ ] to_select . append ( "from_stop_I" ) to_select . append ( "to_stop_I" ) if from_stop_I is not None : where_clauses . append ( "from_stop_I=" + str ( int ( from_stop_I ) ) ) if to_stop_I is not None : where_clauses . append ( "to_stop_I=" + str ( int ( to_stop_I ) ) ) where_clause = "" if len ( where_clauses ) > 0 : where_clause = " WHERE " + " AND " . join ( where_clauses ) if not statistic : to_select . extend ( [ "min" , "mean" , "median" , "max" ] ) else : to_select . append ( statistic ) to_select_clause = "," . join ( to_select ) if not to_select_clause : to_select_clause = "*" sql = "SELECT " + to_select_clause + " FROM " + travel_impedance_measure + where_clause + ";" df = pd . read_sql ( sql , self . conn ) return df
11052	def sync ( self ) : self . log . info ( 'Starting a sync...' ) def log_success ( result ) : self . log . info ( 'Sync completed successfully' ) return result def log_failure ( failure ) : self . log . failure ( 'Sync failed' , failure , LogLevel . error ) return failure return ( self . marathon_client . get_apps ( ) . addCallback ( self . _apps_acme_domains ) . addCallback ( self . _filter_new_domains ) . addCallback ( self . _issue_certs ) . addCallbacks ( log_success , log_failure ) )
7332	async def run_tasks ( self ) : tasks = self . get_tasks ( ) self . _gathered_tasks = asyncio . gather ( * tasks , loop = self . loop ) try : await self . _gathered_tasks except CancelledError : pass
8490	def start_watching ( self ) : if self . watcher and self . watcher . is_alive ( ) : return self . watcher = Watcher ( ) self . watcher . start ( )
2821	def convert_dropout ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting dropout ...' ) if names == 'short' : tf_name = 'DO' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) dropout = keras . layers . Dropout ( rate = params [ 'ratio' ] , name = tf_name ) layers [ scope_name ] = dropout ( layers [ inputs [ 0 ] ] )
13721	def query ( self , wql ) : try : self . __wql = [ 'wmic' , '-U' , self . args . domain + '\\' + self . args . user + '%' + self . args . password , '//' + self . args . host , '--namespace' , self . args . namespace , '--delimiter' , self . args . delimiter , wql ] self . logger . debug ( "wql: {}" . format ( self . __wql ) ) self . __output = subprocess . check_output ( self . __wql ) self . logger . debug ( "output: {}" . format ( self . __output ) ) self . logger . debug ( "wmi connect succeed." ) self . __wmi_output = self . __output . splitlines ( ) [ 1 : ] self . logger . debug ( "wmi_output: {}" . format ( self . __wmi_output ) ) self . __csv_header = csv . DictReader ( self . __wmi_output , delimiter = '|' ) self . logger . debug ( "csv_header: {}" . format ( self . __csv_header ) ) return list ( self . __csv_header ) except subprocess . CalledProcessError as e : self . unknown ( "Connect by wmi and run wql error: %s" % e )
3215	def get_vpc_flow_logs ( vpc , ** conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
6616	def receive ( self ) : pkgidx_result_pairs = self . receive_all ( ) if pkgidx_result_pairs is None : return results = [ r for _ , r in pkgidx_result_pairs ] return results
6757	def reboot_or_dryrun ( self , * args , ** kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , ** kwargs )
4135	def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
5181	def _url ( self , endpoint , path = None ) : log . debug ( '_url called with endpoint: {0} and path: {1}' . format ( endpoint , path ) ) try : endpoint = ENDPOINTS [ endpoint ] except KeyError : raise APIError url = '{base_url}/{endpoint}' . format ( base_url = self . base_url , endpoint = endpoint , ) if path is not None : url = '{0}/{1}' . format ( url , quote ( path ) ) return url
9924	def create ( self , * args , ** kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , ** kwargs ) if is_primary : email . set_primary ( ) return email
2310	def predict_proba ( self , a , b , ** kwargs ) : return self . b_fit_score ( b , a ) - self . b_fit_score ( a , b )
7077	def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nworkers = None ) : pfpkldir = os . path . join ( simbasedir , 'periodfinding' ) if not os . path . exists ( pfpkldir ) : LOGERROR ( 'no "periodfinding" subdirectory in %s, can\'t continue' % simbasedir ) return None pfpkl_list = glob . glob ( os . path . join ( pfpkldir , '*periodfinding*pkl*' ) ) if len ( pfpkl_list ) > 0 : if liststartind : pfpkl_list = pfpkl_list [ liststartind : ] if listmaxobjects : pfpkl_list = pfpkl_list [ : listmaxobjects ] tasks = [ ( x , simbasedir , period_tolerance ) for x in pfpkl_list ] pool = mp . Pool ( nworkers ) results = pool . map ( periodrec_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { x [ 'objectid' ] : x for x in results if x is not None } actual_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and x [ 'actual_vartype' ] in PERIODIC_VARTYPES ) ] , dtype = np . unicode_ ) recovered_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'actual' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_twice_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'twice' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_half_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'half' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) all_objectids = [ x [ 'objectid' ] for x in results ] outdict = { 'simbasedir' : os . path . abspath ( simbasedir ) , 'objectids' : all_objectids , 'period_tolerance' : period_tolerance , 'actual_periodicvars' : actual_periodicvars , 'recovered_periodicvars' : recovered_periodicvars , 'alias_twice_periodicvars' : alias_twice_periodicvars , 'alias_half_periodicvars' : alias_half_periodicvars , 'details' : resdict } outfile = os . path . join ( simbasedir , 'periodicvar-recovery.pkl' ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict else : LOGERROR ( 'no periodfinding result pickles found in %s, can\'t continue' % pfpkldir ) return None
11674	def copy ( self , stack = False , copy_meta = False , memo = None ) : if self . stacked : fs = deepcopy ( self . stacked_features , memo ) n_pts = self . n_pts . copy ( ) elif stack : fs = np . vstack ( self . features ) n_pts = self . n_pts . copy ( ) else : fs = deepcopy ( self . features , memo ) n_pts = None meta = deepcopy ( self . meta , memo ) if copy_meta else self . meta return Features ( fs , n_pts , copy = False , ** meta )
11852	def parse ( self , words , S = 'S' ) : self . chart = [ [ ] for i in range ( len ( words ) + 1 ) ] self . add_edge ( [ 0 , 0 , 'S_' , [ ] , [ S ] ] ) for i in range ( len ( words ) ) : self . scanner ( i , words [ i ] ) return self . chart
13774	def format ( self , record ) : record_fields = record . __dict__ . copy ( ) self . _set_exc_info ( record_fields ) event_name = 'default' if record_fields . get ( 'event_name' ) : event_name = record_fields . pop ( 'event_name' ) log_level = 'INFO' if record_fields . get ( 'log_level' ) : log_level = record_fields . pop ( 'log_level' ) [ record_fields . pop ( k ) for k in record_fields . keys ( ) if k not in self . fields ] defaults = self . defaults . copy ( ) fields = self . fields . copy ( ) fields . update ( record_fields ) filtered_fields = { } for k , v in fields . iteritems ( ) : if v is not None : filtered_fields [ k ] = v defaults . update ( { 'event_timestamp' : self . _get_now ( ) , 'event_name' : event_name , 'log_level' : log_level , 'fields' : filtered_fields } ) return json . dumps ( defaults , default = self . json_default )
788	def jobInfoWithModels ( self , jobID ) : combinedResults = None with ConnectionFactory . get ( ) as conn : query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) )
8162	def publish_event ( event_t , data = None , extra_channels = None , wait = None ) : event = Event ( event_t , data ) pubsub . publish ( "shoebot" , event ) for channel_name in extra_channels or [ ] : pubsub . publish ( channel_name , event ) if wait is not None : channel = pubsub . subscribe ( wait ) channel . listen ( wait )
7826	def payload_element_name ( element_name ) : def decorator ( klass ) : from . stanzapayload import STANZA_PAYLOAD_CLASSES from . stanzapayload import STANZA_PAYLOAD_ELEMENTS if hasattr ( klass , "_pyxmpp_payload_element_name" ) : klass . _pyxmpp_payload_element_name . append ( element_name ) else : klass . _pyxmpp_payload_element_name = [ element_name ] if element_name in STANZA_PAYLOAD_CLASSES : logger = logging . getLogger ( 'pyxmpp.payload_element_name' ) logger . warning ( "Overriding payload class for {0!r}" . format ( element_name ) ) STANZA_PAYLOAD_CLASSES [ element_name ] = klass STANZA_PAYLOAD_ELEMENTS [ klass ] . append ( element_name ) return klass return decorator
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
11723	def init_app ( self , app , ** kwargs ) : self . init_config ( app ) self . limiter = Limiter ( app , key_func = get_ipaddr ) if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , ** app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : request_id = request . headers . get ( requestid_header ) if request_id : g . request_id = request_id [ : 200 ] try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) app . extensions [ 'invenio-app' ] = self
2598	def can_sequence ( obj ) : if istype ( obj , sequence_types ) : t = type ( obj ) return t ( [ can ( i ) for i in obj ] ) else : return obj
5157	def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
2796	def load ( self , use_slug = False ) : identifier = None if use_slug or not self . id : identifier = self . slug else : identifier = self . id if not identifier : raise NotFoundError ( "One of self.id or self.slug must be set." ) data = self . get_data ( "images/%s" % identifier ) image_dict = data [ 'image' ] for attr in image_dict . keys ( ) : setattr ( self , attr , image_dict [ attr ] ) return self
8519	def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f . read ( ) ) . hexdigest ( )
5681	def get_spreading_trips ( self , start_time_ut , lat , lon , max_duration_ut = 4 * 3600 , min_transfer_time = 30 , use_shapes = False ) : from gtfspy . spreading . spreader import Spreader spreader = Spreader ( self , start_time_ut , lat , lon , max_duration_ut , min_transfer_time , use_shapes ) return spreader . spread ( )
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
4627	def encrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . encrypt ( str ( wif ) , self . masterkey ) , "encwif" )
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
4385	def remove_binaries ( ) : patterns = ( "adslib/*.a" , "adslib/*.o" , "adslib/obj/*.o" , "adslib/*.bin" , "adslib/*.so" , ) for f in functools . reduce ( operator . iconcat , [ glob . glob ( p ) for p in patterns ] ) : os . remove ( f )
4371	def spawn ( self , fn , * args , ** kwargs ) : if hasattr ( self , 'exception_handler_decorator' ) : fn = self . exception_handler_decorator ( fn ) new = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( new ) return new
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
1415	def create_pplan ( self , topologyName , pplan ) : if not pplan or not pplan . IsInitialized ( ) : raise_ ( StateException ( "Physical Plan protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_pplan_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) pplanString = pplan . SerializeToString ( ) try : self . client . create ( path , value = pplanString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating pplan" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating pplan" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating pplan" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( ** self . parent . data )
1395	def addNewTopology ( self , state_manager , topologyName ) : topology = Topology ( topologyName , state_manager . name ) Log . info ( "Adding new topology: %s, state_manager: %s" , topologyName , state_manager . name ) self . topologies . append ( topology ) topology . register_watch ( self . setTopologyInfo ) def on_topology_pplan ( data ) : Log . info ( "Watch triggered for topology pplan: " + topologyName ) topology . set_physical_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_packing_plan ( data ) : Log . info ( "Watch triggered for topology packing plan: " + topologyName ) topology . set_packing_plan ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_execution_state ( data ) : Log . info ( "Watch triggered for topology execution state: " + topologyName ) topology . set_execution_state ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_tmaster ( data ) : Log . info ( "Watch triggered for topology tmaster: " + topologyName ) topology . set_tmaster ( data ) if not data : Log . debug ( "No data to be set" ) def on_topology_scheduler_location ( data ) : Log . info ( "Watch triggered for topology scheduler location: " + topologyName ) topology . set_scheduler_location ( data ) if not data : Log . debug ( "No data to be set" ) state_manager . get_pplan ( topologyName , on_topology_pplan ) state_manager . get_packing_plan ( topologyName , on_topology_packing_plan ) state_manager . get_execution_state ( topologyName , on_topology_execution_state ) state_manager . get_tmaster ( topologyName , on_topology_tmaster ) state_manager . get_scheduler_location ( topologyName , on_topology_scheduler_location )
8350	def _toStringSubclass ( self , text , subclass ) : self . endData ( ) self . handle_data ( text ) self . endData ( subclass )
8530	def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
11628	def clear_sent_messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE_CLEAR_OFFSET' , defaults . MAILQUEUE_CLEAR_OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete_before = timezone . now ( ) - offset self . filter ( sent = True , last_attempt__lte = delete_before ) . delete ( )
11156	def print_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . size ) for p in self . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
7669	def slice ( self , start_time , end_time , strict = False ) : sliced_array = AnnotationArray ( ) for ann in self : sliced_array . append ( ann . slice ( start_time , end_time , strict = strict ) ) return sliced_array
3910	def _rename ( self , name , callback ) : self . _coroutine_queue . put ( self . _conversation . rename ( name ) ) callback ( )
2918	def _eval_kwargs ( kwargs , my_task ) : results = { } for kwarg , value in list ( kwargs . items ( ) ) : if isinstance ( value , Attrib ) or isinstance ( value , PathAttrib ) : results [ kwarg ] = valueof ( my_task , value ) else : results [ kwarg ] = value return results
8330	def findAllNext ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextGenerator , ** kwargs )
430	def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : imgs = [ ] for idx in range ( 0 , len ( img_list ) , n_threads ) : b_imgs_list = img_list [ idx : idx + n_threads ] b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) imgs . extend ( b_imgs ) if printable : tl . logging . info ( 'read %d from %s' % ( len ( imgs ) , path ) ) return imgs
5163	def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
12342	def images ( self ) : "List of paths to images." tifs = _pattern ( self . _image_path , extension = 'tif' ) pngs = _pattern ( self . _image_path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
1801	def LAHF ( cpu ) : used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) is_expression = any ( issymbolic ( x ) for x in used_regs ) def make_flag ( val , offset ) : if is_expression : return Operators . ITEBV ( 8 , val , BitVecConstant ( 8 , 1 << offset ) , BitVecConstant ( 8 , 0 ) ) else : return val << offset cpu . AH = ( make_flag ( cpu . SF , 7 ) | make_flag ( cpu . ZF , 6 ) | make_flag ( 0 , 5 ) | make_flag ( cpu . AF , 4 ) | make_flag ( 0 , 3 ) | make_flag ( cpu . PF , 2 ) | make_flag ( 1 , 1 ) | make_flag ( cpu . CF , 0 ) )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
2750	def get_all_sizes ( self ) : data = self . get_data ( "sizes/" ) sizes = list ( ) for jsoned in data [ 'sizes' ] : size = Size ( ** jsoned ) size . token = self . token sizes . append ( size ) return sizes
7655	def update ( self , ** kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
7595	def get_clan ( self , * tags : crtag , ** params : keys ) : url = self . api . CLAN + '/' + ',' . join ( tags ) return self . _get_model ( url , FullClan , ** params )
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
3569	def centralManager_didDiscoverPeripheral_advertisementData_RSSI_ ( self , manager , peripheral , data , rssi ) : logger . debug ( 'centralManager_didDiscoverPeripheral_advertisementData_RSSI called' ) device = device_list ( ) . get ( peripheral ) if device is None : device = device_list ( ) . add ( peripheral , CoreBluetoothDevice ( peripheral ) ) device . _update_advertised ( data )
4592	def colors_no_palette ( colors = None , ** kwds ) : if isinstance ( colors , str ) : colors = _split_colors ( colors ) else : colors = to_triplets ( colors or ( ) ) colors = ( color ( c ) for c in colors or ( ) ) return palette . Palette ( colors , ** kwds )
6717	def has_virtualenv ( self ) : with self . settings ( warn_only = True ) : ret = self . run_or_local ( 'which virtualenv' ) . strip ( ) return bool ( ret )
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
10925	def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
2532	def parse_ext_doc_ref ( self , ext_doc_ref_term ) : for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'externalDocumentId' ] , None ) ) : try : self . builder . set_ext_doc_id ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'External Document ID' ) break for _s , _p , o in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'spdxDocument' ] , None ) ) : try : self . builder . set_spdx_doc_uri ( self . doc , six . text_type ( o ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'SPDX Document URI' ) break for _s , _p , checksum in self . graph . triples ( ( ext_doc_ref_term , self . spdx_namespace [ 'checksum' ] , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : try : self . builder . set_chksum ( self . doc , six . text_type ( value ) ) except SPDXValueError : self . value_error ( 'EXT_DOC_REF_VALUE' , 'Checksum' ) break
7473	def singlecat ( data , sample , bseeds , sidx , nloci ) : LOGGER . info ( "in single cat here" ) isref = 'reference' in data . paramsdict [ "assembly_method" ] with h5py . File ( bseeds , 'r' ) as io5 : hits = io5 [ "uarr" ] [ : ] hits = hits [ hits [ : , 1 ] == sidx , : ] seeds = io5 [ "seedsarr" ] [ : ] seeds = seeds [ seeds [ : , 1 ] == sidx , : ] full = np . concatenate ( ( seeds , hits ) ) full = full [ full [ : , 0 ] . argsort ( ) ] maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 ocatg = np . zeros ( ( nloci , maxlen , 4 ) , dtype = np . uint32 ) onall = np . zeros ( nloci , dtype = np . uint8 ) ochrom = np . zeros ( ( nloci , 3 ) , dtype = np . int64 ) if not sample . files . database : raise IPyradWarningExit ( "missing catg file - {}" . format ( sample . name ) ) with h5py . File ( sample . files . database , 'r' ) as io5 : catarr = io5 [ "catg" ] [ : ] tmp = catarr [ full [ : , 2 ] , : maxlen , : ] del catarr ocatg [ full [ : , 0 ] , : tmp . shape [ 1 ] , : ] = tmp del tmp nall = io5 [ "nalleles" ] [ : ] onall [ full [ : , 0 ] ] = nall [ full [ : , 2 ] ] del nall if isref : chrom = io5 [ "chroms" ] [ : ] ochrom [ full [ : , 0 ] ] = chrom [ full [ : , 2 ] ] del chrom ipath = os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) with h5py . File ( ipath , 'r' ) as ih5 : indels = ih5 [ "indels" ] [ sidx , : , : maxlen ] newcatg = inserted_indels ( indels , ocatg ) del ocatg , indels smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio , 'w' ) as oh5 : oh5 . create_dataset ( "icatg" , data = newcatg , dtype = np . uint32 ) oh5 . create_dataset ( "inall" , data = onall , dtype = np . uint8 ) if isref : oh5 . create_dataset ( "ichrom" , data = ochrom , dtype = np . int64 )
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
12621	def repr_imgs ( imgs ) : if isinstance ( imgs , string_types ) : return imgs if isinstance ( imgs , collections . Iterable ) : return '[{}]' . format ( ', ' . join ( repr_imgs ( img ) for img in imgs ) ) try : filename = imgs . get_filename ( ) if filename is not None : img_str = "{}('{}')" . format ( imgs . __class__ . __name__ , filename ) else : img_str = "{}(shape={}, affine={})" . format ( imgs . __class__ . __name__ , repr ( get_shape ( imgs ) ) , repr ( imgs . get_affine ( ) ) ) except Exception as exc : log . error ( 'Error reading attributes from img.get_filename()' ) return repr ( imgs ) else : return img_str
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
5683	def tripI_takes_place_on_dsut ( self , trip_I , day_start_ut ) : query = "SELECT * FROM days WHERE trip_I=? AND day_start_ut=?" params = ( trip_I , day_start_ut ) cur = self . conn . cursor ( ) rows = list ( cur . execute ( query , params ) ) if len ( rows ) == 0 : return False else : assert len ( rows ) == 1 , 'On a day, a trip_I should be present at most once' return True
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
749	def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict
12835	def on_exit_stage ( self ) : self . forum . on_finish_game ( ) for actor in self . actors : actor . on_finish_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_finish_game ( )
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : r state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) abf . saveThing ( [ Xs , av1 ] , '01_iv' )
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : ret = np . asarray ( arr ) . astype ( dtype ) else : ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) if np . may_share_memory ( ret , arr ) and copy : ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
7311	def is_valid_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return True except ValueError as e : return False
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
13815	def _StructMessageToJsonObject ( message , unused_including_default = False ) : fields = message . fields ret = { } for key in fields : ret [ key ] = _ValueMessageToJsonObject ( fields [ key ] ) return ret
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
7603	def get_player ( self , tag : crtag , timeout = None ) : url = self . api . PLAYER + '/' + tag return self . _get_model ( url , FullPlayer , timeout = timeout )
8442	def _code_search ( query , github_user = None ) : github_client = temple . utils . GithubClient ( ) headers = { 'Accept' : 'application/vnd.github.v3.text-match+json' } resp = github_client . get ( '/search/code' , params = { 'q' : query , 'per_page' : 100 } , headers = headers ) if resp . status_code == requests . codes . unprocessable_entity and github_user : raise temple . exceptions . InvalidGithubUserError ( 'Invalid Github user or org - "{}"' . format ( github_user ) ) resp . raise_for_status ( ) resp_data = resp . json ( ) repositories = collections . defaultdict ( dict ) while True : repositories . update ( { 'git@github.com:{}.git' . format ( repo [ 'repository' ] [ 'full_name' ] ) : repo [ 'repository' ] for repo in resp_data [ 'items' ] } ) next_url = _parse_link_header ( resp . headers ) . get ( 'next' ) if next_url : resp = requests . get ( next_url , headers = headers ) resp . raise_for_status ( ) resp_data = resp . json ( ) else : break return repositories
6981	def get_centroid_offsets ( lcd , t_ing_egr , oot_buffer_time = 0.1 , sample_factor = 3 ) : qnum = int ( np . unique ( lcd [ 'quarter' ] ) ) LOGINFO ( 'Getting centroid offsets (qnum: {:d})...' . format ( qnum ) ) arcsec_per_px = 3.98 times = lcd [ 'ctd_dtr' ] [ 'times' ] ctd_resid_x = lcd [ 'ctd_dtr' ] [ 'ctd_x' ] - lcd [ 'ctd_dtr' ] [ 'fit_ctd_x' ] ctd_resid_y = lcd [ 'ctd_dtr' ] [ 'ctd_y' ] - lcd [ 'ctd_dtr' ] [ 'fit_ctd_y' ] cd = { } for ix , ( t_ing , t_egr ) in enumerate ( t_ing_egr ) : in_tra_times = times [ ( times > t_ing ) & ( times < t_egr ) ] transit_dur = t_egr - t_ing oot_window_len = sample_factor * transit_dur oot_before = times [ ( times < ( t_ing - oot_buffer_time ) ) & ( times > ( t_ing - oot_buffer_time - oot_window_len ) ) ] oot_after = times [ ( times > ( t_egr + oot_buffer_time ) ) & ( times < ( t_egr + oot_buffer_time + oot_window_len ) ) ] oot_times = npconcatenate ( [ oot_before , oot_after ] ) mask_tra = npin1d ( times , in_tra_times ) mask_oot = npin1d ( times , oot_times ) ctd_x_in_tra = ctd_resid_x [ mask_tra ] * arcsec_per_px ctd_y_in_tra = ctd_resid_y [ mask_tra ] * arcsec_per_px ctd_x_oot = ctd_resid_x [ mask_oot ] * arcsec_per_px ctd_y_oot = ctd_resid_y [ mask_oot ] * arcsec_per_px cd [ ix ] = { 'ctd_x_in_tra' : ctd_x_in_tra , 'ctd_y_in_tra' : ctd_y_in_tra , 'ctd_x_oot' : ctd_x_oot , 'ctd_y_oot' : ctd_y_oot , 'npts_in_tra' : len ( ctd_x_in_tra ) , 'npts_oot' : len ( ctd_x_oot ) , 'in_tra_times' : in_tra_times , 'oot_times' : oot_times } LOGINFO ( 'Got centroid offsets (qnum: {:d}).' . format ( qnum ) ) return cd
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
6701	def add_apt_key ( filename = None , url = None , keyid = None , keyserver = 'subkeys.pgp.net' , update = False ) : if keyid is None : if filename is not None : run_as_root ( 'apt-key add %(filename)s' % locals ( ) ) elif url is not None : run_as_root ( 'wget %(url)s -O - | apt-key add -' % locals ( ) ) else : raise ValueError ( 'Either filename, url or keyid must be provided as argument' ) else : if filename is not None : _check_pgp_key ( filename , keyid ) run_as_root ( 'apt-key add %(filename)s' % locals ( ) ) elif url is not None : tmp_key = '/tmp/tmp.burlap.key.%(keyid)s.key' % locals ( ) run_as_root ( 'wget %(url)s -O %(tmp_key)s' % locals ( ) ) _check_pgp_key ( tmp_key , keyid ) run_as_root ( 'apt-key add %(tmp_key)s' % locals ( ) ) else : keyserver_opt = '--keyserver %(keyserver)s' % locals ( ) if keyserver is not None else '' run_as_root ( 'apt-key adv %(keyserver_opt)s --recv-keys %(keyid)s' % locals ( ) ) if update : update_index ( )
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
10306	def calculate_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = tanimoto_set_similarity ( dict_of_sets [ x ] , dict_of_sets [ y ] ) for x in dict_of_sets : result [ x ] [ x ] = 1.0 return dict ( result )
1770	def backup_emulate ( self , insn ) : if not hasattr ( self , 'backup_emu' ) : self . backup_emu = UnicornEmulator ( self ) try : self . backup_emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) ) finally : del self . backup_emu
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
6170	def filter ( self , x ) : y = signal . sosfilt ( self . sos , x ) return y
353	def assign_params ( sess , params , network ) : ops = [ ] for idx , param in enumerate ( params ) : ops . append ( network . all_params [ idx ] . assign ( param ) ) if sess is not None : sess . run ( ops ) return ops
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
1140	def fill ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . fill ( text )
7733	def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( parent = self . xmlnode ) item = MucItem ( "none" , "none" , nick = nick , reason = reason ) self . muc_child . add_item ( item ) return self . muc_child
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
12468	def save_traceback ( err ) : dirname = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) ) ) ) if not os . path . isdir ( dirname ) : os . mkdir ( dirname ) filename = os . path . join ( dirname , '{0}.log' . format ( __script__ ) ) with open ( filename , 'a+' ) as handler : traceback . print_exc ( file = handler ) message = ( 'User aborted workflow' if isinstance ( err , KeyboardInterrupt ) else 'Unexpected error catched' ) print_error ( message ) print_error ( 'Full log stored to {0}' . format ( filename ) , False ) return True
6960	def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd . close ( ) return model
10996	def schedules ( self ) : url = PATHS [ 'GET_SCHEDULES' ] % self . id self . __schedules = self . api . get ( url = url ) return self . __schedules
4205	def levdown ( anxt , enxt = None ) : if anxt [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) anxt = anxt [ 1 : ] knxt = anxt [ - 1 ] if knxt == 1.0 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = ( anxt [ 0 : - 1 ] - knxt * numpy . conj ( anxt [ - 2 : : - 1 ] ) ) / ( 1. - abs ( knxt ) ** 2 ) ecur = None if enxt is not None : ecur = enxt / ( 1. - numpy . dot ( knxt . conj ( ) . transpose ( ) , knxt ) ) acur = numpy . insert ( acur , 0 , 1 ) return acur , ecur
13854	def append_main_thread ( self ) : thread = MainThread ( main_queue = self . main_queue , main_spider = self . main_spider , branch_spider = self . branch_spider ) thread . daemon = True thread . start ( )
247	def map_transaction ( txn ) : if isinstance ( txn [ 'sid' ] , dict ) : sid = txn [ 'sid' ] [ 'sid' ] symbol = txn [ 'sid' ] [ 'symbol' ] else : sid = txn [ 'sid' ] symbol = txn [ 'sid' ] return { 'sid' : sid , 'symbol' : symbol , 'price' : txn [ 'price' ] , 'order_id' : txn [ 'order_id' ] , 'amount' : txn [ 'amount' ] , 'commission' : txn [ 'commission' ] , 'dt' : txn [ 'dt' ] }
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
10317	def single_run_arrays ( spanning_cluster = True , ** kwargs ) : r kwargs [ 'copy_result' ] = False ret = dict ( ) for n , state in enumerate ( sample_states ( spanning_cluster = spanning_cluster , ** kwargs ) ) : if 'N' in ret : assert ret [ 'N' ] == state [ 'N' ] else : ret [ 'N' ] = state [ 'N' ] if 'M' in ret : assert ret [ 'M' ] == state [ 'M' ] else : ret [ 'M' ] = state [ 'M' ] number_of_states = state [ 'M' ] + 1 max_cluster_size = np . empty ( number_of_states ) if spanning_cluster : has_spanning_cluster = np . empty ( number_of_states , dtype = np . bool ) moments = np . empty ( ( 5 , number_of_states ) ) max_cluster_size [ n ] = state [ 'max_cluster_size' ] for k in range ( 5 ) : moments [ k , n ] = state [ 'moments' ] [ k ] if spanning_cluster : has_spanning_cluster [ n ] = state [ 'has_spanning_cluster' ] ret [ 'max_cluster_size' ] = max_cluster_size ret [ 'moments' ] = moments if spanning_cluster : ret [ 'has_spanning_cluster' ] = has_spanning_cluster return ret
3487	def _check ( value , message ) : if value is None : LOGGER . error ( 'Error: LibSBML returned a null value trying ' 'to <' + message + '>.' ) elif type ( value ) is int : if value == libsbml . LIBSBML_OPERATION_SUCCESS : return else : LOGGER . error ( 'Error encountered trying to <' + message + '>.' ) LOGGER . error ( 'LibSBML error code {}: {}' . format ( str ( value ) , libsbml . OperationReturnValue_toString ( value ) . strip ( ) ) ) else : return
4606	def whitelist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "white" ] , account = self )
4507	def get_device ( self , id = None ) : if id is None : if not self . devices : raise ValueError ( 'No default device for %s' % self . hardware_id ) id , ( device , version ) = sorted ( self . devices . items ( ) ) [ 0 ] elif id in self . devices : device , version = self . devices [ id ] else : error = 'Unable to find device with ID %s' % id log . error ( error ) raise ValueError ( error ) log . info ( "Using COM Port: %s, Device ID: %s, Device Ver: %s" , device , id , version ) return id , device , version
1713	def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
9394	def plot_cdf ( self , graphing_library = 'matplotlib' ) : graphed = False for percentile_csv in self . percentiles_files : csv_filename = os . path . basename ( percentile_csv ) column = self . csv_column_map [ percentile_csv . replace ( ".percentiles." , "." ) ] if not self . check_important_sub_metrics ( column ) : continue column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] else : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
5368	def _get_storage_service ( credentials ) : if credentials is None : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
8955	def parse_glob ( pattern ) : if not pattern : return bits = pattern . split ( "/" ) dirs , filename = bits [ : - 1 ] , bits [ - 1 ] for dirname in dirs : if dirname == "**" : yield "(|.+/)" else : yield glob2re ( dirname ) + "/" yield glob2re ( filename )
11744	def goto ( self , rules , symbol ) : return self . closure ( { rule . move_dot ( ) for rule in rules if not rule . at_end and rule . rhs [ rule . pos ] == symbol } , )
592	def _compute ( self , inputs , outputs ) : if self . _sfdr is None : raise RuntimeError ( "Spatial pooler has not been initialized" ) if not self . topDownMode : self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 rfOutput = self . _doBottomUpCompute ( rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , resetSignal = resetSignal ) outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat else : topDownIn = inputs . get ( 'topDownIn' , None ) spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut if temporalTopDownOut is not None : outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut outputs [ 'anomalyScore' ] [ : ] = 0
8353	def parse_declaration ( self , i ) : j = None if self . rawdata [ i : i + 9 ] == '<![CDATA[' : k = self . rawdata . find ( ']]>' , i ) if k == - 1 : k = len ( self . rawdata ) data = self . rawdata [ i + 9 : k ] j = k + 3 self . _toStringSubclass ( data , CData ) else : try : j = SGMLParser . parse_declaration ( self , i ) except SGMLParseError : toHandle = self . rawdata [ i : ] self . handle_data ( toHandle ) j = i + len ( toHandle ) return j
2425	def set_doc_name ( self , doc , name ) : if not self . doc_name_set : doc . name = name self . doc_name_set = True return True else : raise CardinalityError ( 'Document::Name' )
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
9311	def amz_cano_querystring ( qs ) : safe_qs_amz_chars = '&=+' safe_qs_unresvd = '-_.~' if PY2 : qs = qs . encode ( 'utf-8' ) safe_qs_amz_chars = safe_qs_amz_chars . encode ( ) safe_qs_unresvd = safe_qs_unresvd . encode ( ) qs = unquote ( qs ) space = b' ' if PY2 else ' ' qs = qs . split ( space ) [ 0 ] qs = quote ( qs , safe = safe_qs_amz_chars ) qs_items = { } for name , vals in parse_qs ( qs , keep_blank_values = True ) . items ( ) : name = quote ( name , safe = safe_qs_unresvd ) vals = [ quote ( val , safe = safe_qs_unresvd ) for val in vals ] qs_items [ name ] = vals qs_strings = [ ] for name , vals in qs_items . items ( ) : for val in vals : qs_strings . append ( '=' . join ( [ name , val ] ) ) qs = '&' . join ( sorted ( qs_strings ) ) if PY2 : qs = unicode ( qs ) return qs
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
1753	def get_argument_values ( self , model , prefix_args ) : spec = inspect . getfullargspec ( model ) if spec . varargs : logger . warning ( "ABI: A vararg model must be a unary function." ) nargs = len ( spec . args ) - len ( prefix_args ) if inspect . ismethod ( model ) : nargs -= 1 def resolve_argument ( arg ) : if isinstance ( arg , str ) : return self . _cpu . read_register ( arg ) else : return self . _cpu . read_int ( arg ) descriptors = self . get_arguments ( ) argument_iter = map ( resolve_argument , descriptors ) from . . models import isvariadic if isvariadic ( model ) : arguments = prefix_args + ( argument_iter , ) else : arguments = prefix_args + tuple ( islice ( argument_iter , nargs ) ) return arguments
1178	def sub ( self , repl , string , count = 0 ) : return self . _subx ( repl , string , count , False )
7802	def _decode_asn1_string ( data ) : if isinstance ( data , BMPString ) : return bytes ( data ) . decode ( "utf-16-be" ) else : return bytes ( data ) . decode ( "utf-8" )
2510	def handle_extracted_license ( self , extr_lic ) : lic = self . parse_only_extr_license ( extr_lic ) if lic is not None : self . doc . add_extr_lic ( lic ) return lic
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
586	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn classifier_indexes = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . _autoDetectWaitRecords ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None classifier . setParameter ( 'inferenceMode' , True ) classifier . setParameter ( 'learningMode' , False ) classifier . getSelf ( ) . compute ( inputs , outputs ) classifier . setParameter ( 'learningMode' , True ) classifier_distances = classifier . getSelf ( ) . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = classifier . getSelf ( ) . getCategoryList ( ) [ indexID ] return category return None
208	def draw_on_image ( self , image , alpha = 0.75 , cmap = "jet" , resize = "heatmaps" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "heatmaps" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = "cubic" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == "heatmaps" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix
2186	def load ( self , cfgstr = None ) : from six . moves import cPickle as pickle cfgstr = self . _rectify_cfgstr ( cfgstr ) dpath = self . dpath fname = self . fname verbose = self . verbose if not self . enabled : if verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) raise IOError ( 3 , 'Cache Loading Is Disabled' ) fpath = self . get_fpath ( cfgstr = cfgstr ) if not exists ( fpath ) : if verbose > 2 : self . log ( '[cacher] ... cache does not exist: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) raise IOError ( 2 , 'No such file or directory: %r' % ( fpath , ) ) else : if verbose > 3 : self . log ( '[cacher] ... cache exists: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) try : with open ( fpath , 'rb' ) as file_ : data = pickle . load ( file_ ) except Exception as ex : if verbose > 0 : self . log ( 'CORRUPTED? fpath = %s' % ( fpath , ) ) if verbose > 1 : self . log ( '[cacher] ... CORRUPTED? dpath={} cfgstr={}' . format ( basename ( dpath ) , cfgstr ) ) if isinstance ( ex , ( EOFError , IOError , ImportError ) ) : raise IOError ( str ( ex ) ) else : if verbose > 1 : self . log ( '[cacher] ... unknown reason for exception' ) raise else : if self . verbose > 2 : self . log ( '[cacher] ... {} cache hit' . format ( self . fname ) ) elif verbose > 1 : self . log ( '[cacher] ... cache hit' ) return data
11565	def stepper_config ( self , steps_per_revolution , stepper_pins ) : data = [ self . STEPPER_CONFIGURE , steps_per_revolution & 0x7f , ( steps_per_revolution >> 7 ) & 0x7f ] for pin in range ( len ( stepper_pins ) ) : data . append ( stepper_pins [ pin ] ) self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
7426	def refmap_stats ( data , sample ) : mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
10757	def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
10521	def oneright ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
6656	def calibrateEB ( variances , sigma2 ) : if ( sigma2 <= 0 or min ( variances ) == max ( variances ) ) : return ( np . maximum ( variances , 0 ) ) sigma = np . sqrt ( sigma2 ) eb_prior = gfit ( variances , sigma ) part = functools . partial ( gbayes , g_est = eb_prior , sigma = sigma ) if len ( variances ) >= 200 : calib_x = np . percentile ( variances , np . arange ( 0 , 102 , 2 ) ) calib_y = list ( map ( part , calib_x ) ) calib_all = np . interp ( variances , calib_x , calib_y ) else : calib_all = list ( map ( part , variances ) ) return np . asarray ( calib_all )
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) distribution = self . get_distribution ( ) if distribution : self . project_name = distribution . get_name ( ) else : self . project_name = self . project_dir . name if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : package_name = self . project_name . replace ( "-" , "_" ) def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
10817	def can_see_members ( self , user ) : if self . privacy_policy == PrivacyPolicy . PUBLIC : return True elif self . privacy_policy == PrivacyPolicy . MEMBERS : return self . is_member ( user ) or self . is_admin ( user ) elif self . privacy_policy == PrivacyPolicy . ADMINS : return self . is_admin ( user )
9918	def validate_key ( self , key ) : try : confirmation = models . EmailConfirmation . objects . select_related ( "email__user" ) . get ( key = key ) except models . EmailConfirmation . DoesNotExist : raise serializers . ValidationError ( _ ( "The provided verification key is invalid." ) ) if confirmation . is_expired : raise serializers . ValidationError ( _ ( "That verification code has expired." ) ) self . _confirmation = confirmation return key
6633	def sourceDirValidationError ( dirname , component_name ) : if dirname == component_name : return 'Module %s public include directory %s should not contain source files' % ( component_name , dirname ) elif dirname . lower ( ) in ( 'source' , 'src' ) and dirname != 'source' : return 'Module %s has non-standard source directory name: "%s" should be "source"' % ( component_name , dirname ) elif isPotentialTestDir ( dirname ) and dirname != 'test' : return 'Module %s has non-standard test directory name: "%s" should be "test"' % ( component_name , dirname ) elif not Source_Dir_Regex . match ( dirname ) : corrected = Source_Dir_Invalid_Regex . sub ( '' , dirname . lower ( ) ) if not corrected : corrected = 'source' return 'Module %s has non-standard source directory name: "%s" should be "%s"' % ( component_name , dirname , corrected ) else : return None
1498	def process_incoming_tuples ( self ) : if self . output_helper . is_out_queue_available ( ) : self . _read_tuples_and_execute ( ) self . output_helper . send_out_tuples ( ) else : self . bolt_metrics . update_out_queue_full_count ( )
9422	def _process_current ( self , handle , op , dest_path = None , dest_name = None ) : unrarlib . RARProcessFileW ( handle , op , dest_path , dest_name )
8887	def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
1440	def register_metrics ( self , context ) : sys_config = system_config . get_sys_config ( ) interval = float ( sys_config [ constants . HERON_METRICS_EXPORT_INTERVAL_SEC ] ) collector = context . get_metrics_collector ( ) super ( ComponentMetrics , self ) . register_metrics ( collector , interval )
104	def pad_to_aspect_ratio ( arr , aspect_ratio , mode = "constant" , cval = 0 , return_pad_amounts = False ) : pad_top , pad_right , pad_bottom , pad_left = compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) arr_padded = pad ( arr , top = pad_top , right = pad_right , bottom = pad_bottom , left = pad_left , mode = mode , cval = cval ) if return_pad_amounts : return arr_padded , ( pad_top , pad_right , pad_bottom , pad_left ) else : return arr_padded
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
12983	def filename ( file_name , start_on = None , ignore = ( ) , use_short = True , ** queries ) : with open ( file_name ) as template_file : return file ( template_file , start_on = start_on , ignore = ignore , use_short = use_short , ** queries )
1635	def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] fncall = line for pattern in ( r'\bif\s*\((.*)\)\s*{' , r'\bfor\s*\((.*)\)\s*{' , r'\bwhile\s*\((.*)\)\s*[{;]' , r'\bswitch\s*\((.*)\)\s*{' ) : match = Search ( pattern , line ) if match : fncall = match . group ( 1 ) break if ( not Search ( r'\b(if|for|while|switch|return|new|delete|catch|sizeof)\b' , fncall ) and not Search ( r' \([^)]+\)\([^)]*(\)|,$)' , fncall ) and not Search ( r' \([^)]+\)\[[^\]]+\]' , fncall ) ) : if Search ( r'\w\s*\(\s(?!\s*\\$)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space after ( in function call' ) elif Search ( r'\(\s+(?!(\s*\\)|\()' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space after (' ) if ( Search ( r'\w\s+\(' , fncall ) and not Search ( r'_{0,2}asm_{0,2}\s+_{0,2}volatile_{0,2}\s+\(' , fncall ) and not Search ( r'#\s*define|typedef|using\s+\w+\s*=' , fncall ) and not Search ( r'\w\s+\((\w+::)*\*\w+\)\(' , fncall ) and not Search ( r'\bcase\s+\(' , fncall ) ) : if Search ( r'\boperator_*\b' , line ) : error ( filename , linenum , 'whitespace/parens' , 0 , 'Extra space before ( in function call' ) else : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space before ( in function call' ) if Search ( r'[^)]\s+\)\s*[^{\s]' , fncall ) : if Search ( r'^\s+\)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Closing ) should be moved to the previous line' ) else : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space before )' )
8031	def compareChunks ( handles , chunk_size = CHUNK_SIZE ) : chunks = [ ( path , fh , fh . read ( chunk_size ) ) for path , fh , _ in handles ] more , done = [ ] , [ ] while chunks : matches , non_matches = [ chunks [ 0 ] ] , [ ] for chunk in chunks [ 1 : ] : if matches [ 0 ] [ 2 ] == chunk [ 2 ] : matches . append ( chunk ) else : non_matches . append ( chunk ) if len ( matches ) == 1 or matches [ 0 ] [ 2 ] == "" : for x in matches : x [ 1 ] . close ( ) done . append ( [ x [ 0 ] for x in matches ] ) else : more . append ( matches ) chunks = non_matches return more , done
5200	def Select ( self , command , index ) : OutstationApplication . process_point_value ( 'Select' , command , index , None ) return opendnp3 . CommandStatus . SUCCESS
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : fmt = '' if l > 1 : fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : if num_bytes < 4 : fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) fd . write ( struct . pack ( fmt , * data ) )
841	def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern
4402	def remove_node ( self , node ) : self . nodes . remove ( node ) for x in xrange ( self . replicas ) : ring_key = self . hash_method ( b ( "%s:%d" % ( node , x ) ) ) self . ring . pop ( ring_key ) self . sorted_keys . remove ( ring_key )
5685	def increment_day_start_ut ( self , day_start_ut , n_days = 1 ) : old_tz = self . set_current_process_time_zone ( ) day0 = time . localtime ( day_start_ut + 43200 ) dayN = time . mktime ( day0 [ : 2 ] + ( day0 [ 2 ] + n_days , ) + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return dayN
9084	def upload_backend ( index = 'dev' , user = None ) : get_vars ( ) use_devpi ( index = index ) with fab . lcd ( '../application' ) : fab . local ( 'make upload' )
2740	def add_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = POST , params = { "tags" : tags } )
12879	def sep1 ( parser , separator ) : first = [ parser ( ) ] def inner ( ) : separator ( ) return parser ( ) return first + many ( tri ( inner ) )
1124	def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . _accept ( kind ) if result is unmatched : return result return result . loc return rule
6800	def get_free_space ( self ) : cmd = "df -k | grep -vE '^Filesystem|tmpfs|cdrom|none|udev|cgroup' | awk '{ print($1 \" \" $4 }'" lines = [ _ for _ in self . run ( cmd ) . strip ( ) . split ( '\n' ) if _ . startswith ( '/' ) ] assert len ( lines ) == 1 , 'Ambiguous devices: %s' % str ( lines ) device , kb = lines [ 0 ] . split ( ' ' ) free_space = int ( kb ) * 1024 self . vprint ( 'free_space (bytes):' , free_space ) return free_space
245	def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]
8620	def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server = None if serverid is None : if servername is None : raise ValueError ( "one of 'serverid' or 'servername' must be specified" ) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex return server
10066	def file_serializer ( obj ) : return { "id" : str ( obj . file_id ) , "filename" : obj . key , "filesize" : obj . file . size , "checksum" : obj . file . checksum , }
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) return ( average , np . sqrt ( variance ) )
211	def to_uint8 ( self ) : arr_0to255 = np . clip ( np . round ( self . arr_0to1 * 255 ) , 0 , 255 ) arr_uint8 = arr_0to255 . astype ( np . uint8 ) return arr_uint8
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
9604	def raise_for_status ( self ) : if not self . status : return error = find_exception_by_code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise WebDriverException ( error , message , screen , stacktrace )
6003	def sub_to_pix ( self ) : return mapper_util . voronoi_sub_to_pix_from_grids_and_geometry ( sub_grid = self . grid_stack . sub , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , sub_to_regular = self . grid_stack . sub . sub_to_regular , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
12641	def get_config_value ( name , fallback = None ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . get ( 'servicefabric' , name , fallback )
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
8558	def create_lan ( self , datacenter_id , lan ) : data = json . dumps ( self . _create_lan_dict ( lan ) ) response = self . _perform_request ( url = '/datacenters/%s/lans' % datacenter_id , method = 'POST' , data = data ) return response
8081	def rellineto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . rellineto ( x , y )
13499	def parse ( s ) : try : m = _regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with_revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except AttributeError : return None
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
7046	def _bls_runner ( times , mags , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) : workarr_u = npones ( times . size ) workarr_v = npones ( times . size ) blsresult = eebls ( times , mags , workarr_u , workarr_v , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) return { 'power' : blsresult [ 0 ] , 'bestperiod' : blsresult [ 1 ] , 'bestpower' : blsresult [ 2 ] , 'transdepth' : blsresult [ 3 ] , 'transduration' : blsresult [ 4 ] , 'transingressbin' : blsresult [ 5 ] , 'transegressbin' : blsresult [ 6 ] }
13484	def ghpages ( ) : opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) nojekyll = path ( builddir ) / '.nojekyll' nojekyll . touch ( ) sh ( 'ghp-import -p %s' % ( builddir ) )
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
7645	def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
8606	def list_group_users ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/users?depth=%s' % ( group_id , str ( depth ) ) ) return response
1191	def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None
1897	def _assert ( self , expression : Bool ) : assert isinstance ( expression , Bool ) smtlib = translate_to_smtlib ( expression ) self . _send ( '(assert %s)' % smtlib )
8605	def delete_user ( self , user_id ) : response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'DELETE' ) return response
8468	def parseConfig ( cls , value ) : if 'enabled' in value : value [ 'enabled' ] = bool ( value [ 'enabled' ] ) if 'exclude_paths' in value : value [ 'exclude_paths' ] = [ n . strip ( ) for n in ast . literal_eval ( value [ 'exclude_paths' ] ) ] return value
4485	def write_to ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) response = self . _get ( self . _download_url , stream = True ) if response . status_code == 200 : response . raw . decode_content = True copyfileobj ( response . raw , fp , int ( response . headers [ 'Content-Length' ] ) ) else : raise RuntimeError ( "Response has status " "code {}." . format ( response . status_code ) )
7575	def detect_cpus ( ) : if hasattr ( os , "sysconf" ) : if os . sysconf_names . has_key ( "SC_NPROCESSORS_ONLN" ) : ncpus = os . sysconf ( "SC_NPROCESSORS_ONLN" ) if isinstance ( ncpus , int ) and ncpus > 0 : return ncpus else : return int ( os . popen2 ( "sysctl -n hw.ncpu" ) [ 1 ] . read ( ) ) if os . environ . has_key ( "NUMBER_OF_PROCESSORS" ) : ncpus = int ( os . environ [ "NUMBER_OF_PROCESSORS" ] ) if ncpus > 0 : return ncpus return 1
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( ** self . schema )
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
3823	async def get_conversation ( self , get_conversation_request ) : response = hangouts_pb2 . GetConversationResponse ( ) await self . _pb_request ( 'conversations/getconversation' , get_conversation_request , response ) return response
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
10228	def get_separate_unstable_correlation_triples ( graph : BELGraph ) -> Iterable [ NodeTriple ] : cg = get_correlation_graph ( graph ) for a , b , c in get_correlation_triangles ( cg ) : if POSITIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and NEGATIVE_CORRELATION in cg [ a ] [ c ] : yield b , a , c if POSITIVE_CORRELATION in cg [ a ] [ b ] and NEGATIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield a , b , c if NEGATIVE_CORRELATION in cg [ a ] [ b ] and POSITIVE_CORRELATION in cg [ b ] [ c ] and POSITIVE_CORRELATION in cg [ a ] [ c ] : yield c , a , b
13188	async def process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) published_url = ltd_product_data [ 'published_url' ] jsonld_url = urljoin ( published_url , '/metadata.jsonld' ) try : async with session . get ( jsonld_url ) as response : logger . debug ( '%s response status %r' , jsonld_url , response . status ) response . raise_for_status ( ) json_data = await response . text ( ) except aiohttp . ClientResponseError as err : logger . debug ( 'Tried to download %s, got status %d' , jsonld_url , err . code ) raise NotLanderPageError ( ) metadata = decode_jsonld ( json_data ) if mongo_collection is not None : await _upload_to_mongodb ( mongo_collection , metadata ) return metadata
2964	def insert_rule ( self , chain , src = None , dest = None , target = None ) : if not chain : raise ValueError ( "Invalid chain" ) if not target : raise ValueError ( "Invalid target" ) if not ( src or dest ) : raise ValueError ( "Need src, dest, or both" ) args = [ "-I" , chain ] if src : args += [ "-s" , src ] if dest : args += [ "-d" , dest ] args += [ "-j" , target ] self . call ( * args )
6519	def is_excluded_dir ( self , path ) : if self . is_excluded ( path ) : return True return matches_masks ( path . name , ALWAYS_EXCLUDED_DIRS )
6900	def parallel_periodicfeatures_lcdir ( pfpkl_dir , lcbasedir , outdir , pfpkl_glob = 'periodfinding-*.pkl*' , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS , recursive = True , ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None fileglob = pfpkl_glob LOGINFO ( 'searching for periodfinding pickles in %s ...' % pfpkl_dir ) if recursive is False : matching = glob . glob ( os . path . join ( pfpkl_dir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( pfpkl_dir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( pfpkl_dir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s periodfinding pickles, getting periodicfeatures...' % len ( matching ) ) return parallel_periodicfeatures ( matching , lcbasedir , outdir , starfeaturesdir = starfeaturesdir , fourierorder = fourierorder , transitparams = transitparams , ebparams = ebparams , pdiff_threshold = pdiff_threshold , sidereal_threshold = sidereal_threshold , sampling_peak_multiplier = sampling_peak_multiplier , sampling_startp = sampling_startp , sampling_endp = sampling_endp , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , sigclip = sigclip , verbose = verbose , maxobjects = maxobjects , nworkers = nworkers , ) else : LOGERROR ( 'no periodfinding pickles found in %s' % ( pfpkl_dir ) ) return None
8110	def search_news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_NEWS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
5476	def get_zones ( input_list ) : if not input_list : return [ ] output_list = [ ] for zone in input_list : if zone . endswith ( '*' ) : prefix = zone [ : - 1 ] output_list . extend ( [ z for z in _ZONES if z . startswith ( prefix ) ] ) else : output_list . append ( zone ) return output_list
2726	def create ( self , * args , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) if not self . size_slug and self . size : self . size_slug = self . size ssh_keys_id = Droplet . __get_ssh_keys_id_or_fingerprint ( self . ssh_keys , self . token , self . name ) data = { "name" : self . name , "size" : self . size_slug , "image" : self . image , "region" : self . region , "ssh_keys" : ssh_keys_id , "backups" : bool ( self . backups ) , "ipv6" : bool ( self . ipv6 ) , "private_networking" : bool ( self . private_networking ) , "volumes" : self . volumes , "tags" : self . tags , "monitoring" : bool ( self . monitoring ) , } if self . user_data : data [ "user_data" ] = self . user_data data = self . get_data ( "droplets/" , type = POST , params = data ) if data : self . id = data [ 'droplet' ] [ 'id' ] action_id = data [ 'links' ] [ 'actions' ] [ 0 ] [ 'id' ] self . action_ids = [ ] self . action_ids . append ( action_id )
7517	def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : catg [ 0 ] += 1 elif ncol [ idx ] == 65 : catg [ 1 ] += 1 elif ncol [ idx ] == 84 : catg [ 2 ] += 1 elif ncol [ idx ] == 71 : catg [ 3 ] += 1 elif ncol [ idx ] == 82 : catg [ 1 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 75 : catg [ 2 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 83 : catg [ 0 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 89 : catg [ 0 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 87 : catg [ 1 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 77 : catg [ 0 ] += 1 catg [ 1 ] += 1 catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
6509	def set_search_enviroment ( cls , ** kwargs ) : initializer = _load_class ( getattr ( settings , "SEARCH_INITIALIZER" , None ) , cls ) ( ) return initializer . initialize ( ** kwargs )
303	def plot_txn_time_hist ( transactions , bin_minutes = 5 , tz = 'America/New_York' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) txn_time = transactions . copy ( ) txn_time . index = txn_time . index . tz_convert ( pytz . timezone ( tz ) ) txn_time . index = txn_time . index . map ( lambda x : x . hour * 60 + x . minute ) txn_time [ 'trade_value' ] = ( txn_time . amount * txn_time . price ) . abs ( ) txn_time = txn_time . groupby ( level = 0 ) . sum ( ) . reindex ( index = range ( 570 , 961 ) ) txn_time . index = ( txn_time . index / bin_minutes ) . astype ( int ) * bin_minutes txn_time = txn_time . groupby ( level = 0 ) . sum ( ) txn_time [ 'time_str' ] = txn_time . index . map ( lambda x : str ( datetime . time ( int ( x / 60 ) , x % 60 ) ) [ : - 3 ] ) trade_value_sum = txn_time . trade_value . sum ( ) txn_time . trade_value = txn_time . trade_value . fillna ( 0 ) / trade_value_sum ax . bar ( txn_time . index , txn_time . trade_value , width = bin_minutes , ** kwargs ) ax . set_xlim ( 570 , 960 ) ax . set_xticks ( txn_time . index [ : : int ( 30 / bin_minutes ) ] ) ax . set_xticklabels ( txn_time . time_str [ : : int ( 30 / bin_minutes ) ] ) ax . set_title ( 'Transaction time distribution' ) ax . set_ylabel ( 'Proportion' ) ax . set_xlabel ( '' ) return ax
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
6508	def generate_field_filters ( cls , ** kwargs ) : generator = _load_class ( getattr ( settings , "SEARCH_FILTER_GENERATOR" , None ) , cls ) ( ) return ( generator . field_dictionary ( ** kwargs ) , generator . filter_dictionary ( ** kwargs ) , generator . exclude_dictionary ( ** kwargs ) , )
8248	def rotate_ryb ( self , angle = 180 ) : h = self . h * 360 angle = angle % 360 wheel = [ ( 0 , 0 ) , ( 15 , 8 ) , ( 30 , 17 ) , ( 45 , 26 ) , ( 60 , 34 ) , ( 75 , 41 ) , ( 90 , 48 ) , ( 105 , 54 ) , ( 120 , 60 ) , ( 135 , 81 ) , ( 150 , 103 ) , ( 165 , 123 ) , ( 180 , 138 ) , ( 195 , 155 ) , ( 210 , 171 ) , ( 225 , 187 ) , ( 240 , 204 ) , ( 255 , 219 ) , ( 270 , 234 ) , ( 285 , 251 ) , ( 300 , 267 ) , ( 315 , 282 ) , ( 330 , 298 ) , ( 345 , 329 ) , ( 360 , 0 ) ] for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if y0 <= h <= y1 : a = 1.0 * x0 + ( x1 - x0 ) * ( h - y0 ) / ( y1 - y0 ) break a = ( a + angle ) % 360 for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if x0 <= a <= x1 : h = 1.0 * y0 + ( y1 - y0 ) * ( a - x0 ) / ( x1 - x0 ) break h = h % 360 return Color ( h / 360 , self . s , self . brightness , self . a , mode = "hsb" , name = "" )
10931	def update_param_vals ( self , new_vals , incremental = False ) : self . _last_vals = self . param_vals . copy ( ) if incremental : self . param_vals += new_vals else : self . param_vals = new_vals . copy ( ) self . _fresh_JTJ = False
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
6040	def regular_data_1d_from_sub_data_1d ( self , sub_array_1d ) : return np . multiply ( self . sub_grid_fraction , sub_array_1d . reshape ( - 1 , self . sub_grid_length ) . sum ( axis = 1 ) )
12989	def setup_notebook ( debug = False ) : output_notebook ( INLINE , hide_banner = True ) if debug : _setup_logging ( logging . DEBUG ) logging . debug ( 'Running notebook in debug mode.' ) else : _setup_logging ( logging . WARNING ) if 'JUPYTERHUB_SERVICE_PREFIX' not in os . environ : global jupyter_proxy_url jupyter_proxy_url = 'localhost:8888' logging . info ( 'Setting jupyter proxy to local mode.' )
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
11446	def create_deleted_record ( self , record ) : identifier = record_get_field_value ( record , tag = "037" , code = "a" ) recid = identifier . split ( ":" ) [ - 1 ] try : source = identifier . split ( ":" ) [ 1 ] except IndexError : source = "Unknown" record_add_field ( record , "035" , subfields = [ ( "9" , source ) , ( "a" , recid ) ] ) record_add_field ( record , "980" , subfields = [ ( "c" , "DELETED" ) ] ) return record
7934	def _compute_handshake ( self ) : return hashlib . sha1 ( to_utf8 ( self . stream_id ) + to_utf8 ( self . secret ) ) . hexdigest ( )
10991	def makestate ( im , pos , rad , slab = None , mem_level = 'hi' ) : if slab is not None : o = comp . ComponentCollection ( [ objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) , slab ] , category = 'obj' ) else : o = objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) p = exactpsf . FixedSSChebLinePSF ( ) npts , iorder = _calc_ilm_order ( im . get_image ( ) . shape ) i = ilms . BarnesStreakLegPoly2P1D ( npts = npts , zorder = iorder ) b = ilms . LegendrePoly2P1D ( order = ( 9 , 3 , 5 ) , category = 'bkg' ) c = comp . GlobalScalar ( 'offset' , 0.0 ) s = states . ImageState ( im , [ o , i , b , c , p ] ) runner . link_zscale ( s ) if mem_level != 'hi' : s . set_mem_level ( mem_level ) opt . do_levmarq ( s , [ 'ilm-scale' ] , max_iter = 1 , run_length = 6 , max_mem = 1e4 ) return s
11270	def safe_substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . safe_substitute ( data )
2410	def create_essay_set_and_dump_model ( text , score , prompt , model_path , additional_array = None ) : essay_set = create_essay_set ( text , score , prompt ) feature_ext , clf = extract_features_and_generate_model ( essay_set , additional_array ) dump_model_to_file ( prompt , feature_ext , clf , model_path )
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
11482	def _upload_folder_recursive ( local_folder , parent_folder_id , leaf_folders_as_items = False , reuse_existing = False ) : if leaf_folders_as_items and _has_only_files ( local_folder ) : print ( 'Creating item from {0}' . format ( local_folder ) ) _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing ) return else : print ( 'Creating folder from {0}' . format ( local_folder ) ) new_folder_id = _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing ) for entry in sorted ( os . listdir ( local_folder ) ) : full_entry = os . path . join ( local_folder , entry ) if os . path . islink ( full_entry ) : continue elif os . path . isdir ( full_entry ) : _upload_folder_recursive ( full_entry , new_folder_id , leaf_folders_as_items , reuse_existing ) else : print ( 'Uploading item from {0}' . format ( full_entry ) ) _upload_as_item ( entry , new_folder_id , full_entry , reuse_existing )
13163	def serialize_text ( out , text ) : padding = len ( out ) add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
9815	def stop ( ctx , commit , yes ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not yes and not click . confirm ( "Are sure you want to stop notebook " "for project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without stopping notebook.' ) sys . exit ( 1 ) if commit is None : commit = True try : PolyaxonClient ( ) . project . stop_notebook ( user , project_name , commit ) Printer . print_success ( 'Notebook is being deleted' ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
8042	def parse ( self , filelike , filename ) : self . log = log self . source = filelike . readlines ( ) src = "" . join ( self . source ) compile ( src , filename , "exec" ) self . stream = TokenStream ( StringIO ( src ) ) self . filename = filename self . all = None self . future_imports = set ( ) self . _accumulated_decorators = [ ] return self . parse_module ( )
10575	def get_local_playlist_songs ( playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None ) : logger . info ( "Loading local playlist songs..." ) if os . name == 'nt' and CYGPATH_RE . match ( playlist ) : playlist = convert_cygwin_path ( playlist ) filepaths = [ ] base_filepath = os . path . dirname ( os . path . abspath ( playlist ) ) with open ( playlist ) as local_playlist : for line in local_playlist . readlines ( ) : line = line . strip ( ) if line . lower ( ) . endswith ( SUPPORTED_SONG_FORMATS ) : path = line if not os . path . isabs ( path ) : path = os . path . join ( base_filepath , path ) if os . path . isfile ( path ) : filepaths . append ( path ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local playlist songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
6585	def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
11337	def connect ( self ) : for tried_connection_count in range ( CFG_FTP_CONNECTION_ATTEMPTS ) : try : self . ftp = FtpHandler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( "Successful connection to the " "Oxford University Press server" ) ) return except socket_timeout_exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried_connection_count + 1 , CFG_FTP_CONNECTION_ATTEMPTS , CFG_FTP_TIMEOUT_SLEEP_DURATION ) ) time . sleep ( CFG_FTP_TIMEOUT_SLEEP_DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise LoginException ( err )
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
4141	def _arburg2 ( X , order ) : x = np . array ( X ) N = len ( x ) if order <= 0. : raise ValueError ( "order must be > 0" ) rho = sum ( abs ( x ) ** 2. ) / N den = rho * 2. * N ef = np . zeros ( N , dtype = complex ) eb = np . zeros ( N , dtype = complex ) for j in range ( 0 , N ) : ef [ j ] = x [ j ] eb [ j ] = x [ j ] a = np . zeros ( 1 , dtype = complex ) a [ 0 ] = 1 ref = np . zeros ( order , dtype = complex ) temp = 1. E = np . zeros ( order + 1 ) E [ 0 ] = rho for m in range ( 0 , order ) : efp = ef [ 1 : ] ebp = eb [ 0 : - 1 ] num = - 2. * np . dot ( ebp . conj ( ) . transpose ( ) , efp ) den = np . dot ( efp . conj ( ) . transpose ( ) , efp ) den += np . dot ( ebp , ebp . conj ( ) . transpose ( ) ) ref [ m ] = num / den ef = efp + ref [ m ] * ebp eb = ebp + ref [ m ] . conj ( ) . transpose ( ) * efp a . resize ( len ( a ) + 1 ) a = a + ref [ m ] * np . flipud ( a ) . conjugate ( ) E [ m + 1 ] = ( 1 - ref [ m ] . conj ( ) . transpose ( ) * ref [ m ] ) * E [ m ] return a , E [ - 1 ] , ref
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
3230	def service_list ( service = None , key_name = None , ** kwargs ) : resp_list = [ ] req = service . list ( ** kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
9443	def call ( self , call_params ) : path = '/' + self . api_version + '/Call/' method = 'POST' return self . request ( path , method , call_params )
11800	def infer_assignment ( self ) : "Return the partial assignment implied by the current inferences." self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
8771	def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
10219	def remove_nodes_by_function_namespace ( graph : BELGraph , func : str , namespace : Strings ) -> None : remove_filtered_nodes ( graph , function_namespace_inclusion_builder ( func , namespace ) )
2897	def cancel ( self , success = False ) : self . success = success cancel = [ ] mask = Task . NOT_FINISHED_MASK for task in Task . Iterator ( self . task_tree , mask ) : cancel . append ( task ) for task in cancel : task . cancel ( )
7830	def _new_from_xml ( cls , xmlnode ) : field_type = xmlnode . prop ( "type" ) label = from_utf8 ( xmlnode . prop ( "label" ) ) name = from_utf8 ( xmlnode . prop ( "var" ) ) child = xmlnode . children values = [ ] options = [ ] required = False desc = None while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "required" : required = True elif child . name == "desc" : desc = from_utf8 ( child . getContent ( ) ) elif child . name == "value" : values . append ( from_utf8 ( child . getContent ( ) ) ) elif child . name == "option" : options . append ( Option . _new_from_xml ( child ) ) child = child . next if field_type and not field_type . endswith ( "-multi" ) and len ( values ) > 1 : raise BadRequestProtocolError ( "Multiple values for a single-value field" ) return cls ( name , values , field_type , label , options , required , desc )
10067	def json_files_serializer ( objs , status = None ) : files = [ file_serializer ( obj ) for obj in objs ] return make_response ( json . dumps ( files ) , status )
239	def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig
5985	def inversion_psf_shape_tag_from_inversion_psf_shape ( inversion_psf_shape ) : if inversion_psf_shape is None : return '' else : y = str ( inversion_psf_shape [ 0 ] ) x = str ( inversion_psf_shape [ 1 ] ) return ( '_inv_psf_' + y + 'x' + x )
13779	def FindMessageTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _descriptors : self . FindFileContainingSymbol ( full_name ) return self . _descriptors [ full_name ]
5458	def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : job = yaml . load ( yaml_string ) dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
9080	def get_providers ( self , ** kwargs ) : if 'ids' in kwargs : ids = [ self . concept_scheme_uri_map . get ( id , id ) for id in kwargs [ 'ids' ] ] providers = [ self . providers [ k ] for k in self . providers . keys ( ) if k in ids ] else : providers = list ( self . providers . values ( ) ) if 'subject' in kwargs : providers = [ p for p in providers if kwargs [ 'subject' ] in p . metadata [ 'subject' ] ] return providers
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
11947	def jocker ( test_options = None ) : version = ver_check ( ) options = test_options or docopt ( __doc__ , version = version ) _set_global_verbosity_level ( options . get ( '--verbose' ) ) jocker_lgr . debug ( options ) jocker_run ( options )
4122	def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
90	def copy_random_state ( random_state , force_copy = False ) : if random_state == np . random and not force_copy : return random_state else : rs_copy = dummy_random_state ( ) orig_state = random_state . get_state ( ) rs_copy . set_state ( orig_state ) return rs_copy
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
13696	def parse_int ( s ) : try : val = int ( s ) except ValueError : print_err ( '\nInvalid integer: {}' . format ( s ) ) sys . exit ( 1 ) return val
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
9706	def get_field_settings ( self ) : field_settings = None if self . field_settings : if isinstance ( self . field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . PROFILE_KEY , { } ) field_settings = profiles . get ( self . field_settings ) else : field_settings = self . field_settings return field_settings
7548	def cluster_info ( ipyclient , spacer = "" ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
7528	def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
815	def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x
3768	def zs_to_Vfs ( zs , Vms ) : r vol_is = [ zi * Vmi for zi , Vmi in zip ( zs , Vms ) ] tot = sum ( vol_is ) return [ vol_i / tot for vol_i in vol_is ]
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
13305	def foex ( a , b ) : return ( np . sum ( a > b , dtype = float ) / len ( a ) - 0.5 ) * 100
7339	def log_error ( msg = None , exc_info = None , logger = None , ** kwargs ) : if logger is None : logger = _logger if not exc_info : exc_info = sys . exc_info ( ) if msg is None : msg = "" exc_class , exc_msg , _ = exc_info if all ( info is not None for info in exc_info ) : logger . error ( msg , exc_info = exc_info )
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) course . update ( enterprise_context ) return course
285	def plot_drawdown_underwater ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , ** kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
3278	def get_resource_inst ( self , path , environ ) : _logger . info ( "get_resource_inst('%s')" % path ) self . _count_get_resource_inst += 1 root = RootCollection ( environ ) return root . resolve ( "" , path )
9754	def get ( ctx , job ) : def get_experiment ( ) : try : response = PolyaxonClient ( ) . experiment . get_experiment ( user , project_name , _experiment ) cache . cache ( config_manager = ExperimentManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load experiment `{}` info.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_experiment_details ( response ) def get_experiment_job ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_job ( user , project_name , _experiment , _job ) cache . cache ( config_manager = ExperimentJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . resources : get_resources ( response . resources . to_dict ( ) , header = "Job resources:" ) response = Printer . add_status_color ( response . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'definition' , 'experiment' , 'unique_name' , 'resources' ] ) ) Printer . print_header ( "Job info:" ) dict_tabulate ( response ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job ( ) else : get_experiment ( )
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
665	def numpyStr ( array , format = '%f' , includeIndices = False , includeZeros = True ) : shape = array . shape assert ( len ( shape ) <= 2 ) items = [ '[' ] if len ( shape ) == 1 : if includeIndices : format = '%d:' + format if includeZeros : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) ] else : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) if x != 0 ] else : rowItems = [ format % ( x ) for x in array ] items . extend ( rowItems ) else : ( rows , cols ) = shape if includeIndices : format = '%d,%d:' + format for r in xrange ( rows ) : if includeIndices : rowItems = [ format % ( r , c , x ) for c , x in enumerate ( array [ r ] ) ] else : rowItems = [ format % ( x ) for x in array [ r ] ] if r > 0 : items . append ( '' ) items . append ( '[' ) items . extend ( rowItems ) if r < rows - 1 : items . append ( ']\n' ) else : items . append ( ']' ) items . append ( ']' ) return ' ' . join ( items )
12375	def allowed_operations ( self ) : if self . slug is not None : return self . meta . detail_allowed_operations return self . meta . list_allowed_operations
9022	def add_row ( self , id_ ) : row = self . _parser . new_row ( id_ ) self . _rows . append ( row ) return row
2178	def authorization_url ( self , url , request_token = None , ** kwargs ) : kwargs [ "oauth_token" ] = request_token or self . _client . client . resource_owner_key log . debug ( "Adding parameters %s to url %s" , kwargs , url ) return add_params_to_uri ( url , kwargs . items ( ) )
7063	def sqs_get_item ( queue_url , max_items = 1 , wait_time_seconds = 5 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : resp = client . receive_message ( QueueUrl = queue_url , AttributeNames = [ 'All' ] , MaxNumberOfMessages = max_items , WaitTimeSeconds = wait_time_seconds ) if not resp : LOGERROR ( 'could not receive messages from queue: %s' % queue_url ) else : messages = [ ] for msg in resp . get ( 'Messages' , [ ] ) : try : messages . append ( { 'id' : msg [ 'MessageId' ] , 'receipt_handle' : msg [ 'ReceiptHandle' ] , 'md5' : msg [ 'MD5OfBody' ] , 'attributes' : msg [ 'Attributes' ] , 'item' : json . loads ( msg [ 'Body' ] ) , } ) except Exception as e : LOGEXCEPTION ( 'could not deserialize message ID: %s, body: %s' % ( msg [ 'MessageId' ] , msg [ 'Body' ] ) ) continue return messages except Exception as e : LOGEXCEPTION ( 'could not get items from queue: %s' % queue_url ) if raiseonfail : raise return None
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
1012	def _cleanUpdatesList ( self , col , cellIdx , seg ) : for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )
7543	def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
11802	def conflicted_vars ( self , current ) : "Return a list of variables in current assignment that are in conflict" return [ var for var in self . vars if self . nconflicts ( var , current [ var ] , current ) > 0 ]
9829	def edges ( self ) : return [ self . delta [ d , d ] * numpy . arange ( self . shape [ d ] + 1 ) + self . origin [ d ] - 0.5 * self . delta [ d , d ] for d in range ( self . rank ) ]
7007	def _fourier_func ( fourierparams , phase , mags ) : order = int ( len ( fourierparams ) / 2 ) f_amp = fourierparams [ : order ] f_pha = fourierparams [ order : ] f_orders = [ f_amp [ x ] * npcos ( 2.0 * pi_value * x * phase + f_pha [ x ] ) for x in range ( order ) ] total_f = npmedian ( mags ) for fo in f_orders : total_f += fo return total_f
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
8785	def update_port ( self , context , port_id , ** kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
10400	def done_chomping ( self ) -> bool : return self . tag in self . graph . nodes [ self . target_node ]
11595	def _rc_renamenx ( self , src , dst ) : "Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist" if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
11537	def set_pin_direction ( self , pin , direction ) : if type ( pin ) is list : for p in pin : self . set_pin_direction ( p , direction ) return pin_id = self . _pin_mapping . get ( pin , None ) if pin_id and type ( direction ) is ahio . Direction : self . _set_pin_direction ( pin_id , direction ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
8768	def add_job_to_context ( context , job_id ) : db_job = db_api . async_transaction_find ( context , id = job_id , scope = db_api . ONE ) if not db_job : return context . async_job = { "job" : v . _make_job_dict ( db_job ) }
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
7654	def summary ( obj , indent = 0 ) : if hasattr ( obj , '__summary__' ) : rep = obj . __summary__ ( ) elif isinstance ( obj , SortedKeyList ) : rep = '<{:d} observations>' . format ( len ( obj ) ) else : rep = repr ( obj ) return rep . replace ( '\n' , '\n' + ' ' * indent )
2226	def _convert_hexstr_base ( hexstr , base ) : r if base is _ALPHABET_16 : return hexstr baselen = len ( base ) x = int ( hexstr , 16 ) if x == 0 : return '0' sign = 1 if x > 0 else - 1 x *= sign digits = [ ] while x : digits . append ( base [ x % baselen ] ) x //= baselen if sign < 0 : digits . append ( '-' ) digits . reverse ( ) newbase_str = '' . join ( digits ) return newbase_str
3670	def Wilson ( xs , params ) : r gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
5384	def _operation_status_message ( self ) : metadata = self . _op [ 'metadata' ] if not self . _op [ 'done' ] : if 'events' in metadata and metadata [ 'events' ] : last_event = metadata [ 'events' ] [ - 1 ] msg = last_event [ 'description' ] ds = last_event [ 'startTime' ] else : msg = 'Pending' ds = metadata [ 'createTime' ] else : ds = metadata [ 'endTime' ] if 'error' in self . _op : msg = self . _op [ 'error' ] [ 'message' ] else : msg = 'Success' return ( msg , google_base . parse_rfc3339_utc_string ( ds ) )
1824	def SETZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
7034	def import_apikey ( lcc_server , apikey_text_json ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) respdict = json . loads ( apikey_text_json ) apikey = respdict [ 'apikey' ] expires = respdict [ 'expires' ] if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
197	def Fog ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return CloudLayer ( intensity_mean = ( 220 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.5 ) , intensity_coarse_scale = 2 , alpha_min = ( 0.7 , 0.9 ) , alpha_multiplier = 0.3 , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 4.0 , - 2.0 ) , sparsity = 0.9 , density_multiplier = ( 0.4 , 0.9 ) , name = name , deterministic = deterministic , random_state = random_state )
1517	def start_master_nodes ( masters , cl_args ) : pids = [ ] for master in masters : Log . info ( "Starting master on %s" % master ) cmd = "%s agent -config %s >> /tmp/nomad_server_log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_master_config_file ( cl_args ) ) if not is_self ( master ) : cmd = ssh_remote_execute ( cmd , master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : master } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start master on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting masters" )
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
4368	def error ( self , error_name , error_message , msg_id = None , quiet = False ) : self . socket . error ( error_name , error_message , endpoint = self . ns_name , msg_id = msg_id , quiet = quiet )
12211	def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
12450	def _get_effect_statement ( self , effect , methods ) : statements = [ ] if len ( methods ) > 0 : statement = self . _get_empty_statement ( effect ) for method in methods : if ( method [ 'conditions' ] is None or len ( method [ 'conditions' ] ) == 0 ) : statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) else : cond_statement = self . _get_empty_statement ( effect ) cond_statement [ 'Resource' ] . append ( method [ 'resource_arn' ] ) cond_statement [ 'Condition' ] = method [ 'conditions' ] statements . append ( cond_statement ) statements . append ( statement ) return statements
10209	def check_write_permissions ( file ) : try : open ( file , 'a' ) except IOError : print ( "Can't open file {}. " "Please grant write permissions or change the path in your config" . format ( file ) ) sys . exit ( 1 )
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
11745	def closure ( self , rules ) : closure = set ( ) todo = set ( rules ) while todo : rule = todo . pop ( ) closure . add ( rule ) if rule . at_end : continue symbol = rule . rhs [ rule . pos ] for production in self . nonterminals [ symbol ] : for first in self . first ( rule . rest ) : if EPSILON in production . rhs : new_rule = DottedRule ( production , 1 , first ) else : new_rule = DottedRule ( production , 0 , first ) if new_rule not in closure : todo . add ( new_rule ) return frozenset ( closure )
12733	def move_next_to ( self , body_a , body_b , offset_a , offset_b ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) if ba is None : return bb . relative_offset_to_world ( offset_b ) if bb is None : return ba . relative_offset_to_world ( offset_a ) anchor = ba . relative_offset_to_world ( offset_a ) offset = bb . relative_offset_to_world ( offset_b ) bb . position = bb . position + anchor - offset return anchor
2809	def convert_constant ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting constant ...' ) params_list = params [ 'value' ] . numpy ( ) def target_layer ( x , value = params_list ) : return tf . constant ( value . tolist ( ) , shape = value . shape ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name + '_np' ] = params_list layers [ scope_name ] = lambda_layer ( layers [ list ( layers . keys ( ) ) [ 0 ] ] )
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
13533	def descendents ( self ) : visited = set ( [ ] ) self . _depth_descend ( self , visited ) try : visited . remove ( self ) except KeyError : pass return list ( visited )
1629	def GetHeaderGuardCPPVariable ( filename ) : filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
10107	def get_context_data ( self , ** kwargs ) : context = super ( TabView , self ) . get_context_data ( ** kwargs ) context . update ( kwargs ) process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( ** process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id if self . tab_parent is not None : if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) parent = self . tab_parent ( ) process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( ** process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( ** process_children_kwargs ) return context
7288	def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : if document is None : return None current_key , new_key_array = trim_field_key ( document , field_key ) key_array_digit = int ( new_key_array [ - 1 ] ) if new_key_array and has_digit ( new_key_array ) else None new_key = make_key ( new_key_array ) if key_array_digit is not None and len ( new_key_array ) > 0 : if len ( new_key_array ) == 1 : return_data = document . _data . get ( current_key , [ ] ) elif isinstance ( document , BaseList ) : return_list = [ ] if len ( document ) > 0 : return_list = [ get_value ( doc , new_key ) for doc in document ] return_data = return_list else : return_data = get_value ( getattr ( document , current_key ) , new_key ) elif len ( new_key_array ) > 0 : return_data = get_value ( document . _data . get ( current_key ) , new_key ) else : try : return_data = ( document . _data . get ( None , None ) if current_key == "id" else document . _data . get ( current_key , None ) ) except : return_data = document . _data . get ( current_key , None ) return return_data if self . is_initialized : return get_value ( self . model_instance , field_key ) else : return None
1586	def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
13508	def create_position ( self , params = { } ) : url = "/2/positions/" body = params data = self . _post_resource ( url , body ) return self . position_from_json ( data [ "position" ] )
12031	def get_protocol_sequence ( self , sweep ) : self . setsweep ( sweep ) return list ( self . protoSeqX ) , list ( self . protoSeqY )
3340	def make_complete_url ( environ , localUri = None ) : url = environ [ "wsgi.url_scheme" ] + "://" if environ . get ( "HTTP_HOST" ) : url += environ [ "HTTP_HOST" ] else : url += environ [ "SERVER_NAME" ] if environ [ "wsgi.url_scheme" ] == "https" : if environ [ "SERVER_PORT" ] != "443" : url += ":" + environ [ "SERVER_PORT" ] else : if environ [ "SERVER_PORT" ] != "80" : url += ":" + environ [ "SERVER_PORT" ] url += compat . quote ( environ . get ( "SCRIPT_NAME" , "" ) ) if localUri is None : url += compat . quote ( environ . get ( "PATH_INFO" , "" ) ) if environ . get ( "QUERY_STRING" ) : url += "?" + environ [ "QUERY_STRING" ] else : url += localUri return url
13891	def ReadLink ( path ) : _AssertIsLocal ( path ) if sys . platform != 'win32' : return os . readlink ( path ) if not IsLink ( path ) : from . _exceptions import FileNotFoundError raise FileNotFoundError ( path ) import jaraco . windows . filesystem result = jaraco . windows . filesystem . readlink ( path ) if '\\??\\' in result : result = result . split ( '\\??\\' ) [ 1 ] return result
4250	def country_name_by_addr ( self , addr ) : VALID_EDITIONS = ( const . COUNTRY_EDITION , const . COUNTRY_EDITION_V6 ) if self . _databaseType in VALID_EDITIONS : country_id = self . id_by_addr ( addr ) return const . COUNTRY_NAMES [ country_id ] elif self . _databaseType in const . CITY_EDITIONS : return self . record_by_addr ( addr ) . get ( 'country_name' ) else : message = 'Invalid database type, expected Country or City' raise GeoIPError ( message )
6084	def blurred_image_of_planes_from_1d_images_and_convolver ( total_planes , image_plane_image_1d_of_planes , image_plane_blurring_image_1d_of_planes , convolver , map_to_scaled_array ) : blurred_image_of_planes = [ ] for plane_index in range ( total_planes ) : if np . count_nonzero ( image_plane_image_1d_of_planes [ plane_index ] ) > 0 : blurred_image_1d_of_plane = blurred_image_1d_from_1d_unblurred_and_blurring_images ( unblurred_image_1d = image_plane_image_1d_of_planes [ plane_index ] , blurring_image_1d = image_plane_blurring_image_1d_of_planes [ plane_index ] , convolver = convolver ) blurred_image_of_plane = map_to_scaled_array ( array_1d = blurred_image_1d_of_plane ) blurred_image_of_planes . append ( blurred_image_of_plane ) else : blurred_image_of_planes . append ( None ) return blurred_image_of_planes
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
9192	def _get_file_sha1 ( file ) : bits = file . read ( ) file . seek ( 0 ) h = hashlib . new ( 'sha1' , bits ) . hexdigest ( ) return h
5139	def process_token ( self , kind , string , start , end , line ) : if self . current_block . is_comment : if kind == tokenize . COMMENT : self . current_block . add ( string , start , end , line ) else : self . new_noncomment ( start [ 0 ] , end [ 0 ] ) else : if kind == tokenize . COMMENT : self . new_comment ( string , start , end , line ) else : self . current_block . add ( string , start , end , line )
12380	def put ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) data = self . _clean ( target , self . request . read ( deserialize = True ) ) if target is not None : self . assert_operations ( 'update' ) try : self . update ( target , data ) except AttributeError : raise http . exceptions . NotImplemented ( ) self . make_response ( target ) else : self . assert_operations ( 'create' ) target = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( target )
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
5865	def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
10547	def find_taskruns ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'taskrun' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : return res except : raise
6887	def parallel_epd_lclist ( lclist , externalparams , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols outdict = { } for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , externalparams , lcformat , lcformatdir , epdsmooth_sigclip , epdsmooth_windowsize , epdsmooth_func , epdsmooth_extraparams ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_epd_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
12502	def smooth_imgs ( images , fwhm ) : if fwhm <= 0 : return images if not isinstance ( images , string_types ) and hasattr ( images , '__iter__' ) : only_one = False else : only_one = True images = [ images ] result = [ ] for img in images : img = check_img ( img ) affine = img . get_affine ( ) smooth = _smooth_data_array ( img . get_data ( ) , affine , fwhm = fwhm , copy = True ) result . append ( nib . Nifti1Image ( smooth , affine ) ) if only_one : return result [ 0 ] else : return result
502	def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )
4744	def dev_get_rprt ( dev_name , pugrp = None , punit = None ) : cmd = [ "nvm_cmd" , "rprt_all" , dev_name ] if not ( pugrp is None and punit is None ) : cmd = [ "nvm_cmd" , "rprt_lun" , dev_name , str ( pugrp ) , str ( punit ) ] _ , _ , _ , struct = cij . test . command_to_struct ( cmd ) if not struct : return None return struct [ "rprt_descr" ]
12250	def _get_key_internal ( self , * args , ** kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , ** kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
2027	def SHA3 ( self , start , size ) : data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) value = 0 known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
6557	def assert_penaltymodel_factory_available ( ) : from pkg_resources import iter_entry_points from penaltymodel . core import FACTORY_ENTRYPOINT from itertools import chain supported = ( 'maxgap' , 'mip' ) factories = chain ( * ( iter_entry_points ( FACTORY_ENTRYPOINT , name ) for name in supported ) ) try : next ( factories ) except StopIteration : raise AssertionError ( "To use 'dwavebinarycsp', at least one penaltymodel factory must be installed. " "Try {}." . format ( " or " . join ( "'pip install dwavebinarycsp[{}]'" . format ( name ) for name in supported ) ) )
1320	def GetTopLevelControl ( self ) -> 'Control' : handle = self . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) if topHandle : if topHandle == handle : return self else : return ControlFromHandle ( topHandle ) else : pass else : control = self while True : control = control . GetParentControl ( ) handle = control . NativeWindowHandle if handle : topHandle = GetAncestor ( handle , GAFlag . Root ) return ControlFromHandle ( topHandle )
11209	def datetime_exists ( dt , tz = None ) : if tz is None : if dt . tzinfo is None : raise ValueError ( 'Datetime is naive and no time zone provided.' ) tz = dt . tzinfo dt = dt . replace ( tzinfo = None ) dt_rt = dt . replace ( tzinfo = tz ) . astimezone ( tzutc ( ) ) . astimezone ( tz ) dt_rt = dt_rt . replace ( tzinfo = None ) return dt == dt_rt
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
11849	def percept ( self , agent ) : "By default, agent perceives things within a default radius." return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
5632	def unindent ( lines ) : try : indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
3265	def build_url ( base , seg , query = None ) : def clean_segment ( segment ) : segment = segment . strip ( '/' ) if isinstance ( segment , basestring ) : segment = segment . encode ( 'utf-8' ) return segment seg = ( quote ( clean_segment ( s ) ) for s in seg ) if query is None or len ( query ) == 0 : query_string = '' else : query_string = "?" + urlencode ( query ) path = '/' . join ( seg ) + query_string adjusted_base = base . rstrip ( '/' ) + '/' return urljoin ( str ( adjusted_base ) , str ( path ) )
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
1716	def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
8572	def list_nics ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
10774	def get_settings_path ( settings_module ) : cwd = os . getcwd ( ) settings_filename = '%s.py' % ( settings_module . split ( '.' ) [ - 1 ] ) while cwd : if settings_filename in os . listdir ( cwd ) : break cwd = os . path . split ( cwd ) [ 0 ] if os . name == 'nt' and NT_ROOT . match ( cwd ) : return None elif cwd == '/' : return None return cwd
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
2697	def build_graph ( json_iter ) : global DEBUG , WordNode graph = nx . DiGraph ( ) for meta in json_iter : if DEBUG : print ( meta [ "graf" ] ) for pair in get_tiles ( map ( WordNode . _make , meta [ "graf" ] ) ) : if DEBUG : print ( pair ) for word_id in pair : if not graph . has_node ( word_id ) : graph . add_node ( word_id ) try : graph . edge [ pair [ 0 ] ] [ pair [ 1 ] ] [ "weight" ] += 1.0 except KeyError : graph . add_edge ( pair [ 0 ] , pair [ 1 ] , weight = 1.0 ) return graph
5171	def auto_client ( cls , host , server , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : client = { "mode" : "p2p" , "nobind" : True , "resolv_retry" : "infinite" , "tls_client" : True } port = server . get ( 'port' ) or 1195 client [ 'remote' ] = [ { 'host' : host , 'port' : port } ] if server . get ( 'proto' ) == 'tcp-server' : client [ 'proto' ] = 'tcp-client' else : client [ 'proto' ] = 'udp' if 'server' in server or 'server_bridge' in server : client [ 'pull' ] = True if 'tls_server' not in server or not server [ 'tls_server' ] : client [ 'tls_client' ] = False ns_cert_type = { None : '' , '' : '' , 'client' : 'server' } client [ 'ns_cert_type' ] = ns_cert_type [ server . get ( 'ns_cert_type' ) ] remote_cert_tls = { None : '' , '' : '' , 'client' : 'server' } client [ 'remote_cert_tls' ] = remote_cert_tls [ server . get ( 'remote_cert_tls' ) ] copy_keys = [ 'name' , 'dev_type' , 'dev' , 'comp_lzo' , 'auth' , 'cipher' , 'ca' , 'cert' , 'key' , 'pkcs12' , 'mtu_disc' , 'mtu_test' , 'fragment' , 'mssfix' , 'keepalive' , 'persist_tun' , 'mute' , 'persist_key' , 'script_security' , 'user' , 'group' , 'log' , 'mute_replay_warnings' , 'secret' , 'reneg_sec' , 'tls_timeout' , 'tls_cipher' , 'float' , 'fast_io' , 'verb' ] for key in copy_keys : if key in server : client [ key ] = server [ key ] files = cls . _auto_client_files ( client , ca_path , ca_contents , cert_path , cert_contents , key_path , key_contents ) return { 'openvpn' : [ client ] , 'files' : files }
3960	def update_local_repo_async ( self , task_queue , force = False ) : self . ensure_local_repo ( ) task_queue . enqueue_task ( self . update_local_repo , force = force )
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
756	def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path
2736	def create ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'droplet_id' : self . droplet_id } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
8699	def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush ( )
13346	def run ( * args , ** kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , ** kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
985	def mmGetMetricSequencesPredictedActiveCellsShared ( self ) : self . _mmComputeTransitionTraces ( ) numSequencesForCell = defaultdict ( lambda : 0 ) for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : for cell in predictedActiveCells : numSequencesForCell [ cell ] += 1 return Metric ( self , "# sequences each predicted => active cells appears in" , numSequencesForCell . values ( ) )
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : return False return a < b and ( not args or less ( b , * args ) )
1482	def get_commands_to_run ( self ) : if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) commands . update ( self . _get_heron_support_processes ( ) ) return commands
3759	def atom_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count tot = sum ( things . values ( ) ) return { atom : value / tot for atom , value in things . iteritems ( ) }
11148	def create_package ( self , path = None , name = None , mode = None ) : assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:' if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) for dpath in sorted ( list ( self . walk_directories_path ( recursive = True ) ) ) : t = tarfile . TarInfo ( dpath ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) tarHandler . add ( os . path . join ( self . __path , dpath , self . __dirInfo ) , arcname = self . __dirInfo ) for fpath in self . walk_files_path ( recursive = True ) : relaPath , fname = os . path . split ( fpath ) tarHandler . add ( os . path . join ( self . __path , fpath ) , arcname = fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) , arcname = self . __fileInfo % fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileClass % fname ) , arcname = self . __fileClass % fname ) tarHandler . add ( os . path . join ( self . __path , self . __repoFile ) , arcname = ".pyrepinfo" ) tarHandler . close ( )
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
2163	def list ( self , group = None , host_filter = None , ** kwargs ) : if group : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'groups__in' , group ) , ) if host_filter : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'host_filter' , host_filter ) , ) return super ( Resource , self ) . list ( ** kwargs )
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
13067	def make_parents ( self , collection , lang = None ) : return [ { "id" : member . id , "label" : str ( member . get_label ( lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in collection . parents if member . get_label ( ) ]
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
5363	def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
3877	async def _on_event ( self , event_ ) : conv_id = event_ . conversation_id . id try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for event notification: %s' , conv_id ) else : self . _sync_timestamp = parsers . from_timestamp ( event_ . timestamp ) conv_event = conv . add_event ( event_ ) if conv_event is not None : await self . on_event . fire ( conv_event ) await conv . on_event . fire ( conv_event )
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
911	def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore
1988	def load_state ( self , key , delete = True ) : with self . load_stream ( key , binary = True ) as f : state = self . _serializer . deserialize ( f ) if delete : self . rm ( key ) return state
7408	def worker ( self ) : fullseqs = self . sample_loci ( ) liters = itertools . product ( * self . imap . values ( ) ) hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) nsites , nvar = count_var ( nex ) if nvar > self . minsnps : nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) weights . append ( treeorder ) rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
1803	def MOVBE ( cpu , dest , src ) : size = dest . size arg0 = dest . read ( ) temp = 0 for pos in range ( 0 , size , 8 ) : temp = ( temp << 8 ) | ( arg0 & 0xff ) arg0 = arg0 >> 8 dest . write ( arg0 )
3466	def gene_name_reaction_rule ( self ) : names = { i . id : i . name for i in self . _genes } ast = parse_gpr ( self . _gene_reaction_rule ) [ 0 ] return ast2str ( ast , names = names )
9150	def to_indra_statements ( self , * args , ** kwargs ) : graph = self . to_bel ( * args , ** kwargs ) return to_indra_statements ( graph )
5156	def _get_install_context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )
10804	def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : raise http . exceptions . MethodNotAllowed ( allowed )
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) mapview . markers [ mid ] = self self . marker . setTag ( mid ) for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) del self . options
12546	def apply_mask ( img , mask ) : from . mask import apply_mask vol , _ = apply_mask ( img , mask ) return vector_to_volume ( vol , read_img ( mask ) . get_data ( ) . astype ( bool ) )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
13368	def is_git_repo ( path ) : if path . startswith ( 'git@' ) or path . startswith ( 'https://' ) : return True if os . path . exists ( unipath ( path , '.git' ) ) : return True return False
5071	def get_configuration_value_for_site ( site , key , default = None ) : if hasattr ( site , 'configuration' ) : return site . configuration . get_value ( key , default ) return default
8361	def create_rcontext ( self , size , frame ) : self . frame = frame width , height = size meta_surface = cairo . RecordingSurface ( cairo . CONTENT_COLOR_ALPHA , ( 0 , 0 , width , height ) ) ctx = cairo . Context ( meta_surface ) return ctx
2472	def reset_file_stat ( self ) : self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
4919	def course_run_detail ( self , request , pk , course_id ) : enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
275	def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
13463	def add_event ( request ) : form = AddEventForm ( request . POST or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . sites = settings . SITE_ID instance . submitted_by = request . user instance . approved = True instance . slug = slugify ( instance . name ) instance . save ( ) messages . success ( request , 'Your event has been added.' ) return HttpResponseRedirect ( reverse ( 'events_index' ) ) return render ( request , 'happenings/event_form.html' , { 'form' : form , 'form_title' : 'Add an event' } )
3503	def add_loopless ( model , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) internal = [ i for i , r in enumerate ( model . reactions ) if not r . boundary ] s_int = create_stoichiometric_matrix ( model ) [ : , numpy . array ( internal ) ] n_int = nullspace ( s_int ) . T max_bound = max ( max ( abs ( b ) for b in r . bounds ) for r in model . reactions ) prob = model . problem to_add = [ ] for i in internal : rxn = model . reactions [ i ] indicator = prob . Variable ( "indicator_" + rxn . id , type = "binary" ) on_off_constraint = prob . Constraint ( rxn . flux_expression - max_bound * indicator , lb = - max_bound , ub = 0 , name = "on_off_" + rxn . id ) delta_g = prob . Variable ( "delta_g_" + rxn . id ) delta_g_range = prob . Constraint ( delta_g + ( max_bound + 1 ) * indicator , lb = 1 , ub = max_bound , name = "delta_g_range_" + rxn . id ) to_add . extend ( [ indicator , on_off_constraint , delta_g , delta_g_range ] ) model . add_cons_vars ( to_add ) for i , row in enumerate ( n_int ) : name = "nullspace_constraint_" + str ( i ) nullspace_constraint = prob . Constraint ( Zero , lb = 0 , ub = 0 , name = name ) model . add_cons_vars ( [ nullspace_constraint ] ) coefs = { model . variables [ "delta_g_" + model . reactions [ ridx ] . id ] : row [ i ] for i , ridx in enumerate ( internal ) if abs ( row [ i ] ) > zero_cutoff } model . constraints [ name ] . set_linear_coefficients ( coefs )
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
9593	def switch_to_frame ( self , frame_reference = None ) : if frame_reference is not None and type ( frame_reference ) not in [ int , WebElement ] : raise TypeError ( 'Type of frame_reference must be None or int or WebElement' ) self . _execute ( Command . SWITCH_TO_FRAME , { 'id' : frame_reference } )
13532	def ancestors_root ( self ) : if self . is_root ( ) : return [ ] ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors , True ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
8864	def icon_from_typename ( name , icon_type ) : ICONS = { 'CLASS' : ICON_CLASS , 'IMPORT' : ICON_NAMESPACE , 'STATEMENT' : ICON_VAR , 'FORFLOW' : ICON_VAR , 'FORSTMT' : ICON_VAR , 'WITHSTMT' : ICON_VAR , 'GLOBALSTMT' : ICON_VAR , 'MODULE' : ICON_NAMESPACE , 'KEYWORD' : ICON_KEYWORD , 'PARAM' : ICON_VAR , 'ARRAY' : ICON_VAR , 'INSTANCEELEMENT' : ICON_VAR , 'INSTANCE' : ICON_VAR , 'PARAM-PRIV' : ICON_VAR , 'PARAM-PROT' : ICON_VAR , 'FUNCTION' : ICON_FUNC , 'DEF' : ICON_FUNC , 'FUNCTION-PRIV' : ICON_FUNC_PRIVATE , 'FUNCTION-PROT' : ICON_FUNC_PROTECTED } ret_val = None icon_type = icon_type . upper ( ) if hasattr ( name , "string" ) : name = name . string if icon_type == "FORFLOW" or icon_type == "STATEMENT" : icon_type = "PARAM" if icon_type == "PARAM" or icon_type == "FUNCTION" : if name . startswith ( "__" ) : icon_type += "-PRIV" elif name . startswith ( "_" ) : icon_type += "-PROT" if icon_type in ICONS : ret_val = ICONS [ icon_type ] elif icon_type : _logger ( ) . warning ( "Unimplemented completion icon_type: %s" , icon_type ) return ret_val
3863	def _get_event_request_header ( self ) : otr_status = ( hangouts_pb2 . OFF_THE_RECORD_STATUS_OFF_THE_RECORD if self . is_off_the_record else hangouts_pb2 . OFF_THE_RECORD_STATUS_ON_THE_RECORD ) return hangouts_pb2 . EventRequestHeader ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , client_generated_id = self . _client . get_client_generated_id ( ) , expected_otr = otr_status , delivery_medium = self . _get_default_delivery_medium ( ) , )
1853	def SHR ( cpu , dest , src ) : OperandSize = dest . size count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) value = dest . read ( ) res = dest . write ( value >> count ) MASK = ( 1 << OperandSize ) - 1 SIGN_MASK = 1 << ( OperandSize - 1 ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( count != 0 , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
11433	def _shift_field_positions_global ( record , start , delta = 1 ) : if not delta : return for tag , fields in record . items ( ) : newfields = [ ] for field in fields : if field [ 4 ] < start : newfields . append ( field ) else : newfields . append ( tuple ( list ( field [ : 4 ] ) + [ field [ 4 ] + delta ] ) ) record [ tag ] = newfields
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
10051	def post ( self , pid , record , action ) : record = getattr ( record , action ) ( pid = pid ) db . session . commit ( ) db . session . refresh ( pid ) db . session . refresh ( record . model ) post_action . send ( current_app . _get_current_object ( ) , action = action , pid = pid , deposit = record ) response = self . make_response ( pid , record , 202 if action == 'publish' else 201 ) endpoint = '.{0}_item' . format ( pid . pid_type ) location = url_for ( endpoint , pid_value = pid . pid_value , _external = True ) response . headers . extend ( dict ( Location = location ) ) return response
6405	def cmp_features ( feat1 , feat2 ) : if feat1 < 0 or feat2 < 0 : return - 1.0 if feat1 == feat2 : return 1.0 magnitude = len ( _FEATURE_MASK ) featxor = feat1 ^ feat2 diffbits = 0 while featxor : if featxor & 0b1 : diffbits += 1 featxor >>= 1 return 1 - ( diffbits / ( 2 * magnitude ) )
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
9943	def delete_file ( self , path , prefixed_path , source_storage ) : if self . storage . exists ( prefixed_path ) : try : target_last_modified = self . storage . modified_time ( prefixed_path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : try : source_last_modified = source_storage . modified_time ( path ) except ( OSError , NotImplementedError , AttributeError ) : pass else : if self . local : full_path = self . storage . path ( prefixed_path ) else : full_path = None if ( target_last_modified . replace ( microsecond = 0 ) >= source_last_modified . replace ( microsecond = 0 ) ) : if not ( ( self . symlink and full_path and not os . path . islink ( full_path ) ) or ( not self . symlink and full_path and os . path . islink ( full_path ) ) ) : if prefixed_path not in self . unmodified_files : self . unmodified_files . append ( prefixed_path ) self . log ( "Skipping '%s' (not modified)" % path ) return False if self . dry_run : self . log ( "Pretending to delete '%s'" % path ) else : self . log ( "Deleting '%s'" % path ) self . storage . delete ( prefixed_path ) return True
9778	def login ( token , username , password ) : auth_client = PolyaxonClient ( ) . auth if username : if not password : password = click . prompt ( 'Please enter your password' , type = str , hide_input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = CredentialsConfig ( username = username , password = password ) try : access_code = auth_client . login ( credentials = credentials ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not login.' ) Printer . print_error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access_code : Printer . print_error ( "Failed to login" ) return else : if not token : token_url = "{}/app/token" . format ( auth_client . config . http_host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token_url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide_input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon_cli/commands/auth" ) return access_code = token . strip ( " " ) try : AuthConfigManager . purge ( ) user = PolyaxonClient ( ) . auth . get_user ( token = access_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access_token = AccessTokenConfig ( username = user . username , token = access_code ) AuthConfigManager . set_config ( access_token ) Printer . print_success ( "Login successful" ) server_version = get_server_version ( ) current_version = get_current_version ( ) log_handler = get_log_handler ( ) CliConfigManager . reset ( check_count = 0 , current_version = current_version , min_version = server_version . min_version , log_handler = log_handler )
8246	def morguefile ( query , n = 10 , top = 10 ) : from web import morguefile images = morguefile . search ( query ) [ : top ] path = choice ( images ) . download ( thumbnail = True , wait = 10 ) return ColorList ( path , n , name = query )
3683	def calculate ( self , T , method ) : r if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
9023	def write ( self , bytes_ ) : string = bytes_ . decode ( self . _encoding ) self . _file . write ( string )
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
2864	def readS8 ( self , register ) : result = self . readU8 ( register ) if result > 127 : result -= 256 return result
13615	def write ( ) : click . echo ( "Fantastic. Let's get started. " ) title = click . prompt ( "What's the title?" ) url = slugify ( title ) url = click . prompt ( "What's the URL?" , default = url ) click . echo ( "Got it. Creating %s..." % url ) scaffold_piece ( title , url )
7776	def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and vns and ns . getContent ( ) != vns . getContent ( ) ) : n = n . next continue if n . name == 'POBOX' : self . pobox = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( 'EXTADR' , 'EXTADD' ) : self . extadr = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'STREET' : self . street = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'LOCALITY' : self . locality = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'REGION' : self . region = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'PCODE' : self . pcode = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'CTRY' : self . ctry = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( "HOME" , "WORK" , "POSTAL" , "PARCEL" , "DOM" , "INTL" , "PREF" ) : self . type . append ( n . name . lower ( ) ) n = n . next if self . type == [ ] : self . type = [ "intl" , "postal" , "parcel" , "work" ] elif "dom" in self . type and "intl" in self . type : raise ValueError ( "Both 'dom' and 'intl' specified in vcard ADR" )
10283	def count_sources ( edge_iter : EdgeIterator ) -> Counter : return Counter ( u for u , _ , _ in edge_iter )
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
11793	def forward_checking ( csp , var , value , assignment , removals ) : "Prune neighbor values inconsistent with var=value." for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr_domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr_domains [ B ] : return False return True
9901	def _updateType ( self ) : data = self . _data ( ) if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
4511	def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
6927	def newcursor ( self , dictcursor = False ) : handle = hashlib . sha256 ( os . urandom ( 12 ) ) . hexdigest ( ) if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return ( self . cursors [ handle ] , handle )
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
10536	def get_category ( category_id ) : try : res = _pybossa_req ( 'get' , 'category' , category_id ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
7262	def get_most_recent_images ( self , results , types = [ ] , sensors = [ ] , N = 1 ) : if not len ( results ) : return None if types : results = [ r for r in results if r [ 'type' ] in types ] if sensors : results = [ r for r in results if r [ 'properties' ] . get ( 'sensorPlatformName' ) in sensors ] newlist = sorted ( results , key = lambda k : k [ 'properties' ] . get ( 'timestamp' ) , reverse = True ) return newlist [ : N ]
12982	def file ( file_object , start_on = None , ignore = ( ) , use_short = True , ** queries ) : return string ( file_object . read ( ) , start_on = start_on , ignore = ignore , use_short = use_short , ** queries )
9584	def write_var_data ( fd , data ) : fd . write ( struct . pack ( 'b3xI' , etypes [ 'miMATRIX' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : if element [ 'required' ] is True : content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : break if element_start is None and element [ 'required' ] is False : continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
11979	def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
11708	def print_parents ( self ) : if self . gender == female : title = 'Daughter' elif self . gender == male : title = 'Son' else : title = 'Child' p1 = self . parents [ 0 ] p2 = self . parents [ 1 ] template = '%s of %s, the %s, and %s, the %s.' print ( template % ( title , p1 . name , p1 . epithet , p2 . name , p2 . epithet ) )
5276	def _terminalSymbolsGenerator ( self ) : py2 = sys . version [ 0 ] < '3' UPPAs = list ( list ( range ( 0xE000 , 0xF8FF + 1 ) ) + list ( range ( 0xF0000 , 0xFFFFD + 1 ) ) + list ( range ( 0x100000 , 0x10FFFD + 1 ) ) ) for i in UPPAs : if py2 : yield ( unichr ( i ) ) else : yield ( chr ( i ) ) raise ValueError ( "To many input strings." )
11196	def compress ( obj , level = 6 , return_type = "bytes" ) : if isinstance ( obj , binary_type ) : b = zlib . compress ( obj , level ) elif isinstance ( obj , string_types ) : b = zlib . compress ( obj . encode ( "utf-8" ) , level ) else : b = zlib . compress ( pickle . dumps ( obj , protocol = 2 ) , level ) if return_type == "bytes" : return b elif return_type == "str" : return base64 . b64encode ( b ) . decode ( "utf-8" ) else : raise ValueError ( "'return_type' has to be one of 'bytes', 'str'!" )
11553	def disable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , ** kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , ** kwargs ) if isinstance ( filename , string_types ) : libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
3216	def get_classic_link ( vpc , ** conn ) : result = { } try : cl_result = describe_vpc_classic_link ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "Enabled" ] = cl_result [ "ClassicLinkEnabled" ] dns_result = describe_vpc_classic_link_dns_support ( VpcIds = [ vpc [ "id" ] ] , ** conn ) [ 0 ] result [ "DnsEnabled" ] = dns_result [ "ClassicLinkDnsSupported" ] except ClientError as e : if 'UnsupportedOperation' not in str ( e ) : raise e return result
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( ** space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
6716	def bootstrap ( self , force = 0 ) : force = int ( force ) if self . has_pip ( ) and not force : return r = self . local_renderer if r . env . bootstrap_method == GET_PIP : r . sudo ( 'curl --silent --show-error --retry 5 https://bootstrap.pypa.io/get-pip.py | python' ) elif r . env . bootstrap_method == EZ_SETUP : r . run ( 'wget http://peak.telecommunity.com/dist/ez_setup.py -O /tmp/ez_setup.py' ) with self . settings ( warn_only = True ) : r . sudo ( 'python /tmp/ez_setup.py -U setuptools' ) r . sudo ( 'easy_install -U pip' ) elif r . env . bootstrap_method == PYTHON_PIP : r . sudo ( 'apt-get install -y python-pip' ) else : raise NotImplementedError ( 'Unknown pip bootstrap method: %s' % r . env . bootstrap_method ) r . sudo ( 'pip {quiet_flag} install --upgrade pip' ) r . sudo ( 'pip {quiet_flag} install --upgrade virtualenv' )
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
11919	def index_row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup_url_kwarg ] ] . to_frame ( ) . T
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
1377	def check_java_home_set ( ) : if "JAVA_HOME" not in os . environ : Log . error ( "JAVA_HOME not set" ) return False java_path = get_java_path ( ) if os . path . isfile ( java_path ) and os . access ( java_path , os . X_OK ) : return True Log . error ( "JAVA_HOME/bin/java either does not exist or not an executable" ) return False
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
753	def _translateMetricsToJSON ( self , metrics , label ) : metricsDict = metrics def _mapNumpyValues ( obj ) : import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( "UNEXPECTED OBJ: %s; class=%s" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString
6043	def regular_to_sparse ( self ) : return mapping_util . regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse = self . regular_to_unmasked_sparse , unmasked_sparse_to_sparse = self . unmasked_sparse_to_sparse ) . astype ( 'int' )
9699	def event_choices ( events ) : if events is None : msg = "Please add some events in settings.WEBHOOK_EVENTS." raise ImproperlyConfigured ( msg ) try : choices = [ ( x , x ) for x in events ] except TypeError : msg = "settings.WEBHOOK_EVENTS must be an iterable object." raise ImproperlyConfigured ( msg ) return choices
705	def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , completed , completionReason , matured , numRecords ) : if results is None : metricResult = None else : metricResult = results [ 1 ] . values ( ) [ 0 ] errScore = self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = metricResult , completed = completed , completionReason = completionReason , matured = matured , numRecords = numRecords ) self . logger . debug ( 'Received progress on model %d: completed: %s, ' 'cmpReason: %s, numRecords: %d, errScore: %s' , modelID , completed , completionReason , numRecords , errScore ) ( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) self . logger . debug ( 'Best err score seen so far: %s on model %s' % ( bestResult , bestModelID ) )
5550	def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
808	def handleLogOutput ( self , output ) : if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut
7553	def nworker ( data , chunk ) : oldlimit = set_mkl_thread_limit ( 1 ) with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : , 0 ] smps = io5 [ "quartets" ] [ chunk : chunk + data . _chunksize ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smps . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rinvariants = np . zeros ( ( smps . shape [ 0 ] , 16 , 16 ) , dtype = np . uint16 ) for idx in xrange ( smps . shape [ 0 ] ) : sidx = smps [ idx ] seqs = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqs == seqs [ 0 ] , axis = 0 ) bidx , invar = calculate ( seqs , maparr , nmask , TESTS ) rquartets [ idx ] = smps [ idx ] [ bidx ] rinvariants [ idx ] = invar set_mkl_thread_limit ( oldlimit ) return rquartets , rinvariants
5218	def hist_file ( ticker : str , dt , typ = 'TRADE' ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return '' asset = ticker . split ( ) [ - 1 ] proper_ticker = ticker . replace ( '/' , '_' ) cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'
12310	def pull_stream ( self , uri , ** kwargs ) : return self . protocol . execute ( 'pullStream' , uri = uri , ** kwargs )
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( ** load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( ** load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( ** rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
1671	def ProcessConfigOverrides ( filename ) : abs_filename = os . path . abspath ( filename ) cfg_filters = [ ] keep_looking = True while keep_looking : abs_path , base_name = os . path . split ( abs_filename ) if not base_name : break cfg_file = os . path . join ( abs_path , "CPPLINT.cfg" ) abs_filename = abs_path if not os . path . isfile ( cfg_file ) : continue try : with open ( cfg_file ) as file_handle : for line in file_handle : line , _ , _ = line . partition ( '#' ) if not line . strip ( ) : continue name , _ , val = line . partition ( '=' ) name = name . strip ( ) val = val . strip ( ) if name == 'set noparent' : keep_looking = False elif name == 'filter' : cfg_filters . append ( val ) elif name == 'exclude_files' : if base_name : pattern = re . compile ( val ) if pattern . match ( base_name ) : _cpplint_state . PrintInfo ( 'Ignoring "%s": file excluded by ' '"%s". File path component "%s" matches pattern "%s"\n' % ( filename , cfg_file , base_name , val ) ) return False elif name == 'linelength' : global _line_length try : _line_length = int ( val ) except ValueError : _cpplint_state . PrintError ( 'Line length must be numeric.' ) elif name == 'extensions' : global _valid_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _valid_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'headers' : global _header_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _header_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'root' : global _root _root = val else : _cpplint_state . PrintError ( 'Invalid configuration option (%s) in file %s\n' % ( name , cfg_file ) ) except IOError : _cpplint_state . PrintError ( "Skipping config file '%s': Can't open for reading\n" % cfg_file ) keep_looking = False for cfg_filter in reversed ( cfg_filters ) : _AddFilters ( cfg_filter ) return True
1866	def PSHUFD ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) order = Operators . ZEXTEND ( op3 . read ( ) , size ) arg0 = arg0 & 0xffffffffffffffffffffffffffffffff00000000000000000000000000000000 arg0 |= ( ( arg1 >> ( ( ( order >> 0 ) & 3 ) * 32 ) ) & 0xffffffff ) arg0 |= ( ( arg1 >> ( ( ( order >> 2 ) & 3 ) * 32 ) ) & 0xffffffff ) << 32 arg0 |= ( ( arg1 >> ( ( ( order >> 4 ) & 3 ) * 32 ) ) & 0xffffffff ) << 64 arg0 |= ( ( arg1 >> ( ( ( order >> 6 ) & 3 ) * 32 ) ) & 0xffffffff ) << 96 op0 . write ( arg0 )
998	def printConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var , i ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c , i ] s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatFPRow ( aState , i )
7479	def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) os . mkdir ( data . tmpdir ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " building clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp" ) usort = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) async1 = "" if not os . path . exists ( usort ) or force : LOGGER . info ( "building reads file -- loading utemp file into mem" ) async1 = lbview . apply ( sort_seeds , * ( uhandle , usort ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async1 . ready ( ) : break else : time . sleep ( 0.1 ) async2 = lbview . apply ( count_seeds , usort ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 1 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async2 . ready ( ) : break else : time . sleep ( 0.1 ) nseeds = async2 . result ( ) async3 = lbview . apply ( sub_build_clustbits , * ( data , usort , nseeds ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 2 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async3 . ready ( ) : break else : time . sleep ( 0.1 ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 3 , printstr . format ( elapsed ) , spacer = data . _spacer ) print ( "" ) for job in [ async1 , async2 , async3 ] : try : if not job . successful ( ) : raise IPyradWarningExit ( job . result ( ) ) except AttributeError : pass
4387	def adsAddRoute ( net_id , ip_address ) : add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
12459	def main ( * args ) : r with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : return True pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : return True run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) return False
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
13899	def PushPopItem ( obj , key , value ) : if key in obj : old_value = obj [ key ] obj [ key ] = value yield value obj [ key ] = old_value else : obj [ key ] = value yield value del obj [ key ]
2337	def remove_indirect_links ( g , alg = "aracne" , ** kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , ** kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
7608	def get_all_cards ( self , timeout : int = None ) : url = self . api . CARDS return self . _get_model ( url , timeout = timeout )
1430	def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
4044	def publications ( self ) : if self . library_type != "users" : raise ze . CallDoesNotExist ( "This API call does not exist for group libraries" ) query_string = "/{t}/{u}/publications/items" return self . _build_query ( query_string )
8356	def _toUnicode ( self , data , encoding ) : if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
4692	def env ( ) : ipmi = cij . env_to_dict ( PREFIX , REQUIRED ) if ipmi is None : ipmi [ "USER" ] = "admin" ipmi [ "PASS" ] = "admin" ipmi [ "HOST" ] = "localhost" ipmi [ "PORT" ] = "623" cij . info ( "ipmi.env: USER: %s, PASS: %s, HOST: %s, PORT: %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] ) ) cij . env_export ( PREFIX , EXPORTED , ipmi ) return 0
5334	def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
4933	def get_content_id ( self , content_metadata_item ) : content_id = content_metadata_item . get ( 'key' , '' ) if content_metadata_item [ 'content_type' ] == 'program' : content_id = content_metadata_item . get ( 'uuid' , '' ) return content_id
7106	def export ( self , model_name , export_folder ) : for transformer in self . transformers : if isinstance ( transformer , MultiLabelBinarizer ) : joblib . dump ( transformer , join ( export_folder , "label.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , TfidfVectorizer ) : joblib . dump ( transformer , join ( export_folder , "tfidf.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , CountVectorizer ) : joblib . dump ( transformer , join ( export_folder , "count.transformer.bin" ) , protocol = 2 ) if isinstance ( transformer , NumberRemover ) : joblib . dump ( transformer , join ( export_folder , "number.transformer.bin" ) , protocol = 2 ) model = [ model for model in self . models if model . name == model_name ] [ 0 ] e = Experiment ( self . X , self . y , model . estimator , None ) model_filename = join ( export_folder , "model.bin" ) e . export ( model_filename )
4537	def wheel_helper ( pos , length , cycle_step ) : return wheel_color ( ( pos * len ( _WHEEL ) / length ) + cycle_step )
8007	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) stanza = Presence ( stanza_type = "error" , from_jid = self . from_jid , to_jid = self . to_jid , stanza_id = self . stanza_id , status = self . _status , show = self . _show , priority = self . _priority , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : stanza . add_payload ( payload ) return stanza
4261	def _restore_cache ( gallery ) : cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) try : if os . path . exists ( cachePath ) : with open ( cachePath , "rb" ) as cacheFile : gallery . exifCache = pickle . load ( cacheFile ) logger . debug ( "Loaded cache with %d entries" , len ( gallery . exifCache ) ) else : gallery . exifCache = { } except Exception as e : logger . warn ( "Could not load cache: %s" , e ) gallery . exifCache = { }
12857	def with_peer ( events ) : stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : stack . append ( obj ) yield obj , None elif obj [ 'type' ] == EXIT : yield obj , stack . pop ( ) else : yield obj , None
5705	def timeit ( method ) : def timed ( * args , ** kw ) : time_start = time . time ( ) result = method ( * args , ** kw ) time_end = time . time ( ) print ( 'timeit: %r %2.2f sec (%r, %r) ' % ( method . __name__ , time_end - time_start , str ( args ) [ : 20 ] , kw ) ) return result return timed
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
8198	def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , ** kwargs ) : data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , ** kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
1081	def tzname ( self ) : if self . _tzinfo is None : return None name = self . _tzinfo . tzname ( None ) _check_tzname ( name ) return name
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
12361	def send_request ( self , kind , url_components , ** kwargs ) : return self . api . send_request ( kind , self . resource_path , url_components , ** kwargs )
6983	def _legendre_dtr ( x , y , y_err , legendredeg = 10 ) : try : p = Legendre . fit ( x , y , legendredeg ) fit_y = p ( x ) except Exception as e : fit_y = npzeros_like ( y ) fitchisq = npsum ( ( ( fit_y - y ) * ( fit_y - y ) ) / ( y_err * y_err ) ) nparams = legendredeg + 1 fitredchisq = fitchisq / ( len ( y ) - nparams - 1 ) LOGINFO ( 'legendre detrend applied. chisq = %.5f, reduced chisq = %.5f' % ( fitchisq , fitredchisq ) ) return fit_y , fitchisq , fitredchisq
6737	def get_component_settings ( prefixes = None ) : prefixes = prefixes or [ ] assert isinstance ( prefixes , ( tuple , list ) ) , 'Prefixes must be a sequence type, not %s.' % type ( prefixes ) data = { } for name in prefixes : name = name . lower ( ) . strip ( ) for k in sorted ( env ) : if k . startswith ( '%s_' % name ) : new_k = k [ len ( name ) + 1 : ] data [ new_k ] = env [ k ] return data
1076	def _ymd2ord ( year , month , day ) : "year, month, day -> ordinal, considering 01-Jan-0001 as day 1." assert 1 <= month <= 12 , 'month must be in 1..12' dim = _days_in_month ( year , month ) assert 1 <= day <= dim , ( 'day must be in 1..%d' % dim ) return ( _days_before_year ( year ) + _days_before_month ( year , month ) + day )
10189	def consume ( self , event_type , no_ack = True , payload = True ) : assert event_type in self . events return current_queues . queues [ 'stats-{}' . format ( event_type ) ] . consume ( payload = payload )
5645	def make_views ( cls , conn ) : conn . execute ( 'DROP VIEW IF EXISTS main.day_trips' ) conn . execute ( 'CREATE VIEW day_trips AS ' 'SELECT day_trips2.*, trips.* ' 'FROM day_trips2 JOIN trips USING (trip_I);' ) conn . commit ( ) conn . execute ( 'DROP VIEW IF EXISTS main.day_stop_times' ) conn . execute ( 'CREATE VIEW day_stop_times AS ' 'SELECT day_trips2.*, trips.*, stop_times.*, ' 'day_trips2.day_start_ut+stop_times.arr_time_ds AS arr_time_ut, ' 'day_trips2.day_start_ut+stop_times.dep_time_ds AS dep_time_ut ' 'FROM day_trips2 ' 'JOIN trips USING (trip_I) ' 'JOIN stop_times USING (trip_I)' ) conn . commit ( )
8209	def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
1541	def build_and_submit ( self ) : class_dict = self . _construct_topo_class_dict ( ) topo_cls = TopologyType ( self . topology_name , ( Topology , ) , class_dict ) topo_cls . write ( )
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
5375	def simple_pattern_exists_in_gcs ( file_pattern , credentials = None ) : if '*' not in file_pattern : return _file_exists_in_gcs ( file_pattern , credentials ) if not file_pattern . startswith ( 'gs://' ) : raise ValueError ( 'file name must start with gs://' ) gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = file_pattern [ len ( 'gs://' ) : ] . split ( '/' , 1 ) if '*' in bucket_name : raise ValueError ( 'Wildcards may not appear in the bucket name' ) assert '*' in prefix prefix_no_wildcard = prefix [ : prefix . index ( '*' ) ] request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix_no_wildcard ) response = request . execute ( ) if 'items' not in response : return False items_list = [ i [ 'name' ] for i in response [ 'items' ] ] return any ( fnmatch . fnmatch ( i , prefix ) for i in items_list )
949	def _createPeriodicActivities ( self ) : periodicActivities = [ ] class MetricsReportCb ( object ) : def __init__ ( self , taskRunner ) : self . __taskRunner = taskRunner return def __call__ ( self ) : self . __taskRunner . _getAndEmitExperimentMetrics ( ) reportMetrics = PeriodicActivityRequest ( repeating = True , period = 1000 , cb = MetricsReportCb ( self ) ) periodicActivities . append ( reportMetrics ) class IterationProgressCb ( object ) : PROGRESS_UPDATE_PERIOD_TICKS = 1000 def __init__ ( self , taskLabel , requestedIterationCount , logger ) : self . __taskLabel = taskLabel self . __requestedIterationCount = requestedIterationCount self . __logger = logger self . __numIterationsSoFar = 0 def __call__ ( self ) : self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS self . __logger . debug ( "%s: ITERATION PROGRESS: %s of %s" % ( self . __taskLabel , self . __numIterationsSoFar , self . __requestedIterationCount ) ) iterationProgressCb = IterationProgressCb ( taskLabel = self . __task [ 'taskLabel' ] , requestedIterationCount = self . __task [ 'iterationCount' ] , logger = self . __logger ) iterationProgressReporter = PeriodicActivityRequest ( repeating = True , period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , cb = iterationProgressCb ) periodicActivities . append ( iterationProgressReporter ) return periodicActivities
11028	def _sse_content_with_protocol ( response , handler , ** sse_kwargs ) : protocol = SseProtocol ( handler , ** sse_kwargs ) finished = protocol . when_finished ( ) response . deliverBody ( protocol ) return finished , protocol
2101	def log ( s , header = '' , file = sys . stderr , nl = 1 , ** kwargs ) : if not settings . verbose : return if header : word_arr = s . split ( ' ' ) multi = [ ] word_arr . insert ( 0 , '%s:' % header . upper ( ) ) i = 0 while i < len ( word_arr ) : to_add = [ '***' ] count = 3 while count <= 79 : count += len ( word_arr [ i ] ) + 1 if count <= 79 : to_add . append ( word_arr [ i ] ) i += 1 if i == len ( word_arr ) : break if len ( to_add ) == 1 : to_add . append ( word_arr [ i ] ) i += 1 if i != len ( word_arr ) : count -= len ( word_arr [ i ] ) + 1 to_add . append ( '*' * ( 78 - count ) ) multi . append ( ' ' . join ( to_add ) ) s = '\n' . join ( multi ) lines = len ( multi ) else : lines = 1 if isinstance ( nl , int ) and nl > lines : s += '\n' * ( nl - lines ) return secho ( s , file = file , ** kwargs )
12895	def get_modes ( self ) : if not self . __modes : self . __modes = yield from self . handle_list ( self . API . get ( 'valid_modes' ) ) return self . __modes
9980	def extract_names ( source ) : if source is None : return None source = dedent ( source ) funcdef = find_funcdef ( source ) params = extract_params ( source ) names = [ ] if isinstance ( funcdef , ast . FunctionDef ) : stmts = funcdef . body elif isinstance ( funcdef , ast . Lambda ) : stmts = [ funcdef . body ] else : raise ValueError ( "must not happen" ) for stmt in stmts : for node in ast . walk ( stmt ) : if isinstance ( node , ast . Name ) : if node . id not in names and node . id not in params : names . append ( node . id ) return names
6575	def from_json_list ( cls , api_client , data ) : return [ cls . from_json ( api_client , item ) for item in data ]
9151	def _convert_coordinatelist ( input_obj ) : cdl = pgmagick . CoordinateList ( ) for obj in input_obj : cdl . append ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) return cdl
4223	def init_backend ( limit = None ) : backend . _limit = limit keyrings = filter ( limit , backend . get_all_keyring ( ) ) set_keyring ( load_env ( ) or load_config ( ) or max ( keyrings , default = fail . Keyring ( ) , key = backend . by_priority ) )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )
13237	def next_interval ( self , after = None ) : if after is None : after = timezone . now ( ) after = self . to_timezone ( after ) return next ( self . intervals ( range_start = after ) , None )
10552	def _forbidden_attributes ( obj ) : for key in list ( obj . data . keys ( ) ) : if key in list ( obj . reserved_keys . keys ( ) ) : obj . data . pop ( key ) return obj
1122	def action ( inner_rule , loc = None ) : def decorator ( mapper ) : @ llrule ( loc , inner_rule . expected ) def outer_rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : return result if isinstance ( result , tuple ) : return mapper ( parser , * result ) else : return mapper ( parser , result ) return outer_rule return decorator
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
12119	def headerHTML ( self , fname = None ) : if fname is None : fname = self . fname . replace ( ".abf" , "_header.html" ) html = "<html><body><code>" html += "<h2>abfinfo() for %s.abf</h2>" % self . ID html += self . abfinfo ( ) . replace ( "<" , "&lt;" ) . replace ( ">" , "&gt;" ) . replace ( "\n" , "<br>" ) html += "<h2>Header for %s.abf</h2>" % self . ID html += pprint . pformat ( self . header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "WRITING HEADER TO:" ) print ( fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( )
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : root = os . path . realpath ( root ) if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
12141	def load_dframe ( self , dframe ) : filename_series = dframe [ self . key ] loaded_data = filename_series . map ( self . filetype . data ) keys = [ list ( el . keys ( ) ) for el in loaded_data . values ] for key in set ( ) . union ( * keys ) : key_exists = key in dframe . columns if key_exists : self . warning ( "Appending '_data' suffix to data key %r to avoid" "overwriting existing metadata with the same name." % key ) suffix = '_data' if key_exists else '' dframe [ key + suffix ] = loaded_data . map ( lambda x : x . get ( key , np . nan ) ) return dframe
1578	def make_shell_logfile_data_url ( host , shell_port , instance_id , offset , length ) : return "http://%s:%d/filedata/log-files/%s.log.0?offset=%s&length=%s" % ( host , shell_port , instance_id , offset , length )
12840	async def async_connect ( self ) : if self . _waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . _waiters or self . max_connections and ( len ( self . _in_use ) >= self . max_connections ) : waiter = asyncio . Future ( loop = self . _loop ) self . _waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . _waiters . remove ( waiter ) self . connect ( ) return self . _state . conn
10246	def count_confidences ( graph : BELGraph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for _ , _ , data in graph . edges ( data = True ) if CITATION in data )
13369	def is_home_environment ( path ) : home = unipath ( os . environ . get ( 'CPENV_HOME' , '~/.cpenv' ) ) path = unipath ( path ) return path . startswith ( home )
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
12175	def plotAllSweeps ( abfFile ) : r = io . AxonIO ( filename = abfFile ) bl = r . read_block ( lazy = False , cascade = True ) print ( abfFile + "\nplotting %d sweeps..." % len ( bl . segments ) ) plt . figure ( figsize = ( 12 , 10 ) ) plt . title ( abfFile ) for sweep in range ( len ( bl . segments ) ) : trace = bl . segments [ sweep ] . analogsignals [ 0 ] plt . plot ( trace . times - trace . times [ 0 ] , trace . magnitude , alpha = .5 ) plt . ylabel ( trace . dimensionality ) plt . xlabel ( "seconds" ) plt . show ( ) plt . close ( )
9259	def delete_by_time ( self , issues , older_tag , newer_tag ) : if not older_tag and not newer_tag : return copy . deepcopy ( issues ) newer_tag_time = self . get_time_of_tag ( newer_tag ) older_tag_time = self . get_time_of_tag ( older_tag ) filtered = [ ] for issue in issues : if issue . get ( 'actual_date' ) : rslt = older_tag_time < issue [ 'actual_date' ] <= newer_tag_time if rslt : filtered . append ( copy . deepcopy ( issue ) ) return filtered
3481	def read_sbml_model ( filename , number = float , f_replace = F_REPLACE , set_missing_bounds = False , ** kwargs ) : try : doc = _get_doc_from_filename ( filename ) return _sbml_to_model ( doc , number = number , f_replace = f_replace , set_missing_bounds = set_missing_bounds , ** kwargs ) except IOError as e : raise e except Exception : LOGGER . error ( traceback . print_exc ( ) ) raise CobraSBMLError ( "Something went wrong reading the SBML model. Most likely the SBML" " model is not valid. Please check that your model is valid using " "the `cobra.io.sbml.validate_sbml_model` function or via the " "online validator at http://sbml.org/validator .\n" "\t`(model, errors) = validate_sbml_model(filename)`" "\nIf the model is valid and cannot be read please open an issue " "at https://github.com/opencobra/cobrapy/issues ." )
7700	def verify_roster_set ( self , fix = False , settings = None ) : try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
2902	def complete_next ( self , pick_up = True , halt_on_manual = True ) : blacklist = [ ] if pick_up and self . last_task is not None : try : iter = Task . Iterator ( self . last_task , Task . READY ) task = next ( iter ) except StopIteration : task = None self . last_task = None if task is not None : if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) for task in Task . Iterator ( self . task_tree , Task . READY ) : for blacklisted_task in blacklist : if task . _is_descendant_of ( blacklisted_task ) : continue if not ( halt_on_manual and task . task_spec . manual ) : if task . complete ( ) : self . last_task = task return True blacklist . append ( task ) for task in Task . Iterator ( self . task_tree , Task . WAITING ) : task . task_spec . _update ( task ) if not task . _has_state ( Task . WAITING ) : self . last_task = task return True return False
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
331	def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
5958	def break_array ( a , threshold = numpy . pi , other = None ) : assert len ( a . shape ) == 1 , "Only 1D arrays supported" if other is not None and a . shape != other . shape : raise ValueError ( "arrays must be of identical shape" ) breaks = numpy . where ( numpy . abs ( numpy . diff ( a ) ) >= threshold ) [ 0 ] breaks += 1 m = len ( breaks ) b = numpy . empty ( ( len ( a ) + m ) ) b_breaks = breaks + numpy . arange ( m ) mask = numpy . zeros_like ( b , dtype = numpy . bool ) mask [ b_breaks ] = True b [ ~ mask ] = a b [ mask ] = numpy . NAN if other is not None : c = numpy . empty_like ( b ) c [ ~ mask ] = other c [ mask ] = numpy . NAN ma_c = numpy . ma . array ( c , mask = mask ) else : ma_c = None return numpy . ma . array ( b , mask = mask ) , ma_c
1836	def JG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
9789	def projects ( ctx , page ) : user = get_username_or_local ( ctx . obj . get ( 'username' ) ) page = page or 1 try : response = PolyaxonClient ( ) . bookmark . projects ( username = user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No bookmarked projects found for user `{}`.' . format ( user ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
3620	def unregister ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) del self . __registered_models [ model ] post_save . disconnect ( self . __post_save_receiver , model ) pre_delete . disconnect ( self . __pre_delete_receiver , model ) logger . info ( 'UNREGISTER %s' , model )
986	def mmPrettyPrintConnections ( self ) : text = "" text += ( "Segments: (format => " "(#) [(source cell=permanence ...), ...]\n" ) text += "------------------------------------\n" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = "({0})" . format ( " " . join ( synapseStringList ) ) text += ( "Column {0:3} / Cell {1:3}:\t({2}) {3}\n" . format ( column , cell , len ( segmentDict . values ( ) ) , "[{0}]" . format ( ", " . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : text += "\n" text += "------------------------------------\n" return text
13751	def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
8040	def is_public ( self ) : for decorator in self . decorators : if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , ** kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
1428	def build_extra_args_dict ( cl_args ) : component_parallelism = cl_args [ 'component_parallelism' ] runtime_configs = cl_args [ 'runtime_config' ] container_number = cl_args [ 'container_number' ] if ( component_parallelism and runtime_configs ) or ( container_number and runtime_configs ) : raise Exception ( "(component-parallelism or container_num) and runtime-config " + "can't be updated at the same time" ) dict_extra_args = { } nothing_set = True if component_parallelism : dict_extra_args . update ( { 'component_parallelism' : component_parallelism } ) nothing_set = False if container_number : dict_extra_args . update ( { 'container_number' : container_number } ) nothing_set = False if runtime_configs : dict_extra_args . update ( { 'runtime_config' : runtime_configs } ) nothing_set = False if nothing_set : raise Exception ( "Missing arguments --component-parallelism or --runtime-config or --container-number" ) if cl_args [ 'dry_run' ] : dict_extra_args . update ( { 'dry_run' : True } ) if 'dry_run_format' in cl_args : dict_extra_args . update ( { 'dry_run_format' : cl_args [ "dry_run_format" ] } ) return dict_extra_args
9368	def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
13148	def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : base_file_names = data [ 'docnames' ] except KeyError : base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
3120	def value_to_string ( self , obj ) : value = self . _get_val_from_obj ( obj ) return self . get_prep_value ( value )
3429	def add_boundary ( self , metabolite , type = "exchange" , reaction_id = None , lb = None , ub = None , sbo_term = None ) : ub = CONFIGURATION . upper_bound if ub is None else ub lb = CONFIGURATION . lower_bound if lb is None else lb types = { "exchange" : ( "EX" , lb , ub , sbo_terms [ "exchange" ] ) , "demand" : ( "DM" , 0 , ub , sbo_terms [ "demand" ] ) , "sink" : ( "SK" , lb , ub , sbo_terms [ "sink" ] ) } if type == "exchange" : external = find_external_compartment ( self ) if metabolite . compartment != external : raise ValueError ( "The metabolite is not an external metabolite" " (compartment is `%s` but should be `%s`). " "Did you mean to add a demand or sink? " "If not, either change its compartment or " "rename the model compartments to fix this." % ( metabolite . compartment , external ) ) if type in types : prefix , lb , ub , default_term = types [ type ] if reaction_id is None : reaction_id = "{}_{}" . format ( prefix , metabolite . id ) if sbo_term is None : sbo_term = default_term if reaction_id is None : raise ValueError ( "Custom types of boundary reactions require a custom " "identifier. Please set the `reaction_id`." ) if reaction_id in self . reactions : raise ValueError ( "Boundary reaction '{}' already exists." . format ( reaction_id ) ) name = "{} {}" . format ( metabolite . name , type ) rxn = Reaction ( id = reaction_id , name = name , lower_bound = lb , upper_bound = ub ) rxn . add_metabolites ( { metabolite : - 1 } ) if sbo_term : rxn . annotation [ "sbo" ] = sbo_term self . add_reactions ( [ rxn ] ) return rxn
7621	def hierarchy ( ref , est , ** kwargs ) : r namespace = 'multi_segment' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_hier , ref_hier_lab = hierarchy_flatten ( ref ) est_hier , est_hier_lab = hierarchy_flatten ( est ) return mir_eval . hierarchy . evaluate ( ref_hier , ref_hier_lab , est_hier , est_hier_lab , ** kwargs )
13688	def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
3566	def read_value ( self ) : pass self . _value_read . clear ( ) self . _device . _peripheral . readValueForDescriptor ( self . _descriptor ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _value
100	def compute_line_intersection_point ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 ) : def _make_line ( p1 , p2 ) : A = ( p1 [ 1 ] - p2 [ 1 ] ) B = ( p2 [ 0 ] - p1 [ 0 ] ) C = ( p1 [ 0 ] * p2 [ 1 ] - p2 [ 0 ] * p1 [ 1 ] ) return A , B , - C L1 = _make_line ( ( x1 , y1 ) , ( x2 , y2 ) ) L2 = _make_line ( ( x3 , y3 ) , ( x4 , y4 ) ) D = L1 [ 0 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 0 ] Dx = L1 [ 2 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 2 ] Dy = L1 [ 0 ] * L2 [ 2 ] - L1 [ 2 ] * L2 [ 0 ] if D != 0 : x = Dx / D y = Dy / D return x , y else : return False
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
6267	def set_time ( self , value : float ) : if value < 0 : value = 0 self . offset += self . get_time ( ) - value
115	def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )
3817	async def _pb_request ( self , endpoint , request_pb , response_pb ) : logger . debug ( 'Sending Protocol Buffer request %s:\n%s' , endpoint , request_pb ) res = await self . _base_request ( 'https://clients6.google.com/chat/v1/{}' . format ( endpoint ) , 'application/x-protobuf' , 'proto' , request_pb . SerializeToString ( ) ) try : response_pb . ParseFromString ( base64 . b64decode ( res . body ) ) except binascii . Error as e : raise exceptions . NetworkError ( 'Failed to decode base64 response: {}' . format ( e ) ) except google . protobuf . message . DecodeError as e : raise exceptions . NetworkError ( 'Failed to decode Protocol Buffer response: {}' . format ( e ) ) logger . debug ( 'Received Protocol Buffer response:\n%s' , response_pb ) status = response_pb . response_header . status if status != hangouts_pb2 . RESPONSE_STATUS_OK : description = response_pb . response_header . error_description raise exceptions . NetworkError ( 'Request failed with status {}: \'{}\'' . format ( status , description ) )
683	def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
12723	def max_forces ( self , max_forces ) : _set_params ( self . ode_obj , 'FMax' , max_forces , self . ADOF + self . LDOF )
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common locals_ [ 'shell' ] = shell for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
7628	def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservation' ] ) for key in [ 'value' , 'confidence' ] : try : sch [ 'properties' ] [ key ] = __NAMESPACE__ [ ns_key ] [ key ] except KeyError : pass return sch
7564	def get_client ( cluster_id , profile , engines , timeout , cores , quiet , spacer , ** kwargs ) : save_stdout = sys . stdout save_stderr = sys . stderr sys . stdout = cStringIO . StringIO ( ) sys . stderr = cStringIO . StringIO ( ) connection_string = "{}establishing parallel connection:" . format ( spacer ) try : if profile not in [ None , "default" ] : args = { 'profile' : profile , "timeout" : timeout } else : clusterargs = [ cluster_id , profile , timeout ] argnames = [ "cluster_id" , "profile" , "timeout" ] args = { key : value for key , value in zip ( argnames , clusterargs ) } ipyclient = ipp . Client ( ** args ) sys . stdout = save_stdout sys . stderr = save_stderr if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : if not quiet : print ( connection_string ) for _ in range ( 6000 ) : initid = len ( ipyclient ) time . sleep ( 0.01 ) if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : if cores : time . sleep ( 0.1 ) if initid == cores : break if initid : time . sleep ( 3 ) if len ( ipyclient ) == initid : break else : if cores : if initid == cores : break else : if initid : break except KeyboardInterrupt as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst except IOError as inst : sys . stdout = save_stdout sys . stderr = save_stderr if "ipyrad-cli-" in cluster_id : raise IPyradWarningExit ( NO_IPCLUSTER_CLI ) else : raise IPyradWarningExit ( NO_IPCLUSTER_API ) except ( ipp . TimeoutError , ipp . NoEnginesRegistered ) as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst except Exception as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst finally : sys . stdout = save_stdout sys . stderr = save_stderr return ipyclient
12687	def get_notification_language ( user ) : if getattr ( settings , "NOTIFICATION_LANGUAGE_MODULE" , False ) : try : app_label , model_name = settings . NOTIFICATION_LANGUAGE_MODULE . split ( "." ) model = models . get_model ( app_label , model_name ) language_model = model . _default_manager . get ( user__id__exact = user . id ) if hasattr ( language_model , "language" ) : return language_model . language except ( ImportError , ImproperlyConfigured , model . DoesNotExist ) : raise LanguageStoreNotAvailable raise LanguageStoreNotAvailable
5229	def _load_yaml_ ( file_name ) : if not os . path . exists ( file_name ) : return dict ( ) with open ( file_name , 'r' , encoding = 'utf-8' ) as fp : return YAML ( ) . load ( stream = fp )
9388	def plot_diff ( self , graphing_library = 'matplotlib' ) : diff_datasource = sorted ( set ( self . reports [ 0 ] . datasource ) & set ( self . reports [ 1 ] . datasource ) ) graphed = False for submetric in diff_datasource : baseline_csv = naarad . utils . get_default_csv ( self . reports [ 0 ] . local_location , ( submetric + '.percentiles' ) ) current_csv = naarad . utils . get_default_csv ( self . reports [ 1 ] . local_location , ( submetric + '.percentiles' ) ) if ( not ( naarad . utils . is_valid_file ( baseline_csv ) & naarad . utils . is_valid_file ( current_csv ) ) ) : continue baseline_plot = PD ( input_csv = baseline_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'baseline' , x_label = 'Percentiles' ) current_plot = PD ( input_csv = current_csv , csv_column = 1 , series_name = submetric , y_label = submetric , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' , plot_label = 'current' , x_label = 'Percentiles' ) graphed , div_file = Diff . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( [ baseline_plot , current_plot ] , os . path . join ( self . output_directory , self . resource_path ) , self . resource_path , ( submetric + '.diff' ) ) if graphed : self . plot_files . append ( div_file ) return True
2737	def reserve ( self , * args , ** kwargs ) : data = self . get_data ( 'floating_ips/' , type = POST , params = { 'region' : self . region_slug } ) if data : self . ip = data [ 'floating_ip' ] [ 'ip' ] self . region = data [ 'floating_ip' ] [ 'region' ] return self
5842	def get_design_run_status ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_status ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) status = response [ "data" ] return ProcessStatus ( result = status . get ( "result" ) , progress = status . get ( "progress" ) , status = status . get ( "status" ) , messages = status . get ( "messages" ) )
8505	def _default_value_only ( self ) : line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return '' return match . group ( 1 )
4013	def _ensure_managed_repos_dir_exists ( ) : if not os . path . exists ( constants . REPOS_DIR ) : os . makedirs ( constants . REPOS_DIR )
1426	def getInstanceJstack ( self , topology_info , instance_id ) : pid_response = yield getInstancePid ( topology_info , instance_id ) try : http_client = tornado . httpclient . AsyncHTTPClient ( ) pid_json = json . loads ( pid_response ) pid = pid_json [ 'stdout' ] . strip ( ) if pid == '' : raise Exception ( 'Failed to get pid' ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/jstack/%s" % ( endpoint , pid ) response = yield http_client . fetch ( url ) Log . debug ( "HTTP call for url: %s" , url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
6406	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 return ( len ( src ) / len ( tar ) if len ( src ) < len ( tar ) else len ( tar ) / len ( src ) )
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
2444	def set_annotation_spdx_id ( self , doc , spdx_id ) : if len ( doc . annotations ) != 0 : if not self . annotation_spdx_id_set : self . annotation_spdx_id_set = True doc . annotations [ - 1 ] . spdx_id = spdx_id return True else : raise CardinalityError ( 'Annotation::SPDXREF' ) else : raise OrderError ( 'Annotation::SPDXREF' )
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
963	def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal
10364	def has_degradation_increases_activity ( data : Dict ) -> bool : return part_has_modifier ( data , SUBJECT , DEGRADATION ) and part_has_modifier ( data , OBJECT , ACTIVITY )
6582	def play_station ( self , station ) : for song in iterate_forever ( station . get_playlist ) : try : self . play ( song ) except StopIteration : self . stop ( ) return
11153	def sha512file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha512 , nbytes = nbytes , chunk_size = chunk_size )
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
6187	def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
381	def zca_whitening ( x , principal_components ) : flatx = np . reshape ( x , ( x . size ) ) whitex = np . dot ( flatx , principal_components ) x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) return x
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
8547	def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response
6894	def parallel_starfeatures_lcdir ( lcdir , outdir , lc_catalog_pickle , neighbor_radius_arcsec , fileglob = None , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS , recursive = True ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting starfeatures...' % len ( matching ) ) return parallel_starfeatures ( matching , outdir , lc_catalog_pickle , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , maxobjects = maxobjects , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
12728	def axes ( self , axes ) : self . lmotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
6369	def recall ( self ) : r if self . _tp + self . _fn == 0 : return float ( 'NaN' ) return self . _tp / ( self . _tp + self . _fn )
3479	def _clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
13411	def removeLogbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . logList : if len ( logs ) == 0 or logs == "All" : del self . logList [ type ] else : for logbook in logs : if logbook in self . logList [ type ] : self . logList [ type ] . remove ( logbook ) self . changeLogType ( )
2123	def disassociate_failure_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'failure' ) , parent , child )
4000	def _mount_repo ( repo , wait_for_server = False ) : check_call_on_vm ( 'sudo mkdir -p {}' . format ( repo . vm_path ) ) if wait_for_server : for i in range ( 0 , 10 ) : try : _run_mount_command ( repo ) return except CalledProcessError as e : if 'Connection refused' in e . output : logging . info ( 'Failed to mount repo; waiting for nfsd to restart' ) time . sleep ( 1 ) else : logging . info ( e . output ) raise e log_to_client ( 'Failed to mount repo {}' . format ( repo . short_name ) ) raise RuntimeError ( 'Unable to mount repo with NFS' ) else : _run_mount_command ( repo )
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
10383	def main ( ) : logging . basicConfig ( level = logging . INFO ) log . setLevel ( logging . INFO ) bms_base = get_bms_base ( ) neurommsig_base = get_neurommsig_base ( ) neurommsig_excel_dir = os . path . join ( neurommsig_base , 'resources' , 'excels' , 'neurommsig' ) nift_values = get_nift_values ( ) log . info ( 'Starting Alzheimers' ) ad_path = os . path . join ( neurommsig_excel_dir , 'alzheimers' , 'alzheimers.xlsx' ) ad_df = preprocess ( ad_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'alzheimers' , 'neurommsigdb_ad.bel' ) , 'w' ) as ad_file : write_neurommsig_bel ( ad_file , ad_df , mesh_alzheimer , nift_values ) log . info ( 'Starting Parkinsons' ) pd_path = os . path . join ( neurommsig_excel_dir , 'parkinsons' , 'parkinsons.xlsx' ) pd_df = preprocess ( pd_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'parkinsons' , 'neurommsigdb_pd.bel' ) , 'w' ) as pd_file : write_neurommsig_bel ( pd_file , pd_df , mesh_parkinson , nift_values )
3940	async def listen ( self ) : retries = 0 need_new_sid = True while retries <= self . _max_retries : if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) logger . error ( 'Ran out of retries for long-polling request' )
13674	def add_directory ( self , * args , ** kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
13473	def loop_in_background ( interval , callback ) : loop = GeventLoop ( interval , callback ) loop . start ( ) try : yield loop finally : if loop . has_started ( ) : loop . stop ( )
9841	def __field ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'component' ) : component = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'value' ) : raise DXParseError ( 'field: "value" expected' ) classid = self . __consume ( ) . value ( ) try : self . currentobject [ 'components' ] [ component ] = classid except KeyError : self . currentobject [ 'components' ] = { component : classid } else : raise DXParseError ( 'field: ' + str ( tok ) + ' not recognized.' )
5469	def get_last_update ( op ) : last_update = get_end_time ( op ) if not last_update : last_event = get_last_event ( op ) if last_event : last_update = last_event [ 'timestamp' ] if not last_update : last_update = get_create_time ( op ) return last_update
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
823	def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )
370	def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x
6035	def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
4900	def handle ( self , * args , ** options ) : if not CourseEnrollment : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) days , enterprise_customer = self . parse_arguments ( * args , ** options ) if enterprise_customer : try : lrs_configuration = XAPILRSConfiguration . objects . get ( active = True , enterprise_customer = enterprise_customer ) except XAPILRSConfiguration . DoesNotExist : raise CommandError ( 'No xAPI Configuration found for "{enterprise_customer}"' . format ( enterprise_customer = enterprise_customer . name ) ) self . send_xapi_statements ( lrs_configuration , days ) else : for lrs_configuration in XAPILRSConfiguration . objects . filter ( active = True ) : self . send_xapi_statements ( lrs_configuration , days )
8575	def update_nic ( self , datacenter_id , server_id , nic_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
10375	def calculate_concordance_helper ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , ) -> Tuple [ int , int , int , int ] : scores = defaultdict ( int ) for u , v , k , d in graph . edges ( keys = True , data = True ) : c = edge_concords ( graph , u , v , k , d , key , cutoff = cutoff ) scores [ c ] += 1 return ( scores [ Concordance . correct ] , scores [ Concordance . incorrect ] , scores [ Concordance . ambiguous ] , scores [ Concordance . unassigned ] , )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
2280	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_ccdr ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
7697	def from_xml ( cls , element ) : if element . tag != ITEM_TAG : raise ValueError ( "{0!r} is not a roster item" . format ( element ) ) try : jid = JID ( element . get ( "jid" ) ) except ValueError : raise BadRequestProtocolError ( u"Bad item JID" ) subscription = element . get ( "subscription" ) ask = element . get ( "ask" ) name = element . get ( "name" ) duplicate_group = False groups = set ( ) for child in element : if child . tag != GROUP_TAG : continue group = child . text if group is None : group = u"" if group in groups : duplicate_group = True else : groups . add ( group ) approved = element . get ( "approved" ) if approved == "true" : approved = True elif approved in ( "false" , None ) : approved = False else : logger . debug ( "RosterItem.from_xml: got unknown 'approved':" " {0!r}, changing to False" . format ( approved ) ) approved = False result = cls ( jid , name , groups , subscription , ask , approved ) result . _duplicate_group = duplicate_group return result
12348	def stitch_coordinates ( self , well_row = 0 , well_column = 0 ) : well = [ w for w in self . wells if attribute ( w , 'u' ) == well_column and attribute ( w , 'v' ) == well_row ] if len ( well ) == 1 : well = well [ 0 ] tile = os . path . join ( well , 'TileConfiguration.registered.txt' ) with open ( tile ) as f : data = [ x . strip ( ) for l in f . readlines ( ) if l [ 0 : 7 ] == 'image--' for x in l . split ( ';' ) ] coordinates = ( ast . literal_eval ( x ) for x in data [ 2 : : 3 ] ) coordinates = sum ( coordinates , ( ) ) attr = tuple ( attributes ( x ) for x in data [ 0 : : 3 ] ) return coordinates [ 0 : : 2 ] , coordinates [ 1 : : 2 ] , attr else : print ( 'leicaexperiment stitch_coordinates' '({}, {}) Well not found' . format ( well_row , well_column ) )
12633	def calculate_file_distances ( dicom_files , field_weights = None , dist_method_cls = None , ** kwargs ) : if dist_method_cls is None : dist_method = LevenshteinDicomFileDistance ( field_weights ) else : try : dist_method = dist_method_cls ( field_weights = field_weights , ** kwargs ) except : log . exception ( 'Could not instantiate {} object with field_weights ' 'and {}' . format ( dist_method_cls , kwargs ) ) dist_dtype = np . float16 n_files = len ( dicom_files ) try : file_dists = np . zeros ( ( n_files , n_files ) , dtype = dist_dtype ) except MemoryError as mee : import scipy . sparse file_dists = scipy . sparse . lil_matrix ( ( n_files , n_files ) , dtype = dist_dtype ) for idxi in range ( n_files ) : dist_method . set_dicom_file1 ( dicom_files [ idxi ] ) for idxj in range ( idxi + 1 , n_files ) : dist_method . set_dicom_file2 ( dicom_files [ idxj ] ) if idxi != idxj : file_dists [ idxi , idxj ] = dist_method . transform ( ) return file_dists
8111	def search_blogs ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_BLOGS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
10702	def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
8426	def hue_pal ( h = .01 , l = .6 , s = .65 , color_space = 'hls' ) : if not all ( [ 0 <= val <= 1 for val in ( h , l , s ) ] ) : msg = ( "hue_pal expects values to be between 0 and 1. " " I got h={}, l={}, s={}" . format ( h , l , s ) ) raise ValueError ( msg ) if color_space not in ( 'hls' , 'husl' ) : msg = "color_space should be one of ['hls', 'husl']" raise ValueError ( msg ) name = '{}_palette' . format ( color_space ) palette = globals ( ) [ name ] def _hue_pal ( n ) : colors = palette ( n , h = h , l = l , s = s ) return [ mcolors . rgb2hex ( c ) for c in colors ] return _hue_pal
10121	def from_dict ( cls , spec ) : spec = spec . copy ( ) center = spec . pop ( 'center' , None ) radius = spec . pop ( 'radius' , None ) if center and radius : return cls . circle ( center , radius , ** spec ) vertices = spec . pop ( 'vertices' ) if len ( vertices ) == 2 : return cls . rectangle ( vertices , ** spec ) return cls ( vertices , ** spec )
6510	def _parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = "iso8859-1" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( "iso8859-1" ) . decode ( "windows-1252" ) self . _eat_name_line ( line . strip ( ) )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
7092	def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
4202	def aryule ( X , order , norm = 'biased' , allow_singularity = True ) : r assert norm in [ 'biased' , 'unbiased' ] r = CORRELATION ( X , maxlags = order , norm = norm ) A , P , k = LEVINSON ( r , allow_singularity = allow_singularity ) return A , P , k
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
3859	def update_conversation ( self , conversation ) : new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
9019	def _rows ( self , spec ) : rows = self . new_row_collection ( ) for row in spec : rows . append ( self . _row ( row ) ) return rows
10028	def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return elif me in self . __readers : if self . __upgradewritercount : raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : if self . __upgradewritercount : if upgradewriter : self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return elif self . __pendingwriters [ 0 ] is me : self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : if upgradewriter : self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
12523	def check_call ( cmd_args ) : p = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) ( output , err ) = p . communicate ( ) return output
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
13095	def callback ( self , event ) : if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) self . ips . remove ( host ) self . write_targets ( )
11001	def _kpad ( self , field , finalshape , zpad = False , norm = True ) : currshape = np . array ( field . shape ) if any ( finalshape < currshape ) : raise IndexError ( "PSF tile size is less than minimum support size" ) d = finalshape - currshape o = d % 2 d = np . floor_divide ( d , 2 ) if not zpad : o [ 0 ] = 0 axes = None pad = tuple ( ( d [ i ] + o [ i ] , d [ i ] ) for i in [ 0 , 1 , 2 ] ) rpsf = np . pad ( field , pad , mode = 'constant' , constant_values = 0 ) rpsf = np . fft . ifftshift ( rpsf , axes = axes ) kpsf = fft . rfftn ( rpsf , ** fftkwargs ) if norm : kpsf /= kpsf [ 0 , 0 , 0 ] return kpsf
13336	def module_resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , VirtualEnvironment ) : env = resolver . resolved [ 0 ] mod = env . get_module ( path ) if mod : return mod raise ResolveError
3953	def get_last_result ( self ) : result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
13388	def ttl ( self , response ) : if response . code != 200 : return 0 if not self . request . method in [ 'GET' , 'HEAD' , 'OPTIONS' ] : return 0 try : pragma = self . request . headers [ 'pragma' ] if pragma == 'no-cache' : return 0 except KeyError : pass try : cache_control = self . request . headers [ 'cache-control' ] for option in [ 'private' , 'no-cache' , 'no-store' , 'must-revalidate' , 'proxy-revalidate' ] : if cache_control . find ( option ) : return 0 options = parse_cache_control ( cache_control ) try : return int ( options [ 's-maxage' ] ) except KeyError : pass try : return int ( options [ 'max-age' ] ) except KeyError : pass if 's-maxage' in options : max_age = options [ 's-maxage' ] if max_age < ttl : ttl = max_age if 'max-age' in options : max_age = options [ 'max-age' ] if max_age < ttl : ttl = max_age return ttl except KeyError : pass try : expires = self . request . headers [ 'expires' ] return time . mktime ( time . strptime ( expires , '%a, %d %b %Y %H:%M:%S' ) ) - time . time ( ) except KeyError : pass
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
1993	def load_state ( self , state_id , delete = True ) : return self . _store . load_state ( f'{self._prefix}{state_id:08x}{self._suffix}' , delete = delete )
5902	def prehook ( self , ** kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
10570	def get_suggested_filename ( metadata ) : if metadata . get ( 'title' ) and metadata . get ( 'track_number' ) : suggested_filename = '{track_number:0>2} {title}' . format ( ** metadata ) elif metadata . get ( 'title' ) and metadata . get ( 'trackNumber' ) : suggested_filename = '{trackNumber:0>2} {title}' . format ( ** metadata ) elif metadata . get ( 'title' ) and metadata . get ( 'tracknumber' ) : suggested_filename = '{tracknumber:0>2} {title}' . format ( ** metadata ) else : suggested_filename = '00 {}' . format ( metadata . get ( 'title' , '' ) ) return suggested_filename
11814	def score ( self , plaintext ) : "Return a score for text based on how common letters pairs are." s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
5431	def _get_filtered_mounts ( mounts , mount_param_type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount_param_type ) ] )
11366	def _do_unzip ( zipped_file , output_directory ) : z = zipfile . ZipFile ( zipped_file ) for path in z . namelist ( ) : relative_path = os . path . join ( output_directory , path ) dirname , dummy = os . path . split ( relative_path ) try : if relative_path . endswith ( os . sep ) and not os . path . exists ( dirname ) : os . makedirs ( relative_path ) elif not os . path . exists ( relative_path ) : dirname = os . path . join ( output_directory , os . path . dirname ( path ) ) if os . path . dirname ( path ) and not os . path . exists ( dirname ) : os . makedirs ( dirname ) fd = open ( relative_path , "w" ) fd . write ( z . read ( path ) ) fd . close ( ) except IOError , e : raise e return output_directory
11615	def print_read ( self , rid ) : if self . rname is not None : print self . rname [ rid ] print '--' r = self . get_read_data ( rid ) aligned_loci = np . unique ( r . nonzero ( ) [ 1 ] ) for locus in aligned_loci : nzvec = r [ : , locus ] . todense ( ) . transpose ( ) [ 0 ] . A . flatten ( ) if self . lname is not None : print self . lname [ locus ] , else : print locus , print nzvec
7501	def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
10524	def login ( self , username = None , password = None , android_id = None ) : cls_name = type ( self ) . __name__ if username is None : username = input ( "Enter your Google username or email address: " ) if password is None : password = getpass . getpass ( "Enter your Google Music password: " ) if android_id is None : android_id = Mobileclient . FROM_MAC_ADDRESS try : self . api . login ( username , password , android_id ) except OSError : logger . exception ( "{} authentication failed." . format ( cls_name ) ) if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
5327	def sha_github_file ( cls , config , repo_file , repository_api , repository_branch ) : repo_file_sha = None cfg = config . get_conf ( ) github_token = cfg [ 'sortinghat' ] [ 'identities_api_token' ] headers = { "Authorization" : "token " + github_token } url_dir = repository_api + "/git/trees/" + repository_branch logger . debug ( "Gettting sha data from tree: %s" , url_dir ) raw_repo_file_info = requests . get ( url_dir , headers = headers ) raw_repo_file_info . raise_for_status ( ) for rfile in raw_repo_file_info . json ( ) [ 'tree' ] : if rfile [ 'path' ] == repo_file : logger . debug ( "SHA found: %s, " , rfile [ "sha" ] ) repo_file_sha = rfile [ "sha" ] break return repo_file_sha
850	def setParameter ( self , parameterName , index , parameterValue ) : if parameterName == 'topDownMode' : self . topDownMode = parameterValue elif parameterName == 'predictedField' : self . predictedField = parameterValue else : raise Exception ( 'Unknown parameter: ' + parameterName )
8777	def _chunks ( self , iterable , chunk_size ) : iterator = iter ( iterable ) chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) ) while chunk : yield chunk chunk = list ( itertools . islice ( iterator , 0 , chunk_size ) )
12120	def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
5535	def read ( self , output_tile ) : if self . config . mode not in [ "readonly" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be readonly, continue or overwrite" ) if isinstance ( output_tile , tuple ) : output_tile = self . config . output_pyramid . tile ( * output_tile ) elif isinstance ( output_tile , BufferedTile ) : pass else : raise TypeError ( "output_tile must be tuple or BufferedTile" ) return self . config . output . read ( output_tile )
6673	def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
1886	def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
473	def build_words_dataset ( words = None , vocabulary_size = 50000 , printable = True , unk_key = 'UNK' ) : if words is None : raise Exception ( "words : list of str or byte" ) count = [ [ unk_key , - 1 ] ] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 ) ) dictionary = dict ( ) for word , _ in count : dictionary [ word ] = len ( dictionary ) data = list ( ) unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 unk_count += 1 data . append ( index ) count [ 0 ] [ 1 ] = unk_count reverse_dictionary = dict ( zip ( dictionary . values ( ) , dictionary . keys ( ) ) ) if printable : tl . logging . info ( 'Real vocabulary size %d' % len ( collections . Counter ( words ) . keys ( ) ) ) tl . logging . info ( 'Limited vocabulary size {}' . format ( vocabulary_size ) ) if len ( collections . Counter ( words ) . keys ( ) ) < vocabulary_size : raise Exception ( "len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size" ) return data , count , dictionary , reverse_dictionary
1181	def group ( self , * args ) : if len ( args ) == 0 : args = ( 0 , ) grouplist = [ ] for group in args : grouplist . append ( self . _get_slice ( self . _get_index ( group ) , None ) ) if len ( grouplist ) == 1 : return grouplist [ 0 ] else : return tuple ( grouplist )
5319	def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
2654	def pull_file ( self , remote_source , local_dir ) : local_dest = local_dir + '/' + os . path . basename ( remote_source ) try : os . makedirs ( local_dir ) except OSError as e : if e . errno != errno . EEXIST : logger . exception ( "Failed to create script_dir: {0}" . format ( script_dir ) ) raise BadScriptPath ( e , self . hostname ) if os . path . exists ( local_dest ) : logger . exception ( "Remote file copy will overwrite a local file:{0}" . format ( local_dest ) ) raise FileExists ( None , self . hostname , filename = local_dest ) try : self . sftp_client . get ( remote_source , local_dest ) except Exception as e : logger . exception ( "File pull failed" ) raise FileCopyException ( e , self . hostname ) return local_dest
11290	def load_dict ( self , source , namespace = '' ) : for key , value in source . items ( ) : if isinstance ( key , str ) : nskey = ( namespace + '.' + key ) . strip ( '.' ) if isinstance ( value , dict ) : self . load_dict ( value , namespace = nskey ) else : self [ nskey ] = value else : raise TypeError ( 'Key has type %r (not a string)' % type ( key ) ) return self
7356	def create_input_peptides_files ( peptides , max_peptides_per_file = None , group_by_length = False ) : if group_by_length : peptide_lengths = { len ( p ) for p in peptides } peptide_groups = { l : [ ] for l in peptide_lengths } for p in peptides : peptide_groups [ len ( p ) ] . append ( p ) else : peptide_groups = { "" : peptides } file_names = [ ] for key , group in peptide_groups . items ( ) : n_peptides = len ( group ) if not max_peptides_per_file : max_peptides_per_file = n_peptides input_file = None for i , p in enumerate ( group ) : if i % max_peptides_per_file == 0 : if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) input_file = make_writable_tempfile ( prefix_number = i // max_peptides_per_file , prefix_name = key , suffix = ".txt" ) input_file . write ( "%s\n" % p ) if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) return file_names
11000	def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
2167	def list_resource_commands ( self ) : resource_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , os . pardir , 'resources' ) ) answer = set ( [ ] ) for _ , name , _ in pkgutil . iter_modules ( [ resource_path ] ) : res = tower_cli . get_resource ( name ) if not getattr ( res , 'internal' , False ) : answer . add ( name ) return sorted ( answer )
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , ** kwargs ) : if self . try_delegation : inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , ** kwargs ) return method ( self , * args , ** kwargs ) return delegator
11273	def get_dict ( self ) : return dict ( current_page = self . current_page , total_page_count = self . total_page_count , items = self . items , total_item_count = self . total_item_count , page_size = self . page_size )
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
9777	def pprint ( value ) : click . echo ( json . dumps ( value , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) )
641	def dict ( cls ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) result = dict ( cls . _properties ) keys = os . environ . keys ( ) replaceKeys = filter ( lambda x : x . startswith ( cls . envPropPrefix ) , keys ) for envKey in replaceKeys : key = envKey [ len ( cls . envPropPrefix ) : ] key = key . replace ( '_' , '.' ) result [ key ] = os . environ [ envKey ] return result
6481	def mem_size ( self ) : data_len = self . _data_mem_size node_count = len ( list ( self . xml_doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node_count + data_len + 630 else : tags_len = 0 for e in self . xml_doc . iter ( tag = etree . Element ) : e_len = max ( len ( e . tag ) , 8 ) e_len = ( e_len + 3 ) & ~ 3 tags_len += e_len size = 56 * node_count + data_len + 630 + tags_len return ( size + 8 ) & ~ 7
5341	def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
10277	def get_neurommsig_scores_prestratified ( subgraphs : Mapping [ str , BELGraph ] , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , ) -> Optional [ Mapping [ str , float ] ] : return { name : get_neurommsig_score ( graph = subgraph , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , ) for name , subgraph in subgraphs . items ( ) }
1346	def gradient ( self , image , label ) : _ , gradient = self . predictions_and_gradient ( image , label ) return gradient
4161	def _select_block ( str_in , start_tag , end_tag ) : start_pos = str_in . find ( start_tag ) if start_pos < 0 : raise ValueError ( 'start_tag not found' ) depth = 0 for pos in range ( start_pos , len ( str_in ) ) : if str_in [ pos ] == start_tag : depth += 1 elif str_in [ pos ] == end_tag : depth -= 1 if depth == 0 : break sel = str_in [ start_pos + 1 : pos ] return sel
13613	def combine_filenames ( filenames , max_length = 40 ) : path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
6600	def open ( self ) : self . path = self . _prepare_dir ( self . topdir ) self . _copy_executable ( area_path = self . path ) self . _save_logging_levels ( area_path = self . path ) self . _put_python_modules ( modules = self . python_modules , area_path = self . path )
13230	def get_macros ( tex_source ) : r macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
12543	def merge_images ( images , axis = 't' ) : if not images : return None axis_dim = { 'x' : 0 , 'y' : 1 , 'z' : 2 , 't' : 3 , } if axis not in axis_dim : raise ValueError ( 'Expected `axis` to be one of ({}), got {}.' . format ( set ( axis_dim . keys ( ) ) , axis ) ) img1 = images [ 0 ] for img in images : check_img_compatibility ( img1 , img ) image_data = [ ] for img in images : image_data . append ( check_img ( img ) . get_data ( ) ) work_axis = axis_dim [ axis ] ndim = image_data [ 0 ] . ndim if ndim - 1 < work_axis : image_data = [ np . expand_dims ( img , axis = work_axis ) for img in image_data ] return np . concatenate ( image_data , axis = work_axis )
5696	def copy ( cls , conn , ** where ) : cur = conn . cursor ( ) if where and cls . copy_where : copy_where = cls . copy_where . format ( ** where ) else : copy_where = '' cur . execute ( 'INSERT INTO %s ' 'SELECT * FROM source.%s %s' % ( cls . table , cls . table , copy_where ) )
7423	def ref_muscle_chunker ( data , sample ) : LOGGER . info ( 'entering ref_muscle_chunker' ) regions = bedtools_merge ( data , sample ) if len ( regions ) > 0 : get_overlapping_reads ( data , sample , regions ) else : msg = "No reads mapped to reference sequence - {}" . format ( sample . name ) LOGGER . warn ( msg )
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )
4499	def _json ( self , response , status_code ) : if isinstance ( status_code , numbers . Integral ) : status_code = ( status_code , ) if response . status_code in status_code : return response . json ( ) else : raise RuntimeError ( "Response has status " "code {} not {}" . format ( response . status_code , status_code ) )
10539	def update_category ( category ) : try : res = _pybossa_req ( 'put' , 'category' , category . id , payload = category . data ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
11883	def scanProcessForOpenFile ( pid , searchPortion , isExactMatch = True , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e prefixDir = "/proc/%d/fd" % ( pid , ) processFDs = os . listdir ( prefixDir ) matchedFDs = [ ] matchedFilenames = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor == totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) == totalPath . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor in totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) in totalPath . lower ( ) ) for fd in processFDs : fdPath = os . readlink ( prefixDir + '/' + fd ) if isMatch ( searchPortion , fdPath ) : matchedFDs . append ( fd ) matchedFilenames . append ( fdPath ) if len ( matchedFDs ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'fds' : matchedFDs , 'filenames' : matchedFilenames , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
2218	def _rectify_countdown_or_bool ( count_or_bool ) : if count_or_bool is True or count_or_bool is False : count_or_bool_ = count_or_bool elif isinstance ( count_or_bool , int ) : if count_or_bool == 0 : return 0 elif count_or_bool > 0 : count_or_bool_ = count_or_bool - 1 else : count_or_bool_ = count_or_bool else : count_or_bool_ = False return count_or_bool_
11563	def set_digital_latch ( self , pin , threshold_type , cb = None ) : if 0 <= threshold_type <= 1 : self . _command_handler . set_digital_latch ( pin , threshold_type , cb ) return True else : return False
2021	def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
8360	def draw ( self , widget , cr ) : if self . bot_size is None : self . draw_default_image ( cr ) return cr = driver . ensure_pycairo_context ( cr ) surface = self . backing_store . surface cr . set_source_surface ( surface ) cr . paint ( )
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
12165	def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
2987	def get_app_kwarg_dict ( appInstance = None ) : app = ( appInstance or current_app ) app_config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors_' , '' ) : app_config . get ( k ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None }
3538	def yandex_metrica ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return YandexMetricaNode ( )
755	def __getDictMetaInfo ( self , inferenceElement , inferenceDict ) : fieldMetaInfo = [ ] inferenceLabel = InferenceElement . getLabel ( inferenceElement ) if InferenceElement . getInputElement ( inferenceElement ) : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , type = FieldMetaType . string , special = '' ) ) keys = sorted ( inferenceDict . keys ( ) ) for key in keys : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + "." + str ( key ) , type = FieldMetaType . string , special = '' ) ) return fieldMetaInfo
4577	def get_server ( self , key , ** kwds ) : kwds = dict ( self . kwds , ** kwds ) server = self . servers . get ( key ) if server : server . check_keywords ( self . constructor , kwds ) else : server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
7363	def with_prefix ( self , prefix , strict = False ) : def decorated ( func ) : return EventHandler ( func = func , event = self . event , prefix = prefix , strict = strict ) return decorated
3605	def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_get_request ( endpoint , params , headers , connection = connection )
4954	def _disconnect_user_post_save_for_migrations ( self , sender , ** kwargs ) : from django . db . models . signals import post_save post_save . disconnect ( sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID )
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
12229	def autodiscover_siteprefs ( admin_site = None ) : if admin_site is None : admin_site = admin . site if 'manage' not in sys . argv [ 0 ] or ( len ( sys . argv ) > 1 and sys . argv [ 1 ] in MANAGE_SAFE_COMMANDS ) : import_prefs ( ) Preference . read_prefs ( get_prefs ( ) ) register_admin_models ( admin_site )
221	async def check_config ( self ) -> None : if self . directory is None : return try : stat_result = await aio_stat ( self . directory ) except FileNotFoundError : raise RuntimeError ( f"StaticFiles directory '{self.directory}' does not exist." ) if not ( stat . S_ISDIR ( stat_result . st_mode ) or stat . S_ISLNK ( stat_result . st_mode ) ) : raise RuntimeError ( f"StaticFiles path '{self.directory}' is not a directory." )
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : for x , y in zip ( a , b ) : result |= x ^ y else : for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
2687	def get_library_config ( name ) : try : proc = Popen ( [ 'pkg-config' , '--cflags' , '--libs' , name ] , stdout = PIPE , stderr = PIPE ) except OSError : print ( 'pkg-config is required for building PyAV' ) exit ( 1 ) raw_cflags , err = proc . communicate ( ) if proc . wait ( ) : return known , unknown = parse_cflags ( raw_cflags . decode ( 'utf8' ) ) if unknown : print ( "pkg-config returned flags we don't understand: {}" . format ( unknown ) ) exit ( 1 ) return known
3706	def COSTALD ( T , Tc , Vc , omega ) : r Tr = T / Tc V_delta = ( - 0.296123 + 0.386914 * Tr - 0.0427258 * Tr ** 2 - 0.0480645 * Tr ** 3 ) / ( Tr - 1.00001 ) V_0 = 1 - 1.52816 * ( 1 - Tr ) ** ( 1 / 3. ) + 1.43907 * ( 1 - Tr ) ** ( 2 / 3. ) - 0.81446 * ( 1 - Tr ) + 0.190454 * ( 1 - Tr ) ** ( 4 / 3. ) return Vc * V_0 * ( 1 - omega * V_delta )
10250	def is_node_highlighted ( graph : BELGraph , node : BaseEntity ) -> bool : return NODE_HIGHLIGHT in graph . node [ node ]
4220	def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
5880	def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
2099	def batch_update ( self , pk = None , ** kwargs ) : res = self . get ( pk = pk , ** kwargs ) url = self . endpoint + '%d/%s/' % ( res [ 'id' ] , 'update_inventory_sources' ) return client . post ( url , data = { } ) . json ( )
8293	def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
1918	def run ( self ) : current_state = None current_state_id = None with WithKeyboardInterruptAs ( self . shutdown ) : self . _notify_start_run ( ) logger . debug ( "Starting Manticore Symbolic Emulator Worker (pid %d)." , os . getpid ( ) ) solver = Z3Solver ( ) while not self . is_shutdown ( ) : try : try : if current_state is None : with self . _lock : self . _notify_stop_run ( ) try : current_state_id = self . get ( ) if current_state_id is not None : self . _publish ( 'will_load_state' , current_state_id ) current_state = self . _workspace . load_state ( current_state_id ) self . forward_events_from ( current_state , True ) self . _publish ( 'did_load_state' , current_state , current_state_id ) logger . info ( "load state %r" , current_state_id ) finally : self . _notify_start_run ( ) if current_state is None : logger . debug ( "No more states in the queue, byte bye!" ) break assert current_state is not None assert current_state . constraints is current_state . platform . constraints while not self . is_shutdown ( ) : if not current_state . execute ( ) : break else : self . _publish ( 'will_terminate_state' , current_state , current_state_id , TerminateState ( 'Shutdown' ) ) current_state = None except Concretize as e : logger . debug ( "Generic state fork on condition" ) current_state = self . fork ( current_state , e . expression , e . policy , e . setstate ) except TerminateState as e : self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) logger . debug ( "Generic terminate state" ) if e . testcase : self . _publish ( 'internal_generate_testcase' , current_state , message = str ( e ) ) current_state = None except SolverError as e : import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) if solver . check ( current_state . constraints ) : self . _publish ( 'internal_generate_testcase' , current_state , message = "Solver failed" + str ( e ) ) current_state = None except ( Exception , AssertionError ) as e : import traceback trace = traceback . format_exc ( ) logger . error ( "Exception: %s\n%s" , str ( e ) , trace ) self . _publish ( 'will_terminate_state' , current_state , current_state_id , e ) current_state = None assert current_state is None or self . is_shutdown ( ) self . _notify_stop_run ( )
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
5238	def shift_time ( start_time , mins ) -> str : s_time = pd . Timestamp ( start_time ) e_time = s_time + np . sign ( mins ) * pd . Timedelta ( f'00:{abs(mins)}:00' ) return e_time . strftime ( '%H:%M' )
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
9454	def play_stop ( self , call_params ) : path = '/' + self . api_version + '/PlayStop/' method = 'POST' return self . request ( path , method , call_params )
11980	def set_ip ( self , ip ) : self . set ( ip = ip , netmask = self . _nm )
8130	def layer ( self , img , x = 0 , y = 0 , name = "" ) : from types import StringType if isinstance ( img , Image . Image ) : img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1 if isinstance ( img , Layer ) : img . canvas = self self . layers . append ( img ) return len ( self . layers ) - 1 if type ( img ) == StringType : img = Image . open ( img ) img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
11234	def translate_array ( self , string , language , level = 3 , retdata = False ) : language = language . lower ( ) assert self . is_built_in ( language ) or language in self . outer_templates , "Sorry, " + language + " is not a supported language." data = phpserialize . loads ( bytes ( string , 'utf-8' ) , array_hook = list , decode_strings = True ) if self . is_built_in ( language ) : self . get_built_in ( language , level , data ) print ( self ) return self . data_structure if retdata else None def loop_print ( iterable , level = 3 ) : retval = '' indentation = ' ' * level if not self . is_iterable ( iterable ) or isinstance ( iterable , str ) : non_iterable = str ( iterable ) return str ( non_iterable ) for item in iterable : if isinstance ( item , tuple ) and len ( item ) == 2 : key = item [ 0 ] val = loop_print ( item [ 1 ] , level = level + 3 ) val = self . translate_val ( language , val ) if language in self . lang_specific_values and val in self . lang_specific_values [ language ] else val key = str ( key ) if isinstance ( key , int ) else '\'' + str ( key ) + '\'' needs_unpacking = hasattr ( item [ 0 ] , '__iter__' ) == False and hasattr ( item [ 1 ] , '__iter__' ) == True if needs_unpacking : retval += self . get_inner_template ( language , 'iterable' , indentation , key , val ) else : val = str ( val ) if val . isdigit ( ) or val in self . lang_specific_values [ language ] . values ( ) else '\'' + str ( val ) + '\'' retval += self . get_inner_template ( language , 'singular' , indentation , key , val ) return retval self . data_structure = self . outer_templates [ language ] % ( loop_print ( data ) ) print ( self ) return self . data_structure if retdata else None
13228	def get_installation_token ( installation_id , integration_jwt ) : api_root = 'https://api.github.com' url = '{root}/installations/{id_:d}/access_tokens' . format ( api_root = api_root , id_ = installation_id ) headers = { 'Authorization' : 'Bearer {0}' . format ( integration_jwt . decode ( 'utf-8' ) ) , 'Accept' : 'application/vnd.github.machine-man-preview+json' } resp = requests . post ( url , headers = headers ) resp . raise_for_status ( ) return resp . json ( )
9726	async def send_xml ( self , xml ) : return await asyncio . wait_for ( self . _protocol . send_command ( xml , command_type = QRTPacketType . PacketXML ) , timeout = self . _timeout , )
6364	def to_tuple ( self ) : return self . _tp , self . _tn , self . _fp , self . _fn
2805	def convert_elementwise_add ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_add ...' ) if 'broadcast' in params : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : layer = tf . add ( x [ 0 ] , x [ 1 ] ) return layer lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name ) layers [ scope_name ] = lambda_layer ( [ layers [ inputs [ 0 ] ] , layers [ inputs [ 1 ] ] ] ) else : model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'A' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) add = keras . layers . Add ( name = tf_name ) layers [ scope_name ] = add ( [ model0 , model1 ] )
1420	def _get_scheduler_location_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_scheduler_location_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_scheduler_location ( data , stats ) : if data : scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) callback ( scheduler_location ) else : callback ( None ) return isWatching
13213	def rename ( self , from_name , to_name ) : log . info ( 'renaming database from %s to %s' % ( from_name , to_name ) ) self . _run_stmt ( 'alter database %s rename to %s' % ( from_name , to_name ) )
10397	def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
6416	def var ( nums , mean_func = amean , ddof = 0 ) : r x_bar = mean_func ( nums ) return sum ( ( x - x_bar ) ** 2 for x in nums ) / ( len ( nums ) - ddof )
2543	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True self . file ( doc ) . license_comment = text return True else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
3872	def get_all ( self , include_archived = False ) : return [ conv for conv in self . _conv_dict . values ( ) if not conv . is_archived or include_archived ]
2719	def get_object ( cls , api_token , action_id ) : action = cls ( token = api_token , id = action_id ) action . load_directly ( ) return action
2590	def stage_in ( self , file , executor ) : if file . scheme == 'ftp' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _ftp_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'http' or file . scheme == 'https' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _http_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_in_app = self . _globus_stage_in_app ( ) app_fut = stage_in_app ( globus_ep , outputs = [ file ] ) return app_fut . _outputs [ 0 ] else : raise Exception ( 'Staging in with unknown file scheme {} is not supported' . format ( file . scheme ) )
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
13086	def get ( self , section , key ) : try : return self . config . get ( section , key ) except configparser . NoSectionError : pass except configparser . NoOptionError : pass return self . defaults [ section ] [ key ]
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
4728	def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
3989	def _nginx_location_spec ( port_spec , bridge_ip ) : location_string_spec = "\t \t location / { \n" for location_setting in [ 'proxy_http_version 1.1;' , 'proxy_set_header Upgrade $http_upgrade;' , 'proxy_set_header Connection "upgrade";' , 'proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' , 'proxy_set_header Host $http_host;' , _nginx_proxy_string ( port_spec , bridge_ip ) ] : location_string_spec += "\t \t \t {} \n" . format ( location_setting ) location_string_spec += "\t \t } \n" return location_string_spec
6032	def grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_grid = RegularGrid . from_mask ( mask ) sub_grid = SubGrid . from_mask_and_sub_grid_size ( mask , sub_grid_size ) blurring_grid = RegularGrid . blurring_grid_from_mask_and_psf_shape ( mask , psf_shape ) return GridStack ( regular_grid , sub_grid , blurring_grid )
1791	def IMUL ( cpu , * operands ) : dest = operands [ 0 ] OperandSize = dest . size reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] arg0 = dest . read ( ) arg1 = None arg2 = None res = None if len ( operands ) == 1 : arg1 = cpu . read_register ( reg_name_l ) temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) res = Operators . EXTRACT ( temp , 0 , OperandSize ) elif len ( operands ) == 2 : arg1 = operands [ 1 ] . read ( ) arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) else : arg1 = operands [ 1 ] . read ( ) arg2 = operands [ 2 ] . read ( ) temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) cpu . OF = cpu . CF
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
6490	def _process_filters ( filter_dictionary ) : def filter_item ( field ) : if filter_dictionary [ field ] is not None : return { "or" : [ _get_filter_field ( field , filter_dictionary [ field ] ) , { "missing" : { "field" : field } } ] } return { "missing" : { "field" : field } } return [ filter_item ( field ) for field in filter_dictionary ]
12632	def copy_groups_to_folder ( dicom_groups , folder_path , groupby_field_name ) : if dicom_groups is None or not dicom_groups : raise ValueError ( 'Expected a boyle.dicom.sets.DicomFileSet.' ) if not os . path . exists ( folder_path ) : os . makedirs ( folder_path , exist_ok = False ) for dcmg in dicom_groups : if groupby_field_name is not None and len ( groupby_field_name ) > 0 : dfile = DicomFile ( dcmg ) dir_name = '' for att in groupby_field_name : dir_name = os . path . join ( dir_name , dfile . get_attributes ( att ) ) dir_name = str ( dir_name ) else : dir_name = os . path . basename ( dcmg ) group_folder = os . path . join ( folder_path , dir_name ) os . makedirs ( group_folder , exist_ok = False ) log . debug ( 'Copying files to {}.' . format ( group_folder ) ) import shutil dcm_files = dicom_groups [ dcmg ] for srcf in dcm_files : destf = os . path . join ( group_folder , os . path . basename ( srcf ) ) while os . path . exists ( destf ) : destf += '+' shutil . copy2 ( srcf , destf )
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
8743	def delete_floatingip ( context , id ) : LOG . info ( 'delete_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . FLOATING )
3404	def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
11346	def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
5502	def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
4014	def register_consumer ( ) : global _consumers hostname , port = request . form [ 'hostname' ] , request . form [ 'port' ] app_name = _app_name_from_forwarding_info ( hostname , port ) containers = get_dusty_containers ( [ app_name ] , include_exited = True ) if not containers : raise ValueError ( 'No container exists for app {}' . format ( app_name ) ) container = containers [ 0 ] new_id = uuid1 ( ) new_consumer = Consumer ( container [ 'Id' ] , datetime . utcnow ( ) ) _consumers [ str ( new_id ) ] = new_consumer response = jsonify ( { 'app_name' : app_name , 'consumer_id' : new_id } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
9822	def list ( page ) : user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
2174	def token_from_fragment ( self , authorization_response ) : self . _client . parse_request_uri_response ( authorization_response , state = self . _state ) self . token = self . _client . token return self . token
8950	def error ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;37;41mERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
12625	def get_file_list ( file_dir , regex = '' ) : file_list = os . listdir ( file_dir ) file_list . sort ( ) if regex : file_list = search_list ( file_list , regex ) file_list = [ op . join ( file_dir , fname ) for fname in file_list ] return file_list
4096	def KIC ( N , rho , k ) : r from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
2688	def update_extend ( dst , src ) : for k , v in src . items ( ) : existing = dst . setdefault ( k , [ ] ) for x in v : if x not in existing : existing . append ( x )
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
3425	def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
7808	def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) except AttributeError : return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject_name = data . get ( 'subject' ) cert . alt_names = defaultdict ( list ) if 'subjectAltName' in data : for name , value in data [ 'subjectAltName' ] : cert . alt_names [ name ] . append ( value ) if 'notAfter' in data : tstamp = ssl . cert_time_to_seconds ( data [ 'notAfter' ] ) cert . not_after = datetime . utcfromtimestamp ( tstamp ) if sys . version_info . major < 3 : cert . _decode_names ( ) cert . common_names = [ ] if cert . subject_name : for part in cert . subject_name : for name , value in part : if name == 'commonName' : cert . common_names . append ( value ) return cert
11344	def request ( self , url , method = "GET" , data = None , params = None , retry = True ) : headers = config . REQUEST_HEADERS if params and self . _session_id : params [ 'sessionid' ] = self . _session_id if method == "GET" : response = requests . get ( url , headers = headers , params = params ) elif method == "POST" : response = requests . post ( url , headers = headers , params = params , data = data ) if response . status_code == 401 and retry : _LOGGER . warn ( "NuHeat APIrequest unauthorized [401]. Try to re-authenticate." ) self . _session_id = None self . authenticate ( ) return self . request ( url , method = method , data = data , params = params , retry = False ) response . raise_for_status ( ) try : return response . json ( ) except ValueError : return response
8820	def delete_network ( context , id ) : LOG . info ( "delete_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( net . id ) : raise n_exc . NotAuthorized ( net_id = id ) if net . ports : raise n_exc . NetworkInUse ( net_id = id ) net_driver = registry . DRIVER_REGISTRY . get_driver ( net [ "network_plugin" ] ) net_driver . delete_network ( context , id ) for subnet in net [ "subnets" ] : subnets . _delete_subnet ( context , subnet ) db_api . network_delete ( context , net )
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
682	def getSDRforValue ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . numRecords > j encoding = self . fields [ i ] . encodings [ j ] return encoding
13530	def add_child ( self , ** kwargs ) : data_class = self . graph . data_content_type . model_class ( ) node = Node . objects . create ( graph = self . graph ) data_class . objects . create ( node = node , ** kwargs ) node . parents . add ( self ) self . children . add ( node ) return node
5309	def check_hex ( value ) : length = len ( value ) if length not in ( 3 , 6 ) : raise ValueError ( 'Hex string #{} is too long' . format ( value ) ) regex = r'[0-9a-f]{{{length}}}' . format ( length = length ) if not re . search ( regex , value , re . I ) : raise ValueError ( 'Invalid Hex String: #{}' . format ( value ) )
12002	def _remove_magic ( self , data ) : if not self . magic : return data magic_size = len ( self . magic ) magic = data [ : magic_size ] if magic != self . magic : raise Exception ( 'Invalid magic' ) data = data [ magic_size : ] return data
8496	def _parse_and_output ( filename , args ) : relpath = os . path . dirname ( filename ) if os . path . isfile ( filename ) : calls = _parse_file ( filename , relpath ) elif os . path . isdir ( filename ) : calls = _parse_dir ( filename , relpath ) else : _error ( "Could not determine file type: %r" , filename ) if not calls : _error ( "No pyconfig calls." ) if args . load_configs : keys = set ( ) for call in calls : keys . add ( call . key ) conf = pyconfig . Config ( ) for key , value in conf . settings . items ( ) : if key in keys : continue calls . append ( _PyconfigCall ( 'set' , key , value , [ None ] * 4 ) ) _output ( calls , args )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
12643	def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
9716	async def await_event ( self , event = None , timeout = 30 ) : return await self . _protocol . await_event ( event , timeout = timeout )
12095	def indexImages ( folder , fname = "index.html" ) : html = "<html><body>" for item in glob . glob ( folder + "/*.*" ) : if item . split ( "." ) [ - 1 ] in [ 'jpg' , 'png' ] : html += "<h3>%s</h3>" % os . path . basename ( item ) html += '<img src="%s">' % os . path . basename ( item ) html += '<br>' * 10 html += "</html></body>" f = open ( folder + "/" + fname , 'w' ) f . write ( html ) f . close print ( "indexed:" ) print ( " " , os . path . abspath ( folder + "/" + fname ) ) return
8696	def set_timeout ( self , timeout ) : timeout = int ( timeout ) self . _timeout = timeout == 0 and 999999 or timeout
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
7424	def check_insert_size ( data , sample ) : cmd1 = [ ipyrad . bins . samtools , "stats" , sample . files . mapped_reads ] cmd2 = [ "grep" , "SN" ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , res ) avg_insert = 0 stdv_insert = 0 avg_len = 0 for line in res . split ( "\n" ) : if "insert size average" in line : avg_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) elif "insert size standard deviation" in line : stdv_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) + 0.1 elif "average length" in line : avg_len = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) LOGGER . debug ( "avg {} stdv {} avg_len {}" . format ( avg_insert , stdv_insert , avg_len ) ) if all ( [ avg_insert , stdv_insert , avg_len ] ) : if stdv_insert < 5 : stdv_insert = 5. if ( 2 * avg_len ) < avg_insert : hack = avg_insert + ( 3 * np . math . ceil ( stdv_insert ) ) - ( 2 * avg_len ) else : hack = ( avg_insert - avg_len ) + ( 3 * np . math . ceil ( stdv_insert ) ) LOGGER . info ( "stdv: hacked insert size is %s" , hack ) data . _hackersonly [ "max_inner_mate_distance" ] = int ( np . math . ceil ( hack ) ) else : data . _hackersonly [ "max_inner_mate_distance" ] = 300 LOGGER . debug ( "inner mate distance for {} - {}" . format ( sample . name , data . _hackersonly [ "max_inner_mate_distance" ] ) )
7504	def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except StopIteration : break
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
10328	def rank_causalr_hypothesis ( graph , node_to_regulation , regulator_node ) : upregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } downregulation_hypothesis = { 'correct' : 0 , 'incorrect' : 0 , 'ambiguous' : 0 } targets = [ node for node in node_to_regulation if node != regulator_node ] predicted_regulations = run_cna ( graph , regulator_node , targets ) for _ , target_node , predicted_regulation in predicted_regulations : if ( predicted_regulation is Effect . inhibition or predicted_regulation is Effect . activation ) and ( predicted_regulation . value == node_to_regulation [ target_node ] ) : upregulation_hypothesis [ 'correct' ] += 1 downregulation_hypothesis [ 'incorrect' ] += 1 elif predicted_regulation is Effect . ambiguous : upregulation_hypothesis [ 'ambiguous' ] += 1 downregulation_hypothesis [ 'ambiguous' ] += 1 elif predicted_regulation is Effect . no_effect : continue else : downregulation_hypothesis [ 'correct' ] += 1 upregulation_hypothesis [ 'incorrect' ] += 1 upregulation_hypothesis [ 'score' ] = upregulation_hypothesis [ 'correct' ] - upregulation_hypothesis [ 'incorrect' ] downregulation_hypothesis [ 'score' ] = downregulation_hypothesis [ 'correct' ] - downregulation_hypothesis [ 'incorrect' ] return upregulation_hypothesis , downregulation_hypothesis
6658	def _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained = False , memory_limit = None , test_mode = False ) : if not memory_constrained : return np . sum ( ( np . dot ( inbag - 1 , pred_centered . T ) / n_trees ) ** 2 , 0 ) if not memory_limit : raise ValueError ( 'If memory_constrained=True, must provide' , 'memory_limit.' ) chunk_size = int ( ( memory_limit * 1e6 ) / ( 8.0 * X_train . shape [ 0 ] ) ) if chunk_size == 0 : min_limit = 8.0 * X_train . shape [ 0 ] / 1e6 raise ValueError ( 'memory_limit provided is too small.' + 'For these dimensions, memory_limit must ' + 'be greater than or equal to %.3e' % min_limit ) chunk_edges = np . arange ( 0 , X_test . shape [ 0 ] + chunk_size , chunk_size ) inds = range ( X_test . shape [ 0 ] ) chunks = [ inds [ chunk_edges [ i ] : chunk_edges [ i + 1 ] ] for i in range ( len ( chunk_edges ) - 1 ) ] if test_mode : print ( 'Number of chunks: %d' % ( len ( chunks ) , ) ) V_IJ = np . concatenate ( [ np . sum ( ( np . dot ( inbag - 1 , pred_centered [ chunk ] . T ) / n_trees ) ** 2 , 0 ) for chunk in chunks ] ) return V_IJ
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
1134	def updatecache ( filename , module_globals = None ) : if filename in cache : del cache [ filename ] if not filename or ( filename . startswith ( '<' ) and filename . endswith ( '>' ) ) : return [ ] fullname = filename try : stat = os . stat ( fullname ) except OSError : basename = filename if module_globals and '__loader__' in module_globals : name = module_globals . get ( '__name__' ) loader = module_globals [ '__loader__' ] get_source = getattr ( loader , 'get_source' , None ) if name and get_source : try : data = get_source ( name ) except ( ImportError , IOError ) : pass else : if data is None : return [ ] cache [ filename ] = ( len ( data ) , None , [ line + '\n' for line in data . splitlines ( ) ] , fullname ) return cache [ filename ] [ 2 ] if os . path . isabs ( filename ) : return [ ] for dirname in sys . path : try : fullname = os . path . join ( dirname , basename ) except ( TypeError , AttributeError ) : continue try : stat = os . stat ( fullname ) break except os . error : pass else : return [ ] try : with open ( fullname , 'rU' ) as fp : lines = fp . readlines ( ) except IOError : return [ ] if lines and not lines [ - 1 ] . endswith ( '\n' ) : lines [ - 1 ] += '\n' size , mtime = stat . st_size , stat . st_mtime cache [ filename ] = size , mtime , lines , fullname return lines
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) if into == 'file' : s = s . replace ( '_' , ' ' ) if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
1943	def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
10331	def group_nodes_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , Set [ BaseEntity ] ] : result = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( u ) result [ d [ ANNOTATIONS ] [ annotation ] ] . add ( v ) return dict ( result )
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
1815	def SETNLE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) )
714	def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , outputLabel = outputLabel ) searchJob = _HyperSearchJob ( nupicJobID = jobID ) return searchJob
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) return payload . encode ( 'utf-8' )
13797	def handle_validate ( self , function_name , new_doc , old_doc , user_ctx ) : try : function = get_function ( function_name ) except Exception , exc : self . log ( repr ( exc ) ) return False try : return function ( new_doc , old_doc , user_ctx ) except Exception , exc : self . log ( repr ( exc ) ) return repr ( exc )
8121	def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
10477	def _leftMouseDragged ( self , stopCoord , strCoord , speed ) : appPid = self . _getPid ( ) if strCoord == ( 0 , 0 ) : loc = AppKit . NSEvent . mouseLocation ( ) strCoord = ( loc . x , Quartz . CGDisplayPixelsHigh ( 0 ) - loc . y ) appPid = self . _getPid ( ) pressLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDown , strCoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , pressLeftButton ) time . sleep ( 5 ) speed = round ( 1 / float ( speed ) , 2 ) xmoved = stopCoord [ 0 ] - strCoord [ 0 ] ymoved = stopCoord [ 1 ] - strCoord [ 1 ] if ymoved == 0 : raise ValueError ( 'Not support horizontal moving' ) else : k = abs ( ymoved / xmoved ) if xmoved != 0 : for xpos in range ( int ( abs ( xmoved ) ) ) : if xmoved > 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] + xpos * k ) elif xmoved > 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] + xpos * k ) dragLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDragged , currcoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , dragLeftButton ) time . sleep ( speed ) else : raise ValueError ( 'Not support vertical moving' ) upLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseUp , stopCoord , Quartz . kCGMouseButtonLeft ) time . sleep ( 5 ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , upLeftButton )
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
5096	def refresh_persistent_maps ( self ) : for robot in self . _robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/persistent_maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _persistent_maps . update ( { robot . serial : resp2 . json ( ) } )
11529	def upload_json_results ( self , token , filepath , community_id , producer_display_name , metric_name , producer_revision , submit_time , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'parent_keys' : parameters [ 'parentKeys' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] file_payload = open ( filepath , 'rb' ) response = self . request ( 'midas.tracker.results.upload.json' , parameters , file_payload ) return response
5633	def find_sections ( lines ) : sections = [ ] for line in lines : if is_heading ( line ) : sections . append ( get_heading ( line ) ) return sections
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
637	def read ( cls , proto ) : protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 return connections
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
13605	def url_correct ( self , point , auth = None , export = None ) : newUrl = self . __url + point + '.json' if auth or export : newUrl += "?" if auth : newUrl += ( "auth=" + auth ) if export : if not newUrl . endswith ( '?' ) : newUrl += "&" newUrl += "format=export" return newUrl
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
3376	def fix_objective_as_constraint ( model , fraction = 1 , bound = None , name = 'fixed_objective_{}' ) : fix_objective_name = name . format ( model . objective . name ) if fix_objective_name in model . constraints : model . solver . remove ( fix_objective_name ) if bound is None : bound = model . slim_optimize ( error_value = None ) * fraction if model . objective . direction == 'max' : ub , lb = None , bound else : ub , lb = bound , None constraint = model . problem . Constraint ( model . objective . expression , name = fix_objective_name , ub = ub , lb = lb ) add_cons_vars_to_problem ( model , constraint , sloppy = True ) return bound
10358	def shuffle_node_data ( graph : BELGraph , key : str , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_nodes ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) for _ in range ( swaps ) : s , t = random . sample ( result . node , 2 ) result . nodes [ s ] [ key ] , result . nodes [ t ] [ key ] = result . nodes [ t ] [ key ] , result . nodes [ s ] [ key ] return result
12613	def is_unique ( self , table_name , sample , unique_fields = None ) : try : eid = find_unique ( self . table ( table_name ) , sample = sample , unique_fields = unique_fields ) except : return False else : return eid is not None
980	def _overlapOK ( self , i , j , overlap = None ) : if overlap is None : overlap = self . _countOverlapIndices ( i , j ) if abs ( i - j ) < self . w : if overlap == ( self . w - abs ( i - j ) ) : return True else : return False else : if overlap <= self . _maxOverlap : return True else : return False
2359	def t_intnumber ( self , t ) : r'-?\d+' t . value = int ( t . value ) t . type = 'NUMBER' return t
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , ** obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
6208	def save_photon_hdf5 ( self , identity = None , overwrite = True , path = None ) : filepath = self . filepath if path is not None : filepath = Path ( path , filepath . name ) self . merge_da ( ) data = self . _make_photon_hdf5 ( identity = identity ) phc . hdf5 . save_photon_hdf5 ( data , h5_fname = str ( filepath ) , overwrite = overwrite )
1657	def IsInitializerList ( clean_lines , linenum ) : for i in xrange ( linenum , 1 , - 1 ) : line = clean_lines . elided [ i ] if i == linenum : remove_function_body = Match ( r'^(.*)\{\s*$' , line ) if remove_function_body : line = remove_function_body . group ( 1 ) if Search ( r'\s:\s*\w+[({]' , line ) : return True if Search ( r'\}\s*,\s*$' , line ) : return True if Search ( r'[{};]\s*$' , line ) : return False return False
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
7420	def index_reference_sequence ( data , force = False ) : refseq_file = data . paramsdict [ 'reference_sequence' ] index_files = [ ] if "smalt" in data . _hackersonly [ "aligner" ] : index_files . extend ( [ ".sma" , ".smi" ] ) else : index_files . extend ( [ ".amb" , ".ann" , ".bwt" , ".pac" , ".sa" ] ) index_files . extend ( [ ".fai" ] ) if not force : if all ( [ os . path . isfile ( refseq_file + i ) for i in index_files ] ) : return if "smalt" in data . _hackersonly [ "aligner" ] : cmd1 = [ ipyrad . bins . smalt , "index" , "-k" , str ( data . _hackersonly [ "smalt_index_wordlen" ] ) , refseq_file , refseq_file ] else : cmd1 = [ ipyrad . bins . bwa , "index" , refseq_file ] LOGGER . info ( " " . join ( cmd1 ) ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) error1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "faidx" , refseq_file ] LOGGER . info ( " " . join ( cmd2 ) ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) error2 = proc2 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( error1 ) if error2 : if "please use bgzip" in error2 : raise IPyradWarningExit ( NO_ZIP_BINS . format ( refseq_file ) ) else : raise IPyradWarningExit ( error2 )
5708	def process_request ( self , request ) : try : session = request . session except AttributeError : raise ImproperlyConfigured ( 'django-lockdown requires the Django ' 'sessions framework' ) if settings . ENABLED is False : return None if self . remote_addr_exceptions : remote_addr_exceptions = self . remote_addr_exceptions else : remote_addr_exceptions = settings . REMOTE_ADDR_EXCEPTIONS if remote_addr_exceptions : trusted_proxies = self . trusted_proxies or settings . TRUSTED_PROXIES remote_addr = request . META . get ( 'REMOTE_ADDR' ) if remote_addr in remote_addr_exceptions : return None if remote_addr in trusted_proxies : x_forwarded_for = request . META . get ( 'HTTP_X_FORWARDED_FOR' ) if x_forwarded_for : remote_addr = x_forwarded_for . split ( ',' ) [ - 1 ] . strip ( ) if remote_addr in remote_addr_exceptions : return None if self . url_exceptions : url_exceptions = compile_url_exceptions ( self . url_exceptions ) else : url_exceptions = compile_url_exceptions ( settings . URL_EXCEPTIONS ) for pattern in url_exceptions : if pattern . search ( request . path ) : return None try : resolved_path = resolve ( request . path ) except Resolver404 : pass else : if resolved_path . func in settings . VIEW_EXCEPTIONS : return None if self . until_date : until_date = self . until_date else : until_date = settings . UNTIL_DATE if self . after_date : after_date = self . after_date else : after_date = settings . AFTER_DATE if until_date or after_date : locked_date = False if until_date and datetime . datetime . now ( ) < until_date : locked_date = True if after_date and datetime . datetime . now ( ) > after_date : locked_date = True if not locked_date : return None form_data = request . POST if request . method == 'POST' else None if self . form : form_class = self . form else : form_class = get_lockdown_form ( settings . FORM ) form = form_class ( data = form_data , ** self . form_kwargs ) authorized = False token = session . get ( self . session_key ) if hasattr ( form , 'authenticate' ) : if form . authenticate ( token ) : authorized = True elif token is True : authorized = True if authorized and self . logout_key and self . logout_key in request . GET : if self . session_key in session : del session [ self . session_key ] querystring = request . GET . copy ( ) del querystring [ self . logout_key ] return self . redirect ( request ) if authorized : return None if form . is_valid ( ) : if hasattr ( form , 'generate_token' ) : token = form . generate_token ( ) else : token = True session [ self . session_key ] = token return self . redirect ( request ) page_data = { 'until_date' : until_date , 'after_date' : after_date } if not hasattr ( form , 'show_form' ) or form . show_form ( ) : page_data [ 'form' ] = form if self . extra_context : page_data . update ( self . extra_context ) return render ( request , 'lockdown/form.html' , page_data )
8069	def randomChildElement ( self , node ) : choices = [ e for e in node . childNodes if e . nodeType == e . ELEMENT_NODE ] chosen = random . choice ( choices ) if _debug : sys . stderr . write ( '%s available choices: %s\n' % ( len ( choices ) , [ e . toxml ( ) for e in choices ] ) ) sys . stderr . write ( 'Chosen: %s\n' % chosen . toxml ( ) ) return chosen
1257	def create_atomic_observe_operations ( self , states , actions , internals , terminal , reward , index ) : num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : states = util . map_tensors ( fn = tf . stop_gradient , tensors = states ) internals = util . map_tensors ( fn = tf . stop_gradient , tensors = internals ) actions = util . map_tensors ( fn = tf . stop_gradient , tensors = actions ) terminal = tf . stop_gradient ( input = terminal ) reward = tf . stop_gradient ( input = reward ) observation = self . fn_observe_timestep ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( observation , ) ) : self . unbuffered_episode_output = self . global_episode + 0
1199	def get_variables ( self , include_nontrainable = False ) : if include_nontrainable : return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] else : return [ self . variables [ key ] for key in sorted ( self . variables ) ]
13866	def fromtsms ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts / 1000 ) . replace ( microsecond = ts % 1000 * 1000 ) when = when . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
11876	def getch ( ) : try : termios . tcsetattr ( _fd , termios . TCSANOW , _new_settings ) ch = sys . stdin . read ( 1 ) finally : termios . tcsetattr ( _fd , termios . TCSADRAIN , _old_settings ) return ch
12472	def get_extension ( filepath , check_if_exists = False , allowed_exts = ALLOWED_EXTS ) : if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) rest , ext = op . splitext ( filepath ) if ext in allowed_exts : alloweds = allowed_exts [ ext ] _ , ext2 = op . splitext ( rest ) if ext2 in alloweds : ext = ext2 + ext return ext
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
2267	def invert_dict ( dict_ , unique_vals = True ) : r if unique_vals : if isinstance ( dict_ , OrderedDict ) : inverted = OrderedDict ( ( val , key ) for key , val in dict_ . items ( ) ) else : inverted = { val : key for key , val in dict_ . items ( ) } else : inverted = defaultdict ( set ) for key , value in dict_ . items ( ) : inverted [ value ] . add ( key ) inverted = dict ( inverted ) return inverted
4458	def sort_by ( self , field , asc = True ) : self . _sortby = SortbyField ( field , asc ) return self
8792	def validate ( self , value ) : try : vlan_id_int = int ( value ) assert vlan_id_int >= self . MIN_VLAN_ID assert vlan_id_int <= self . MAX_VLAN_ID except Exception : msg = ( "Invalid vlan_id. Got '%(vlan_id)s'. " "vlan_id should be an integer between %(min)d and %(max)d " "inclusive." % { 'vlan_id' : value , 'min' : self . MIN_VLAN_ID , 'max' : self . MAX_VLAN_ID } ) raise TagValidationError ( value , msg ) return True
3240	def get_group_policy_document ( group_name , policy_name , client = None , ** kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , ** kwargs ) [ 'PolicyDocument' ]
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
4547	def bresenham_line ( setter , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if colorFunc : color = colorFunc ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx
13391	def format_uuid ( uuid , max_length = 10 ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( uuid ) > max_length : uuid = "{}..." . format ( uuid [ 0 : max_length - 3 ] ) return uuid
7221	def ingest_vectors ( self , output_port_value ) : ingest_task = Task ( 'IngestItemJsonToVectorServices' ) ingest_task . inputs . items = output_port_value ingest_task . impersonation_allowed = True stage_task = Task ( 'StageDataToS3' ) stage_task . inputs . destination = 's3://{vector_ingest_bucket}/{recipe_id}/{run_id}/{task_name}' stage_task . inputs . data = ingest_task . outputs . result . value self . definition [ 'tasks' ] . append ( ingest_task . generate_task_workflow_json ( ) ) self . definition [ 'tasks' ] . append ( stage_task . generate_task_workflow_json ( ) )
1263	def states ( self ) : screen = self . env . getScreenRGB ( ) return dict ( shape = screen . shape , type = 'int' )
3000	def splitsDF ( symbol , timeframe = 'ytd' , token = '' , version = '' ) : s = splits ( symbol , timeframe , token , version ) df = _splitsToDF ( s ) return df
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
978	def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( "Either i or j don't exist" )
493	def close ( self ) : self . _logger . info ( "Closing" ) if self . _opened : self . _opened = False else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
5812	def raise_expired_not_yet_valid ( certificate ) : validity = certificate [ 'tbs_certificate' ] [ 'validity' ] not_after = validity [ 'not_after' ] . native not_before = validity [ 'not_before' ] . native now = datetime . now ( timezone . utc ) if not_before > now : formatted_before = not_before . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate not valid until %s' % formatted_before elif not_after < now : formatted_after = not_after . strftime ( '%Y-%m-%d %H:%M:%SZ' ) message = 'Server certificate verification failed - certificate expired %s' % formatted_after raise TLSVerificationError ( message , certificate )
13398	def check_docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . __name__ for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . __name__ , breadcrumbs , cls . __module__ ) raise InternalCashewException ( msg % args ) max_line_length = cls . _class_settings . get ( 'max-docstring-length' ) if max_line_length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max_line_length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . __name__ , len ( line ) - max_line_length ) raise Exception ( msg % args ) return docstring
11811	def score ( self , word , docid ) : "Compute a score for this word on this docid." return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
525	def _inhibitColumnsGlobal ( self , overlaps , density ) : numActive = int ( density * self . _numColumns ) sortedWinnerIndices = numpy . argsort ( overlaps , kind = 'mergesort' ) start = len ( sortedWinnerIndices ) - numActive while start < len ( sortedWinnerIndices ) : i = sortedWinnerIndices [ start ] if overlaps [ i ] >= self . _stimulusThreshold : break else : start += 1 return sortedWinnerIndices [ start : ] [ : : - 1 ]
1707	def run ( command , data = None , timeout = None , kill_timeout = None , env = None , cwd = None ) : command = expand_args ( command ) history = [ ] for c in command : if len ( history ) : data = history [ - 1 ] . std_out [ 0 : 10 * 1024 ] cmd = Command ( c ) try : out , err = cmd . run ( data , timeout , kill_timeout , env , cwd ) status_code = cmd . returncode except OSError as e : out , err = '' , u"\n" . join ( [ e . strerror , traceback . format_exc ( ) ] ) status_code = 127 r = Response ( process = cmd ) r . command = c r . std_out = out r . std_err = err r . status_code = status_code history . append ( r ) r = history . pop ( ) r . history = history return r
9438	def load_network ( self , layers = 1 ) : if layers : ctor = payload_type ( self . type ) [ 0 ] if ctor : ctor = ctor payload = self . payload self . payload = ctor ( payload , layers - 1 ) else : pass
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
7413	def plot ( self ) : if self . results_table == None : return "no results found" else : bb = self . results_table . sort_values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
4639	def set_shared_config ( cls , config ) : assert isinstance ( config , dict ) cls . _sharedInstance . config . update ( config ) if cls . _sharedInstance . instance : cls . _sharedInstance . instance = None
9225	def permutations_with_replacement ( iterable , r = None ) : pool = tuple ( iterable ) n = len ( pool ) r = n if r is None else r for indices in itertools . product ( range ( n ) , repeat = r ) : yield list ( pool [ i ] for i in indices )
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : return [ ] else : return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == ' ' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) return
5566	def params_at_zoom ( self , zoom ) : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) out = dict ( self . _params_at_zoom [ zoom ] , input = { } , output = self . output ) if "input" in self . _params_at_zoom [ zoom ] : flat_inputs = { } for k , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) : if v is None : flat_inputs [ k ] = None else : flat_inputs [ k ] = self . input [ get_hash ( v ) ] out [ "input" ] = _unflatten_tree ( flat_inputs ) else : out [ "input" ] = { } return out
6516	def execute_tools ( config , path , progress = None ) : progress = progress or QuietProgress ( ) progress . on_start ( ) manager = SyncManager ( ) manager . start ( ) num_tools = 0 tools = manager . Queue ( ) for name , cls in iteritems ( get_tools ( ) ) : if config [ name ] [ 'use' ] and cls . can_be_used ( ) : num_tools += 1 tools . put ( { 'name' : name , 'config' : config [ name ] , } ) collector = Collector ( config ) if not num_tools : progress . on_finish ( ) return collector notifications = manager . Queue ( ) environment = manager . dict ( { 'finder' : Finder ( path , config ) , } ) workers = [ ] for _ in range ( config [ 'workers' ] ) : worker = Worker ( args = ( tools , notifications , environment , ) , ) worker . start ( ) workers . append ( worker ) while num_tools : try : notification = notifications . get ( True , 0.25 ) except Empty : pass else : if notification [ 'type' ] == 'start' : progress . on_tool_start ( notification [ 'tool' ] ) elif notification [ 'type' ] == 'complete' : collector . add_issues ( notification [ 'issues' ] ) progress . on_tool_finish ( notification [ 'tool' ] ) num_tools -= 1 progress . on_finish ( ) return collector
13196	def ensure_format ( doc , format ) : assert format in ( 'xml' , 'json' ) if getattr ( doc , 'tag' , None ) == 'open511' : if format == 'json' : return xml_to_json ( doc ) elif isinstance ( doc , dict ) and 'meta' in doc : if format == 'xml' : return json_doc_to_xml ( doc ) else : raise ValueError ( "Unrecognized input document" ) return doc
6401	def _undouble ( self , word ) : if ( len ( word ) > 1 and word [ - 1 ] == word [ - 2 ] and word [ - 1 ] in { 'd' , 'k' , 't' } ) : return word [ : - 1 ] return word
4508	def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IOError ( e )
1946	def emulate ( self , instruction ) : while True : self . _should_try_again = False self . _to_raise = None self . _step ( instruction ) if not self . _should_try_again : break
10744	def print_runtime ( function ) : def wrapper ( * args , ** kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , ** kwargs ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'tot' ) . print_stats ( 20 ) return output return wrapper
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
2018	def MOD ( self , a , b ) : try : result = Operators . ITEBV ( 256 , b == 0 , 0 , a % b ) except ZeroDivisionError : result = 0 return result
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
2380	def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
3051	def FromResponse ( cls , response ) : kwargs = { 'device_code' : response [ 'device_code' ] , 'user_code' : response [ 'user_code' ] , } verification_url = response . get ( 'verification_url' , response . get ( 'verification_uri' ) ) if verification_url is None : raise OAuth2DeviceCodeError ( 'No verification_url provided in server response' ) kwargs [ 'verification_url' ] = verification_url kwargs . update ( { 'interval' : response . get ( 'interval' ) , 'user_code_expiry' : None , } ) if 'expires_in' in response : kwargs [ 'user_code_expiry' ] = ( _UTCNOW ( ) + datetime . timedelta ( seconds = int ( response [ 'expires_in' ] ) ) ) return cls ( ** kwargs )
8019	async def websocket_send ( self , message , stream_name ) : text = message . get ( "text" ) json = await self . decode_json ( text ) data = { "stream" : stream_name , "payload" : json } await self . send_json ( data )
9684	def sn ( self ) : string = [ ] self . cnxn . xfer ( [ 0x10 ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] string . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( string )
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
1953	def input_from_cons ( constupl , datas ) : ' solve bytes in |datas| based on ' def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
3726	def Pc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Pc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Pc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) : methods . append ( PSRK ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) : methods . append ( PD ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Pc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Pc = float ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) elif Method == MATTHEWS : _Pc = float ( _crit_Matthews . at [ CASRN , 'Pc' ] ) elif Method == CRC : _Pc = float ( _crit_CRC . at [ CASRN , 'Pc' ] ) elif Method == PSRK : _Pc = float ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) elif Method == PD : _Pc = float ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) elif Method == YAWS : _Pc = float ( _crit_Yaws . at [ CASRN , 'Pc' ] ) elif Method == SURF : _Pc = third_property ( CASRN = CASRN , P = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Pc
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
3049	def _get_implicit_credentials ( cls ) : environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
2708	def limit_keyphrases ( path , phrase_limit = 20 ) : rank_thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json_iter ( path ) : rl = RankedLexeme ( ** meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank_thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank_thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != "v" : if ( used > phrase_limit ) or ( rl . rank < rank_thresh ) : return used += 1 yield rl . text . replace ( " - " , "-" )
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
3968	def get_compose_dict ( assembled_specs , port_specs ) : compose_dict = _compose_dict_for_nginx ( port_specs ) for app_name in assembled_specs [ 'apps' ] . keys ( ) : compose_dict [ app_name ] = _composed_app_dict ( app_name , assembled_specs , port_specs ) for service_spec in assembled_specs [ 'services' ] . values ( ) : compose_dict [ service_spec . name ] = _composed_service_dict ( service_spec ) return compose_dict
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
4316	def validate_output_file ( output_filepath ) : nowrite_conditions = [ bool ( os . path . dirname ( output_filepath ) ) or not os . access ( os . getcwd ( ) , os . W_OK ) , not os . access ( os . path . dirname ( output_filepath ) , os . W_OK ) ] if all ( nowrite_conditions ) : raise IOError ( "SoX cannot write to output_filepath {}" . format ( output_filepath ) ) ext = file_extension ( output_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) ) if os . path . exists ( output_filepath ) : logger . warning ( 'output_file: %s already exists and will be overwritten on build' , output_filepath )
11756	def prop_symbols ( x ) : "Return a list of all propositional symbols in x." if not isinstance ( x , Expr ) : return [ ] elif is_prop_symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop_symbols ( arg ) ) )
2868	def get_platform_gpio ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , ** keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , ** keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
3432	def add_groups ( self , group_list ) : def existing_filter ( group ) : if group . id in self . groups : LOGGER . warning ( "Ignoring group '%s' since it already exists." , group . id ) return False return True if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] pruned = DictList ( filter ( existing_filter , group_list ) ) for group in pruned : group . _model = self for member in group . members : if isinstance ( member , Metabolite ) : if member not in self . metabolites : self . add_metabolites ( [ member ] ) if isinstance ( member , Reaction ) : if member not in self . reactions : self . add_reactions ( [ member ] ) self . groups += [ group ]
7773	def _quote ( data ) : data = data . replace ( b'\\' , b'\\\\' ) data = data . replace ( b'"' , b'\\"' ) return data
12486	def get_possible_paths ( base_path , path_regex ) : if not path_regex : return [ ] if len ( path_regex ) < 1 : return [ ] if path_regex [ 0 ] == os . sep : path_regex = path_regex [ 1 : ] rest_files = '' if os . sep in path_regex : node_names = path_regex . partition ( os . sep ) first_node = node_names [ 0 ] rest_nodes = node_names [ 2 ] folder_names = filter_list ( os . listdir ( base_path ) , first_node ) for nom in folder_names : new_base = op . join ( base_path , nom ) if op . isdir ( new_base ) : rest_files = get_possible_paths ( new_base , rest_nodes ) else : rest_files = filter_list ( os . listdir ( base_path ) , path_regex ) files = [ ] if rest_files : files = [ op . join ( base_path , f ) for f in rest_files ] return files
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
2001	def visit_BitVecOr ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left elif right . value == left . mask : return right elif isinstance ( left , BitVecOr ) : left_left = left . operands [ 0 ] left_right = left . operands [ 1 ] if isinstance ( right , Constant ) : return BitVecOr ( left_left , ( left_right | right ) , taint = expression . taint ) elif isinstance ( left , BitVecConstant ) : return BitVecOr ( right , left , taint = expression . taint )
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
13012	def pprint ( arr , columns = ( 'temperature' , 'luminosity' ) , names = ( 'Temperature (Kelvin)' , 'Luminosity (solar units)' ) , max_rows = 32 , precision = 2 ) : if max_rows is True : pd . set_option ( 'display.max_rows' , 1000 ) elif type ( max_rows ) is int : pd . set_option ( 'display.max_rows' , max_rows ) pd . set_option ( 'precision' , precision ) df = pd . DataFrame ( arr . flatten ( ) , index = arr [ 'id' ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ 0 ] : '{:.0f}' , names [ 1 ] : '{:.2f}' } )
8179	def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
5926	def get_tool_names ( ) : names = [ ] for group in cfg . get ( 'Gromacs' , 'groups' ) . split ( ) : names . extend ( cfg . get ( 'Gromacs' , group ) . split ( ) ) return names
11879	def scanProcessForCwd ( pid , searchPortion , isExactMatch = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e cwd = getProcessCwd ( pid ) if not cwd : return None isMatch = False if isExactMatch is True : if searchPortion == cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] == cwd : isMatch = True else : if searchPortion in cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] in cwd : isMatch = True if not isMatch : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'cwd' : cwd , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
3534	def olark ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OlarkNode ( )
9569	def build_message ( self , data ) : if not data : return None return Message ( id = data [ 'message' ] [ 'mid' ] , platform = self . platform , text = data [ 'message' ] [ 'text' ] , user = data [ 'sender' ] [ 'id' ] , timestamp = data [ 'timestamp' ] , raw = data , chat = None , )
11863	def show_approx ( self , numfmt = '%.3g' ) : return ', ' . join ( [ ( '%s: ' + numfmt ) % ( v , p ) for ( v , p ) in sorted ( self . prob . items ( ) ) ] )
10650	def add_activity ( self , activity ) : self . gl . structure . validate_account_names ( activity . get_referenced_accounts ( ) ) self . activities . append ( activity ) activity . set_parent_path ( self . path )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
1997	def sync_svc ( state ) : syscall = state . cpu . R7 name = linux_syscalls . armv7 [ syscall ] logger . debug ( f"Syncing syscall: {name}" ) try : if 'mmap' in name : returned = gdb . getR ( 'R0' ) logger . debug ( f"Syncing mmap ({returned:x})" ) state . cpu . write_register ( 'R0' , returned ) if 'exit' in name : return except ValueError : for reg in state . cpu . canonical_registers : print ( f'{reg}: {state.cpu.read_register(reg):x}' ) raise
2463	def set_file_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_comment_set : self . file_comment_set = True if validations . validate_file_comment ( text ) : self . file ( doc ) . comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'File::Comment' ) else : raise CardinalityError ( 'File::Comment' ) else : raise OrderError ( 'File::Comment' )
7354	def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
13782	def _ConvertEnumDescriptor ( self , enum_proto , package = None , file_desc = None , containing_type = None , scope = None ) : if package : enum_name = '.' . join ( ( package , enum_proto . name ) ) else : enum_name = enum_proto . name if file_desc is None : file_name = None else : file_name = file_desc . name values = [ self . _MakeEnumValueDescriptor ( value , index ) for index , value in enumerate ( enum_proto . value ) ] desc = descriptor . EnumDescriptor ( name = enum_proto . name , full_name = enum_name , filename = file_name , file = file_desc , values = values , containing_type = containing_type , options = enum_proto . options ) scope [ '.%s' % enum_name ] = desc self . _enum_descriptors [ enum_name ] = desc return desc
11334	def table ( * columns , ** kwargs ) : ret = [ ] prefix = kwargs . get ( 'prefix' , '' ) buf_count = kwargs . get ( 'buf_count' , 2 ) if len ( columns ) == 1 : columns = list ( columns [ 0 ] ) else : columns = list ( zip ( * columns ) ) headers = kwargs . get ( "headers" , [ ] ) if headers : columns . insert ( 0 , headers ) widths = kwargs . get ( "widths" , [ ] ) row_counts = Counter ( ) for i in range ( len ( widths ) ) : row_counts [ i ] = int ( widths [ i ] ) width = int ( kwargs . get ( "width" , 0 ) ) for row in columns : for i , c in enumerate ( row ) : if isinstance ( c , basestring ) : cl = len ( c ) else : cl = len ( str ( c ) ) if cl > row_counts [ i ] : row_counts [ i ] = cl width = int ( kwargs . get ( "width" , 0 ) ) if width : for i in row_counts : if row_counts [ i ] < width : row_counts [ i ] = width def colstr ( c ) : if isinstance ( c , basestring ) : return c return str ( c ) def rowstr ( row , prefix , row_counts ) : row_format = prefix cols = list ( map ( colstr , row ) ) for i in range ( len ( row_counts ) ) : c = cols [ i ] if re . match ( r"^\d+(?:\.\d+)?$" , c ) : if i == 0 : row_format += "{:>" + str ( row_counts [ i ] ) + "}" else : row_format += "{:>" + str ( row_counts [ i ] + buf_count ) + "}" else : row_format += "{:<" + str ( row_counts [ i ] + buf_count ) + "}" return row_format . format ( * cols ) for row in columns : ret . append ( rowstr ( row , prefix , row_counts ) ) out ( os . linesep . join ( ret ) )
13221	def breakfast ( self , message = "Breakfast is ready" , shout : bool = False ) : return self . helper . output ( message , shout )
2150	def delete ( self , pk = None , fail_on_missing = False , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . delete ( pk = pk , fail_on_missing = fail_on_missing , ** kwargs )
13913	def _InternalUnpackAny ( msg ) : type_url = msg . type_url db = symbol_database . Default ( ) if not type_url : return None type_name = type_url . split ( "/" ) [ - 1 ] descriptor = db . pool . FindMessageTypeByName ( type_name ) if descriptor is None : return None message_class = db . GetPrototype ( descriptor ) message = message_class ( ) message . ParseFromString ( msg . value ) return message
12521	def to_file ( self , output_file , smooth_fwhm = 0 , outdtype = None ) : outmat , mask_indices , mask_shape = self . to_matrix ( smooth_fwhm , outdtype ) exporter = ExportData ( ) content = { 'data' : outmat , 'labels' : self . labels , 'mask_indices' : mask_indices , 'mask_shape' : mask_shape , } if self . others : content . update ( self . others ) log . debug ( 'Creating content in file {}.' . format ( output_file ) ) try : exporter . save_variables ( output_file , content ) except Exception as exc : raise Exception ( 'Error saving variables to file {}.' . format ( output_file ) ) from exc
4060	def item_template ( self , itemtype ) : template_name = "item_template_" + itemtype query_string = "/items/new?itemType={i}" . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return copy . deepcopy ( self . templates [ template_name ] [ "tmplt" ] ) retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
11606	def condense_ranges ( cls , ranges ) : result = [ ] if ranges : ranges . sort ( key = lambda tup : tup [ 0 ] ) result . append ( ranges [ 0 ] ) for i in range ( 1 , len ( ranges ) ) : if result [ - 1 ] [ 1 ] + 1 >= ranges [ i ] [ 0 ] : result [ - 1 ] = ( result [ - 1 ] [ 0 ] , max ( result [ - 1 ] [ 1 ] , ranges [ i ] [ 1 ] ) ) else : result . append ( ranges [ i ] ) return result
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
9765	def check ( file , version , definition ) : file = file or 'polyaxonfile.yaml' specification = check_polyaxonfile ( file ) . specification if version : Printer . decorate_format_value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job_condition = ( specification . is_job or specification . is_build or specification . is_notebook or specification . is_tensorboard ) if specification . is_experiment : Printer . decorate_format_value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job_condition : Printer . decorate_format_value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is_group : experiments_def = specification . experiments_def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get_group_experiments_info ( ** experiments_def ) return specification
10813	def search ( cls , query , q ) : return query . filter ( Group . name . like ( '%{0}%' . format ( q ) ) )
3571	def centralManager_didDisconnectPeripheral_error_ ( self , manager , peripheral , error ) : logger . debug ( 'centralManager_didDisconnectPeripheral called' ) device = device_list ( ) . get ( peripheral ) if device is not None : device . _set_disconnected ( ) device_list ( ) . remove ( peripheral )
2459	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True if validations . validate_pkg_summary ( text ) : doc . package . summary = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Summary' ) else : raise CardinalityError ( 'Package::Summary' )
1667	def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : if is_forward_declaration : return len ( nesting_state . stack ) >= 1 and ( isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) return ( len ( nesting_state . stack ) > 1 and nesting_state . stack [ - 1 ] . check_namespace_indentation and isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) )
5764	def _unarmor_pem ( data , password = None ) : object_type , headers , der_bytes = pem . unarmor ( data ) type_regex = '^((DSA|EC|RSA) PRIVATE KEY|ENCRYPTED PRIVATE KEY|PRIVATE KEY|PUBLIC KEY|RSA PUBLIC KEY|CERTIFICATE)' armor_type = re . match ( type_regex , object_type ) if not armor_type : raise ValueError ( pretty_message ( ) ) pem_header = armor_type . group ( 1 ) data = data . strip ( ) if pem_header in set ( [ 'RSA PRIVATE KEY' , 'DSA PRIVATE KEY' , 'EC PRIVATE KEY' ] ) : algo = armor_type . group ( 2 ) . lower ( ) return ( 'private key' , algo , _unarmor_pem_openssl_private ( headers , der_bytes , password ) ) key_type = pem_header . lower ( ) algo = None if key_type == 'encrypted private key' : key_type = 'private key' elif key_type == 'rsa public key' : key_type = 'public key' algo = 'rsa' return ( key_type , algo , der_bytes )
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
3218	def get_route_tables ( vpc , ** conn ) : route_tables = describe_route_tables ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) rt_ids = [ ] for r in route_tables : rt_ids . append ( r [ "RouteTableId" ] ) return rt_ids
8880	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'tree' ] ) X = check_array ( X ) return self . tree . query ( X ) [ 0 ] . flatten ( )
8466	def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { "http" : self . config [ 'proxy' ] , "https" : self . config [ 'proxy' ] } options [ "url" ] = self . config [ 'url' ] options [ "data" ] = { "issues" : json . dumps ( map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( ** options ) else : requests . post ( ** options )
3671	def identify_phase ( T , P , Tm = None , Tb = None , Tc = None , Psat = None ) : r if Tm and T <= Tm : return 's' elif Tc and T >= Tc : return 'g' elif Psat : if P <= Psat : return 'g' elif P > Psat : return 'l' elif Tb : if 9E4 < P < 1.1E5 : if T < Tb : return 'l' else : return 'g' elif P > 1.1E5 and T <= Tb : return 'l' else : return None else : return None
6371	def fallout ( self ) : r if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
13864	def tsms ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) ) * 1000 + int ( round ( when . microsecond / 1000.0 ) )
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
2344	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{CUTOFF}' ] = str ( self . cutoff ) self . arguments [ '{VARSEL}' ] = str ( self . variablesel ) . upper ( ) self . arguments [ '{SELMETHOD}' ] = self . var_selection [ self . selmethod ] self . arguments [ '{PRUNING}' ] = str ( self . pruning ) . upper ( ) self . arguments [ '{PRUNMETHOD}' ] = self . var_selection [ self . prunmethod ] self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_cam ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
3026	def save_to_well_known_file ( credentials , well_known_file = None ) : if well_known_file is None : well_known_file = _get_well_known_file ( ) config_dir = os . path . dirname ( well_known_file ) if not os . path . isdir ( config_dir ) : raise OSError ( 'Config directory does not exist: {0}' . format ( config_dir ) ) credentials_data = credentials . serialization_data _save_private_file ( well_known_file , credentials_data )
3212	def _update_cache_stats ( self , key , result ) : if result is None : self . _CACHE_STATS [ 'access_stats' ] . setdefault ( key , { 'hit' : 0 , 'miss' : 0 , 'expired' : 0 } ) else : self . _CACHE_STATS [ 'access_stats' ] [ key ] [ result ] += 1
7	def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : assert isinstance ( ob_space , Discrete ) or isinstance ( ob_space , Box ) or isinstance ( ob_space , MultiDiscrete ) , 'Can only deal with Discrete and Box observation spaces for now' dtype = ob_space . dtype if dtype == np . int8 : dtype = np . uint8 return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name )
1732	def is_empty_object ( n , last ) : if n . strip ( ) : return False last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
718	def emit ( self , modelInfo ) : if self . __csvFileObj is None : self . __openAndInitCSVFile ( modelInfo ) csv = self . __csvFileObj print >> csv , "%s, " % ( self . __searchJobID ) , print >> csv , "%s, " % ( modelInfo . getModelID ( ) ) , print >> csv , "%s, " % ( modelInfo . statusAsString ( ) ) , if modelInfo . isFinished ( ) : print >> csv , "%s, " % ( modelInfo . getCompletionReason ( ) ) , else : print >> csv , "NA, " , if not modelInfo . isWaitingToStart ( ) : print >> csv , "%s, " % ( modelInfo . getStartTime ( ) ) , else : print >> csv , "NA, " , if modelInfo . isFinished ( ) : dateFormat = "%Y-%m-%d %H:%M:%S" startTime = modelInfo . getStartTime ( ) endTime = modelInfo . getEndTime ( ) print >> csv , "%s, " % endTime , st = datetime . strptime ( startTime , dateFormat ) et = datetime . strptime ( endTime , dateFormat ) print >> csv , "%s, " % ( str ( ( et - st ) . seconds ) ) , else : print >> csv , "NA, " , print >> csv , "NA, " , print >> csv , "%s, " % str ( modelInfo . getModelDescription ( ) ) , print >> csv , "%s, " % str ( modelInfo . getNumRecords ( ) ) , paramLabelsDict = modelInfo . getParamLabels ( ) for key in self . __sortedVariableNames : if key in paramLabelsDict : print >> csv , "%s, " % ( paramLabelsDict [ key ] ) , else : print >> csv , "None, " , metrics = modelInfo . getReportMetrics ( ) for key in self . __sortedMetricsKeys : value = metrics . get ( key , "NA" ) value = str ( value ) value = value . replace ( "\n" , " " ) print >> csv , "%s, " % ( value ) , print >> csv
10903	def examine_unexplained_noise ( state , bins = 1000 , xlim = ( - 10 , 10 ) ) : r = state . residuals q = np . fft . fftn ( r ) calc_sig = lambda x : np . sqrt ( np . dot ( x , x ) / x . size ) rh , xr = np . histogram ( r . ravel ( ) / calc_sig ( r . ravel ( ) ) , bins = bins , density = True ) bigq = np . append ( q . real . ravel ( ) , q . imag . ravel ( ) ) qh , xq = np . histogram ( bigq / calc_sig ( q . real . ravel ( ) ) , bins = bins , density = True ) xr = 0.5 * ( xr [ 1 : ] + xr [ : - 1 ] ) xq = 0.5 * ( xq [ 1 : ] + xq [ : - 1 ] ) gauss = lambda t : np . exp ( - t * t * 0.5 ) / np . sqrt ( 2 * np . pi ) plt . figure ( figsize = [ 16 , 8 ] ) axes = [ ] for a , ( x , r , lbl ) in enumerate ( [ [ xr , rh , 'Real' ] , [ xq , qh , 'Fourier' ] ] ) : ax = plt . subplot ( 1 , 2 , a + 1 ) ax . semilogy ( x , r , label = 'Data' ) ax . plot ( x , gauss ( x ) , label = 'Gauss Fit' , scalex = False , scaley = False ) ax . set_xlabel ( 'Residuals value $r/\sigma$' ) ax . set_ylabel ( 'Probability $P(r/\sigma)$' ) ax . legend ( loc = 'upper right' ) ax . set_title ( '{}-Space' . format ( lbl ) ) ax . set_xlim ( xlim ) axes . append ( ax ) return axes
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
2063	def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
8974	def new_knitting_pattern_set_loader ( specification = DefaultSpecification ( ) ) : parser = specification . new_parser ( specification ) loader = specification . new_loader ( parser . knitting_pattern_set ) return loader
11556	def enable_digital_reporting ( self , pin ) : port = pin // 8 command = [ self . _command_handler . REPORT_DIGITAL + port , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
1351	def make_response ( self , status ) : response = { constants . RESPONSE_KEY_STATUS : status , constants . RESPONSE_KEY_VERSION : constants . API_VERSION , constants . RESPONSE_KEY_EXECUTION_TIME : 0 , constants . RESPONSE_KEY_MESSAGE : "" , } return response
2336	def aracne ( m , ** kwargs ) : I0 = kwargs . get ( 'I0' , 0.0 ) W0 = kwargs . get ( 'W0' , 0.05 ) m = np . where ( m > I0 , m , 0 ) for i in range ( m . shape [ 0 ] - 2 ) : for j in range ( i + 1 , m . shape [ 0 ] - 1 ) : for k in range ( j + 1 , m . shape [ 0 ] ) : triplet = [ m [ i , j ] , m [ j , k ] , m [ i , k ] ] min_index , min_value = min ( enumerate ( triplet ) , key = operator . itemgetter ( 1 ) ) if 0 < min_value < W0 : if min_index == 0 : m [ i , j ] = m [ j , i ] = 0. elif min_index == 1 : m [ j , k ] = m [ k , j ] = 0. else : m [ i , k ] = m [ k , i ] = 0. return m
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
8	def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : placeholder = observation_placeholder ( ob_space , batch_size , name ) return placeholder , encode_observation ( ob_space , placeholder )
7500	def get_spans ( maparr , spans ) : bidx = 1 spans = np . zeros ( ( maparr [ - 1 , 0 ] , 2 ) , np . uint64 ) for idx in xrange ( 1 , maparr . shape [ 0 ] ) : cur = maparr [ idx , 0 ] if cur != bidx : idy = idx + 1 spans [ cur - 2 , 1 ] = idx spans [ cur - 1 , 0 ] = idx bidx = cur spans [ - 1 , 1 ] = maparr [ - 1 , - 1 ] return spans
10481	def _generateChildren ( self ) : try : children = self . AXChildren except _a11y . Error : return if children : for child in children : yield child
4724	def main ( conf ) : fpath = yml_fpath ( conf [ "OUTPUT" ] ) if os . path . exists ( fpath ) : cij . err ( "main:FAILED { fpath: %r }, exists" % fpath ) return 1 trun = trun_setup ( conf ) if not trun : return 1 trun_to_file ( trun ) trun_emph ( trun ) tr_err = 0 tr_ent_err = trun_enter ( trun ) for tsuite in ( ts for ts in trun [ "testsuites" ] if not tr_ent_err ) : ts_err = 0 ts_ent_err = tsuite_enter ( trun , tsuite ) for tcase in ( tc for tc in tsuite [ "testcases" ] if not ts_ent_err ) : tc_err = tcase_enter ( trun , tsuite , tcase ) if not tc_err : tc_err += script_run ( trun , tcase ) tc_err += tcase_exit ( trun , tsuite , tcase ) tcase [ "status" ] = "FAIL" if tc_err else "PASS" trun [ "progress" ] [ tcase [ "status" ] ] += 1 trun [ "progress" ] [ "UNKN" ] -= 1 ts_err += tc_err trun_to_file ( trun ) if not ts_ent_err : ts_err += tsuite_exit ( trun , tsuite ) ts_err += ts_ent_err tr_err += ts_err tsuite [ "status" ] = "FAIL" if ts_err else "PASS" cij . emph ( "rnr:tsuite %r" % tsuite [ "status" ] , tsuite [ "status" ] != "PASS" ) if not tr_ent_err : trun_exit ( trun ) tr_err += tr_ent_err trun [ "status" ] = "FAIL" if tr_err else "PASS" trun [ "stamp" ] [ "end" ] = int ( time . time ( ) ) + 1 trun_to_file ( trun ) cij . emph ( "rnr:main:progress %r" % trun [ "progress" ] ) cij . emph ( "rnr:main:trun %r" % trun [ "status" ] , trun [ "status" ] != "PASS" ) return trun [ "progress" ] [ "UNKN" ] + trun [ "progress" ] [ "FAIL" ]
640	def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )
11192	def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
3646	def sendToWatchlist ( self , trade_id ) : method = 'PUT' url = 'watchlist' data = { 'auctionInfo' : [ { 'id' : trade_id } ] } return self . __request__ ( method , url , data = json . dumps ( data ) )
3542	def popen_streaming_output ( cmd , callback , timeout = None ) : if os . name == 'nt' : process = subprocess . Popen ( shlex . split ( cmd ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) stdout = process . stdout else : master , slave = os . openpty ( ) process = subprocess . Popen ( shlex . split ( cmd , posix = True ) , stdout = slave , stderr = slave ) stdout = os . fdopen ( master ) os . close ( slave ) def kill ( process_ ) : try : process_ . kill ( ) except OSError : pass timer = Timer ( timeout , kill , [ process ] ) timer . setDaemon ( True ) timer . start ( ) while process . returncode is None : try : if os . name == 'nt' : line = stdout . readline ( ) line = line . decode ( "utf-8" ) if line : callback ( line . rstrip ( ) ) else : while True : line = stdout . readline ( ) if not line : break callback ( line . rstrip ( ) ) except ( IOError , OSError ) : pass if not timer . is_alive ( ) : raise TimeoutError ( "subprocess running command '{}' timed out after {} seconds" . format ( cmd , timeout ) ) process . poll ( ) timer . cancel ( ) return process . returncode
10438	def startprocessmonitor ( self , process_name , interval = 2 ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) self . _process_stats [ process_name ] = ProcessStats ( process_name , interval ) self . _process_stats [ process_name ] . start ( ) return 1
11691	def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get_bounds ( ch ) . intersects ( self . area ) ]
5553	def _validate_zooms ( zooms ) : if isinstance ( zooms , dict ) : if any ( [ a not in zooms for a in [ "min" , "max" ] ] ) : raise MapcheteConfigError ( "min and max zoom required" ) zmin = _validate_zoom ( zooms [ "min" ] ) zmax = _validate_zoom ( zooms [ "max" ] ) if zmin > zmax : raise MapcheteConfigError ( "max zoom must not be smaller than min zoom" ) return list ( range ( zmin , zmax + 1 ) ) elif isinstance ( zooms , list ) : if len ( zooms ) == 1 : return zooms elif len ( zooms ) == 2 : zmin , zmax = sorted ( [ _validate_zoom ( z ) for z in zooms ] ) return list ( range ( zmin , zmax + 1 ) ) else : return zooms else : return [ _validate_zoom ( zooms ) ]
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , ** kwargs )
9920	def save ( self ) : token = models . PasswordResetToken . objects . get ( key = self . validated_data [ "key" ] ) token . email . user . set_password ( self . validated_data [ "password" ] ) token . email . user . save ( ) logger . info ( "Reset password for %s" , token . email . user ) token . delete ( )
2731	def create ( self ) : data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
1229	def optimizer_arguments ( self , states , internals , actions , terminal , reward , next_states , next_internals ) : arguments = dict ( time = self . global_timestep , variables = self . get_variables ( ) , arguments = dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = tf . constant ( value = True ) ) , fn_reference = self . fn_reference , fn_loss = self . fn_loss ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . get_variables ( ) return arguments
958	def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }
12932	def nav_to_vcf_dir ( ftp , build ) : if build == 'b37' : ftp . cwd ( DIR_CLINVAR_VCF_B37 ) elif build == 'b38' : ftp . cwd ( DIR_CLINVAR_VCF_B38 ) else : raise IOError ( "Genome build not recognized." )
7013	def read_hatpi_pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except UnicodeDecodeError : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict
12809	def received ( self , messages ) : if messages : if self . _queue : self . _queue . put_nowait ( messages ) if self . _callback : self . _callback ( messages )
7314	def search ( self ) : try : filters = json . loads ( self . query ) except ValueError : return False result = self . model_query if 'filter' in filters . keys ( ) : result = self . parse_filter ( filters [ 'filter' ] ) if 'sort' in filters . keys ( ) : result = result . order_by ( * self . sort ( filters [ 'sort' ] ) ) return result
8747	def create_scalingip ( context , content ) : LOG . info ( 'create_scalingip for tenant %s and body %s' , context . tenant_id , content ) network_id = content . get ( 'scaling_network_id' ) ip_address = content . get ( 'scaling_ip_address' ) requested_ports = content . get ( 'ports' , [ ] ) network = _get_network ( context , network_id ) port_fixed_ips = { } for req_port in requested_ports : port = _get_port ( context , req_port [ 'port_id' ] ) fixed_ip = _get_fixed_ip ( context , req_port . get ( 'fixed_ip_address' ) , port ) port_fixed_ips [ port . id ] = { "port" : port , "fixed_ip" : fixed_ip } scip = _allocate_ip ( context , network , None , ip_address , ip_types . SCALING ) _create_flip ( context , scip , port_fixed_ips ) return v . _make_scaling_ip_dict ( scip )
1823	def SETS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) )
2842	def write_gppu ( self , gppu = None ) : if gppu is not None : self . gppu = gppu self . _device . writeList ( self . GPPU , self . gppu )
13482	def sphinx_make ( * targets ) : sh ( 'make %s' % ' ' . join ( targets ) , cwd = options . paved . docs . path )
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
7316	def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
7261	def search ( self , searchAreaWkt = None , filters = None , startDate = None , endDate = None , types = None ) : if not types : types = [ 'Acquisition' ] if startDate : startDateTime = datetime . datetime . strptime ( startDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if endDate : endDateTime = datetime . datetime . strptime ( endDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if startDate and endDate : diff = endDateTime - startDateTime if diff . days < 0 : raise Exception ( "startDate must come before endDate." ) postdata = { "searchAreaWkt" : searchAreaWkt , "types" : types , "startDate" : startDate , "endDate" : endDate , } if filters : postdata [ 'filters' ] = filters if searchAreaWkt : postdata [ 'searchAreaWkt' ] = searchAreaWkt url = '%(base_url)s/search' % { 'base_url' : self . base_url } headers = { 'Content-Type' : 'application/json' } r = self . gbdx_connection . post ( url , headers = headers , data = json . dumps ( postdata ) ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] return results
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
10159	def fresh_cookies ( ctx , mold = '' ) : mold = mold or "https://github.com/Springerle/py-generic-project.git" tmpdir = os . path . join ( tempfile . gettempdir ( ) , "cc-upgrade-pygments-markdown-lexer" ) if os . path . isdir ( '.git' ) : pass if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore_patterns ( ".git" , ".svn" , "*~" , ) ) else : ctx . run ( "git clone {} {}" . format ( mold , tmpdir ) ) shutil . copy2 ( "project.d/cookiecutter.json" , tmpdir ) with pushd ( '..' ) : ctx . run ( "cookiecutter --no-input {}" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( "git status" )
8592	def restore_snapshot ( self , datacenter_id , volume_id , snapshot_id ) : data = { 'snapshotId' : snapshot_id } response = self . _perform_request ( url = '/datacenters/%s/volumes/%s/restore-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION' , data = urlencode ( data ) ) return response
6086	def unmasked_blurred_image_of_planes_and_galaxies_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : return [ plane . unmasked_blurred_image_of_galaxies_from_psf ( padded_grid_stack , psf ) for plane in planes ]
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
8316	def parse_images ( self , markup , treshold = 6 ) : images = [ ] m = re . findall ( self . re [ "image" ] , markup ) for p in m : p = self . parse_balanced_image ( p ) img = p . split ( "|" ) path = img [ 0 ] . replace ( "[[Image:" , "" ) . strip ( ) description = u"" links = { } properties = [ ] if len ( img ) > 1 : img = "|" . join ( img [ 1 : ] ) links = self . parse_links ( img ) properties = self . plain ( img ) . split ( "|" ) description = u"" if len ( properties [ - 1 ] ) > treshold : description = properties [ - 1 ] properties = properties [ : - 1 ] img = WikipediaImage ( path , description , links , properties ) images . append ( img ) markup = markup . replace ( p , "" ) return images , markup . strip ( )
12400	def add ( self , requirements , required = None ) : if isinstance ( requirements , RequirementsManager ) : requirements = list ( requirements ) elif not isinstance ( requirements , list ) : requirements = [ requirements ] for req in requirements : name = req . project_name if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req , required = required ) elif required is not None : req . required = required add = True if name in self . requirements : for existing_req in self . requirements [ name ] : if req == existing_req : add = False break replace = False if ( req . specs and req . specs [ 0 ] [ 0 ] == '==' and existing_req . specs and existing_req . specs [ 0 ] [ 0 ] == '==' ) : if pkg_resources . parse_version ( req . specs [ 0 ] [ 1 ] ) < pkg_resources . parse_version ( existing_req . specs [ 0 ] [ 1 ] ) : req . requirement = existing_req . requirement replace = True if not ( req . specs and existing_req . specs ) : if existing_req . specs : req . requirement = existing_req . requirement replace = True if replace : req . required |= existing_req . required if existing_req . required_by and not req . required_by : req . required_by = existing_req . required_by self . requirements [ name ] . remove ( existing_req ) break if add : self . requirements [ name ] . append ( req )
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
1726	def except_token ( source , start , token , throw = True ) : start = pass_white ( source , start ) if start < len ( source ) and source [ start ] == token : return start + 1 if throw : raise SyntaxError ( 'Missing token. Expected %s' % token ) return None
8224	def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
8631	def get_project_by_id ( session , project_id , project_details = None , user_details = None ) : query = { } if project_details : query . update ( project_details ) if user_details : query . update ( user_details ) response = make_get_request ( session , 'projects/{}' . format ( project_id ) , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
11463	def connect ( self ) : self . _ftp . connect ( ) self . _ftp . login ( user = self . _username , passwd = self . _passwd )
6942	def invgauss_eclipses_func ( ebparams , times , mags , errs ) : ( period , epoch , pdepth , pduration , depthratio , secondaryphase ) = ebparams iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) primaryecl_amp = - pdepth secondaryecl_amp = - pdepth * depthratio primaryecl_std = pduration / 5.0 secondaryecl_std = pduration / 5.0 halfduration = pduration / 2.0 primary_eclipse_ingress = ( ( phase >= ( 1.0 - halfduration ) ) & ( phase <= 1.0 ) ) primary_eclipse_egress = ( ( phase >= 0.0 ) & ( phase <= halfduration ) ) secondary_eclipse_phase = ( ( phase >= ( secondaryphase - halfduration ) ) & ( phase <= ( secondaryphase + halfduration ) ) ) modelmags [ primary_eclipse_ingress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_ingress ] , primaryecl_amp , 1.0 , primaryecl_std ) ) modelmags [ primary_eclipse_egress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_egress ] , primaryecl_amp , 0.0 , primaryecl_std ) ) modelmags [ secondary_eclipse_phase ] = ( zerolevel + _gaussian ( phase [ secondary_eclipse_phase ] , secondaryecl_amp , secondaryphase , secondaryecl_std ) ) return modelmags , phase , ptimes , pmags , perrs
9686	def pm ( self ) : resp = [ ] data = { } self . cnxn . xfer ( [ 0x32 ] ) sleep ( 10e-3 ) for i in range ( 12 ) : r = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] resp . append ( r ) data [ 'PM1' ] = self . _calculate_float ( resp [ 0 : 4 ] ) data [ 'PM2.5' ] = self . _calculate_float ( resp [ 4 : 8 ] ) data [ 'PM10' ] = self . _calculate_float ( resp [ 8 : ] ) sleep ( 0.1 ) return data
4662	def proposal ( self , proposer = None , proposal_expiration = None , proposal_review = None ) : if not self . _propbuffer : return self . new_proposal ( self . tx ( ) , proposer , proposal_expiration , proposal_review ) if proposer : self . _propbuffer [ 0 ] . set_proposer ( proposer ) if proposal_expiration : self . _propbuffer [ 0 ] . set_expiration ( proposal_expiration ) if proposal_review : self . _propbuffer [ 0 ] . set_review ( proposal_review ) return self . _propbuffer [ 0 ]
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
5039	def is_user_enrolled ( cls , user , course_id , course_mode ) : enrollment_client = EnrollmentApiClient ( ) try : enrollments = enrollment_client . get_course_enrollment ( user . username , course_id ) if enrollments and course_mode == enrollments . get ( 'mode' ) : return True except HttpClientError as exc : logging . error ( 'Error while checking enrollment status of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) except KeyError as exc : logging . warning ( 'Error while parsing enrollment data of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) return False
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
7789	def get_item ( self , address , state = 'fresh' ) : self . _lock . acquire ( ) try : item = self . _items . get ( address ) if not item : return None self . update_item ( item ) if _state_values [ state ] >= item . state_value : return item return None finally : self . _lock . release ( )
13837	def ConsumeIdentifier ( self ) : result = self . token if not self . _IDENTIFIER . match ( result ) : raise self . _ParseError ( 'Expected identifier.' ) self . NextToken ( ) return result
8365	def create_rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDFSurface ( self . _output_file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PSSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVGSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , * size ) return cairo . Context ( surface )
4659	def as_quote ( self , quote ) : if quote == self [ "quote" ] [ "symbol" ] : return self . copy ( ) elif quote == self [ "base" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
8328	def _lastRecursiveChild ( self ) : "Finds the last element beneath this object to be parsed." lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
1018	def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] )
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size dest . write ( src . read ( ) ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
3630	def add_dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
11015	def publish ( context ) : header ( 'Recording changes...' ) run ( 'git add -A' ) header ( 'Displaying changes...' ) run ( 'git -c color.status=always status' ) if not click . confirm ( '\nContinue publishing' ) : run ( 'git reset HEAD --' ) abort ( context ) header ( 'Saving changes...' ) try : run ( 'git commit -m "{message}"' . format ( message = 'Publishing {}' . format ( choose_commit_emoji ( ) ) ) , capture = True ) except subprocess . CalledProcessError as e : if 'nothing to commit' not in e . stdout : raise else : click . echo ( 'Nothing to commit.' ) header ( 'Pushing to GitHub...' ) branch = get_branch ( ) run ( 'git push origin {branch}:{branch}' . format ( branch = branch ) ) pr_link = get_pr_link ( branch ) if pr_link : click . launch ( pr_link )
12065	def gain ( abf ) : Ys = np . nan_to_num ( swhlab . ap . getAvgBySweep ( abf , 'freq' ) ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , Ys , '.-' , ms = 20 , alpha = .5 , color = 'b' ) pylab . axhline ( 0 , alpha = .5 , lw = 2 , color = 'r' , ls = "--" ) pylab . margins ( .1 , .1 )
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
3203	def delete ( self , store_id , cart_id , line_id ) : self . store_id = store_id self . cart_id = cart_id self . line_id = line_id return self . _mc_client . _delete ( url = self . _build_path ( store_id , 'carts' , cart_id , 'lines' , line_id ) )
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
7452	def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
2587	def start ( self ) : start = time . time ( ) self . _kill_event = threading . Event ( ) self . procs = { } for worker_id in range ( self . worker_count ) : p = multiprocessing . Process ( target = worker , args = ( worker_id , self . uid , self . pending_task_queue , self . pending_result_queue , self . ready_worker_queue , ) ) p . start ( ) self . procs [ worker_id ] = p logger . debug ( "Manager synced with workers" ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) logger . info ( "Loop start" ) self . _kill_event . wait ( ) logger . critical ( "[MAIN] Received kill event, terminating worker processes" ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) for proc_id in self . procs : self . procs [ proc_id ] . terminate ( ) logger . critical ( "Terminating worker {}:{}" . format ( self . procs [ proc_id ] , self . procs [ proc_id ] . is_alive ( ) ) ) self . procs [ proc_id ] . join ( ) logger . debug ( "Worker:{} joined successfully" . format ( self . procs [ proc_id ] ) ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "process_worker_pool ran for {} seconds" . format ( delta ) ) return
6652	def findProgram ( self , builddir , program ) : if os . path . isfile ( os . path . join ( builddir , program ) ) : logging . info ( 'found %s' % program ) return program exact_matches = [ ] insensitive_matches = [ ] approx_matches = [ ] for path , dirs , files in os . walk ( builddir ) : if program in files : exact_matches . append ( os . path . relpath ( os . path . join ( path , program ) , builddir ) ) continue files_lower = [ f . lower ( ) for f in files ] if program . lower ( ) in files_lower : insensitive_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( program . lower ( ) ) ] ) , builddir ) ) continue pg_basen_lower_noext = os . path . splitext ( os . path . basename ( program ) . lower ( ) ) [ 0 ] for f in files_lower : if pg_basen_lower_noext in f : approx_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( f ) ] ) , builddir ) ) if len ( exact_matches ) == 1 : logging . info ( 'found %s at %s' , program , exact_matches [ 0 ] ) return exact_matches [ 0 ] elif len ( exact_matches ) > 1 : logging . error ( '%s matches multiple executables, please use a full path (one of %s)' % ( program , ', or ' . join ( [ '"' + os . path . join ( m , program ) + '"' for m in exact_matches ] ) ) ) return None reduced_approx_matches = [ ] for m in approx_matches : root = os . path . splitext ( m ) [ 0 ] if ( m == root ) or ( root not in approx_matches ) : reduced_approx_matches . append ( m ) approx_matches = reduced_approx_matches for matches in ( insensitive_matches , approx_matches ) : if len ( matches ) == 1 : logging . info ( 'found %s at %s' % ( program , matches [ 0 ] ) ) return matches [ 0 ] elif len ( matches ) > 1 : logging . error ( '%s is similar to several executables found. Please use an exact name:\n%s' % ( program , '\n' . join ( matches ) ) ) return None logging . error ( 'could not find program "%s" to debug' % program ) return None
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : return False if ignore_field_order : rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
3548	def _descriptor_changed ( self , descriptor ) : desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
3142	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . file_id = response [ 'id' ] else : self . file_id = None return response
12476	def ux_file_len ( filepath ) : p = subprocess . Popen ( [ 'wc' , '-l' , filepath ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) result , err = p . communicate ( ) if p . returncode != 0 : raise IOError ( err ) l = result . strip ( ) l = int ( l . split ( ) [ 0 ] ) return l
4770	def is_length ( self , length ) : if type ( length ) is not int : raise TypeError ( 'given arg must be an int' ) if length < 0 : raise ValueError ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . _err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
12901	def set_sleep ( self , value = False ) : return ( yield from self . handle_set ( self . API . get ( 'sleep' ) , int ( value ) ) )
11479	def _create_bitstream ( file_path , local_file , item_id , log_ind = None ) : checksum = _streaming_file_md5 ( file_path ) upload_token = session . communicator . generate_upload_token ( session . token , item_id , local_file , checksum ) if upload_token != '' : log_trace = 'Uploading bitstream from {0}' . format ( file_path ) session . communicator . perform_upload ( upload_token , local_file , filepath = file_path , itemid = item_id ) else : log_trace = 'Adding a bitstream link in this item to an existing ' 'bitstream from {0}' . format ( file_path ) if log_ind is not None : log_trace += log_ind print ( log_trace )
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
2013	def _push ( self , value ) : assert isinstance ( value , int ) or isinstance ( value , BitVec ) and value . size == 256 if len ( self . stack ) >= 1024 : raise StackOverflow ( ) if isinstance ( value , int ) : value = value & TT256M1 value = simplify ( value ) if isinstance ( value , Constant ) and not value . taint : value = value . value self . stack . append ( value )
1353	def make_error_response ( self , message ) : response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) response [ constants . RESPONSE_KEY_MESSAGE ] = message return response
4033	def parse ( s ) : if IS_PY3 : r = sre_parse . parse ( s , flags = U ) else : r = sre_parse . parse ( s . decode ( 'utf-8' ) , flags = U ) return list ( r )
8137	def brightness ( self , value = 1.0 ) : b = ImageEnhance . Brightness ( self . img ) self . img = b . enhance ( value )
3488	def _parse_notes_dict ( sbase ) : notes = sbase . getNotesString ( ) if notes and len ( notes ) > 0 : pattern = r"<p>\s*(\w+\s*\w*)\s*:\s*([\w|\s]+)<" matches = re . findall ( pattern , notes ) d = { k . strip ( ) : v . strip ( ) for ( k , v ) in matches } return { k : v for k , v in d . items ( ) if len ( v ) > 0 } else : return { }
6194	def datafile_from_hash ( hash_ , prefix , path ) : pattern = '%s_%s*.h*' % ( prefix , hash_ ) datafiles = list ( path . glob ( pattern ) ) if len ( datafiles ) == 0 : raise NoMatchError ( 'No matches for "%s"' % pattern ) if len ( datafiles ) > 1 : raise MultipleMatchesError ( 'More than 1 match for "%s"' % pattern ) return datafiles [ 0 ]
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
2725	def __get_ssh_keys_id_or_fingerprint ( ssh_keys , token , name ) : ssh_keys_id = list ( ) for ssh_key in ssh_keys : if type ( ssh_key ) in [ int , type ( 2 ** 64 ) ] : ssh_keys_id . append ( int ( ssh_key ) ) elif type ( ssh_key ) == SSHKey : ssh_keys_id . append ( ssh_key . id ) elif type ( ssh_key ) in [ type ( u'' ) , type ( '' ) ] : regexp_of_fingerprint = '([0-9a-fA-F]{2}:){15}[0-9a-fA-F]' match = re . match ( regexp_of_fingerprint , ssh_key ) if match is not None and match . end ( ) == len ( ssh_key ) - 1 : ssh_keys_id . append ( ssh_key ) else : key = SSHKey ( ) key . token = token results = key . load_by_pub_key ( ssh_key ) if results is None : key . public_key = ssh_key key . name = "SSH Key %s" % name key . create ( ) else : key = results ssh_keys_id . append ( key . id ) else : raise BadSSHKeyFormat ( "Droplet.ssh_keys should be a list of IDs, public keys" + " or fingerprints." ) return ssh_keys_id
1128	def SeqN ( n , * inner_rules , ** kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
1590	def _get_dict_from_config ( topology_config ) : config = { } for kv in topology_config . kvs : if kv . HasField ( "value" ) : assert kv . type == topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) if PhysicalPlanHelper . _is_number ( kv . value ) : config [ kv . key ] = PhysicalPlanHelper . _get_number ( kv . value ) elif kv . value . lower ( ) in ( "true" , "false" ) : config [ kv . key ] = True if kv . value . lower ( ) == "true" else False else : config [ kv . key ] = kv . value elif kv . HasField ( "serialized_value" ) and kv . type == topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) : config [ kv . key ] = default_serializer . deserialize ( kv . serialized_value ) else : assert kv . HasField ( "type" ) Log . error ( "Unsupported config <key:value> found: %s, with type: %s" % ( str ( kv ) , str ( kv . type ) ) ) continue return config
7865	def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage realPath = os . path . join ( self . __path , relativePath ) if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass self . save ( )
8286	def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
3065	def _add_query_parameter ( url , name , value ) : if value is None : return url else : return update_query_params ( url , { name : value } )
12842	def receive_id_from_server ( self ) : for message in self . pipe . receive ( ) : if isinstance ( message , IdFactory ) : self . actor_id_factory = message return True return False
7196	def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , : , : ] - data [ 0 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
2576	def _add_input_deps ( self , executor , args , kwargs ) : if executor == 'data_manager' : return args , kwargs inputs = kwargs . get ( 'inputs' , [ ] ) for idx , f in enumerate ( inputs ) : if isinstance ( f , File ) and f . is_remote ( ) : inputs [ idx ] = self . data_manager . stage_in ( f , executor ) for kwarg , f in kwargs . items ( ) : if isinstance ( f , File ) and f . is_remote ( ) : kwargs [ kwarg ] = self . data_manager . stage_in ( f , executor ) newargs = list ( args ) for idx , f in enumerate ( newargs ) : if isinstance ( f , File ) and f . is_remote ( ) : newargs [ idx ] = self . data_manager . stage_in ( f , executor ) return tuple ( newargs ) , kwargs
10601	def _httplib2_init ( username , password ) : obj = httplib2 . Http ( ) if username and password : obj . add_credentials ( username , password ) return obj
3677	def rdkitmol_Hs ( self ) : r if self . __rdkitmol_Hs : return self . __rdkitmol_Hs else : try : self . __rdkitmol_Hs = Chem . AddHs ( self . rdkitmol ) return self . __rdkitmol_Hs except : return None
10718	def normalize_unitnumber ( unit_number ) : try : try : unit_number = int ( unit_number ) except ValueError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) except TypeError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) if not ( 1 <= unit_number <= 16 ) : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) return unit_number
1867	def PMOVMSKB ( cpu , op0 , op1 ) : arg0 = op0 . read ( ) arg1 = op1 . read ( ) res = 0 for i in reversed ( range ( 7 , op1 . size , 8 ) ) : res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) )
8725	def at_time ( cls , at , target ) : at = cls . _from_timestamp ( at ) cmd = cls . from_datetime ( at ) cmd . delay = at - now ( ) cmd . target = target return cmd
11535	def available_drivers ( ) : global __modules global __available if type ( __modules ) is not list : __modules = list ( __modules ) if not __available : __available = [ d . ahioDriverInfo . NAME for d in __modules if d . ahioDriverInfo . AVAILABLE ] return __available
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
9717	async def get_current_frame ( self , components = None ) -> QRTPacket : if components is None : components = [ "all" ] else : _validate_components ( components ) cmd = "getcurrentframe %s" % " " . join ( components ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
10864	def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
4866	def save ( self ) : enterprise_customer = self . validated_data [ 'enterprise_customer' ] ecu = models . EnterpriseCustomerUser ( user_id = self . user . pk , enterprise_customer = enterprise_customer , ) ecu . save ( )
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
7451	def get_quart_iter ( tups ) : if tups [ 0 ] . endswith ( ".gz" ) : ofunc = gzip . open else : ofunc = open ofile1 = ofunc ( tups [ 0 ] , 'r' ) fr1 = iter ( ofile1 ) quart1 = itertools . izip ( fr1 , fr1 , fr1 , fr1 ) if tups [ 1 ] : ofile2 = ofunc ( tups [ 1 ] , 'r' ) fr2 = iter ( ofile2 ) quart2 = itertools . izip ( fr2 , fr2 , fr2 , fr2 ) quarts = itertools . izip ( quart1 , quart2 ) else : ofile2 = 0 quarts = itertools . izip ( quart1 , iter ( int , 1 ) ) def feedme ( quarts ) : for quart in quarts : yield quart genquarts = feedme ( quarts ) return genquarts , ofile1 , ofile2
1589	def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
6047	def map_to_2d_keep_padded ( self , padded_array_1d ) : return mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = padded_array_1d , shape = self . mask . shape )
6411	def heronian_mean ( nums ) : r mag = len ( nums ) rolling_sum = 0 for i in range ( mag ) : for j in range ( i , mag ) : if nums [ i ] == nums [ j ] : rolling_sum += nums [ i ] else : rolling_sum += ( nums [ i ] * nums [ j ] ) ** 0.5 return rolling_sum * 2 / ( mag * ( mag + 1 ) )
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
5305	def detect_color_support ( env ) : if env . get ( 'COLORFUL_DISABLE' , '0' ) == '1' : return NO_COLORS if env . get ( 'COLORFUL_FORCE_8_COLORS' , '0' ) == '1' : return ANSI_8_COLORS if env . get ( 'COLORFUL_FORCE_16_COLORS' , '0' ) == '1' : return ANSI_16_COLORS if env . get ( 'COLORFUL_FORCE_256_COLORS' , '0' ) == '1' : return ANSI_256_COLORS if env . get ( 'COLORFUL_FORCE_TRUE_COLORS' , '0' ) == '1' : return TRUE_COLORS if not sys . stdout . isatty ( ) : return NO_COLORS colorterm_env = env . get ( 'COLORTERM' ) if colorterm_env : if colorterm_env in { 'truecolor' , '24bit' } : return TRUE_COLORS if colorterm_env in { '8bit' } : return ANSI_256_COLORS termprog_env = env . get ( 'TERM_PROGRAM' ) if termprog_env : if termprog_env in { 'iTerm.app' , 'Hyper' } : return TRUE_COLORS if termprog_env in { 'Apple_Terminal' } : return ANSI_256_COLORS term_env = env . get ( 'TERM' ) if term_env : if term_env in { 'screen-256' , 'screen-256color' , 'xterm-256' , 'xterm-256color' } : return ANSI_256_COLORS if term_env in { 'screen' , 'xterm' , 'vt100' , 'color' , 'ansi' , 'cygwin' , 'linux' } : return ANSI_16_COLORS if colorterm_env : return ANSI_16_COLORS return ANSI_8_COLORS
8445	def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( ) : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )
2479	def datetime_iso_format ( date ) : return "{0:0>4}-{1:0>2}-{2:0>2}T{3:0>2}:{4:0>2}:{5:0>2}Z" . format ( date . year , date . month , date . day , date . hour , date . minute , date . second )
1724	def execute ( self , js = None , use_compilation_plan = False ) : try : cache = self . __dict__ [ 'cache' ] except KeyError : cache = self . __dict__ [ 'cache' ] = { } hashkey = hashlib . md5 ( js . encode ( 'utf-8' ) ) . digest ( ) try : compiled = cache [ hashkey ] except KeyError : code = translate_js ( js , '' , use_compilation_plan = use_compilation_plan ) compiled = cache [ hashkey ] = compile ( code , '<EvalJS snippet>' , 'exec' ) exec ( compiled , self . _context )
4405	def parse_line ( line , document = None ) : result = re . match ( line_pattern , line ) if result : _ , lineno , offset , severity , msg = result . groups ( ) lineno = int ( lineno or 1 ) offset = int ( offset or 0 ) errno = 2 if severity == 'error' : errno = 1 diag = { 'source' : 'mypy' , 'range' : { 'start' : { 'line' : lineno - 1 , 'character' : offset } , 'end' : { 'line' : lineno - 1 , 'character' : offset + 1 } } , 'message' : msg , 'severity' : errno } if document : word = document . word_at_position ( diag [ 'range' ] [ 'start' ] ) if word : diag [ 'range' ] [ 'end' ] [ 'character' ] = ( diag [ 'range' ] [ 'start' ] [ 'character' ] + len ( word ) ) return diag
4294	def supported_versions ( django , cms ) : cms_version = None django_version = None try : cms_version = Decimal ( cms ) except ( ValueError , InvalidOperation ) : try : cms_version = CMS_VERSION_MATRIX [ str ( cms ) ] except KeyError : pass try : django_version = Decimal ( django ) except ( ValueError , InvalidOperation ) : try : django_version = DJANGO_VERSION_MATRIX [ str ( django ) ] except KeyError : pass try : if ( cms_version and django_version and not ( LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 0 ] ) <= LooseVersion ( compat . unicode ( django_version ) ) <= LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 1 ] ) ) ) : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) except KeyError : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) return ( compat . unicode ( django_version ) if django_version else django_version , compat . unicode ( cms_version ) if cms_version else cms_version )
1464	def __replace ( config , wildcards , config_file ) : for config_key in config : config_value = config [ config_key ] original_value = config_value if isinstance ( config_value , str ) : for token in wildcards : if wildcards [ token ] : config_value = config_value . replace ( token , wildcards [ token ] ) found = re . findall ( r'\${[A-Z_]+}' , config_value ) if found : raise ValueError ( "%s=%s in file %s contains unsupported or unset wildcard tokens: %s" % ( config_key , original_value , config_file , ", " . join ( found ) ) ) config [ config_key ] = config_value return config
5212	def bdh ( tickers , flds = None , start_date = None , end_date = 'today' , adjust = None , ** kwargs ) -> pd . DataFrame : logger = logs . get_logger ( bdh , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) if isinstance ( adjust , str ) and adjust : if adjust == 'all' : kwargs [ 'CshAdjNormal' ] = True kwargs [ 'CshAdjAbnormal' ] = True kwargs [ 'CapChg' ] = True else : kwargs [ 'CshAdjNormal' ] = 'normal' in adjust or 'dvd' in adjust kwargs [ 'CshAdjAbnormal' ] = 'abn' in adjust or 'dvd' in adjust kwargs [ 'CapChg' ] = 'split' in adjust con , _ = create_connection ( ) elms = assist . proc_elms ( ** kwargs ) ovrds = assist . proc_ovrds ( ** kwargs ) if isinstance ( tickers , str ) : tickers = [ tickers ] if flds is None : flds = [ 'Last_Price' ] if isinstance ( flds , str ) : flds = [ flds ] e_dt = utils . fmt_dt ( end_date , fmt = '%Y%m%d' ) if start_date is None : start_date = pd . Timestamp ( e_dt ) - relativedelta ( months = 3 ) s_dt = utils . fmt_dt ( start_date , fmt = '%Y%m%d' ) logger . info ( f'loading historical data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) logger . debug ( f'\nflds={flds}\nelms={elms}\novrds={ovrds}\nstart_date={s_dt}\nend_date={e_dt}' ) res = con . bdh ( tickers = tickers , flds = flds , elms = elms , ovrds = ovrds , start_date = s_dt , end_date = e_dt ) res . index . name = None if ( len ( flds ) == 1 ) and kwargs . get ( 'keep_one' , False ) : return res . xs ( flds [ 0 ] , axis = 1 , level = 1 ) return res
1611	def ParseNolintSuppressions ( filename , raw_line , linenum , error ) : matched = Search ( r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?' , raw_line ) if matched : if matched . group ( 1 ) : suppressed_line = linenum + 1 else : suppressed_line = linenum category = matched . group ( 2 ) if category in ( None , '(*)' ) : _error_suppressions . setdefault ( None , set ( ) ) . add ( suppressed_line ) else : if category . startswith ( '(' ) and category . endswith ( ')' ) : category = category [ 1 : - 1 ] if category in _ERROR_CATEGORIES : _error_suppressions . setdefault ( category , set ( ) ) . add ( suppressed_line ) elif category not in _LEGACY_ERROR_CATEGORIES : error ( filename , linenum , 'readability/nolint' , 5 , 'Unknown NOLINT error category: %s' % category )
1424	def validate_state_locations ( self ) : names = map ( lambda loc : loc [ "name" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , "Names of state locations must be unique"
13625	def Boolean ( value , true = ( u'yes' , u'1' , u'true' ) , false = ( u'no' , u'0' , u'false' ) , encoding = None ) : value = Text ( value , encoding ) if value is not None : value = value . lower ( ) . strip ( ) if value in true : return True elif value in false : return False return None
1336	def name ( self ) : names = ( criterion . name ( ) for criterion in self . _criteria ) return '__' . join ( sorted ( names ) )
388	def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
10930	def find_LM_updates ( self , grad , do_correct_damping = True , subblock = None ) : if subblock is not None : if ( subblock . sum ( ) == 0 ) or ( subblock . size == 0 ) : CLOG . fatal ( 'Empty subblock in find_LM_updates' ) raise ValueError ( 'Empty sub-block' ) j = self . J [ subblock ] JTJ = np . dot ( j , j . T ) damped_JTJ = self . _calc_damped_jtj ( JTJ , subblock = subblock ) grad = grad [ subblock ] else : damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if self . use_accel : accel_correction = self . calc_accel_correction ( damped_JTJ , delta ) nrm_d0 = np . sqrt ( np . sum ( delta ** 2 ) ) nrm_corr = np . sqrt ( np . sum ( accel_correction ** 2 ) ) CLOG . debug ( '|correction| / |LM step|\t%e' % ( nrm_corr / nrm_d0 ) ) if nrm_corr / nrm_d0 < self . max_accel_correction : delta += accel_correction elif do_correct_damping : CLOG . debug ( 'Untrustworthy step! Increasing damping...' ) self . increase_damping ( ) damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if np . any ( np . isnan ( delta ) ) : CLOG . fatal ( 'Calculated steps have nans!?' ) raise FloatingPointError ( 'Calculated steps have nans!?' ) return delta
2560	def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
11822	def is_compatible ( cls , value ) : if not hasattr ( cls , 'value_type' ) : raise NotImplementedError ( 'You must define a `value_type` attribute or override the ' '`is_compatible()` method on `SettingValueModel` subclasses.' ) return isinstance ( value , cls . value_type )
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , ** kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
63	def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None
9309	def get_sig_string ( req , cano_req , scope ) : amz_date = req . headers [ 'x-amz-date' ] hsh = hashlib . sha256 ( cano_req . encode ( ) ) sig_items = [ 'AWS4-HMAC-SHA256' , amz_date , scope , hsh . hexdigest ( ) ] sig_string = '\n' . join ( sig_items ) return sig_string
10685	def G_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : g = 1 - ( self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 6 + tau ** 9 / 135 + tau ** 15 / 600 ) ) / self . _D_mag else : g = - ( tau ** - 5 / 10 + tau ** - 15 / 315 + tau ** - 25 / 1500 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * g
4225	def load_config ( ) : filename = 'keyringrc.cfg' keyring_cfg = os . path . join ( platform . config_root ( ) , filename ) if not os . path . exists ( keyring_cfg ) : return config = configparser . RawConfigParser ( ) config . read ( keyring_cfg ) _load_keyring_path ( config ) try : if config . has_section ( "backend" ) : keyring_name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . NoOptionError ( 'backend' , 'default-keyring' ) except ( configparser . NoOptionError , ImportError ) : logger = logging . getLogger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring_cfg ) return return load_keyring ( keyring_name )
7157	def assign_prompter ( self , prompter ) : if is_string ( prompter ) : if prompter not in prompters : eprint ( "Error: '{}' is not a core prompter" . format ( prompter ) ) sys . exit ( ) self . prompter = prompters [ prompter ] else : self . prompter = prompter
2716	def __extract_resources_from_droplets ( self , data ) : resources = [ ] if not isinstance ( data , list ) : return data for a_droplet in data : res = { } try : if isinstance ( a_droplet , unicode ) : res = { "resource_id" : a_droplet , "resource_type" : "droplet" } except NameError : pass if isinstance ( a_droplet , str ) or isinstance ( a_droplet , int ) : res = { "resource_id" : str ( a_droplet ) , "resource_type" : "droplet" } elif isinstance ( a_droplet , Droplet ) : res = { "resource_id" : str ( a_droplet . id ) , "resource_type" : "droplet" } if len ( res ) > 0 : resources . append ( res ) return resources
373	def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x
8018	async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
2937	def deserialize_data ( self , workflow , start_node ) : name = start_node . getAttribute ( 'name' ) value = start_node . getAttribute ( 'value' ) return name , value
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
1188	def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
11033	def parse ( self , value : str , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , ) -> typing . Any : if type_ is bool : return type_ ( value . lower ( ) in self . TRUE_STRINGS ) try : if isinstance ( type_ , type ) and issubclass ( type_ , ( list , tuple , set , frozenset ) ) : return type_ ( self . parse ( v . strip ( " " ) , subtype ) for v in value . split ( "," ) if value . strip ( " " ) ) return type_ ( value ) except ValueError as e : raise ConfigError ( * e . args )
11770	def name ( object ) : "Try to find some reasonable name for the object." return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
5619	def execute ( mp , td_resampling = "nearest" , td_matching_method = "gdal" , td_matching_max_zoom = None , td_matching_precision = 8 , td_fallback_to_higher_zoom = False , clip_pixelbuffer = 0 , ** kwargs ) : if "clip" in mp . params [ "input" ] : clip_geom = mp . open ( "clip" ) . read ( ) if not clip_geom : logger . debug ( "no clip data over tile" ) return "empty" else : clip_geom = [ ] with mp . open ( "raster" , matching_method = td_matching_method , matching_max_zoom = td_matching_max_zoom , matching_precision = td_matching_precision , fallback_to_higher_zoom = td_fallback_to_higher_zoom , resampling = td_resampling ) as raster : raster_data = raster . read ( ) if raster . is_empty ( ) or raster_data [ 0 ] . mask . all ( ) : logger . debug ( "raster empty" ) return "empty" if clip_geom : clipped = mp . clip ( np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data ) , clip_geom , clip_buffer = clip_pixelbuffer , inverted = True ) return np . where ( clipped . mask , clipped , mp . params [ "output" ] . nodata ) else : return np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data )
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
10717	def normalize_housecode ( house_code ) : if house_code is None : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) if not isinstance ( house_code , basestring ) : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) if len ( house_code ) != 1 : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) house_code = house_code . upper ( ) if not ( 'A' <= house_code <= 'P' ) : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) return house_code
11823	def exp_schedule ( k = 20 , lam = 0.005 , limit = 100 ) : "One possible schedule function for simulated annealing" return lambda t : if_ ( t < limit , k * math . exp ( - lam * t ) , 0 )
3999	def copy_to_local ( local_path , remote_name , remote_path , demote = True ) : if not container_path_exists ( remote_name , remote_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( remote_path , remote_name ) ) temp_identifier = str ( uuid . uuid1 ( ) ) copy_path_inside_container ( remote_name , remote_path , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) ) vm_path = os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) is_dir = vm_path_is_directory ( vm_path ) sync_local_path_from_vm ( local_path , vm_path , demote = demote , is_dir = is_dir )
12605	def _to_string ( data ) : sdata = data . copy ( ) for k , v in data . items ( ) : if isinstance ( v , datetime ) : sdata [ k ] = timestamp_to_date_str ( v ) elif not isinstance ( v , ( string_types , float , int ) ) : sdata [ k ] = str ( v ) return sdata
8199	def transform_from_local ( xp , yp , cphi , sphi , mx , my ) : x = xp * cphi - yp * sphi + mx y = xp * sphi + yp * cphi + my return ( x , y )
8165	def load_edited_source ( self , source , good_cb = None , bad_cb = None , filename = None ) : with LiveExecution . lock : self . good_cb = good_cb self . bad_cb = bad_cb try : compile ( source + '\n\n' , filename or self . filename , "exec" ) self . edited_source = source except Exception as e : if bad_cb : self . edited_source = None tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) return if filename is not None : self . filename = filename
6993	def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( flareparams , times , mags , errs ) return ( mags - modelmags ) / errs
11499	def get_community_by_id ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
8362	def encode ( self , o ) : if isinstance ( o , basestring ) : if isinstance ( o , str ) : _encoding = self . encoding if ( _encoding is not None and not ( _encoding == 'utf-8' ) ) : o = o . decode ( _encoding ) if self . ensure_ascii : return encode_basestring_ascii ( o ) else : return encode_basestring ( o ) chunks = list ( self . iterencode ( o ) ) return '' . join ( chunks )
8008	def activate ( self ) : obj = self . find_paypal_object ( ) if obj . state == enums . BillingPlanState . CREATED : success = obj . activate ( ) if not success : raise PaypalApiError ( "Failed to activate plan: %r" % ( obj . error ) ) self . get_or_update_from_api_data ( obj , always_sync = True ) return obj
4344	def stat ( self , input_filepath , scale = None , rms = False ) : effect_args = [ 'channels' , '1' , 'stat' ] if scale is not None : if not is_number ( scale ) or scale <= 0 : raise ValueError ( "scale must be a positive number." ) effect_args . extend ( [ '-s' , '{:f}' . format ( scale ) ] ) if rms : effect_args . append ( '-rms' ) _ , _ , stat_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stat_dict = { } lines = stat_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stat_dict [ key . strip ( ':' ) ] = value return stat_dict
8989	def first_produced_mesh ( self ) : for instruction in self . instructions : if instruction . produces_meshes ( ) : return instruction . first_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
7810	def from_der_data ( cls , data ) : logger . debug ( "Decoding DER certificate: {0!r}" . format ( data ) ) if cls . _cert_asn1_type is None : cls . _cert_asn1_type = Certificate ( ) cert = der_decoder . decode ( data , asn1Spec = cls . _cert_asn1_type ) [ 0 ] result = cls ( ) tbs_cert = cert . getComponentByName ( 'tbsCertificate' ) subject = tbs_cert . getComponentByName ( 'subject' ) logger . debug ( "Subject: {0!r}" . format ( subject ) ) result . _decode_subject ( subject ) validity = tbs_cert . getComponentByName ( 'validity' ) result . _decode_validity ( validity ) extensions = tbs_cert . getComponentByName ( 'extensions' ) if extensions : for extension in extensions : logger . debug ( "Extension: {0!r}" . format ( extension ) ) oid = extension . getComponentByName ( 'extnID' ) logger . debug ( "OID: {0!r}" . format ( oid ) ) if oid != SUBJECT_ALT_NAME_OID : continue value = extension . getComponentByName ( 'extnValue' ) logger . debug ( "Value: {0!r}" . format ( value ) ) if isinstance ( value , Any ) : value = der_decoder . decode ( value , asn1Spec = OctetString ( ) ) [ 0 ] alt_names = der_decoder . decode ( value , asn1Spec = GeneralNames ( ) ) [ 0 ] logger . debug ( "SubjectAltName: {0!r}" . format ( alt_names ) ) result . _decode_alt_names ( alt_names ) return result
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
3954	def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
1674	def ParseArguments ( args ) : try : ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 'counting=' , 'filter=' , 'root=' , 'repository=' , 'linelength=' , 'extensions=' , 'exclude=' , 'headers=' , 'quiet' , 'recursive' ] ) except getopt . GetoptError : PrintUsage ( 'Invalid arguments.' ) verbosity = _VerboseLevel ( ) output_format = _OutputFormat ( ) filters = '' counting_style = '' recursive = False for ( opt , val ) in opts : if opt == '--help' : PrintUsage ( None ) elif opt == '--output' : if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : PrintUsage ( 'The only allowed output formats are emacs, vs7, eclipse ' 'and junit.' ) output_format = val elif opt == '--verbose' : verbosity = int ( val ) elif opt == '--filter' : filters = val if not filters : PrintCategories ( ) elif opt == '--counting' : if val not in ( 'total' , 'toplevel' , 'detailed' ) : PrintUsage ( 'Valid counting options are total, toplevel, and detailed' ) counting_style = val elif opt == '--root' : global _root _root = val elif opt == '--repository' : global _repository _repository = val elif opt == '--linelength' : global _line_length try : _line_length = int ( val ) except ValueError : PrintUsage ( 'Line length must be digits.' ) elif opt == '--exclude' : global _excludes if not _excludes : _excludes = set ( ) _excludes . update ( glob . glob ( val ) ) elif opt == '--extensions' : global _valid_extensions try : _valid_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--headers' : global _header_extensions try : _header_extensions = set ( val . split ( ',' ) ) except ValueError : PrintUsage ( 'Extensions must be comma seperated list.' ) elif opt == '--recursive' : recursive = True elif opt == '--quiet' : global _quiet _quiet = True if not filenames : PrintUsage ( 'No files were specified.' ) if recursive : filenames = _ExpandDirectories ( filenames ) if _excludes : filenames = _FilterExcludedFiles ( filenames ) _SetOutputFormat ( output_format ) _SetVerboseLevel ( verbosity ) _SetFilters ( filters ) _SetCountingStyle ( counting_style ) return filenames
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
9894	def _uptime_solaris ( ) : global __boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( AttributeError , OSError ) : return None KSTAT_STRLEN = 31 class anon_union ( ctypes . Union ) : _fields_ = [ ( 'c' , ctypes . c_char * 16 ) , ( 'time' , ctypes . c_int ) ] class kstat_named_t ( ctypes . Structure ) : _fields_ = [ ( 'name' , ctypes . c_char * KSTAT_STRLEN ) , ( 'data_type' , ctypes . c_char ) , ( 'value' , anon_union ) ] kstat . kstat_open . restype = ctypes . c_void_p kstat . kstat_lookup . restype = ctypes . c_void_p kstat . kstat_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ] kstat . kstat_read . restype = ctypes . c_int kstat . kstat_read . argtypes = [ ctypes . c_void_p , ctypes . c_void_p , ctypes . c_void_p ] kstat . kstat_data_lookup . restype = ctypes . POINTER ( kstat_named_t ) kstat . kstat_data_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p ] kc = kstat . kstat_open ( ) if not kc : return None ksp = kstat . kstat_lookup ( kc , 'unix' , 0 , 'system_misc' ) if ksp and kstat . kstat_read ( kc , ksp , None ) != - 1 : data = kstat . kstat_data_lookup ( ksp , 'boot_time' ) if data : __boottime = data . contents . value . time kstat . kstat_close ( kc ) if __boottime is not None : return time . time ( ) - __boottime return None
7398	def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) bib [ - 1 ] [ key ] = value return bib
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
5962	def parse ( self , stride = None ) : if stride is None : stride = self . stride self . corrupted_lineno = [ ] irow = 0 with utilities . openany ( self . real_filename ) as xvg : rows = [ ] ncol = None for lineno , line in enumerate ( xvg ) : line = line . strip ( ) if len ( line ) == 0 : continue if "label" in line and "xaxis" in line : self . xaxis = line . split ( '"' ) [ - 2 ] if "label" in line and "yaxis" in line : self . yaxis = line . split ( '"' ) [ - 2 ] if line . startswith ( "@ legend" ) : if not "legend" in self . metadata : self . metadata [ "legend" ] = [ ] self . metadata [ "legend" ] . append ( line . split ( "legend " ) [ - 1 ] ) if line . startswith ( "@ s" ) and "subtitle" not in line : name = line . split ( "legend " ) [ - 1 ] . replace ( '"' , '' ) . strip ( ) self . names . append ( name ) if line . startswith ( ( '#' , '@' ) ) : continue if line . startswith ( '&' ) : raise NotImplementedError ( '{0!s}: Multi-data not supported, only simple NXY format.' . format ( self . real_filename ) ) try : row = [ float ( el ) for el in line . split ( ) ] except : if self . permissive : self . logger . warn ( "%s: SKIPPING unparsable line %d: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue self . logger . error ( "%s: Cannot parse line %d: %r" , self . real_filename , lineno + 1 , line ) raise if ncol is not None and len ( row ) != ncol : if self . permissive : self . logger . warn ( "%s: SKIPPING line %d with wrong number of columns: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue errmsg = "{0!s}: Wrong number of columns in line {1:d}: {2!r}" . format ( self . real_filename , lineno + 1 , line ) self . logger . error ( errmsg ) raise IOError ( errno . ENODATA , errmsg , self . real_filename ) if irow % stride == 0 : ncol = len ( row ) rows . append ( row ) irow += 1 try : self . __array = numpy . array ( rows ) . transpose ( ) except : self . logger . error ( "%s: Failed reading XVG file, possibly data corrupted. " "Check the last line of the file..." , self . real_filename ) raise finally : del rows
2091	def copy ( self , pk = None , new_name = None , ** kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , ** newresource ) else : if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
1187	def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
1676	def FindHeader ( self , header ) : for section_list in self . include_list : for f in section_list : if f [ 0 ] == header : return f [ 1 ] return - 1
7345	async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) + 1 if self . fill_gaps : if data [ - 1 ] [ 'id' ] != since_id : max_id = data [ - 1 ] [ 'id' ] - 1 responses = with_max_id ( self . request ( ** self . kwargs , max_id = max_id ) ) async for tweets in responses : data . extend ( tweets ) if data [ - 1 ] [ 'id' ] == self . last_id : data = data [ : - 1 ] if not data and not self . force : raise StopAsyncIteration await self . set_param ( data )
3132	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . list_id = response [ 'id' ] else : self . list_id = None return response
8424	def husl_palette ( n_colors = 6 , h = .01 , s = .9 , l = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues *= 359 s *= 99 l *= 99 palette = [ husl . husl_to_rgb ( h_i , s , l ) for h_i in hues ] return palette
1921	def _hook_callback ( self , state , pc , instruction ) : 'Invoke all registered generic hooks' if issymbolic ( pc ) : return for cb in self . _hooks . get ( pc , [ ] ) : cb ( state ) for cb in self . _hooks . get ( None , [ ] ) : cb ( state )
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
7383	def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
8841	def jsonLogic ( tests , data = None ) : if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] values = [ jsonLogic ( val , data ) for val in values ] if operator == 'var' : return get_var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing_some' : return missing_some ( data , * values ) if operator not in operations : raise ValueError ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
13847	def splitext_files_only ( filepath ) : "Custom version of splitext that doesn't perform splitext on directories" return ( ( filepath , '' ) if os . path . isdir ( filepath ) else os . path . splitext ( filepath ) )
4883	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'sap_success_factors' kwargs [ 'model_name' ] = 'SapSuccessFactorsLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'sapsf_user_id' super ( SapSuccessFactorsLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
10237	def _generate_citation_dict ( graph : BELGraph ) -> Mapping [ str , Mapping [ Tuple [ BaseEntity , BaseEntity ] , str ] ] : results = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if CITATION not in data : continue results [ data [ CITATION ] [ CITATION_TYPE ] ] [ u , v ] . add ( data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) return dict ( results )
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
10153	def _extract_operation_from_view ( self , view , args ) : op = { 'responses' : { 'default' : { 'description' : 'UNDOCUMENTED RESPONSE' } } , } renderer = args . get ( 'renderer' , '' ) if "json" in renderer : produces = [ 'application/json' ] elif renderer == 'xml' : produces = [ 'text/xml' ] else : produces = None if produces : op . setdefault ( 'produces' , produces ) consumes = args . get ( 'content_type' ) if consumes is not None : consumes = to_list ( consumes ) consumes = [ x for x in consumes if not callable ( x ) ] op [ 'consumes' ] = consumes is_colander = self . _is_colander_schema ( args ) if is_colander : schema = self . _extract_transform_colander_schema ( args ) parameters = self . parameters . from_schema ( schema ) else : parameters = None if parameters : op [ 'parameters' ] = parameters if isinstance ( view , six . string_types ) : if 'klass' in args : ob = args [ 'klass' ] view_ = getattr ( ob , view . lower ( ) ) docstring = trim ( view_ . __doc__ ) else : docstring = str ( trim ( view . __doc__ ) ) if docstring and self . summary_docstrings : op [ 'summary' ] = docstring if 'response_schemas' in args : op [ 'responses' ] = self . responses . from_schema_mapping ( args [ 'response_schemas' ] ) if 'tags' in args : op [ 'tags' ] = args [ 'tags' ] if 'operation_id' in args : op [ 'operationId' ] = args [ 'operation_id' ] if 'api_security' in args : op [ 'security' ] = args [ 'api_security' ] return op
8763	def update_security_group_rule ( context , id , security_group_rule ) : LOG . info ( "update_security_group_rule for tenant %s" % ( context . tenant_id ) ) new_rule = security_group_rule [ "security_group_rule" ] new_rule = _filter_update_security_group_rule ( new_rule ) with context . session . begin ( ) : rule = db_api . security_group_rule_find ( context , id = id , scope = db_api . ONE ) if not rule : raise sg_ext . SecurityGroupRuleNotFound ( id = id ) db_rule = db_api . security_group_rule_update ( context , rule , ** new_rule ) group_id = db_rule . group_id group = db_api . security_group_find ( context , id = group_id , scope = db_api . ONE ) if not group : raise sg_ext . SecurityGroupNotFound ( id = group_id ) if group : _perform_async_update_rule ( context , group_id , group , rule . id , RULE_UPDATE ) return v . _make_security_group_rule_dict ( db_rule )
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
9435	def _read_a_packet ( file_h , hdrp , layers = 0 ) : raw_packet_header = file_h . read ( 16 ) if not raw_packet_header or len ( raw_packet_header ) != 16 : return None if hdrp [ 0 ] . byteorder == 'big' : packet_header = struct . unpack ( '>IIII' , raw_packet_header ) else : packet_header = struct . unpack ( '<IIII' , raw_packet_header ) ( timestamp , timestamp_us , capture_len , packet_len ) = packet_header raw_packet_data = file_h . read ( capture_len ) if not raw_packet_data or len ( raw_packet_data ) != capture_len : return None if layers > 0 : layers -= 1 raw_packet = linklayer . clookup ( hdrp [ 0 ] . ll_type ) ( raw_packet_data , layers = layers ) else : raw_packet = raw_packet_data packet = pcap_packet ( hdrp , timestamp , timestamp_us , capture_len , packet_len , raw_packet ) return packet
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2729	def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( ** jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : break return kernels
8642	def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6140	def wait_for_simulation_stop ( self , timeout = None ) : start = datetime . now ( ) while self . get_is_sim_running ( ) : sleep ( 0.5 ) if timeout is not None : if ( datetime . now ( ) - start ) . seconds >= timeout : ret = None break else : ret = self . simulation_info ( ) return ret
2788	def resize ( self , size_gigabytes , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "resize" , "size_gigabytes" : size_gigabytes , "region" : region } )
2092	def last_job_data ( self , pk = None , ** kwargs ) : ujt = self . get ( pk , include_debug_header = True , ** kwargs ) if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
9079	def decode_timestamp ( data : str ) -> datetime . datetime : year = 2000 + int ( data [ 0 : 2 ] ) month = int ( data [ 2 : 4 ] ) day = int ( data [ 4 : 6 ] ) hour = int ( data [ 6 : 8 ] ) minute = int ( data [ 8 : 10 ] ) second = int ( data [ 10 : 12 ] ) if minute == 60 : minute = 0 hour += 1 return datetime . datetime ( year = year , month = month , day = day , hour = hour , minute = minute , second = second )
6146	def IIR_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , Ripple_pass , Atten_stop , fs = 1.00 , ftype = 'butter' ) : b , a = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'ba' ) sos = signal . iirdesign ( [ 2 * float ( f_pass1 ) / fs , 2 * float ( f_pass2 ) / fs ] , [ 2 * float ( f_stop1 ) / fs , 2 * float ( f_stop2 ) / fs ] , Ripple_pass , Atten_stop , ftype = ftype , output = 'sos' ) tag = 'IIR ' + ftype + ' order' print ( '%s = %d.' % ( tag , len ( a ) - 1 ) ) return b , a , sos
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
6017	def absolute_signal_to_noise_map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise_map )
12884	def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
1435	def register_metrics ( self , metrics_collector , interval ) : for field , metrics in self . metrics . items ( ) : metrics_collector . register_metric ( field , metrics , interval )
11935	def reuse ( context , block_list , ** kwargs ) : try : block_context = context . render_context [ BLOCK_CONTEXT_KEY ] except KeyError : block_context = BlockContext ( ) if not isinstance ( block_list , ( list , tuple ) ) : block_list = [ block_list ] for block in block_list : block = block_context . get_block ( block ) if block : break else : return '' with context . push ( kwargs ) : return block . render ( context )
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
3125	def verify_signed_jwt_with_certs ( jwt , certs , audience = None ) : jwt = _helpers . _to_bytes ( jwt ) if jwt . count ( b'.' ) != 2 : raise AppIdentityError ( 'Wrong number of segments in token: {0}' . format ( jwt ) ) header , payload , signature = jwt . split ( b'.' ) message_to_sign = header + b'.' + payload signature = _helpers . _urlsafe_b64decode ( signature ) payload_bytes = _helpers . _urlsafe_b64decode ( payload ) try : payload_dict = json . loads ( _helpers . _from_bytes ( payload_bytes ) ) except : raise AppIdentityError ( 'Can\'t parse token: {0}' . format ( payload_bytes ) ) _verify_signature ( message_to_sign , signature , certs . values ( ) ) _verify_time_range ( payload_dict ) _check_audience ( payload_dict , audience ) return payload_dict
273	def to_utc ( df ) : try : df . index = df . index . tz_localize ( 'UTC' ) except TypeError : df . index = df . index . tz_convert ( 'UTC' ) return df
2509	def parse_only_extr_license ( self , extr_lic ) : ident = self . get_extr_license_ident ( extr_lic ) text = self . get_extr_license_text ( extr_lic ) comment = self . get_extr_lics_comment ( extr_lic ) xrefs = self . get_extr_lics_xref ( extr_lic ) name = self . get_extr_lic_name ( extr_lic ) if not ident : return lic = document . ExtractedLicense ( ident ) if text is not None : lic . text = text if name is not None : lic . full_name = name if comment is not None : lic . comment = comment lic . cross_ref = map ( lambda x : six . text_type ( x ) , xrefs ) return lic
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
11286	def flush ( self , line ) : sys . stdout . write ( line ) sys . stdout . flush ( )
13002	def _filter_cluster_data ( self ) : min_temp = self . temperature_range_slider . value [ 0 ] max_temp = self . temperature_range_slider . value [ 1 ] temp_mask = np . logical_and ( self . cluster . catalog [ 'temperature' ] >= min_temp , self . cluster . catalog [ 'temperature' ] <= max_temp ) min_lum = self . luminosity_range_slider . value [ 0 ] max_lum = self . luminosity_range_slider . value [ 1 ] lum_mask = np . logical_and ( self . cluster . catalog [ 'luminosity' ] >= min_lum , self . cluster . catalog [ 'luminosity' ] <= max_lum ) selected_mask = np . isin ( self . cluster . catalog [ 'id' ] , self . selection_ids ) filter_mask = temp_mask & lum_mask & selected_mask self . filtered_data = self . cluster . catalog [ filter_mask ] . data self . source . data = { 'id' : list ( self . filtered_data [ 'id' ] ) , 'temperature' : list ( self . filtered_data [ 'temperature' ] ) , 'luminosity' : list ( self . filtered_data [ 'luminosity' ] ) , 'color' : list ( self . filtered_data [ 'color' ] ) } logging . debug ( "Selected data is now: %s" , self . filtered_data )
6524	def get_issues ( self , sortby = None ) : self . _ensure_cleaned_issues ( ) return self . _sort_issues ( self . _cleaned_issues , sortby )
4295	def less_than_version ( value ) : items = list ( map ( int , str ( value ) . split ( '.' ) ) ) if len ( items ) == 1 : items . append ( 0 ) items [ 1 ] += 1 if value == '1.11' : return '2.0' else : return '.' . join ( map ( str , items ) )
9678	def read_info_string ( self ) : infostring = [ ] self . cnxn . xfer ( [ 0x3F ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] infostring . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( infostring )
9170	def declare_api_routes ( config ) : add_route = config . add_route add_route ( 'get-content' , '/contents/{ident_hash}' ) add_route ( 'get-resource' , '/resources/{hash}' ) add_route ( 'license-request' , '/contents/{uuid}/licensors' ) add_route ( 'roles-request' , '/contents/{uuid}/roles' ) add_route ( 'acl-request' , '/contents/{uuid}/permissions' ) add_route ( 'publications' , '/publications' ) add_route ( 'get-publication' , '/publications/{id}' ) add_route ( 'publication-license-acceptance' , '/publications/{id}/license-acceptances/{uid}' ) add_route ( 'publication-role-acceptance' , '/publications/{id}/role-acceptances/{uid}' ) add_route ( 'collate-content' , '/contents/{ident_hash}/collate-content' ) add_route ( 'bake-content' , '/contents/{ident_hash}/baked' ) add_route ( 'moderation' , '/moderations' ) add_route ( 'moderate' , '/moderations/{id}' ) add_route ( 'moderation-rss' , '/feeds/moderations.rss' ) add_route ( 'api-keys' , '/api-keys' ) add_route ( 'api-key' , '/api-keys/{id}' )
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
6677	def md5sum ( self , filename , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : if exists ( u'/usr/bin/md5sum' ) : res = func ( u'/usr/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/sbin/md5' ) : res = func ( u'/sbin/md5 -r %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/gnu/bin/md5sum' ) : res = func ( u'/opt/local/gnu/bin/md5sum %(filename)s' % locals ( ) ) elif exists ( u'/opt/local/bin/md5sum' ) : res = func ( u'/opt/local/bin/md5sum %(filename)s' % locals ( ) ) else : md5sum = func ( u'which md5sum' ) md5 = func ( u'which md5' ) if exists ( md5sum ) : res = func ( '%(md5sum)s %(filename)s' % locals ( ) ) elif exists ( md5 ) : res = func ( '%(md5)s %(filename)s' % locals ( ) ) else : abort ( 'No MD5 utility was found on this system.' ) if res . succeeded : _md5sum = res else : warn ( res ) _md5sum = None if isinstance ( _md5sum , six . string_types ) : _md5sum = _md5sum . strip ( ) . split ( '\n' ) [ - 1 ] . split ( ) [ 0 ] return _md5sum
1655	def IsDerivedFunction ( clean_lines , linenum ) : for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : match = Match ( r'^([^()]*\w+)\(' , clean_lines . elided [ i ] ) if match : line , _ , closing_paren = CloseExpression ( clean_lines , i , len ( match . group ( 1 ) ) ) return ( closing_paren >= 0 and Search ( r'\boverride\b' , line [ closing_paren : ] ) ) return False
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
5512	def connect ( ) : ftp_class = ftplib . FTP if not SSL else ftplib . FTP_TLS ftp = ftp_class ( timeout = TIMEOUT ) ftp . connect ( HOST , PORT ) ftp . login ( USER , PASSWORD ) if SSL : ftp . prot_p ( ) return ftp
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
3761	def draw_2d ( self , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mols = [ i . rdkitmol_Hs for i in self . Chemicals ] else : mols = [ i . rdkitmol for i in self . Chemicals ] return Draw . MolsToImage ( mols ) except : return 'Rdkit is required for this feature.'
1221	def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor
5564	def input ( self ) : delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) raw_inputs = { get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
9674	def get_days_span ( self , month_index ) : is_first_month = month_index == 0 is_last_month = month_index == self . __len__ ( ) - 1 y = int ( self . start_date . year + ( self . start_date . month + month_index ) / 13 ) m = int ( ( self . start_date . month + month_index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is_first_month and is_last_month : return ( self . end_date - self . start_date ) . days + 1 else : if is_first_month : return total - self . start_date . day + 1 elif is_last_month : return self . end_date . day else : return total
12684	def query ( self , input = '' , params = { } ) : payload = { 'input' : input , 'appid' : self . appid } for key , value in params . items ( ) : if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
1175	def unlock ( self ) : if self . queue : function , argument = self . queue . popleft ( ) function ( argument ) else : self . locked = False
3426	def medium ( self , medium ) : def set_active_bound ( reaction , bound ) : if reaction . reactants : reaction . lower_bound = - bound elif reaction . products : reaction . upper_bound = bound media_rxns = list ( ) exchange_rxns = frozenset ( self . exchanges ) for rxn_id , bound in iteritems ( medium ) : rxn = self . reactions . get_by_id ( rxn_id ) if rxn not in exchange_rxns : LOGGER . warn ( "%s does not seem to be an" " an exchange reaction. Applying bounds anyway." , rxn . id ) media_rxns . append ( rxn ) set_active_bound ( rxn , bound ) media_rxns = frozenset ( media_rxns ) for rxn in ( exchange_rxns - media_rxns ) : set_active_bound ( rxn , 0 )
5195	def main ( ) : app = OutstationApplication ( ) _log . debug ( 'Initialization complete. In command loop.' ) app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
2317	def _run_pc ( self , data , fixedEdges = None , fixedGaps = None , verbose = True ) : if ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'hsic' ] and self . arguments [ '{METHOD_INDEP}' ] == self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the hsic test,' ' setting the hsic.gamma method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'hsic_gamma' ] elif ( self . arguments [ '{CITEST}' ] == self . dir_CI_test [ 'gaussian' ] and self . arguments [ '{METHOD_INDEP}' ] != self . dir_method_indep [ 'corr' ] ) : warnings . warn ( 'Selected method for indep is unfit for the selected test,' ' setting the classic correlation-based method.' ) self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ 'corr' ] id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_pc' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_pc' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_pc' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_pc' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None and fixedEdges is not None : fixedGaps . to_csv ( '/tmp/cdt_pc' + id + '/fixedgaps.csv' , index = False , header = False ) fixedEdges . to_csv ( '/tmp/cdt_pc' + id + '/fixededges.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' pc_result = launch_R_script ( "{}/R_templates/pc.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt_pc' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_pc' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_pc' + id + '' ) return pc_result
9113	def message ( self ) : try : with open ( join ( self . fs_path , u'message' ) ) as message_file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message_file . readlines ( ) ] ) except IOError : return u''
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) return data . get_bytes ( )
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) alias = self . _field [ 1 : ] self . _alias = alias return self
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
2527	def get_annotation_comment ( self , r_term ) : comment_list = list ( self . graph . triples ( ( r_term , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . error = True msg = 'Annotation can have at most one comment.' self . logger . log ( msg ) return else : return six . text_type ( comment_list [ 0 ] [ 2 ] )
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
12561	def create_rois_mask ( roislist , filelist ) : roifiles = [ ] for roi in roislist : try : roi_file = search_list ( roi , filelist ) [ 0 ] except Exception as exc : raise Exception ( 'Error creating list of roi files. \n {}' . format ( str ( exc ) ) ) else : roifiles . append ( roi_file ) return binarise ( roifiles )
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
6761	def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
7118	def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstance ( d1 , dict ) and isinstance ( d2 , dict ) : for k , v in d2 . items ( ) : if isinstance ( v , MissingValue ) and v . name is None : v . name = '.' . join ( _path + ( k , ) ) if isinstance ( v , DeletedValue ) : d1 . pop ( k , None ) elif k not in d1 : if isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( { } , v , _path + ( k , ) ) else : d1 [ k ] = v else : if isinstance ( d1 [ k ] , dict ) and isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( d1 [ k ] , v , _path + ( k , ) ) elif isinstance ( d1 [ k ] , list ) and isinstance ( v , list ) : d1 [ k ] += v elif isinstance ( d1 [ k ] , MissingValue ) : d1 [ k ] = v elif d1 [ k ] is None : d1 [ k ] = v elif type ( d1 [ k ] ) == type ( v ) : d1 [ k ] = v else : raise TypeError ( 'Refusing to replace a %s with a %s' % ( type ( d1 [ k ] ) , type ( v ) ) ) else : raise TypeError ( 'Cannot merge a %s with a %s' % ( type ( d1 ) , type ( d2 ) ) ) return d1
10518	def setmin ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 0 return 1
4067	def update_item ( self , payload , last_modified = None ) : to_send = self . check_items ( [ payload ] ) [ 0 ] if last_modified is None : modified = payload [ "version" ] else : modified = last_modified ident = payload [ "key" ] headers = { "If-Unmodified-Since-Version" : str ( modified ) } headers . update ( self . default_headers ( ) ) req = requests . patch ( url = self . endpoint + "/{t}/{u}/items/{id}" . format ( t = self . library_type , u = self . library_id , id = ident ) , headers = headers , data = json . dumps ( to_send ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
2311	def b_fit_score ( self , x , y ) : x = np . reshape ( minmax_scale ( x ) , ( - 1 , 1 ) ) y = np . reshape ( minmax_scale ( y ) , ( - 1 , 1 ) ) poly = PolynomialFeatures ( degree = self . degree ) poly_x = poly . fit_transform ( x ) poly_x [ : , 1 ] = 0 poly_x [ : , 2 ] = 0 regressor = LinearRegression ( ) regressor . fit ( poly_x , y ) y_predict = regressor . predict ( poly_x ) error = mean_squared_error ( y_predict , y ) return error
1165	def run ( self ) : try : if self . __target : self . __target ( * self . __args , ** self . __kwargs ) finally : del self . __target , self . __args , self . __kwargs
12862	def add_business_days ( self , days_int , holiday_obj = None ) : res = self if days_int >= 0 : count = 0 while count < days_int : res = BusinessDate . add_days ( res , 1 ) if BusinessDate . is_business_day ( res , holiday_obj ) : count += 1 else : count = 0 while count > days_int : res = BusinessDate . add_days ( res , - 1 ) if BusinessDate . is_business_day ( res , holiday_obj ) : count -= 1 return res
6141	def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
2141	def parse_kv ( var_string ) : return_dict = { } if var_string is None : return { } fix_encoding_26 = False if sys . version_info < ( 2 , 7 ) and '\x00' in shlex . split ( u'a' ) [ 0 ] : fix_encoding_26 = True is_unicode = False if fix_encoding_26 or not isinstance ( var_string , str ) : if isinstance ( var_string , six . text_type ) : var_string = var_string . encode ( 'UTF-8' ) is_unicode = True else : var_string = str ( var_string ) for token in shlex . split ( var_string ) : if ( is_unicode ) : token = token . decode ( 'UTF-8' ) if fix_encoding_26 : token = six . text_type ( token ) if '=' in token : ( k , v ) = token . split ( '=' , 1 ) if len ( k ) == 0 or len ( v ) == 0 : raise Exception try : return_dict [ k ] = ast . literal_eval ( v ) except Exception : return_dict [ k ] = v else : raise Exception return return_dict
11807	def encode ( plaintext , code ) : "Encodes text, using a code which is a permutation of the alphabet." from string import maketrans trans = maketrans ( alphabet + alphabet . upper ( ) , code + code . upper ( ) ) return plaintext . translate ( trans )
11997	def verify_signature ( self , data ) : data = self . _remove_magic ( data ) data = urlsafe_nopadding_b64decode ( data ) options = self . _read_header ( data ) data = self . _add_magic ( data ) self . _unsign_data ( data , options )
5036	def _handle_singular ( cls , enterprise_customer , manage_learners_form ) : form_field_value = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . EMAIL_OR_USERNAME ] email = email_or_username__to__email ( form_field_value ) try : validate_email_to_link ( email , form_field_value , ValidationMessages . INVALID_EMAIL_OR_USERNAME , True ) except ValidationError as exc : manage_learners_form . add_error ( ManageLearnersForm . Fields . EMAIL_OR_USERNAME , exc ) else : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) return [ email ]
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return if filestat . st_size < min_size : return return filestat . st_size
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
7521	def write_str ( data , sidx , pnames ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : snparr = io5 [ "snparr" ] bisarr = io5 [ "bisarr" ] bend = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( bend ) : bend = bend . min ( ) else : bend = bisarr . shape [ 1 ] send = np . where ( np . all ( snparr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( send ) : send = send . min ( ) else : send = snparr . shape [ 1 ] out1 = open ( data . outfiles . str , 'w' ) out2 = open ( data . outfiles . ustr , 'w' ) numdict = { 'A' : '0' , 'T' : '1' , 'G' : '2' , 'C' : '3' , 'N' : '-9' , '-' : '-9' } if data . paramsdict [ "max_alleles_consens" ] > 1 : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) else : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out1 . close ( ) out2 . close ( ) LOGGER . debug ( "finished writing str in: %s" , time . time ( ) - start )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
10310	def safe_add_edge ( graph , u , v , key , attr_dict , ** attr ) : if key < 0 : graph . add_edge ( u , v , key = key , attr_dict = attr_dict , ** attr ) else : graph . add_edge ( u , v , attr_dict = attr_dict , ** attr )
11557	def extended_analog ( self , pin , data ) : analog_data = [ pin , data & 0x7f , ( data >> 7 ) & 0x7f , ( data >> 14 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . EXTENDED_ANALOG , analog_data )
3906	async def _on_connect ( self ) : self . _user_list , self . _conv_list = ( await hangups . build_user_conversation_list ( self . _client ) ) self . _conv_list . on_event . add_observer ( self . _on_event ) conv_picker = ConversationPickerWidget ( self . _conv_list , self . on_select_conversation , self . _keys ) self . _tabbed_window = TabbedWindowWidget ( self . _keys ) self . _tabbed_window . set_tab ( conv_picker , switch = True , title = 'Conversations' ) self . _urwid_loop . widget = self . _tabbed_window
8591	def create_snapshot ( self , datacenter_id , volume_id , name = None , description = None ) : data = { 'name' : name , 'description' : description } response = self . _perform_request ( '/datacenters/%s/volumes/%s/create-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION-JSON' , data = urlencode ( data ) ) return response
11591	def _rc_sunion ( self , src , * args ) : args = list_or_args ( src , args ) src_set = self . smembers ( args . pop ( 0 ) ) if src_set is not set ( [ ] ) : for key in args : src_set . update ( self . smembers ( key ) ) return src_set
9725	async def set_qtm_event ( self , event = None ) : cmd = "event%s" % ( "" if event is None else " " + event ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
10752	def download ( self , bands , download_dir = None , metadata = False ) : super ( AWSDownloader , self ) . validate_bands ( bands ) if download_dir is None : download_dir = DOWNLOAD_DIR dest_dir = check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) downloaded = [ ] for band in bands : if band == 'BQA' : filename = '%s_%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) else : filename = '%s_B%s.%s' % ( self . sceneInfo . name , band , self . __remote_file_ext ) band_url = join ( self . base_url , filename ) downloaded . append ( self . fetch ( band_url , dest_dir , filename ) ) if metadata : filename = '%s_MTL.txt' % ( self . sceneInfo . name ) url = join ( self . base_url , filename ) self . fetch ( url , dest_dir , filename ) return downloaded
4717	def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
1762	def push_bytes ( self , data , force = False ) : self . STACK -= len ( data ) self . write_bytes ( self . STACK , data , force ) return self . STACK
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) return self
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
2556	def get ( self , tag = None , ** kwargs ) : if tag is None : tag = dom_tag attrs = [ ( dom_tag . clean_attribute ( attr ) , value ) for attr , value in kwargs . items ( ) ] results = [ ] for child in self . children : if ( isinstance ( tag , basestring ) and type ( child ) . __name__ == tag ) or ( not isinstance ( tag , basestring ) and isinstance ( child , tag ) ) : if all ( child . attributes . get ( attribute ) == value for attribute , value in attrs ) : results . append ( child ) if isinstance ( child , dom_tag ) : results . extend ( child . get ( tag , ** kwargs ) ) return results
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
9735	def get_3d_markers ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPosition , component_info , data , component_position )
3307	def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( ** server_args ) startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
13201	def format_short_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . short_title is None : return None output_text = convert_lsstdoc_tex ( self . short_title , 'html5' , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
1166	def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( "Thread.__init__() not called" ) if not self . __started . is_set ( ) : raise RuntimeError ( "cannot join thread before it is started" ) if self is current_thread ( ) : raise RuntimeError ( "cannot join current thread" ) if __debug__ : if not self . __stopped : self . _note ( "%s.join(): waiting until thread stops" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( "%s.join(): timed out" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( "%s.join(): thread stopped" , self ) finally : self . __block . release ( )
9355	def money ( min = 0 , max = 10 ) : value = random . choice ( range ( min * 100 , max * 100 ) ) return "%1.2f" % ( float ( value ) / 100 )
1306	def GetConsoleTitle ( ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleTitleW ( values , MAX_PATH ) return values . value
9006	def to_svg ( self , converter = None ) : if converter is None : from knittingpattern . convert . InstructionSVGCache import default_svg_cache converter = default_svg_cache ( ) return converter . to_svg ( self )
7699	def verify_roster_push ( self , fix = False ) : self . _verify ( ( None , u"from" , u"to" , u"both" , u"remove" ) , fix )
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) self . _greenlet = gevent . spawn ( self . _loop )
7101	def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) del self . options
11189	def edit ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) try : readme_content = unicode ( readme_content , "utf-8" ) except NameError : pass edited_content = click . edit ( readme_content ) if edited_content is not None : _validate_and_put_readme ( dataset , edited_content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset_uri )
6892	def serial_starfeatures ( lclist , outdir , lc_catalog_pickle , neighbor_radius_arcsec , maxobjects = None , deredden = True , custom_bandpasses = None , lcformat = 'hat-sql' , lcformatdir = None ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] with open ( lc_catalog_pickle , 'rb' ) as infd : kdt_dict = pickle . load ( infd ) kdt = kdt_dict [ 'kdtree' ] objlist = kdt_dict [ 'objects' ] [ 'objectid' ] objlcfl = kdt_dict [ 'objects' ] [ 'lcfname' ] tasks = [ ( x , outdir , kdt , objlist , objlcfl , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) for x in lclist ] for task in tqdm ( tasks ) : result = _starfeatures_worker ( task ) return result
11665	def _build_indices ( X , flann_args ) : "Builds FLANN indices for each bag." logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( ** flann_args ) idx . build_index ( bag ) return indices
8113	def age ( self , id ) : path = self . hash ( id ) if os . path . exists ( path ) : modified = datetime . datetime . fromtimestamp ( os . stat ( path ) [ 8 ] ) age = datetime . datetime . today ( ) - modified return age . days else : return 0
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
764	def createNetwork ( dataSource ) : network = Network ( ) sensor = createRecordSensor ( network , name = _RECORD_SENSOR , dataSource = dataSource ) createSpatialPooler ( network , name = _L1_SPATIAL_POOLER , inputWidth = sensor . encoder . getWidth ( ) ) linkType = "UniformLink" linkParams = "" network . link ( _RECORD_SENSOR , _L1_SPATIAL_POOLER , linkType , linkParams ) l1temporalMemory = createTemporalMemory ( network , _L1_TEMPORAL_MEMORY ) network . link ( _L1_SPATIAL_POOLER , _L1_TEMPORAL_MEMORY , linkType , linkParams ) classifierParams = { 'alpha' : 0.005 , 'steps' : '1' , 'implementation' : 'py' , 'verbosity' : 0 } l1Classifier = network . addRegion ( _L1_CLASSIFIER , "py.SDRClassifierRegion" , json . dumps ( classifierParams ) ) l1Classifier . setParameter ( 'inferenceMode' , True ) l1Classifier . setParameter ( 'learningMode' , True ) network . link ( _L1_TEMPORAL_MEMORY , _L1_CLASSIFIER , linkType , linkParams , srcOutput = "bottomUpOut" , destInput = "bottomUpIn" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = "categoryOut" , destInput = "categoryIn" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = "actValueOut" , destInput = "actValueIn" ) l2inputWidth = l1temporalMemory . getSelf ( ) . getOutputElementCount ( "bottomUpOut" ) createSpatialPooler ( network , name = _L2_SPATIAL_POOLER , inputWidth = l2inputWidth ) network . link ( _L1_TEMPORAL_MEMORY , _L2_SPATIAL_POOLER , linkType , linkParams ) createTemporalMemory ( network , _L2_TEMPORAL_MEMORY ) network . link ( _L2_SPATIAL_POOLER , _L2_TEMPORAL_MEMORY , linkType , linkParams ) l2Classifier = network . addRegion ( _L2_CLASSIFIER , "py.SDRClassifierRegion" , json . dumps ( classifierParams ) ) l2Classifier . setParameter ( 'inferenceMode' , True ) l2Classifier . setParameter ( 'learningMode' , True ) network . link ( _L2_TEMPORAL_MEMORY , _L2_CLASSIFIER , linkType , linkParams , srcOutput = "bottomUpOut" , destInput = "bottomUpIn" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = "categoryOut" , destInput = "categoryIn" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = "actValueOut" , destInput = "actValueIn" ) return network
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
2570	def send_UDP_message ( self , message ) : x = 0 if self . tracking_enabled : try : proc = udp_messenger ( self . domain_name , self . UDP_IP , self . UDP_PORT , self . sock_timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
464	def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( " clear_all ------- %s" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]
5675	def get_shape_distance_between_stops ( self , trip_I , from_stop_seq , to_stop_seq ) : query_template = "SELECT shape_break FROM stop_times WHERE trip_I={trip_I} AND seq={seq} " stop_seqs = [ from_stop_seq , to_stop_seq ] shape_breaks = [ ] for seq in stop_seqs : q = query_template . format ( seq = seq , trip_I = trip_I ) shape_breaks . append ( self . conn . execute ( q ) . fetchone ( ) ) query_template = "SELECT max(d) - min(d) " "FROM shapes JOIN trips ON(trips.shape_id=shapes.shape_id) " "WHERE trip_I={trip_I} AND shapes.seq>={from_stop_seq} AND shapes.seq<={to_stop_seq};" distance_query = query_template . format ( trip_I = trip_I , from_stop_seq = from_stop_seq , to_stop_seq = to_stop_seq ) return self . conn . execute ( distance_query ) . fetchone ( ) [ 0 ]
3939	def get_chunks ( self , new_data_bytes ) : self . _buf += new_data_bytes while True : buf_decoded = _best_effort_decode ( self . _buf ) buf_utf16 = buf_decoded . encode ( 'utf-16' ) [ 2 : ] length_str_match = LEN_REGEX . match ( buf_decoded ) if length_str_match is None : break else : length_str = length_str_match . group ( 1 ) length = int ( length_str ) * 2 length_length = len ( ( length_str + '\n' ) . encode ( 'utf-16' ) [ 2 : ] ) if len ( buf_utf16 ) - length_length < length : break submission = buf_utf16 [ length_length : length_length + length ] yield submission . decode ( 'utf-16' ) drop_length = ( len ( ( length_str + '\n' ) . encode ( ) ) + len ( submission . decode ( 'utf-16' ) . encode ( ) ) ) self . _buf = self . _buf [ drop_length : ]
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
13712	def invalidate_cache ( self ) : if self . _use_cache : self . _cache_version += 1 self . _cache . increment ( 'cached_httpbl_{0}_version' . format ( self . _api_key ) )
12311	def record ( self , localStreamName , pathToFile , ** kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , ** kwargs )
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
9888	def _call_multi_fortran_z_attr ( self , names , data_types , num_elems , entry_nums , attr_nums , var_names , input_type_code , func , data_offset = None ) : idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : max_num = num_elems [ idx ] . max ( ) sub_num_elems = num_elems [ idx ] sub_names = np . array ( names ) [ idx ] sub_var_names = np . array ( var_names ) [ idx ] sub_entry_nums = entry_nums [ idx ] sub_attr_nums = attr_nums [ idx ] status , data = func ( self . fname , sub_attr_nums , sub_entry_nums , len ( sub_attr_nums ) , max_num , len ( self . fname ) ) if ( status == 0 ) . all ( ) : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset self . _process_return_multi_z_attr ( data , sub_names , sub_var_names , sub_num_elems ) else : idx , = np . where ( status != 0 ) raise IOError ( fortran_cdf . statusreporter ( status [ idx ] [ 0 ] ) )
10576	def _create_element_list_ ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
3424	def get_solution ( model , reactions = None , metabolites = None , raise_error = False ) : check_solver_status ( model . solver . status , raise_error = raise_error ) if reactions is None : reactions = model . reactions if metabolites is None : metabolites = model . metabolites rxn_index = list ( ) fluxes = empty ( len ( reactions ) ) reduced = empty ( len ( reactions ) ) var_primals = model . solver . primal_values shadow = empty ( len ( metabolites ) ) if model . solver . is_integer : reduced . fill ( nan ) shadow . fill ( nan ) for ( i , rxn ) in enumerate ( reactions ) : rxn_index . append ( rxn . id ) fluxes [ i ] = var_primals [ rxn . id ] - var_primals [ rxn . reverse_id ] met_index = [ met . id for met in metabolites ] else : var_duals = model . solver . reduced_costs for ( i , rxn ) in enumerate ( reactions ) : forward = rxn . id reverse = rxn . reverse_id rxn_index . append ( forward ) fluxes [ i ] = var_primals [ forward ] - var_primals [ reverse ] reduced [ i ] = var_duals [ forward ] - var_duals [ reverse ] met_index = list ( ) constr_duals = model . solver . shadow_prices for ( i , met ) in enumerate ( metabolites ) : met_index . append ( met . id ) shadow [ i ] = constr_duals [ met . id ] return Solution ( model . solver . objective . value , model . solver . status , Series ( index = rxn_index , data = fluxes , name = "fluxes" ) , Series ( index = rxn_index , data = reduced , name = "reduced_costs" ) , Series ( index = met_index , data = shadow , name = "shadow_prices" ) )
2709	def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( ** meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
7192	def histogram_equalize ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) flattened = data . flatten ( ) if 0 in data : masked = np . ma . masked_values ( data , 0 ) . compressed ( ) image_histogram , bin_edges = np . histogram ( masked , 256 ) else : image_histogram , bin_edges = np . histogram ( flattened , 256 ) bins = ( bin_edges [ : - 1 ] + bin_edges [ 1 : ] ) / 2.0 cdf = image_histogram . cumsum ( ) cdf = cdf / float ( cdf [ - 1 ] ) image_equalized = np . interp ( flattened , bins , cdf ) . reshape ( data . shape ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( image_equalized , ** kwargs ) else : return image_equalized
7238	def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield self . randwindow ( window_shape ) else : for i in xrange ( count ) : yield self . randwindow ( window_shape )
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
10063	def process_minter ( value ) : try : return current_pidstore . minters [ value ] except KeyError : raise click . BadParameter ( 'Unknown minter {0}. Please use one of {1}.' . format ( value , ', ' . join ( current_pidstore . minters . keys ( ) ) ) )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
8890	def serialize ( self ) : opts = self . _meta data = { } for f in opts . concrete_fields : if f . attname in self . morango_fields_not_to_serialize : continue if f . attname in self . _morango_internal_fields_not_to_serialize : continue if f . attname in getattr ( self , '_internal_mptt_fields_not_to_serialize' , '_internal_fields_not_to_serialize' ) : continue if hasattr ( f , 'value_from_object_json_compatible' ) : data [ f . attname ] = f . value_from_object_json_compatible ( self ) else : data [ f . attname ] = f . value_from_object ( self ) return data
276	def customize ( func ) : @ wraps ( func ) def call_w_context ( * args , ** kwargs ) : set_context = kwargs . pop ( 'set_context' , True ) if set_context : with plotting_context ( ) , axes_style ( ) : return func ( * args , ** kwargs ) else : return func ( * args , ** kwargs ) return call_w_context
5604	def _get_warped_array ( input_file = None , indexes = None , dst_bounds = None , dst_shape = None , dst_crs = None , resampling = None , src_nodata = None , dst_nodata = None ) : try : return _rasterio_read ( input_file = input_file , indexes = indexes , dst_bounds = dst_bounds , dst_shape = dst_shape , dst_crs = dst_crs , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata ) except Exception as e : logger . exception ( "error while reading file %s: %s" , input_file , e ) raise
13597	def get_state ( self ) : return [ os . path . join ( dp , f ) for dp , _ , fn in os . walk ( self . dir ) for f in fn ]
3300	def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
8625	def set_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_put_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotSetException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
706	def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : if not self . _createCheckpoints : modelCheckpointGUID = None self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( "Running Model. \nmodelParams: %s, \nmodelID=%s, " % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) cpuTimeStart = time . clock ( ) logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( "%s" , e )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
1872	def RDTSC ( cpu ) : val = cpu . icount cpu . RAX = val & 0xffffffff cpu . RDX = ( val >> 32 ) & 0xffffffff
5758	def get_package_counts ( package_descriptors , targets , repos_data ) : counts = { } for target in targets : counts [ target ] = [ 0 ] * len ( repos_data ) for package_descriptor in package_descriptors . values ( ) : debian_pkg_name = package_descriptor . debian_pkg_name for target in targets : for i , repo_data in enumerate ( repos_data ) : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if version : counts [ target ] [ i ] += 1 return counts
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
5789	def handle_openssl_error ( result , exception_class = None ) : if result > 0 : return if exception_class is None : exception_class = OSError error_num = libcrypto . ERR_get_error ( ) buffer = buffer_from_bytes ( 120 ) libcrypto . ERR_error_string ( error_num , buffer ) error_string = byte_string_from_buffer ( buffer ) raise exception_class ( _try_decode ( error_string ) )
9737	def get_3d_markers_no_label ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabel , component_info , data , component_position )
8358	def shoebot_example ( ** shoebot_kwargs ) : def decorator ( f ) : def run ( ) : from shoebot import ShoebotInstallError print ( " Shoebot - %s:" % f . __name__ . replace ( "_" , " " ) ) try : import shoebot outputfile = "/tmp/shoebot-%s.png" % f . __name__ bot = shoebot . create_bot ( outputfile = outputfile ) f ( bot ) bot . finish ( ) print ( ' [passed] : %s' % outputfile ) print ( '' ) except ShoebotInstallError as e : print ( ' [failed]' , e . args [ 0 ] ) print ( '' ) except Exception : print ( ' [failed] - traceback:' ) for line in traceback . format_exc ( ) . splitlines ( ) : print ( ' %s' % line ) print ( '' ) return run return decorator
13240	def includes ( self , query_date , query_time = None ) : if self . start_date and query_date < self . start_date : return False if self . end_date and query_date > self . end_date : return False if query_date . weekday ( ) not in self . weekdays : return False if not query_time : return True if query_time >= self . period . start and query_time <= self . period . end : return True return False
2442	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True if validations . validate_annotation_comment ( comment ) : doc . annotations [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'AnnotationComment::Comment' ) else : raise CardinalityError ( 'AnnotationComment::Comment' ) else : raise OrderError ( 'AnnotationComment::Comment' )
13389	def manifest ( ) : prune = options . paved . dist . manifest . prune graft = set ( ) if options . paved . dist . manifest . include_sphinx_docroot : docroot = options . get ( 'docroot' , 'docs' ) graft . update ( [ docroot ] ) if options . paved . dist . manifest . exclude_sphinx_builddir : builddir = docroot + '/' + options . get ( "builddir" , ".build" ) prune . update ( [ builddir ] ) with open ( options . paved . cwd / 'MANIFEST.in' , 'w' ) as fo : for item in graft : fo . write ( 'graft %s\n' % item ) for item in options . paved . dist . manifest . include : fo . write ( 'include %s\n' % item ) for item in options . paved . dist . manifest . recursive_include : fo . write ( 'recursive-include %s\n' % item ) for item in prune : fo . write ( 'prune %s\n' % item )
5028	def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
1614	def Match ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
1661	def ExpectingFunctionArgs ( clean_lines , linenum ) : line = clean_lines . elided [ linenum ] return ( Match ( r'^\s*MOCK_(CONST_)?METHOD\d+(_T)?\(' , line ) or ( linenum >= 2 and ( Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\((?:\S+,)?\s*$' , clean_lines . elided [ linenum - 1 ] ) or Match ( r'^\s*MOCK_(?:CONST_)?METHOD\d+(?:_T)?\(\s*$' , clean_lines . elided [ linenum - 2 ] ) or Search ( r'\bstd::m?function\s*\<\s*$' , clean_lines . elided [ linenum - 1 ] ) ) ) )
2131	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the role record.' , header = 'details' ) data , self . endpoint = self . data_endpoint ( kwargs ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** data ) item_dict = response [ 'results' ] [ 0 ] self . configure_display ( item_dict ) return item_dict
12706	def state ( self , state ) : assert self . name == state . name , 'state name "{}" != body name "{}"' . format ( state . name , self . name ) self . position = state . position self . quaternion = state . quaternion self . linear_velocity = state . linear_velocity self . angular_velocity = state . angular_velocity
4990	def get ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : try : course_run_id = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTRV000' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and program_uuid {program_uuid} ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) kwargs [ 'course_id' ] = course_run_id with transaction . atomic ( ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) resource_id = course_run_id or program_uuid if self . eligible_for_direct_audit_enrollment ( request , enterprise_customer , resource_id , course_key ) : try : enterprise_customer_user . enroll ( resource_id , 'audit' , cohort = request . GET . get ( 'cohort' , None ) ) track_enrollment ( 'direct-audit-enrollment' , request . user . id , resource_id , request . get_full_path ( ) ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError ) : pass return redirect ( LMS_COURSEWARE_URL . format ( course_id = resource_id ) ) return self . redirect ( request , * args , ** kwargs )
5978	def mask_blurring_from_mask_and_psf_shape ( mask , psf_shape ) : blurring_mask = np . full ( mask . shape , True ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : for y1 in range ( ( - psf_shape [ 0 ] + 1 ) // 2 , ( psf_shape [ 0 ] + 1 ) // 2 ) : for x1 in range ( ( - psf_shape [ 1 ] + 1 ) // 2 , ( psf_shape [ 1 ] + 1 ) // 2 ) : if 0 <= x + x1 <= mask . shape [ 1 ] - 1 and 0 <= y + y1 <= mask . shape [ 0 ] - 1 : if mask [ y + y1 , x + x1 ] : blurring_mask [ y + y1 , x + x1 ] = False else : raise exc . MaskException ( "setup_blurring_mask extends beyond the sub_grid_size of the masks - pad the " "datas array before masking" ) return blurring_mask
1399	def extract_tmaster ( self , topology ) : tmasterLocation = { "name" : None , "id" : None , "host" : None , "controller_port" : None , "master_port" : None , "stats_port" : None , } if topology . tmaster : tmasterLocation [ "name" ] = topology . tmaster . topology_name tmasterLocation [ "id" ] = topology . tmaster . topology_id tmasterLocation [ "host" ] = topology . tmaster . host tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port tmasterLocation [ "master_port" ] = topology . tmaster . master_port tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port return tmasterLocation
1880	def PSRLQ ( cpu , dest , src ) : count = src . read ( ) count = Operators . ITEBV ( src . size , Operators . UGT ( count , 63 ) , 64 , count ) count = Operators . EXTRACT ( count , 0 , 64 ) if dest . size == 64 : dest . write ( dest . read ( ) >> count ) else : hi = Operators . EXTRACT ( dest . read ( ) , 64 , 64 ) >> count low = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) >> count dest . write ( Operators . CONCAT ( 128 , hi , low ) )
3373	def add_cons_vars_to_problem ( model , what , ** kwargs ) : context = get_context ( model ) model . solver . add ( what , ** kwargs ) if context : context ( partial ( model . solver . remove , what ) )
9844	def __refill_tokenbuffer ( self ) : if len ( self . tokens ) == 0 : self . __tokenize ( self . dxfile . readline ( ) )
13294	def convert_text ( content , from_fmt , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : logger = logging . getLogger ( __name__ ) if extra_args is not None : extra_args = list ( extra_args ) else : extra_args = [ ] if mathjax : extra_args . append ( '--mathjax' ) if smart : extra_args . append ( '--smart' ) if deparagraph : extra_args . append ( '--filter=lsstprojectmeta-deparagraph' ) extra_args . append ( '--wrap=none' ) extra_args = set ( extra_args ) logger . debug ( 'Running pandoc from %s to %s with extra_args %s' , from_fmt , to_fmt , extra_args ) output = pypandoc . convert_text ( content , to_fmt , format = from_fmt , extra_args = extra_args ) return output
13010	def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
13192	def geojson_to_gml ( gj , set_srs = True ) : tag = G ( gj [ 'type' ] ) if set_srs : tag . set ( 'srsName' , 'urn:ogc:def:crs:EPSG::4326' ) if gj [ 'type' ] == 'Point' : tag . append ( G . pos ( _reverse_geojson_coords ( gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'LineString' : tag . append ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in gj [ 'coordinates' ] ) ) ) elif gj [ 'type' ] == 'Polygon' : rings = [ G . LinearRing ( G . posList ( ' ' . join ( _reverse_geojson_coords ( ll ) for ll in ring ) ) ) for ring in gj [ 'coordinates' ] ] tag . append ( G . exterior ( rings . pop ( 0 ) ) ) for ring in rings : tag . append ( G . interior ( ring ) ) elif gj [ 'type' ] in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = gj [ 'type' ] [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' for coord in gj [ 'coordinates' ] : tag . append ( G ( member_tag , geojson_to_gml ( { 'type' : single_type , 'coordinates' : coord } , set_srs = False ) ) ) else : raise NotImplementedError return tag
5286	def post ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
3452	def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }
6695	def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
9386	def parse ( self ) : for infile in self . infile_list : logger . info ( 'Processing : %s' , infile ) status = True file_status = naarad . utils . is_valid_file ( infile ) if not file_status : return False with open ( infile ) as fh : for line in fh : words = line . split ( ) if not words : continue if re . match ( '^\d\d\d\d-\d\d-\d\d$' , line ) : self . ts_date = words [ 0 ] continue prefix_word = words [ 0 ] . strip ( ) if prefix_word == 'top' : self . process_top_line ( words ) self . saw_pid = False elif self . ts_valid_lines : if prefix_word == 'Tasks:' : self . process_tasks_line ( words ) elif prefix_word == 'Cpu(s):' : self . process_cpu_line ( words ) elif prefix_word == 'Mem:' : self . process_mem_line ( words ) elif prefix_word == 'Swap:' : self . process_swap_line ( words ) elif prefix_word == 'PID' : self . saw_pid = True self . process_headers = words else : if self . saw_pid and len ( words ) >= len ( self . process_headers ) : self . process_individual_command ( words ) for out_csv in self . data . keys ( ) : self . csv_files . append ( out_csv ) with open ( out_csv , 'w' ) as fh : fh . write ( '\n' . join ( self . data [ out_csv ] ) ) gc . collect ( ) return status
8315	def parse_links ( self , markup ) : links = [ ] m = re . findall ( self . re [ "link" ] , markup ) for link in m : if link . find ( "{" ) >= 0 : link = re . sub ( "\{{1,3}[0-9]{0,2}\|" , "" , link ) link = link . replace ( "{" , "" ) link = link . replace ( "}" , "" ) link = link . split ( "|" ) link [ 0 ] = link [ 0 ] . split ( "#" ) page = link [ 0 ] [ 0 ] . strip ( ) if not page in links : links . append ( page ) links . sort ( ) return links
1638	def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : raw = clean_lines . lines_without_raw_strings line = raw [ linenum ] if ( IsBlankLine ( line ) and not nesting_state . InNamespaceBody ( ) and not nesting_state . InExternC ( ) ) : elided = clean_lines . elided prev_line = elided [ linenum - 1 ] prevbrace = prev_line . rfind ( '{' ) if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : exception = False if Match ( r' {6}\w' , prev_line ) : search_position = linenum - 2 while ( search_position >= 0 and Match ( r' {6}\w' , elided [ search_position ] ) ) : search_position -= 1 exception = ( search_position >= 0 and elided [ search_position ] [ : 5 ] == ' :' ) else : exception = ( Match ( r' {4}\w[^\(]*\)\s*(const\s*)?(\{\s*$|:)' , prev_line ) or Match ( r' {4}:' , prev_line ) ) if not exception : error ( filename , linenum , 'whitespace/blank_line' , 2 , 'Redundant blank line at the start of a code block ' 'should be deleted.' ) if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] if ( next_line and Match ( r'\s*}' , next_line ) and next_line . find ( '} else ' ) == - 1 ) : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Redundant blank line at the end of a code block ' 'should be deleted.' ) matched = Match ( r'\s*(public|protected|private):' , prev_line ) if matched : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Do not leave a blank line after "%s:"' % matched . group ( 1 ) ) next_line_start = 0 if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) CheckComment ( line , filename , linenum , next_line_start , error ) line = clean_lines . elided [ linenum ] if Search ( r'\w\s+\[' , line ) and not Search ( r'(?:delete|return)\s+\[' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Extra space before [' ) if ( Search ( r'for *\(.*[^:]:[^: ]' , line ) or Search ( r'for *\(.*[^: ]:[^:]' , line ) ) : error ( filename , linenum , 'whitespace/forcolon' , 2 , 'Missing space around colon in range-based for loop' )
7873	def get_all_payload ( self , specialize = False ) : if self . _payload is None : self . decode_payload ( specialize ) elif specialize : for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ i ] = payload return list ( self . _payload )
9152	def _convert_vpathlist ( input_obj ) : vpl = pgmagick . VPathList ( ) for obj in input_obj : obj = pgmagick . PathMovetoAbs ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) vpl . append ( obj ) return vpl
9261	def filter_wo_labels ( self , all_issues ) : issues_wo_labels = [ ] if not self . options . add_issues_wo_labels : for issue in all_issues : if not issue [ 'labels' ] : issues_wo_labels . append ( issue ) return issues_wo_labels
4732	def generate_rt_pic ( process_data , para_meter , scale ) : pic_path = para_meter [ 'filename' ] + '.png' plt . figure ( figsize = ( 5.6 * scale , 3.2 * scale ) ) for key in process_data . keys ( ) : plt . plot ( process_data [ key ] [ : , 0 ] , process_data [ key ] [ : , 1 ] , label = str ( key ) ) plt . title ( para_meter [ 'title' ] ) plt . xlabel ( para_meter [ 'x_axis_name' ] ) plt . ylabel ( para_meter [ 'y_axis_name' ] ) plt . legend ( loc = 'upper left' ) plt . savefig ( pic_path ) return pic_path
9967	def shareable_parameters ( cells ) : result = [ ] for c in cells . values ( ) : params = c . formula . parameters for i in range ( min ( len ( result ) , len ( params ) ) ) : if params [ i ] != result [ i ] : return None for i in range ( len ( result ) , len ( params ) ) : result . append ( params [ i ] ) return result
12255	def sync ( self ) : for key in mimicdb . backend . smembers ( tpl . bucket % self . name ) : mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) mimicdb . backend . delete ( tpl . bucket % self . name ) mimicdb . backend . sadd ( tpl . connection , self . name ) for key in self . list ( force = True ) : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) )
4182	def window_blackman_nuttall ( N ) : r a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
7027	def objectlist_radeclbox ( radeclbox , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns} from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "BOX('ICRS',{ra_center:.5f},{decl_center:.5f}," "{ra_width:.5f},{decl_height:.5f}))=1" "{extra_filter_str}" ) ra_min , ra_max , decl_min , decl_max = radeclbox ra_center = ( ra_max + ra_min ) / 2.0 decl_center = ( decl_max + decl_min ) / 2.0 ra_width = ra_max - ra_min decl_height = decl_max - decl_min if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( columns = ', ' . join ( columns ) , extra_filter_str = extra_filter_str , ra_center = ra_center , decl_center = decl_center , ra_width = ra_width , decl_height = decl_height ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
11333	def banner ( * lines , ** kwargs ) : sep = kwargs . get ( "sep" , "*" ) count = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) out ( sep * count ) if lines : out ( sep ) for line in lines : out ( "{} {}" . format ( sep , line ) ) out ( sep ) out ( sep * count )
10604	def create_entity ( self , name , gl_structure , description = None ) : new_entity = Entity ( name , gl_structure , description = description ) self . entities . append ( new_entity ) return new_entity
2275	def _win32_is_hardlinked ( fpath1 , fpath2 ) : def get_read_handle ( fpath ) : if os . path . isdir ( fpath ) : dwFlagsAndAttributes = jwfs . api . FILE_FLAG_BACKUP_SEMANTICS else : dwFlagsAndAttributes = 0 hFile = jwfs . api . CreateFile ( fpath , jwfs . api . GENERIC_READ , jwfs . api . FILE_SHARE_READ , None , jwfs . api . OPEN_EXISTING , dwFlagsAndAttributes , None ) return hFile def get_unique_id ( hFile ) : info = jwfs . api . BY_HANDLE_FILE_INFORMATION ( ) res = jwfs . api . GetFileInformationByHandle ( hFile , info ) jwfs . handle_nonzero_success ( res ) unique_id = ( info . volume_serial_number , info . file_index_high , info . file_index_low ) return unique_id hFile1 = get_read_handle ( fpath1 ) hFile2 = get_read_handle ( fpath2 ) try : are_equal = ( get_unique_id ( hFile1 ) == get_unique_id ( hFile2 ) ) except Exception : raise finally : jwfs . api . CloseHandle ( hFile1 ) jwfs . api . CloseHandle ( hFile2 ) return are_equal
7076	def periodrec_worker ( task ) : pfpkl , simbasedir , period_tolerance = task try : return periodicvar_recovery ( pfpkl , simbasedir , period_tolerance = period_tolerance ) except Exception as e : LOGEXCEPTION ( 'periodic var recovery failed for %s' % repr ( task ) ) return None
13690	def remove_peer ( self , peer ) : if type ( peer ) == list : for x in peer : check_url ( x ) for i in self . PEERS : if x in i : self . PEERS . remove ( i ) elif type ( peer ) == str : check_url ( peer ) for i in self . PEERS : if peer == i : self . PEERS . remove ( i ) else : raise ValueError ( 'peer paramater did not pass url validation' )
4569	def dump ( data , file = sys . stdout , use_yaml = None , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , ** kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , ** kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
9325	def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + '08c09de5' + '30009ce8' + '010055e1' + '00005401' + '0100a013' + '0000a003' + '0c008c08' + '3000bde8' + '1eff2fe1' ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + '1eff2fe1' ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + '000053e1' + '0000a003' + '00108205' + '0100a013' + '1eff2fe1' ) self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + '010090e8' + '1eff2fe1' ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
10851	def local_max_featuring ( im , radius = 2.5 , noise_size = 1. , bkg_size = None , minmass = 1. , trim_edge = False ) : if radius <= 0 : raise ValueError ( '`radius` must be > 0' ) filtered = nd . gaussian_filter ( im , noise_size , mode = 'mirror' ) if bkg_size is None : bkg_size = 2 * radius filtered -= nd . gaussian_filter ( filtered , bkg_size , mode = 'mirror' ) footprint = generate_sphere ( radius ) e = nd . maximum_filter ( filtered , footprint = footprint ) mass_im = nd . convolve ( filtered , footprint , mode = 'mirror' ) good_im = ( e == filtered ) * ( mass_im > minmass ) pos = np . transpose ( np . nonzero ( good_im ) ) if trim_edge : good = np . all ( pos > 0 , axis = 1 ) & np . all ( pos + 1 < im . shape , axis = 1 ) pos = pos [ good , : ] . copy ( ) masses = mass_im [ pos [ : , 0 ] , pos [ : , 1 ] , pos [ : , 2 ] ] . copy ( ) return pos , masses
129	def find_closest_point_index ( self , x , y , return_distance = False ) : ia . do_assert ( len ( self . exterior ) > 0 ) distances = [ ] for x2 , y2 in self . exterior : d = ( x2 - x ) ** 2 + ( y2 - y ) ** 2 distances . append ( d ) distances = np . sqrt ( distances ) closest_idx = np . argmin ( distances ) if return_distance : return closest_idx , distances [ closest_idx ] return closest_idx
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , ** kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
1111	def _qformat ( self , aline , bline , atags , btags ) : r common = min ( _count_leading ( aline , "\t" ) , _count_leading ( bline , "\t" ) ) common = min ( common , _count_leading ( atags [ : common ] , " " ) ) common = min ( common , _count_leading ( btags [ : common ] , " " ) ) atags = atags [ common : ] . rstrip ( ) btags = btags [ common : ] . rstrip ( ) yield "- " + aline if atags : yield "? %s%s\n" % ( "\t" * common , atags ) yield "+ " + bline if btags : yield "? %s%s\n" % ( "\t" * common , btags )
11269	def substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . substitute ( data )
6470	def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
7187	def remove_function_signature_type_comment ( body ) : for node in body . children : if node . type == token . INDENT : prefix = node . prefix . lstrip ( ) if prefix . startswith ( '# type: ' ) : node . prefix = '\n' . join ( prefix . split ( '\n' ) [ 1 : ] ) break
11083	def help ( self , msg , args ) : output = [ ] if len ( args ) == 0 : commands = sorted ( self . _bot . dispatcher . commands . items ( ) , key = itemgetter ( 0 ) ) commands = filter ( lambda x : x [ 1 ] . is_subcmd is False , commands ) if self . _should_filter_help_commands ( msg . user ) : commands = filter ( lambda x : x [ 1 ] . admin_only is False , commands ) for name , cmd in commands : output . append ( self . _get_short_help_for_command ( name ) ) else : name = '!' + args [ 0 ] output = [ self . _get_help_for_command ( name ) ] return '\n' . join ( output )
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : r if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
9957	def setup_ipython ( self ) : if self . is_ipysetup : return from ipykernel . kernelapp import IPKernelApp self . shell = IPKernelApp . instance ( ) . shell if not self . shell and is_ipython ( ) : self . shell = get_ipython ( ) if self . shell : shell_class = type ( self . shell ) shell_class . default_showtraceback = shell_class . showtraceback shell_class . showtraceback = custom_showtraceback self . is_ipysetup = True else : raise RuntimeError ( "IPython shell not found." )
5560	def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : Log . debug ( "Try again error" ) else : Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
1102	def restore ( delta , which ) : r try : tag = { 1 : "- " , 2 : "+ " } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( " " , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]
1519	def read_and_parse_roles ( cl_args ) : roles = dict ( ) with open ( get_inventory_file ( cl_args ) , 'r' ) as stream : try : roles = yaml . load ( stream ) except yaml . YAMLError as exc : Log . error ( "Error parsing inventory file: %s" % exc ) sys . exit ( - 1 ) if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : Log . error ( "Zookeeper servers node defined!" ) sys . exit ( - 1 ) if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : Log . error ( "Heron cluster nodes defined!" ) sys . exit ( - 1 ) roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) return roles
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
10494	def clickMouseButtonRight ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _postQueuedEvents ( )
1922	def resolve ( self , symbol ) : with open ( self . binary_path , 'rb' ) as f : elffile = ELFFile ( f ) for section in elffile . iter_sections ( ) : if not isinstance ( section , SymbolTableSection ) : continue symbols = section . get_symbol_by_name ( symbol ) if not symbols : continue return symbols [ 0 ] . entry [ 'st_value' ] raise ValueError ( f"The {self.binary_path} ELFfile does not contain symbol {symbol}" )
11786	def sanitize ( self , example ) : "Return a copy of example, with non-input attributes replaced by None." return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , ** kwargs ) : completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
8527	def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child )
3150	def get ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
1949	def write_back_register ( self , reg , val ) : if self . write_backs_disabled : return if issymbolic ( val ) : logger . warning ( "Skipping Symbolic write-back" ) return if reg in self . flag_registers : self . _emu . reg_write ( self . _to_unicorn_id ( 'EFLAGS' ) , self . _cpu . read_register ( 'EFLAGS' ) ) return self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val )
13808	def run ( self ) : config = config_creator ( ) debug = config . debug branch_thread_sleep = config . branch_thread_sleep while 1 : url = self . branch_queue . get ( ) if debug : print ( 'branch thread-{} start' . format ( url ) ) branch_spider = self . branch_spider ( url ) sleep ( random . randrange ( * branch_thread_sleep ) ) branch_spider . request_page ( ) if debug : print ( 'branch thread-{} end' . format ( url ) ) self . branch_queue . task_done ( )
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
9374	def download_file ( url ) : try : ( local_file , headers ) = urllib . urlretrieve ( url ) except : sys . exit ( "ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting..." ) return local_file
6966	def initialize ( self , executor , secret ) : self . executor = executor self . secret = secret
9606	def check_unused_args ( self , used_args , args , kwargs ) : for k , v in kwargs . items ( ) : if k in used_args : self . _used_kwargs . update ( { k : v } ) else : self . _unused_kwargs . update ( { k : v } )
8340	def toEncoding ( self , s , encoding = None ) : if isinstance ( s , unicode ) : if encoding : s = s . encode ( encoding ) elif isinstance ( s , str ) : if encoding : s = s . encode ( encoding ) else : s = unicode ( s ) else : if encoding : s = self . toEncoding ( str ( s ) , encoding ) else : s = unicode ( s ) return s
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
8187	def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
7766	def _close_stream ( self ) : self . stream . close ( ) if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
7403	def above ( self , ref ) : if not self . _valid_ordering_reference ( ref ) : raise ValueError ( "%r can only be moved above instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) if self . order == ref . order : return if self . order > ref . order : o = ref . order else : o = self . get_ordering_queryset ( ) . filter ( order__lt = ref . order ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) or 0 self . to ( o )
12339	def compress ( images , delete_tif = False , folder = None ) : if type ( images ) == str : return [ compress_blocking ( images , delete_tif , folder ) ] filenames = copy ( images ) return Parallel ( n_jobs = _pools ) ( delayed ( compress_blocking ) ( image = image , delete_tif = delete_tif , folder = folder ) for image in filenames )
9646	def get_attributes ( var ) : is_valid = partial ( is_valid_in_template , var ) return list ( filter ( is_valid , dir ( var ) ) )
1503	def template_uploader_yaml ( cl_args , masters ) : single_master = masters [ 0 ] uploader_config_template = "%s/standalone/templates/uploader.template.yaml" % cl_args [ "config_path" ] uploader_config_actual = "%s/standalone/uploader.yaml" % cl_args [ "config_path" ] template_file ( uploader_config_template , uploader_config_actual , { "<http_uploader_uri>" : "http://%s:9000/api/v1/file/upload" % single_master } )
6190	def set_sim_params ( self , nparams , attr_params ) : for name , value in nparams . items ( ) : val = value [ 0 ] if value [ 0 ] is not None else 'none' self . h5file . create_array ( '/parameters' , name , obj = val , title = value [ 1 ] ) for name , value in attr_params . items ( ) : self . h5file . set_node_attr ( '/parameters' , name , value )
9961	def show_tree ( model = None ) : if model is None : model = mx . cur_model ( ) view = get_modeltree ( model ) app = QApplication . instance ( ) if not app : raise RuntimeError ( "QApplication does not exist." ) view . show ( ) app . exec_ ( )
8241	def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) return colors
848	def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData
3890	def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
279	def plot_monthly_returns_heatmap ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { "size" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , ** kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( "Monthly returns (%)" ) return ax
5967	def solvate ( struct = 'top/protein.pdb' , top = 'top/system.top' , distance = 0.9 , boxtype = 'dodecahedron' , concentration = 0 , cation = 'NA' , anion = 'CL' , water = 'tip4p' , solvent_name = 'SOL' , with_membrane = False , ndx = 'main.ndx' , mainselection = '"Protein"' , dirname = 'solvate' , ** kwargs ) : sol = solvate_sol ( struct = struct , top = top , distance = distance , boxtype = boxtype , water = water , solvent_name = solvent_name , with_membrane = with_membrane , dirname = dirname , ** kwargs ) ion = solvate_ion ( struct = sol [ 'struct' ] , top = top , concentration = concentration , cation = cation , anion = anion , solvent_name = solvent_name , ndx = ndx , mainselection = mainselection , dirname = dirname , ** kwargs ) return ion
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
13634	def _parseAccept ( headers ) : def sort ( value ) : return float ( value [ 1 ] . get ( 'q' , 1 ) ) return OrderedDict ( sorted ( _splitHeaders ( headers ) , key = sort , reverse = True ) )
11903	def static ( ** kwargs ) : def wrap ( fn ) : fn . func_globals [ 'static' ] = fn fn . __dict__ . update ( kwargs ) return fn return wrap
672	def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( "actualValues" ) probabilities = classifierRegion . getOutputData ( "probabilities" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ "predictedValue" ] = predictedValue results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence return results
10479	def _getActions ( self ) : actions = _a11y . AXUIElement . _getActions ( self ) return [ action [ 2 : ] for action in actions ]
13646	def hump_to_underscore ( name ) : new_name = '' pos = 0 for c in name : if pos == 0 : new_name = c . lower ( ) elif 65 <= ord ( c ) <= 90 : new_name += '_' + c . lower ( ) pass else : new_name += c pos += 1 pass return new_name
505	def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
5499	def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
4031	def _decrypt ( self , value , encrypted_value ) : if sys . platform == 'win32' : return self . _decrypt_windows_chrome ( value , encrypted_value ) if value or ( encrypted_value [ : 3 ] != b'v10' ) : return value encrypted_value = encrypted_value [ 3 : ] encrypted_value_half_len = int ( len ( encrypted_value ) / 2 ) cipher = pyaes . Decrypter ( pyaes . AESModeOfOperationCBC ( self . key , self . iv ) ) decrypted = cipher . feed ( encrypted_value [ : encrypted_value_half_len ] ) decrypted += cipher . feed ( encrypted_value [ encrypted_value_half_len : ] ) decrypted += cipher . feed ( ) return decrypted . decode ( "utf-8" )
6488	def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
5888	def __crawl ( self , crawl_candidate ) : def crawler_wrapper ( parser , parsers_lst , crawl_candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl_candidate ) except ( UnicodeDecodeError , ValueError ) as ex : if parsers_lst : parser = parsers_lst . pop ( 0 ) return crawler_wrapper ( parser , parsers_lst , crawl_candidate ) else : raise ex return article parsers = list ( self . config . available_parsers ) parsers . remove ( self . config . parser_class ) return crawler_wrapper ( self . config . parser_class , parsers , crawl_candidate )
9877	def _ratio_metric ( v1 , v2 , ** _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
10943	def calc_J ( self ) : r0 = self . state . residuals . copy ( ) . ravel ( ) dl = np . zeros ( self . param_vals . size ) p0 = self . param_vals . copy ( ) J = [ ] for a in range ( self . param_vals . size ) : dl *= 0 dl [ a ] += self . dl self . update_function ( p0 + dl ) r1 = self . state . residuals . copy ( ) . ravel ( ) J . append ( ( r1 - r0 ) / self . dl ) self . update_function ( p0 ) return np . array ( J )
2025	def SGT ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
7364	async def set_tz ( self ) : settings = await self . api . account . settings . get ( ) tz = settings . time_zone . tzinfo_name os . environ [ 'TZ' ] = tz time . tzset ( )
7978	def _post_auth ( self ) : ClientStream . _post_auth ( self ) if not self . initiator : self . unset_iq_get_handler ( "query" , "jabber:iq:auth" ) self . unset_iq_set_handler ( "query" , "jabber:iq:auth" )
13406	def submitEntry ( self ) : mcclogs , physlogs = self . selectedLogs ( ) success = True if mcclogs != [ ] : if not self . acceptedUser ( "MCC" ) : QMessageBox ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return fileName = self . xmlSetup ( "MCC" , mcclogs ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "MCC" ) success = self . sendToLogbook ( fileName , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : fileName = self . xmlSetup ( "Physics" , physlogs [ i ] ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "Physics" ) success_phys = self . sendToLogbook ( fileName , "Physics" , physlogs [ i ] ) success = success and success_phys self . done ( success )
11299	def register ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s is not a subclass of BaseProvider' % provider_class . __name__ ) if provider_class in self . _registered_providers : raise AlreadyRegistered ( '%s is already registered' % provider_class . __name__ ) if issubclass ( provider_class , DjangoProvider ) : signals . post_save . connect ( self . invalidate_stored_oembeds , sender = provider_class . _meta . model ) self . _registered_providers . append ( provider_class ) self . invalidate_providers ( )
11542	def read ( self , pin ) : if type ( pin ) is list : return [ self . read ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : value = self . _read ( pin_id ) lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'read' ] ) is tuple : read_range = lpin [ 'read' ] value = self . _linear_interpolation ( value , * read_range ) return value else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
11368	def punctuate_authorname ( an ) : name = an . strip ( ) parts = [ x for x in name . split ( ',' ) if x != '' ] ret_str = '' for idx , part in enumerate ( parts ) : subparts = part . strip ( ) . split ( ' ' ) for sidx , substr in enumerate ( subparts ) : ret_str += substr if len ( substr ) == 1 : ret_str += '.' if sidx < ( len ( subparts ) - 1 ) : ret_str += ' ' if idx < ( len ( parts ) - 1 ) : ret_str += ', ' return ret_str . strip ( )
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) for row in reader : task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
10703	def get_usage ( _id ) : url = USAGE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False try : return arequest . json ( ) except ValueError : _LOGGER . info ( "Failed to get usage. Not supported by unit?" ) return None
13225	async def process_ltd_doc_products ( session , product_urls , github_api_token , mongo_collection = None ) : tasks = [ asyncio . ensure_future ( process_ltd_doc ( session , github_api_token , product_url , mongo_collection = mongo_collection ) ) for product_url in product_urls ] await asyncio . gather ( * tasks )
4076	def cfg_to_args ( config ) : kwargs = { } opts_to_args = { 'metadata' : [ ( 'name' , 'name' ) , ( 'author' , 'author' ) , ( 'author-email' , 'author_email' ) , ( 'maintainer' , 'maintainer' ) , ( 'maintainer-email' , 'maintainer_email' ) , ( 'home-page' , 'url' ) , ( 'summary' , 'description' ) , ( 'description' , 'long_description' ) , ( 'download-url' , 'download_url' ) , ( 'classifier' , 'classifiers' ) , ( 'platform' , 'platforms' ) , ( 'license' , 'license' ) , ( 'keywords' , 'keywords' ) , ] , 'files' : [ ( 'packages_root' , 'package_dir' ) , ( 'packages' , 'packages' ) , ( 'modules' , 'py_modules' ) , ( 'scripts' , 'scripts' ) , ( 'package_data' , 'package_data' ) , ( 'data_files' , 'data_files' ) , ] , } opts_to_args [ 'metadata' ] . append ( ( 'requires-dist' , 'install_requires' ) ) if IS_PY2K and not which ( '3to2' ) : kwargs [ 'setup_requires' ] = [ '3to2' ] kwargs [ 'zip_safe' ] = False for section in opts_to_args : for option , argname in opts_to_args [ section ] : value = get_cfg_value ( config , section , option ) if value : kwargs [ argname ] = value if 'long_description' not in kwargs : kwargs [ 'long_description' ] = read_description_file ( config ) if 'package_dir' in kwargs : kwargs [ 'package_dir' ] = { '' : kwargs [ 'package_dir' ] } if 'keywords' in kwargs : kwargs [ 'keywords' ] = split_elements ( kwargs [ 'keywords' ] ) if 'package_data' in kwargs : kwargs [ 'package_data' ] = get_package_data ( kwargs [ 'package_data' ] ) if 'data_files' in kwargs : kwargs [ 'data_files' ] = get_data_files ( kwargs [ 'data_files' ] ) kwargs [ 'version' ] = get_version ( ) if not IS_PY2K : kwargs [ 'test_suite' ] = 'test' return kwargs
3579	def clear_cached_data ( self ) : for device in self . list_devices ( ) : if device . is_connected : continue adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) adapter . RemoveDevice ( device . _device . object_path )
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
7484	def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . realpath ( data . paramsdict [ "project_dir" ] ) , data . name + "_edits" ) if not os . path . exists ( data . dirs . edits ) : os . makedirs ( data . dirs . edits ) subsamples = choose_samples ( samples , force ) if int ( data . paramsdict [ "filter_adapters" ] ) == 3 : if not data . _hackersonly [ "p3_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p3_adapters_extra" ] . append ( poly ) if not data . _hackersonly [ "p5_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p5_adapters_extra" ] . append ( poly ) else : data . _hackersonly [ "p5_adapters_extra" ] = [ ] data . _hackersonly [ "p3_adapters_extra" ] = [ ] subsamples = concat_reads ( data , subsamples , ipyclient ) lbview = ipyclient . load_balanced_view ( targets = ipyclient . ids [ : : 2 ] ) run_cutadapt ( data , subsamples , lbview ) assembly_cleanup ( data )
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
11378	def get_publication_date ( self , xml_doc ) : start_date = get_value_in_tag ( xml_doc , "prism:coverDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , "prism:coverDisplayDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , 'oa:openAccessEffective' ) if start_date : start_date = datetime . datetime . strptime ( start_date , "%Y-%m-%dT%H:%M:%SZ" ) return start_date . strftime ( "%Y-%m-%d" ) import dateutil . parser start_date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start_date ) try : date = dateutil . parser . parse ( start_date ) except ValueError : return '' if len ( start_date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start_date ) is 8 : start_date = time . strftime ( '%Y-%m-%d' , time . strptime ( start_date , '%Y%m%d' ) ) elif len ( start_date ) is 6 : start_date = time . strftime ( '%Y-%m' , time . strptime ( start_date , '%Y%m' ) ) return start_date
5908	def make_ndx_captured ( ** kwargs ) : kwargs [ 'stdout' ] = False user_input = kwargs . pop ( 'input' , [ ] ) user_input = [ cmd for cmd in user_input if cmd != 'q' ] kwargs [ 'input' ] = user_input + [ '' , 'q' ] return gromacs . make_ndx ( ** kwargs )
1458	def _get_deps_list ( abs_path_to_pex ) : pex = zipfile . ZipFile ( abs_path_to_pex , mode = 'r' ) deps = list ( set ( [ re . match ( egg_regex , i ) . group ( 1 ) for i in pex . namelist ( ) if re . match ( egg_regex , i ) is not None ] ) ) return deps
5217	def check_hours ( tickers , tz_exch , tz_loc = DEFAULT_TZ ) -> pd . DataFrame : cols = [ 'Trading_Day_Start_Time_EOD' , 'Trading_Day_End_Time_EOD' ] con , _ = create_connection ( ) hours = con . ref ( tickers = tickers , flds = cols ) cur_dt = pd . Timestamp ( 'today' ) . strftime ( '%Y-%m-%d ' ) hours . loc [ : , 'local' ] = hours . value . astype ( str ) . str [ : - 3 ] hours . loc [ : , 'exch' ] = pd . DatetimeIndex ( cur_dt + hours . value . astype ( str ) ) . tz_localize ( tz_loc ) . tz_convert ( tz_exch ) . strftime ( '%H:%M' ) hours = pd . concat ( [ hours . set_index ( [ 'ticker' , 'field' ] ) . exch . unstack ( ) . loc [ : , cols ] , hours . set_index ( [ 'ticker' , 'field' ] ) . local . unstack ( ) . loc [ : , cols ] , ] , axis = 1 ) hours . columns = [ 'Exch_Start' , 'Exch_End' , 'Local_Start' , 'Local_End' ] return hours
2655	def isdir ( self , path ) : result = True try : self . sftp_client . lstat ( path ) except FileNotFoundError : result = False return result
4794	def contains_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . _err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( missing ) ) ) return self
5372	def _file_exists_in_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get ( bucket = bucket_name , object = object_name , projection = 'noAcl' ) try : request . execute ( ) return True except errors . HttpError : return False
13452	def imgmin ( self ) : if not hasattr ( self , '_imgmin' ) : imgmin = _np . min ( self . images [ 0 ] ) for img in self . images : imin = _np . min ( img ) if imin > imgmin : imgmin = imin self . _imgmin = imgmin return _np . min ( self . image )
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
7942	def _got_addresses ( self , name , port , addrs ) : with self . lock : if not addrs : if self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _dst_addrs = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve address record for {0!r}" . format ( name ) ) self . _dst_addrs = [ ( family , ( addr , port ) ) for ( family , addr ) in addrs ] self . _set_state ( "connect" )
2276	def _win32_dir ( path , star = '' ) : from ubelt import util_cmd import re wrapper = 'cmd /S /C "{}"' command = 'dir /-C "{}"{}' . format ( path , star ) wrapped = wrapper . format ( command ) info = util_cmd . cmd ( wrapped , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) lines = info [ 'out' ] . split ( '\n' ) [ 5 : - 3 ] splitter = re . compile ( '( +)' ) for line in lines : parts = splitter . split ( line ) date , sep , time , sep , ampm , sep , type_or_size , sep = parts [ : 8 ] name = '' . join ( parts [ 8 : ] ) if name == '.' or name == '..' : continue if type_or_size in [ '<JUNCTION>' , '<SYMLINKD>' , '<SYMLINK>' ] : pos = name . find ( ':' ) bpos = name [ : pos ] . rfind ( '[' ) name = name [ : bpos - 1 ] pointed = name [ bpos + 1 : - 1 ] yield type_or_size , name , pointed else : yield type_or_size , name , None
6174	def reset_lock ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) self . celery_self . backend . client . delete ( redis_key )
13179	def get_or_default ( func = None , default = None ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : try : return func ( * args , ** kwargs ) except ObjectDoesNotExist : if callable ( default ) : return default ( ) else : return default return wrapper if func is None : return decorator else : return decorator ( func )
2313	def predict_proba ( self , a , b , ** kwargs ) : a = scale ( a ) . reshape ( ( - 1 , 1 ) ) b = scale ( b ) . reshape ( ( - 1 , 1 ) ) return self . anm_score ( b , a ) - self . anm_score ( a , b )
836	def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , ** kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
291	def plot_rolling_volatility ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_vol_ts = timeseries . rolling_volatility ( returns , rolling_window ) rolling_vol_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_vol_ts_factor = timeseries . rolling_volatility ( factor_returns , rolling_window ) rolling_vol_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling volatility (6-month)' ) ax . axhline ( rolling_vol_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_ylabel ( 'Volatility' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Volatility' , 'Benchmark volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
1996	def cmp_regs ( cpu , should_print = False ) : differing = False gdb_regs = gdb . getCanonicalRegisters ( ) for name in sorted ( gdb_regs ) : vg = gdb_regs [ name ] if name . endswith ( 'psr' ) : name = 'apsr' v = cpu . read_register ( name . upper ( ) ) if should_print : logger . debug ( f'{name} gdb:{vg:x} mcore:{v:x}' ) if vg != v : if should_print : logger . warning ( '^^ unequal' ) differing = True if differing : logger . debug ( qemu . correspond ( None ) ) return differing
7648	def deprecated ( version , version_removed ) : def __wrapper ( func , * args , ** kwargs ) : code = six . get_function_code ( func ) warnings . warn_explicit ( "{:s}.{:s}\n\tDeprecated as of JAMS version {:s}." "\n\tIt will be removed in JAMS version {:s}." . format ( func . __module__ , func . __name__ , version , version_removed ) , category = DeprecationWarning , filename = code . co_filename , lineno = code . co_firstlineno + 1 ) return func ( * args , ** kwargs ) return decorator ( __wrapper )
10857	def _i2p ( self , ind , coord ) : return '-' . join ( [ self . param_prefix , str ( ind ) , coord ] )
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
3711	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeLiquids ] return Amgat ( zs , Vms ) elif method == COSTALD_MIXTURE : return COSTALD_mixture ( zs , T , self . Tcs , self . Vcs , self . omegas ) elif method == COSTALD_MIXTURE_FIT : return COSTALD_mixture ( zs , T , self . Tcs , self . COSTALD_Vchars , self . COSTALD_omegas ) elif method == RACKETT : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Zcs ) elif method == RACKETT_PARAMETERS : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Z_RAs ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) rho = Laliberte_density ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return rho_to_Vm ( rho , MW ) else : raise Exception ( 'Method not valid' )
5351	def __autorefresh_studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend_section ] or 'enrich_areas_of_code:git' not in self . conf [ self . backend_section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc_index = self . conf [ 'enrich_areas_of_code:git' ] . get ( 'out_index' , GitEnrich . GIT_AOC_ENRICHED ) if not aoc_index : aoc_index = GitEnrich . GIT_AOC_ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc_index ) es = Elasticsearch ( [ self . conf [ 'es_enrichment' ] [ 'url' ] ] , timeout = 100 , verify_certs = self . _get_enrich_backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc_index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) aoc_backend = GitEnrich ( self . db_sh , None , cfg [ 'projects' ] [ 'projects_file' ] , self . db_user , self . db_password , self . db_host ) aoc_backend . mapping = None aoc_backend . roles = [ 'author' ] elastic_enrich = get_elastic ( self . conf [ 'es_enrichment' ] [ 'url' ] , aoc_index , clean = False , backend = aoc_backend ) aoc_backend . set_elastic ( elastic_enrich ) self . __autorefresh ( aoc_backend , studies = True )
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
7102	def fit_transform ( self , raw_documents , y = None ) : documents = super ( CountVectorizer , self ) . fit_transform ( raw_documents = raw_documents , y = y ) self . n = len ( raw_documents ) m = ( self . transform ( raw_documents ) > 0 ) . astype ( int ) m = m . sum ( axis = 0 ) . A1 self . period_ = m self . df_ = m / self . n return documents
10150	def generate ( self , title = None , version = None , base_path = None , info = None , swagger = None , ** kwargs ) : title = title or self . api_title version = version or self . api_version info = info or self . swagger . get ( 'info' , { } ) swagger = swagger or self . swagger base_path = base_path or self . base_path swagger = swagger . copy ( ) info . update ( title = title , version = version ) swagger . update ( swagger = '2.0' , info = info , basePath = base_path ) paths , tags = self . _build_paths ( ) if tags : swagger . setdefault ( 'tags' , [ ] ) tag_names = { t [ 'name' ] for t in swagger [ 'tags' ] } for tag in tags : if tag [ 'name' ] not in tag_names : swagger [ 'tags' ] . append ( tag ) if paths : swagger . setdefault ( 'paths' , { } ) merge_dicts ( swagger [ 'paths' ] , paths ) definitions = self . definitions . definition_registry if definitions : swagger . setdefault ( 'definitions' , { } ) merge_dicts ( swagger [ 'definitions' ] , definitions ) parameters = self . parameters . parameter_registry if parameters : swagger . setdefault ( 'parameters' , { } ) merge_dicts ( swagger [ 'parameters' ] , parameters ) responses = self . responses . response_registry if responses : swagger . setdefault ( 'responses' , { } ) merge_dicts ( swagger [ 'responses' ] , responses ) return swagger
5123	def show_active ( self , ** kwargs ) : g = self . g for v in g . nodes ( ) : self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) is_active = False my_iter = g . in_edges ( v ) if g . is_directed ( ) else g . out_edges ( v ) for e in my_iter : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : is_active = True break if is_active : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_active' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) for e in g . edges ( ) : ei = g . edge_index [ e ] if self . edge2queue [ ei ] . _active : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , ** kwargs ) self . _update_all_colors ( )
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
4272	def create_output_directories ( self ) : check_or_create_dir ( self . dst_path ) if self . medias : check_or_create_dir ( join ( self . dst_path , self . settings [ 'thumb_dir' ] ) ) if self . medias and self . settings [ 'keep_orig' ] : self . orig_path = join ( self . dst_path , self . settings [ 'orig_dir' ] ) check_or_create_dir ( self . orig_path )
4826	def enroll_user_in_course ( self , username , course_id , mode , cohort = None ) : return self . client . enrollment . post ( { 'user' : username , 'course_details' : { 'course_id' : course_id } , 'mode' : mode , 'cohort' : cohort , } )
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
12089	def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) swhlab . plot . save ( abf , tag = '05-gain' )
2536	def set_chksum ( self , doc , chk_sum ) : if chk_sum : doc . ext_document_references [ - 1 ] . check_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) else : raise SPDXValueError ( 'ExternalDocumentRef::Checksum' )
8900	def max_parameter_substitution ( ) : if os . path . isfile ( SQLITE_VARIABLE_FILE_CACHE ) : return conn = sqlite3 . connect ( ':memory:' ) low = 1 high = 1000 conn . execute ( 'CREATE TABLE T1 (id C1)' ) while low < high - 1 : guess = ( low + high ) // 2 try : statement = 'select * from T1 where id in (%s)' % ',' . join ( [ '?' for _ in range ( guess ) ] ) values = [ i for i in range ( guess ) ] conn . execute ( statement , values ) except sqlite3 . DatabaseError as ex : if 'too many SQL variables' in str ( ex ) : high = guess else : raise else : low = guess conn . close ( ) with open ( SQLITE_VARIABLE_FILE_CACHE , 'w' ) as file : file . write ( str ( low ) )
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
8309	def pangocairo_create_context ( cr ) : try : return PangoCairo . create_context ( cr ) except KeyError as e : if e . args == ( 'could not find foreign type Context' , ) : raise ShoebotInstallError ( "Error creating PangoCairo missing dependency: python-gi-cairo" ) else : raise
13777	def AddEnumDescriptor ( self , enum_desc ) : if not isinstance ( enum_desc , descriptor . EnumDescriptor ) : raise TypeError ( 'Expected instance of descriptor.EnumDescriptor.' ) self . _enum_descriptors [ enum_desc . full_name ] = enum_desc self . AddFileDescriptor ( enum_desc . file )
3213	def get_access_details ( self , key = None ) : if key in self . _CACHE_STATS : return self . _CACHE_STATS [ 'access_stats' ] [ key ] else : return self . _CACHE_STATS [ 'access_stats' ]
9673	def resolve ( self , context , quiet = True ) : try : obj = context for level in self . levels : if isinstance ( obj , dict ) : obj = obj [ level ] elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : obj = obj [ int ( level ) ] else : if callable ( getattr ( obj , level ) ) : try : obj = getattr ( obj , level ) ( ) except KeyError : obj = getattr ( obj , level ) else : display = 'get_%s_display' % level obj = getattr ( obj , display ) ( ) if hasattr ( obj , display ) else getattr ( obj , level ) if not obj : break return obj except Exception as e : if quiet : return '' else : raise e
3869	async def update_read_timestamp ( self , read_timestamp = None ) : if read_timestamp is None : read_timestamp = ( self . events [ - 1 ] . timestamp if self . events else datetime . datetime . now ( datetime . timezone . utc ) ) if read_timestamp > self . latest_read_timestamp : logger . info ( 'Setting {} latest_read_timestamp from {} to {}' . format ( self . id_ , self . latest_read_timestamp , read_timestamp ) ) state = self . _conversation . self_conversation_state state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( read_timestamp ) ) try : await self . _client . update_watermark ( hangouts_pb2 . UpdateWatermarkRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , last_read_timestamp = parsers . to_timestamp ( read_timestamp ) , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to update read timestamp: {}' . format ( e ) ) raise
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
6232	def calc_scene_bbox ( self ) : bbox_min , bbox_max = None , None for node in self . root_nodes : bbox_min , bbox_max = node . calc_global_bbox ( matrix44 . create_identity ( ) , bbox_min , bbox_max ) self . bbox_min = bbox_min self . bbox_max = bbox_max self . diagonal_size = vector3 . length ( self . bbox_max - self . bbox_min )
5895	def formfield ( self , ** kwargs ) : defaults = { 'form_class' : RichTextFormField , 'config' : self . config , } defaults . update ( kwargs ) return super ( RichTextField , self ) . formfield ( ** defaults )
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
7901	def configure_room ( self , form ) : if form . type == "cancel" : return None elif form . type != "submit" : raise ValueError ( "A 'submit' form required to configure a room" ) iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "set" ) query = iq . new_query ( MUC_OWNER_NS , "query" ) form . as_xml ( query ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_success , self . process_configuration_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
12097	def delete ( self , force = False , ** kwargs ) : if force : return super ( BaseActivatableModel , self ) . delete ( ** kwargs ) else : setattr ( self , self . ACTIVATABLE_FIELD_NAME , False ) return self . save ( update_fields = [ self . ACTIVATABLE_FIELD_NAME ] )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
12000	def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
12929	def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
4697	def cat_file ( path ) : cmd = [ "cat" , path ] status , stdout , _ = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise RuntimeError ( "cij.nvme.env: cat %s failed" % path ) return stdout . strip ( )
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
12846	def generate ( request ) : models . DataItem . create ( content = '' . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 20 ) ) ) return muffin . HTTPFound ( '/' )
1084	def time ( self ) : "Return the time part, with tzinfo None." return time ( self . hour , self . minute , self . second , self . microsecond )
7401	def down ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__gt = self . order ) )
6389	def _sb_short_word ( self , term , r1_prefixes = None ) : if self . _sb_r1 ( term , r1_prefixes ) == len ( term ) and self . _sb_ends_in_short_syllable ( term ) : return True return False
10715	def sendCommands ( comPort , commands ) : mutex . acquire ( ) try : try : port = serial . Serial ( port = comPort ) header = '11010101 10101010' footer = '10101101' for command in _translateCommands ( commands ) : _sendBinaryData ( port , header + command + footer ) except serial . SerialException : print ( 'Unable to open serial port %s' % comPort ) print ( '' ) raise finally : mutex . release ( )
1822	def SETPO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , 1 , 0 ) )
10836	def filter ( self , ** kwargs ) : if not len ( self ) : self . all ( ) new_list = filter ( lambda item : [ True for arg in kwargs if item [ arg ] == kwargs [ arg ] ] != [ ] , self ) return Profiles ( self . api , new_list )
4467	def serialize ( transform , ** kwargs ) : params = transform . get_params ( ) return jsonpickle . encode ( params , ** kwargs )
10261	def _collapse_variants_by_function ( graph : BELGraph , func : str ) -> None : for parent_node , variant_node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS_VARIANT and parent_node . function == func : collapse_pair ( graph , from_node = variant_node , to_node = parent_node )
4043	def _build_query ( self , query_string , no_params = False ) : try : query = quote ( query_string . format ( u = self . library_id , t = self . library_type ) ) except KeyError as err : raise ze . ParamNotPassed ( "There's a request parameter missing: %s" % err ) if no_params is False : if not self . url_params : self . add_parameters ( ) query = "%s?%s" % ( query , self . url_params ) return query
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : products_data = json . load ( products_file ) uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : products_data . append ( item ) json . dump ( products_data , temp_file ) return True return None
12398	def gen_methods ( self , * args , ** kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , ** kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) yield from self . gen_generic ( )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
6926	def cursor ( self , handle , dictcursor = False ) : if handle in self . cursors : return self . cursors [ handle ] else : if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return self . cursors [ handle ]
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
12624	def recursive_dir_match ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in dirs if re . match ( regex , f ) ] ) return outlist
1514	def wait_for_job_to_start ( single_master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise RuntimeError ( ) except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
5183	def node ( self , name ) : nodes = self . nodes ( path = name ) return next ( node for node in nodes )
383	def pt2map ( list_points = None , size = ( 100 , 100 ) , val = 1 ) : if list_points is None : raise Exception ( "list_points : list of 2 int" ) i_m = np . zeros ( size ) if len ( list_points ) == 0 : return i_m for xx in list_points : for x in xx : i_m [ int ( np . round ( x [ 0 ] ) ) ] [ int ( np . round ( x [ 1 ] ) ) ] = val return i_m
660	def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : totalSamples = len ( vectors ) windowSize = numSamples numWindows = 0 pctStable = 0 for wStart in range ( 0 , totalSamples - windowSize + 1 ) : data = vectors [ wStart : wStart + windowSize ] outputSums = data . sum ( axis = 0 ) stableOutputs = ( outputSums == windowSize ) . sum ( ) samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) print samplePctStable pctStable += samplePctStable numWindows += 1 return float ( pctStable ) / numWindows
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6210	def print_children ( data_file , group = '/' ) : base = data_file . get_node ( group ) print ( 'Groups in:\n %s\n' % base ) for node in base . _f_walk_groups ( ) : if node is not base : print ( ' %s' % node ) print ( '\nLeaf-nodes in %s:' % group ) for node in base . _v_leaves . itervalues ( ) : info = node . shape if len ( info ) == 0 : info = node . read ( ) print ( '\t%s, %s' % ( node . name , info ) ) if len ( node . title ) > 0 : print ( '\t %s' % node . title )
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
2742	def get_object ( cls , api_token , ssh_key_id ) : ssh_key = cls ( token = api_token , id = ssh_key_id ) ssh_key . load ( ) return ssh_key
1563	def add_task_hook ( self , task_hook ) : if not isinstance ( task_hook , ITaskHook ) : raise TypeError ( "In add_task_hook(): attempt to add non ITaskHook instance, given: %s" % str ( type ( task_hook ) ) ) self . task_hooks . append ( task_hook )
8489	def get_watcher ( self ) : if not self . watching : raise StopIteration ( ) return self . client . eternal_watch ( self . prefix , recursive = True )
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
3723	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
5244	def missing_info ( ** kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
9774	def resources ( ctx , gpu ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . job . resources ( user , project_name , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9339	def loadtxt2 ( fname , dtype = None , delimiter = ' ' , newline = '\n' , comment_character = '#' , skiplines = 0 ) : dtypert = [ None , None , None ] def preparedtype ( dtype ) : dtypert [ 0 ] = dtype flatten = flatten_dtype ( dtype ) dtypert [ 1 ] = flatten dtypert [ 2 ] = numpy . dtype ( [ ( 'a' , ( numpy . int8 , flatten . itemsize ) ) ] ) buf = numpy . empty ( ( ) , dtype = dtypert [ 1 ] ) converters = [ _default_conv [ flatten [ name ] . char ] for name in flatten . names ] return buf , converters , flatten . names def fileiter ( fh ) : converters = [ ] buf = None if dtype is not None : buf , converters , names = preparedtype ( dtype ) yield None for lineno , line in enumerate ( fh ) : if lineno < skiplines : continue if line [ 0 ] in comment_character : if buf is None and line [ 1 ] == '?' : ddtype = pickle . loads ( base64 . b64decode ( line [ 2 : ] ) ) buf , converters , names = preparedtype ( ddtype ) yield None continue for word , c , name in zip ( line . split ( ) , converters , names ) : buf [ name ] = c ( word ) buf2 = buf . copy ( ) . view ( dtype = dtypert [ 2 ] ) yield buf2 if isinstance ( fname , basestring ) : fh = file ( fh , 'r' ) cleanup = lambda : fh . close ( ) else : fh = iter ( fname ) cleanup = lambda : None try : i = fileiter ( fh ) i . next ( ) return numpy . fromiter ( i , dtype = dtypert [ 2 ] ) . view ( dtype = dtypert [ 0 ] ) finally : cleanup ( )
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
5486	def jsonify_status_code ( status_code , * args , ** kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , ** kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , ** kw ) response . status_code = status_code return response
12806	def attach ( self , observer ) : if not observer in self . _observers : self . _observers . append ( observer ) return self
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
4613	def blocks ( self , start = None , stop = None ) : self . block_interval = self . get_block_interval ( ) if not start : start = self . get_current_block_num ( ) while True : if stop : head_block = stop else : head_block = self . get_current_block_num ( ) for blocknum in range ( start , head_block + 1 ) : block = self . wait_for_and_get_block ( blocknum ) block . update ( { "block_num" : blocknum } ) yield block start = head_block + 1 if stop and start > stop : return time . sleep ( self . block_interval )
12722	def velocities ( self , velocities ) : _set_params ( self . ode_obj , 'Vel' , velocities , self . ADOF + self . LDOF )
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
838	def infer ( self , inputPattern , computeScores = True , overCategories = True , partitionId = None ) : sparsity = 0.0 if self . minSparsity > 0.0 : sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / len ( inputPattern ) ) if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : winner = None inferenceResult = numpy . zeros ( 1 ) dist = numpy . ones ( 1 ) categoryDist = numpy . ones ( 1 ) else : maxCategoryIdx = max ( self . _categoryList ) inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) dist = self . _getDistances ( inputPattern , partitionId = partitionId ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) if self . exact : exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] if len ( exactMatches ) > 0 : for i in exactMatches [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ i ] ] += 1.0 else : sorted = dist . argsort ( ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 if inferenceResult . any ( ) : winner = inferenceResult . argmax ( ) inferenceResult /= inferenceResult . sum ( ) else : winner = None categoryDist = min_score_per_category ( maxCategoryIdx , self . _categoryList , dist ) categoryDist . clip ( 0 , 1.0 , categoryDist ) if self . verbosity >= 1 : print "%s infer:" % ( g_debugPrefix ) print " active inputs:" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) print " winner category:" , winner print " pct neighbors of each category:" , inferenceResult print " dist of each prototype:" , dist print " dist of each category:" , categoryDist result = ( winner , inferenceResult , dist , categoryDist ) return result
4618	def parse_time ( block_time ) : return datetime . strptime ( block_time , timeFormat ) . replace ( tzinfo = timezone . utc )
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
3883	def from_entity ( entity , self_user_id ) : user_id = UserID ( chat_id = entity . id . chat_id , gaia_id = entity . id . gaia_id ) return User ( user_id , entity . properties . display_name , entity . properties . first_name , entity . properties . photo_url , entity . properties . email , ( self_user_id == user_id ) or ( self_user_id is None ) )
5505	def save ( url , * args , ** kwargs ) : device = heimdallDevice ( kwargs . get ( 'device' , None ) ) kwargs [ 'width' ] = kwargs . get ( 'width' , None ) or device . width kwargs [ 'height' ] = kwargs . get ( 'height' , None ) or device . height kwargs [ 'user_agent' ] = kwargs . get ( 'user_agent' , None ) or device . user_agent screenshot_image = screenshot ( url , ** kwargs ) if kwargs . get ( 'optimize' ) : image = Image . open ( screenshot_image . path ) image . save ( screenshot_image . path , optimize = True ) return screenshot_image
3742	def StielPolar ( Tc = None , Pc = None , omega = None , CASRN = '' , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if Tc and Pc and omega : methods . append ( 'DEFINITION' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc * 0.6 ) if not P : factor = None else : Pr = P / Pc factor = log10 ( Pr ) + 1.70 * omega + 1.552 elif Method == 'NONE' : factor = None else : raise Exception ( 'Failure in in function' ) return factor
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
12924	def safe_repr ( obj ) : try : obj_repr = repr ( obj ) except : obj_repr = "({0}<{1}> repr error)" . format ( type ( obj ) , id ( obj ) ) return obj_repr
5944	def isstream ( obj ) : signature_methods = ( "close" , ) alternative_methods = ( ( "read" , "readline" , "readlines" ) , ( "write" , "writeline" , "writelines" ) ) for m in signature_methods : if not hasmethod ( obj , m ) : return False alternative_results = [ numpy . all ( [ hasmethod ( obj , m ) for m in alternatives ] ) for alternatives in alternative_methods ] return numpy . any ( alternative_results )
5960	def _tcorrel ( self , nstep = 100 , ** kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , ** kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
13682	def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
6501	def course_discovery_search ( search_term = None , size = 20 , from_ = 0 , field_dictionary = None ) : use_search_fields = [ "org" ] ( search_fields , _ , exclude_dictionary ) = SearchFilterGenerator . generate_field_filters ( ) use_field_dictionary = { } use_field_dictionary . update ( { field : search_fields [ field ] for field in search_fields if field in use_search_fields } ) if field_dictionary : use_field_dictionary . update ( field_dictionary ) if not getattr ( settings , "SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING" , False ) : use_field_dictionary [ "enrollment_start" ] = DateRange ( None , datetime . utcnow ( ) ) searcher = SearchEngine . get_search_engine ( getattr ( settings , "COURSEWARE_INDEX_NAME" , "courseware_index" ) ) if not searcher : raise NoSearchEngineError ( "No search engine specified in settings.SEARCH_ENGINE" ) results = searcher . search ( query_string = search_term , doc_type = "course_info" , size = size , from_ = from_ , field_dictionary = use_field_dictionary , filter_dictionary = { "enrollment_end" : DateRange ( datetime . utcnow ( ) , None ) } , exclude_dictionary = exclude_dictionary , facet_terms = course_discovery_facets ( ) , ) return results
262	def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax
1235	def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
12899	def get_play_status ( self ) : status = yield from self . handle_int ( self . API . get ( 'status' ) ) return self . PLAY_STATES . get ( status )
2073	def convert_input ( X ) : if not isinstance ( X , pd . DataFrame ) : if isinstance ( X , list ) : X = pd . DataFrame ( X ) elif isinstance ( X , ( np . generic , np . ndarray ) ) : X = pd . DataFrame ( X ) elif isinstance ( X , csr_matrix ) : X = pd . DataFrame ( X . todense ( ) ) elif isinstance ( X , pd . Series ) : X = pd . DataFrame ( X ) else : raise ValueError ( 'Unexpected input type: %s' % ( str ( type ( X ) ) ) ) X = X . apply ( lambda x : pd . to_numeric ( x , errors = 'ignore' ) ) return X
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
5858	def __generate_search_template ( self , dataset_ids ) : data = { "dataset_ids" : dataset_ids } failure_message = "Failed to generate a search template from columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/search_templates/builders/from-dataset-ids' , data , failure_message = failure_message ) ) [ 'data' ]
11335	def prompt ( question , choices = None ) : if not re . match ( "\s$" , question ) : question = "{}: " . format ( question ) while True : if sys . version_info [ 0 ] > 2 : answer = input ( question ) else : answer = raw_input ( question ) if not choices or answer in choices : break return answer
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
8444	def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running "temple update" and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )
7133	def add_icon ( icon_data , dest ) : with open ( os . path . join ( dest , "icon.png" ) , "wb" ) as f : f . write ( icon_data )
6088	def scaled_noise_map_from_hyper_galaxies_and_contribution_maps ( contribution_maps , hyper_galaxies , noise_map ) : scaled_noise_maps = list ( map ( lambda hyper_galaxy , contribution_map : hyper_galaxy . hyper_noise_from_contributions ( noise_map = noise_map , contributions = contribution_map ) , hyper_galaxies , contribution_maps ) ) return noise_map + sum ( scaled_noise_maps )
10288	def enrich_composites ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
13429	def get_sites ( self ) : url = "/2/sites" data = self . _get_resource ( url ) sites = [ ] for entry in data [ 'sites' ] : sites . append ( self . site_from_json ( entry ) ) return sites
11091	def select_file ( self , filters = all_true , recursive = True ) : for p in self . select ( filters , recursive ) : if p . is_file ( ) : yield p
4158	def ma ( X , Q , M ) : if Q <= 0 or Q >= M : raise ValueError ( 'Q(MA) must be in ]0,lag[' ) a , rho , _c = yulewalker . aryule ( X , M , 'biased' ) a = np . insert ( a , 0 , 1 ) ma_params , _p , _c = yulewalker . aryule ( a , Q , 'biased' ) return ma_params , rho
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
6334	def dist ( self , src , tar ) : if src == tar : return 0.0 return self . dist_abs ( src , tar ) / ( len ( src ) + len ( tar ) )
5270	def _get_word_start_index ( self , idx ) : i = 0 for _idx in self . word_starts [ 1 : ] : if idx < _idx : return i else : i += 1 return i
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
13135	def autocomplete ( query , country = None , hurricanes = False , cities = True , timeout = 5 ) : data = { } data [ 'query' ] = quote ( query ) data [ 'country' ] = country or '' data [ 'hurricanes' ] = 1 if hurricanes else 0 data [ 'cities' ] = 1 if cities else 0 data [ 'format' ] = 'JSON' r = requests . get ( AUTOCOMPLETE_URL . format ( ** data ) , timeout = timeout ) results = json . loads ( r . content ) [ 'RESULTS' ] return results
2132	def _compare_node_lists ( old , new ) : to_expand = [ ] to_delete = [ ] to_recurse = [ ] old_records = { } new_records = { } for tree_node in old : old_records . setdefault ( tree_node . unified_job_template , [ ] ) old_records [ tree_node . unified_job_template ] . append ( tree_node ) for tree_node in new : new_records . setdefault ( tree_node . unified_job_template , [ ] ) new_records [ tree_node . unified_job_template ] . append ( tree_node ) for ujt_id in old_records : if ujt_id not in new_records : to_delete . extend ( old_records [ ujt_id ] ) continue old_list = old_records [ ujt_id ] new_list = new_records . pop ( ujt_id ) if len ( old_list ) == 1 and len ( new_list ) == 1 : to_recurse . append ( ( old_list [ 0 ] , new_list [ 0 ] ) ) else : to_delete . extend ( old_list ) to_expand . extend ( new_list ) for nodes in new_records . values ( ) : to_expand . extend ( nodes ) return to_expand , to_delete , to_recurse
2373	def statements ( self ) : if len ( self . rows ) == 0 : return [ ] current_statement = Statement ( self . rows [ 0 ] ) current_statement . startline = self . rows [ 0 ] . linenumber current_statement . endline = self . rows [ 0 ] . linenumber statements = [ ] for row in self . rows [ 1 : ] : if len ( row ) > 0 and row [ 0 ] == "..." : current_statement += row [ 1 : ] current_statement . endline = row . linenumber else : if len ( current_statement ) > 0 : statements . append ( current_statement ) current_statement = Statement ( row ) current_statement . startline = row . linenumber current_statement . endline = row . linenumber if len ( current_statement ) > 0 : statements . append ( current_statement ) while ( len ( statements [ - 1 ] ) == 0 or ( ( len ( statements [ - 1 ] ) == 1 ) and len ( statements [ - 1 ] [ 0 ] ) == 0 ) ) : statements . pop ( ) return statements
9495	def _parse_document_id ( elm_tree ) : xpath = '//md:content-id/text()' return [ x for x in elm_tree . xpath ( xpath , namespaces = COLLECTION_NSMAP ) ] [ 0 ]
1160	def notify ( self , n = 1 ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot notify on un-acquired lock" ) __waiters = self . __waiters waiters = __waiters [ : n ] if not waiters : if __debug__ : self . _note ( "%s.notify(): no waiters" , self ) return self . _note ( "%s.notify(): notifying %d waiter%s" , self , n , n != 1 and "s" or "" ) for waiter in waiters : waiter . release ( ) try : __waiters . remove ( waiter ) except ValueError : pass
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : module , attribute = full_name . rsplit ( '.' , 1 ) module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
13787	def flush ( self ) : queue = self . queue size = queue . qsize ( ) queue . join ( ) self . log . debug ( 'successfully flushed %s items.' , size )
10242	def get_evidences_by_pmid ( graph : BELGraph , pmids : Union [ str , Iterable [ str ] ] ) : result = defaultdict ( set ) for _ , _ , _ , data in filter_edges ( graph , build_pmid_inclusion_filter ( pmids ) ) : result [ data [ CITATION ] [ CITATION_REFERENCE ] ] . add ( data [ EVIDENCE ] ) return dict ( result )
1207	def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )
5424	def _wait_and_retry ( provider , job_id , poll_interval , retries , job_descriptor ) : while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = [ job_id ] ) running_tasks = set ( ) completed_tasks = set ( ) canceled_tasks = set ( ) fully_failed_tasks = set ( ) task_fail_count = dict ( ) message_task = None task_dict = dict ( ) for t in tasks : task_id = job_model . numeric_task_id ( t . get_field ( 'task-id' ) ) task_dict [ task_id ] = t status = t . get_field ( 'task-status' ) if status == 'FAILURE' : task_fail_count [ task_id ] = task_fail_count . get ( task_id , 0 ) + 1 if task_fail_count [ task_id ] > retries : fully_failed_tasks . add ( task_id ) message_task = t elif status == 'CANCELED' : canceled_tasks . add ( task_id ) if not message_task : message_task = t elif status == 'SUCCESS' : completed_tasks . add ( task_id ) elif status == 'RUNNING' : running_tasks . add ( task_id ) retry_tasks = ( set ( task_fail_count ) . difference ( fully_failed_tasks ) . difference ( running_tasks ) . difference ( completed_tasks ) . difference ( canceled_tasks ) ) if not retry_tasks and not running_tasks : if message_task : return [ provider . get_tasks_completion_messages ( [ message_task ] ) ] return [ ] for task_id in retry_tasks : identifier = '{}.{}' . format ( job_id , task_id ) if task_id else job_id print ( ' {} (attempt {}) failed. Retrying.' . format ( identifier , task_fail_count [ task_id ] ) ) msg = task_dict [ task_id ] . get_field ( 'status-message' ) print ( ' Failure message: {}' . format ( msg ) ) _retry_task ( provider , job_descriptor , task_id , task_fail_count [ task_id ] + 1 ) SLEEP_FUNCTION ( poll_interval )
6646	def _mirrorStructure ( dictionary , value ) : result = type ( dictionary ) ( ) for k in dictionary . keys ( ) : if isinstance ( dictionary [ k ] , dict ) : result [ k ] = _mirrorStructure ( dictionary [ k ] , value ) else : result [ k ] = value return result
11132	def _on_file_moved ( self , event : FileSystemMovedEvent ) : if not event . is_directory and self . is_data_file ( event . src_path ) : delete_event = FileSystemEvent ( event . src_path ) delete_event . event_type = EVENT_TYPE_DELETED self . _on_file_deleted ( delete_event ) create_event = FileSystemEvent ( event . dest_path ) create_event . event_type = EVENT_TYPE_CREATED self . _on_file_created ( create_event )
4719	def tsuite_setup ( trun , declr , enum ) : suite = copy . deepcopy ( TESTSUITE ) suite [ "name" ] = declr . get ( "name" ) if suite [ "name" ] is None : cij . err ( "rnr:tsuite_setup: no testsuite is given" ) return None suite [ "alias" ] = declr . get ( "alias" ) suite [ "ident" ] = "%s_%d" % ( suite [ "name" ] , enum ) suite [ "res_root" ] = os . sep . join ( [ trun [ "conf" ] [ "OUTPUT" ] , suite [ "ident" ] ] ) suite [ "aux_root" ] = os . sep . join ( [ suite [ "res_root" ] , "_aux" ] ) suite [ "evars" ] . update ( copy . deepcopy ( trun [ "evars" ] ) ) suite [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( suite [ "res_root" ] ) os . makedirs ( suite [ "aux_root" ] ) suite [ "hooks" ] = hooks_setup ( trun , suite , declr . get ( "hooks" ) ) suite [ "hooks_pr_tcase" ] = declr . get ( "hooks_pr_tcase" , [ ] ) suite [ "fname" ] = "%s.suite" % suite [ "name" ] suite [ "fpath" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTSUITES" ] , suite [ "fname" ] ] ) tcase_fpaths = [ ] if os . path . exists ( suite [ "fpath" ] ) : suite_lines = ( l . strip ( ) for l in open ( suite [ "fpath" ] ) . read ( ) . splitlines ( ) ) tcase_fpaths . extend ( ( l for l in suite_lines if len ( l ) > 1 and l [ 0 ] != "#" ) ) else : tcase_fpaths . extend ( declr . get ( "testcases" , [ ] ) ) if len ( set ( tcase_fpaths ) ) != len ( tcase_fpaths ) : cij . err ( "rnr:suite: failed: duplicate tcase in suite not supported" ) return None for tcase_fname in tcase_fpaths : tcase = tcase_setup ( trun , suite , tcase_fname ) if not tcase : cij . err ( "rnr:suite: failed: tcase_setup" ) return None suite [ "testcases" ] . append ( tcase ) return suite
6199	def simulate_diffusion ( self , save_pos = False , total_emission = True , radial = False , rs = None , seed = 1 , path = './' , wrap_func = wrap_periodic , chunksize = 2 ** 19 , chunkslice = 'times' , verbose = True ) : if rs is None : rs = np . random . RandomState ( seed = seed ) self . open_store_traj ( chunksize = chunksize , chunkslice = chunkslice , radial = radial , path = path ) self . traj_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) em_store = self . emission_tot if total_emission else self . emission print ( '- Start trajectories simulation - %s' % ctime ( ) , flush = True ) if verbose : print ( '[PID %d] Diffusion time:' % os . getpid ( ) , end = '' ) i_chunk = 0 t_chunk_size = self . emission . chunkshape [ 1 ] chunk_duration = t_chunk_size * self . t_step par_start_pos = self . particles . positions prev_time = 0 for time_size in iter_chunksize ( self . n_samples , t_chunk_size ) : if verbose : curr_time = int ( chunk_duration * ( i_chunk + 1 ) ) if curr_time > prev_time : print ( ' %ds' % curr_time , end = '' , flush = True ) prev_time = curr_time POS , em = self . _sim_trajectories ( time_size , par_start_pos , rs , total_emission = total_emission , save_pos = save_pos , radial = radial , wrap_func = wrap_func ) em_store . append ( em ) if save_pos : self . position . append ( np . vstack ( POS ) . astype ( 'float32' ) ) i_chunk += 1 self . store . h5file . flush ( ) self . traj_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . store . h5file . flush ( ) print ( '\n- End trajectories simulation - %s' % ctime ( ) , flush = True )
872	def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )
8806	def calc_periods ( hour = 0 , minute = 0 ) : period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
7912	def add_setting ( cls , name , type = unicode , default = None , factory = None , cache = False , default_d = None , doc = None , cmdline_help = None , validator = None , basic = False ) : setting_def = _SettingDefinition ( name , type , default , factory , cache , default_d , doc , cmdline_help , validator , basic ) if name not in cls . _defs : cls . _defs [ name ] = setting_def return duplicate = cls . _defs [ name ] if duplicate . type != setting_def . type : raise ValueError ( "Setting duplicate, with a different type" ) if duplicate . default != setting_def . default : raise ValueError ( "Setting duplicate, with a different default" ) if duplicate . factory != setting_def . factory : raise ValueError ( "Setting duplicate, with a different factory" )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
13153	def dict_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . DICT ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
9817	def install ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . install_on_kubernetes ( ) elif self . is_docker_compose : self . install_on_docker_compose ( ) elif self . is_docker : self . install_on_docker ( ) elif self . is_heroku : self . install_on_heroku ( )
7686	def downbeat ( annotation , sr = 22050 , length = None , ** kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
2349	def open ( self ) : if self . seed_url : self . driver_adapter . open ( self . seed_url ) self . wait_for_page_to_load ( ) return self raise UsageError ( "Set a base URL or URL_TEMPLATE to open this page." )
9622	def buttons ( self ) : return [ name for name , value in rController . _buttons . items ( ) if self . gamepad . wButtons & value == value ]
8670	def get_key ( key_name , value_name , jsonify , no_decrypt , stash , passphrase , backend ) : if value_name and no_decrypt : sys . exit ( 'VALUE_NAME cannot be used in conjuction with --no-decrypt' ) stash = _get_stash ( backend , stash , passphrase , quiet = jsonify or value_name ) try : key = stash . get ( key_name = key_name , decrypt = not no_decrypt ) except GhostError as ex : sys . exit ( ex ) if not key : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) if value_name : key = key [ 'value' ] . get ( value_name ) if not key : sys . exit ( 'Value name `{0}` could not be found under key `{1}`' . format ( value_name , key_name ) ) if jsonify or value_name : click . echo ( json . dumps ( key , indent = 4 , sort_keys = False ) . strip ( '"' ) , nl = True ) else : click . echo ( 'Retrieving key...' ) click . echo ( '\n' + _prettify_dict ( key ) )
9378	def is_valid_file ( filename ) : if os . path . exists ( filename ) : if not os . path . getsize ( filename ) : logger . warning ( '%s : file is empty.' , filename ) return False else : logger . warning ( '%s : file does not exist.' , filename ) return False return True
9872	def start_response ( self , status , response_headers , exc_info = None ) : if exc_info : try : if self . headers_sent : raise finally : exc_info = None elif self . header_set : raise AssertionError ( "Headers already set!" ) if PY3K and not isinstance ( status , str ) : self . status = str ( status , 'ISO-8859-1' ) else : self . status = status try : self . header_set = Headers ( response_headers ) except UnicodeDecodeError : self . error = ( '500 Internal Server Error' , 'HTTP Headers should be bytes' ) self . err_log . error ( 'Received HTTP Headers from client that contain' ' invalid characters for Latin-1 encoding.' ) return self . write_warning
5536	def write ( self , process_tile , data ) : if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif not isinstance ( process_tile , BufferedTile ) : raise ValueError ( "invalid process_tile type: %s" % type ( process_tile ) ) if self . config . mode not in [ "continue" , "overwrite" ] : raise ValueError ( "cannot write output in current process mode" ) if self . config . mode == "continue" and ( self . config . output . tiles_exist ( process_tile ) ) : message = "output exists, not overwritten" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) elif data is None : message = "output empty, nothing written" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) else : with Timer ( ) as t : self . config . output . write ( process_tile = process_tile , data = data ) message = "output written in %s" % t logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = True , write_msg = message )
10589	def validate_account_names ( self , names ) : for name in names : if self . get_account ( name ) is None : raise ValueError ( "The account '{}' does not exist in the" " general ledger structure." . format ( name ) )
3269	def md_jdbc_virtual_table ( key , node ) : name = node . find ( "name" ) sql = node . find ( "sql" ) escapeSql = node . find ( "escapeSql" ) escapeSql = escapeSql . text if escapeSql is not None else None keyColumn = node . find ( "keyColumn" ) keyColumn = keyColumn . text if keyColumn is not None else None n_g = node . find ( "geometry" ) geometry = JDBCVirtualTableGeometry ( n_g . find ( "name" ) , n_g . find ( "type" ) , n_g . find ( "srid" ) ) parameters = [ ] for n_p in node . findall ( "parameter" ) : p_name = n_p . find ( "name" ) p_defaultValue = n_p . find ( "defaultValue" ) p_defaultValue = p_defaultValue . text if p_defaultValue is not None else None p_regexpValidator = n_p . find ( "regexpValidator" ) p_regexpValidator = p_regexpValidator . text if p_regexpValidator is not None else None parameters . append ( JDBCVirtualTableParam ( p_name , p_defaultValue , p_regexpValidator ) ) return JDBCVirtualTable ( name , sql , escapeSql , geometry , keyColumn , parameters )
11216	def valid ( self , time : int = None ) -> bool : if time is None : epoch = datetime ( 1970 , 1 , 1 , 0 , 0 , 0 ) now = datetime . utcnow ( ) time = int ( ( now - epoch ) . total_seconds ( ) ) if isinstance ( self . valid_from , int ) and time < self . valid_from : return False if isinstance ( self . valid_to , int ) and time > self . valid_to : return False return True
3760	def mass_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count return mass_fractions ( things )
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
5317	def use_style ( self , style_name ) : try : style = getattr ( styles , style_name . upper ( ) ) except AttributeError : raise ColorfulError ( 'the style "{0}" is undefined' . format ( style_name ) ) else : self . colorpalette = style
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
171	def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
557	def getCompletingSwarms ( self ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'status' ] == 'completing' : swarmIds . append ( swarmId ) return swarmIds
3648	def applyConsumable ( self , item_id , resource_id ) : method = 'POST' url = 'item/resource/%s' % resource_id data = { 'apply' : [ { 'id' : item_id } ] } self . __request__ ( method , url , data = json . dumps ( data ) )
4235	def _convert ( value , to_type , default = None ) : try : return default if value is None else to_type ( value ) except ValueError : return default
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
5221	def exch_info ( ticker : str ) -> pd . Series : logger = logs . get_logger ( exch_info , level = 'debug' ) if ' ' not in ticker . strip ( ) : ticker = f'XYZ {ticker.strip()} Equity' info = param . load_info ( cat = 'exch' ) . get ( market_info ( ticker = ticker ) . get ( 'exch' , '' ) , dict ( ) ) if ( 'allday' in info ) and ( 'day' not in info ) : info [ 'day' ] = info [ 'allday' ] if any ( req not in info for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return pd . Series ( ) for ss in ValidSessions : if ss not in info : continue info [ ss ] = [ param . to_hour ( num = s ) for s in info [ ss ] ] return pd . Series ( info )
11224	def dump_deque ( self , obj , class_name = "collections.deque" ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
1790	def IDIV ( cpu , src ) : reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] dividend = Operators . CONCAT ( src . size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = src . read ( ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) dst_size = src . size * 2 divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) mask = ( 1 << dst_size ) - 1 sign_mask = 1 << ( dst_size - 1 ) dividend_sign = ( dividend & sign_mask ) != 0 divisor_sign = ( divisor & sign_mask ) != 0 if isinstance ( divisor , int ) : if divisor_sign : divisor = ( ( ~ divisor ) + 1 ) & mask divisor = - divisor if isinstance ( dividend , int ) : if dividend_sign : dividend = ( ( ~ dividend ) + 1 ) & mask dividend = - dividend quotient = Operators . SDIV ( dividend , divisor ) if ( isinstance ( dividend , int ) and isinstance ( dividend , int ) ) : remainder = dividend - ( quotient * divisor ) else : remainder = Operators . SREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) )
880	def create ( modelConfig , logLevel = logging . ERROR ) : logger = ModelFactory . __getLogger ( ) logger . setLevel ( logLevel ) logger . debug ( "ModelFactory returning Model from dict: %s" , modelConfig ) modelClass = None if modelConfig [ 'model' ] == "HTMPrediction" : modelClass = HTMPredictionModel elif modelConfig [ 'model' ] == "TwoGram" : modelClass = TwoGramModel elif modelConfig [ 'model' ] == "PreviousValue" : modelClass = PreviousValueModel else : raise Exception ( "ModelFactory received unsupported Model type: %s" % modelConfig [ 'model' ] ) return modelClass ( ** modelConfig [ 'modelParams' ] )
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
460	def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) tl . logging . info ( 'accuracy-score : %f' % acc ) return c_mat , f1 , acc , f1_macro
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
1249	def _is_action_available_left ( self , state ) : for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False
8039	def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
8436	def map ( cls , x , palette , limits , na_value = None ) : n = len ( limits ) pal = palette ( n ) [ match ( x , limits ) ] try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
12926	def as_dict ( self ) : self_as_dict = dict ( ) self_as_dict [ 'sequence' ] = self . sequence if hasattr ( self , 'frequency' ) : self_as_dict [ 'frequency' ] = self . frequency return self_as_dict
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
6576	def populate_fields ( api_client , instance , data ) : for key , value in instance . __class__ . _fields . items ( ) : default = getattr ( value , "default" , None ) newval = data . get ( value . field , default ) if isinstance ( value , SyntheticField ) : newval = value . formatter ( api_client , data , newval ) setattr ( instance , key , newval ) continue model_class = getattr ( value , "model" , None ) if newval and model_class : if isinstance ( newval , list ) : newval = model_class . from_json_list ( api_client , newval ) else : newval = model_class . from_json ( api_client , newval ) if newval and value . formatter : newval = value . formatter ( api_client , newval ) setattr ( instance , key , newval )
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
12303	def url_is_valid ( self , url ) : if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
12813	def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
5879	def store_image ( cls , http_client , link_hash , src , config ) : image = cls . read_localfile ( link_hash , src , config ) if image : return image if src . startswith ( 'data:image' ) : image = cls . write_localfile_base64 ( link_hash , src , config ) return image data = http_client . fetch ( src ) if data : image = cls . write_localfile ( data , link_hash , src , config ) if image : return image return None
2420	def write_document ( document , out , validate = True ) : messages = [ ] messages = document . validate ( messages ) if validate and messages : raise InvalidDocumentError ( messages ) out . write ( '# Document Information\n\n' ) write_value ( 'SPDXVersion' , str ( document . version ) , out ) write_value ( 'DataLicense' , document . data_license . identifier , out ) write_value ( 'DocumentName' , document . name , out ) write_value ( 'SPDXID' , 'SPDXRef-DOCUMENT' , out ) write_value ( 'DocumentNamespace' , document . namespace , out ) if document . has_comment : write_text_value ( 'DocumentComment' , document . comment , out ) for doc_ref in document . ext_document_references : doc_ref_str = ' ' . join ( [ doc_ref . external_document_id , doc_ref . spdx_document_uri , doc_ref . check_sum . identifier + ':' + doc_ref . check_sum . value ] ) write_value ( 'ExternalDocumentRef' , doc_ref_str , out ) write_separators ( out ) write_creation_info ( document . creation_info , out ) write_separators ( out ) for review in sorted ( document . reviews ) : write_review ( review , out ) write_separators ( out ) for annotation in sorted ( document . annotations ) : write_annotation ( annotation , out ) write_separators ( out ) write_package ( document . package , out ) write_separators ( out ) out . write ( '# Extracted Licenses\n\n' ) for lic in sorted ( document . extracted_licenses ) : write_extracted_licenses ( lic , out ) write_separators ( out )
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF if buf not in cpu . memory : logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
2496	def create_package_node ( self , package ) : package_node = BNode ( ) type_triple = ( package_node , RDF . type , self . spdx_namespace . Package ) self . graph . add ( type_triple ) self . handle_pkg_optional_fields ( package , package_node ) name_triple = ( package_node , self . spdx_namespace . name , Literal ( package . name ) ) self . graph . add ( name_triple ) down_loc_node = ( package_node , self . spdx_namespace . downloadLocation , self . to_special_value ( package . download_location ) ) self . graph . add ( down_loc_node ) verif_node = self . package_verif_node ( package ) verif_triple = ( package_node , self . spdx_namespace . packageVerificationCode , verif_node ) self . graph . add ( verif_triple ) conc_lic_node = self . license_or_special ( package . conc_lics ) conc_lic_triple = ( package_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) decl_lic_node = self . license_or_special ( package . license_declared ) decl_lic_triple = ( package_node , self . spdx_namespace . licenseDeclared , decl_lic_node ) self . graph . add ( decl_lic_triple ) licenses_from_files_nodes = map ( lambda el : self . license_or_special ( el ) , package . licenses_from_files ) lic_from_files_predicate = self . spdx_namespace . licenseInfoFromFiles lic_from_files_triples = [ ( package_node , lic_from_files_predicate , node ) for node in licenses_from_files_nodes ] for triple in lic_from_files_triples : self . graph . add ( triple ) cr_text_node = self . to_special_value ( package . cr_text ) cr_text_triple = ( package_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) self . handle_package_has_file ( package , package_node ) return package_node
11070	def proxy_factory ( BaseSchema , label , ProxiedClass , get_key ) : def local ( ) : key = get_key ( ) try : return proxies [ BaseSchema ] [ label ] [ key ] except KeyError : proxies [ BaseSchema ] [ label ] [ key ] = ProxiedClass ( ) return proxies [ BaseSchema ] [ label ] [ key ] return LocalProxy ( local )
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
4778	def is_not_empty ( self ) : if len ( self . val ) == 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected not empty string, but was empty.' ) else : self . _err ( 'Expected not empty, but was empty.' ) return self
11580	def _string_data ( self , data ) : print ( "_string_data:" ) string_to_print = [ ] for i in data [ : : 2 ] : string_to_print . append ( chr ( i ) ) print ( "" . join ( string_to_print ) )
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
1902	def summarized_name ( self , name ) : components = name . split ( '.' ) prefix = '.' . join ( c [ 0 ] for c in components [ : - 1 ] ) return f'{prefix}.{components[-1]}'
13588	def post_required ( method_or_options = [ ] ) : def decorator ( method ) : expected_fields = [ ] if not callable ( method_or_options ) : expected_fields = method_or_options @ wraps ( method ) def wrapper ( * args , ** kwargs ) : request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) missing = [ ] for field in expected_fields : if field not in request . POST : missing . append ( field ) if missing : s = 'Expected fields missing in POST: %s' % missing logger . error ( s ) raise Http404 ( s ) return method ( * args , ** kwargs ) return wrapper if callable ( method_or_options ) : return decorator ( method_or_options ) return decorator
6065	def density_between_circular_annuli_in_angular_units ( self , inner_annuli_radius , outer_annuli_radius ) : annuli_area = ( np . pi * outer_annuli_radius ** 2.0 ) - ( np . pi * inner_annuli_radius ** 2.0 ) return ( self . mass_within_circle_in_units ( radius = outer_annuli_radius ) - self . mass_within_circle_in_units ( radius = inner_annuli_radius ) ) / annuli_area
12532	def _store_dicom_paths ( self , folders ) : if isinstance ( folders , str ) : folders = [ folders ] for folder in folders : if not os . path . exists ( folder ) : raise FolderNotFound ( folder ) self . items . extend ( list ( find_all_dicom_files ( folder ) ) )
8378	def add_color_info ( e , path ) : _ctx . colormode ( RGB , 1.0 ) def _color ( hex , alpha = 1.0 ) : if hex == "none" : return None n = int ( hex [ 1 : ] , 16 ) r = ( n >> 16 ) & 0xff g = ( n >> 8 ) & 0xff b = n & 0xff return _ctx . color ( r / 255.0 , g / 255.0 , b / 255.0 , alpha ) path . fill = ( 0 , 0 , 0 , 0 ) path . stroke = ( 0 , 0 , 0 , 0 ) path . strokewidth = 0 alpha = get_attribute ( e , "opacity" , default = "" ) if alpha == "" : alpha = 1.0 else : alpha = float ( alpha ) try : path . fill = _color ( get_attribute ( e , "fill" , default = "#00000" ) , alpha ) except : pass try : path . stroke = _color ( get_attribute ( e , "stroke" , default = "none" ) , alpha ) except : pass try : path . strokewidth = float ( get_attribute ( e , "stroke-width" , default = "1" ) ) except : pass style = get_attribute ( e , "style" , default = "" ) . split ( ";" ) for s in style : try : if s . startswith ( "fill:" ) : path . fill = _color ( s . replace ( "fill:" , "" ) ) elif s . startswith ( "stroke:" ) : path . stroke = _color ( s . replace ( "stroke:" , "" ) ) elif s . startswith ( "stroke-width:" ) : path . strokewidth = float ( s . replace ( "stroke-width:" , "" ) ) except : pass path . closed = False if path [ 0 ] . x == path [ len ( path ) - 1 ] . x and path [ 0 ] . y == path [ len ( path ) - 1 ] . y : path . closed = True for i in range ( 1 , - 1 ) : if path [ i ] . cmd == MOVETO : path . closed = False return path
6612	def put_multiple ( self , task_args_kwargs_list ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return packages = [ ] for t in task_args_kwargs_list : try : task = t [ 'task' ] args = t . get ( 'args' , ( ) ) kwargs = t . get ( 'kwargs' , { } ) package = TaskPackage ( task = task , args = args , kwargs = kwargs ) except TypeError : package = TaskPackage ( task = t , args = ( ) , kwargs = { } ) packages . append ( package ) return self . dropbox . put_multiple ( packages )
10665	def stoichiometry_coefficient ( compound , element ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return stoichiometry [ element ]
1324	def threadFunc ( root ) : th = threading . currentThread ( ) auto . Logger . WriteLine ( '\nThis is running in a new thread. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan ) time . sleep ( 2 ) auto . InitializeUIAutomationInCurrentThread ( ) auto . GetConsoleWindow ( ) . CaptureToImage ( 'console_newthread.png' ) newRoot = auto . GetRootControl ( ) auto . EnumAndLogControl ( newRoot , 1 ) auto . UninitializeUIAutomationInCurrentThread ( ) auto . Logger . WriteLine ( '\nThread exits. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan )
3308	def _run_flup ( app , config , mode ) : if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8517	def _warn_if_not_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : warnings . warn ( "Result contains NaN, infinity" " or a value too large for %r." % X . dtype , category = UserWarning )
3746	def calculate_P ( self , T , P , method ) : r if method == LUCAS : mu = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat mu = Lucas ( T , P , self . Tc , self . Pc , self . omega , Psat , mu ) elif method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
1757	def write_int ( self , where , expression , size = None , force = False ) : if size is None : size = self . address_bit_size assert size in SANE_SIZES self . _publish ( 'will_write_memory' , where , expression , size ) data = [ Operators . CHR ( Operators . EXTRACT ( expression , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] self . _memory . write ( where , data , force ) self . _publish ( 'did_write_memory' , where , expression , size )
3535	def clickmap ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickmapNode ( )
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
13022	def query ( self , sql_string , * args , ** kwargs ) : commit = None columns = None if kwargs . get ( 'commit' ) is not None : commit = kwargs . pop ( 'commit' ) if kwargs . get ( 'columns' ) is not None : columns = kwargs . pop ( 'columns' ) query = self . _assemble_simple ( sql_string , * args , ** kwargs ) return self . _execute ( query , commit = commit , working_columns = columns )
9442	def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
9051	def poisson_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) link = LogLink ( ) lik = PoissonProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
7943	def _start_connect ( self ) : family , addr = self . _dst_addrs . pop ( 0 ) self . _socket = socket . socket ( family , socket . SOCK_STREAM ) self . _socket . setblocking ( False ) self . _dst_addr = addr self . _family = family try : self . _socket . connect ( addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] in BLOCKING_ERRORS : self . _set_state ( "connecting" ) self . _write_queue . append ( ContinueConnect ( ) ) self . _write_queue_cond . notify ( ) self . event ( ConnectingEvent ( addr ) ) return elif self . _dst_addrs : self . _set_state ( "connect" ) return elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise self . _connected ( )
4040	def _cleanup ( self , to_clean , allow = ( ) ) : if to_clean . keys ( ) == [ "links" , "library" , "version" , "meta" , "key" , "data" ] : to_clean = to_clean [ "data" ] return dict ( [ [ k , v ] for k , v in list ( to_clean . items ( ) ) if ( k in allow or k not in self . temp_keys ) ] )
6987	def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
6186	def get_last_commit_line ( git_path = None ) : if git_path is None : git_path = GIT_PATH output = check_output ( [ git_path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
871	def edit ( cls , properties ) : copyOfProperties = copy ( properties ) configFilePath = cls . getPath ( ) try : with open ( configFilePath , 'r' ) as fp : contents = fp . read ( ) except IOError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s reading custom configuration store " "from %s, while editing properties %s." , e . errno , configFilePath , properties ) raise contents = '<configuration/>' try : elements = ElementTree . XML ( contents ) ElementTree . tostring ( elements ) except Exception , e : msg = "File contents of custom configuration is corrupt. File " "location: %s; Contents: '%s'. Original Error (%s): %s." % ( configFilePath , contents , type ( e ) , e ) _getLogger ( ) . exception ( msg ) raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] if elements . tag != 'configuration' : e = "Expected top-level element to be 'configuration' but got '%s'" % ( elements . tag ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyItem in elements . findall ( './property' ) : propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) name = propInfo [ 'name' ] if name in copyOfProperties : foundValues = propertyItem . findall ( './value' ) if len ( foundValues ) > 0 : foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) if not copyOfProperties : break else : e = "Property %s missing value tag." % ( name , ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyName , value in copyOfProperties . iteritems ( ) : newProp = ElementTree . Element ( 'property' ) nameTag = ElementTree . Element ( 'name' ) nameTag . text = propertyName newProp . append ( nameTag ) valueTag = ElementTree . Element ( 'value' ) valueTag . text = str ( value ) newProp . append ( valueTag ) elements . append ( newProp ) try : makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) with open ( configFilePath , 'w' ) as fp : fp . write ( ElementTree . tostring ( elements ) ) except Exception , e : _getLogger ( ) . exception ( "Error while saving custom configuration " "properties %s in %s." , properties , configFilePath ) raise
13739	def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
13889	def ListMappedNetworkDrives ( ) : if sys . platform != 'win32' : raise NotImplementedError drives_list = [ ] netuse = _CallWindowsNetCommand ( [ 'use' ] ) for line in netuse . split ( EOL_STYLE_WINDOWS ) : match = re . match ( "(\w*)\s+(\w:)\s+(.+)" , line . rstrip ( ) ) if match : drives_list . append ( ( match . group ( 2 ) , match . group ( 3 ) , match . group ( 1 ) == 'OK' ) ) return drives_list
5091	def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
11598	def prepare ( self ) : attributes , elements = OrderedDict ( ) , [ ] nsmap = dict ( [ self . meta . namespace ] ) for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : nsmap . update ( [ item . namespace ] ) elements . append ( item ) return attributes , elements , nsmap
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
8383	def update ( self ) : if self . delay > 0 : self . delay -= 1 return if self . fi == 0 : if len ( self . q ) == 1 : self . fn = float ( "inf" ) else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
10272	def get_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> Iterable [ BaseEntity ] : if key is None : key = WEIGHT for node in graph : if is_unweighted_source ( graph , node , key ) : yield node
12399	def parse ( cls , s , required = False ) : req = pkg_resources . Requirement . parse ( s ) return cls ( req , required = required )
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
7225	def delete ( self , project_id ) : self . logger . debug ( 'Deleting project by id: ' + project_id ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project_id } r = self . gbdx_connection . delete ( url ) r . raise_for_status ( )
6719	def what_requires ( self , name ) : r = self . local_renderer r . env . name = name r . local ( 'pipdeptree -p {name} --reverse' )
99	def angle_between_vectors ( v1 , v2 ) : l1 = np . linalg . norm ( v1 ) l2 = np . linalg . norm ( v2 ) v1_u = ( v1 / l1 ) if l1 > 0 else np . float32 ( v1 ) * 0 v2_u = ( v2 / l2 ) if l2 > 0 else np . float32 ( v2 ) * 0 return np . arccos ( np . clip ( np . dot ( v1_u , v2_u ) , - 1.0 , 1.0 ) )
3030	def _extract_id_token ( id_token ) : if type ( id_token ) == bytes : segments = id_token . split ( b'.' ) else : segments = id_token . split ( u'.' ) if len ( segments ) != 3 : raise VerifyJwtTokenError ( 'Wrong number of segments in token: {0}' . format ( id_token ) ) return json . loads ( _helpers . _from_bytes ( _helpers . _urlsafe_b64decode ( segments [ 1 ] ) ) )
8901	def authenticate_credentials ( self , userargs , password , request = None ) : credentials = { 'password' : password } if "=" not in userargs : credentials [ get_user_model ( ) . USERNAME_FIELD ] = userargs else : for arg in userargs . split ( "&" ) : key , val = arg . split ( "=" ) credentials [ key ] = val user = authenticate ( ** credentials ) if user is None : raise exceptions . AuthenticationFailed ( 'Invalid credentials.' ) if not user . is_active : raise exceptions . AuthenticationFailed ( 'User inactive or deleted.' ) return ( user , None )
5177	def resources ( self , type_ = None , title = None , ** kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return resources
13043	def create_pipe_workers ( configfile , directory ) : type_map = { 'service' : ServiceSearch , 'host' : HostSearch , 'range' : RangeSearch , 'user' : UserSearch } config = configparser . ConfigParser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print_error ( "No named pipes configured" ) return print_notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create_query ( section ) object_type = type_map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object_type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe_worker , args = args ) ) return workers
12445	def options ( self , request , response ) : response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) response . status = http . client . OK
4309	def _validate_num_channels ( input_filepath_list , combine_type ) : channels = [ file_info . channels ( f ) for f in input_filepath_list ] if not core . all_equal ( channels ) : raise IOError ( "Input files do not have the same number of channels. The " "{} combine type requires that all files have the same " "number of channels" . format ( combine_type ) )
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
9082	def get_all ( self , ** kwargs ) : kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . get_all ( ** kwarguments ) } for p in self . providers . values ( ) ]
7588	def nexmake ( mdict , nlocus , dirs , mcmc_burnin , mcmc_ngen , mcmc_sample_freq ) : max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) handle = os . path . join ( dirs , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( ** { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : mcmc_ngen , "sfreq" : mcmc_sample_freq , "burnin" : mcmc_burnin , } ) )
4512	def resize ( image , x , y , stretch = False , top = None , left = None , mode = 'RGB' , resample = None ) : if x <= 0 : raise ValueError ( 'x must be greater than zero' ) if y <= 0 : raise ValueError ( 'y must be greater than zero' ) from PIL import Image resample = Image . ANTIALIAS if resample is None else resample if not isinstance ( resample , numbers . Number ) : try : resample = getattr ( Image , resample . upper ( ) ) except : raise ValueError ( "(1) Didn't understand resample=%s" % resample ) if not isinstance ( resample , numbers . Number ) : raise ValueError ( "(2) Didn't understand resample=%s" % resample ) size = x , y if stretch : return image . resize ( size , resample = resample ) result = Image . new ( mode , size ) ratios = [ d1 / d2 for d1 , d2 in zip ( size , image . size ) ] if ratios [ 0 ] < ratios [ 1 ] : new_size = ( size [ 0 ] , int ( image . size [ 1 ] * ratios [ 0 ] ) ) else : new_size = ( int ( image . size [ 0 ] * ratios [ 1 ] ) , size [ 1 ] ) image = image . resize ( new_size , resample = resample ) if left is None : box_x = int ( ( x - new_size [ 0 ] ) / 2 ) elif left : box_x = 0 else : box_x = x - new_size [ 0 ] if top is None : box_y = int ( ( y - new_size [ 1 ] ) / 2 ) elif top : box_y = 0 else : box_y = y - new_size [ 1 ] result . paste ( image , box = ( box_x , box_y ) ) return result
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( ** mpl_kwargs ) edge_collection = LineCollection ( ** line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( ** scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , ** new_kwargs ) else : plt . ion ( ) plt . show ( )
1950	def update_segment ( self , selector , base , size , perms ) : logger . info ( "Updating selector %s to 0x%02x (%s bytes) (%s)" , selector , base , size , perms ) if selector == 99 : self . set_fs ( base ) else : logger . error ( "No way to write segment: %d" , selector )
3442	def to_json ( model , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , ** kwargs )
1313	def ControlFromPoint ( x : int , y : int ) -> Control : element = _AutomationClient . instance ( ) . IUIAutomation . ElementFromPoint ( ctypes . wintypes . POINT ( x , y ) ) return Control . CreateControlFromElement ( element )
8473	def _addConfig ( instance , config , parent_section ) : try : section_name = "{p}/{n}" . format ( p = parent_section , n = instance . NAME . lower ( ) ) config . add_section ( section_name ) for k in instance . CONFIG . keys ( ) : config . set ( section_name , k , instance . CONFIG [ k ] ) except Exception as e : print "[!] %s" % e
1533	def get_scheduler_location ( self , topologyName , callback = None ) : if callback : self . scheduler_location_watchers [ topologyName ] . append ( callback ) else : scheduler_location_path = self . get_scheduler_location_path ( topologyName ) with open ( scheduler_location_path ) as f : data = f . read ( ) scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) return scheduler_location
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
10046	def extract_actions_from_class ( record_class ) : for name in dir ( record_class ) : method = getattr ( record_class , name , None ) if method and getattr ( method , '__deposit_action__' , False ) : yield method . __name__
310	def var_cov_var_normal ( P , c , mu = 0 , sigma = 1 ) : alpha = sp . stats . norm . ppf ( 1 - c , mu , sigma ) return P - P * ( alpha + 1 )
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
7290	def make_key ( * args , ** kwargs ) : sep = kwargs . get ( 'sep' , u"_" ) exclude_last_string = kwargs . get ( 'exclude_last_string' , False ) string_array = [ ] for arg in args : if isinstance ( arg , list ) : string_array . append ( six . text_type ( sep . join ( arg ) ) ) else : if exclude_last_string : new_key_array = arg . split ( sep ) [ : - 1 ] if len ( new_key_array ) > 0 : string_array . append ( make_key ( new_key_array ) ) else : string_array . append ( six . text_type ( arg ) ) return sep . join ( string_array )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
8469	def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None
8796	def serialize_groups ( self , groups ) : rules = [ ] for group in groups : rules . extend ( self . serialize_rules ( group . rules ) ) return rules
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : stringelems = _squeeze ( filterstring ) . lower ( ) stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] wordset = set ( stringwords2 ) allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) if len ( validatecheck ) > 0 : LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
3715	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
308	def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax
6472	def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
649	def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , "can't generate non-overlapping coincidences" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
13350	def add_file ( self , file , ** kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
4071	def split_multiline ( value ) : return [ element for element in ( line . strip ( ) for line in value . split ( '\n' ) ) if element ]
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
4938	def logo_path ( instance , filename ) : extension = os . path . splitext ( filename ) [ 1 ] . lower ( ) instance_id = str ( instance . id ) fullname = os . path . join ( "enterprise/branding/" , instance_id , instance_id + "_logo" + extension ) if default_storage . exists ( fullname ) : default_storage . delete ( fullname ) return fullname
9919	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = True ) except models . EmailAddress . DoesNotExist : return None token = models . PasswordResetToken . objects . create ( email = email ) token . send ( ) return token
7096	def init_info_window_adapter ( self ) : adapter = self . adapter if adapter : return adapter = GoogleMap . InfoWindowAdapter ( ) adapter . getInfoContents . connect ( self . on_info_window_contents_requested ) adapter . getInfoWindow . connect ( self . on_info_window_requested ) self . map . setInfoWindowAdapter ( adapter )
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
11877	def format ( self , record ) : try : record . message = record . getMessage ( ) except TypeError : if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( ** record . args ) else : record . message = record . msg . format ( record . args ) self . _fmt = self . getfmt ( record . levelname ) if self . usesTime ( ) : record . asctime = self . formatTime ( record , self . datefmt ) s = self . _fmt . format ( ** record . __dict__ ) if record . exc_info : if not record . exc_text : record . exc_text = self . formatException ( record . exc_info ) if record . exc_text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc_text except UnicodeError : s = s + record . exc_text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
11256	def flatten ( prev , depth = sys . maxsize ) : def inner_flatten ( iterable , curr_level , max_levels ) : for i in iterable : if hasattr ( i , '__iter__' ) and curr_level < max_levels : for j in inner_flatten ( i , curr_level + 1 , max_levels ) : yield j else : yield i for d in prev : if hasattr ( d , '__iter__' ) and depth > 0 : for inner_d in inner_flatten ( d , 1 , depth ) : yield inner_d else : yield d
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
191	def blend_alpha ( image_fg , image_bg , alpha , eps = 1e-2 ) : assert image_fg . shape == image_bg . shape assert image_fg . dtype . kind == image_bg . dtype . kind assert image_fg . dtype . name not in [ "float128" ] assert image_bg . dtype . name not in [ "float128" ] input_was_2d = ( len ( image_fg . shape ) == 2 ) if input_was_2d : image_fg = np . atleast_3d ( image_fg ) image_bg = np . atleast_3d ( image_bg ) input_was_bool = False if image_fg . dtype . kind == "b" : input_was_bool = True image_fg = image_fg . astype ( np . float32 ) image_bg = image_bg . astype ( np . float32 ) alpha = np . array ( alpha , dtype = np . float64 ) if alpha . size == 1 : pass else : if alpha . ndim == 2 : assert alpha . shape == image_fg . shape [ 0 : 2 ] alpha = alpha . reshape ( ( alpha . shape [ 0 ] , alpha . shape [ 1 ] , 1 ) ) elif alpha . ndim == 3 : assert alpha . shape == image_fg . shape or alpha . shape == image_fg . shape [ 0 : 2 ] + ( 1 , ) else : alpha = alpha . reshape ( ( 1 , 1 , - 1 ) ) if alpha . shape [ 2 ] != image_fg . shape [ 2 ] : alpha = np . tile ( alpha , ( 1 , 1 , image_fg . shape [ 2 ] ) ) if not input_was_bool : if np . all ( alpha >= 1.0 - eps ) : return np . copy ( image_fg ) elif np . all ( alpha <= eps ) : return np . copy ( image_bg ) assert 0 <= alpha . item ( 0 ) <= 1.0 dt_images = iadt . get_minimal_dtype ( [ image_fg , image_bg ] ) isize = dt_images . itemsize * 2 isize = max ( isize , 4 ) dt_blend = np . dtype ( "f%d" % ( isize , ) ) if alpha . dtype != dt_blend : alpha = alpha . astype ( dt_blend ) if image_fg . dtype != dt_blend : image_fg = image_fg . astype ( dt_blend ) if image_bg . dtype != dt_blend : image_bg = image_bg . astype ( dt_blend ) image_blend = image_bg + alpha * ( image_fg - image_bg ) if input_was_bool : image_blend = image_blend > 0.5 else : image_blend = iadt . restore_dtypes_ ( image_blend , dt_images , clip = False , round = True ) if input_was_2d : return image_blend [ : , : , 0 ] return image_blend
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
1992	def _get_id ( self ) : id_ = self . _last_id . value self . _last_id . value += 1 return id_
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
10802	def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
4895	def get_enterprise_user_id ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . id
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
3614	def _get_available_choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained_model_field + "_id" ) filter = { self . chained_model_field : pk } except AttributeError : try : pks = getattr ( item , self . chained_model_field ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : try : pks = getattr ( item , self . chained_model_field + "_set" ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : filter = { } filtered = list ( get_model ( self . to_app_name , self . to_model_name ) . objects . filter ( ** filter ) . distinct ( ) ) if self . sort : sort_results ( filtered ) else : filtered = [ ] return filtered
1007	def _learnPhase1 ( self , activeColumns , readOnly = False ) : self . lrnActiveState [ 't' ] . fill ( 0 ) numUnpredictedColumns = 0 for c in activeColumns : predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictedCells = len ( predictingCells ) assert numPredictedCells <= 1 if numPredictedCells == 1 : i = predictingCells [ 0 ] self . lrnActiveState [ 't' ] [ c , i ] = 1 continue numUnpredictedColumns += 1 if readOnly : continue i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) if s is not None and s . isSequenceSegment ( ) : if self . verbosity >= 4 : print "Learn branch 0, found segment match. Learning on col=" , c self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) s . totalActivations += 1 trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , minNumSyns = 0 ) else : i = self . _getCellForNewSegment ( c ) if ( self . verbosity >= 4 ) : print "Learn branch 1, no match. Learning on col=" , c , print ", newCellIdxInCol=" , i self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) segUpdate . sequenceSegment = True self . _adaptSegment ( segUpdate ) numBottomUpColumns = len ( activeColumns ) if numUnpredictedColumns < numBottomUpColumns / 2 : return True else : return False
13068	def r_collections ( self , lang = None ) : collection = self . resolver . getMetadata ( ) return { "template" : "main::collection.html" , "current_label" : collection . get_label ( lang ) , "collections" : { "members" : self . make_members ( collection , lang = lang ) } }
655	def _fillInOnTimes ( vector , durations ) : nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] if len ( nonzeros ) == 0 : return if len ( nonzeros ) == 1 : durations [ nonzeros [ 0 ] ] = 1 return prev = nonzeros [ 0 ] onTime = 1 onStartIdx = prev endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) onTime = 1 onStartIdx = idx else : onTime += 1 prev = idx durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 )
4984	def extend_course ( course , enterprise_customer , request ) : course_run_id = course [ 'course_runs' ] [ 0 ] [ 'key' ] try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : error_code = 'ENTPEV000' LOGGER . error ( 'CourseCatalogApiServiceClient is improperly configured. ' 'Returned error code {error_code} to user {userid} ' 'and enterprise_customer {enterprise_customer} ' 'for course_run_id {course_run_id}' . format ( error_code = error_code , userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) course_details , course_run_details = catalog_api_client . get_course_and_course_run ( course_run_id ) if not course_details or not course_run_details : error_code = 'ENTPEV001' LOGGER . error ( 'User {userid} of enterprise customer {enterprise_customer} encountered an error.' 'No course_details or course_run_details found for ' 'course_run_id {course_run_id}. ' 'The following error code reported to the user: {error_code}' . format ( userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , error_code = error_code , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) weeks_to_complete = course_run_details [ 'weeks_to_complete' ] course_run_image = course_run_details [ 'image' ] or { } course . update ( { 'course_image_uri' : course_run_image . get ( 'src' , '' ) , 'course_title' : course_run_details [ 'title' ] , 'course_level_type' : course_run_details . get ( 'level_type' , '' ) , 'course_short_description' : course_run_details [ 'short_description' ] or '' , 'course_full_description' : clean_html_for_template_rendering ( course_run_details [ 'full_description' ] or '' ) , 'expected_learning_items' : course_details . get ( 'expected_learning_items' , [ ] ) , 'staff' : course_run_details . get ( 'staff' , [ ] ) , 'course_effort' : ungettext_min_max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course_run_details [ 'min_effort' ] or None , course_run_details [ 'max_effort' ] or None , ) or '' , 'weeks_to_complete' : ungettext ( '{} week' , '{} weeks' , weeks_to_complete ) . format ( weeks_to_complete ) if weeks_to_complete else '' , } ) return course , None
1605	def run_bolts ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) bolt_name = cl_args [ 'bolt' ] if bolt_name : if bolt_name in bolts : bolts = [ bolt_name ] else : Log . error ( 'Unknown bolt: \'%s\'' % bolt_name ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False bolts_result = [ ] for bolt in bolts : try : metrics = tracker_access . get_component_metrics ( bolt , cluster , env , topology , role ) stat , header = to_table ( metrics ) bolts_result . append ( ( bolt , stat , header ) ) except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False for i , ( bolt , stat , header ) in enumerate ( bolts_result ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % bolt ) print ( tabulate ( stat , headers = header ) ) return True
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
4623	def _derive_checksum ( self , s ) : checksum = hashlib . sha256 ( bytes ( s , "ascii" ) ) . hexdigest ( ) return checksum [ : 4 ]
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
2270	def _win32_symlink2 ( path , link , allow_fallback = True , verbose = 0 ) : if _win32_can_symlink ( ) : return _win32_symlink ( path , link , verbose ) else : return _win32_junction ( path , link , verbose )
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 self . check_object_permissions ( self . request , obj ) return obj
326	def simulate_paths ( is_returns , num_days , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = np . empty ( ( num_samples , num_days ) ) seed = np . random . RandomState ( seed = random_seed ) for i in range ( num_samples ) : samples [ i , : ] = is_returns . sample ( num_days , replace = True , random_state = seed ) return samples
3849	async def fetch ( self , method , url , params = None , headers = None , data = None ) : logger . debug ( 'Sending request %s %s:\n%r' , method , url , data ) for retry_num in range ( MAX_RETRIES ) : try : async with self . fetch_raw ( method , url , params = params , headers = headers , data = data ) as res : async with async_timeout . timeout ( REQUEST_TIMEOUT ) : body = await res . read ( ) logger . debug ( 'Received response %d %s:\n%r' , res . status , res . reason , body ) except asyncio . TimeoutError : error_msg = 'Request timed out' except aiohttp . ServerDisconnectedError as err : error_msg = 'Server disconnected error: {}' . format ( err ) except ( aiohttp . ClientError , ValueError ) as err : error_msg = 'Request connection error: {}' . format ( err ) else : break logger . info ( 'Request attempt %d failed: %s' , retry_num , error_msg ) else : logger . info ( 'Request failed after %d attempts' , MAX_RETRIES ) raise exceptions . NetworkError ( error_msg ) if res . status != 200 : logger . info ( 'Request returned unexpected status: %d %s' , res . status , res . reason ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) return FetchResponse ( res . status , body )
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
6103	def luminosities_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
132	def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
822	def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
504	def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList
5522	def parse_list_line_windows ( self , b ) : line = b . decode ( encoding = self . encoding ) . rstrip ( "\r\n" ) date_time_end = line . index ( "M" ) date_time_str = line [ : date_time_end + 1 ] . strip ( ) . split ( " " ) date_time_str = " " . join ( [ x for x in date_time_str if len ( x ) > 0 ] ) line = line [ date_time_end + 1 : ] . lstrip ( ) with setlocale ( "C" ) : strptime = datetime . datetime . strptime date_time = strptime ( date_time_str , "%m/%d/%Y %I:%M %p" ) info = { } info [ "modify" ] = self . format_date_time ( date_time ) next_space = line . index ( " " ) if line . startswith ( "<DIR>" ) : info [ "type" ] = "dir" else : info [ "type" ] = "file" info [ "size" ] = line [ : next_space ] . replace ( "," , "" ) if not info [ "size" ] . isdigit ( ) : raise ValueError filename = line [ next_space : ] . lstrip ( ) if filename == "." or filename == ".." : raise ValueError return pathlib . PurePosixPath ( filename ) , info
11071	def with_proxies ( proxy_map , get_key ) : def wrapper ( cls ) : for label , ProxiedClass in six . iteritems ( proxy_map ) : proxy = proxy_factory ( cls , label , ProxiedClass , get_key ) setattr ( cls , label , proxy ) return cls return wrapper
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
11131	def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
9299	def apply_filters ( self , query , filters ) : assert isinstance ( query , peewee . Query ) assert isinstance ( filters , dict )
881	def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )
7018	def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) metadata = re . sub ( r'^\d+\.+' , '' , metadata ) metadata = re . sub ( r'[^\w\s]' , '' , metadata ) metadata = re . sub ( r'\s+' , ' ' , metadata ) metadata = re . sub ( r'^\s+' , '' , metadata ) metadata = re . sub ( r'\s+$' , '' , metadata ) metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) return metadata
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
6821	def configure_modsecurity ( self ) : r = self . local_renderer if r . env . modsecurity_enabled and not self . last_manifest . modsecurity_enabled : self . install_packages ( ) fn = self . render_to_file ( 'apache/apache_modsecurity.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/modsecurity/modsecurity.conf' , use_sudo = True ) r . env . modsecurity_download_filename = '/tmp/owasp-modsecurity-crs.tar.gz' r . sudo ( 'cd /tmp; wget --output-document={apache_modsecurity_download_filename} {apache_modsecurity_download_url}' ) r . env . modsecurity_download_top = r . sudo ( "cd /tmp; " "tar tzf %(apache_modsecurity_download_filename)s | sed -e 's@/.*@@' | uniq" % self . genv ) r . sudo ( 'cd /tmp; tar -zxvf %(apache_modsecurity_download_filename)s' % self . genv ) r . sudo ( 'cd /tmp; cp -R %(apache_modsecurity_download_top)s/* /etc/modsecurity/' % self . genv ) r . sudo ( 'mv /etc/modsecurity/modsecurity_crs_10_setup.conf.example /etc/modsecurity/modsecurity_crs_10_setup.conf' ) r . sudo ( 'rm -f /etc/modsecurity/activated_rules/*' ) r . sudo ( 'cd /etc/modsecurity/base_rules; ' 'for f in * ; do ln -s /etc/modsecurity/base_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . sudo ( 'cd /etc/modsecurity/optional_rules; ' 'for f in * ; do ln -s /etc/modsecurity/optional_rules/$f /etc/modsecurity/activated_rules/$f ; done' ) r . env . httpd_conf_append . append ( 'Include "/etc/modsecurity/activated_rules/*.conf"' ) self . enable_mod ( 'evasive' ) self . enable_mod ( 'headers' ) elif not self . env . modsecurity_enabled and self . last_manifest . modsecurity_enabled : self . disable_mod ( 'modsecurity' )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
11103	def backup ( self , dst = None , ignore = None , ignore_ext = None , ignore_pattern = None , ignore_size_smaller_than = None , ignore_size_larger_than = None , case_sensitive = False ) : def preprocess_arg ( arg ) : if arg is None : return [ ] if isinstance ( arg , ( tuple , list ) ) : return list ( arg ) else : return [ arg , ] self . assert_is_dir_and_exists ( ) ignore = preprocess_arg ( ignore ) for i in ignore : if i . startswith ( "/" ) or i . startswith ( "\\" ) : raise ValueError ignore_ext = preprocess_arg ( ignore_ext ) for ext in ignore_ext : if not ext . startswith ( "." ) : raise ValueError ignore_pattern = preprocess_arg ( ignore_pattern ) if case_sensitive : pass else : ignore = [ i . lower ( ) for i in ignore ] ignore_ext = [ i . lower ( ) for i in ignore_ext ] ignore_pattern = [ i . lower ( ) for i in ignore_pattern ] def filters ( p ) : relpath = p . relative_to ( self ) . abspath if not case_sensitive : relpath = relpath . lower ( ) for i in ignore : if relpath . startswith ( i ) : return False if case_sensitive : ext = p . ext else : ext = p . ext . lower ( ) if ext in ignore_ext : return False for pattern in ignore_pattern : if pattern in relpath : return False if ignore_size_smaller_than : if p . size < ignore_size_smaller_than : return False if ignore_size_larger_than : if p . size > ignore_size_larger_than : return False return True self . make_zip_archive ( dst = dst , filters = filters , compress = True , overwrite = False , verbose = True , )
9846	def resample_factor ( self , factor ) : newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
9130	def store_drop ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_drop ( resource ) _store_helper ( action , session = session ) return action
12313	def instantiate ( repo , name = None , filename = None ) : default_transformers = repo . options . get ( 'transformer' , { } ) transformers = { } if name is not None : if name in default_transformers : transformers = { name : default_transformers [ name ] } else : transformers = { name : { 'files' : [ ] , } } else : transformers = default_transformers input_matching_files = None if filename is not None : input_matching_files = repo . find_matching_files ( [ filename ] ) for t in transformers : for k in transformers [ t ] : if "files" not in k : continue if k == "files" and input_matching_files is not None : transformers [ t ] [ k ] = input_matching_files else : if transformers [ t ] [ k ] is None or len ( transformers [ t ] [ k ] ) == 0 : transformers [ t ] [ k ] = [ ] else : matching_files = repo . find_matching_files ( transformers [ t ] [ k ] ) transformers [ t ] [ k ] = matching_files return transformers
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
12017	def _dump_field ( self , fd ) : v = { } v [ 'label' ] = Pbd . LABELS [ fd . label ] v [ 'type' ] = fd . type_name if len ( fd . type_name ) > 0 else Pbd . TYPES [ fd . type ] v [ 'name' ] = fd . name v [ 'number' ] = fd . number v [ 'default' ] = '[default = {}]' . format ( fd . default_value ) if len ( fd . default_value ) > 0 else '' f = '{label} {type} {name} = {number} {default};' . format ( ** v ) f = ' ' . join ( f . split ( ) ) self . _print ( f ) if len ( fd . type_name ) > 0 : self . uses . append ( fd . type_name )
7136	def redirect_stdout ( new_stdout ) : old_stdout , sys . stdout = sys . stdout , new_stdout try : yield None finally : sys . stdout = old_stdout
13427	def delete_messages ( self , messages ) : url = "/2/messages/?%s" % urlencode ( [ ( 'ids' , "," . join ( messages ) ) ] ) data = self . _delete_resource ( url ) return data
7626	def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , ** kwargs )
7879	def add_prefix ( self , namespace , prefix ) : if prefix == "xml" and namespace != XML_NS : raise ValueError , "Cannot change 'xml' prefix meaning" self . _prefixes [ namespace ] = prefix
12959	def _peekNextID ( self , conn = None ) : if conn is None : conn = self . _get_connection ( ) return to_unicode ( conn . get ( self . _get_next_id_key ( ) ) or 0 )
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
9116	def reset_cleansers ( confirm = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) get_vars ( ) cleanser_count = AV [ 'ploy_cleanser_count' ] fab . run ( 'ezjail-admin stop worker' ) for cleanser_index in range ( cleanser_count ) : cindex = '{:02d}' . format ( cleanser_index + 1 ) fab . run ( 'ezjail-admin stop cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy tank/jails/cleanser_{cindex}@jdispatch_rollback' . format ( cindex = cindex ) ) fab . run ( 'ezjail-admin delete -fw cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'umount -f /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'rm -rf /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy -R tank/jails/cleanser@clonesource' ) fab . run ( 'ezjail-admin start worker' ) fab . run ( 'ezjail-admin stop cleanser' ) fab . run ( 'ezjail-admin start cleanser' )
1001	def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print "----- computeEnd summary: " print "learn:" , learn print "numBurstingCols: %s, " % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print "curPredScore2: %s, " % ( self . _internalStats [ 'curPredictionScore2' ] ) , print "curFalsePosScore: %s, " % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print "1-curFalseNegScore: %s, " % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print "numSegments: " , self . getNumSegments ( ) , print "avgLearnedSeqLength: " , self . avgLearnedSeqLength print "----- infActiveState (%d on) ------" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print "----- infPredictedState (%d on)-----" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print "----- lrnActiveState (%d on) ------" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print "----- lrnPredictedState (%d on)-----" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print "----- cellConfidence -----" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print "----- colConfidence -----" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print "----- cellConfidence[t-1] for currently active cells -----" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print "Cells, predicted segments only:" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print "TM: learn:" , learn print "TM: active outputs(%d):" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )
3458	def main ( argv ) : source , target , tag = argv if "a" in tag : bump = "alpha" if "b" in tag : bump = "beta" else : bump = find_bump ( target , tag ) filename = "{}.md" . format ( tag ) destination = copy ( join ( source , filename ) , target ) build_hugo_md ( destination , tag , bump )
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
8715	def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
3880	async def _handle_watermark_notification ( self , watermark_notification ) : conv_id = watermark_notification . conversation_id . id res = parsers . parse_watermark_notification ( watermark_notification ) await self . on_watermark_notification . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for watermark notification: %s' , conv_id ) else : await conv . on_watermark_notification . fire ( res )
12081	def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
9663	def get_tied_targets ( original_targets , the_ties ) : my_ties = [ ] for original_target in original_targets : for item in the_ties : if original_target in item : for thing in item : my_ties . append ( thing ) my_ties = list ( set ( my_ties ) ) if my_ties : ties_message = "" ties_message += "The following targets share dependencies and must be run together:" for item in sorted ( my_ties ) : ties_message += "\n - {}" . format ( item ) return list ( set ( my_ties + original_targets ) ) , ties_message return original_targets , ""
11058	def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
2715	def create ( self , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) params = { "name" : self . name } output = self . get_data ( "tags" , type = "POST" , params = params ) if output : self . name = output [ 'tag' ] [ 'name' ] self . resources = output [ 'tag' ] [ 'resources' ]
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
4452	def aggregate ( self , query ) : if isinstance ( query , AggregateRequest ) : has_schema = query . _with_schema has_cursor = bool ( query . _cursor ) cmd = [ self . AGGREGATE_CMD , self . index_name ] + query . build_args ( ) elif isinstance ( query , Cursor ) : has_schema = False has_cursor = True cmd = [ self . CURSOR_CMD , 'READ' , self . index_name ] + query . build_args ( ) else : raise ValueError ( 'Bad query' , query ) raw = self . redis . execute_command ( * cmd ) if has_cursor : if isinstance ( query , Cursor ) : query . cid = raw [ 1 ] cursor = query else : cursor = Cursor ( raw [ 1 ] ) raw = raw [ 0 ] else : cursor = None if query . _with_schema : schema = raw [ 0 ] rows = raw [ 2 : ] else : schema = None rows = raw [ 1 : ] res = AggregateResult ( rows , cursor , schema ) return res
5105	def poisson_random_measure ( t , rate , rate_max ) : scale = 1.0 / rate_max t = t + exponential ( scale ) while rate_max * uniform ( ) > rate ( t ) : t = t + exponential ( scale ) return t
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
2690	def new_compiler ( * args , ** kwargs ) : make_silent = kwargs . pop ( 'silent' , True ) cc = _new_compiler ( * args , ** kwargs ) if is_msvc ( cc ) : from distutils . msvc9compiler import get_build_version if get_build_version ( ) == 10 : cc . initialize ( ) for ldflags in [ cc . ldflags_shared , cc . ldflags_shared_debug ] : unique_extend ( ldflags , [ '/MANIFEST' ] ) elif get_build_version ( ) == 14 : make_silent = False if make_silent : cc . spawn = _CCompiler_spawn_silent return cc
8037	def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
6382	def sim_hamming ( src , tar , diff_lens = True ) : return Hamming ( ) . sim ( src , tar , diff_lens )
2663	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : external_block_id = str ( len ( self . blocks ) ) launch_cmd = self . launch_cmd . format ( block_id = external_block_id ) internal_block = self . provider . submit ( launch_cmd , 1 , 1 ) logger . debug ( "Launched block {}->{}" . format ( external_block_id , internal_block ) ) if not internal_block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) r . extend ( [ external_block_id ] ) self . blocks [ external_block_id ] = internal_block else : logger . error ( "No execution provider available" ) r = None return r
124	def to_normalized_batch ( self ) : assert all ( [ attr is None for attr_name , attr in self . __dict__ . items ( ) if attr_name . endswith ( "_aug" ) ] ) , "Expected UnnormalizedBatch to not contain any augmented data " "before normalization, but at least one '*_aug' attribute was " "already set." images_unaug = nlib . normalize_images ( self . images_unaug ) shapes = None if images_unaug is not None : shapes = [ image . shape for image in images_unaug ] return Batch ( images = images_unaug , heatmaps = nlib . normalize_heatmaps ( self . heatmaps_unaug , shapes ) , segmentation_maps = nlib . normalize_segmentation_maps ( self . segmentation_maps_unaug , shapes ) , keypoints = nlib . normalize_keypoints ( self . keypoints_unaug , shapes ) , bounding_boxes = nlib . normalize_bounding_boxes ( self . bounding_boxes_unaug , shapes ) , polygons = nlib . normalize_polygons ( self . polygons_unaug , shapes ) , line_strings = nlib . normalize_line_strings ( self . line_strings_unaug , shapes ) , data = self . data )
11950	def _set_global_verbosity_level ( is_verbose_output = False ) : global verbose_output verbose_output = is_verbose_output if verbose_output : jocker_lgr . setLevel ( logging . DEBUG ) else : jocker_lgr . setLevel ( logging . INFO )
5394	def _localize_inputs_recursive_command ( self , task_dir , inputs ) : data_dir = os . path . join ( task_dir , _DATA_SUBDIR ) provider_commands = [ providers_util . build_recursive_localize_command ( data_dir , inputs , file_provider ) for file_provider in _SUPPORTED_INPUT_PROVIDERS ] return '\n' . join ( provider_commands )
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
558	def bestModelInSprint ( self , sprintIdx ) : swarms = self . getAllSwarms ( sprintIdx ) bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
1013	def _getBestMatchingCell ( self , c , activeState , minThreshold ) : bestActivityInCol = minThreshold bestSegIdxInCol = - 1 bestCellInCol = - 1 for i in xrange ( self . cellsPerColumn ) : maxSegActivity = 0 maxSegIdx = 0 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState ) if activity > maxSegActivity : maxSegActivity = activity maxSegIdx = j if maxSegActivity >= bestActivityInCol : bestActivityInCol = maxSegActivity bestSegIdxInCol = maxSegIdx bestCellInCol = i if bestCellInCol == - 1 : return ( None , None , None ) else : return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , bestActivityInCol )
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : gene = cobra_model . genes [ gene_index ] cobra_model . genes . _dict . pop ( gene . id ) gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
8435	def map ( cls , x , palette , limits , na_value = None , oob = censor ) : x = oob ( rescale ( x , _from = limits ) ) pal = palette ( x ) try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
10937	def update_Broyden_J ( self ) : CLOG . debug ( 'Broyden update.' ) delta_vals = self . param_vals - self . _last_vals delta_residuals = self . calc_residuals ( ) - self . _last_residuals nrm = np . sqrt ( np . dot ( delta_vals , delta_vals ) ) direction = delta_vals / nrm vals = delta_residuals / nrm self . _rank_1_J_update ( direction , vals ) self . JTJ = np . dot ( self . J , self . J . T )
4276	def get_albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get_albums ( subdir ) : yield subname , self . albums [ subdir ]
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
12139	def from_pattern ( cls , pattern , filetype = None , key = 'filename' , root = None , ignore = [ ] ) : filepattern = FilePattern ( key , pattern , root = root ) if FileInfo . filetype and filetype is None : filetype = FileInfo . filetype elif filetype is None : raise Exception ( "The filetype argument must be supplied unless " "an appropriate default has been specified as " "FileInfo.filetype" ) return FileInfo ( filepattern , key , filetype , ignore = ignore )
11729	def flag_inner_classes ( obj ) : for tup in class_members ( obj ) : tup [ 1 ] . _parent = obj tup [ 1 ] . _parent_inst = None tup [ 1 ] . __getattr__ = my_getattr flag_inner_classes ( tup [ 1 ] )
3238	def get_role_managed_policy_documents ( role , client = None , ** kwargs ) : policies = get_role_managed_policies ( role , force_client = client ) policy_names = ( policy [ 'name' ] for policy in policies ) delayed_gmpd_calls = ( delayed ( get_managed_policy_document ) ( policy [ 'arn' ] , force_client = client ) for policy in policies ) policy_documents = Parallel ( n_jobs = 20 , backend = "threading" ) ( delayed_gmpd_calls ) return dict ( zip ( policy_names , policy_documents ) )
11968	def _dec_to_bin ( ip ) : bits = [ ] while ip : bits . append ( _BYTES_TO_BITS [ ip & 255 ] ) ip >>= 8 bits . reverse ( ) return '' . join ( bits ) or 32 * '0'
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
4825	def has_course_mode ( self , course_run_id , mode ) : course_modes = self . get_course_modes ( course_run_id ) return any ( course_mode for course_mode in course_modes if course_mode [ 'slug' ] == mode )
6011	def load_psf ( psf_path , psf_hdu , pixel_scale , renormalize = False ) : if renormalize : return PSF . from_fits_renormalized ( file_path = psf_path , hdu = psf_hdu , pixel_scale = pixel_scale ) if not renormalize : return PSF . from_fits_with_scale ( file_path = psf_path , hdu = psf_hdu , pixel_scale = pixel_scale )
6968	def smooth_magseries_savgol ( mags , windowsize , polyorder = 2 ) : smoothed = savgol_filter ( mags , windowsize , polyorder ) return smoothed
13093	def write_targets ( self ) : if len ( self . ldap_strings ) == 0 and len ( self . ips ) == 0 : print_notification ( "No targets left" ) if self . auto_exit : if self . notifier : self . notifier . stop ( ) self . terminate_processes ( ) with open ( self . targets_file , 'w' ) as f : f . write ( '\n' . join ( self . ldap_strings + self . ips ) )
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
8580	def create_server ( self , datacenter_id , server ) : data = json . dumps ( self . _create_server_dict ( server ) ) response = self . _perform_request ( url = '/datacenters/%s/servers' % ( datacenter_id ) , method = 'POST' , data = data ) return response
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
6875	def _pyuncompress_sqlitecurve ( sqlitecurve , force = False ) : outfile = sqlitecurve . replace ( '.gz' , '' ) try : if os . path . exists ( outfile ) and not force : return outfile else : with gzip . open ( sqlitecurve , 'rb' ) as infd : with open ( outfile , 'wb' ) as outfd : shutil . copyfileobj ( infd , outfd ) if os . path . exists ( outfile ) : return outfile except Exception as e : return None
13810	def MakeDescriptor ( desc_proto , package = '' , build_file_if_cpp = True , syntax = None ) : if api_implementation . Type ( ) == 'cpp' and build_file_if_cpp : from typy . google . protobuf import descriptor_pb2 file_descriptor_proto = descriptor_pb2 . FileDescriptorProto ( ) file_descriptor_proto . message_type . add ( ) . MergeFrom ( desc_proto ) proto_name = str ( uuid . uuid4 ( ) ) if package : file_descriptor_proto . name = os . path . join ( package . replace ( '.' , '/' ) , proto_name + '.proto' ) file_descriptor_proto . package = package else : file_descriptor_proto . name = proto_name + '.proto' _message . default_pool . Add ( file_descriptor_proto ) result = _message . default_pool . FindFileByName ( file_descriptor_proto . name ) if _USE_C_DESCRIPTORS : return result . message_types_by_name [ desc_proto . name ] full_message_name = [ desc_proto . name ] if package : full_message_name . insert ( 0 , package ) enum_types = { } for enum_proto in desc_proto . enum_type : full_name = '.' . join ( full_message_name + [ enum_proto . name ] ) enum_desc = EnumDescriptor ( enum_proto . name , full_name , None , [ EnumValueDescriptor ( enum_val . name , ii , enum_val . number ) for ii , enum_val in enumerate ( enum_proto . value ) ] ) enum_types [ full_name ] = enum_desc nested_types = { } for nested_proto in desc_proto . nested_type : full_name = '.' . join ( full_message_name + [ nested_proto . name ] ) nested_desc = MakeDescriptor ( nested_proto , package = '.' . join ( full_message_name ) , build_file_if_cpp = False , syntax = syntax ) nested_types [ full_name ] = nested_desc fields = [ ] for field_proto in desc_proto . field : full_name = '.' . join ( full_message_name + [ field_proto . name ] ) enum_desc = None nested_desc = None if field_proto . HasField ( 'type_name' ) : type_name = field_proto . type_name full_type_name = '.' . join ( full_message_name + [ type_name [ type_name . rfind ( '.' ) + 1 : ] ] ) if full_type_name in nested_types : nested_desc = nested_types [ full_type_name ] elif full_type_name in enum_types : enum_desc = enum_types [ full_type_name ] field = FieldDescriptor ( field_proto . name , full_name , field_proto . number - 1 , field_proto . number , field_proto . type , FieldDescriptor . ProtoTypeToCppProtoType ( field_proto . type ) , field_proto . label , None , nested_desc , enum_desc , None , False , None , options = field_proto . options , has_default_value = False ) fields . append ( field ) desc_name = '.' . join ( full_message_name ) return Descriptor ( desc_proto . name , desc_name , None , None , fields , list ( nested_types . values ( ) ) , list ( enum_types . values ( ) ) , [ ] , options = desc_proto . options )
270	def check_intraday ( estimate , returns , positions , transactions ) : if estimate == 'infer' : if positions is not None and transactions is not None : if detect_intraday ( positions , transactions ) : warnings . warn ( 'Detected intraday strategy; inferring positi' + 'ons from transactions. Set estimate_intraday' + '=False to disable.' ) return estimate_intraday ( returns , positions , transactions ) else : return positions else : return positions elif estimate : if positions is not None and transactions is not None : return estimate_intraday ( returns , positions , transactions ) else : raise ValueError ( 'Positions and txns needed to estimate intraday' ) else : return positions
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
7201	def is_ordered ( cat_id ) : url = 'https://rda.geobigdata.io/v1/stripMetadata/{}' . format ( cat_id ) auth = Auth ( ) r = _req_with_retries ( auth . gbdx_connection , url ) if r is not None : return r . status_code == 200 return False
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
4871	def create ( self , validated_data ) : ret = [ ] for attrs in validated_data : if 'non_field_errors' not in attrs and not any ( isinstance ( attrs [ field ] , list ) for field in attrs ) : ret . append ( self . child . create ( attrs ) ) else : ret . append ( attrs ) return ret
11249	def average ( numbers , numtype = 'float' ) : if type == 'decimal' : return Decimal ( sum ( numbers ) ) / len ( numbers ) else : return float ( sum ( numbers ) ) / len ( numbers )
9520	def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
2969	def _sm_stop_from_pain ( self , * args , ** kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _do_reset_all ( )
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) jsonschema . validate ( definition , schema ) assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
10021	def get_environments ( self ) : response = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) return response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ]
4568	def dumps ( data , use_yaml = None , safe = True , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML if use_yaml : dumps = yaml . safe_dump if safe else yaml . dump else : dumps = json . dumps kwds . update ( indent = 4 , sort_keys = True ) if not safe : kwds . update ( default = repr ) return dumps ( data , ** kwds )
9086	def _sort ( self , concepts , sort = None , language = 'any' , reverse = False ) : sorted = copy . copy ( concepts ) if sort : sorted . sort ( key = methodcaller ( '_sortkey' , sort , language ) , reverse = reverse ) return sorted
5253	def assemble_one ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : try : instruction_table = instruction_tables [ fork ] asmcode = asmcode . strip ( ) . split ( ' ' ) instr = instruction_table [ asmcode [ 0 ] . upper ( ) ] if pc : instr . pc = pc if instr . operand_size > 0 : assert len ( asmcode ) == 2 instr . operand = int ( asmcode [ 1 ] , 0 ) return instr except : raise AssembleError ( "Something wrong at pc %d" % pc )
1585	def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
8646	def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
855	def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
7483	def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
7241	def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : return self else : return self [ g ]
4869	def to_representation ( self , instance ) : updated_program = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_program [ 'enrollment_url' ] = enterprise_customer_catalog . get_program_enrollment_url ( updated_program [ 'uuid' ] ) for course in updated_program [ 'courses' ] : course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( course [ 'key' ] ) for course_run in course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_program
1227	def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : raise NotImplementedError
6440	def dist_euclidean ( src , tar , qval = 2 , alphabet = None ) : return Euclidean ( ) . dist ( src , tar , qval , alphabet )
11712	def flatten ( d ) : if not isinstance ( d , dict ) : return [ [ d ] ] returned = [ ] for key , value in d . items ( ) : nested = flatten ( value ) for nest in nested : current_row = [ key ] current_row . extend ( nest ) returned . append ( current_row ) return returned
4165	def _get_link ( self , cobj ) : fname_idx = None full_name = cobj [ 'module_short' ] + '.' + cobj [ 'name' ] if full_name in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ full_name ] if isinstance ( value , dict ) : value = value [ next ( iter ( value . keys ( ) ) ) ] fname_idx = value [ 0 ] elif cobj [ 'module_short' ] in self . _searchindex [ 'objects' ] : value = self . _searchindex [ 'objects' ] [ cobj [ 'module_short' ] ] if cobj [ 'name' ] in value . keys ( ) : fname_idx = value [ cobj [ 'name' ] ] [ 0 ] if fname_idx is not None : fname = self . _searchindex [ 'filenames' ] [ fname_idx ] + '.html' if self . _is_windows : fname = fname . replace ( '/' , '\\' ) link = os . path . join ( self . doc_url , fname ) else : link = posixpath . join ( self . doc_url , fname ) if hasattr ( link , 'decode' ) : link = link . decode ( 'utf-8' , 'replace' ) if link in self . _page_cache : html = self . _page_cache [ link ] else : html = get_data ( link , self . gallery_dir ) self . _page_cache [ link ] = html comb_names = [ cobj [ 'module_short' ] + '.' + cobj [ 'name' ] ] if self . extra_modules_test is not None : for mod in self . extra_modules_test : comb_names . append ( mod + '.' + cobj [ 'name' ] ) url = False if hasattr ( html , 'decode' ) : html = html . decode ( 'utf-8' , 'replace' ) for comb_name in comb_names : if hasattr ( comb_name , 'decode' ) : comb_name = comb_name . decode ( 'utf-8' , 'replace' ) if comb_name in html : url = link + u'#' + comb_name link = url else : link = False return link
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
5699	def write_stats_as_csv ( gtfs , path_to_csv , re_write = False ) : stats_dict = get_stats ( gtfs ) if re_write : os . remove ( path_to_csv ) is_new = True mode = 'r' if os . path . exists ( path_to_csv ) else 'w+' with open ( path_to_csv , mode ) as csvfile : for line in csvfile : if line : is_new = False else : is_new = True with open ( path_to_csv , 'a' ) as csvfile : if ( sys . version_info > ( 3 , 0 ) ) : delimiter = u"," else : delimiter = b"," statswriter = csv . writer ( csvfile , delimiter = delimiter ) if is_new : statswriter . writerow ( [ key for key in sorted ( stats_dict . keys ( ) ) ] ) row_to_write = [ ] for key in sorted ( stats_dict . keys ( ) ) : row_to_write . append ( stats_dict [ key ] ) statswriter . writerow ( row_to_write )
13118	def argument_search ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . search ( ** vars ( arguments ) )
9952	def get_object ( name : str ) : elms = name . split ( "." ) parent = get_models ( ) [ elms . pop ( 0 ) ] while len ( elms ) > 0 : obj = elms . pop ( 0 ) parent = getattr ( parent , obj ) return parent
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
1948	def write_back_memory ( self , where , expr , size ) : if self . write_backs_disabled : return if type ( expr ) is bytes : self . _emu . mem_write ( where , expr ) else : if issymbolic ( expr ) : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] concrete_data = [ ] for c in data : if issymbolic ( c ) : c = chr ( solver . get_value ( self . _cpu . memory . constraints , c ) ) concrete_data . append ( c ) data = concrete_data else : data = [ Operators . CHR ( Operators . EXTRACT ( expr , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] logger . debug ( f"Writing back {hr_size(size // 8)} to {hex(where)}: {data}" ) self . _emu . mem_write ( where , b'' . join ( b . encode ( 'utf-8' ) if type ( b ) is str else b for b in data ) )
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
11203	def picknthweekday ( year , month , dayofweek , hour , minute , whichweek ) : first = datetime . datetime ( year , month , 1 , hour , minute ) weekdayone = first . replace ( day = ( ( dayofweek - first . isoweekday ( ) ) % 7 ) + 1 ) wd = weekdayone + ( ( whichweek - 1 ) * ONEWEEK ) if ( wd . month != month ) : wd -= ONEWEEK return wd
5441	def rewrite_uris ( self , raw_uri , file_provider ) : if file_provider == job_model . P_GCS : normalized , docker_path = _gcs_uri_rewriter ( raw_uri ) elif file_provider == job_model . P_LOCAL : normalized , docker_path = _local_uri_rewriter ( raw_uri ) else : raise ValueError ( 'File provider not supported: %r' % file_provider ) return normalized , os . path . join ( self . _relative_path , docker_path )
6795	def syncdb ( self , site = None , all = 0 , database = None , ignore_errors = 1 ) : r = self . local_renderer ignore_errors = int ( ignore_errors ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) use_run_syncdb = self . version_tuple >= ( 1 , 9 , 0 ) r . env . db_syncdb_all_flag = '--all' if int ( all ) else '' r . env . db_syncdb_database = '' if database : r . env . db_syncdb_database = ' --database=%s' % database if self . is_local : r . env . project_dir = r . env . local_project_dir site = site or self . genv . SITE for _site , site_data in r . iter_unique_databases ( site = site ) : r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : if post_south : if use_run_syncdb : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --run-syncdb --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} syncdb --noinput {db_syncdb_all_flag} {db_syncdb_database}' )
8905	def create_access_token ( self , valid_in_hours = 1 , data = None ) : data = data or { } token = AccessToken ( token = self . generate ( ) , expires_at = expires_at ( hours = valid_in_hours ) , data = data ) return token
10738	def grid_evaluation ( X , Y , f , vectorized = True ) : XX = np . reshape ( np . concatenate ( [ X [ ... , None ] , Y [ ... , None ] ] , axis = 2 ) , ( X . size , 2 ) , order = 'C' ) if vectorized : ZZ = f ( XX ) else : ZZ = np . array ( [ f ( x ) for x in XX ] ) return np . reshape ( ZZ , X . shape , order = 'C' )
9255	def generate_log_for_tag ( self , pull_requests , issues , newer_tag , older_tag_name ) : newer_tag_link , newer_tag_name , newer_tag_time = self . detect_link_tag_time ( newer_tag ) github_site = "https://github.com" or self . options . github_endpoint project_url = "{0}/{1}/{2}" . format ( github_site , self . options . user , self . options . project ) log = self . generate_header ( newer_tag_name , newer_tag_link , newer_tag_time , older_tag_name , project_url ) if self . options . issues : log += self . issues_to_log ( issues , pull_requests ) if self . options . include_pull_request : log += self . generate_sub_section ( pull_requests , self . options . merge_prefix ) return log
5917	def check_output ( self , make_ndx_output , message = None , err = None ) : if message is None : message = "" else : message = '\n' + message def format ( output , w = 60 ) : hrule = "====[ GromacsError (diagnostic output) ]" . ljust ( w , "=" ) return hrule + '\n' + str ( output ) + hrule rc = True if self . _is_empty_group ( make_ndx_output ) : warnings . warn ( "Selection produced empty group.{message!s}" . format ( ** vars ( ) ) , category = GromacsValueWarning ) rc = False if self . _has_syntax_error ( make_ndx_output ) : rc = False out_formatted = format ( make_ndx_output ) raise GromacsError ( "make_ndx encountered a Syntax Error, " "%(message)s\noutput:\n%(out_formatted)s" % vars ( ) ) if make_ndx_output . strip ( ) == "" : rc = False out_formatted = format ( err ) raise GromacsError ( "make_ndx produced no output, " "%(message)s\nerror output:\n%(out_formatted)s" % vars ( ) ) return rc
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
2136	def disassociate_notification_template ( self , workflow , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , workflow , notification_template )
6447	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) for suffix_len in range ( 11 , 0 , - 1 ) : ending = word [ - suffix_len : ] if ( ending in self . _suffix and len ( word ) - suffix_len >= 2 and ( self . _suffix [ ending ] is None or self . _suffix [ ending ] ( word , suffix_len ) ) ) : word = word [ : - suffix_len ] break if word [ - 2 : ] in { 'bb' , 'dd' , 'gg' , 'll' , 'mm' , 'nn' , 'pp' , 'rr' , 'ss' , 'tt' , } : word = word [ : - 1 ] for ending , replacement in self . _recode : if word . endswith ( ending ) : if callable ( replacement ) : word = replacement ( word ) else : word = word [ : - len ( ending ) ] + replacement return word
13131	def parse_domain_computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = HostSearch ( ) count = 0 entry_count = 0 print_notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry_count += 1 parsed = parse_single_computer ( system ) if parsed . ip : try : host = hs . id_to_object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns_hostname ) if parsed . os : host . os = parsed . os host . domain_controller = parsed . dc host . add_tag ( 'domaindump' ) host . save ( ) count += 1 except ValueError : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry_count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
2638	def submit ( self , command , blocksize , tasks_per_node , job_name = "parsl.auto" ) : wrapped_cmd = self . launcher ( command , tasks_per_node , 1 ) instance , name = self . create_instance ( command = wrapped_cmd ) self . provisioned_blocks += 1 self . resources [ name ] = { "job_id" : name , "status" : translate_table [ instance [ 'status' ] ] } return name
7491	def compute_tree_stats ( self , ipyclient ) : names = self . samples if self . params . nboots : fulltre = ete3 . Tree ( self . trees . tree , format = 0 ) fulltre . unroot ( ) with open ( self . trees . boots , 'r' ) as inboots : bb = [ ete3 . Tree ( i . strip ( ) , format = 0 ) for i in inboots . readlines ( ) ] wboots = [ fulltre ] + bb [ - self . params . nboots : ] wctre , wcounts = consensus_tree ( wboots , names = names ) self . trees . cons = os . path . join ( self . dirs , self . name + ".cons" ) with open ( self . trees . cons , 'w' ) as ocons : ocons . write ( wctre . write ( format = 0 ) ) else : wctre = ete3 . Tree ( self . trees . tree , format = 0 ) wctre . unroot ( ) self . trees . nhx = os . path . join ( self . dirs , self . name + ".nhx" ) with open ( self . files . stats , 'w' ) as ostats : if self . params . nboots : ostats . write ( "## splits observed in {} trees\n" . format ( len ( wboots ) ) ) for i , j in enumerate ( self . samples ) : ostats . write ( "{:<3} {}\n" . format ( i , j ) ) ostats . write ( "\n" ) for split , freq in wcounts : if split . count ( '1' ) > 1 : ostats . write ( "{} {:.2f}\n" . format ( split , round ( freq , 2 ) ) ) ostats . write ( "\n" ) lbview = ipyclient . load_balanced_view ( ) qtots = { } qsamp = { } tots = sum ( 1 for i in wctre . iter_leaves ( ) ) totn = set ( wctre . get_leaf_names ( ) ) for node in wctre . traverse ( ) : qtots [ node ] = lbview . apply ( _get_total , * ( tots , node ) ) qsamp [ node ] = lbview . apply ( _get_sampled , * ( self , totn , node ) ) ipyclient . wait ( ) for node in wctre . traverse ( ) : total = qtots [ node ] . result ( ) sampled = qsamp [ node ] . result ( ) node . add_feature ( "quartets_total" , total ) node . add_feature ( "quartets_sampled" , sampled ) features = [ "quartets_total" , "quartets_sampled" ] with open ( self . trees . nhx , 'w' ) as outtre : outtre . write ( wctre . write ( format = 0 , features = features ) )
9457	def sound_touch ( self , call_params ) : path = '/' + self . api_version + '/SoundTouch/' method = 'POST' return self . request ( path , method , call_params )
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
8939	def _zipped ( self , docs_base ) : with pushd ( docs_base ) : with tempfile . NamedTemporaryFile ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip_name = shutil . make_archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} MiB from '{}' to '{}'..." . format ( os . path . getsize ( zip_name ) / 1024.0 , zip_name , self . target ) ) with io . open ( zip_name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
6448	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) self . _compressor . compress ( src ) src_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar ) tar_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( src + tar ) concat_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar + src ) concat_comp2 = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
13290	def _iter_filepaths_with_extension ( extname , root_dir = '.' ) : if not extname . startswith ( '.' ) : extname = '.' + extname root_dir = os . path . abspath ( root_dir ) for dirname , sub_dirnames , filenames in os . walk ( root_dir ) : for filename in filenames : if os . path . splitext ( filename ) [ - 1 ] == extname : full_filename = os . path . join ( dirname , filename ) rel_filepath = os . path . relpath ( full_filename , start = root_dir ) yield rel_filepath
12435	def parse ( cls , path ) : for resource , pattern in cls . meta . patterns : match = re . match ( pattern , path ) if match is not None : return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] return None if not cls . meta . patterns else False
6189	def get_bromo_fnames_da ( d_em_kHz , d_bg_kHz , a_em_kHz , a_bg_kHz , ID = '1+2+3+4+5+6' , t_tot = '480' , num_p = '30' , pM = '64' , t_step = 0.5e-6 , D = 1.2e-11 , dir_ = '' ) : clk_p = t_step / 32. E_sim = 1. * a_em_kHz / ( a_em_kHz + d_em_kHz ) FRET_val = 100. * E_sim print ( "Simulated FRET value: %.1f%%" % FRET_val ) d_em_kHz_str = "%04d" % d_em_kHz a_em_kHz_str = "%04d" % a_em_kHz d_bg_kHz_str = "%04.1f" % d_bg_kHz a_bg_kHz_str = "%04.1f" % a_bg_kHz print ( "D: EM %s BG %s " % ( d_em_kHz_str , d_bg_kHz_str ) ) print ( "A: EM %s BG %s " % ( a_em_kHz_str , a_bg_kHz_str ) ) fname_d = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = d_em_kHz_str , bg = d_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) fname_a = ( 'ph_times_{t_tot}s_D{D}_{np}P_{pM}pM_' 'step{ts_us}us_ID{ID}_EM{em}kHz_BG{bg}kHz.npy' ) . format ( em = a_em_kHz_str , bg = a_bg_kHz_str , t_tot = t_tot , pM = pM , np = num_p , ID = ID , ts_us = t_step * 1e6 , D = D ) print ( fname_d ) print ( fname_a ) name = ( 'BroSim_E{:.1f}_dBG{:.1f}k_aBG{:.1f}k_' 'dEM{:.0f}k' ) . format ( FRET_val , d_bg_kHz , a_bg_kHz , d_em_kHz ) return dir_ + fname_d , dir_ + fname_a , name , clk_p , E_sim
11604	def check_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if isinstance ( start , int ) or isinstance ( end , int ) : if isinstance ( start , int ) and not ( 0 <= start < length ) : continue elif isinstance ( start , int ) and isinstance ( end , int ) and not ( start <= end ) : continue elif start is None and end == 0 : continue result . append ( ( start , end ) ) return result
10692	def rgb_to_yiq ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 y = ( 0.299 * r ) + ( 0.587 * g ) + ( 0.114 * b ) i = ( 0.596 * r ) - ( 0.275 * g ) - ( 0.321 * b ) q = ( 0.212 * r ) - ( 0.528 * g ) + ( 0.311 * b ) return round ( y , 3 ) , round ( i , 3 ) , round ( q , 3 )
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
10118	def regular_polygon ( cls , center , radius , n_vertices , start_angle = 0 , ** kwargs ) : angles = ( np . arange ( n_vertices ) * 2 * np . pi / n_vertices ) + start_angle return cls ( center + radius * np . array ( [ np . cos ( angles ) , np . sin ( angles ) ] ) . T , ** kwargs )
5886	def close ( self ) : if self . fetcher is not None : self . shutdown_network ( ) self . finalizer . atexit = False
11928	def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) if exists ( config . filepath ) : paths . append ( config . filepath ) files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
5423	def _wait_after ( provider , job_ids , poll_interval , stop_on_failure ) : job_ids_to_check = { j for j in job_ids if j != dsub_util . NO_JOB } error_messages = [ ] while job_ids_to_check and ( not error_messages or not stop_on_failure ) : print ( 'Waiting for: %s.' % ( ', ' . join ( job_ids_to_check ) ) ) jobs_left = _wait_for_any_job ( provider , job_ids_to_check , poll_interval ) jobs_completed = job_ids_to_check . difference ( jobs_left ) tasks_completed = provider . lookup_job_tasks ( { '*' } , job_ids = jobs_completed ) dominant_job_tasks = _dominant_task_for_jobs ( tasks_completed ) if len ( dominant_job_tasks ) != len ( jobs_completed ) : jobs_found = dsub_util . tasks_to_job_ids ( dominant_job_tasks ) jobs_not_found = jobs_completed . difference ( jobs_found ) for j in jobs_not_found : error = '%s: not found' % j print_error ( ' %s' % error ) error_messages += [ error ] for t in dominant_job_tasks : job_id = t . get_field ( 'job-id' ) status = t . get_field ( 'task-status' ) print ( ' %s: %s' % ( str ( job_id ) , str ( status ) ) ) if status in [ 'FAILURE' , 'CANCELED' ] : error_messages += [ provider . get_tasks_completion_messages ( [ t ] ) ] job_ids_to_check = jobs_left return error_messages
7952	def wait_for_writability ( self ) : with self . lock : while True : if self . _state in ( "closing" , "closed" , "aborted" ) : return False if self . _socket and bool ( self . _write_queue ) : return True self . _write_queue_cond . wait ( ) return False
696	def getModelIDFromParamsHash ( self , paramsHash ) : entryIdx = self . _paramsHashToIndexes . get ( paramsHash , None ) if entryIdx is not None : return self . _allResults [ entryIdx ] [ 'modelID' ] else : return None
9922	def create ( self , validated_data ) : email = validated_data . pop ( "email" ) password = validated_data . pop ( "password" ) user = get_user_model ( ) ( ** validated_data ) user . set_password ( password ) user . email = email email_query = models . EmailAddress . objects . filter ( email = email ) if email_query . exists ( ) : existing_email = email_query . get ( ) existing_email . send_duplicate_notification ( ) else : user . save ( ) email_instance = models . EmailAddress . objects . create ( email = email , user = user ) email_instance . send_confirmation ( ) signals . user_registered . send ( sender = self . __class__ , user = user ) return user
5477	def parse_rfc3339_utc_string ( rfc3339_utc_string ) : m = re . match ( r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2}).?(\d*)Z' , rfc3339_utc_string ) if not m : return None groups = m . groups ( ) if len ( groups [ 6 ] ) not in ( 0 , 3 , 6 , 9 ) : return None g = [ int ( val ) for val in groups [ : 6 ] ] fraction = groups [ 6 ] if not fraction : micros = 0 elif len ( fraction ) == 3 : micros = int ( fraction ) * 1000 elif len ( fraction ) == 6 : micros = int ( fraction ) elif len ( fraction ) == 9 : micros = int ( round ( int ( fraction ) / 1000 ) ) else : assert False , 'Fraction length not 0, 6, or 9: {}' . len ( fraction ) try : return datetime ( g [ 0 ] , g [ 1 ] , g [ 2 ] , g [ 3 ] , g [ 4 ] , g [ 5 ] , micros , tzinfo = pytz . utc ) except ValueError as e : assert False , 'Could not parse RFC3339 datestring: {} exception: {}' . format ( rfc3339_utc_string , e )
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
12579	def mask_and_flatten ( self ) : self . _check_for_mask ( ) return self . get_data ( smoothed = True , masked = True , safe_copy = False ) [ self . get_mask_indices ( ) ] , self . get_mask_indices ( ) , self . mask . shape
2612	def deserialize_object ( buffers , g = None ) : bufs = list ( buffers ) pobj = buffer_to_bytes_py2 ( bufs . pop ( 0 ) ) canned = pickle . loads ( pobj ) if istype ( canned , sequence_types ) and len ( canned ) < MAX_ITEMS : for c in canned : _restore_buffers ( c , bufs ) newobj = uncan_sequence ( canned , g ) elif istype ( canned , dict ) and len ( canned ) < MAX_ITEMS : newobj = { } for k in sorted ( canned ) : c = canned [ k ] _restore_buffers ( c , bufs ) newobj [ k ] = uncan ( c , g ) else : _restore_buffers ( canned , bufs ) newobj = uncan ( canned , g ) return newobj , bufs
2995	def _getJsonIEXCloud ( url , token = '' , version = 'beta' ) : url = _URL_PREFIX2 . format ( version = version ) + url resp = requests . get ( urlparse ( url ) . geturl ( ) , proxies = _PYEX_PROXIES , params = { 'token' : token } ) if resp . status_code == 200 : return resp . json ( ) raise PyEXception ( 'Response %d - ' % resp . status_code , resp . text )
1052	def print_stack ( f = None , limit = None , file = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back print_list ( extract_stack ( f , limit ) , file )
9314	def _format_datetime ( dttm ) : if dttm . tzinfo is None or dttm . tzinfo . utcoffset ( dttm ) is None : zoned = pytz . utc . localize ( dttm ) else : zoned = dttm . astimezone ( pytz . utc ) ts = zoned . strftime ( "%Y-%m-%dT%H:%M:%S" ) ms = zoned . strftime ( "%f" ) precision = getattr ( dttm , "precision" , None ) if precision == "second" : pass elif precision == "millisecond" : ts = ts + "." + ms [ : 3 ] elif zoned . microsecond > 0 : ts = ts + "." + ms . rstrip ( "0" ) return ts + "Z"
1981	def sys_transmit ( self , cpu , fd , buf , count , tx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( tx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_transmit ( cpu , fd , buf , count , tx_bytes )
13176	def get_observations ( self ) : if self . empty : return [ ] rows = list ( self . tbody ) observations = [ ] for row_observation , row_details in zip ( rows [ : : 2 ] , rows [ 1 : : 2 ] ) : data = { } cells = OBSERVATION_XPATH ( row_observation ) data [ 'name' ] = _clean_cell ( cells [ 0 ] ) data [ 'date' ] = _clean_cell ( cells [ 1 ] ) data [ 'magnitude' ] = _clean_cell ( cells [ 3 ] ) data [ 'obscode' ] = _clean_cell ( cells [ 6 ] ) cells = DETAILS_XPATH ( row_details ) data [ 'comp1' ] = _clean_cell ( cells [ 0 ] ) data [ 'chart' ] = _clean_cell ( cells [ 3 ] ) . replace ( 'None' , '' ) data [ 'comment_code' ] = _clean_cell ( cells [ 4 ] ) data [ 'notes' ] = _clean_cell ( cells [ 5 ] ) observations . append ( data ) return observations
10027	def delete_unused_versions ( self , versions_to_keep = 10 ) : environments = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) environments = environments [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] versions_in_use = [ ] for env in environments : versions_in_use . append ( env [ 'VersionLabel' ] ) versions = self . ebs . describe_application_versions ( application_name = self . app_name ) versions = versions [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ] versions = sorted ( versions , reverse = True , key = functools . cmp_to_key ( lambda x , y : ( x [ 'DateCreated' ] > y [ 'DateCreated' ] ) - ( x [ 'DateCreated' ] < y [ 'DateCreated' ] ) ) ) for version in versions [ versions_to_keep : ] : if version [ 'VersionLabel' ] in versions_in_use : out ( "Not deleting " + version [ "VersionLabel" ] + " because it is in use" ) else : out ( "Deleting unused version: " + version [ "VersionLabel" ] ) self . ebs . delete_application_version ( application_name = self . app_name , version_label = version [ 'VersionLabel' ] ) sleep ( 2 )
5147	def generate ( self ) : tar_bytes = BytesIO ( ) tar = tarfile . open ( fileobj = tar_bytes , mode = 'w' ) self . _generate_contents ( tar ) self . _process_files ( tar ) tar . close ( ) tar_bytes . seek ( 0 ) gzip_bytes = BytesIO ( ) gz = gzip . GzipFile ( fileobj = gzip_bytes , mode = 'wb' , mtime = 0 ) gz . write ( tar_bytes . getvalue ( ) ) gz . close ( ) gzip_bytes . seek ( 0 ) return gzip_bytes
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } object_migration_map = { } foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) object_migration_map [ foreign_var ] = new_var name_migration_map [ foreign_var . name ] = new_var . name migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
9838	def __gridpositions ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridpositions: no shape parameters' ) self . currentobject [ 'shape' ] = shape elif tok . equals ( 'origin' ) : origin = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) origin . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( origin ) == 0 : raise DXParseError ( 'gridpositions: no origin parameters' ) self . currentobject [ 'origin' ] = origin elif tok . equals ( 'delta' ) : d = [ ] try : while ( self . __peek ( ) . iscode ( 'INTEGER' ) or self . __peek ( ) . iscode ( 'REAL' ) ) : tok = self . __consume ( ) d . append ( tok . value ( ) ) except DXParserNoTokens : pass if len ( d ) == 0 : raise DXParseError ( 'gridpositions: missing delta parameters' ) try : self . currentobject [ 'delta' ] . append ( d ) except KeyError : self . currentobject [ 'delta' ] = [ d ] else : raise DXParseError ( 'gridpositions: ' + str ( tok ) + ' not recognized.' )
11528	def add_scalar_data ( self , token , community_id , producer_display_name , metric_name , producer_revision , submit_time , value , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time parameters [ 'value' ] = value optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' , 'submission_id' , 'submission_uuid' , 'unit' , 'reproduction_command' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'submission_id' : parameters [ 'submissionId' ] = kwargs [ key ] elif key == 'submission_uuid' : parameters [ 'submissionUuid' ] = kwargs [ key ] elif key == 'unit' : parameters [ 'unit' ] = kwargs [ key ] elif key == 'reproduction_command' : parameters [ 'reproductionCommand' ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.tracker.scalar.add' , parameters ) return response
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
206	def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) existing [ e . type ] . append ( e )
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
8761	def delete_subnet ( context , id ) : LOG . info ( "delete_subnet %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : subnet = db_api . subnet_find ( context , id = id , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) if not context . is_admin : if STRATEGY . is_provider_network ( subnet . network_id ) : if subnet . tenant_id == context . tenant_id : raise n_exc . NotAuthorized ( subnet_id = id ) else : raise n_exc . SubnetNotFound ( subnet_id = id ) _delete_subnet ( context , subnet )
11550	def setup ( self , configuration = "ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)" ) : from pymodbus3 . client . sync import ModbusSerialClient , ModbusUdpClient , ModbusTcpClient self . _client = eval ( configuration ) self . _client . connect ( )
6383	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) start = word [ 0 : 1 ] consonant_part = '' vowel_part = '' for char in word [ 1 : ] : if char != start : if char in self . _vowels : if char not in vowel_part : vowel_part += char elif char not in consonant_part : consonant_part += char return start + consonant_part + vowel_part
4328	def echo ( self , gain_in = 0.8 , gain_out = 0.9 , n_echos = 1 , delays = [ 60 ] , decays = [ 0.4 ] ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not isinstance ( n_echos , int ) or n_echos <= 0 : raise ValueError ( "n_echos must be a positive integer." ) if not isinstance ( delays , list ) : raise ValueError ( "delays must be a list" ) if len ( delays ) != n_echos : raise ValueError ( "the length of delays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 ) for p in delays ) : raise ValueError ( "the elements of delays must be numbers > 0" ) if not isinstance ( decays , list ) : raise ValueError ( "decays must be a list" ) if len ( decays ) != n_echos : raise ValueError ( "the length of decays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 or p > 1 ) for p in decays ) : raise ValueError ( "the elements of decays must be between 0 and 1" ) effect_args = [ 'echo' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) ] for i in range ( n_echos ) : effect_args . extend ( [ '{}' . format ( delays [ i ] ) , '{}' . format ( decays [ i ] ) ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'echo' ) return self
46	def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )
5664	def interpolate_shape_times ( shape_distances , shape_breaks , stop_times ) : shape_times = np . zeros ( len ( shape_distances ) ) shape_times [ : shape_breaks [ 0 ] ] = stop_times [ 0 ] for i in range ( len ( shape_breaks ) - 1 ) : cur_break = shape_breaks [ i ] cur_time = stop_times [ i ] next_break = shape_breaks [ i + 1 ] next_time = stop_times [ i + 1 ] if cur_break == next_break : shape_times [ cur_break ] = stop_times [ i ] else : cur_distances = shape_distances [ cur_break : next_break + 1 ] norm_distances = ( ( np . array ( cur_distances ) - float ( cur_distances [ 0 ] ) ) / float ( cur_distances [ - 1 ] - cur_distances [ 0 ] ) ) times = ( 1. - norm_distances ) * cur_time + norm_distances * next_time shape_times [ cur_break : next_break ] = times [ : - 1 ] shape_times [ shape_breaks [ - 1 ] : ] = stop_times [ - 1 ] return list ( shape_times )
9795	def group ( ctx , project , group ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'group' ] = group
9040	def add_instruction ( self , specification ) : instruction = self . as_instruction ( specification ) self . _type_to_instruction [ instruction . type ] = instruction
8736	def construct_datetime ( cls , * args , ** kwargs ) : if len ( args ) == 1 : arg = args [ 0 ] method = cls . __get_dt_constructor ( type ( arg ) . __module__ , type ( arg ) . __name__ , ) result = method ( arg ) try : result = result . replace ( tzinfo = kwargs . pop ( 'tzinfo' ) ) except KeyError : pass if kwargs : first_key = kwargs . keys ( ) [ 0 ] tmpl = ( "{first_key} is an invalid keyword " "argument for this function." ) raise TypeError ( tmpl . format ( ** locals ( ) ) ) else : result = datetime . datetime ( * args , ** kwargs ) return result
13164	def format_value ( value ) : value_id = id ( value ) if value_id in recursion_breaker . processed : return u'<recursion>' recursion_breaker . processed . add ( value_id ) try : if isinstance ( value , six . binary_type ) : return u"'{0}'" . format ( value . decode ( 'utf-8' ) ) elif isinstance ( value , six . text_type ) : return u"u'{0}'" . format ( value ) elif isinstance ( value , ( list , tuple ) ) : values = list ( map ( format_value , value ) ) result = serialize_list ( u'[' , values , delimiter = u',' ) + u']' return force_unicode ( result ) elif isinstance ( value , dict ) : items = six . iteritems ( value ) items = ( tuple ( map ( format_value , item ) ) for item in items ) items = list ( items ) items . sort ( ) items = [ serialize_text ( u'{0}: ' . format ( key ) , item_value ) for key , item_value in items ] result = serialize_list ( u'{' , items , delimiter = u',' ) + u'}' return force_unicode ( result ) return force_unicode ( repr ( value ) ) finally : recursion_breaker . processed . remove ( value_id )
7343	def clone_with_updates ( self , ** kwargs ) : fields_dict = self . to_dict ( ) fields_dict . update ( kwargs ) return BindingPrediction ( ** fields_dict )
2743	def load ( self ) : identifier = None if self . id : identifier = self . id elif self . fingerprint is not None : identifier = self . fingerprint data = self . get_data ( "account/keys/%s" % identifier , type = GET ) ssh_key = data [ 'ssh_key' ] for attr in ssh_key . keys ( ) : setattr ( self , attr , ssh_key [ attr ] ) self . id = ssh_key [ 'id' ]
11272	def register_default_types ( ) : register_type ( type , pipe . map ) register_type ( types . FunctionType , pipe . map ) register_type ( types . MethodType , pipe . map ) register_type ( tuple , seq ) register_type ( list , seq ) register_type ( types . GeneratorType , seq ) register_type ( string_type , sh ) register_type ( unicode_type , sh ) register_type ( file_type , fileobj ) if is_py3 : register_type ( range , seq ) register_type ( map , seq )
2273	def _win32_read_junction ( path ) : if not jwfs . is_reparse_point ( path ) : raise ValueError ( 'not a junction' ) handle = jwfs . api . CreateFile ( path , 0 , 0 , None , jwfs . api . OPEN_EXISTING , jwfs . api . FILE_FLAG_OPEN_REPARSE_POINT | jwfs . api . FILE_FLAG_BACKUP_SEMANTICS , None ) if handle == jwfs . api . INVALID_HANDLE_VALUE : raise WindowsError ( ) res = jwfs . reparse . DeviceIoControl ( handle , jwfs . api . FSCTL_GET_REPARSE_POINT , None , 10240 ) bytes = jwfs . create_string_buffer ( res ) p_rdb = jwfs . cast ( bytes , jwfs . POINTER ( jwfs . api . REPARSE_DATA_BUFFER ) ) rdb = p_rdb . contents if rdb . tag not in [ 2684354563 , jwfs . api . IO_REPARSE_TAG_SYMLINK ] : raise RuntimeError ( "Expected <2684354563 or 2684354572>, but got %d" % rdb . tag ) jwfs . handle_nonzero_success ( jwfs . api . CloseHandle ( handle ) ) subname = rdb . get_substitute_name ( ) if subname . startswith ( '?\\' ) : subname = subname [ 2 : ] return subname
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8166	def reload_functions ( self ) : with LiveExecution . lock : if self . edited_source : tree = ast . parse ( self . edited_source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . FunctionDef ) ] : self . ns [ f . name ] . __code__ = meta . decompiler . compile_func ( f , self . filename , self . ns ) . __code__
4233	def autodetect_url ( ) : for url in [ "http://routerlogin.net:5000" , "https://routerlogin.net" , "http://routerlogin.net" ] : try : r = requests . get ( url + "/soap/server_sa/" , headers = _get_soap_headers ( "Test:1" , "test" ) , verify = False ) if r . status_code == 200 : return url except requests . exceptions . RequestException : pass return None
3556	def power_off ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_off . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 0 ) if not self . _powered_off . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power off!' )
12644	def cert_info ( ) : sec_type = security_type ( ) if sec_type == 'pem' : return get_config_value ( 'pem_path' , fallback = None ) if sec_type == 'cert' : cert_path = get_config_value ( 'cert_path' , fallback = None ) key_path = get_config_value ( 'key_path' , fallback = None ) return cert_path , key_path return None
4769	def is_instance_of ( self , some_class ) : try : if not isinstance ( self . val , some_class ) : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some_class . __name__ ) ) except TypeError : raise TypeError ( 'given arg must be a class' ) return self
11452	def _attach_fulltext ( self , rec , doi ) : url = os . path . join ( self . url_prefix , doi ) record_add_field ( rec , 'FFT' , subfields = [ ( 'a' , url ) , ( 't' , 'INSPIRE-PUBLIC' ) , ( 'd' , 'Fulltext' ) ] )
6823	def maint_up ( self ) : r = self . local_renderer fn = self . render_to_file ( r . env . maintenance_template , extra = { 'current_hostname' : self . current_hostname } ) r . put ( local_path = fn , remote_path = r . env . maintenance_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {maintenance_path}' )
8154	def create ( self , name , overwrite = True ) : self . _name = name . rstrip ( ".db" ) from os import unlink if overwrite : try : unlink ( self . _name + ".db" ) except : pass self . _con = sqlite . connect ( self . _name + ".db" ) self . _cur = self . _con . cursor ( )
12215	def get_frame_locals ( stepback = 0 ) : with Frame ( stepback = stepback ) as frame : locals_dict = frame . f_locals return locals_dict
8512	def _create_kernel ( self ) : kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , ** params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , ** params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
4523	def color_scale ( color , level ) : return tuple ( [ int ( i * level ) >> 8 for i in list ( color ) ] )
13204	def _parse_documentclass ( self ) : command = LatexCommand ( 'documentclass' , { 'name' : 'options' , 'required' : False , 'bracket' : '[' } , { 'name' : 'class_name' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no documentclass' ) self . _document_options = [ ] try : content = parsed [ 'options' ] self . _document_options = [ opt . strip ( ) for opt in content . split ( ',' ) ] except KeyError : self . _logger . warning ( 'lsstdoc has no documentclass options' ) self . _document_options = [ ]
13140	def read ( self ) : if not self . __content__ : self . __retriever__ = self . __resolver__ . resolve ( self . uri ) self . __content__ , self . __mimetype__ = self . __retriever__ . read ( self . uri ) return self . __content__
3272	def _init ( self ) : self . provider . _count_get_resource_inst_init += 1 tableName , primKey = self . provider . _split_path ( self . path ) display_type = "Unknown" displayTypeComment = "" contentType = "text/html" if tableName is None : display_type = "Database" elif primKey is None : display_type = "Database Table" else : contentType = "text/csv" if primKey == "_ENTIRE_CONTENTS" : display_type = "Database Table Contents" displayTypeComment = "CSV Representation of Table Contents" else : display_type = "Database Record" displayTypeComment = "Attributes available as properties" is_collection = primKey is None self . _cache = { "content_length" : None , "contentType" : contentType , "created" : time . time ( ) , "display_name" : self . name , "etag" : hashlib . md5 ( ) . update ( self . path ) . hexdigest ( ) , "modified" : None , "support_ranges" : False , "display_info" : { "type" : display_type , "typeComment" : displayTypeComment } , } if not is_collection : self . _cache [ "modified" ] = time . time ( ) _logger . debug ( "- % self . provider . _count_initConnection )
8288	def get_child_by_name ( parent , name ) : def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
3633	def cardInfo ( self , resource_id ) : base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
7393	def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
7679	def event ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , ** kwargs )
8128	def search_news ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_NEWS return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
264	def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : positions = get_percent_alloc ( positions ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
9847	def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
271	def estimate_intraday ( returns , positions , transactions , EOD_hour = 23 ) : txn_val = transactions . copy ( ) txn_val . index . names = [ 'date' ] txn_val [ 'value' ] = txn_val . amount * txn_val . price txn_val = txn_val . reset_index ( ) . pivot_table ( index = 'date' , values = 'value' , columns = 'symbol' ) . replace ( np . nan , 0 ) txn_val [ 'date' ] = txn_val . index . date txn_val = txn_val . groupby ( 'date' ) . cumsum ( ) txn_val [ 'exposure' ] = txn_val . abs ( ) . sum ( axis = 1 ) condition = ( txn_val [ 'exposure' ] == txn_val . groupby ( pd . TimeGrouper ( '24H' ) ) [ 'exposure' ] . transform ( max ) ) txn_val = txn_val [ condition ] . drop ( 'exposure' , axis = 1 ) txn_val [ 'cash' ] = - txn_val . sum ( axis = 1 ) positions_shifted = positions . copy ( ) . shift ( 1 ) . fillna ( 0 ) starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns [ 0 ] ) positions_shifted . cash [ 0 ] = starting_capital txn_val . index = txn_val . index . normalize ( ) corrected_positions = positions_shifted . add ( txn_val , fill_value = 0 ) corrected_positions . index . name = 'period_close' corrected_positions . columns . name = 'sid' return corrected_positions
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
12413	def serialize ( self , data , format = None ) : return self . _resource . serialize ( data , response = self , format = format )
2665	def readinto ( self , buf , ** kwargs ) : self . i2c . readfrom_into ( self . device_address , buf , ** kwargs ) if self . _debug : print ( "i2c_device.readinto:" , [ hex ( i ) for i in buf ] )
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
7954	def starttls ( self , ** kwargs ) : with self . lock : self . event ( TLSConnectingEvent ( ) ) self . _write_queue . append ( StartTLS ( ** kwargs ) ) self . _write_queue_cond . notify ( )
7302	def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
12530	def open_volume_file ( filepath ) : if not op . exists ( filepath ) : raise IOError ( 'Could not find file {}.' . format ( filepath ) ) def open_nifti_file ( filepath ) : return NiftiImage ( filepath ) def open_mhd_file ( filepath ) : return MedicalImage ( filepath ) vol_data , hdr_data = load_raw_data_with_mhd ( filepath ) return vol_data , hdr_data def open_mha_file ( filepath ) : raise NotImplementedError ( 'This function has not been implemented yet.' ) def _load_file ( filepath , loader ) : return loader ( filepath ) filext_loader = { 'nii' : open_nifti_file , 'mhd' : open_mhd_file , 'mha' : open_mha_file , } ext = get_extension ( filepath ) loader = None for e in filext_loader : if ext in e : loader = filext_loader [ e ] if loader is None : raise ValueError ( 'Could not find a loader for file {}.' . format ( filepath ) ) return _load_file ( filepath , loader )
6157	def FIR_header ( fname_out , h ) : M = len ( h ) N = 3 f = open ( fname_out , 'wt' ) f . write ( '//define a FIR coefficient Array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef M_FIR\n' ) f . write ( '#define M_FIR %d\n' % M ) f . write ( '#endif\n' ) f . write ( '/************************************************************************/\n' ) f . write ( '/* FIR Filter Coefficients */\n' ) f . write ( 'float32_t h_FIR[M_FIR] = {' ) kk = 0 for k in range ( M ) : if ( kk < N - 1 ) and ( k < M - 1 ) : f . write ( '%15.12f,' % h [ k ] ) kk += 1 elif ( kk == N - 1 ) & ( k < M - 1 ) : f . write ( '%15.12f,\n' % h [ k ] ) if k < M : f . write ( ' ' ) kk = 0 else : f . write ( '%15.12f' % h [ k ] ) f . write ( '};\n' ) f . write ( '/************************************************************************/\n' ) f . close ( )
9206	def get_prefix ( multicodec ) : try : prefix = varint . encode ( NAME_TABLE [ multicodec ] ) except KeyError : raise ValueError ( '{} multicodec is not supported.' . format ( multicodec ) ) return prefix
1182	def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
3684	def solve ( self ) : self . check_sufficient_inputs ( ) if self . V : if self . P : self . T = self . solve_T ( self . P , self . V ) self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) self . P = R * self . T / ( self . V - self . b ) - self . a_alpha / ( self . V * self . V + self . delta * self . V + self . epsilon ) Vs = [ self . V , 1j , 1j ] else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) Vs = self . volume_solutions ( self . T , self . P , self . b , self . delta , self . epsilon , self . a_alpha ) self . set_from_PT ( Vs )
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( ** params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
13381	def env_to_dict ( env , pathsep = os . pathsep ) : out_dict = { } for k , v in env . iteritems ( ) : if pathsep in v : out_dict [ k ] = v . split ( pathsep ) else : out_dict [ k ] = v return out_dict
4956	def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
12043	def algo_exp ( x , m , t , b ) : return m * np . exp ( - t * x ) + b
6443	def _cond_n ( self , word , suffix_len ) : if len ( word ) - suffix_len >= 3 : if word [ - suffix_len - 3 ] == 's' : if len ( word ) - suffix_len >= 4 : return True else : return True return False
12918	def delete ( self ) : if len ( self ) == 0 : return 0 mdl = self . getModel ( ) return mdl . deleter . deleteMultiple ( self )
13171	def iter ( self , name = None ) : for c in self . _children : if name is None or c . tagname == name : yield c for gc in c . find ( name ) : yield gc
4607	def blacklist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
4497	def project ( self , project_id ) : type_ = self . guid ( project_id ) url = self . _build_url ( type_ , project_id ) if type_ in Project . _types : return Project ( self . _json ( self . _get ( url ) , 200 ) , self . session ) raise OSFException ( '{} is unrecognized type {}. Clone supports projects and registrations' . format ( project_id , type_ ) )
2853	def mpsse_read_gpio ( self ) : self . _write ( '\x81\x83' ) data = self . _poll_read ( 2 ) low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
1135	def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S_ISREG ( st . st_mode )
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , ** kwargs ) return inst
9451	def record_start ( self , call_params ) : path = '/' + self . api_version + '/RecordStart/' method = 'POST' return self . request ( path , method , call_params )
10249	def highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None , color : Optional [ str ] = None ) : color = color or NODE_HIGHLIGHT_DEFAULT_COLOR for node in nodes if nodes is not None else graph : graph . node [ node ] [ NODE_HIGHLIGHT ] = color
7605	def get_clan ( self , tag : crtag , timeout : int = None ) : url = self . api . CLAN + '/' + tag return self . _get_model ( url , FullClan , timeout = timeout )
6895	def pwd_phasebin ( phases , mags , binsize = 0.002 , minbin = 9 ) : bins = np . arange ( 0.0 , 1.0 , binsize ) binnedphaseinds = npdigitize ( phases , bins ) binnedphases , binnedmags = [ ] , [ ] for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_phases = phases [ thisbin_inds ] thisbin_mags = mags [ thisbin_inds ] if thisbin_inds . size > minbin : binnedphases . append ( npmedian ( thisbin_phases ) ) binnedmags . append ( npmedian ( thisbin_mags ) ) return np . array ( binnedphases ) , np . array ( binnedmags )
4803	def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
4960	def get_earliest_start_date_from_program ( program ) : start_dates = [ ] for course in program . get ( 'courses' , [ ] ) : for run in course . get ( 'course_runs' , [ ] ) : if run . get ( 'start' ) : start_dates . append ( parse_lms_api_datetime ( run [ 'start' ] ) ) if not start_dates : return None return min ( start_dates )
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
6030	def grid_interpolate ( func ) : @ wraps ( func ) def wrapper ( profile , grid , grid_radial_minimum = None , * args , ** kwargs ) : if hasattr ( grid , "interpolator" ) : interpolator = grid . interpolator if grid . interpolator is not None : values = func ( profile , interpolator . interp_grid , grid_radial_minimum , * args , ** kwargs ) if values . ndim == 1 : return interpolator . interpolated_values_from_values ( values = values ) elif values . ndim == 2 : y_values = interpolator . interpolated_values_from_values ( values = values [ : , 0 ] ) x_values = interpolator . interpolated_values_from_values ( values = values [ : , 1 ] ) return np . asarray ( [ y_values , x_values ] ) . T return func ( profile , grid , grid_radial_minimum , * args , ** kwargs ) return wrapper
2542	def set_file_chksum ( self , doc , chk_sum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum . Algorithm ( 'SHA1' , chk_sum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
4902	def course_modal ( context , course = None ) : if course : context . update ( { 'course_image_uri' : course . get ( 'course_image_uri' , '' ) , 'course_title' : course . get ( 'course_title' , '' ) , 'course_level_type' : course . get ( 'course_level_type' , '' ) , 'course_short_description' : course . get ( 'course_short_description' , '' ) , 'course_effort' : course . get ( 'course_effort' , '' ) , 'course_full_description' : course . get ( 'course_full_description' , '' ) , 'expected_learning_items' : course . get ( 'expected_learning_items' , [ ] ) , 'staff' : course . get ( 'staff' , [ ] ) , 'premium_modes' : course . get ( 'premium_modes' , [ ] ) , } ) return context
6916	def add_variability_to_fakelc_collection ( simbasedir , override_paramdists = None , overwrite_existingvar = False ) : infof = os . path . join ( simbasedir , 'fakelcs-info.pkl' ) with open ( infof , 'rb' ) as infd : lcinfo = pickle . load ( infd ) lclist = lcinfo [ 'lcfpath' ] varflag = lcinfo [ 'isvariable' ] vartypes = lcinfo [ 'vartype' ] vartind = 0 varinfo = { } for lc , varf , _lcind in zip ( lclist , varflag , range ( len ( lclist ) ) ) : if varf : thisvartype = vartypes [ vartind ] if ( override_paramdists and isinstance ( override_paramdists , dict ) and thisvartype in override_paramdists and isinstance ( override_paramdists [ thisvartype ] , dict ) ) : thisoverride_paramdists = override_paramdists [ thisvartype ] else : thisoverride_paramdists = None varlc = add_fakelc_variability ( lc , thisvartype , override_paramdists = thisoverride_paramdists , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } vartind = vartind + 1 else : varlc = add_fakelc_variability ( lc , None , overwrite = overwrite_existingvar ) varinfo [ varlc [ 'objectid' ] ] = { 'params' : varlc [ 'actual_varparams' ] , 'vartype' : varlc [ 'actual_vartype' ] } lcinfo [ 'varinfo' ] = varinfo tempoutf = '%s.%s' % ( infof , md5 ( npr . bytes ( 4 ) ) . hexdigest ( ) [ - 8 : ] ) with open ( tempoutf , 'wb' ) as outfd : pickle . dump ( lcinfo , outfd , pickle . HIGHEST_PROTOCOL ) if os . path . exists ( tempoutf ) : shutil . copy ( tempoutf , infof ) os . remove ( tempoutf ) else : LOGEXCEPTION ( 'could not write output light curve file to dir: %s' % os . path . dirname ( tempoutf ) ) raise return lcinfo
2031	def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get_code ( account ) self . _allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . _store ( address + i , extbytecode [ offset + i ] ) else : self . _store ( address + i , 0 )
5921	def strip_fit ( self , ** kwargs ) : kwargs . setdefault ( 'fit' , 'rot+trans' ) kw_fit = { } for k in ( 'xy' , 'fit' , 'fitgroup' , 'input' ) : if k in kwargs : kw_fit [ k ] = kwargs . pop ( k ) kwargs [ 'input' ] = kwargs . pop ( 'strip_input' , [ 'Protein' ] ) kwargs [ 'force' ] = kw_fit [ 'force' ] = kwargs . pop ( 'force' , self . force ) paths = self . strip_water ( ** kwargs ) transformer_nowater = self . nowater [ paths [ 'xtc' ] ] return transformer_nowater . fit ( ** kw_fit )
7841	def get_category ( self ) : var = self . xmlnode . prop ( "category" ) if not var : var = "?" return var . decode ( "utf-8" )
9431	def dostime_to_timetuple ( dostime ) : dostime = dostime >> 16 dostime = dostime & 0xffff day = dostime & 0x1f month = ( dostime >> 5 ) & 0xf year = 1980 + ( dostime >> 9 ) second = 2 * ( dostime & 0x1f ) minute = ( dostime >> 5 ) & 0x3f hour = dostime >> 11 return ( year , month , day , hour , minute , second )
5509	def get_permissions ( self , path ) : path = pathlib . PurePosixPath ( path ) parents = filter ( lambda p : p . is_parent ( path ) , self . permissions ) perm = min ( parents , key = lambda p : len ( path . relative_to ( p . path ) . parts ) , default = Permission ( ) , ) return perm
9494	def compile ( code : list , consts : list , names : list , varnames : list , func_name : str = "<unknown, compiled>" , arg_count : int = 0 , kwarg_defaults : Tuple [ Any ] = ( ) , use_safety_wrapper : bool = True ) : varnames = tuple ( varnames ) consts = tuple ( consts ) names = tuple ( names ) code = util . flatten ( code ) if arg_count > len ( varnames ) : raise CompileError ( "arg_count > len(varnames)" ) if len ( kwarg_defaults ) > len ( varnames ) : raise CompileError ( "len(kwarg_defaults) > len(varnames)" ) bc = compile_bytecode ( code ) dis . dis ( bc ) if PY36 : pass else : if bc [ - 1 ] != tokens . RETURN_VALUE : raise CompileError ( "No default RETURN_VALUE. Add a `pyte.tokens.RETURN_VALUE` to the end of your " "bytecode if you don't need one." ) flags = 1 | 2 | 64 frame_data = inspect . stack ( ) [ 1 ] if sys . version_info [ 0 : 2 ] > ( 3 , 3 ) : stack_size = _simulate_stack ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) else : warnings . warn ( "Cannot check stack for safety." ) stack_size = 99 _optimize_warn_pass ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) obb = types . CodeType ( arg_count , 0 , len ( varnames ) , stack_size , flags , bc , consts , names , varnames , frame_data [ 1 ] , func_name , frame_data [ 2 ] , b'' , ( ) , ( ) ) f_globals = frame_data [ 0 ] . f_globals f = types . FunctionType ( obb , f_globals ) f . __name__ = func_name f . __defaults__ = kwarg_defaults if use_safety_wrapper : def __safety_wrapper ( * args , ** kwargs ) : try : return f ( * args , ** kwargs ) except SystemError as e : if 'opcode' not in ' ' . join ( e . args ) : raise msg = "Bytecode exception!" "\nFunction {} returned an invalid opcode." "\nFunction dissection:\n\n" . format ( f . __name__ ) file = io . StringIO ( ) with contextlib . redirect_stdout ( file ) : dis . dis ( f ) msg += file . getvalue ( ) raise SystemError ( msg ) from e returned_func = __safety_wrapper returned_func . wrapped = f else : returned_func = f return returned_func
9285	def close ( self ) : self . _connected = False self . buf = b'' if self . sock is not None : self . sock . close ( )
12430	def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : if self . _user : cfg += 'user {0};\n' . format ( self . _user ) cfg += 'worker_processes 1;\nerror_log {0}-errors.log;\n\pid {1}_ nginx.pid;\n\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' cfg += '\t\t}\n' cfg += '\t}\n' if not self . _shared_hosting : cfg += '}\n' f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
13263	def get_parameters ( self ) : if self . plugin_class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . KEYWORD_ONLY , parameter . POSITIONAL_OR_KEYWORD ] : raise RuntimeError ( "Task {} contains an unsupported {} parameter" . format ( parameter , parameter . kind ) ) yield parameter else : var_keyword_seen = set ( ) for cls in inspect . getmro ( self . plugin_class ) : if issubclass ( cls , BasePlugin ) and hasattr ( cls , self . func . __name__ ) : func = getattr ( cls , self . func . __name__ ) logger . debug ( "Found method %s from class %s" , func , cls ) var_keyword_found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : continue if parameter . kind == inspect . Parameter . VAR_KEYWORD : var_keyword_found = True continue if parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . VAR_POSITIONAL ] : raise RuntimeError ( "Task {} contains an unsupported parameter \"{}\"" . format ( func , parameter ) ) if not parameter . name in var_keyword_seen : var_keyword_seen . add ( parameter . name ) logger . debug ( "Found parameter %s (%s)" , parameter , parameter . kind ) yield parameter if not var_keyword_found : break
265	def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : alphas = a . gradient ( image , target ) * mask if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
3553	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
3222	def _gcp_client ( project , mod_name , pkg_name , key_file = None , http_auth = None , user_agent = None ) : client = None if http_auth is None : http_auth = _googleauth ( key_file = key_file , user_agent = user_agent ) try : google_module = importlib . import_module ( '.' + mod_name , package = pkg_name ) client = google_module . Client ( use_GAX = USE_GAX , project = project , http = http_auth ) except ImportError as ie : import_err = 'Unable to import %s.%s' % ( pkg_name , mod_name ) raise ImportError ( import_err ) except TypeError : client = google_module . Client ( project = project , http = http_auth ) if user_agent and hasattr ( client , 'user_agent' ) : client . user_agent = user_agent return client
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
3724	def load_group_assignments_DDBST ( ) : if DDBST_UNIFAC_assignments : return None with open ( os . path . join ( folder , 'DDBST UNIFAC assignments.tsv' ) ) as f : _group_assignments = [ DDBST_UNIFAC_assignments , DDBST_MODIFIED_UNIFAC_assignments , DDBST_PSRK_assignments ] for line in f . readlines ( ) : key , valids , original , modified , PSRK = line . split ( '\t' ) valids = [ True if i == '1' else False for i in valids . split ( ' ' ) ] for groups , storage , valid in zip ( [ original , modified , PSRK ] , _group_assignments , valids ) : if valid : groups = groups . rstrip ( ) . split ( ' ' ) d_data = { } for i in range ( int ( len ( groups ) / 2 ) ) : d_data [ int ( groups [ i * 2 ] ) ] = int ( groups [ i * 2 + 1 ] ) storage [ key ] = d_data
3077	def has_credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
1269	def _fly ( self , board , layers , things , the_plot ) : if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )
10188	def publish ( self , event_type , events ) : assert event_type in self . events current_queues . queues [ 'stats-{}' . format ( event_type ) ] . publish ( events )
10254	def get_causal_out_edges ( graph : BELGraph , nbunch : Union [ BaseEntity , Iterable [ BaseEntity ] ] , ) -> Set [ Tuple [ BaseEntity , BaseEntity ] ] : return { ( u , v ) for u , v , k , d in graph . out_edges ( nbunch , keys = True , data = True ) if is_causal_relation ( graph , u , v , k , d ) }
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
3191	def update ( self , folder_id , data ) : if 'name' not in data : raise KeyError ( 'The template folder must have a name' ) self . folder_id = folder_id return self . _mc_client . _patch ( url = self . _build_path ( folder_id ) , data = data )
2565	def async_process ( fn ) : def run ( * args , ** kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
3547	def _characteristic_changed ( self , characteristic ) : on_changed = self . _char_on_changed . get ( characteristic , None ) if on_changed is not None : on_changed ( characteristic . value ( ) . bytes ( ) . tobytes ( ) ) char = characteristic_list ( ) . get ( characteristic ) if char is not None : char . _value_read . set ( )
12504	def smooth_img ( imgs , fwhm , ** kwargs ) : if hasattr ( imgs , "__iter__" ) and not isinstance ( imgs , string_types ) : single_img = False else : single_img = True imgs = [ imgs ] ret = [ ] for img in imgs : img = check_niimg ( img ) affine = img . get_affine ( ) filtered = _smooth_array ( img . get_data ( ) , affine , fwhm = fwhm , ensure_finite = True , copy = True , ** kwargs ) ret . append ( new_img_like ( img , filtered , affine , copy_header = True ) ) if single_img : return ret [ 0 ] else : return ret
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
5973	def generate_submit_array ( templates , directories , ** kwargs ) : dirname = kwargs . setdefault ( 'dirname' , os . path . curdir ) reldirs = [ relpath ( p , start = dirname ) for p in asiterable ( directories ) ] missing = [ p for p in ( os . path . join ( dirname , subdir ) for subdir in reldirs ) if not os . path . exists ( p ) ] if len ( missing ) > 0 : logger . debug ( "template=%(template)r: dirname=%(dirname)r reldirs=%(reldirs)r" , vars ( ) ) logger . error ( "Some directories are not accessible from the array script: " "%(missing)r" , vars ( ) ) def write_script ( template ) : qsystem = detect_queuing_system ( template ) if qsystem is None or not qsystem . has_arrays ( ) : logger . warning ( "Not known how to make a job array for %(template)r; skipping..." , vars ( ) ) return None kwargs [ 'jobarray_string' ] = qsystem . array ( reldirs ) return generate_submit_scripts ( template , ** kwargs ) [ 0 ] return [ write_script ( template ) for template in config . get_templates ( templates ) ]
10145	def from_schema ( self , schema_node ) : params = [ ] for param_schema in schema_node . children : location = param_schema . name if location is 'body' : name = param_schema . __class__ . __name__ if name == 'body' : name = schema_node . __class__ . __name__ + 'Body' param = self . parameter_converter ( location , param_schema ) param [ 'name' ] = name if self . ref : param = self . _ref ( param ) params . append ( param ) elif location in ( ( 'path' , 'header' , 'headers' , 'querystring' , 'GET' ) ) : for node_schema in param_schema . children : param = self . parameter_converter ( location , node_schema ) if self . ref : param = self . _ref ( param ) params . append ( param ) return params
11706	def reproduce_sexually ( self , egg_donor , sperm_donor ) : egg_word = random . choice ( egg_donor . genome ) egg = self . generate_gamete ( egg_word ) sperm_word = random . choice ( sperm_donor . genome ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . parents = [ egg_donor . name , sperm_donor . name ] self . generation = max ( egg_donor . generation , sperm_donor . generation ) + 1 sum_ = egg_donor . divinity + sperm_donor . divinity self . divinity = int ( npchoice ( divinities , 1 , p = p_divinity [ sum_ ] ) [ 0 ] )
10037	def execute ( helper , config , args ) : out ( "Available solution stacks" ) for stack in helper . list_available_solution_stacks ( ) : out ( " " + str ( stack ) ) return 0
10566	def _check_field_value ( field_value , pattern ) : if isinstance ( field_value , list ) : return any ( re . search ( pattern , str ( value ) , re . I ) for value in field_value ) else : return re . search ( pattern , str ( field_value ) , re . I )
6031	def unmasked_blurred_image_from_psf_and_unmasked_image ( self , psf , unmasked_image_1d ) : blurred_image_1d = self . regular . convolve_array_1d_with_psf ( padded_array_1d = unmasked_image_1d , psf = psf ) return self . regular . scaled_array_2d_from_array_1d ( array_1d = blurred_image_1d )
3374	def remove_cons_vars_from_problem ( model , what ) : context = get_context ( model ) model . solver . remove ( what ) if context : context ( partial ( model . solver . add , what ) )
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
4546	def fill_circle ( setter , x0 , y0 , r , color = None ) : _draw_fast_vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) _fill_circle_helper ( setter , x0 , y0 , r , 3 , 0 , color )
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz ox , oz = - sx / 2 , - sz / 2 def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
11854	def scanner ( self , j , word ) : "For each edge expecting a word of this category here, extend the edge." for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
9124	def belanno ( keyword : str , file : TextIO ) : directory = get_data_dir ( keyword ) obo_url = f'http://purl.obolibrary.org/obo/{keyword}.obo' obo_path = os . path . join ( directory , f'{keyword}.obo' ) obo_cache_path = os . path . join ( directory , f'{keyword}.obo.pickle' ) obo_getter = make_obo_getter ( obo_url , obo_path , preparsed_path = obo_cache_path ) graph = obo_getter ( ) convert_obo_graph_to_belanno ( graph , file = file , )
5439	def _interval_to_seconds ( interval , valid_units = 'smhdw' ) : if not interval : return None try : last_char = interval [ - 1 ] if last_char == 's' and 's' in valid_units : return str ( float ( interval [ : - 1 ] ) ) + 's' elif last_char == 'm' and 'm' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 ) + 's' elif last_char == 'h' and 'h' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 ) + 's' elif last_char == 'd' and 'd' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 ) + 's' elif last_char == 'w' and 'w' in valid_units : return str ( float ( interval [ : - 1 ] ) * 60 * 60 * 24 * 7 ) + 's' else : raise ValueError ( 'Unsupported units in interval string %s: %s' % ( interval , last_char ) ) except ( ValueError , OverflowError ) as e : raise ValueError ( 'Unable to parse interval string %s: %s' % ( interval , e ) )
12941	def pprint ( self , stream = None ) : pprint . pprint ( self . asDict ( includeMeta = True , forStorage = False , strKeys = True ) , stream = stream )
9402	def _exist ( self , name ) : cmd = 'exist("%s")' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) exist = int ( resp . split ( ) [ - 1 ] ) if exist == 0 : msg = 'Value "%s" does not exist in Octave workspace' raise Oct2PyError ( msg % name ) return exist
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
11684	def _readxml ( self ) : block = re . sub ( r'<(/?)s>' , r'&lt;\1s&gt;' , self . _readblock ( ) ) try : xml = XML ( block ) except ParseError : xml = None return xml
7169	def remove_entity ( self , name ) : self . entities . remove ( name ) self . padaos . remove_entity ( name )
10285	def get_subgraph_edges ( graph : BELGraph , annotation : str , value : str , source_filter = None , target_filter = None , ) : if source_filter is None : source_filter = keep_node_permissive if target_filter is None : target_filter = keep_node_permissive for u , v , k , data in graph . edges ( keys = True , data = True ) : if not edge_has_annotation ( data , annotation ) : continue if data [ ANNOTATIONS ] [ annotation ] == value and source_filter ( graph , u ) and target_filter ( graph , v ) : yield u , v , k , data
5108	def next_event_description ( self ) : if self . _departures [ 0 ] . _time < self . _arrivals [ 0 ] . _time : return 2 elif self . _arrivals [ 0 ] . _time < infty : return 1 else : return 0
11656	def fit_transform ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , ** params ) return self . _gather_outputs ( X , X_new )
595	def _getTPClass ( temporalImp ) : if temporalImp == 'py' : return backtracking_tm . BacktrackingTM elif temporalImp == 'cpp' : return backtracking_tm_cpp . BacktrackingTMCPP elif temporalImp == 'tm_py' : return backtracking_tm_shim . TMShim elif temporalImp == 'tm_cpp' : return backtracking_tm_shim . TMCPPShim elif temporalImp == 'monitored_tm_py' : return backtracking_tm_shim . MonitoredTMShim else : raise RuntimeError ( "Invalid temporalImp '%s'. Legal values are: 'py', " "'cpp', 'tm_py', 'monitored_tm_py'" % ( temporalImp ) )
8176	def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
2345	def forward ( self , x ) : features = self . conv ( x ) . mean ( dim = 2 ) return self . dense ( features )
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
4421	async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
893	def cellsForColumn ( self , column ) : self . _validateColumn ( column ) start = self . cellsPerColumn * column end = start + self . cellsPerColumn return range ( start , end )
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
8965	def whichgen ( command , path = None , verbose = 0 , exts = None ) : matches = [ ] if path is None : using_given_path = 0 path = os . environ . get ( "PATH" , "" ) . split ( os . pathsep ) if sys . platform . startswith ( "win" ) : path . insert ( 0 , os . curdir ) else : using_given_path = 1 if sys . platform . startswith ( "win" ) : if exts is None : exts = os . environ . get ( "PATHEXT" , "" ) . split ( os . pathsep ) for ext in exts : if ext . lower ( ) == ".exe" : break else : exts = [ '.COM' , '.EXE' , '.BAT' ] elif not isinstance ( exts , list ) : raise TypeError ( "'exts' argument must be a list or None" ) else : if exts is not None : raise WhichError ( "'exts' argument is not supported on platform '%s'" % sys . platform ) exts = [ ] if os . sep in command or os . altsep and os . altsep in command : pass else : for i , dir_name in enumerate ( path ) : if sys . platform . startswith ( "win" ) and len ( dir_name ) >= 2 and dir_name [ 0 ] == '"' and dir_name [ - 1 ] == '"' : dir_name = dir_name [ 1 : - 1 ] for ext in [ '' ] + exts : abs_name = os . path . abspath ( os . path . normpath ( os . path . join ( dir_name , command + ext ) ) ) if os . path . isfile ( abs_name ) : if using_given_path : from_where = "from given path element %d" % i elif not sys . platform . startswith ( "win" ) : from_where = "from PATH element %d" % i elif i == 0 : from_where = "from current directory" else : from_where = "from PATH element %d" % ( i - 1 ) match = _cull ( ( abs_name , from_where ) , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ] match = _get_registered_executable ( command ) if match is not None : match = _cull ( match , matches , verbose ) if match : if verbose : yield match else : yield match [ 0 ]
10318	def _microcanonical_average_spanning_cluster ( has_spanning_cluster , alpha ) : r ret = dict ( ) runs = has_spanning_cluster . size k = has_spanning_cluster . sum ( dtype = np . float ) ret [ 'spanning_cluster' ] = ( ( k + 1 ) / ( runs + 2 ) ) ret [ 'spanning_cluster_ci' ] = scipy . stats . beta . ppf ( [ alpha / 2 , 1 - alpha / 2 ] , k + 1 , runs - k + 1 ) return ret
2572	def dbm_starter ( priority_msgs , resource_msgs , * args , ** kwargs ) : dbm = DatabaseManager ( * args , ** kwargs ) dbm . start ( priority_msgs , resource_msgs )
1023	def assertNoTMDiffs ( tms ) : if len ( tms ) == 1 : return if len ( tms ) > 2 : raise "Not implemented for more than 2 TMs" same = fdrutils . tmDiff2 ( tms . values ( ) , verbosity = VERBOSITY ) assert ( same ) return
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
8261	def reverse ( self ) : colors = ColorList . copy ( self ) _list . reverse ( colors ) return colors
6520	def files ( self , filters = None ) : filters = compile_masks ( filters or [ r'.*' ] ) for files in itervalues ( self . _found ) : for file_ in files : relpath = text_type ( Path ( file_ ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : yield file_
10989	def link_zscale ( st ) : psf = st . get ( 'psf' ) psf . param_dict [ 'zscale' ] = psf . param_dict [ 'psf-zscale' ] psf . params [ psf . params . index ( 'psf-zscale' ) ] = 'zscale' psf . global_zscale = True psf . param_dict . pop ( 'psf-zscale' ) st . trigger_parameter_change ( ) st . reset ( )
10409	def finalized_canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ( 'p' , 'float64' ) , ( 'alpha' , 'float64' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_std' , 'float64' ) , ( 'percolation_probability_ci' , '(2,)float64' ) , ] ) fields . extend ( [ ( 'percolation_strength_mean' , 'float64' ) , ( 'percolation_strength_std' , 'float64' ) , ( 'percolation_strength_ci' , '(2,)float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_std' , '(5,)float64' ) , ( 'moments_ci' , '(5,2)float64' ) , ] ) return _ndarray_dtype ( fields )
4539	def multi ( method ) : @ functools . wraps ( method ) def multi ( self , address = '' ) : values = flask . request . values address = urllib . parse . unquote_plus ( address ) if address and values and not address . endswith ( '.' ) : address += '.' result = { } for a in values or '' : try : if not self . project : raise ValueError ( 'No Project is currently loaded' ) ed = editor . Editor ( address + a , self . project ) result [ address + a ] = { 'value' : method ( self , ed , a ) } except : if self . project : traceback . print_exc ( ) result [ address + a ] = { 'error' : 'Could not multi addr %s' % a } return flask . jsonify ( result ) return multi
12898	def set_mute ( self , value = False ) : mute = ( yield from self . handle_set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )
1913	def locked_context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . _executor . locked_context ( '.' . join ( keys ) , default ) as policy_context : yield policy_context
3687	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r if not full : return self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 else : if quick : Tc , kappa = self . Tc , self . kappa x0 = T ** 0.5 x1 = Tc ** - 0.5 x2 = kappa * ( x0 * x1 - 1. ) - 1. x3 = self . a * kappa a_alpha = self . a * x2 * x2 da_alpha_dT = x1 * x2 * x3 / x0 d2a_alpha_dT2 = x3 * ( - 0.5 * T ** - 1.5 * x1 * x2 + 0.5 / ( T * Tc ) * kappa ) else : a_alpha = self . a * ( 1 + self . kappa * ( 1 - ( T / self . Tc ) ** 0.5 ) ) ** 2 da_alpha_dT = - self . a * self . kappa * sqrt ( T / self . Tc ) * ( self . kappa * ( - sqrt ( T / self . Tc ) + 1. ) + 1. ) / T d2a_alpha_dT2 = self . a * self . kappa * ( self . kappa / self . Tc - sqrt ( T / self . Tc ) * ( self . kappa * ( sqrt ( T / self . Tc ) - 1. ) - 1. ) / T ) / ( 2. * T ) return a_alpha , da_alpha_dT , d2a_alpha_dT2
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
11462	def update_subject_categories ( self , primary , secondary , kb ) : category_fields = record_get_field_instances ( self . record , tag = '650' , ind1 = '1' , ind2 = '7' ) record_delete_fields ( self . record , "650" ) for field in category_fields : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : new_value = self . get_config_item ( value , kb ) if new_value != value : new_subs = [ ( '2' , secondary ) , ( 'a' , new_value ) ] else : new_subs = [ ( '2' , primary ) , ( 'a' , value ) ] record_add_field ( self . record , "650" , ind1 = "1" , ind2 = "7" , subfields = new_subs ) break
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
9801	def config ( list ) : if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
6979	def filter_kepler_lcdict ( lcdict , filterflags = True , nanfilter = 'sap,pdc' , timestoignore = None ) : cols = lcdict [ 'columns' ] if filterflags : nbefore = lcdict [ 'time' ] . size filterind = lcdict [ 'sap_quality' ] == 0 for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ filterind ] else : lcdict [ col ] = lcdict [ col ] [ filterind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'applied quality flag filter, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if nanfilter and nanfilter == 'sap,pdc' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'sap' : notnanind = ( npisfinite ( lcdict [ 'sap' ] [ 'sap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) elif nanfilter and nanfilter == 'pdc' : notnanind = ( npisfinite ( lcdict [ 'pdc' ] [ 'pdcsap_flux' ] ) & npisfinite ( lcdict [ 'time' ] ) ) if nanfilter : nbefore = lcdict [ 'time' ] . size for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ notnanind ] else : lcdict [ col ] = lcdict [ col ] [ notnanind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed nans, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) if ( timestoignore and isinstance ( timestoignore , list ) and len ( timestoignore ) > 0 ) : exclind = npfull_like ( lcdict [ 'time' ] , True , dtype = np . bool_ ) nbefore = exclind . size for ignoretime in timestoignore : time0 , time1 = ignoretime [ 0 ] , ignoretime [ 1 ] thismask = ~ ( ( lcdict [ 'time' ] >= time0 ) & ( lcdict [ 'time' ] <= time1 ) ) exclind = exclind & thismask for col in cols : if '.' in col : key , subkey = col . split ( '.' ) lcdict [ key ] [ subkey ] = lcdict [ key ] [ subkey ] [ exclind ] else : lcdict [ col ] = lcdict [ col ] [ exclind ] nafter = lcdict [ 'time' ] . size LOGINFO ( 'removed timestoignore, ndet before = %s, ndet after = %s' % ( nbefore , nafter ) ) return lcdict
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
13424	def get_message ( self , message_id ) : url = "/2/messages/%s" % message_id return self . message_from_json ( self . _get_resource ( url ) [ "message" ] )
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
5819	def _map_oids ( oids ) : new_oids = set ( ) for oid in oids : if oid in _oid_map : new_oids |= _oid_map [ oid ] return oids | new_oids
7690	def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , ** kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , ** kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
2482	def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
417	def find_datasets ( self , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( "[Database] FAIL! Cannot find any dataset: {}" . format ( kwargs ) ) return False print ( "[Database] Find {} datasets SUCCESS, took: {}s" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
6938	def parallel_update_objectinfo_cpdir ( cpdir , cpglob = 'checkplot-*.pkl*' , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : cplist = sorted ( glob . glob ( os . path . join ( cpdir , cpglob ) ) ) return parallel_update_objectinfo_cplist ( cplist , liststartindex = liststartindex , maxobjects = maxobjects , nworkers = nworkers , fast_mode = fast_mode , findercmap = findercmap , finderconvolve = finderconvolve , deredden_object = deredden_object , custom_bandpasses = custom_bandpasses , gaia_submit_timeout = gaia_submit_timeout , gaia_submit_tries = gaia_submit_tries , gaia_max_timeout = gaia_max_timeout , gaia_mirror = gaia_mirror , complete_query_later = complete_query_later , lclistpkl = lclistpkl , nbrradiusarcsec = nbrradiusarcsec , maxnumneighbors = maxnumneighbors , plotdpi = plotdpi , findercachedir = findercachedir , verbose = verbose )
12019	def find_imports ( self , pbds ) : imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import_file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import_file ) )
4856	def _delete_transmissions ( self , content_metadata_item_ids ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id__in = content_metadata_item_ids ) . delete ( )
9578	def read_numeric_array ( fd , endian , header , data_etypes ) : if header [ 'is_complex' ] : raise ParseError ( 'Complex arrays are not supported' ) data = read_elements ( fd , endian , data_etypes ) if not isinstance ( data , Sequence ) : return data rowcount = header [ 'dims' ] [ 0 ] colcount = header [ 'dims' ] [ 1 ] array = [ list ( data [ c * rowcount + r ] for c in range ( colcount ) ) for r in range ( rowcount ) ] return squeeze ( array )
4245	def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
13599	def push ( self , k ) : if not self . _first : self . _first = self . _last = node = DLL . Node ( k ) elif self . _first . value == k : return else : try : self . delete ( k ) except KeyError : pass self . _first = node = self . _first . insert_before ( k ) self . _index [ k ] = node self . _size += 1
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
11762	def alphabeta_search ( state , game , d = 4 , cutoff_test = None , eval_fn = None ) : player = game . to_move ( state ) def max_value ( state , alpha , beta , depth ) : if cutoff_test ( state , depth ) : return eval_fn ( state ) v = - infinity for a in game . actions ( state ) : v = max ( v , min_value ( game . result ( state , a ) , alpha , beta , depth + 1 ) ) if v >= beta : return v alpha = max ( alpha , v ) return v def min_value ( state , alpha , beta , depth ) : if cutoff_test ( state , depth ) : return eval_fn ( state ) v = infinity for a in game . actions ( state ) : v = min ( v , max_value ( game . result ( state , a ) , alpha , beta , depth + 1 ) ) if v <= alpha : return v beta = min ( beta , v ) return v cutoff_test = ( cutoff_test or ( lambda state , depth : depth > d or game . terminal_test ( state ) ) ) eval_fn = eval_fn or ( lambda state : game . utility ( state , player ) ) return argmax ( game . actions ( state ) , lambda a : min_value ( game . result ( state , a ) , - infinity , infinity , 0 ) )
13365	def gather ( obj ) : if hasattr ( obj , '__distob_gather__' ) : return obj . __distob_gather__ ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string_types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
9310	def amz_cano_path ( self , path ) : safe_chars = '/~' qs = '' fixed_path = path if '?' in fixed_path : fixed_path , qs = fixed_path . split ( '?' , 1 ) fixed_path = posixpath . normpath ( fixed_path ) fixed_path = re . sub ( '/+' , '/' , fixed_path ) if path . endswith ( '/' ) and not fixed_path . endswith ( '/' ) : fixed_path += '/' full_path = fixed_path if PY2 : full_path = full_path . encode ( 'utf-8' ) safe_chars = safe_chars . encode ( 'utf-8' ) qs = qs . encode ( 'utf-8' ) if self . service in [ 's3' , 'host' ] : full_path = unquote ( full_path ) full_path = quote ( full_path , safe = safe_chars ) if qs : qm = b'?' if PY2 else '?' full_path = qm . join ( ( full_path , qs ) ) if PY2 : full_path = unicode ( full_path ) return full_path
4104	def CORRELATION ( x , y = None , maxlags = None , norm = 'unbiased' ) : r assert norm in [ 'unbiased' , 'biased' , 'coeff' , None ] x = np . array ( x ) if y is None : y = x else : y = np . array ( y ) N = max ( len ( x ) , len ( y ) ) if len ( x ) < N : x = y . copy ( ) x . resize ( N ) if len ( y ) < N : y = y . copy ( ) y . resize ( N ) if maxlags is None : maxlags = N - 1 assert maxlags < N , 'lag must be less than len(x)' realdata = np . isrealobj ( x ) and np . isrealobj ( y ) if realdata == True : r = np . zeros ( maxlags , dtype = float ) else : r = np . zeros ( maxlags , dtype = complex ) if norm == 'coeff' : rmsx = pylab_rms_flat ( x ) rmsy = pylab_rms_flat ( y ) for k in range ( 0 , maxlags + 1 ) : nk = N - k - 1 if realdata == True : sum = 0 for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] else : sum = 0. + 0j for j in range ( 0 , nk + 1 ) : sum = sum + x [ j + k ] * y [ j ] . conjugate ( ) if k == 0 : if norm in [ 'biased' , 'unbiased' ] : r0 = sum / float ( N ) elif norm is None : r0 = sum else : r0 = 1. else : if norm == 'unbiased' : r [ k - 1 ] = sum / float ( N - k ) elif norm == 'biased' : r [ k - 1 ] = sum / float ( N ) elif norm is None : r [ k - 1 ] = sum elif norm == 'coeff' : r [ k - 1 ] = sum / ( rmsx * rmsy ) / float ( N ) r = np . insert ( r , 0 , r0 ) return r
8999	def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
6027	def voronoi_from_pixel_centers ( pixel_centers ) : return scipy . spatial . Voronoi ( np . asarray ( [ pixel_centers [ : , 1 ] , pixel_centers [ : , 0 ] ] ) . T , qhull_options = 'Qbb Qc Qx Qm' )
1618	def IsCppString ( line ) : line = line . replace ( r'\\' , 'XX' ) return ( ( line . count ( '"' ) - line . count ( r'\"' ) - line . count ( "'\"'" ) ) & 1 ) == 1
2974	def cmd_up ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print_containers ( containers , opts . json )
13695	def debug ( * args , ** kwargs ) : if not ( DEBUG and args ) : return None parent = kwargs . get ( 'parent' , None ) with suppress ( KeyError ) : kwargs . pop ( 'parent' ) backlevel = kwargs . get ( 'back' , 1 ) with suppress ( KeyError ) : kwargs . pop ( 'back' ) frame = inspect . currentframe ( ) while backlevel > 0 : frame = frame . f_back backlevel -= 1 fname = os . path . split ( frame . f_code . co_filename ) [ - 1 ] lineno = frame . f_lineno if parent : func = '{}.{}' . format ( parent . __class__ . __name__ , frame . f_code . co_name ) else : func = frame . f_code . co_name lineinfo = '{}:{} {}: ' . format ( C ( fname , 'yellow' ) , C ( str ( lineno ) . ljust ( 4 ) , 'blue' ) , C ( ) . join ( C ( func , 'magenta' ) , '()' ) . ljust ( 20 ) ) pargs = list ( C ( a , 'green' ) . str ( ) for a in args ) pargs [ 0 ] = '' . join ( ( lineinfo , pargs [ 0 ] ) ) print_err ( * pargs , ** kwargs )
1294	def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
12075	def analyze ( fname = False , save = True , show = None ) : if fname and os . path . exists ( fname . replace ( ".abf" , ".rst" ) ) : print ( "SKIPPING DUE TO RST FILE" ) return swhlab . plotting . core . IMAGE_SAVE = save if show is None : if cm . isIpython ( ) : swhlab . plotting . core . IMAGE_SHOW = True else : swhlab . plotting . core . IMAGE_SHOW = False abf = ABF ( fname ) print ( ">>>>> PROTOCOL >>>>>" , abf . protocomment ) runFunction = "proto_unknown" if "proto_" + abf . protocomment in globals ( ) : runFunction = "proto_" + abf . protocomment abf . log . debug ( "running %s()" % ( runFunction ) ) plt . close ( 'all' ) globals ( ) [ runFunction ] ( abf ) try : globals ( ) [ runFunction ] ( abf ) except : abf . log . error ( "EXCEPTION DURING PROTOCOL FUNCTION" ) abf . log . error ( sys . exc_info ( ) [ 0 ] ) return "ERROR" plt . close ( 'all' ) return "SUCCESS"
9182	def validate_model ( cursor , model ) : _validate_license ( model ) _validate_roles ( model ) required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) _validate_derived_from ( cursor , model ) _validate_subjects ( cursor , model )
6291	def add_texture_dir ( self , directory ) : dirs = list ( self . TEXTURE_DIRS ) dirs . append ( directory ) self . TEXTURE_DIRS = dirs
2857	def read ( self , length ) : if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
9531	def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
561	def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
3029	def verify_id_token ( id_token , audience , http = None , cert_uri = ID_TOKEN_VERIFICATION_CERTS ) : _require_crypto_or_die ( ) if http is None : http = transport . get_cached_http ( ) resp , content = transport . request ( http , cert_uri ) if resp . status == http_client . OK : certs = json . loads ( _helpers . _from_bytes ( content ) ) return crypt . verify_signed_jwt_with_certs ( id_token , certs , audience ) else : raise VerifyJwtTokenError ( 'Status code: {0}' . format ( resp . status ) )
432	def save_images ( images , size , image_path = '_temp.png' ) : if len ( images . shape ) == 3 : images = images [ : , : , : , np . newaxis ] def merge ( images , size ) : h , w = images . shape [ 1 ] , images . shape [ 2 ] img = np . zeros ( ( h * size [ 0 ] , w * size [ 1 ] , 3 ) , dtype = images . dtype ) for idx , image in enumerate ( images ) : i = idx % size [ 1 ] j = idx // size [ 1 ] img [ j * h : j * h + h , i * w : i * w + w , : ] = image return img def imsave ( images , size , path ) : if np . max ( images ) <= 1 and ( - 1 <= np . min ( images ) < 0 ) : images = ( ( images + 1 ) * 127.5 ) . astype ( np . uint8 ) elif np . max ( images ) <= 1 and np . min ( images ) >= 0 : images = ( images * 255 ) . astype ( np . uint8 ) return imageio . imwrite ( path , merge ( images , size ) ) if len ( images ) > size [ 0 ] * size [ 1 ] : raise AssertionError ( "number of images should be equal or less than size[0] * size[1] {}" . format ( len ( images ) ) ) return imsave ( images , size , image_path )
8823	def start_rpc_listeners ( self ) : self . _setup_rpc ( ) if not self . endpoints : return [ ] self . conn = n_rpc . create_connection ( ) self . conn . create_consumer ( self . topic , self . endpoints , fanout = False ) return self . conn . consume_in_threads ( )
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
6805	def init_ubuntu_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) if not self . dryrun : device_question = 'SD card present at %s? ' % self . env . sd_device inp = raw_input ( device_question ) . strip ( ) print ( 'inp:' , inp ) if not yes and inp and not inp . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local ( 'ls {sd_device}' ) r . env . ubuntu_image_fn = os . path . abspath ( os . path . split ( self . env . ubuntu_download_url ) [ - 1 ] ) r . local ( '[ ! -f {ubuntu_image_fn} ] && wget {ubuntu_download_url} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2}' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'xzcat {ubuntu_image_fn} | dd bs=4M of={sd_device}' ) r . run ( 'sync' )
10807	def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
9599	def elements ( self , using , value ) : return self . _execute ( Command . FIND_ELEMENTS , { 'using' : using , 'value' : value } )
7122	def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
12752	def indices_for_joint ( self , name ) : j = 0 for joint in self . joints : if joint . name == name : return list ( range ( j , j + joint . ADOF ) ) j += joint . ADOF return [ ]
6624	def availableVersions ( self ) : r = [ ] for t in self . _getTags ( ) : logger . debug ( "available version tag: %s" , t ) if not len ( t [ 0 ] . strip ( ) ) : continue try : r . append ( GithubComponentVersion ( t [ 0 ] , t [ 0 ] , url = t [ 1 ] , name = self . name , cache_key = None ) ) except ValueError : logger . debug ( 'invalid version tag: %s' , t ) return r
6279	def clear ( self ) : self . ctx . fbo . clear ( red = self . clear_color [ 0 ] , green = self . clear_color [ 1 ] , blue = self . clear_color [ 2 ] , alpha = self . clear_color [ 3 ] , depth = self . clear_depth , )
8938	def confluence ( ctx , no_publish = False , clean = False , opts = '' ) : cfg = config . load ( ) if clean : ctx . run ( "invoke clean --docs" ) cmd = [ 'sphinx-build' , '-b' , 'confluence' ] cmd . extend ( [ '-E' , '-a' ] ) if opts : cmd . append ( opts ) cmd . extend ( [ '.' , ctx . rituals . docs . build + '_cf' ] ) if no_publish : cmd . extend ( [ '-Dconfluence_publish=False' ] ) notify . info ( "Starting Sphinx build..." ) with pushd ( ctx . rituals . docs . sources ) : ctx . run ( ' ' . join ( cmd ) , pty = True )
1787	def DAA ( cpu ) : cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) oldAL = cpu . AL cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( cpu . CF , cpu . AL < oldAL ) , cpu . CF ) cpu . CF = Operators . OR ( ( cpu . AL & 0xf0 ) > 0x90 , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , cpu . CF , cpu . AL + 0x60 , cpu . AL ) cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
2945	def refresh_waiting_tasks ( self ) : assert not self . read_only for my_task in self . get_tasks ( Task . WAITING ) : my_task . task_spec . _update ( my_task )
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
6101	def intensities_from_grid_radii ( self , grid_radii ) : np . seterr ( all = 'ignore' ) return np . multiply ( self . intensity , np . exp ( np . multiply ( - self . sersic_constant , np . add ( np . power ( np . divide ( grid_radii , self . effective_radius ) , 1. / self . sersic_index ) , - 1 ) ) ) )
12825	def handle_extends ( self , text ) : match = self . re_extends . match ( text ) if match : extra_text = self . re_extends . sub ( '' , text , count = 1 ) blocks = self . get_blocks ( extra_text ) path = os . path . join ( self . base_dir , match . group ( 'path' ) ) with open ( path , encoding = 'utf-8' ) as fp : return self . replace_blocks_in_extends ( fp . read ( ) , blocks ) else : return None
7924	def is_ipv6_available ( ) : try : socket . socket ( socket . AF_INET6 ) . close ( ) except ( socket . error , AttributeError ) : return False return True
10869	def calc_pts_hg ( npts = 20 ) : pts_hg , wts_hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts_hg = pts_hg [ npts : ] wts_hg = wts_hg [ npts : ] * np . exp ( pts_hg * pts_hg ) return pts_hg , wts_hg
11586	def getnodefor ( self , name ) : "Return the node where the ``name`` would land to" node = self . _getnodenamefor ( name ) return { node : self . cluster [ 'nodes' ] [ node ] }
11128	def ensure_str ( value ) : if isinstance ( value , six . string_types ) : return value else : return six . text_type ( value )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
2161	def get_command ( self , ctx , name ) : if not hasattr ( self . resource , name ) : return None method = getattr ( self . resource , name ) attrs = getattr ( method , '_cli_command_attrs' , { } ) help_text = inspect . getdoc ( method ) attrs [ 'help' ] = self . _auto_help_text ( help_text or '' ) ignore_defaults = attrs . pop ( 'ignore_defaults' , False ) new_method = self . _echo_method ( method ) click_params = getattr ( method , '__click_params__' , [ ] ) new_method . __click_params__ = copy ( click_params ) new_method = with_global_options ( new_method ) fao = attrs . pop ( 'use_fields_as_options' , True ) if fao : for field in reversed ( self . resource . fields ) : if not field . is_option : continue if not isinstance ( fao , bool ) and field . name not in fao : continue args = [ field . option ] if field . key : args . insert ( 0 , field . key ) short_fields = { 'name' : 'n' , 'description' : 'd' , 'inventory' : 'i' , 'extra_vars' : 'e' } if field . name in short_fields : args . append ( '-' + short_fields [ field . name ] ) option_help = field . help if isinstance ( field . type , StructuredInput ) : option_help += ' Use @ to get JSON or YAML from a file.' if field . required : option_help = '[REQUIRED] ' + option_help elif field . read_only : option_help = '[READ ONLY] ' + option_help option_help = '[FIELD]' + option_help click . option ( * args , default = field . default if not ignore_defaults else None , help = option_help , type = field . type , show_default = field . show_default , multiple = field . multiple , is_eager = False ) ( new_method ) cmd = click . command ( name = name , cls = ActionSubcommand , ** attrs ) ( new_method ) code = six . get_function_code ( method ) if 'pk' in code . co_varnames : click . argument ( 'pk' , nargs = 1 , required = False , type = str , metavar = '[ID]' ) ( cmd ) return cmd
6778	def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
2714	def load ( self ) : tags = self . get_data ( "tags/%s" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : self . _createBucket ( index - 1 ) self . _createBucket ( index )
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
1481	def start_process_monitor ( self ) : Log . info ( "Start process monitor" ) while True : if len ( self . processes_to_monitor ) > 0 : ( pid , status ) = os . wait ( ) with self . process_lock : if pid in self . processes_to_monitor . keys ( ) : old_process_info = self . processes_to_monitor [ pid ] name = old_process_info . name command = old_process_info . command Log . info ( "%s (pid=%s) exited with status %d. command=%s" % ( name , pid , status , command ) ) self . _wait_process_std_out_err ( name , old_process_info . process ) if os . path . isfile ( "core.%d" % pid ) : os . system ( "chmod a+r core.%d" % pid ) if old_process_info . attempts >= self . max_runs : Log . info ( "%s exited too many times" % name ) sys . exit ( 1 ) time . sleep ( self . interval_between_runs ) p = self . _run_process ( name , command ) del self . processes_to_monitor [ pid ] self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) log_pid_for_process ( name , p . pid )
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : if not bool ( todo ) : break if full_tz in todo : tz_map [ full_tz ] = full_tz todo . discard ( full_tz ) continue if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
1109	def compare ( self , a , b ) : r cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
9692	def send ( self , data ) : while len ( self . senders ) >= self . window : pass self . senders [ self . new_seq_no ] = self . Sender ( self . write , self . send_lock , data , self . new_seq_no , timeout = self . sending_timeout , callback = self . send_callback , ) self . senders [ self . new_seq_no ] . start ( ) self . new_seq_no = ( self . new_seq_no + 1 ) % HDLController . MAX_SEQ_NO
8304	def eof ( self ) : return ( not self . is_alive ( ) ) and self . _queue . empty ( ) or self . _fd . closed
10152	def _extract_path_from_service ( self , service ) : path_obj = { } path = service . path route_name = getattr ( service , 'pyramid_route' , None ) if route_name : registry = self . pyramid_registry or get_current_registry ( ) route_intr = registry . introspector . get ( 'routes' , route_name ) if route_intr : path = route_intr [ 'pattern' ] else : msg = 'Route `{}` is not found by ' 'pyramid introspector' . format ( route_name ) raise ValueError ( msg ) for subpath_marker in ( '*subpath' , '*traverse' ) : path = path . replace ( subpath_marker , '{subpath}' ) parameters = self . parameters . from_path ( path ) if parameters : path_obj [ 'parameters' ] = parameters return path , path_obj
7532	def trackjobs ( func , results , spacer ) : LOGGER . info ( "inside trackjobs of %s" , func ) asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] start = time . time ( ) while 1 : ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
439	def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( " param {:3}: {:20} {:15} {} (mean: {:<18}, median: {:<18}, std: {:<18}) " . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( "Hint: print params details after tl.layers.initialize_global_variables(sess) " "or use network.print_params(False)." ) else : logging . info ( " param {:3}: {:20} {:15} {}" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( " num of params: %d" % self . count_params ( ) )
3841	async def sync_all_new_events ( self , sync_all_new_events_request ) : response = hangouts_pb2 . SyncAllNewEventsResponse ( ) await self . _pb_request ( 'conversations/syncallnewevents' , sync_all_new_events_request , response ) return response
13731	def validate_is_not_none ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
7067	def gcs_put_file ( local_file , bucketname , service_account_json = None , client = None , raiseonfail = False ) : if not client : if ( service_account_json is not None and os . path . exists ( service_account_json ) ) : client = storage . Client . from_service_account_json ( service_account_json ) else : client = storage . Client ( ) try : bucket = client . get_bucket ( bucketname ) remote_blob = bucket . blob ( local_file ) remote_blob . upload_from_filename ( local_file ) return 'gs://%s/%s' % ( bucketname , local_file . lstrip ( '/' ) ) except Exception as e : LOGEXCEPTION ( 'could not upload %s to bucket %s' % ( local_file , bucket ) ) if raiseonfail : raise return None
5515	def setlocale ( name ) : with LOCALE_LOCK : old_locale = locale . setlocale ( locale . LC_ALL ) try : yield locale . setlocale ( locale . LC_ALL , name ) finally : locale . setlocale ( locale . LC_ALL , old_locale )
1924	def binary_symbols ( binary ) : def substr_after ( string , delim ) : return string . partition ( delim ) [ 2 ] with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) for section in elffile . iter_sections ( ) : if not isinstance ( section , SymbolTableSection ) : continue symbols = [ sym . name for sym in section . iter_symbols ( ) if sym ] return [ substr_after ( name , PREPEND_SYM ) for name in symbols if name . startswith ( PREPEND_SYM ) ]
4881	def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
1734	def remove_objects ( code , count = 1 ) : replacements = { } br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) if is_object ( n , last ) : res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e else : n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : res += e last = e return res , replacements , count
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
7334	async def _chunked_upload ( self , media , media_size , path = None , media_type = None , media_category = None , chunk_size = 2 ** 20 , ** params ) : if isinstance ( media , bytes ) : media = io . BytesIO ( media ) chunk = media . read ( chunk_size ) is_coro = asyncio . iscoroutine ( chunk ) if is_coro : chunk = await chunk if media_type is None : media_metadata = await utils . get_media_metadata ( chunk , path ) media_type , media_category = media_metadata elif media_category is None : media_category = utils . get_category ( media_type ) response = await self . upload . media . upload . post ( command = "INIT" , total_bytes = media_size , media_type = media_type , media_category = media_category , ** params ) media_id = response [ 'media_id' ] i = 0 while chunk : if is_coro : req = self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk , _ = await asyncio . gather ( media . read ( chunk_size ) , req ) else : await self . upload . media . upload . post ( command = "APPEND" , media_id = media_id , media = chunk , segment_index = i ) chunk = media . read ( chunk_size ) i += 1 status = await self . upload . media . upload . post ( command = "FINALIZE" , media_id = media_id ) if 'processing_info' in status : while status [ 'processing_info' ] . get ( 'state' ) != "succeeded" : processing_info = status [ 'processing_info' ] if processing_info . get ( 'state' ) == "failed" : error = processing_info . get ( 'error' , { } ) message = error . get ( 'message' , str ( status ) ) raise exceptions . MediaProcessingError ( data = status , message = message , ** params ) delay = processing_info [ 'check_after_secs' ] await asyncio . sleep ( delay ) status = await self . upload . media . upload . get ( command = "STATUS" , media_id = media_id , ** params ) return response
1646	def FindCheckMacro ( line ) : for macro in _CHECK_MACROS : i = line . find ( macro ) if i >= 0 : matched = Match ( r'^(.*\b' + macro + r'\s*)\(' , line ) if not matched : continue return ( macro , len ( matched . group ( 1 ) ) ) return ( None , - 1 )
3862	def _get_default_delivery_medium ( self ) : medium_options = ( self . _conversation . self_conversation_state . delivery_medium_option ) try : default_medium = medium_options [ 0 ] . delivery_medium except IndexError : logger . warning ( 'Conversation %r has no delivery medium' , self . id_ ) default_medium = hangouts_pb2 . DeliveryMedium ( medium_type = hangouts_pb2 . DELIVERY_MEDIUM_BABEL ) for medium_option in medium_options : if medium_option . current_default : default_medium = medium_option . delivery_medium return default_medium
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
13705	def iter_char_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 text = ( self . text if text is None else text ) or '' text = ' ' . join ( text . split ( '\n' ) ) escapecodes = get_codes ( text ) if not escapecodes : yield from ( fmtfunc ( text [ i : i + width ] ) for i in range ( 0 , len ( text ) , width ) ) else : blockwidth = 0 block = [ ] for i , s in enumerate ( get_indices_list ( text ) ) : block . append ( s ) if len ( s ) == 1 : blockwidth += 1 if blockwidth == width : yield '' . join ( block ) block = [ ] blockwidth = 0 if block : yield '' . join ( block )
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
7055	def register_lcformat ( formatkey , fileglob , timecols , magcols , errcols , readerfunc_module , readerfunc , readerfunc_kwargs = None , normfunc_module = None , normfunc = None , normfunc_kwargs = None , magsarefluxes = False , overwrite_existing = False , lcformat_dir = '~/.astrobase/lcformat-jsons' ) : LOGINFO ( 'adding %s to LC format registry...' % formatkey ) lcformat_dpath = os . path . abspath ( os . path . expanduser ( lcformat_dir ) ) if not os . path . exists ( lcformat_dpath ) : os . makedirs ( lcformat_dpath ) lcformat_jsonpath = os . path . join ( lcformat_dpath , '%s.json' % formatkey ) if os . path . exists ( lcformat_jsonpath ) and not overwrite_existing : LOGERROR ( 'There is an existing lcformat JSON: %s ' 'for this formatkey: %s and ' 'overwrite_existing = False, skipping...' % ( lcformat_jsonpath , formatkey ) ) return None readermodule = _check_extmodule ( readerfunc_module , formatkey ) if not readermodule : LOGERROR ( "could not import the required " "module: %s to read %s light curves" % ( readerfunc_module , formatkey ) ) return None try : getattr ( readermodule , readerfunc ) readerfunc_in = readerfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified reader ' 'function: %s for lcformat: %s ' 'from module: %s' % ( formatkey , readerfunc_module , readerfunc ) ) raise if normfunc_module : normmodule = _check_extmodule ( normfunc_module , formatkey ) if not normmodule : LOGERROR ( "could not import the required " "module: %s to normalize %s light curves" % ( normfunc_module , formatkey ) ) return None else : normmodule = None if normfunc_module and normfunc : try : getattr ( normmodule , normfunc ) normfunc_in = normfunc except AttributeError : LOGEXCEPTION ( 'Could not get the specified norm ' 'function: %s for lcformat: %s ' 'from module: %s' % ( normfunc , formatkey , normfunc_module ) ) raise else : normfunc_in = None formatdict = { 'fileglob' : fileglob , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'magsarefluxes' : magsarefluxes , 'lcreader_module' : readerfunc_module , 'lcreader_func' : readerfunc_in , 'lcreader_kwargs' : readerfunc_kwargs , 'lcnorm_module' : normfunc_module , 'lcnorm_func' : normfunc_in , 'lcnorm_kwargs' : normfunc_kwargs } with open ( lcformat_jsonpath , 'w' ) as outfd : json . dump ( formatdict , outfd , indent = 4 ) return lcformat_jsonpath
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
5937	def transform_args ( self , * args , ** kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : if len ( option ) == 1 : option = '-' + option else : option = '--' + option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) else : options . extend ( ( option , str ( value ) ) ) return options + list ( args )
6734	def get_hosts_retriever ( s = None ) : s = s or env . hosts_retriever if not s : return env_hosts_retriever return str_to_callable ( s ) or env_hosts_retriever
11188	def interactive ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) readme_template = _get_readme_template ( ) yaml = YAML ( ) yaml . explicit_start = True yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) descriptive_metadata = yaml . load ( readme_template ) descriptive_metadata = _prompt_for_values ( descriptive_metadata ) stream = StringIO ( ) yaml . dump ( descriptive_metadata , stream ) proto_dataset . put_readme ( stream . getvalue ( ) ) click . secho ( "Updated readme " , fg = "green" ) click . secho ( "To edit the readme using your default editor:" ) click . secho ( "dtool readme edit {}" . format ( proto_dataset_uri ) , fg = "cyan" )
56	def deepcopy ( self , keypoints = None , shape = None ) : if keypoints is None : keypoints = [ kp . deepcopy ( ) for kp in self . keypoints ] if shape is None : shape = tuple ( self . shape ) return KeypointsOnImage ( keypoints , shape )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 while resource_id > 0x01000000 : version += 1 if version == 1 : resource_id -= 0x80000000 elif version == 2 : resource_id -= 0x03000000 else : resource_id -= 0x01000000 if return_version : return resource_id , version - 67 return resource_id
6672	def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
3582	def _get_objects_by_path ( self , paths ) : return map ( lambda x : self . _bus . get_object ( 'org.bluez' , x ) , paths )
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
13640	def get_version ( ) : version_module_path = os . path . join ( os . path . dirname ( __file__ ) , "txspinneret" , "_version.py" ) with open ( version_module_path ) as version_module : exec ( version_module . read ( ) ) return locals ( ) [ "__version__" ]
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
7620	def hierarchy_flatten ( annotation ) : intervals , values = annotation . to_interval_values ( ) ordering = dict ( ) for interval , value in zip ( intervals , values ) : level = value [ 'level' ] if level not in ordering : ordering [ level ] = dict ( intervals = list ( ) , labels = list ( ) ) ordering [ level ] [ 'intervals' ] . append ( interval ) ordering [ level ] [ 'labels' ] . append ( value [ 'label' ] ) levels = sorted ( list ( ordering . keys ( ) ) ) hier_intervals = [ ordering [ level ] [ 'intervals' ] for level in levels ] hier_labels = [ ordering [ level ] [ 'labels' ] for level in levels ] return hier_intervals , hier_labels
13600	def increment ( cls , name ) : with transaction . atomic ( ) : counter = Counter . objects . select_for_update ( ) . get ( name = name ) counter . value += 1 counter . save ( ) return counter . value
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
13219	def shell ( self , expect = pexpect ) : dsn = self . connection_dsn ( ) log . debug ( 'connection string: %s' % dsn ) child = expect . spawn ( 'psql "%s"' % dsn ) if self . _connect_args [ 'password' ] is not None : child . expect ( 'Password: ' ) child . sendline ( self . _connect_args [ 'password' ] ) child . interact ( )
11725	def camel2word ( string ) : def wordize ( match ) : return ' ' + match . group ( 1 ) . lower ( ) return string [ 0 ] + re . sub ( r'([A-Z])' , wordize , string [ 1 : ] )
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , ** kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
4306	def play ( args ) : if args [ 0 ] . lower ( ) != "play" : args . insert ( 0 , "play" ) else : args [ 0 ] = "play" try : logger . info ( "Executing: %s" , " " . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) status = process_handle . wait ( ) if process_handle . stderr is not None : logger . info ( process_handle . stderr ) if status == 0 : return True else : logger . info ( "Play returned with error code %s" , status ) return False except OSError as error_msg : logger . error ( "OSError: Play failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return False
1203	def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) A_conjugate = self . fn_x ( conjugate ) if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
12307	def auto_get_repo ( autooptions , debug = False ) : pluginmgr = plugins_get_mgr ( ) repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
11306	def store_providers ( self , provider_data ) : if not hasattr ( provider_data , '__iter__' ) : raise OEmbedException ( 'Autodiscovered response not iterable' ) provider_pks = [ ] for provider in provider_data : if 'endpoint' not in provider or 'matches' not in provider : continue resource_type = provider . get ( 'type' ) if resource_type not in RESOURCE_TYPES : continue stored_provider , created = StoredProvider . objects . get_or_create ( wildcard_regex = provider [ 'matches' ] ) if created : stored_provider . endpoint_url = relative_to_full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored_provider . resource_type = resource_type stored_provider . save ( ) provider_pks . append ( stored_provider . pk ) return StoredProvider . objects . filter ( pk__in = provider_pks )
13755	def write_to_file ( file_path , contents , encoding = "utf-8" ) : with codecs . open ( file_path , "w" , encoding ) as f : f . write ( contents )
7725	def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != MUC_USER_NS : continue if n . name == "actor" : actor = n . getContent ( ) if n . name == "reason" : reason = n . getContent ( ) n = n . next self . __init ( from_utf8 ( xmlnode . prop ( "affiliation" ) ) , from_utf8 ( xmlnode . prop ( "role" ) ) , from_utf8 ( xmlnode . prop ( "jid" ) ) , from_utf8 ( xmlnode . prop ( "nick" ) ) , from_utf8 ( actor ) , from_utf8 ( reason ) , )
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
5428	def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
9623	def maybe_decode_header ( header ) : value , encoding = decode_header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
12378	def get ( self , request , response ) : self . assert_operations ( 'read' ) items = self . read ( ) if not items : raise http . exceptions . NotFound ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string_types ) ) and items : items = pagination . paginate ( self . request , self . response , items ) self . make_response ( items )
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
12304	def post ( self , repo ) : datapackage = repo . package url = self . url token = self . token headers = { 'Authorization' : 'Token {}' . format ( token ) , 'Content-Type' : 'application/json' } try : r = requests . post ( url , data = json . dumps ( datapackage ) , headers = headers ) return r except Exception as e : raise NetworkError ( ) return ""
7694	def _sasl_authenticate ( self , stream , username , authzid ) : if not stream . initiator : raise SASLAuthenticationFailed ( "Only initiating entity start" " SASL authentication" ) if stream . features is None or not self . peer_sasl_mechanisms : raise SASLNotAvailable ( "Peer doesn't support SASL" ) props = dict ( stream . auth_properties ) if not props . get ( "service-domain" ) and ( stream . peer and stream . peer . domain ) : props [ "service-domain" ] = stream . peer . domain if username is not None : props [ "username" ] = username if authzid is not None : props [ "authzid" ] = authzid if "password" in self . settings : props [ "password" ] = self . settings [ "password" ] props [ "available_mechanisms" ] = self . peer_sasl_mechanisms enabled = sasl . filter_mechanism_list ( self . settings [ 'sasl_mechanisms' ] , props , self . settings [ 'insecure_auth' ] ) if not enabled : raise SASLNotAvailable ( "None of SASL mechanism selected can be used" ) props [ "enabled_mechanisms" ] = enabled mechanism = None for mech in enabled : if mech in self . peer_sasl_mechanisms : mechanism = mech break if not mechanism : raise SASLMechanismNotAvailable ( "Peer doesn't support any of" " our SASL mechanisms" ) logger . debug ( "Our mechanism: {0!r}" . format ( mechanism ) ) stream . auth_method_used = mechanism self . authenticator = sasl . client_authenticator_factory ( mechanism ) initial_response = self . authenticator . start ( props ) if not isinstance ( initial_response , sasl . Response ) : raise SASLAuthenticationFailed ( "SASL initiation failed" ) element = ElementTree . Element ( AUTH_TAG ) element . set ( "mechanism" , mechanism ) if initial_response . data : if initial_response . encode : element . text = initial_response . encode ( ) else : element . text = initial_response . data stream . write_element ( element )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( ** jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( ** jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( ** jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( ** rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
161	def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
13650	def get_fuel_price_trends ( self , latitude : float , longitude : float , fuel_types : List [ str ] ) -> PriceTrends : response = requests . post ( '{}/prices/trends/' . format ( API_URL_BASE ) , json = { 'location' : { 'latitude' : latitude , 'longitude' : longitude , } , 'fueltypes' : [ { 'code' : type } for type in fuel_types ] , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) return PriceTrends ( variances = [ Variance . deserialize ( variance ) for variance in data [ 'Variances' ] ] , average_prices = [ AveragePrice . deserialize ( avg_price ) for avg_price in data [ 'AveragePrices' ] ] )
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
9248	def generate_header ( self , newer_tag_name , newer_tag_link , newer_tag_time , older_tag_link , project_url ) : log = "" time_string = newer_tag_time . strftime ( self . options . date_format ) if self . options . release_url : release_url = self . options . release_url . format ( newer_tag_link ) else : release_url = u"{project_url}/tree/{newer_tag_link}" . format ( project_url = project_url , newer_tag_link = newer_tag_link ) if not self . options . unreleased_with_date and newer_tag_name == self . options . unreleased_label : log += u"## [{newer_tag_name}]({release_url})\n\n" . format ( newer_tag_name = newer_tag_name , release_url = release_url ) else : log += u"## [{newer_tag_name}]({release_url}) " u"({time_string})\n" . format ( newer_tag_name = newer_tag_name , release_url = release_url , time_string = time_string ) if self . options . compare_link and older_tag_link != REPO_CREATED_TAG_NAME : log += u"[Full Changelog]" log += u"({project_url}/compare/{older_tag_link}" . format ( project_url = project_url , older_tag_link = older_tag_link , ) log += u"...{newer_tag_link})\n\n" . format ( newer_tag_link = newer_tag_link ) return log
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( ** experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
7115	def config_sources ( app , environment , cluster , configs_dirs , app_dir , local = False , build = False ) : sources = [ ( configs_dirs , 'hostname' ) , ( configs_dirs , 'hostname-local' ) , ( configs_dirs , 'hostname-build' ) , ( configs_dirs , 'common' ) , ( configs_dirs , 'common-%s' % environment ) , ( configs_dirs , 'common-%s-%s' % ( environment , cluster ) ) , ( configs_dirs , 'common-local' ) , ( configs_dirs , 'common-build' ) , ( configs_dirs , 'common-overrides' ) , ( [ app_dir ] , '%s-default' % app ) , ( [ app_dir ] , '%s-%s' % ( app , environment ) ) , ( [ app_dir ] , '%s-%s-%s' % ( app , environment , cluster ) ) , ( configs_dirs , app ) , ( configs_dirs , '%s-%s' % ( app , environment ) ) , ( configs_dirs , '%s-%s-%s' % ( app , environment , cluster ) ) , ( [ app_dir ] , '%s-local' % app ) , ( [ app_dir ] , '%s-build' % app ) , ( configs_dirs , '%s-local' % app ) , ( configs_dirs , '%s-build' % app ) , ( configs_dirs , '%s-overrides' % app ) , ] if not build : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-build' ) ] if not local : sources = [ source for source in sources if not source [ 1 ] . endswith ( '-local' ) ] return available_sources ( sources )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
3009	def has_credentials ( self ) : credentials = _credentials_from_request ( self . request ) return ( credentials and not credentials . invalid and credentials . has_scopes ( self . _get_scopes ( ) ) )
1326	def from_keras ( cls , model , bounds , input_shape = None , channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : import tensorflow as tf if input_shape is None : try : input_shape = model . input_shape [ 1 : ] except AttributeError : raise ValueError ( 'Please specify input_shape manually or ' 'provide a model with an input_shape attribute' ) with tf . keras . backend . get_session ( ) . as_default ( ) : inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) logits = model ( inputs ) return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing )
13617	def publish ( ) : try : build_site ( dev_mode = False , clean = True ) click . echo ( 'Deploying the site...' ) call ( "rsync -avz -e ssh --progress %s/ %s" % ( BUILD_DIR , CONFIG [ "scp_target" ] , ) , shell = True ) if "cloudflare" in CONFIG and "purge" in CONFIG [ "cloudflare" ] and CONFIG [ "cloudflare" ] [ "purge" ] : do_purge ( ) except ( KeyboardInterrupt , SystemExit ) : raise sys . exit ( 1 )
3443	def save_json_model ( model , filename , sort = False , pretty = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC if pretty : dump_opts = { "indent" : 4 , "separators" : ( "," , ": " ) , "sort_keys" : True , "allow_nan" : False } else : dump_opts = { "indent" : 0 , "separators" : ( "," , ":" ) , "sort_keys" : False , "allow_nan" : False } dump_opts . update ( ** kwargs ) if isinstance ( filename , string_types ) : with open ( filename , "w" ) as file_handle : json . dump ( obj , file_handle , ** dump_opts ) else : json . dump ( obj , filename , ** dump_opts )
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
2676	def get_callable_handler_function ( src , handler ) : os . chdir ( src ) module_name , function_name = handler . split ( '.' ) filename = get_handler_filename ( handler ) path_to_module_file = os . path . join ( src , filename ) module = load_source ( module_name , path_to_module_file ) return getattr ( module , function_name )
12551	def dump_raw_data ( filename , data ) : if data . ndim == 3 : data = data . reshape ( [ data . shape [ 0 ] , data . shape [ 1 ] * data . shape [ 2 ] ] ) a = array . array ( 'f' ) for o in data : a . fromlist ( list ( o . flatten ( ) ) ) with open ( filename , 'wb' ) as rawf : a . tofile ( rawf )
13742	def get_conn ( self , aws_access_key = None , aws_secret_key = None ) : return boto . connect_dynamodb ( aws_access_key_id = aws_access_key , aws_secret_access_key = aws_secret_key , )
11694	def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
1639	def CheckParenthesisSpacing ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r' (if\(|for\(|while\(|switch\()' , line ) if match : error ( filename , linenum , 'whitespace/parens' , 5 , 'Missing space before ( in %s' % match . group ( 1 ) ) match = Search ( r'\b(if|for|while|switch)\s*' r'\(([ ]*)(.).*[^ ]+([ ]*)\)\s*{\s*$' , line ) if match : if len ( match . group ( 2 ) ) != len ( match . group ( 4 ) ) : if not ( match . group ( 3 ) == ';' and len ( match . group ( 2 ) ) == 1 + len ( match . group ( 4 ) ) or not match . group ( 2 ) and Search ( r'\bfor\s*\(.*; \)' , line ) ) : error ( filename , linenum , 'whitespace/parens' , 5 , 'Mismatching spaces inside () in %s' % match . group ( 1 ) ) if len ( match . group ( 2 ) ) not in [ 0 , 1 ] : error ( filename , linenum , 'whitespace/parens' , 5 , 'Should have zero or one spaces inside ( and ) in %s' % match . group ( 1 ) )
11277	def run_program ( prog_list , debug , shell ) : try : if not shell : process = Popen ( prog_list , stdout = PIPE , stderr = PIPE ) stdout , stderr = process . communicate ( ) retcode = process . returncode if debug >= 1 : print ( "Program : " , " " . join ( prog_list ) ) print ( "Return Code: " , retcode ) print ( "Stdout: " , stdout ) print ( "Stderr: " , stderr ) return bool ( retcode ) else : command = " " . join ( prog_list ) os . system ( command ) return True except : return False
5541	def hillshade ( self , elevation , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : return commons_hillshade . hillshade ( elevation , self , azimuth , altitude , z , scale )
1147	def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , "__deepcopy__" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(deep)copyable object of type %s" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) return y
1574	def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
7191	def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
6244	def draw ( self , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh_program : self . mesh_program . draw ( self , projection_matrix = projection_matrix , view_matrix = view_matrix , camera_matrix = camera_matrix , time = time )
3	def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )
12870	def migrate ( migrator , database , ** kwargs ) : @ migrator . create_table class DataItem ( pw . Model ) : created = pw . DateTimeField ( default = dt . datetime . utcnow ) content = pw . CharField ( )
7791	def purge_items ( self ) : self . _lock . acquire ( ) try : il = self . _items_list num_items = len ( il ) need_remove = num_items - int ( 0.75 * self . max_items ) for _unused in range ( need_remove ) : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass while il and il [ 0 ] . update_state ( ) == "purged" : item = il . pop ( 0 ) try : del self . _items [ item . address ] except KeyError : pass finally : self . _lock . release ( )
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
2615	def cancel ( self , job_ids ) : for job in job_ids : logger . debug ( "Terminating job/proc_id: {0}" . format ( job ) ) if self . resources [ job ] [ 'proc' ] : proc = self . resources [ job ] [ 'proc' ] os . killpg ( os . getpgid ( proc . pid ) , signal . SIGTERM ) self . resources [ job ] [ 'status' ] = 'CANCELLED' elif self . resources [ job ] [ 'remote_pid' ] : cmd = "kill -- -$(ps -o pgid={} | grep -o '[0-9]*')" . format ( self . resources [ job ] [ 'remote_pid' ] ) retcode , stdout , stderr = self . channel . execute_wait ( cmd , self . cmd_timeout ) if retcode != 0 : logger . warning ( "Failed to kill PID: {} and child processes on {}" . format ( self . resources [ job ] [ 'remote_pid' ] , self . label ) ) rets = [ True for i in job_ids ] return rets
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
2402	def gen_bag_feats ( self , e_set ) : if ( hasattr ( self , '_stem_dict' ) ) : sfeats = self . _stem_dict . transform ( e_set . _clean_stem_text ) nfeats = self . _normal_dict . transform ( e_set . _text ) bag_feats = numpy . concatenate ( ( sfeats . toarray ( ) , nfeats . toarray ( ) ) , axis = 1 ) else : raise util_functions . InputError ( self , "Dictionaries must be initialized prior to generating bag features." ) return bag_feats . copy ( )
10953	def set_model ( self , mdl ) : self . mdl = mdl self . mdl . check_inputs ( self . comps ) for c in self . comps : setattr ( self , '_comp_' + c . category , c )
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
10623	def extract ( self , other ) : if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mass ( other ) elif self . _is_compound_mass_tuple ( other ) : return self . _extract_compound_mass ( other [ 0 ] , other [ 1 ] ) elif type ( other ) is str : return self . _extract_compound ( other ) elif type ( other ) is Material : return self . _extract_material ( other ) else : raise TypeError ( "Invalid extraction argument." )
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( ** values )
13764	def RegisterMessage ( self , message ) : desc = message . DESCRIPTOR self . _symbols [ desc . full_name ] = message if desc . file . name not in self . _symbols_by_file : self . _symbols_by_file [ desc . file . name ] = { } self . _symbols_by_file [ desc . file . name ] [ desc . full_name ] = message self . pool . AddDescriptor ( desc ) return message
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
9981	def is_funcdef ( src ) : module_node = ast . parse ( dedent ( src ) ) if len ( module_node . body ) == 1 and isinstance ( module_node . body [ 0 ] , ast . FunctionDef ) : return True else : return False
9946	def cur_space ( self , name = None ) : if name is None : return self . _impl . model . currentspace . interface else : self . _impl . model . currentspace = self . _impl . spaces [ name ] return self . cur_space ( )
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
9964	def update_lazyevals ( self ) : if self . lazy_evals is None : return elif isinstance ( self . lazy_evals , LazyEval ) : self . lazy_evals . get_updated ( ) else : for lz in self . lazy_evals : lz . get_updated ( )
12387	def parse_segment ( text ) : "we expect foo=bar" if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) if path [ - 1 ] in constants . OPERATOR_SUFFIXES : if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided. Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
10939	def calc_accel_correction ( self , damped_JTJ , delta0 ) : _ = self . update_function ( self . param_vals ) rm0 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals + delta0 ) rm1 = self . calc_residuals ( ) . copy ( ) _ = self . update_function ( self . param_vals - delta0 ) rm2 = self . calc_residuals ( ) . copy ( ) der2 = ( rm2 + rm1 - 2 * rm0 ) corr , res , rank , s = np . linalg . lstsq ( damped_JTJ , np . dot ( self . J , der2 ) , rcond = self . min_eigval ) corr *= - 0.5 return corr
13753	def prepare_path ( path ) : if type ( path ) == list : return os . path . join ( * path ) return path
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
2639	def cancel ( self , job_ids ) : statuses = [ ] for job_id in job_ids : try : self . delete_instance ( job_id ) statuses . append ( True ) self . provisioned_blocks -= 1 except Exception : statuses . append ( False ) return statuses
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
2432	def add_creator ( self , doc , creator ) : if validations . validate_creator ( creator ) : doc . creation_info . add_creator ( creator ) return True else : raise SPDXValueError ( 'CreationInfo::Creator' )
1355	def get_argument_role ( self ) : try : return self . get_argument ( constants . PARAM_ROLE , default = None ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
4913	def courses ( self , request , pk = None ) : enterprise_customer = self . get_object ( ) self . check_object_permissions ( request , enterprise_customer ) self . ensure_data_exists ( request , enterprise_customer . catalog , error_message = "No catalog is associated with Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( enterprise_customer . catalog , request . GET ) self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses for " "Enterprise {enterprise_name} from endpoint '{path}'." . format ( enterprise_name = enterprise_customer . name , path = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) serializer . update_enterprise_courses ( enterprise_customer , catalog_id = enterprise_customer . catalog ) return get_paginated_response ( serializer . data , request )
13577	def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
146	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) polygons = [ poly . project ( self . shape , shape ) for poly in self . polygons ] return PolygonsOnImage ( polygons , shape )
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
2994	def _getJson ( url , token = '' , version = '' ) : if token : return _getJsonIEXCloud ( url , token , version ) return _getJsonOrig ( url )
6547	def wait_for_field ( self ) : self . exec_command ( "Wait({0}, InputField)" . format ( self . timeout ) . encode ( "ascii" ) ) if self . status . keyboard != b"U" : raise KeyboardStateError ( "keyboard not unlocked, state was: {0}" . format ( self . status . keyboard . decode ( "ascii" ) ) )
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) if word [ - 3 : ] == 'que' : if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] noun = word verb = word for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
10284	def count_targets ( edge_iter : EdgeIterator ) -> Counter : return Counter ( v for _ , v , _ in edge_iter )
5703	def get_vehicle_hours_by_type ( gtfs , route_type ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT * , SUM(end_time_ds - start_time_ds)/3600 as vehicle_hours_type" " FROM" " (SELECT * FROM day_trips as q1" " INNER JOIN" " (SELECT route_I, type FROM routes) as q2" " ON q1.route_I = q2.route_I" " WHERE type = {route_type}" " AND date = '{day}')" . format ( day = day , route_type = route_type ) ) df = gtfs . execute_custom_query_pandas ( query ) return df [ 'vehicle_hours_type' ] . item ( )
7701	def groups ( self ) : groups = set ( ) for item in self . _items : groups |= item . groups return groups
10611	def _calculate_H_coal ( self , T ) : m_C = 0 m_H = 0 m_O = 0 m_N = 0 m_S = 0 H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) if stoich . element_mass_fraction ( compound , 'C' ) == 1.0 : m_C += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'H' ) == 1.0 : m_H += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'O' ) == 1.0 : m_O += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'N' ) == 1.0 : m_N += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'S' ) == 1.0 : m_S += self . _compound_masses [ index ] else : dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H += dH m_total = y_C + y_H + y_O + y_N + y_S y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 Hdaf = H - H298 + self . _DH298 Hdaf *= m_total H += Hdaf return H
4159	def CORRELOGRAMPSD ( X , Y = None , lag = - 1 , window = 'hamming' , norm = 'unbiased' , NFFT = 4096 , window_params = { } , correlation_method = 'xcorr' ) : N = len ( X ) assert lag < N , 'lag must be < size of input data' assert correlation_method in [ 'CORRELATION' , 'xcorr' ] if Y is None : Y = numpy . array ( X ) crosscorrelation = False else : crosscorrelation = True if NFFT is None : NFFT = N psd = numpy . zeros ( NFFT , dtype = complex ) w = Window ( 2. * lag + 1 , window , ** window_params ) w = w . data [ lag + 1 : ] if correlation_method == 'CORRELATION' : rxy = CORRELATION ( X , Y , maxlags = lag , norm = norm ) elif correlation_method == 'xcorr' : rxy , _l = xcorr ( X , Y , maxlags = lag , norm = norm ) rxy = rxy [ lag : ] psd [ 0 ] = rxy [ 0 ] psd [ 1 : lag + 1 ] = rxy [ 1 : ] * w if crosscorrelation is True : if correlation_method == 'CORRELATION' : ryx = CORRELATION ( Y , X , maxlags = lag , norm = norm ) elif correlation_method == 'xcorr' : ryx , _l = xcorr ( Y , X , maxlags = lag , norm = norm ) ryx = ryx [ lag : ] psd [ - 1 : NFFT - lag - 1 : - 1 ] = ryx [ 1 : ] . conjugate ( ) * w else : psd [ - 1 : NFFT - lag - 1 : - 1 ] = rxy [ 1 : ] . conjugate ( ) * w psd = numpy . real ( fft ( psd ) ) return psd
11492	def login_with_api_key ( self , email , api_key , application = 'Default' ) : parameters = dict ( ) parameters [ 'email' ] = BaseDriver . email = email parameters [ 'apikey' ] = BaseDriver . apikey = api_key parameters [ 'appname' ] = application response = self . request ( 'midas.login' , parameters ) if 'token' in response : return response [ 'token' ] if 'mfa_token_id' : return response [ 'mfa_token_id' ]
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) try : _try_toparse_body ( packet_type , body , parsed ) except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
4418	async def play_at ( self , index : int ) : self . queue = self . queue [ min ( index , len ( self . queue ) - 1 ) : len ( self . queue ) ] await self . play ( ignore_shuffle = True )
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) if self . ready_event . emit ( my_task . workflow , my_task ) : for assignment in self . post_assign : assignment . assign ( my_task , my_task ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
2810	def convert_transpose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transpose ...' ) if params [ 'perm' ] [ 0 ] != 0 : if inputs [ 0 ] in layers : print ( '!!! Cannot permute batch dimension. Result may be wrong !!!' ) layers [ scope_name ] = layers [ inputs [ 0 ] ] else : print ( 'Skip weight matrix transpose, result may be wrong.' ) else : if names : tf_name = 'PERM' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) permute = keras . layers . Permute ( params [ 'perm' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = permute ( layers [ inputs [ 0 ] ] )
12992	def line_chunker ( text , getreffs , lines = 30 ) : level = len ( text . citation ) source_reffs = [ reff . split ( ":" ) [ - 1 ] for reff in getreffs ( level = level ) ] reffs = [ ] i = 0 while i + lines - 1 < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ i + lines - 1 ] , source_reffs [ i ] ] ) ) i += lines if i < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ len ( source_reffs ) - 1 ] , source_reffs [ i ] ] ) ) return reffs
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
1008	def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
9148	def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
12495	def column_or_1d ( y , warn = False ) : shape = np . shape ( y ) if len ( shape ) == 1 : return np . ravel ( y ) if len ( shape ) == 2 and shape [ 1 ] == 1 : if warn : warnings . warn ( "A column-vector y was passed when a 1d array was" " expected. Please change the shape of y to " "(n_samples, ), for example using ravel()." , DataConversionWarning , stacklevel = 2 ) return np . ravel ( y ) raise ValueError ( "bad input shape {0}" . format ( shape ) )
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
11244	def reformat_css ( input_file , output_file ) : line_count = get_line_count ( input_file ) f = open ( input_file , 'r+' ) output = open ( output_file , 'w' ) for line in range ( line_count ) : string = f . readline ( ) . strip ( ) string = re . sub ( '\{' , '{\n' , string ) string = re . sub ( '; ' , ';' , string ) string = re . sub ( ';' , ';\n' , string ) string = re . sub ( '} /*' , '}/*' , string ) string = re . sub ( '\}' , '}\n' , string ) string = re . sub ( '\*/' , '*/\n' , string ) output . write ( string ) output . close ( ) f . close ( ) indent_css ( output_file , output_file ) add_whitespace_before ( "{" , output_file , output_file )
6560	def _bqm_from_1sat ( constraint ) : configurations = constraint . configurations num_configurations = len ( configurations ) bqm = dimod . BinaryQuadraticModel . empty ( constraint . vartype ) if num_configurations == 1 : val , = next ( iter ( configurations ) ) v , = constraint . variables bqm . add_variable ( v , - 1 if val > 0 else + 1 , vartype = dimod . SPIN ) else : bqm . add_variables_from ( ( v , 0.0 ) for v in constraint . variables ) return bqm
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
8476	def _executeMassiveMethod ( path , method , args = None , classArgs = None ) : response = { } if args is None : args = { } if classArgs is None : classArgs = { } sys . path . append ( path ) exclude = [ "__init__.py" , "base.py" ] for f in AtomShieldsScanner . _getFiles ( path , "*.py" , exclude = exclude ) : try : instance = AtomShieldsScanner . _getClassInstance ( path = f , args = classArgs ) if instance is not None : if callable ( method ) : args [ "instance" ] = instance output = method ( ** args ) response [ instance . __class__ . NAME ] = output else : if hasattr ( instance , method ) : output = getattr ( instance , method ) ( ** args ) response [ instance . __class__ . NAME ] = output else : continue except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) sys . path . remove ( path ) return response
9537	def match_pattern ( regex ) : prog = re . compile ( regex ) def checker ( v ) : result = prog . match ( v ) if result is None : raise ValueError ( v ) return checker
12090	def proto_01_13_steps025dual ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) f1 = swhlab . ap . getAvgBySweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . getAvgBySweep ( abf , 'freq' , 1 , None ) f1 = np . nan_to_num ( f1 ) f2 = np . nan_to_num ( f2 ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
5686	def _get_possible_day_starts ( self , start_ut , end_ut , max_time_overnight = None ) : if max_time_overnight is None : max_time_overnight = 7 * 60 * 60 assert start_ut < end_ut start_day_ut = self . day_start_ut ( start_ut ) start_day_ds = start_ut - start_day_ut end_day_ut = self . day_start_ut ( end_ut ) if start_day_ds < max_time_overnight : start_day_ut = self . increment_day_start_ut ( start_day_ut , n_days = - 1 ) day_start_times_ut = [ start_day_ut ] while day_start_times_ut [ - 1 ] < end_day_ut : day_start_times_ut . append ( self . increment_day_start_ut ( day_start_times_ut [ - 1 ] ) ) start_times_ds = [ ] end_times_ds = [ ] for dsut in day_start_times_ut : day_start_ut = max ( 0 , start_ut - dsut ) start_times_ds . append ( day_start_ut ) day_end_ut = end_ut - dsut end_times_ds . append ( day_end_ut ) return day_start_times_ut , start_times_ds , end_times_ds
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
8011	def from_request ( cls , request , webhook_id = PAYPAL_WEBHOOK_ID ) : headers = fix_django_headers ( request . META ) assert headers try : body = request . body . decode ( request . encoding or "utf-8" ) except Exception : body = "(error decoding body)" ip = request . META [ "REMOTE_ADDR" ] obj = cls . objects . create ( headers = headers , body = body , remote_ip = ip ) try : obj . valid = obj . verify ( PAYPAL_WEBHOOK_ID ) if obj . valid : obj . process ( save = False ) except Exception as e : max_length = WebhookEventTrigger . _meta . get_field ( "exception" ) . max_length obj . exception = str ( e ) [ : max_length ] obj . traceback = format_exc ( ) finally : obj . save ( ) return obj
5452	def ensure_task_params_are_complete ( task_descriptors ) : for task_desc in task_descriptors : for param in [ 'labels' , 'envs' , 'inputs' , 'outputs' , 'input-recursives' , 'output-recursives' ] : if not task_desc . task_params . get ( param ) : task_desc . task_params [ param ] = set ( )
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
11373	def return_letters_from_string ( text ) : out = "" for letter in text : if letter . isalpha ( ) : out += letter return out
10985	def optimize_from_initial ( s , max_mem = 1e9 , invert = 'guess' , desc = '' , rz_order = 3 , min_rad = None , max_rad = None ) : RLOG . info ( 'Initial burn:' ) if desc is not None : desc_burn = desc + 'initial-burn' desc_polish = desc + 'addsub-polish' else : desc_burn , desc_polish = [ None ] * 2 opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 0.1 , desc = desc_burn , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) rad = s . obj_get_radii ( ) if min_rad is None : min_rad = 0.5 * np . median ( rad ) if max_rad is None : max_rad = 1.5 * np . median ( rad ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'initial-addsub' ) RLOG . info ( 'Final polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 8 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' ) return s
8866	def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?)?" dq3string = r'(\b[rRuU])?)?' uf_sq3string = r"(\b[rRuU])?)$" uf_dq3string = r'(\b[rRuU])?)$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
10763	def get_unique_token ( self ) : if self . _unique_token is None : self . _unique_token = self . _random_token ( ) return self . _unique_token
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
7367	def loads ( json_data , encoding = "utf-8" , ** kwargs ) : if isinstance ( json_data , bytes ) : json_data = json_data . decode ( encoding ) return json . loads ( json_data , object_hook = JSONData , ** kwargs )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : "Not accurate false due to spikes are observed" parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
7807	def verify_client ( self , client_jid = None , domains = None ) : jids = [ jid for jid in self . get_jids ( ) if jid . local ] if not jids : return None if client_jid is not None and client_jid in jids : return client_jid if domains is None : return jids [ 0 ] for jid in jids : for domain in domains : if are_domains_equal ( jid . domain , domain ) : return jid return None
9420	def is_rarfile ( filename ) : mode = constants . RAR_OM_LIST_INCSPLIT archive = unrarlib . RAROpenArchiveDataEx ( filename , mode = mode ) try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : return False unrarlib . RARCloseArchive ( handle ) return ( archive . OpenResult == constants . SUCCESS )
13760	def _format_iso_time ( self , time ) : if isinstance ( time , str ) : return time elif isinstance ( time , datetime ) : return time . strftime ( '%Y-%m-%dT%H:%M:%S.%fZ' ) else : return None
12714	def connect_to ( self , joint , other_body , offset = ( 0 , 0 , 0 ) , other_offset = ( 0 , 0 , 0 ) , ** kwargs ) : anchor = self . world . move_next_to ( self , other_body , offset , other_offset ) self . world . join ( joint , self , other_body , anchor = anchor , ** kwargs )
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
7935	def _auth ( self ) : if self . authenticated : self . __logger . debug ( "_auth: already authenticated" ) return self . __logger . debug ( "doing handshake..." ) hash_value = self . _compute_handshake ( ) n = common_root . newTextChild ( None , "handshake" , hash_value ) self . _write_node ( n ) n . unlinkNode ( ) n . freeNode ( ) self . __logger . debug ( "handshake hash sent." )
6374	def pr_lmean ( self ) : r precision = self . precision ( ) recall = self . recall ( ) if not precision or not recall : return 0.0 elif precision == recall : return precision return ( precision - recall ) / ( math . log ( precision ) - math . log ( recall ) )
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : total = 0 for i in pos : total += counts [ i ] total = float ( total ) for i in pos : x [ i ] = counts [ i ] / total else : x [ pos ] = 1 return x
12212	def get_field_for_proxy ( pref_proxy ) : field = { bool : models . BooleanField , int : models . IntegerField , float : models . FloatField , datetime : models . DateTimeField , } . get ( type ( pref_proxy . default ) , models . TextField ) ( ) update_field_from_proxy ( field , pref_proxy ) return field
12834	def on_update_stage ( self , dt ) : for actor in self . actors : actor . on_update_game ( dt ) self . forum . on_update_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_update_game ( dt ) if self . world . has_game_ended ( ) : self . exit_stage ( )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
4808	def generate_best_dataset ( best_path , output_path = 'cleaned_data' , create_val = False ) : if not os . path . isdir ( output_path ) : os . mkdir ( output_path ) if not os . path . isdir ( os . path . join ( output_path , 'train' ) ) : os . makedirs ( os . path . join ( output_path , 'train' ) ) if not os . path . isdir ( os . path . join ( output_path , 'test' ) ) : os . makedirs ( os . path . join ( output_path , 'test' ) ) if not os . path . isdir ( os . path . join ( output_path , 'val' ) ) and create_val : os . makedirs ( os . path . join ( output_path , 'val' ) ) for article_type in article_types : files = glob ( os . path . join ( best_path , article_type , '*.txt' ) ) files_train , files_test = train_test_split ( files , random_state = 0 , test_size = 0.1 ) if create_val : files_train , files_val = train_test_split ( files_train , random_state = 0 , test_size = 0.1 ) val_words = generate_words ( files_val ) val_df = create_char_dataframe ( val_words ) val_df . to_csv ( os . path . join ( output_path , 'val' , 'df_best_{}_val.csv' . format ( article_type ) ) , index = False ) train_words = generate_words ( files_train ) test_words = generate_words ( files_test ) train_df = create_char_dataframe ( train_words ) test_df = create_char_dataframe ( test_words ) train_df . to_csv ( os . path . join ( output_path , 'train' , 'df_best_{}_train.csv' . format ( article_type ) ) , index = False ) test_df . to_csv ( os . path . join ( output_path , 'test' , 'df_best_{}_test.csv' . format ( article_type ) ) , index = False ) print ( "Save {} to CSV file" . format ( article_type ) )
8370	def create_bot ( src = None , grammar = NODEBOX , format = None , outputfile = None , iterations = 1 , buff = None , window = False , title = None , fullscreen = None , server = False , port = 7777 , show_vars = False , vars = None , namespace = None ) : canvas = create_canvas ( src , format , outputfile , iterations > 1 , buff , window , title , fullscreen = fullscreen , show_vars = show_vars ) if grammar == DRAWBOT : from shoebot . grammar import DrawBot bot = DrawBot ( canvas , namespace = namespace , vars = vars ) else : from shoebot . grammar import NodeBot bot = NodeBot ( canvas , namespace = namespace , vars = vars ) if server : from shoebot . sbio import SocketServer socket_server = SocketServer ( bot , "" , port = port ) return bot
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
3353	def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
1522	def is_self ( addr ) : ips = [ ] for i in netifaces . interfaces ( ) : entry = netifaces . ifaddresses ( i ) if netifaces . AF_INET in entry : for ipv4 in entry [ netifaces . AF_INET ] : if "addr" in ipv4 : ips . append ( ipv4 [ "addr" ] ) return addr in ips or addr == get_self_hostname ( )
4726	def get_chunk_meta_item ( self , chunk_meta , grp , pug , chk ) : num_chk = self . envs [ "NUM_CHK" ] num_pu = self . envs [ "NUM_PU" ] index = grp * num_pu * num_chk + pug * num_chk + chk return chunk_meta [ index ]
2315	def orient_undirected_graph ( self , data , graph , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_pc ( data , fixedEdges = fe , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
2214	def repr2 ( data , ** kwargs ) : custom_extensions = kwargs . get ( 'extensions' , None ) _return_info = kwargs . get ( '_return_info' , False ) kwargs [ '_root_info' ] = _rectify_root_info ( kwargs . get ( '_root_info' , None ) ) outstr = None _leaf_info = None if custom_extensions : func = custom_extensions . lookup ( data ) if func is not None : outstr = func ( data , ** kwargs ) if outstr is None : if isinstance ( data , dict ) : outstr , _leaf_info = _format_dict ( data , ** kwargs ) elif isinstance ( data , ( list , tuple , set , frozenset ) ) : outstr , _leaf_info = _format_list ( data , ** kwargs ) if outstr is None : func = _FORMATTER_EXTENSIONS . lookup ( data ) if func is not None : outstr = func ( data , ** kwargs ) else : outstr = _format_object ( data , ** kwargs ) if _return_info : _leaf_info = _rectify_leaf_info ( _leaf_info ) return outstr , _leaf_info else : return outstr
11119	def get_file_info ( self , relativePath , name = None ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' can't be a file name." if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) errorMessage = "" dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) if dirInfoDict is None : return None , errorMessage fileInfo = dict . __getitem__ ( dirInfoDict , "files" ) . get ( name , None ) if fileInfo is None : errorMessage = "file %s does not exist in relative path '%s'" % ( name , relativePath ) return fileInfo , errorMessage
12569	def save ( self , ds_name , data , dtype = None ) : return self . create_dataset ( ds_name , data , dtype )
8784	def create_port ( self , context , network_id , port_id , ** kwargs ) : LOG . info ( "create_port %s %s %s" % ( context . tenant_id , network_id , port_id ) ) if not kwargs . get ( 'base_net_driver' ) : raise IronicException ( msg = 'base_net_driver required.' ) base_net_driver = kwargs [ 'base_net_driver' ] if not kwargs . get ( 'device_id' ) : raise IronicException ( msg = 'device_id required.' ) device_id = kwargs [ 'device_id' ] if not kwargs . get ( 'instance_node_id' ) : raise IronicException ( msg = 'instance_node_id required.' ) instance_node_id = kwargs [ 'instance_node_id' ] if not kwargs . get ( 'mac_address' ) : raise IronicException ( msg = 'mac_address is required.' ) mac_address = str ( netaddr . EUI ( kwargs [ "mac_address" ] [ "address" ] ) ) mac_address = mac_address . replace ( '-' , ':' ) if kwargs . get ( 'security_groups' ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) fixed_ips = [ ] addresses = kwargs . get ( 'addresses' ) if not isinstance ( addresses , list ) : addresses = [ addresses ] for address in addresses : fixed_ips . append ( self . _make_fixed_ip_dict ( context , address ) ) body = { "id" : port_id , "network_id" : network_id , "device_id" : device_id , "device_owner" : kwargs . get ( 'device_owner' , '' ) , "tenant_id" : context . tenant_id or "quark" , "roles" : context . roles , "mac_address" : mac_address , "fixed_ips" : fixed_ips , "switch:hardware_id" : instance_node_id , "dynamic_network" : not STRATEGY . is_provider_network ( network_id ) } net_info = self . _get_base_network_info ( context , network_id , base_net_driver ) body . update ( net_info ) try : LOG . info ( "creating downstream port: %s" % ( body ) ) port = self . _create_port ( context , body ) LOG . info ( "created downstream port: %s" % ( port ) ) return { "uuid" : port [ 'port' ] [ 'id' ] , "vlan_id" : port [ 'port' ] [ 'vlan_id' ] } except Exception as e : msg = "failed to create downstream port. Exception: %s" % ( e ) raise IronicException ( msg = msg )
1502	def template_scheduler_yaml ( cl_args , masters ) : single_master = masters [ 0 ] scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] template_file ( scheduler_config_template , scheduler_config_actual , { "<scheduler_uri>" : "http://%s:4646" % single_master } )
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
2393	def gen_preds ( clf , arr ) : if ( hasattr ( clf , "predict_proba" ) ) : ret = clf . predict ( arr ) else : ret = clf . predict ( arr ) return ret
5547	def _get_zoom ( zoom , input_raster , pyramid_type ) : if not zoom : minzoom = 1 maxzoom = get_best_zoom_level ( input_raster , pyramid_type ) elif len ( zoom ) == 1 : minzoom = zoom [ 0 ] maxzoom = zoom [ 0 ] elif len ( zoom ) == 2 : if zoom [ 0 ] < zoom [ 1 ] : minzoom = zoom [ 0 ] maxzoom = zoom [ 1 ] else : minzoom = zoom [ 1 ] maxzoom = zoom [ 0 ] return minzoom , maxzoom
3976	def _expand_libs_in_apps ( specs ) : for app_name , app_spec in specs [ 'apps' ] . iteritems ( ) : if 'depends' in app_spec and 'libs' in app_spec [ 'depends' ] : app_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , app_name , specs , 'apps' )
11949	def configure_custom ( self , config ) : c = config . pop ( '()' ) if not hasattr ( c , '__call__' ) and hasattr ( types , 'ClassType' ) and isinstance ( c , types . ClassType ) : c = self . resolve ( c ) props = config . pop ( '.' , None ) kwargs = dict ( ( k , config [ k ] ) for k in config if valid_ident ( k ) ) result = c ( ** kwargs ) if props : for name , value in props . items ( ) : setattr ( result , name , value ) return result
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
11175	def parse ( self , file ) : if isinstance ( file , basestring ) : file = open ( file ) line_number = 0 label = None block = self . untagged for line in file : line_number += 1 line = line . rstrip ( '\n' ) if self . tabsize > 0 : line = line . replace ( '\t' , ' ' * self . tabsize ) if self . decommenter : line = self . decommenter . decomment ( line ) if line is None : continue tag = line . split ( ':' , 1 ) [ 0 ] . strip ( ) if tag not in self . names : if block is None : if line and not line . isspace ( ) : raise ParseError ( file . name , line , "garbage before first block: %r" % line ) continue block . addline ( line ) continue name = self . names [ tag ] label = line . split ( ':' , 1 ) [ 1 ] . strip ( ) if name in self . labelled_classes : if not label : raise ParseError ( file . name , line , "missing label for %r block" % name ) block = self . blocks [ name ] . setdefault ( label , self . labelled_classes [ name ] ( ) ) else : if label : msg = "label %r present for unlabelled block %r" % ( label , name ) raise ParseError ( file . name , line_number , msg ) block = self . blocks [ name ] block . startblock ( )
6856	def query ( query , use_sudo = True , ** kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : if not line . strip ( ) : continue if not line . startswith ( '#' ) : reqs . append ( line ) continue match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) return reqs
9916	def validate_is_primary ( self , is_primary ) : if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
12270	def read_file ( self , filename ) : try : fh = open ( filename , 'rb' ) table_set = any_tableset ( fh ) except : table_set = None return table_set
5775	def _advapi32_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : algo = private_key . algorithm if algo == 'rsa' and hash_algorithm == 'raw' : padded_data = add_pkcs1v15_signature_padding ( private_key . byte_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) padded_data = add_pss_padding ( hash_algorithm , hash_length , private_key . bit_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if private_key . algorithm == 'dsa' and hash_algorithm == 'md5' : raise ValueError ( pretty_message ( ) ) hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( private_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) out_len = new ( advapi32 , 'DWORD *' ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , null ( ) , out_len ) handle_error ( res ) buffer_length = deref ( out_len ) buffer_ = buffer_from_bytes ( buffer_length ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , buffer_ , out_len ) handle_error ( res ) output = bytes_from_buffer ( buffer_ , deref ( out_len ) ) output = output [ : : - 1 ] if algo == 'dsa' : half_len = len ( output ) // 2 output = output [ half_len : ] + output [ : half_len ] output = algos . DSASignature . from_p1363 ( output ) . dump ( ) return output finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
5312	def resolve_modifier_to_ansi_code ( modifiername , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' try : start_code , end_code = ansi . MODIFIERS [ modifiername ] except KeyError : raise ColorfulError ( 'the modifier "{0}" is unknown. Use one of: {1}' . format ( modifiername , ansi . MODIFIERS . keys ( ) ) ) else : return ansi . ANSI_ESCAPE_CODE . format ( code = start_code ) , ansi . ANSI_ESCAPE_CODE . format ( code = end_code )
10259	def get_modifications_count ( graph : BELGraph ) -> Mapping [ str , int ] : return remove_falsy_values ( { 'Translocations' : len ( get_translocated ( graph ) ) , 'Degradations' : len ( get_degradations ( graph ) ) , 'Molecular Activities' : len ( get_activities ( graph ) ) , } )
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
3775	def solve_prop ( self , goal , reset_method = True ) : r if self . Tmin is None or self . Tmax is None : raise Exception ( 'Both a minimum and a maximum value are not present indicating there is not enough data for temperature dependency.' ) if not self . test_property_validity ( goal ) : raise Exception ( 'Input property is not considered plausible; no method would calculate it.' ) def error ( T ) : if reset_method : self . method = None return self . T_dependent_property ( T ) - goal try : return brenth ( error , self . Tmin , self . Tmax ) except ValueError : raise Exception ( 'To within the implemented temperature range, it is not possible to calculate the desired value.' )
1131	def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''
8310	def is_list ( str ) : for chunk in str . split ( "\n" ) : chunk = chunk . replace ( "\t" , "" ) if not chunk . lstrip ( ) . startswith ( "*" ) and not re . search ( r"^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) : return False return True
11769	def printf ( format , * args ) : sys . stdout . write ( str ( format ) % args ) return if_ ( args , lambda : args [ - 1 ] , lambda : format )
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , ** kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
9404	def _get_function_ptr ( self , name ) : func = _make_function_ptr_instance self . _function_ptrs . setdefault ( name , func ( self , name ) ) return self . _function_ptrs [ name ]
11020	def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article_filename = find_last_article ( config [ 'CONTENT_DIR' ] ) if not article_filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article_filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find_images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\nAdd these images to the latest article' ) : abort ( config ) url_prefix = os . path . join ( '{filename}' , IMAGES_PATH ) images_dir = os . path . join ( config [ 'CONTENT_DIR' ] , IMAGES_PATH ) os . makedirs ( images_dir , exist_ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image_basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url_prefix , image_basename ) ) image_filename = os . path . join ( images_dir , image_basename ) print ( filename , image_filename ) import_image ( filename , image_filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article_filename ) ) with click . open_file ( article_filename , 'a' ) as f : f . write ( content ) click . launch ( article_filename )
10618	def get_compound_amounts ( self ) : result = self . _compound_masses * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
8921	def _get_version ( self ) : if "version" in self . document . attrib : value = self . document . attrib [ "version" ] . lower ( ) if value in allowed_versions [ self . params [ 'service' ] ] : self . params [ "version" ] = value else : raise OWSInvalidParameterValue ( "Version %s is not supported" % value , value = "version" ) elif self . _get_request_type ( ) == "getcapabilities" : self . params [ "version" ] = None else : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) return self . params [ "version" ]
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
89	def new_random_state ( seed = None , fully_random = False ) : if seed is None : if not fully_random : seed = CURRENT_RANDOM_STATE . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return np . random . RandomState ( seed )
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
6772	def install_required ( self , type = None , service = None , list_only = 0 , ** kwargs ) : r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
12104	def _qsub_collate_and_launch ( self , output_dir , error_dir , job_names ) : job_name = "%s_%s_collate_%d" % ( self . batch_name , self . job_timestamp , self . collate_count ) overrides = [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) , ( '-hold_jid' , ',' . join ( job_names ) ) ] resume_cmds = [ "import os, pickle, lancet" , ( "pickle_path = os.path.join(%r, 'qlauncher.pkl')" % self . root_directory ) , "launcher = pickle.load(open(pickle_path,'rb'))" , "launcher.collate_and_launch()" ] cmd_args = [ self . command . executable , '-c' , ';' . join ( resume_cmds ) ] popen_args = self . _qsub_args ( overrides , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) self . collate_count += 1 self . message ( "Invoked qsub for next batch." ) return job_name
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
9666	def write_dot_file ( G , filename ) : with io . open ( filename , "w" ) as fh : fh . write ( "strict digraph DependencyDiagram {\n" ) edge_list = G . edges ( ) node_list = set ( G . nodes ( ) ) if edge_list : for edge in sorted ( edge_list ) : source , targ = edge node_list = node_list - set ( source ) node_list = node_list - set ( targ ) line = '"{}" -> "{}";\n' fh . write ( line . format ( source , targ ) ) if node_list : for node in sorted ( node_list ) : line = '"{}"\n' . format ( node ) fh . write ( line ) fh . write ( "}" )
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) mt = mk [ - 1 ] sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
6641	def hasDependencyRecursively ( self , name , target = None , test_dependencies = False ) : dependencies = self . getDependenciesRecursive ( target = target , test = test_dependencies ) return ( name in dependencies )
13122	def id_to_object ( self , line ) : result = Range . get ( line , ignore = 404 ) if not result : result = Range ( range = line ) result . save ( ) return result
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
1765	def pop_int ( self , force = False ) : value = self . read_int ( self . STACK , force = force ) self . STACK += self . address_bit_size // 8 return value
11686	def get_user_details ( user_id ) : reasons = [ ] try : url = OSM_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : user_data = user_request . content xml_data = ET . fromstring ( user_data ) . getchildren ( ) [ 0 ] . getchildren ( ) changesets = [ i for i in xml_data if i . tag == 'changesets' ] [ 0 ] blocks = [ i for i in xml_data if i . tag == 'blocks' ] [ 0 ] if int ( changesets . get ( 'count' ) ) <= 5 : reasons . append ( 'New mapper' ) elif int ( changesets . get ( 'count' ) ) <= 30 : url = MAPBOX_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : mapping_days = int ( user_request . json ( ) . get ( 'extra' ) . get ( 'mapping_days' ) ) if mapping_days <= 5 : reasons . append ( 'New mapper' ) if int ( blocks . getchildren ( ) [ 0 ] . get ( 'count' ) ) > 1 : reasons . append ( 'User has multiple blocks' ) except Exception as e : message = 'Could not verify user of the changeset: {}, {}' print ( message . format ( user_id , str ( e ) ) ) return reasons
11021	def _generate_circle ( self ) : total_weight = 0 for node in self . nodes : total_weight += self . weights . get ( node , 1 ) for node in self . nodes : weight = 1 if node in self . weights : weight = self . weights . get ( node ) factor = math . floor ( ( 40 * len ( self . nodes ) * weight ) / total_weight ) for j in range ( 0 , int ( factor ) ) : b_key = bytearray ( self . _hash_digest ( '%s-%s' % ( node , j ) ) ) for i in range ( 0 , 3 ) : key = self . _hash_val ( b_key , lambda x : x + i * 4 ) self . ring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )
5293	def get_context_data ( self , ** kwargs ) : context = { } inlines_names = self . get_inlines_names ( ) if inlines_names : context . update ( zip ( inlines_names , kwargs . get ( 'inlines' , [ ] ) ) ) if 'formset' in kwargs : context [ inlines_names [ 0 ] ] = kwargs [ 'formset' ] context . update ( kwargs ) return super ( NamedFormsetsMixin , self ) . get_context_data ( ** context )
10818	def can_invite_others ( self , user ) : if self . is_managed : return False elif self . is_admin ( user ) : return True elif self . subscription_policy != SubscriptionPolicy . CLOSED : return True else : return False
11134	def is_not_exist_or_allow_overwrite ( self , overwrite = False ) : if self . exists ( ) and overwrite is False : return False else : return True
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( ** self . _sharedInstance . config ) return self . _sharedInstance . instance
10193	def get_user ( ) : return dict ( ip_address = request . remote_addr , user_agent = request . user_agent . string , user_id = ( current_user . get_id ( ) if current_user . is_authenticated else None ) , session_id = session . get ( 'sid_s' ) )
10790	def load_wisdom ( wisdomfile ) : if wisdomfile is None : return try : pyfftw . import_wisdom ( pickle . load ( open ( wisdomfile , 'rb' ) ) ) except ( IOError , TypeError ) as e : log . warn ( "No wisdom present, generating some at %r" % wisdomfile ) save_wisdom ( wisdomfile )
7758	def send ( self , stanza ) : if self . uplink : self . uplink . send ( stanza ) else : raise NoRouteError ( "No route for stanza" )
5548	def validate_values ( config , values ) : if not isinstance ( config , dict ) : raise TypeError ( "config must be a dictionary" ) for value , vtype in values : if value not in config : raise ValueError ( "%s not given" % value ) if not isinstance ( config [ value ] , vtype ) : raise TypeError ( "%s must be %s" % ( value , vtype ) ) return True
12528	def upload ( ctx , repo ) : artifacts = ' ' . join ( shlex . quote ( str ( n ) ) for n in ROOT . joinpath ( 'dist' ) . glob ( 'pipfile[-_]cli-*' ) ) ctx . run ( f'twine upload --repository="{repo}" {artifacts}' )
8697	def __clear_buffers ( self ) : try : self . _port . reset_input_buffer ( ) self . _port . reset_output_buffer ( ) except AttributeError : self . _port . flushInput ( ) self . _port . flushOutput ( )
11046	def init_logging ( log_level ) : log_level_filter = LogLevelFilterPredicate ( LogLevel . levelWithName ( log_level ) ) log_level_filter . setLogLevelForNamespace ( 'twisted.web.client._HTTP11ClientFactory' , LogLevel . warn ) log_observer = FilteringLogObserver ( textFileLogObserver ( sys . stdout ) , [ log_level_filter ] ) globalLogPublisher . addObserver ( log_observer )
1121	def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )
5565	def baselevels ( self ) : if "baselevels" not in self . _raw : return { } baselevels = self . _raw [ "baselevels" ] minmax = { k : v for k , v in baselevels . items ( ) if k in [ "min" , "max" ] } if not minmax : raise MapcheteConfigError ( "no min and max values given for baselevels" ) for v in minmax . values ( ) : if not isinstance ( v , int ) or v < 0 : raise MapcheteConfigError ( "invalid baselevel zoom parameter given: %s" % minmax . values ( ) ) zooms = list ( range ( minmax . get ( "min" , min ( self . zoom_levels ) ) , minmax . get ( "max" , max ( self . zoom_levels ) ) + 1 ) ) if not set ( self . zoom_levels ) . difference ( set ( zooms ) ) : raise MapcheteConfigError ( "baselevels zooms fully cover process zooms" ) return dict ( zooms = zooms , lower = baselevels . get ( "lower" , "nearest" ) , higher = baselevels . get ( "higher" , "nearest" ) , tile_pyramid = BufferedTilePyramid ( self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . process_pyramid . metatiling ) )
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
13820	def _ConvertListValueMessage ( value , message ) : if not isinstance ( value , list ) : raise ParseError ( 'ListValue must be in [] which is {0}.' . format ( value ) ) message . ClearField ( 'values' ) for item in value : _ConvertValueMessage ( item , message . values . add ( ) )
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
8531	def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]
1633	def CheckForNewlineAtEOF ( filename , lines , error ) : if len ( lines ) < 3 or lines [ - 2 ] : error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 'Could not find a newline character at the end of the file.' )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
6135	def _fix_docs ( this_abc , child_class ) : if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
6177	def reduce_chunk ( func , array ) : res = [ ] for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
12647	def set_auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise ValueError ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise ValueError ( 'Must specify both cert and key' ) if pem : set_config_value ( 'security' , 'pem' ) set_config_value ( 'pem_path' , pem ) elif cert or key : set_config_value ( 'security' , 'cert' ) set_config_value ( 'cert_path' , cert ) set_config_value ( 'key_path' , key ) elif aad : set_config_value ( 'security' , 'aad' ) else : set_config_value ( 'security' , 'none' )
7161	def go_back ( self , n = 1 ) : if not self . can_go_back : return N = max ( len ( self . answers ) - abs ( n ) , 0 ) self . answers = OrderedDict ( islice ( self . answers . items ( ) , N ) )
3199	def delete ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _delete ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
1364	def get_argument_length ( self ) : try : length = self . get_argument ( constants . PARAM_LENGTH ) return length except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3786	def TP_dependent_property_derivative_T ( self , T , P , order = 1 ) : r sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_T ( T , P , method , order ) except : pass return None
7326	def with_continuations ( ** c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , ** dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
9458	def sound_touch_stop ( self , call_params ) : path = '/' + self . api_version + '/SoundTouchStop/' method = 'POST' return self . request ( path , method , call_params )
12318	def drop ( self , repo , args = [ ] ) : rootdir = repo . rootdir if os . path . exists ( rootdir ) : print ( "Cleaning repo directory: {}" . format ( rootdir ) ) shutil . rmtree ( rootdir ) server_repodir = self . server_rootdir_from_repo ( repo , create = False ) if os . path . exists ( server_repodir ) : print ( "Cleaning data from local git 'server': {}" . format ( server_repodir ) ) shutil . rmtree ( server_repodir ) super ( GitRepoManager , self ) . drop ( repo ) return { 'status' : 'success' , 'message' : "successful cleanup" }
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
6766	def interfaces ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : if is_file ( '/usr/sbin/dladm' ) : res = run ( '/usr/sbin/dladm show-link' ) else : res = sudo ( '/sbin/ifconfig -s' ) return [ line . split ( ' ' ) [ 0 ] for line in res . splitlines ( ) [ 1 : ] ]
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
11752	def compute_precedence ( terminals , productions , precedence_levels ) : precedence = collections . OrderedDict ( ) for terminal in terminals : precedence [ terminal ] = DEFAULT_PREC level_precs = range ( len ( precedence_levels ) , 0 , - 1 ) for i , level in zip ( level_precs , precedence_levels ) : assoc = level [ 0 ] for symbol in level [ 1 : ] : precedence [ symbol ] = ( assoc , i ) for production , prec_symbol in productions : if prec_symbol is None : prod_terminals = [ symbol for symbol in production . rhs if symbol in terminals ] or [ None ] precedence [ production ] = precedence . get ( prod_terminals [ - 1 ] , DEFAULT_PREC ) else : precedence [ production ] = precedence . get ( prec_symbol , DEFAULT_PREC ) return precedence
7265	def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : return self . run_assertions ( ctx ) except Exception as _err : if getattr ( _err , '__legit__' , False ) : raise _err return self . render_error ( ctx , _err )
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
3460	def single_reaction_deletion ( model , reaction_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'reaction' , element_lists = _element_lists ( model . reactions , reaction_list ) , method = method , solution = solution , processes = processes , ** kwargs )
12517	def extract_datasets ( h5file , h5path = '/' ) : if isinstance ( h5file , str ) : _h5file = h5py . File ( h5file , mode = 'r' ) else : _h5file = h5file _datasets = get_datasets ( _h5file , h5path ) datasets = OrderedDict ( ) try : for ds in _datasets : datasets [ ds . name . split ( '/' ) [ - 1 ] ] = ds [ : ] except : raise RuntimeError ( 'Error reading datasets in {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return datasets
8846	def update_terminal_colors ( self ) : self . color_scheme = self . create_color_scheme ( background = self . syntax_highlighter . color_scheme . background , foreground = self . syntax_highlighter . color_scheme . formats [ 'normal' ] . foreground ( ) . color ( ) )
9050	def bernoulli_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : r link = LogitLink ( ) mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) lik = BernoulliProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
11721	def config_loader ( app , ** kwargs_config ) : local_templates_path = os . path . join ( app . instance_path , 'templates' ) if os . path . exists ( local_templates_path ) : app . jinja_loader = ChoiceLoader ( [ FileSystemLoader ( local_templates_path ) , app . jinja_loader , ] ) app . jinja_options = dict ( app . jinja_options , cache_size = 1000 , bytecode_cache = BytecodeCache ( app ) ) invenio_config_loader ( app , ** kwargs_config )
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
12125	def pprint_args ( self , pos_args , keyword_args , infix_operator = None , extra_params = { } ) : if infix_operator and not ( len ( pos_args ) == 2 and keyword_args == [ ] ) : raise Exception ( 'Infix format requires exactly two' ' positional arguments and no keywords' ) ( kwargs , _ , _ , _ ) = self . _pprint_args self . _pprint_args = ( keyword_args + kwargs , pos_args , infix_operator , extra_params )
13160	def delete ( cls , cur , table : str , where_keys : list ) : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _delete_query . format ( table , where_clause ) yield from cur . execute ( query , values ) return cur . rowcount
3052	def step1_get_authorize_url ( self , redirect_uri = None , state = None ) : if redirect_uri is not None : logger . warning ( ( 'The redirect_uri parameter for ' 'OAuth2WebServerFlow.step1_get_authorize_url is deprecated. ' 'Please move to passing the redirect_uri in via the ' 'constructor.' ) ) self . redirect_uri = redirect_uri if self . redirect_uri is None : raise ValueError ( 'The value of redirect_uri must not be None.' ) query_params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , 'scope' : self . scope , } if state is not None : query_params [ 'state' ] = state if self . login_hint is not None : query_params [ 'login_hint' ] = self . login_hint if self . _pkce : if not self . code_verifier : self . code_verifier = _pkce . code_verifier ( ) challenge = _pkce . code_challenge ( self . code_verifier ) query_params [ 'code_challenge' ] = challenge query_params [ 'code_challenge_method' ] = 'S256' query_params . update ( self . params ) return _helpers . update_query_params ( self . auth_uri , query_params )
11779	def SyntheticRestaurant ( n = 20 ) : "Generate a DataSet with n examples." def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
10180	def get ( self , timeout = None ) : result = None try : result = self . _result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc_info ) else : return result
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
4416	async def play ( self , track_index : int = 0 , ignore_shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . _lavalink . dispatch_event ( QueueEndEvent ( self ) ) else : if self . shuffle and not ignore_shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track_index , len ( self . queue ) - 1 ) ) self . current = track await self . _lavalink . ws . send ( op = 'play' , guildId = self . guild_id , track = track . track ) await self . _lavalink . dispatch_event ( TrackStartEvent ( self , track ) )
7425	def bedtools_merge ( data , sample ) : LOGGER . info ( "Entering bedtools_merge: %s" , sample . name ) mappedreads = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) cmd1 = [ ipyrad . bins . bedtools , "bamtobed" , "-i" , mappedreads ] cmd2 = [ ipyrad . bins . bedtools , "merge" , "-i" , "-" ] if 'pair' in data . paramsdict [ "datatype" ] : check_insert_size ( data , sample ) cmd2 . insert ( 2 , str ( data . _hackersonly [ "max_inner_mate_distance" ] ) ) cmd2 . insert ( 2 , "-d" ) else : cmd2 . insert ( 2 , str ( - 1 * data . _hackersonly [ "min_SE_refmap_overlap" ] ) ) cmd2 . insert ( 2 , "-d" ) LOGGER . info ( "stdv: bedtools merge cmds: %s %s" , cmd1 , cmd2 ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) result = proc2 . communicate ( ) [ 0 ] proc1 . stdout . close ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , result ) if os . path . exists ( ipyrad . __debugflag__ ) : with open ( os . path . join ( data . dirs . refmapping , sample . name + ".bed" ) , 'w' ) as outfile : outfile . write ( result ) nregions = len ( result . strip ( ) . split ( "\n" ) ) LOGGER . info ( "bedtools_merge: Got # regions: %s" , nregions ) return result
346	def load_matt_mahoney_text8_dataset ( path = 'data' ) : path = os . path . join ( path , 'mm_test8' ) logging . info ( "Load or Download matt_mahoney_text8 Dataset> {}" . format ( path ) ) filename = 'text8.zip' url = 'http://mattmahoney.net/dc/' maybe_download_and_extract ( filename , path , url , expected_bytes = 31344016 ) with zipfile . ZipFile ( os . path . join ( path , filename ) ) as f : word_list = f . read ( f . namelist ( ) [ 0 ] ) . split ( ) for idx , _ in enumerate ( word_list ) : word_list [ idx ] = word_list [ idx ] . decode ( ) return word_list
1575	def hex_escape ( bin_str ) : printable = string . ascii_letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str )
149	def clip_out_of_image ( self ) : polys_cut = [ poly . clip_out_of_image ( self . shape ) for poly in self . polygons if poly . is_partly_within_image ( self . shape ) ] polys_cut_flat = [ poly for poly_lst in polys_cut for poly in poly_lst ] return PolygonsOnImage ( polys_cut_flat , shape = self . shape )
13344	def mean ( a , axis = None , dtype = None , out = None , keepdims = False ) : if ( isinstance ( a , np . ndarray ) or isinstance ( a , RemoteArray ) or isinstance ( a , DistArray ) ) : return a . mean ( axis = axis , dtype = dtype , out = out , keepdims = keepdims ) else : return np . mean ( a , axis = axis , dtype = dtype , out = out , keepdims = keepdims )
12843	def execute_sync ( self , message ) : info ( "synchronizing message: {message}" ) with self . world . _unlock_temporarily ( ) : message . _sync ( self . world ) self . world . _react_to_sync_response ( message ) for actor in self . actors : actor . _react_to_sync_response ( message )
1193	def task_done ( self ) : self . all_tasks_done . acquire ( ) try : unfinished = self . unfinished_tasks - 1 if unfinished <= 0 : if unfinished < 0 : raise ValueError ( 'task_done() called too many times' ) self . all_tasks_done . notify_all ( ) self . unfinished_tasks = unfinished finally : self . all_tasks_done . release ( )
13679	def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : self . data [ v [ self . id_key ] ] = { } self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
5563	def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
11435	def _validate_record_field_positions_global ( record ) : all_fields = [ ] for tag , fields in record . items ( ) : previous_field_position_global = - 1 for field in fields : if field [ 4 ] < previous_field_position_global : return ( "Non ascending global field positions in tag '%s'." % tag ) previous_field_position_global = field [ 4 ] if field [ 4 ] in all_fields : return ( "Duplicate global field position '%d' in tag '%s'" % ( field [ 4 ] , tag ) )
12838	async def async_connect ( self ) : if self . _async_lock is None : raise Exception ( 'Error, database not properly initialized before async connection' ) async with self . _async_lock : self . connect ( True ) return self . _state . conn
10911	def get_num_px_jtj ( s , nparams , decimate = 1 , max_mem = 1e9 , min_redundant = 20 ) : px_mem = int ( max_mem // 8 // nparams ) px_red = min_redundant * nparams px_dec = s . residuals . size // decimate if px_red > px_mem : raise RuntimeError ( 'Insufficient max_mem for desired redundancy.' ) num_px = np . clip ( px_dec , px_red , px_mem ) . astype ( 'int' ) return num_px
8237	def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
2526	def get_annotation_type ( self , r_term ) : for _ , _ , typ in self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationType' ] , None ) ) : if typ is not None : return typ else : self . error = True msg = 'Annotation must have exactly one annotation type.' self . logger . log ( msg ) return
5542	def contours ( self , elevation , interval = 100 , field = 'elev' , base = 0 ) : return commons_contours . extract_contours ( elevation , self . tile , interval = interval , field = field , base = base )
10993	def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
148	def remove_out_of_image ( self , fully = True , partly = False ) : polys_clean = [ poly for poly in self . polygons if not poly . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return PolygonsOnImage ( polys_clean , shape = self . shape )
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
4444	def get_suggestions ( self , prefix , fuzzy = False , num = 10 , with_scores = False , with_payloads = False ) : args = [ AutoCompleter . SUGGET_COMMAND , self . key , prefix , 'MAX' , num ] if fuzzy : args . append ( AutoCompleter . FUZZY ) if with_scores : args . append ( AutoCompleter . WITHSCORES ) if with_payloads : args . append ( AutoCompleter . WITHPAYLOADS ) ret = self . redis . execute_command ( * args ) results = [ ] if not ret : return results parser = SuggestionParser ( with_scores , with_payloads , ret ) return [ s for s in parser ]
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
1561	def get_sources ( self , component_id ) : StreamId = namedtuple ( 'StreamId' , 'id, component_name' ) if component_id in self . inputs : ret = { } for istream in self . inputs . get ( component_id ) : key = StreamId ( id = istream . stream . id , component_name = istream . stream . component_name ) ret [ key ] = istream . gtype return ret else : return None
1366	def validateInterval ( self , startTime , endTime ) : start = int ( startTime ) end = int ( endTime ) if start > end : raise Exception ( "starttime is greater than endtime." )
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
11400	def update_collaboration ( self ) : for field in record_get_field_instances ( self . record , '710' ) : subs = field_get_subfield_instances ( field ) for idx , ( key , value ) in enumerate ( subs [ : ] ) : if key == '5' : subs . pop ( idx ) elif value . startswith ( 'CERN. Geneva' ) : subs . pop ( idx ) if len ( subs ) == 0 : record_delete_field ( self . record , tag = '710' , field_position_global = field [ 4 ] )
6225	def move_state ( self , direction , activate ) : if direction == RIGHT : self . _xdir = POSITIVE if activate else STILL elif direction == LEFT : self . _xdir = NEGATIVE if activate else STILL elif direction == FORWARD : self . _zdir = NEGATIVE if activate else STILL elif direction == BACKWARD : self . _zdir = POSITIVE if activate else STILL elif direction == UP : self . _ydir = POSITIVE if activate else STILL elif direction == DOWN : self . _ydir = NEGATIVE if activate else STILL
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
7468	def multi_muscle_align ( data , samples , ipyclient ) : LOGGER . info ( "starting alignments" ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " aligning clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) path = os . path . join ( data . tmpdir , data . name + ".chunk_*" ) clustbits = glob . glob ( path ) jobs = { } for idx in xrange ( len ( clustbits ) ) : args = [ data , samples , clustbits [ idx ] ] jobs [ idx ] = lbview . apply ( persistent_popen_align3 , * args ) allwait = len ( jobs ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) while 1 : finished = [ i . ready ( ) for i in jobs . values ( ) ] fwait = sum ( finished ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( allwait , fwait , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if all ( finished ) : break keys = jobs . keys ( ) for idx in keys : if not jobs [ idx ] . successful ( ) : LOGGER . error ( "error in persistent_popen_align %s" , jobs [ idx ] . exception ( ) ) raise IPyradWarningExit ( "error in step 6 {}" . format ( jobs [ idx ] . exception ( ) ) ) del jobs [ idx ] print ( "" )
1681	def AddFilters ( self , filters ) : for filt in filters . split ( ',' ) : clean_filt = filt . strip ( ) if clean_filt : self . filters . append ( clean_filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise ValueError ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
4082	def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
7069	def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] varfeaturedir = os . path . join ( simbasedir , 'varfeatures' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) varinfo = lcvfeatures . parallel_varfeatures ( lcfpaths , varfeaturedir , lcformat = fakelc_formatkey , mindet = mindet , nworkers = nworkers ) with open ( os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' ) , 'wb' ) as outfd : pickle . dump ( varinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' )
11185	def publish ( quiet , dataset_uri ) : access_uri = http_publish ( dataset_uri ) if not quiet : click . secho ( "Dataset accessible at " , nl = False , fg = "green" ) click . secho ( access_uri )
3283	def read ( self , size = None ) : while size is None or len ( self . buffer ) < size : try : self . buffer += next ( self . data_stream ) except StopIteration : break sized_chunk = self . buffer [ : size ] if size is None : self . buffer = "" else : self . buffer = self . buffer [ size : ] return sized_chunk
9821	def create ( ctx , name , description , tags , private , init ) : try : tags = tags . split ( ',' ) if tags else None project_dict = dict ( name = name , description = description , is_public = not private , tags = tags ) project_config = ProjectConfig . from_dict ( project_dict ) except ValidationError : Printer . print_error ( 'Project name should contain only alpha numerical, "-", and "_".' ) sys . exit ( 1 ) try : _project = PolyaxonClient ( ) . project . create_project ( project_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not create project `{}`.' . format ( name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project `{}` was created successfully." . format ( _project . name ) ) if init : ctx . obj = { } ctx . invoke ( init_project , project = name )
6951	def jhk_to_sdssz ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSZ_JHK , SDSSZ_JH , SDSSZ_JK , SDSSZ_HK , SDSSZ_J , SDSSZ_H , SDSSZ_K )
12072	def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group_ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group_ind ) ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . __class__ . __name__ )
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
2694	def fix_hypenation ( foo ) : i = 0 bar = [ ] while i < len ( foo ) : text , lemma , pos , tag = foo [ i ] if ( tag == "HYPH" ) and ( i > 0 ) and ( i < len ( foo ) - 1 ) : prev_tok = bar [ - 1 ] next_tok = foo [ i + 1 ] prev_tok [ 0 ] += "-" + next_tok [ 0 ] prev_tok [ 1 ] += "-" + next_tok [ 1 ] bar [ - 1 ] = prev_tok i += 2 else : bar . append ( foo [ i ] ) i += 1 return bar
13113	def zone_transfer ( address , dns_name ) : ips = [ ] try : print_notification ( "Attempting dns zone transfer for {} on {}" . format ( dns_name , address ) ) z = dns . zone . from_xfr ( dns . query . xfr ( address , dns_name ) ) except dns . exception . FormError : print_notification ( "Zone transfer not allowed" ) return ips names = z . nodes . keys ( ) print_success ( "Zone transfer successfull for {}, found {} entries" . format ( address , len ( names ) ) ) for n in names : node = z [ n ] data = node . get_rdataset ( dns . rdataclass . IN , dns . rdatatype . A ) if data : for item in data . items : address = item . address ips . append ( address ) return ips
11430	def record_order_subfields ( rec , tag = None ) : if rec is None : return rec if tag is None : tags = rec . keys ( ) for tag in tags : record_order_subfields ( rec , tag ) elif tag in rec : for i in xrange ( len ( rec [ tag ] ) ) : field = rec [ tag ] [ i ] ordered_subfields = sorted ( field [ 0 ] , key = lambda subfield : subfield [ 0 ] ) rec [ tag ] [ i ] = ( ordered_subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] , field [ 4 ] )
5874	def is_valid_filename ( self , image_node ) : src = self . parser . getAttribute ( image_node , attr = 'src' ) if not src : return False if self . badimages_names_re . search ( src ) : return False return True
13496	def bump ( self , target ) : if target == 'patch' : return Version ( self . major , self . minor , self . patch + 1 ) if target == 'minor' : return Version ( self . major , self . minor + 1 , 0 ) if target == 'major' : return Version ( self . major + 1 , 0 , 0 ) return self . clone ( )
6001	def pix_to_sub ( self ) : pix_to_sub = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . sub_to_pix ) : pix_to_sub [ pix_pixel ] . append ( regular_pixel ) return pix_to_sub
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
12008	def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
3856	def unread_events ( self ) : return [ conv_event for conv_event in self . _events if conv_event . timestamp > self . latest_read_timestamp ]
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
11960	def is_bits_nm ( nm ) : try : bits = int ( str ( nm ) ) except ValueError : return False if bits > 32 or bits < 0 : return False return True
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
3812	async def set_active ( self ) : is_active = ( self . _active_client_state == hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) timed_out = ( time . time ( ) - self . _last_active_secs > SETACTIVECLIENT_LIMIT_SECS ) if not is_active or timed_out : self . _active_client_state = ( hangouts_pb2 . ACTIVE_CLIENT_STATE_IS_ACTIVE ) self . _last_active_secs = time . time ( ) if self . _email is None : try : get_self_info_request = hangouts_pb2 . GetSelfInfoRequest ( request_header = self . get_request_header ( ) , ) get_self_info_response = await self . get_self_info ( get_self_info_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to find email address: {}' . format ( e ) ) return self . _email = ( get_self_info_response . self_entity . properties . email [ 0 ] ) if self . _client_id is None : logger . info ( 'Cannot set active client until client_id is received' ) return try : set_active_request = hangouts_pb2 . SetActiveClientRequest ( request_header = self . get_request_header ( ) , is_active = True , full_jid = "{}/{}" . format ( self . _email , self . _client_id ) , timeout_secs = ACTIVE_TIMEOUT_SECS , ) await self . set_active_client ( set_active_request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set active client: {}' . format ( e ) ) else : logger . info ( 'Set active client for {} seconds' . format ( ACTIVE_TIMEOUT_SECS ) )
9464	def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
2272	def _win32_is_junction ( path ) : if not exists ( path ) : if os . path . isdir ( path ) : if not os . path . islink ( path ) : return True return False return jwfs . is_reparse_point ( path ) and not os . path . islink ( path )
8869	def read_bgen ( filepath , metafile_filepath = None , samples_filepath = None , verbose = True ) : r assert_file_exist ( filepath ) assert_file_readable ( filepath ) metafile_filepath = _get_valid_metafile_filepath ( filepath , metafile_filepath ) if not os . path . exists ( metafile_filepath ) : if verbose : print ( f"We will create the metafile `{metafile_filepath}`. This file will " "speed up further\nreads and only need to be created once. So, please, " "bear with me." ) create_metafile ( filepath , metafile_filepath , verbose ) samples = get_samples ( filepath , samples_filepath , verbose ) variants = map_metadata ( filepath , metafile_filepath ) genotype = map_genotype ( filepath , metafile_filepath , verbose ) return dict ( variants = variants , samples = samples , genotype = genotype )
10568	def filter_google_songs ( songs , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] if include_filters or exclude_filters : for song in songs : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( song ) else : filtered_songs . append ( song ) else : matched_songs += songs return matched_songs , filtered_songs
11700	def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] egg_donors += gen_xx sperm_donors += gen_xy
3624	def decode ( geohash ) : lat , lon , lat_err , lon_err = decode_exactly ( geohash ) lats = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lat_err ) ) ) ) - 1 , lat ) lons = "%.*f" % ( max ( 1 , int ( round ( - log10 ( lon_err ) ) ) ) - 1 , lon ) if '.' in lats : lats = lats . rstrip ( '0' ) if '.' in lons : lons = lons . rstrip ( '0' ) return lats , lons
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
5464	def get_action_by_id ( op , action_id ) : actions = get_actions ( op ) if actions and 1 <= action_id < len ( actions ) : return actions [ action_id - 1 ]
8992	def rows_after ( self ) : rows_after = [ ] for mesh in self . produced_meshes : if mesh . is_consumed ( ) : row = mesh . consuming_row if rows_after not in rows_after : rows_after . append ( row ) return rows_after
8693	def init ( self ) : try : self . client . create_bucket ( Bucket = self . db_path , CreateBucketConfiguration = self . bucket_configuration ) except botocore . exceptions . ClientError as e : if 'BucketAlreadyOwnedByYou' not in str ( e . response [ 'Error' ] [ 'Code' ] ) : raise e
12195	def get_app_locations ( ) : return [ os . path . dirname ( os . path . normpath ( import_module ( app_name ) . __file__ ) ) for app_name in PROJECT_APPS ]
109	def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )
4273	def url ( self ) : url = self . name . encode ( 'utf-8' ) return url_quote ( url ) + '/' + self . url_ext
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
1264	def sanity_check_states ( states_spec ) : states = copy . deepcopy ( states_spec ) is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) for name , state in states . items ( ) : if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique
754	def setLoggedMetrics ( self , metricNames ) : if metricNames is None : self . __metricNames = set ( [ ] ) else : self . __metricNames = set ( metricNames )
8511	def load ( self ) : from pylearn2 . config import yaml_parse from pylearn2 . datasets import Dataset dataset = yaml_parse . load ( self . yaml_string ) assert isinstance ( dataset , Dataset ) data = dataset . iterator ( mode = 'sequential' , num_batches = 1 , data_specs = dataset . data_specs , return_tuple = True ) . next ( ) if len ( data ) == 2 : X , y = data y = np . squeeze ( y ) if self . one_hot : y = np . argmax ( y , axis = 1 ) else : X = data y = None return X , y
8327	def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except ValueError : pass lastChild = self . _lastRecursiveChild ( ) nextElement = lastChild . next if self . previous : self . previous . next = nextElement if nextElement : nextElement . previous = self . previous self . previous = None lastChild . next = None self . parent = None if self . previousSibling : self . previousSibling . nextSibling = self . nextSibling if self . nextSibling : self . nextSibling . previousSibling = self . previousSibling self . previousSibling = self . nextSibling = None return self
479	def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id
10072	def record_schema ( self ) : schema_path = current_jsonschemas . url_to_path ( self [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path and schema_path . startswith ( schema_prefix ) : return current_jsonschemas . path_to_url ( schema_path [ len ( schema_prefix ) : ] )
4274	def thumbnail ( self ) : if self . _thumbnail : return self . _thumbnail thumbnail = self . meta . get ( 'thumbnail' , [ '' ] ) [ 0 ] if thumbnail and isfile ( join ( self . src_path , thumbnail ) ) : self . _thumbnail = url_from_path ( join ( self . name , get_thumb ( self . settings , thumbnail ) ) ) self . logger . debug ( "Thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail else : for f in self . medias : ext = splitext ( f . filename ) [ 1 ] if ext . lower ( ) in self . settings [ 'img_extensions' ] : size = f . size if size is None : size = get_size ( f . src_path ) if size [ 'width' ] > size [ 'height' ] : self . _thumbnail = ( url_quote ( self . name ) + '/' + f . thumbnail ) self . logger . debug ( "Use 1st landscape image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail if not self . _thumbnail and self . medias : for media in self . medias : if media . thumbnail is not None : self . _thumbnail = ( url_quote ( self . name ) + '/' + media . thumbnail ) break else : self . logger . warning ( "No thumbnail found for %r" , self ) return None self . logger . debug ( "Use the 1st image as thumbnail for %r : %s" , self , self . _thumbnail ) return self . _thumbnail if not self . _thumbnail : for path , album in self . gallery . get_albums ( self . path ) : if album . thumbnail : self . _thumbnail = ( url_quote ( self . name ) + '/' + album . thumbnail ) self . logger . debug ( "Using thumbnail from sub-directory for %r : %s" , self , self . _thumbnail ) return self . _thumbnail self . logger . error ( 'Thumbnail not found for %r' , self ) return None
9488	def generate_simple_call ( opcode : int , index : int ) : bs = b"" bs += opcode . to_bytes ( 1 , byteorder = "little" ) if isinstance ( index , int ) : if PY36 : bs += index . to_bytes ( 1 , byteorder = "little" ) else : bs += index . to_bytes ( 2 , byteorder = "little" ) else : bs += index return bs
11102	def make_zip_archive ( self , dst = None , filters = all_true , compress = True , overwrite = False , makedirs = False , verbose = False ) : self . assert_exists ( ) if dst is None : dst = self . _auto_zip_archive_dst ( ) else : dst = self . change ( new_abspath = dst ) if not dst . basename . lower ( ) . endswith ( ".zip" ) : raise ValueError ( "zip archive name has to be endswith '.zip'!" ) if dst . exists ( ) : if not overwrite : raise IOError ( "'%s' already exists!" % dst ) if compress : compression = ZIP_DEFLATED else : compression = ZIP_STORED if not dst . parent . exists ( ) : if makedirs : os . makedirs ( dst . parent . abspath ) if verbose : msg = "Making zip archive for '%s' ..." % self print ( msg ) current_dir = os . getcwd ( ) if self . is_dir ( ) : total_size = 0 selected = list ( ) for p in self . glob ( "**/*" ) : if filters ( p ) : selected . append ( p ) total_size += p . size if verbose : msg = "Got {} files, total size is {}, compressing ..." . format ( len ( selected ) , repr_data_size ( total_size ) , ) print ( msg ) with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . abspath ) for p in selected : relpath = p . relative_to ( self ) . __str__ ( ) f . write ( relpath ) elif self . is_file ( ) : with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . parent . abspath ) f . write ( self . basename ) os . chdir ( current_dir ) if verbose : msg = "Complete! Archive size is {}." . format ( dst . size_in_text ) print ( msg )
1319	def IsTopLevel ( self ) -> bool : handle = self . NativeWindowHandle if handle : return GetAncestor ( handle , GAFlag . Root ) == handle return False
3821	async def delete_conversation ( self , delete_conversation_request ) : response = hangouts_pb2 . DeleteConversationResponse ( ) await self . _pb_request ( 'conversations/deleteconversation' , delete_conversation_request , response ) return response
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
12601	def _check_cols ( df , col_names ) : for col in col_names : if not hasattr ( df , col ) : raise AttributeError ( "DataFrame does not have a '{}' column, got {}." . format ( col , df . columns ) )
4032	def _randone ( d , limit = 20 , grouprefs = None ) : if grouprefs is None : grouprefs = { } ret = '' for i in d : if i [ 0 ] == sre_parse . IN : ret += choice ( _in ( i [ 1 ] ) ) elif i [ 0 ] == sre_parse . LITERAL : ret += unichr ( i [ 1 ] ) elif i [ 0 ] == sre_parse . CATEGORY : ret += choice ( CATEGORIES . get ( i [ 1 ] , [ '' ] ) ) elif i [ 0 ] == sre_parse . ANY : ret += choice ( CATEGORIES [ 'category_any' ] ) elif i [ 0 ] == sre_parse . MAX_REPEAT or i [ 0 ] == sre_parse . MIN_REPEAT : if i [ 1 ] [ 1 ] + 1 - i [ 1 ] [ 0 ] >= limit : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 0 ] + limit - 1 else : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 1 ] for _ in range ( randint ( min , max ) ) : ret += _randone ( list ( i [ 1 ] [ 2 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . BRANCH : ret += _randone ( choice ( i [ 1 ] [ 1 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . SUBPATTERN or i [ 0 ] == sre_parse . ASSERT : subexpr = i [ 1 ] [ 1 ] if IS_PY36_OR_GREATER and i [ 0 ] == sre_parse . SUBPATTERN : subexpr = i [ 1 ] [ 3 ] subp = _randone ( subexpr , limit , grouprefs ) if i [ 1 ] [ 0 ] : grouprefs [ i [ 1 ] [ 0 ] ] = subp ret += subp elif i [ 0 ] == sre_parse . AT : continue elif i [ 0 ] == sre_parse . NOT_LITERAL : c = list ( CATEGORIES [ 'category_any' ] ) if unichr ( i [ 1 ] ) in c : c . remove ( unichr ( i [ 1 ] ) ) ret += choice ( c ) elif i [ 0 ] == sre_parse . GROUPREF : ret += grouprefs [ i [ 1 ] ] elif i [ 0 ] == sre_parse . ASSERT_NOT : pass else : print ( '[!] cannot handle expression "%s"' % str ( i ) ) return ret
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
2391	def regenerate_good_tokens ( string ) : toks = nltk . word_tokenize ( string ) pos_string = nltk . pos_tag ( toks ) pos_seq = [ tag [ 1 ] for tag in pos_string ] pos_ngrams = ngrams ( pos_seq , 2 , 4 ) sel_pos_ngrams = f7 ( pos_ngrams ) return sel_pos_ngrams
9605	def fluent ( func ) : @ wraps ( func ) def fluent_interface ( instance , * args , ** kwargs ) : ret = func ( instance , * args , ** kwargs ) if ret is not None : return ret return instance return fluent_interface
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
5099	def _matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
1687	def _CollapseStrings ( elided ) : if _RE_PATTERN_INCLUDE . match ( elided ) : return elided elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES . sub ( '' , elided ) collapsed = '' while True : match = Match ( r'^([^\'"]*)([\'"])(.*)$' , elided ) if not match : collapsed += elided break head , quote , tail = match . groups ( ) if quote == '"' : second_quote = tail . find ( '"' ) if second_quote >= 0 : collapsed += head + '""' elided = tail [ second_quote + 1 : ] else : collapsed += elided break else : if Search ( r'\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$' , head ) : match_literal = Match ( r'^((?:\'?[0-9a-zA-Z_])*)(.*)$' , "'" + tail ) collapsed += head + match_literal . group ( 1 ) . replace ( "'" , '' ) elided = match_literal . group ( 2 ) else : second_quote = tail . find ( '\'' ) if second_quote >= 0 : collapsed += head + "''" elided = tail [ second_quote + 1 : ] else : collapsed += elided break return collapsed
3725	def dipole_moment ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in _dipole_CCDB . index and not np . isnan ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) : methods . append ( CCCBDB ) if CASRN in _dipole_Muller . index and not np . isnan ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) : methods . append ( MULLER ) if CASRN in _dipole_Poling . index and not np . isnan ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) : methods . append ( POLING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CCCBDB : _dipole = float ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) elif Method == MULLER : _dipole = float ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) elif Method == POLING : _dipole = float ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) elif Method == NONE : _dipole = None else : raise Exception ( 'Failure in in function' ) return _dipole
3297	def is_collection ( self , path , environ ) : res = self . get_resource_inst ( path , environ ) return res and res . is_collection
13772	def _default_json_default ( obj ) : if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time ) ) : return obj . strftime ( default_date_fmt ) else : return str ( obj )
8347	def _getAttrMap ( self ) : if not getattr ( self , 'attrMap' ) : self . attrMap = { } for ( key , value ) in self . attrs : self . attrMap [ key ] = value return self . attrMap
9313	def sign_sha256 ( key , msg ) : if isinstance ( msg , text_type ) : msg = msg . encode ( 'utf-8' ) return hmac . new ( key , msg , hashlib . sha256 ) . digest ( )
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
1935	def get_source_for ( self , asm_offset , runtime = True ) : srcmap = self . get_srcmap ( runtime ) try : beg , size , _ , _ = srcmap [ asm_offset ] except KeyError : return '' output = '' nl = self . source_code [ : beg ] . count ( '\n' ) + 1 snippet = self . source_code [ beg : beg + size ] for l in snippet . split ( '\n' ) : output += ' %s %s\n' % ( nl , l ) nl += 1 return output
3303	def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
5082	def save_statement ( self , statement ) : response = self . lrs . save_statement ( statement ) if not response : raise ClientError ( 'EnterpriseXAPIClient request failed.' )
11115	def save ( self ) : repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
9365	def email_address ( user = None ) : if not user : user = user_name ( ) else : user = user . strip ( ) . replace ( ' ' , '_' ) . lower ( ) return user + '@' + domain_name ( )
762	def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )
9899	def _data ( self ) : if self . is_caching : return self . cache with open ( self . path , "r" ) as f : return json . load ( f )
8735	def date_range ( start = None , stop = None , step = None ) : if step is None : step = datetime . timedelta ( days = 1 ) if start is None : start = datetime . datetime . now ( ) while start < stop : yield start start += step
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
11007	def get_bets ( self , type = None , order_by = None , state = None , project_id = None , page = None , page_size = None ) : if page is None : page = 1 if page_size is None : page_size = 100 if state == 'all' : _states = [ ] elif state == 'closed' : _states = self . CLOSED_STATES else : _states = self . ACTIVE_STATES url = urljoin ( self . settings [ 'bets_url' ] , 'bets?page={}&page_size={}' . format ( page , page_size ) ) url += '&state={}' . format ( ',' . join ( _states ) ) if type is not None : url += '&type={}' . format ( type ) if order_by in [ '-last_stake' , 'last_stake' ] : url += '&order_by={}' . format ( order_by ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) res = self . _req ( url ) return res [ 'bets' ] [ 'results' ]
11702	def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
12328	def init_repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive_template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv_filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv_filename , 'w' ) as fd : fd . write ( content ) self . make_hook_executable ( postrecv_filename ) print ( "Wrote to" , postrecv_filename )
10579	def add_to ( self , other ) : if type ( other ) is MaterialPackage : if self . material == other . material : self . compound_masses += other . compound_masses else : for compound in other . material . compounds : if compound not in self . material . compounds : raise Exception ( "Packages of '" + other . material . name + "' cannot be added to packages of '" + self . material . name + "'. The compound '" + compound + "' was not found in '" + self . material . name + "'." ) self . add_to ( ( compound , other . get_compound_mass ( compound ) ) ) elif self . _is_compound_mass_tuple ( other ) : compound = other [ 0 ] compound_index = self . material . get_compound_index ( compound ) mass = other [ 1 ] self . compound_masses [ compound_index ] += mass else : raise TypeError ( 'Invalid addition argument.' )
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
13088	def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) logger . info ( "DFK cleanup complete" )
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
6901	def _parse_xmatch_catalog_header ( xc , xk ) : catdef = [ ] if xc . endswith ( '.gz' ) : infd = gzip . open ( xc , 'rb' ) else : infd = open ( xc , 'rb' ) for line in infd : if line . decode ( ) . startswith ( '#' ) : catdef . append ( line . decode ( ) . replace ( '#' , '' ) . strip ( ) . rstrip ( '\n' ) ) if not line . decode ( ) . startswith ( '#' ) : break if not len ( catdef ) > 0 : LOGERROR ( "catalog definition not parseable " "for catalog: %s, skipping..." % xc ) return None catdef = ' ' . join ( catdef ) catdefdict = json . loads ( catdef ) catdefkeys = [ x [ 'key' ] for x in catdefdict [ 'columns' ] ] catdefdtypes = [ x [ 'dtype' ] for x in catdefdict [ 'columns' ] ] catdefnames = [ x [ 'name' ] for x in catdefdict [ 'columns' ] ] catdefunits = [ x [ 'unit' ] for x in catdefdict [ 'columns' ] ] catcolinds = [ ] catcoldtypes = [ ] catcolnames = [ ] catcolunits = [ ] for xkcol in xk : if xkcol in catdefkeys : xkcolind = catdefkeys . index ( xkcol ) catcolinds . append ( xkcolind ) catcoldtypes . append ( catdefdtypes [ xkcolind ] ) catcolnames . append ( catdefnames [ xkcolind ] ) catcolunits . append ( catdefunits [ xkcolind ] ) return ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits )
12892	def handle_long ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
12170	def emit ( self , event , * args , ** kwargs ) : listeners = self . _listeners [ event ] listeners = itertools . chain ( listeners , self . _once [ event ] ) self . _once [ event ] = [ ] for listener in listeners : self . _loop . call_soon ( functools . partial ( self . _dispatch , event , listener , * args , ** kwargs , ) ) return self
5037	def _handle_bulk_upload ( cls , enterprise_customer , manage_learners_form , request , email_list = None ) : errors = [ ] emails = set ( ) already_linked_emails = [ ] duplicate_emails = [ ] csv_file = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . BULK_UPLOAD ] if email_list : parsed_csv = [ { ManageLearnersForm . CsvColumns . EMAIL : email } for email in email_list ] else : parsed_csv = parse_csv ( csv_file , expected_columns = { ManageLearnersForm . CsvColumns . EMAIL } ) try : for index , row in enumerate ( parsed_csv ) : email = row [ ManageLearnersForm . CsvColumns . EMAIL ] try : already_linked = validate_email_to_link ( email , ignore_existing = True ) except ValidationError as exc : message = _ ( "Error at line {line}: {message}\n" ) . format ( line = index + 1 , message = exc ) errors . append ( message ) else : if already_linked : already_linked_emails . append ( ( email , already_linked . enterprise_customer ) ) elif email in emails : duplicate_emails . append ( email ) else : emails . add ( email ) except ValidationError as exc : errors . append ( exc ) if errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . GENERAL_ERRORS , ValidationMessages . BULK_LINK_FAILED ) for error in errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . BULK_UPLOAD , error ) return for email in emails : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) count = len ( emails ) messages . success ( request , ungettext ( "{count} new learner was added to {enterprise_customer_name}." , "{count} new learners were added to {enterprise_customer_name}." , count ) . format ( count = count , enterprise_customer_name = enterprise_customer . name ) ) this_customer_linked_emails = [ email for email , customer in already_linked_emails if customer == enterprise_customer ] other_customer_linked_emails = [ email for email , __ in already_linked_emails if email not in this_customer_linked_emails ] if this_customer_linked_emails : messages . warning ( request , _ ( "The following learners were already associated with this Enterprise " "Customer: {list_of_emails}" ) . format ( list_of_emails = ", " . join ( this_customer_linked_emails ) ) ) if other_customer_linked_emails : messages . warning ( request , _ ( "The following learners are already associated with " "another Enterprise Customer. These learners were not " "added to {enterprise_customer_name}: {list_of_emails}" ) . format ( enterprise_customer_name = enterprise_customer . name , list_of_emails = ", " . join ( other_customer_linked_emails ) , ) ) if duplicate_emails : messages . warning ( request , _ ( "The following duplicate email addresses were not added: " "{list_of_emails}" ) . format ( list_of_emails = ", " . join ( duplicate_emails ) ) ) all_processable_emails = list ( emails ) + this_customer_linked_emails return all_processable_emails
8716	def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
9749	def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
5210	def bdp ( tickers , flds , ** kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( ** kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , ** kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
2422	def str_from_text ( text ) : REGEX = re . compile ( '<text>((.|\n)+)</text>' , re . UNICODE ) match = REGEX . match ( text ) if match : return match . group ( 1 ) else : return None
7237	def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ 0 ] , self . shape [ 1 ] ) col = random . randrange ( window_shape [ 1 ] , self . shape [ 2 ] ) return self [ : , row - window_shape [ 0 ] : row , col - window_shape [ 1 ] : col ]
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
2116	def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : return file_obj . read ( ) return file_obj return value
6974	def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = False , epdsmooth = True , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , rf_subsample = 1.0 , rf_ntrees = 300 , rf_extraparams = { 'criterion' : 'mse' , 'oob_score' : False , 'n_jobs' : - 1 } ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] finalparam_arrs = [ ] for ep in externalparam_arrs : finalparam_arrs . append ( ep [ : : ] [ finind ] ) stimes , smags , serrs , eparams = sigclip_magseries_with_extparams ( times , mags , errs , externalparam_arrs , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) if epdsmooth : if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) else : smoothedmags = smags if isinstance ( rf_extraparams , dict ) : RFR = RandomForestRegressor ( n_estimators = rf_ntrees , ** rf_extraparams ) else : RFR = RandomForestRegressor ( n_estimators = rf_ntrees ) features = np . column_stack ( eparams ) if rf_subsample < 1.0 : featureindices = np . arange ( smoothedmags . size ) training_indices = np . sort ( npr . choice ( featureindices , size = int ( rf_subsample * smoothedmags . size ) , replace = False ) ) else : training_indices = np . arange ( smoothedmags . size ) RFR . fit ( features [ training_indices , : ] , smoothedmags [ training_indices ] ) flux_corrections = RFR . predict ( np . column_stack ( finalparam_arrs ) ) corrected_fmags = npmedian ( fmags ) + fmags - flux_corrections retdict = { 'times' : ftimes , 'mags' : corrected_fmags , 'errs' : ferrs , 'feature_importances' : RFR . feature_importances_ , 'regressor' : RFR , 'mags_median' : npmedian ( corrected_fmags ) , 'mags_mad' : npmedian ( npabs ( corrected_fmags - npmedian ( corrected_fmags ) ) ) } return retdict
3076	def credentials ( self ) : ctx = _app_ctx_stack . top if not hasattr ( ctx , _CREDENTIALS_KEY ) : ctx . google_oauth2_credentials = self . storage . get ( ) return ctx . google_oauth2_credentials
3468	def _update_awareness ( self ) : for x in self . _metabolites : x . _reaction . add ( self ) for x in self . _genes : x . _reaction . add ( self )
8664	def _build_dict_from_key_value ( keys_and_values ) : key_dict = { } for key_value in keys_and_values : if '=' not in key_value : raise GhostError ( 'Pair {0} is not of `key=value` format' . format ( key_value ) ) key , value = key_value . split ( '=' , 1 ) key_dict . update ( { str ( key ) : str ( value ) } ) return key_dict
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
11282	def append ( self , next ) : next . chained = True if self . next : self . next . append ( next ) else : self . next = next
86	def is_single_float ( val ) : return isinstance ( val , numbers . Real ) and not is_single_integer ( val ) and not isinstance ( val , bool )
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
9363	def user_name ( with_num = False ) : result = first_name ( ) if with_num : result += str ( random . randint ( 63 , 94 ) ) return result . lower ( )
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
7270	def use ( plugin ) : log . debug ( 'register new plugin: {}' . format ( plugin ) ) if inspect . isfunction ( plugin ) : return plugin ( Engine ) if plugin and hasattr ( plugin , 'register' ) : return plugin . register ( Engine ) raise ValueError ( 'invalid plugin: must be a function or ' 'implement register() method' )
527	def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
5910	def delete_frames ( self ) : for frame in glob . glob ( self . frameglob ) : os . unlink ( frame )
12592	def query_reliabledictionary ( client , application_name , service_name , dictionary_name , query_string , partition_key = None , partition_id = None , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) start = time . time ( ) if ( partition_id != None ) : result = dictionary . query ( query_string , PartitionLookup . ID , partition_id ) elif ( partition_key != None ) : result = dictionary . query ( query_string , PartitionLookup . KEY , partition_key ) else : result = dictionary . query ( query_string ) if type ( result ) is str : print ( result ) return else : result = json . dumps ( result . get ( "value" ) , indent = 4 ) print ( "Query took " + str ( time . time ( ) - start ) + " seconds" ) if ( output_file == None ) : output_file = "{}-{}-{}-query-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( ) print ( 'Printed output to: ' + output_file ) print ( result )
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) if not num or num > len ( npar ) : res += '$' + dig else : res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( lcat , ccat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
11062	def send_im ( self , user , text ) : if isinstance ( user , SlackUser ) : user = user . id channelid = self . _find_im_channel ( user ) else : channelid = user . id self . send_message ( channelid , text )
11523	def extract_dicommetadata ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'item' ] = item_id response = self . request ( 'midas.dicomextractor.extract' , parameters ) return response
8862	def quick_doc ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_definitions ( ) except jedi . NotFoundError : return [ ] else : ret_val = [ d . docstring ( ) for d in definitions ] return ret_val
9703	def monitorTUN ( self ) : packet = self . checkTUN ( ) if packet : try : ret = self . _faraday . send ( packet ) return ret except AttributeError as error : print ( "AttributeError" )
13786	def require ( name , field , data_type ) : if not isinstance ( field , data_type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data_type , field ) raise AssertionError ( msg )
13879	def AppendToFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , encoding = None , binary = False ) : _AssertIsLocal ( filename ) assert isinstance ( contents , six . text_type ) ^ binary , 'Must always receive unicode contents, unless binary=True' if not binary : contents = _HandleContentsEol ( contents , eol_style ) contents = contents . encode ( encoding or sys . getfilesystemencoding ( ) ) oss = open ( filename , 'ab' ) try : oss . write ( contents ) finally : oss . close ( )
444	def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
7888	def error ( self , stanza ) : err = stanza . get_error ( ) self . __logger . debug ( "Error from: %r Condition: %r" % ( stanza . get_from ( ) , err . get_condition ) )
6082	def deflections_of_galaxies_from_grid ( grid , galaxies ) : if len ( galaxies ) > 0 : deflections = sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( grid ) , galaxies ) ) else : deflections = np . full ( ( grid . shape [ 0 ] , 2 ) , 0.0 ) if isinstance ( grid , grids . SubGrid ) : return np . asarray ( [ grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 0 ] ) , grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 1 ] ) ] ) . T return deflections
4111	def rc2poly ( kr , r0 = None ) : from . levinson import levup p = len ( kr ) a = numpy . array ( [ 1 , kr [ 0 ] ] ) e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
13644	def append_arguments ( klass , sub_parsers , default_epilog , general_arguments ) : entry_name = hump_to_underscore ( klass . __name__ ) . replace ( '_component' , '' ) epilog = default_epilog if default_epilog else 'This tool generate by `cliez` ' 'https://www.github.com/wangwenpei/cliez' sub_parser = sub_parsers . add_parser ( entry_name , help = klass . __doc__ , epilog = epilog ) sub_parser . description = klass . add_arguments . __doc__ if hasattr ( klass , 'add_slot_args' ) : slot_args = klass . add_slot_args ( ) or [ ] for v in slot_args : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) sub_parser . description = klass . add_slot_args . __doc__ pass user_arguments = klass . add_arguments ( ) or [ ] for v in user_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) if not klass . exclude_global_option : for v in general_arguments : sub_parser . add_argument ( * v [ 0 ] , ** v [ 1 ] ) return sub_parser
13502	def extra_context ( request ) : host = os . environ . get ( 'DJANGO_LIVE_TEST_SERVER_ADDRESS' , None ) or request . get_host ( ) d = { 'request' : request , 'HOST' : host , 'IN_ADMIN' : request . path . startswith ( '/admin/' ) , } return d
10949	def get_shares ( self ) : self . shares = self . api . get ( url = PATHS [ 'GET_SHARES' ] % self . url ) [ 'shares' ] return self . shares
1782	def AAS ( cpu ) : if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : cpu . AX = cpu . AX - 6 cpu . AH = cpu . AH - 1 cpu . AF = True cpu . CF = True else : cpu . AF = False cpu . CF = False cpu . AL = cpu . AL & 0x0f
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
3261	def get_style ( self , name , workspace = None ) : styles = self . get_styles ( names = name , workspaces = workspace ) return self . _return_first_item ( styles )
9326	def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( ** response ) self . _loaded = True
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
10466	def getFrontmostApp ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) try : if ref . AXFrontmost : return ref except ( _a11y . ErrorUnsupported , _a11y . ErrorCannotComplete , _a11y . ErrorAPIDisabled , _a11y . ErrorNotImplemented ) : pass raise ValueError ( 'No GUI application found.' )
81	def ImpulseNoise ( p = 0 , name = None , deterministic = False , random_state = None ) : return SaltAndPepper ( p = p , per_channel = True , name = name , deterministic = deterministic , random_state = random_state )
2878	def serialize_value ( self , parent_elem , value ) : if isinstance ( value , ( str , int ) ) or type ( value ) . __name__ == 'str' : parent_elem . text = str ( value ) elif value is None : parent_elem . text = None else : parent_elem . append ( value . serialize ( self ) )
11369	def convert_date_to_iso ( value ) : date_formats = [ "%d %b %Y" , "%Y/%m/%d" ] for dformat in date_formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( "%Y-%m-%d" ) except ValueError : pass return value
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps return average
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
2350	def wait_for_page_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_page_to_load ( page = self ) return self
6324	def train ( self , text ) : r text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) counts = Counter ( text ) counts [ '\x00' ] = 1 tot_letters = sum ( counts . values ( ) ) tot = 0 self . _probs = { } prev = Fraction ( 0 ) for char , count in sorted ( counts . items ( ) , key = lambda x : ( x [ 1 ] , x [ 0 ] ) , reverse = True ) : follow = Fraction ( tot + count , tot_letters ) self . _probs [ char ] = ( prev , follow ) prev = follow tot = tot + count
1783	def ADC ( cpu , dest , src ) : cpu . _ADD ( dest , src , carry = True )
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
1748	def _get_offset ( self , index ) : if not self . _in_range ( index ) : raise IndexError ( 'Map index out of range' ) if isinstance ( index , slice ) : index = slice ( index . start - self . start , index . stop - self . start ) else : index -= self . start return index
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
8101	def copy ( self , graph ) : g = styleguide ( graph ) g . order = self . order dict . __init__ ( g , [ ( k , v ) for k , v in self . iteritems ( ) ] ) return g
2917	def _eval_args ( args , my_task ) : results = [ ] for arg in args : if isinstance ( arg , Attrib ) or isinstance ( arg , PathAttrib ) : results . append ( valueof ( my_task , arg ) ) else : results . append ( arg ) return results
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
11844	def run ( self , steps = 1000 ) : "Run the Environment for given number of time steps." for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
2519	def p_file_comments_on_lics ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_license_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comments on license' )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
1999	def _method ( self , expression , * args ) : assert expression . __class__ . __mro__ [ - 1 ] is object for cls in expression . __class__ . __mro__ : sort = cls . __name__ methodname = 'visit_%s' % sort method = getattr ( self , methodname , None ) if method is not None : method ( expression , * args ) return return
10458	def clearContents ( cls ) : log_msg = 'Request to clear contents of pasteboard: general' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) pb . clearContents ( ) return True
8789	def _pop ( self , model ) : tags = [ ] for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) if tags : for tag in tags : model . tags . remove ( tag ) return tags
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : try : if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) self . _set_vdbconfig_path ( ) if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : self . _restore_vdbconfig_path ( ) sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : raise IPyradWarningExit ( "Download failed. Exiting." ) for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) shutil . rmtree ( sradir ) if ipyclient : try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass if not ipyclient . outstanding : ipyclient . purge_everything ( ) else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
3560	def list_services ( self ) : return map ( BluezGattService , get_provider ( ) . _get_objects ( _SERVICE_INTERFACE , self . _device . object_path ) )
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
9098	def write_bel_namespace ( self , file : TextIO , use_names : bool = False ) -> None : if not self . is_populated ( ) : self . populate ( ) if use_names and not self . has_names : raise ValueError values = ( self . _get_namespace_name_to_encoding ( desc = 'writing names' ) if use_names else self . _get_namespace_identifier_to_encoding ( desc = 'writing identifiers' ) ) write_namespace ( namespace_name = self . _get_namespace_name ( ) , namespace_keyword = self . _get_namespace_keyword ( ) , namespace_query_url = self . identifiers_url , values = values , file = file , )
11320	def update_date_year ( self ) : dates = record_get_field_instances ( self . record , '260' ) for field in dates : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'c' : field [ 0 ] [ idx ] = ( 'c' , value [ : 4 ] ) elif key == 't' : del field [ 0 ] [ idx ] if not dates : published_years = record_get_field_values ( self . record , "773" , code = "y" ) if published_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , published_years [ 0 ] [ : 4 ] ) ] ) else : other_years = record_get_field_values ( self . record , "269" , code = "c" ) if other_years : record_add_field ( self . record , "260" , subfields = [ ( "c" , other_years [ 0 ] [ : 4 ] ) ] )
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
2926	def create_package ( self ) : self . input_path_prefix = None for filename in self . input_files : if not os . path . isfile ( filename ) : raise ValueError ( '%s does not exist or is not a file' % filename ) if self . input_path_prefix : full = os . path . abspath ( os . path . dirname ( filename ) ) while not ( full . startswith ( self . input_path_prefix ) and self . input_path_prefix ) : self . input_path_prefix = self . input_path_prefix [ : - 1 ] else : self . input_path_prefix = os . path . abspath ( os . path . dirname ( filename ) ) self . bpmn = { } for filename in self . input_files : bpmn = ET . parse ( filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn for filename , bpmn in list ( self . bpmn . items ( ) ) : bpmn = self . pre_parse_and_validate ( bpmn , filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn for filename , bpmn in list ( self . bpmn . items ( ) ) : self . parser . add_bpmn_xml ( bpmn , filename = filename ) self . wf_spec = self . parser . get_spec ( self . entry_point_process ) self . package_zip = zipfile . ZipFile ( self . package_file , "w" , compression = zipfile . ZIP_DEFLATED ) done_files = set ( ) for spec in self . wf_spec . get_specs_depth_first ( ) : filename = spec . file if filename not in done_files : done_files . add ( filename ) bpmn = self . bpmn [ os . path . abspath ( filename ) ] self . write_to_package_zip ( "%s.bpmn" % spec . name , ET . tostring ( bpmn . getroot ( ) ) ) self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( filename ) , filename ) self . _call_editor_hook ( 'package_for_editor' , spec , filename ) self . write_meta_data ( ) self . write_manifest ( ) self . package_zip . close ( )
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
12928	def _parse_info ( self , info_field ) : info = dict ( ) for item in info_field . split ( ';' ) : info_item_data = item . split ( '=' ) if len ( info_item_data ) == 1 : info [ info_item_data [ 0 ] ] = True elif len ( info_item_data ) == 2 : info [ info_item_data [ 0 ] ] = info_item_data [ 1 ] return info
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
10390	def workflow ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , minimum_nodes : int = 1 , ) -> List [ 'Runner' ] : subgraph = generate_mechanism ( graph , node , key = key ) if subgraph . number_of_nodes ( ) <= minimum_nodes : return [ ] runners = multirun ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) return list ( runners )
1183	def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context
61	def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0
13244	def temp_db ( db , name = None ) : if name is None : name = temp_name ( ) db . create ( name ) if not db . exists ( name ) : raise DatabaseError ( 'failed to create database %s!' ) try : yield name finally : db . drop ( name ) if db . exists ( name ) : raise DatabaseError ( 'failed to drop database %s!' )
10742	def print_profile ( function ) : import memory_profiler def wrapper ( * args , ** kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , ** kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
9361	def characters ( quantity = 10 ) : line = map ( _to_lower_alpha_only , '' . join ( random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ) ) return '' . join ( line ) [ : quantity ]
150	def deepcopy ( self ) : polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
5204	def parse_markdown ( ) : readme_file = f'{PACKAGE_ROOT}/README.md' if path . exists ( readme_file ) : with open ( readme_file , 'r' , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
6884	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = [ ( lctimes [ x ] - lctimes [ x - 1 ] ) for x in range ( 1 , len ( lctimes ) ) ] lc_time_diffs = np . array ( lc_time_diffs ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
1198	def tf_loss ( self , states , internals , reward , update , reference = None ) : prediction = self . predict ( states = states , internals = internals , update = update ) return tf . nn . l2_loss ( t = ( prediction - reward ) )
7132	def prepare_docset ( source , dest , name , index_page , enable_js , online_redirect_url ) : resources = os . path . join ( dest , "Contents" , "Resources" ) docs = os . path . join ( resources , "Documents" ) os . makedirs ( resources ) db_conn = sqlite3 . connect ( os . path . join ( resources , "docSet.dsidx" ) ) db_conn . row_factory = sqlite3 . Row db_conn . execute ( "CREATE TABLE searchIndex(id INTEGER PRIMARY KEY, name TEXT, " "type TEXT, path TEXT)" ) db_conn . commit ( ) plist_path = os . path . join ( dest , "Contents" , "Info.plist" ) plist_cfg = { "CFBundleIdentifier" : name , "CFBundleName" : name , "DocSetPlatformFamily" : name . lower ( ) , "DashDocSetFamily" : "python" , "isDashDocset" : True , "isJavaScriptEnabled" : enable_js , } if index_page is not None : plist_cfg [ "dashIndexFilePath" ] = index_page if online_redirect_url is not None : plist_cfg [ "DashDocSetFallbackURL" ] = online_redirect_url write_plist ( plist_cfg , plist_path ) shutil . copytree ( source , docs ) return DocSet ( path = dest , docs = docs , plist = plist_path , db_conn = db_conn )
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
634	def destroySynapse ( self , synapse ) : self . _numSynapses -= 1 self . _removeSynapseFromPresynapticMap ( synapse ) synapse . segment . _synapses . remove ( synapse )
7299	def get_qset ( self , queryset , q ) : if self . mongoadmin . search_fields and q : params = { } for field in self . mongoadmin . search_fields : if field == 'id' : if is_valid_object_id ( q ) : return queryset . filter ( pk = q ) continue search_key = "{field}__icontains" . format ( field = field ) params [ search_key ] = q queryset = queryset . filter ( ** params ) return queryset
2666	def write ( self , buf , ** kwargs ) : self . i2c . writeto ( self . device_address , buf , ** kwargs ) if self . _debug : print ( "i2c_device.write:" , [ hex ( i ) for i in buf ] )
7675	def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , ** kwargs ) string = re . sub ( r'[{}"]' , '' , string ) string = re . sub ( r',\n' , '\n' , string ) string = re . sub ( r'^\s*$' , '' , string ) return string
6320	def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
4342	def reverse ( self ) : effect_args = [ 'reverse' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'reverse' ) return self
9927	def get_user ( self , user_id ) : try : return get_user_model ( ) . objects . get ( id = user_id ) except get_user_model ( ) . DoesNotExist : return None
7711	def _get_success ( self , stanza ) : payload = stanza . get_payload ( RosterPayload ) if payload is None : if "versioning" in self . server_features and self . roster : logger . debug ( "Server will send roster delta in pushes" ) else : logger . warning ( "Bad roster response (no payload)" ) self . _event_queue . put ( RosterNotReceivedEvent ( self , stanza ) ) return else : items = list ( payload ) for item in items : item . verify_roster_result ( True ) self . roster = Roster ( items , payload . version ) self . _event_queue . put ( RosterReceivedEvent ( self , self . roster ) )
569	def _getReportItem ( itemName , results ) : subKeys = itemName . split ( ':' ) subResults = results for subKey in subKeys : subResults = subResults [ subKey ] return subResults
6158	def FIR_fix_header ( fname_out , h ) : M = len ( h ) hq = int16 ( rint ( h * 2 ** 15 ) ) N = 8 f = open ( fname_out , 'wt' ) f . write ( '//define a FIR coefficient Array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef M_FIR\n' ) f . write ( '#define M_FIR %d\n' % M ) f . write ( '#endif\n' ) f . write ( '/************************************************************************/\n' ) f . write ( '/* FIR Filter Coefficients */\n' ) f . write ( 'int16_t h_FIR[M_FIR] = {' ) kk = 0 for k in range ( M ) : if ( kk < N - 1 ) and ( k < M - 1 ) : f . write ( '%5d,' % hq [ k ] ) kk += 1 elif ( kk == N - 1 ) & ( k < M - 1 ) : f . write ( '%5d,\n' % hq [ k ] ) if k < M : f . write ( ' ' ) kk = 0 else : f . write ( '%5d' % hq [ k ] ) f . write ( '};\n' ) f . write ( '/************************************************************************/\n' ) f . close ( )
13860	def is_date_type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
8805	def build_full_day_ips ( query , period_start , period_end ) : ip_list = query . filter ( models . IPAddress . version == 4L ) . filter ( models . IPAddress . network_id == PUBLIC_NETWORK_ID ) . filter ( models . IPAddress . used_by_tenant_id is not None ) . filter ( models . IPAddress . allocated_at != null ( ) ) . filter ( models . IPAddress . allocated_at < period_start ) . filter ( or_ ( models . IPAddress . _deallocated is False , models . IPAddress . deallocated_at == null ( ) , models . IPAddress . deallocated_at >= period_end ) ) . all ( ) return ip_list
8595	def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url = '/um/groups' , method = 'POST' , data = data ) return response
8305	def live_source_load ( self , source ) : source = source . rstrip ( '\n' ) if source != self . source : self . source = source b64_source = base64 . b64encode ( bytes ( bytearray ( source , "ascii" ) ) ) self . send_command ( CMD_LOAD_BASE64 , b64_source )
5982	def output_figure ( array , as_subplot , output_path , output_filename , output_format ) : if not as_subplot : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : array_util . numpy_array_2d_to_fits ( array_2d = array , file_path = output_path + output_filename + '.fits' , overwrite = True )
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] nalln = np . all ( seq1 == "N" , axis = 1 ) nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
11166	def unusedoptions ( self , sections ) : unused = set ( [ ] ) for section in _list ( sections ) : if not self . has_section ( section ) : continue options = self . options ( section ) raw_values = [ self . get ( section , option , raw = True ) for option in options ] for option in options : formatter = "%(" + option + ")s" for raw_value in raw_values : if formatter in raw_value : break else : unused . add ( option ) return list ( unused )
10291	def enrich_unqualified ( graph : BELGraph ) : enrich_complexes ( graph ) enrich_composites ( graph ) enrich_reactions ( graph ) enrich_variants ( graph )
5579	def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( ** current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
12764	def attach ( self , frame_no ) : assert not self . joints for label , j in self . channels . items ( ) : target = self . targets . get ( label ) if target is None : continue if self . visibility [ frame_no , j ] < 0 : continue if np . linalg . norm ( self . velocities [ frame_no , j ] ) > 10 : continue joint = ode . BallJoint ( self . world . ode_world , self . jointgroup ) joint . attach ( self . bodies [ label ] . ode_body , target . ode_body ) joint . setAnchor1Rel ( [ 0 , 0 , 0 ] ) joint . setAnchor2Rel ( self . offsets [ label ] ) joint . setParam ( ode . ParamCFM , self . cfms [ frame_no , j ] ) joint . setParam ( ode . ParamERP , self . erp ) joint . name = label self . joints [ label ] = joint self . _frame_no = frame_no
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
10908	def circles ( st , layer , axis , ax = None , talpha = 1.0 , cedge = 'white' , cface = 'white' ) : pos = st . obj_get_positions ( ) rad = st . obj_get_radii ( ) shape = st . ishape . shape . tolist ( ) shape . pop ( axis ) if ax is None : fig = plt . figure ( ) axisbg = 'white' if cface == 'black' else 'black' sx , sy = ( ( 1 , shape [ 1 ] / float ( shape [ 0 ] ) ) if shape [ 0 ] > shape [ 1 ] else ( shape [ 0 ] / float ( shape [ 1 ] ) , 1 ) ) ax = fig . add_axes ( ( 0 , 0 , sx , sy ) , axisbg = axisbg ) particles = np . arange ( len ( pos ) ) [ np . abs ( pos [ : , axis ] - layer ) < rad ] scale = 1.0 for i in particles : p = pos [ i ] . copy ( ) r = 2 * np . sqrt ( rad [ i ] ** 2 - ( p [ axis ] - layer ) ** 2 ) if axis == 0 : ix = 1 iy = 2 elif axis == 1 : ix = 0 iy = 2 elif axis == 2 : ix = 0 iy = 1 c = Circle ( ( p [ ix ] / scale , p [ iy ] / scale ) , radius = r / 2 / scale , fc = cface , ec = cedge , alpha = talpha ) ax . add_patch ( c ) plt . axis ( 'equal' ) return ax
9169	def parse_archive_uri ( uri ) : parsed = urlparse ( uri ) path = parsed . path . rstrip ( '/' ) . split ( '/' ) ident_hash = path [ - 1 ] ident_hash = unquote ( ident_hash ) return ident_hash
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
8589	def stop_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/stop' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
1272	def create_distributions ( self ) : distributions = dict ( ) for name in sorted ( self . actions_spec ) : action = self . actions_spec [ name ] if self . distributions_spec is not None and name in self . distributions_spec : kwargs = dict ( action ) kwargs [ 'scope' ] = name kwargs [ 'summary_labels' ] = self . summary_labels distributions [ name ] = Distribution . from_spec ( spec = self . distributions_spec [ name ] , kwargs = kwargs ) elif action [ 'type' ] == 'bool' : distributions [ name ] = Bernoulli ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'int' : distributions [ name ] = Categorical ( shape = action [ 'shape' ] , num_actions = action [ 'num_actions' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'float' : if 'min_value' in action : distributions [ name ] = Beta ( shape = action [ 'shape' ] , min_value = action [ 'min_value' ] , max_value = action [ 'max_value' ] , scope = name , summary_labels = self . summary_labels ) else : distributions [ name ] = Gaussian ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) return distributions
3101	def _SendRecv ( ) : port = int ( os . getenv ( DEVSHELL_ENV , 0 ) ) if port == 0 : raise NoDevshellServer ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL_INFO_REQUEST_JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall ( _helpers . _to_bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\n' not in header : raise CommunicationError ( 'saw no newline in the first 6 bytes' ) len_str , json_str = header . split ( '\n' , 1 ) to_read = int ( len_str ) - len ( json_str ) if to_read > 0 : json_str += sock . recv ( to_read , socket . MSG_WAITALL ) . decode ( ) return CredentialInfoResponse ( json_str )
5064	def update_query_parameters ( url , query_parameters ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) url_params = parse_qs ( query_string ) url_params . update ( query_parameters ) return urlunsplit ( ( scheme , netloc , path , urlencode ( sorted ( url_params . items ( ) ) , doseq = True ) , fragment ) , )
11181	def acquire ( self , * args , ** kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , ** kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
10740	def add_runtime ( function ) : def wrapper ( * args , ** kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , ** kwargs ) pr . disable ( ) return pr , output return wrapper
1490	def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , ** kwargs ) return inner
13881	def MoveDirectory ( source_dir , target_dir ) : if not IsDir ( source_dir ) : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( source_dir ) if Exists ( target_dir ) : from . _exceptions import DirectoryAlreadyExistsError raise DirectoryAlreadyExistsError ( target_dir ) from six . moves . urllib . parse import urlparse source_url = urlparse ( source_dir ) target_url = urlparse ( target_dir ) if _UrlIsLocal ( source_url ) and _UrlIsLocal ( target_url ) : import shutil shutil . move ( source_dir , target_dir ) elif source_url . scheme == 'ftp' and target_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( target_url . scheme ) else : raise NotImplementedError ( 'Can only move directories local->local or ftp->ftp' )
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
13510	def pyflakes ( ) : packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
13199	def format_content ( self , format = 'plain' , mathjax = False , smart = True , extra_args = None ) : output_text = convert_lsstdoc_tex ( self . _tex , format , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
10262	def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
7207	def execute ( self ) : self . generate_workflow_description ( ) if self . batch_values : self . id = self . workflow . launch_batch_workflow ( self . definition ) else : self . id = self . workflow . launch ( self . definition ) return self . id
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval_seconds ) slept = True break if not slept : break
13862	def totz ( when , tz = None ) : if when is None : return None when = to_datetime ( when ) if when . tzinfo is None : when = when . replace ( tzinfo = localtz ) return when . astimezone ( tz or utc )
3137	def create ( self , data ) : self . app_id = None if 'client_id' not in data : raise KeyError ( 'The authorized app must have a client_id' ) if 'client_secret' not in data : raise KeyError ( 'The authorized app must have a client_secret' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
5277	def query ( self , i , j ) : "Query the oracle to find out whether i and j should be must-linked" if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
3185	def get ( self , store_id , product_id , image_id , ** queryparams ) : self . store_id = store_id self . product_id = product_id self . image_id = image_id return self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' , image_id ) , ** queryparams )
2015	def _rollback ( self ) : last_pc , last_gas , last_instruction , last_arguments , fee , allocated = self . _checkpoint_data self . _push_arguments ( last_arguments ) self . _gas = last_gas self . _pc = last_pc self . _allocated = allocated self . _checkpoint_data = None
11120	def get_file_relative_path_by_id ( self , id ) : for path , info in self . walk_files_info ( ) : if info [ 'id' ] == id : return path return None
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
7819	def dispatch ( self , block = False , timeout = None ) : logger . debug ( " dispatching..." ) try : event = self . queue . get ( block , timeout ) except Queue . Empty : logger . debug ( " queue empty" ) return None try : logger . debug ( " event: {0!r}" . format ( event ) ) if event is QUIT : return QUIT handlers = list ( self . _handler_map [ None ] ) klass = event . __class__ if klass in self . _handler_map : handlers += self . _handler_map [ klass ] logger . debug ( " handlers: {0!r}" . format ( handlers ) ) handlers . sort ( key = lambda x : x [ 0 ] ) for dummy , handler in handlers : logger . debug ( u" passing the event to: {0!r}" . format ( handler ) ) result = handler ( event ) if isinstance ( result , Event ) : self . queue . put ( result ) elif result and event is not QUIT : return event return event finally : self . queue . task_done ( )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
2219	def register ( self , type ) : def _decorator ( func ) : if isinstance ( type , tuple ) : for t in type : self . func_registry [ t ] = func else : self . func_registry [ type ] = func return func return _decorator
6080	def convergence_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . convergence_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
8728	def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )
954	def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) self . models_name = self . kwargs . get ( 'models_name' , 'models' ) self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
10896	def load_image ( self ) : try : image = initializers . load_tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float_precision ) except IOError as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
9118	def _add_admin ( self , app , ** kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , ** kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
13887	def CreateDirectory ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) if _UrlIsLocal ( directory_url ) : if not os . path . exists ( directory ) : os . makedirs ( directory ) return directory elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
7490	def get_targets ( ipyclient ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] hostset = set ( hosts ) hostzip = zip ( hosts , ipyclient . ids ) hostdict = { host : [ i [ 1 ] for i in hostzip if i [ 0 ] == host ] for host in hostset } targets = list ( itertools . chain ( * [ hostdict [ i ] [ : 2 ] for i in hostdict ] ) ) return targets
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
12133	def extract_log ( log_path , dict_type = dict ) : log_path = ( log_path if os . path . isfile ( log_path ) else os . path . join ( os . getcwd ( ) , log_path ) ) with open ( log_path , 'r' ) as log : splits = ( line . split ( ) for line in log ) uzipped = ( ( int ( split [ 0 ] ) , json . loads ( " " . join ( split [ 1 : ] ) ) ) for split in splits ) szipped = [ ( i , dict ( ( str ( k ) , v ) for ( k , v ) in d . items ( ) ) ) for ( i , d ) in uzipped ] return dict_type ( szipped )
13894	def MatchMasks ( filename , masks ) : import fnmatch if not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] for i_mask in masks : if fnmatch . fnmatch ( filename , i_mask ) : return True return False
10296	def get_incorrect_names_by_namespace ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) and exc . namespace == namespace }
3666	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpsms = [ i ( T ) for i in self . HeatCapacitySolids ] return mixing_simple ( zs , Cpsms ) else : raise Exception ( 'Method not valid' )
3276	def handle_copy ( self , dest_path , depth_infinity ) : if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
10341	def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
12379	def post ( self , request , response ) : if self . slug is not None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'create' ) data = self . _clean ( None , self . request . read ( deserialize = True ) ) item = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( item )
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
1098	def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( "n must be > 0: %r" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( "cutoff must be in [0.0, 1.0]: %r" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) result = heapq . nlargest ( n , result ) return [ x for score , x in result ]
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
5015	def filter_queryset ( self , request , queryset , view ) : if request . user . is_staff : email = request . query_params . get ( 'email' , None ) username = request . query_params . get ( 'username' , None ) query_parameters = { } if email : query_parameters . update ( email = email ) if username : query_parameters . update ( username = username ) if query_parameters : users = User . objects . filter ( ** query_parameters ) . values_list ( 'id' , flat = True ) queryset = queryset . filter ( user_id__in = users ) else : queryset = queryset . filter ( user_id = request . user . id ) return queryset
4148	def centerdc_gen ( self ) : for a in range ( 0 , self . N ) : yield ( a - self . N / 2 ) * self . df
1117	def make_table ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : self . _make_prefix ( ) fromlines , tolines = self . _tab_newline_replace ( fromlines , tolines ) if context : context_lines = numlines else : context_lines = None diffs = _mdiff ( fromlines , tolines , context_lines , linejunk = self . _linejunk , charjunk = self . _charjunk ) if self . _wrapcolumn : diffs = self . _line_wrapper ( diffs ) fromlist , tolist , flaglist = self . _collect_lines ( diffs ) fromlist , tolist , flaglist , next_href , next_id = self . _convert_flags ( fromlist , tolist , flaglist , context , numlines ) s = [ ] fmt = ' <tr><td class="diff_next"%s>%s</td>%s' + '<td class="diff_next">%s</td>%s</tr>\n' for i in range ( len ( flaglist ) ) : if flaglist [ i ] is None : if i > 0 : s . append ( ' </tbody> \n <tbody>\n' ) else : s . append ( fmt % ( next_id [ i ] , next_href [ i ] , fromlist [ i ] , next_href [ i ] , tolist [ i ] ) ) if fromdesc or todesc : header_row = '<thead><tr>%s%s%s%s</tr></thead>' % ( '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % fromdesc , '<th class="diff_next"><br /></th>' , '<th colspan="2" class="diff_header">%s</th>' % todesc ) else : header_row = '' table = self . _table_template % dict ( data_rows = '' . join ( s ) , header_row = header_row , prefix = self . _prefix [ 1 ] ) return table . replace ( '\0+' , '<span class="diff_add">' ) . replace ( '\0-' , '<span class="diff_sub">' ) . replace ( '\0^' , '<span class="diff_chg">' ) . replace ( '\1' , '</span>' ) . replace ( '\t' , '&nbsp;' )
7297	def get_attrs ( model_field , disabled = False ) : attrs = { } attrs [ 'class' ] = 'span6 xlarge' if disabled or isinstance ( model_field , ObjectIdField ) : attrs [ 'class' ] += ' disabled' attrs [ 'readonly' ] = 'readonly' return attrs
13512	def is_colour ( value ) : global PREDEFINED , HEX_MATCH , RGB_MATCH , RGBA_MATCH , HSL_MATCH , HSLA_MATCH value = value . strip ( ) if HEX_MATCH . match ( value ) or RGB_MATCH . match ( value ) or RGBA_MATCH . match ( value ) or HSL_MATCH . match ( value ) or HSLA_MATCH . match ( value ) or value in PREDEFINED : return True return False
6936	def cp_objectinfo_worker ( task ) : cpf , cpkwargs = task try : newcpf = update_checkplot_objectinfo ( cpf , ** cpkwargs ) return newcpf except Exception as e : LOGEXCEPTION ( 'failed to update objectinfo for %s' % cpf ) return None
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
9087	async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( self . send_command ( 'S00' ) , self . send_command ( 'S14' ) , )
2823	def convert_lrelu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting lrelu ...' ) if names == 'short' : tf_name = 'lRELU' + random_string ( 3 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) leakyrelu = keras . layers . LeakyReLU ( alpha = params [ 'alpha' ] , name = tf_name ) layers [ scope_name ] = leakyrelu ( layers [ inputs [ 0 ] ] )
4255	def compress ( self , filename ) : compressed_filename = self . get_compressed_filename ( filename ) if not compressed_filename : return self . do_compress ( filename , compressed_filename )
4776	def does_not_contain_duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
13614	def apply_orientation ( im ) : try : kOrientationEXIFTag = 0x0112 if hasattr ( im , '_getexif' ) : e = im . _getexif ( ) if e is not None : orientation = e [ kOrientationEXIFTag ] f = orientation_funcs [ orientation ] return f ( im ) except : pass return im
13100	def render ( self , ** kwargs ) : breadcrumbs = [ ] breadcrumbs = [ ] if "collections" in kwargs : breadcrumbs = [ { "title" : "Text Collections" , "link" : ".r_collections" , "args" : { } } ] if "parents" in kwargs [ "collections" ] : breadcrumbs += [ { "title" : parent [ "label" ] , "link" : ".r_collection_semantic" , "args" : { "objectId" : parent [ "id" ] , "semantic" : f_slugify ( parent [ "label" ] ) , } , } for parent in kwargs [ "collections" ] [ "parents" ] ] [ : : - 1 ] if "current" in kwargs [ "collections" ] : breadcrumbs . append ( { "title" : kwargs [ "collections" ] [ "current" ] [ "label" ] , "link" : None , "args" : { } } ) if len ( breadcrumbs ) > 0 : breadcrumbs [ - 1 ] [ "link" ] = None return { "breadcrumbs" : breadcrumbs }
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
5520	def check_codes ( self , expected_codes , received_code , info ) : if not any ( map ( received_code . matches , expected_codes ) ) : raise errors . StatusCodeError ( expected_codes , received_code , info )
4390	def adsSyncReadStateReqEx ( port , address ) : sync_read_state_request = _adsDLL . AdsSyncReadStateReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state = ctypes . c_int ( ) ads_state_pointer = ctypes . pointer ( ads_state ) device_state = ctypes . c_int ( ) device_state_pointer = ctypes . pointer ( device_state ) error_code = sync_read_state_request ( port , ams_address_pointer , ads_state_pointer , device_state_pointer ) if error_code : raise ADSError ( error_code ) return ( ads_state . value , device_state . value )
10660	def mass_fractions ( amounts ) : m = masses ( amounts ) m_total = sum ( m . values ( ) ) return { compound : m [ compound ] / m_total for compound in m . keys ( ) }
4976	def render_page_with_error_code_message ( request , context_data , error_code , log_message ) : LOGGER . error ( log_message ) messages . add_generic_error_message_with_code ( request , error_code ) return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , )
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
1735	def _ensure_regexp ( source , n ) : markers = '(+~"\'=[%:?!*^|&-,;/\\' k = 0 while True : k += 1 if n - k < 0 : return True char = source [ n - k ] if char in markers : return True if char != ' ' and char != '\n' : break return False
6311	def from_single ( cls , meta : ProgramDescription , source : str ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , source ) if GEOMETRY_SHADER in source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , source , ) if FRAGMENT_SHADER in source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , source , ) if TESS_CONTROL_SHADER in source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , source , ) if TESS_EVALUATION_SHADER in source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_evaluation_shader , source , ) return instance
959	def aggregationDivide ( dividend , divisor ) : dividendMonthSec = aggregationToMonthsSeconds ( dividend ) divisorMonthSec = aggregationToMonthsSeconds ( divisor ) if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : raise RuntimeError ( "Aggregation dicts with months/years can only be " "inter-operated with other aggregation dicts that contain " "months/years" ) if dividendMonthSec [ 'months' ] > 0 : return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] else : return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ]
2573	def _create_task_log_info ( self , task_id , fail_mode = None ) : info_to_monitor = [ 'func_name' , 'fn_hash' , 'memoize' , 'checkpoint' , 'fail_count' , 'fail_history' , 'status' , 'id' , 'time_submitted' , 'time_returned' , 'executor' ] task_log_info = { "task_" + k : self . tasks [ task_id ] [ k ] for k in info_to_monitor } task_log_info [ 'run_id' ] = self . run_id task_log_info [ 'timestamp' ] = datetime . datetime . now ( ) task_log_info [ 'task_status_name' ] = self . tasks [ task_id ] [ 'status' ] . name task_log_info [ 'tasks_failed_count' ] = self . tasks_failed_count task_log_info [ 'tasks_completed_count' ] = self . tasks_completed_count task_log_info [ 'task_inputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'inputs' , None ) ) task_log_info [ 'task_outputs' ] = str ( self . tasks [ task_id ] [ 'kwargs' ] . get ( 'outputs' , None ) ) task_log_info [ 'task_stdin' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdin' , None ) task_log_info [ 'task_stdout' ] = self . tasks [ task_id ] [ 'kwargs' ] . get ( 'stdout' , None ) task_log_info [ 'task_depends' ] = None if self . tasks [ task_id ] [ 'depends' ] is not None : task_log_info [ 'task_depends' ] = "," . join ( [ str ( t . _tid ) for t in self . tasks [ task_id ] [ 'depends' ] ] ) task_log_info [ 'task_elapsed_time' ] = None if self . tasks [ task_id ] [ 'time_returned' ] is not None : task_log_info [ 'task_elapsed_time' ] = ( self . tasks [ task_id ] [ 'time_returned' ] - self . tasks [ task_id ] [ 'time_submitted' ] ) . total_seconds ( ) if fail_mode is not None : task_log_info [ 'task_fail_mode' ] = fail_mode return task_log_info
93	def _compute_resized_shape ( from_shape , to_shape ) : if is_np_array ( from_shape ) : from_shape = from_shape . shape if is_np_array ( to_shape ) : to_shape = to_shape . shape to_shape_computed = list ( from_shape ) if to_shape is None : pass elif isinstance ( to_shape , tuple ) : do_assert ( len ( from_shape ) in [ 2 , 3 ] ) do_assert ( len ( to_shape ) in [ 2 , 3 ] ) if len ( from_shape ) == 3 and len ( to_shape ) == 3 : do_assert ( from_shape [ 2 ] == to_shape [ 2 ] ) elif len ( to_shape ) == 3 : to_shape_computed . append ( to_shape [ 2 ] ) do_assert ( all ( [ v is None or is_single_number ( v ) for v in to_shape [ 0 : 2 ] ] ) , "Expected the first two entries in to_shape to be None or numbers, " + "got types %s." % ( str ( [ type ( v ) for v in to_shape [ 0 : 2 ] ] ) , ) ) for i , from_shape_i in enumerate ( from_shape [ 0 : 2 ] ) : if to_shape [ i ] is None : to_shape_computed [ i ] = from_shape_i elif is_single_integer ( to_shape [ i ] ) : to_shape_computed [ i ] = to_shape [ i ] else : to_shape_computed [ i ] = int ( np . round ( from_shape_i * to_shape [ i ] ) ) elif is_single_integer ( to_shape ) or is_single_float ( to_shape ) : to_shape_computed = _compute_resized_shape ( from_shape , ( to_shape , to_shape ) ) else : raise Exception ( "Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int " + "or single float, got %s." % ( type ( to_shape ) , ) ) return tuple ( to_shape_computed )
7171	def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
8818	def get_networks ( context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , filters = None , fields = None ) : LOG . info ( "get_networks for tenant %s with filters %s, fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } nets = db_api . network_find ( context , limit , sorts , marker , page_reverse , join_subnets = True , ** filters ) or [ ] nets = [ v . _make_network_dict ( net , fields = fields ) for net in nets ] return nets
12849	def watch_method ( self , method_name , callback ) : try : method = getattr ( self , method_name ) except AttributeError : raise ApiUsageError ( ) if not isinstance ( method , Token . WatchedMethod ) : setattr ( self , method_name , Token . WatchedMethod ( method ) ) method = getattr ( self , method_name ) method . add_watcher ( callback )
4786	def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
12189	async def from_api_token ( cls , token = None , api_cls = SlackBotApi ) : api = api_cls . from_env ( ) if token is None else api_cls ( api_token = token ) data = await api . execute_method ( cls . API_AUTH_ENDPOINT ) return cls ( data [ 'user_id' ] , data [ 'user' ] , api )
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
2400	def get_good_pos_ngrams ( self ) : if ( os . path . isfile ( NGRAM_PATH ) ) : good_pos_ngrams = pickle . load ( open ( NGRAM_PATH , 'rb' ) ) elif os . path . isfile ( ESSAY_CORPUS_PATH ) : essay_corpus = open ( ESSAY_CORPUS_PATH ) . read ( ) essay_corpus = util_functions . sub_chars ( essay_corpus ) good_pos_ngrams = util_functions . regenerate_good_tokens ( essay_corpus ) pickle . dump ( good_pos_ngrams , open ( NGRAM_PATH , 'wb' ) ) else : good_pos_ngrams = [ 'NN PRP' , 'NN PRP .' , 'NN PRP . DT' , 'PRP .' , 'PRP . DT' , 'PRP . DT NNP' , '. DT' , '. DT NNP' , '. DT NNP NNP' , 'DT NNP' , 'DT NNP NNP' , 'DT NNP NNP NNP' , 'NNP NNP' , 'NNP NNP NNP' , 'NNP NNP NNP NNP' , 'NNP NNP NNP .' , 'NNP NNP .' , 'NNP NNP . TO' , 'NNP .' , 'NNP . TO' , 'NNP . TO NNP' , '. TO' , '. TO NNP' , '. TO NNP NNP' , 'TO NNP' , 'TO NNP NNP' ] return set ( good_pos_ngrams )
12	def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : assert mode in ( 'two_sided' , 'causal' ) if len ( y ) < 2 * radius + 1 : return np . ones_like ( y ) * y . mean ( ) elif mode == 'two_sided' : convkernel = np . ones ( 2 * radius + 1 ) out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) if valid_only : out [ : radius ] = out [ - radius : ] = np . nan elif mode == 'causal' : convkernel = np . ones ( radius ) out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) out = out [ : - radius + 1 ] if valid_only : out [ : radius ] = np . nan return out
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
4573	def hsv2rgb_spectrum ( hsv ) : h , s , v = hsv return hsv2rgb_raw ( ( ( h * 192 ) >> 8 , s , v ) )
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
5718	def pull_datapackage ( descriptor , name , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) datapackage_name = name plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) resources = [ ] for table in storage . buckets : schema = storage . describe ( table ) base = os . path . dirname ( descriptor ) path , name = _restore_path ( table ) fullpath = os . path . join ( base , path ) helpers . ensure_dir ( fullpath ) with io . open ( fullpath , 'wb' ) as file : model = Schema ( deepcopy ( schema ) ) data = storage . iter ( table ) writer = csv . writer ( file , encoding = 'utf-8' ) writer . writerow ( model . headers ) for row in data : writer . writerow ( row ) resource = { 'schema' : schema , 'path' : path } if name is not None : resource [ 'name' ] = name resources . append ( resource ) mode = 'w' encoding = 'utf-8' if six . PY2 : mode = 'wb' encoding = None resources = _restore_resources ( resources ) helpers . ensure_dir ( descriptor ) with io . open ( descriptor , mode = mode , encoding = encoding ) as file : descriptor = { 'name' : datapackage_name , 'resources' : resources , } json . dump ( descriptor , file , indent = 4 ) return storage
7921	def __prepare_domain ( data ) : if not data : raise JIDError ( "Domain must be given" ) data = unicode ( data ) if not data : raise JIDError ( "Domain must be given" ) if u'[' in data : if data [ 0 ] == u'[' and data [ - 1 ] == u']' : try : addr = _validate_ip_address ( socket . AF_INET6 , data [ 1 : - 1 ] ) return "[{0}]" . format ( addr ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) raise JIDError ( u"Invalid IPv6 literal in JID domainpart" ) else : raise JIDError ( u"Invalid use of '[' or ']' in JID domainpart" ) elif data [ 0 ] . isdigit ( ) and data [ - 1 ] . isdigit ( ) : try : addr = _validate_ip_address ( socket . AF_INET , data ) except ValueError , err : logger . debug ( "ValueError: {0}" . format ( err ) ) data = UNICODE_DOT_RE . sub ( u"." , data ) data = data . rstrip ( u"." ) labels = data . split ( u"." ) try : labels = [ idna . nameprep ( label ) for label in labels ] except UnicodeError : raise JIDError ( u"Domain name invalid" ) for label in labels : if not STD3_LABEL_RE . match ( label ) : raise JIDError ( u"Domain name invalid" ) try : idna . ToASCII ( label ) except UnicodeError : raise JIDError ( u"Domain name invalid" ) domain = u"." . join ( labels ) if len ( domain . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Domain name too long" ) return domain
535	def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )
10678	def H ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : if e == - 1.0 : result += c * math . log ( lT / Tref ) else : result += c * ( lT ** ( e + 1.0 ) - Tref ** ( e + 1.0 ) ) / ( e + 1.0 ) return result
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
11496	def get_user_by_email ( self , email ) : parameters = dict ( ) parameters [ 'email' ] = email response = self . request ( 'midas.user.get' , parameters ) return response
12779	def get_stream ( self , error_callback = None , live = True ) : self . join ( ) return Stream ( self , error_callback = error_callback , live = live )
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
11396	def get_record ( self ) : self . update_system_numbers ( ) self . add_systemnumber ( "CDS" ) self . fields_list = [ "024" , "041" , "035" , "037" , "088" , "100" , "110" , "111" , "242" , "245" , "246" , "260" , "269" , "300" , "502" , "650" , "653" , "693" , "700" , "710" , "773" , "856" , "520" , "500" , "980" ] self . keep_only_fields ( ) self . determine_collections ( ) self . add_cms_link ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_date ( ) self . update_pagenumber ( ) self . update_authors ( ) self . update_subject_categories ( "SzGeCERN" , "INSPIRE" , "categories_inspire" ) self . update_keywords ( ) self . update_experiments ( ) self . update_collaboration ( ) self . update_journals ( ) self . update_links_and_ffts ( ) if 'THESIS' in self . collections : self . update_thesis_supervisors ( ) self . update_thesis_information ( ) if 'NOTE' in self . collections : self . add_notes ( ) for collection in self . collections : record_add_field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove_controlfields ( ) return self . record
6899	def parallel_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformat , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _periodicfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( pfpkl_list , results ) } return resdict
1883	def new_symbolic_value ( self , nbits , label = None , taint = frozenset ( ) ) : assert nbits in ( 1 , 4 , 8 , 16 , 32 , 64 , 128 , 256 ) avoid_collisions = False if label is None : label = 'val' avoid_collisions = True expr = self . _constraints . new_bitvec ( nbits , name = label , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) return expr
1292	def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : return self . demo_memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
804	def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : query = 'SELECT model_id FROM %s ' ' WHERE status=%%s ' ' AND job_id=%%s ' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , "Unexpected numRows: %r" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE model_id=%%s ' ' AND status=%%s' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID
8842	def indent ( self ) : if not self . tab_always_indent : super ( PyIndenterMode , self ) . indent ( ) else : cursor = self . editor . textCursor ( ) assert isinstance ( cursor , QtGui . QTextCursor ) if cursor . hasSelection ( ) : self . indent_selection ( cursor ) else : tab_len = self . editor . tab_length cursor . beginEditBlock ( ) if self . editor . use_spaces_instead_of_tabs : cursor . insertText ( tab_len * " " ) else : cursor . insertText ( '\t' ) cursor . endEditBlock ( ) self . editor . setTextCursor ( cursor )
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
2564	def start ( self ) : self . comm . Barrier ( ) logger . debug ( "Manager synced with workers" ) self . _kill_event = threading . Event ( ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) start = None result_counter = 0 task_recv_counter = 0 task_sent_counter = 0 logger . info ( "Loop start" ) while not self . _kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) timer = time . time ( ) + 0.05 counter = min ( 10 , comm . size ) while time . time ( ) < timer : info = MPI . Status ( ) if counter > 10 : logger . debug ( "Hit max mpi events per round" ) break if not self . comm . Iprobe ( status = info ) : logger . debug ( "Timer expired, processed {} mpi events" . format ( counter ) ) break else : tag = info . Get_tag ( ) logger . info ( "Message with tag {} received" . format ( tag ) ) counter += 1 if tag == RESULT_TAG : result = self . recv_result_from_workers ( ) self . pending_result_queue . put ( result ) result_counter += 1 elif tag == TASK_REQUEST_TAG : worker_rank = self . recv_task_request_from_workers ( ) self . ready_worker_queue . put ( worker_rank ) else : logger . error ( "Unknown tag {} - ignoring this message and continuing" . format ( tag ) ) available_worker_cnt = self . ready_worker_queue . qsize ( ) available_task_cnt = self . pending_task_queue . qsize ( ) logger . debug ( "[MAIN] Ready workers: {} Ready tasks: {}" . format ( available_worker_cnt , available_task_cnt ) ) this_round = min ( available_worker_cnt , available_task_cnt ) for i in range ( this_round ) : worker_rank = self . ready_worker_queue . get ( ) task = self . pending_task_queue . get ( ) comm . send ( task , dest = worker_rank , tag = worker_rank ) task_sent_counter += 1 logger . debug ( "Assigning worker:{} task:{}" . format ( worker_rank , task [ 'task_id' ] ) ) if not start : start = time . time ( ) logger . debug ( "Tasks recvd:{} Tasks dispatched:{} Results recvd:{}" . format ( task_recv_counter , task_sent_counter , result_counter ) ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "mpi_worker_pool ran for {} seconds" . format ( delta ) )
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : r if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
1078	def isoformat ( self ) : return "%s-%s-%s" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
4457	def get_args ( self ) : args = [ self . _query_string ] if self . _no_content : args . append ( 'NOCONTENT' ) if self . _fields : args . append ( 'INFIELDS' ) args . append ( len ( self . _fields ) ) args += self . _fields if self . _verbatim : args . append ( 'VERBATIM' ) if self . _no_stopwords : args . append ( 'NOSTOPWORDS' ) if self . _filters : for flt in self . _filters : assert isinstance ( flt , Filter ) args += flt . args if self . _with_payloads : args . append ( 'WITHPAYLOADS' ) if self . _ids : args . append ( 'INKEYS' ) args . append ( len ( self . _ids ) ) args += self . _ids if self . _slop >= 0 : args += [ 'SLOP' , self . _slop ] if self . _in_order : args . append ( 'INORDER' ) if self . _return_fields : args . append ( 'RETURN' ) args . append ( len ( self . _return_fields ) ) args += self . _return_fields if self . _sortby : assert isinstance ( self . _sortby , SortbyField ) args . append ( 'SORTBY' ) args += self . _sortby . args if self . _language : args += [ 'LANGUAGE' , self . _language ] args += self . _summarize_fields + self . _highlight_fields args += [ "LIMIT" , self . _offset , self . _num ] return args
1104	def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
8431	def desaturate_pal ( color , prop , reverse = False ) : if not 0 <= prop <= 1 : raise ValueError ( "prop must be between 0 and 1" ) rgb = mcolors . colorConverter . to_rgb ( color ) h , l , s = colorsys . rgb_to_hls ( * rgb ) s *= prop desaturated_color = colorsys . hls_to_rgb ( h , l , s ) colors = [ color , desaturated_color ] if reverse : colors = colors [ : : - 1 ] return gradient_n_pal ( colors , name = 'desaturated' )
5116	def draw ( self , update_colors = True , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "matplotlib is necessary to draw the network." ) if update_colors : self . _update_all_colors ( ) if 'bgcolor' not in kwargs : kwargs [ 'bgcolor' ] = self . colors [ 'bgcolor' ] self . g . draw_graph ( line_kwargs = line_kwargs , scatter_kwargs = scatter_kwargs , ** kwargs )
4055	def everything ( self , query ) : try : items = [ ] items . extend ( query ) while self . links . get ( "next" ) : items . extend ( self . follow ( ) ) except TypeError : items = copy . deepcopy ( query ) while self . links . get ( "next" ) : items . entries . extend ( self . follow ( ) . entries ) return items
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
11648	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) self . train_ = X memory = get_memory ( self . memory ) lo , = memory . cache ( scipy . linalg . eigvalsh ) ( X , eigvals = ( 0 , 0 ) ) self . shift_ = max ( self . min_eig - lo , 0 ) return self
9668	def _make_package ( args ) : from lingpy . sequence . sound_classes import token2class from lingpy . data import Model columns = [ 'LATEX' , 'FEATURES' , 'SOUND' , 'IMAGE' , 'COUNT' , 'NOTE' ] bipa = TranscriptionSystem ( 'bipa' ) for src , rows in args . repos . iter_sources ( type = 'td' ) : args . log . info ( 'TranscriptionData {0} ...' . format ( src [ 'NAME' ] ) ) uritemplate = URITemplate ( src [ 'URITEMPLATE' ] ) if src [ 'URITEMPLATE' ] else None out = [ [ 'BIPA_GRAPHEME' , 'CLTS_NAME' , 'GENERATED' , 'EXPLICIT' , 'GRAPHEME' , 'URL' ] + columns ] graphemes = set ( ) for row in rows : if row [ 'GRAPHEME' ] in graphemes : args . log . warn ( 'skipping duplicate grapheme: {0}' . format ( row [ 'GRAPHEME' ] ) ) continue graphemes . add ( row [ 'GRAPHEME' ] ) if not row [ 'BIPA' ] : bipa_sound = bipa [ row [ 'GRAPHEME' ] ] explicit = '' else : bipa_sound = bipa [ row [ 'BIPA' ] ] explicit = '+' generated = '+' if bipa_sound . generated else '' if is_valid_sound ( bipa_sound , bipa ) : bipa_grapheme = bipa_sound . s bipa_name = bipa_sound . name else : bipa_grapheme , bipa_name = '<NA>' , '<NA>' url = uritemplate . expand ( ** row ) if uritemplate else row . get ( 'URL' , '' ) out . append ( [ bipa_grapheme , bipa_name , generated , explicit , row [ 'GRAPHEME' ] , url ] + [ row . get ( c , '' ) for c in columns ] ) found = len ( [ o for o in out if o [ 0 ] != '<NA>' ] ) args . log . info ( '... {0} of {1} graphemes found ({2:.0f}%)' . format ( found , len ( out ) , found / len ( out ) * 100 ) ) with UnicodeWriter ( pkg_path ( 'transcriptiondata' , '{0}.tsv' . format ( src [ 'NAME' ] ) ) , delimiter = '\t' ) as writer : writer . writerows ( out ) count = 0 with UnicodeWriter ( pkg_path ( 'soundclasses' , 'lingpy.tsv' ) , delimiter = '\t' ) as writer : writer . writerow ( [ 'CLTS_NAME' , 'BIPA_GRAPHEME' ] + SOUNDCLASS_SYSTEMS ) for grapheme , sound in sorted ( bipa . sounds . items ( ) ) : if not sound . alias : writer . writerow ( [ sound . name , grapheme ] + [ token2class ( grapheme , Model ( cls ) ) for cls in SOUNDCLASS_SYSTEMS ] ) count += 1 args . log . info ( 'SoundClasses: {0} written to file.' . format ( count ) )
10768	def numpy_formatter ( _ , vertices , codes = None ) : if codes is None : return vertices numpy_vertices = [ ] for vertices_ , codes_ in zip ( vertices , codes ) : starts = np . nonzero ( codes_ == MPLPATHCODE . MOVETO ) [ 0 ] stops = np . nonzero ( codes_ == MPLPATHCODE . CLOSEPOLY ) [ 0 ] for start , stop in zip ( starts , stops ) : numpy_vertices . append ( vertices_ [ start : stop + 1 , : ] ) return numpy_vertices
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
6201	def simulate_timestamps_mix ( self , max_rates , populations , bg_rate , rs = None , seed = 1 , chunksize = 2 ** 16 , comp_filter = None , overwrite = False , skip_existing = False , scale = 10 , path = None , t_chunksize = None , timeslice = None ) : self . open_store_timestamp ( chunksize = chunksize , path = path ) rs = self . _get_group_randomstate ( rs , seed , self . ts_group ) if t_chunksize is None : t_chunksize = self . emission . chunkshape [ 1 ] timeslice_size = self . n_samples if timeslice is not None : timeslice_size = timeslice // self . t_step name = self . _get_ts_name_mix ( max_rates , populations , bg_rate , rs = rs ) kw = dict ( name = name , clk_p = self . t_step / scale , max_rates = max_rates , bg_rate = bg_rate , populations = populations , num_particles = self . num_particles , bg_particle = self . num_particles , overwrite = overwrite , chunksize = chunksize ) if comp_filter is not None : kw . update ( comp_filter = comp_filter ) try : self . _timestamps , self . _tparticles = ( self . ts_store . add_timestamps ( ** kw ) ) except ExistingArrayError as e : if skip_existing : print ( ' - Skipping already present timestamps array.' ) return else : raise e self . ts_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'PyBroMo' ] = __version__ ts_list , part_list = [ ] , [ ] bg_rates = [ None ] * ( len ( max_rates ) - 1 ) + [ bg_rate ] prev_time = 0 for i_start , i_end in iter_chunk_index ( timeslice_size , t_chunksize ) : curr_time = np . around ( i_start * self . t_step , decimals = 0 ) if curr_time > prev_time : print ( ' %.1fs' % curr_time , end = '' , flush = True ) prev_time = curr_time em_chunk = self . emission [ : , i_start : i_end ] times_chunk_s , par_index_chunk_s = self . _sim_timestamps_populations ( em_chunk , max_rates , populations , bg_rates , i_start , rs , scale ) ts_list . append ( times_chunk_s ) part_list . append ( par_index_chunk_s ) for ts , part in zip ( ts_list , part_list ) : self . _timestamps . append ( ts ) self . _tparticles . append ( part ) self . ts_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'last_random_state' ] = rs . get_state ( ) self . ts_store . h5file . flush ( )
3402	def is_boundary_type ( reaction , boundary_type , external_compartment ) : sbo_term = reaction . annotation . get ( "sbo" , "" ) if isinstance ( sbo_term , list ) : sbo_term = sbo_term [ 0 ] sbo_term = sbo_term . upper ( ) if sbo_term == sbo_terms [ boundary_type ] : return True if sbo_term in [ sbo_terms [ k ] for k in sbo_terms if k != boundary_type ] : return False correct_compartment = external_compartment in reaction . compartments if boundary_type != "exchange" : correct_compartment = not correct_compartment rev_type = True if boundary_type == "demand" : rev_type = not reaction . reversibility elif boundary_type == "sink" : rev_type = reaction . reversibility return ( reaction . boundary and not any ( ex in reaction . id for ex in excludes [ boundary_type ] ) and correct_compartment and rev_type )
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
914	def lscsum ( lx , epsilon = None ) : lx = numpy . asarray ( lx ) base = lx . max ( ) if numpy . isinf ( base ) : return base if ( epsilon is not None ) and ( base < epsilon ) : return epsilon x = numpy . exp ( lx - base ) ssum = x . sum ( ) result = numpy . log ( ssum ) + base return result
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
7678	def pitch_contour ( annotation , ** kwargs ) : ax = kwargs . pop ( 'ax' , None ) ax = mir_eval . display . __get_axes ( ax = ax ) [ 0 ] times , values = annotation . to_interval_values ( ) indices = np . unique ( [ v [ 'index' ] for v in values ] ) for idx in indices : rows = [ i for ( i , v ) in enumerate ( values ) if v [ 'index' ] == idx ] freqs = np . asarray ( [ values [ r ] [ 'frequency' ] for r in rows ] ) unvoiced = ~ np . asarray ( [ values [ r ] [ 'voiced' ] for r in rows ] ) freqs [ unvoiced ] *= - 1 ax = mir_eval . display . pitch ( times [ rows , 0 ] , freqs , unvoiced = True , ax = ax , ** kwargs ) return ax
1627	def CheckForCopyright ( filename , lines , error ) : for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found. ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
12358	def send_request ( self , kind , resource , url_components , ** kwargs ) : url = self . format_request_url ( resource , * url_components ) meth = getattr ( requests , kind ) headers = self . get_request_headers ( ) req_data = self . format_parameters ( ** kwargs ) response = meth ( url , headers = headers , data = req_data ) data = self . get_response ( response ) if response . status_code >= 300 : msg = data . pop ( 'message' , 'API request returned error' ) raise APIError ( msg , response . status_code , ** data ) return data
7642	def _conversion ( target , source ) : def register ( func ) : __CONVERSION__ [ target ] [ source ] = func return func return register
776	def __getDBNameForVersion ( cls , dbVersion ) : prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) dbName = '%s_%s' % ( prefix , suffix ) return dbName
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
8986	def _instructions_changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in_row = self . _parser . instruction_in_row ( self , instruction ) self . instructions [ index ] = in_row else : instruction . transfer_to_row ( self )
9137	def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
8131	def merge ( self , layers ) : layers . sort ( ) if layers [ 0 ] == 0 : del layers [ 0 ] self . flatten ( layers )
2880	def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
10767	def submit_poll ( self , poll , * , request_policy = None ) : if poll . id is not None : raise ExistingPoll ( ) options = poll . options data = { 'title' : poll . title , 'options' : options , 'multi' : poll . multi , 'dupcheck' : poll . dupcheck , 'captcha' : poll . captcha } return self . _http_client . post ( self . _POLLS , data = data , request_policy = request_policy , cls = strawpoll . Poll )
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
11982	def is_valid_ip ( self , ip ) : if not isinstance ( ip , ( IPv4Address , CIDR ) ) : if str ( ip ) . find ( '/' ) == - 1 : ip = IPv4Address ( ip ) else : ip = CIDR ( ip ) if isinstance ( ip , IPv4Address ) : if ip < self . _first_ip or ip > self . _last_ip : return False elif isinstance ( ip , CIDR ) : if ip . _nm . _ip_dec == 0xFFFFFFFE and self . _nm . _ip_dec != 0xFFFFFFFE : compare_to_first = self . _net_ip . _ip_dec compare_to_last = self . _bc_ip . _ip_dec else : compare_to_first = self . _first_ip . _ip_dec compare_to_last = self . _last_ip . _ip_dec if ip . _first_ip . _ip_dec < compare_to_first or ip . _last_ip . _ip_dec > compare_to_last : return False return True
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
11977	def get_bits ( self ) : return _convert ( self . _ip , notation = NM_BITS , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
8564	def create_loadbalancer ( self , datacenter_id , loadbalancer ) : data = json . dumps ( self . _create_loadbalancer_dict ( loadbalancer ) ) response = self . _perform_request ( url = '/datacenters/%s/loadbalancers' % datacenter_id , method = 'POST' , data = data ) return response
2367	def _load ( self , path ) : self . tables = [ ] current_table = DefaultTable ( self ) with Utf8Reader ( path ) as f : self . raw_text = f . read ( ) f . _file . seek ( 0 ) matcher = Matcher ( re . IGNORECASE ) for linenumber , raw_text in enumerate ( f . readlines ( ) ) : linenumber += 1 raw_text = raw_text . replace ( u'\xA0' , ' ' ) raw_text = raw_text . rstrip ( ) cells = TxtReader . split_row ( raw_text ) _heading_regex = r'^\s*\*+\s*(.*?)[ *]*$' if matcher ( _heading_regex , cells [ 0 ] ) : table_name = matcher . group ( 1 ) current_table = tableFactory ( self , linenumber , table_name , raw_text ) self . tables . append ( current_table ) else : current_table . append ( Row ( linenumber , raw_text , cells ) )
12922	def render ( self , * args , ** kwargs ) : render_to = StringIO ( ) self . output ( render_to , * args , ** kwargs ) return render_to . getvalue ( )
2230	def register ( self , hash_types ) : if not isinstance ( hash_types , ( list , tuple ) ) : hash_types = [ hash_types ] def _decor_closure ( hash_func ) : for hash_type in hash_types : key = ( hash_type . __module__ , hash_type . __name__ ) self . keyed_extensions [ key ] = ( hash_type , hash_func ) return hash_func return _decor_closure
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
3681	def GWP ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in GWP_data . index : methods . append ( IPCC100 ) if not pd . isnull ( GWP_data . at [ CASRN , 'SAR 100yr' ] ) : methods . append ( IPCC100SAR ) methods . append ( IPCC20 ) methods . append ( IPCC500 ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IPCC100 : return float ( GWP_data . at [ CASRN , '100yr GWP' ] ) elif Method == IPCC100SAR : return float ( GWP_data . at [ CASRN , 'SAR 100yr' ] ) elif Method == IPCC20 : return float ( GWP_data . at [ CASRN , '20yr GWP' ] ) elif Method == IPCC500 : return float ( GWP_data . at [ CASRN , '500yr GWP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
2977	def cmd_kill ( opts ) : kill_signal = opts . signal if hasattr ( opts , 'signal' ) else "SIGKILL" __with_containers ( opts , Blockade . kill , signal = kill_signal )
11339	def set_target_fahrenheit ( self , fahrenheit , mode = config . SCHEDULE_HOLD ) : temperature = fahrenheit_to_nuheat ( fahrenheit ) self . set_target_temperature ( temperature , mode )
3302	def _get_checked_path ( path , config , must_exist = True , allow_none = True ) : if path in ( None , "" ) : if allow_none : return None raise ValueError ( "Invalid path {!r}" . format ( path ) ) config_file = config . get ( "_config_file" ) if config_file and not os . path . isabs ( path ) : path = os . path . normpath ( os . path . join ( os . path . dirname ( config_file ) , path ) ) else : path = os . path . abspath ( path ) if must_exist and not os . path . exists ( path ) : raise ValueError ( "Invalid path {!r}" . format ( path ) ) return path
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
4126	def spectrum_data ( filename ) : import os import pkg_resources info = pkg_resources . get_distribution ( 'spectrum' ) location = info . location share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
5271	def lcs ( self , stringIdxs = - 1 ) : if stringIdxs == - 1 or not isinstance ( stringIdxs , list ) : stringIdxs = set ( range ( len ( self . word_starts ) ) ) else : stringIdxs = set ( stringIdxs ) deepestNode = self . _find_lcs ( self . root , stringIdxs ) start = deepestNode . idx end = deepestNode . idx + deepestNode . depth return self . word [ start : end ]
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
9940	def set_options ( self , ** options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry_run = options [ 'dry_run' ] ignore_patterns = options [ 'ignore_patterns' ] if options [ 'use_default_ignore_patterns' ] : ignore_patterns += [ 'CVS' , '.*' , '*~' ] self . ignore_patterns = list ( set ( ignore_patterns ) ) self . post_process = options [ 'post_process' ]
2461	def set_file_name ( self , doc , name ) : if self . has_package ( doc ) : doc . package . files . append ( file . File ( name ) ) self . reset_file_stat ( ) return True else : raise OrderError ( 'File::Name' )
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
11276	def disown ( debug ) : pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False for line in infile : if line . find ( "ardexa.service" ) == - 1 : continue line = line . replace ( "name=" , "" ) items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" if not accounts : continue full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : return False return True
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) enrich_backend . studies = all_studies
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
5770	def rsa_pkcs1v15_verify ( certificate_or_public_key , signature , data , hash_algorithm ) : if certificate_or_public_key . algorithm != 'rsa' : raise ValueError ( 'The key specified is not an RSA public key' ) return _verify ( certificate_or_public_key , signature , data , hash_algorithm )
7081	def send_email ( sender , subject , content , email_recipient_list , email_address_list , email_user = None , email_pass = None , email_server = None ) : if not email_user : email_user = EMAIL_USER if not email_pass : email_pass = EMAIL_PASSWORD if not email_server : email_server = EMAIL_SERVER if not email_server and email_user and email_pass : raise ValueError ( "no email server address and " "credentials available, can't continue" ) msg_text = EMAIL_TEMPLATE . format ( sender = sender , hostname = socket . gethostname ( ) , activity_time = '%sZ' % datetime . utcnow ( ) . isoformat ( ) , activity_report = content ) email_sender = '%s <%s>' % ( sender , EMAIL_USER ) email_recipients = [ ( '%s <%s>' % ( x , y ) ) for ( x , y ) in zip ( email_recipient_list , email_address_list ) ] email_msg = MIMEText ( msg_text ) email_msg [ 'From' ] = email_sender email_msg [ 'To' ] = ', ' . join ( email_recipients ) email_msg [ 'Message-Id' ] = make_msgid ( ) email_msg [ 'Subject' ] = '[%s on %s] %s' % ( sender , socket . gethostname ( ) , subject ) email_msg [ 'Date' ] = formatdate ( time . time ( ) ) try : server = smtplib . SMTP ( EMAIL_SERVER , 587 ) server_ehlo_response = server . ehlo ( ) if server . has_extn ( 'STARTTLS' ) : try : tls_start_response = server . starttls ( ) tls_ehlo_response = server . ehlo ( ) login_response = server . login ( EMAIL_USER , EMAIL_PASSWORD ) send_response = ( server . sendmail ( email_sender , email_address_list , email_msg . as_string ( ) ) ) except Exception as e : print ( 'script email sending failed with error: %s' % e ) send_response = None if send_response is not None : print ( 'script email sent successfully' ) quit_response = server . quit ( ) return True else : quit_response = server . quit ( ) return False else : print ( 'email server does not support STARTTLS,' ' bailing out...' ) quit_response = server . quit ( ) return False except Exception as e : print ( 'sending email failed with error: %s' % e ) returnval = False quit_response = server . quit ( ) return returnval
7114	def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
9702	def checkTUN ( self ) : packet = self . _TUN . _tun . read ( self . _TUN . _tun . mtu ) return ( packet )
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
8470	def execute ( self , shell = True ) : process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = shell ) self . output , self . errors = process . communicate ( )
13607	def pickle ( obj , filepath ) : arr = pkl . dumps ( obj , - 1 ) with open ( filepath , 'wb' ) as f : s = 0 while s < len ( arr ) : e = min ( s + blosc . MAX_BUFFERSIZE , len ( arr ) ) carr = blosc . compress ( arr [ s : e ] , typesize = 8 ) f . write ( carr ) s = e
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
6957	def _log_prior_transit ( theta , priorbounds ) : allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
4971	def clean ( self ) : super ( EnterpriseCustomerIdentityProviderAdminForm , self ) . clean ( ) provider_id = self . cleaned_data . get ( 'provider_id' , None ) enterprise_customer = self . cleaned_data . get ( 'enterprise_customer' , None ) if provider_id is None or enterprise_customer is None : return identity_provider = utils . get_identity_provider ( provider_id ) if not identity_provider : message = _ ( "The specified Identity Provider does not exist. For more " "information, contact a system administrator." , ) logger . exception ( message ) raise ValidationError ( message ) if identity_provider and identity_provider . site != enterprise_customer . site : raise ValidationError ( _ ( "The site for the selected identity provider " "({identity_provider_site}) does not match the site for " "this enterprise customer ({enterprise_customer_site}). " "To correct this problem, select a site that has a domain " "of '{identity_provider_site}', or update the identity " "provider to '{enterprise_customer_site}'." ) . format ( enterprise_customer_site = enterprise_customer . site , identity_provider_site = identity_provider . site , ) , )
9966	def convert_args ( args , kwargs ) : found = False for arg in args : if isinstance ( arg , Cells ) : found = True break if found : args = tuple ( arg . value if isinstance ( arg , Cells ) else arg for arg in args ) if kwargs is not None : for key , arg in kwargs . items ( ) : if isinstance ( arg , Cells ) : kwargs [ key ] = arg . value return args , kwargs
13895	def FindFiles ( dir_ , in_filters = None , out_filters = None , recursive = True , include_root_dir = True , standard_paths = False ) : if in_filters is None : in_filters = [ '*' ] if out_filters is None : out_filters = [ ] result = [ ] for dir_root , directories , filenames in os . walk ( dir_ ) : for i_directory in directories [ : ] : if MatchMasks ( i_directory , out_filters ) : directories . remove ( i_directory ) for filename in directories + filenames : if MatchMasks ( filename , in_filters ) and not MatchMasks ( filename , out_filters ) : result . append ( os . path . join ( dir_root , filename ) ) if not recursive : break if not include_root_dir : dir_prefix = len ( dir_ ) + 1 result = [ file [ dir_prefix : ] for file in result ] if standard_paths : result = map ( StandardizePath , result ) return result
2397	def histogram ( ratings , min_rating = None , max_rating = None ) : ratings = [ int ( r ) for r in ratings ] if min_rating is None : min_rating = min ( ratings ) if max_rating is None : max_rating = max ( ratings ) num_ratings = int ( max_rating - min_rating + 1 ) hist_ratings = [ 0 for x in range ( num_ratings ) ] for r in ratings : hist_ratings [ r - min_rating ] += 1 return hist_ratings
7708	def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( "Checking roster-related features" ) if event . features . find ( FEATURE_ROSTERVER ) is not None : logger . debug ( " Roster versioning available" ) server_features . add ( "versioning" ) if event . features . find ( FEATURE_APPROVALS ) is not None : logger . debug ( " Subscription pre-approvals available" ) server_features . add ( "pre-approvals" ) self . server_features = server_features
8767	def _validate_allocation_pools ( self ) : ip_pools = self . _alloc_pools subnet_cidr = self . _subnet_cidr LOG . debug ( _ ( "Performing IP validity checks on allocation pools" ) ) ip_sets = [ ] for ip_pool in ip_pools : try : start_ip = netaddr . IPAddress ( ip_pool [ 'start' ] ) end_ip = netaddr . IPAddress ( ip_pool [ 'end' ] ) except netaddr . AddrFormatError : LOG . info ( _ ( "Found invalid IP address in pool: " "%(start)s - %(end)s:" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip . version != self . _subnet_cidr . version or end_ip . version != self . _subnet_cidr . version ) : LOG . info ( _ ( "Specified IP addresses do not match " "the subnet IP version" ) ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if end_ip < start_ip : LOG . info ( _ ( "Start IP (%(start)s) is greater than end IP " "(%(end)s)" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . InvalidAllocationPool ( pool = ip_pool ) if ( start_ip < self . _subnet_first_ip or end_ip > self . _subnet_last_ip ) : LOG . info ( _ ( "Found pool larger than subnet " "CIDR:%(start)s - %(end)s" ) , { 'start' : ip_pool [ 'start' ] , 'end' : ip_pool [ 'end' ] } ) raise n_exc_ext . OutOfBoundsAllocationPool ( pool = ip_pool , subnet_cidr = subnet_cidr ) ip_sets . append ( netaddr . IPSet ( netaddr . IPRange ( ip_pool [ 'start' ] , ip_pool [ 'end' ] ) . cidrs ( ) ) ) LOG . debug ( _ ( "Checking for overlaps among allocation pools " "and gateway ip" ) ) ip_ranges = ip_pools [ : ] for l_cursor in xrange ( len ( ip_sets ) ) : for r_cursor in xrange ( l_cursor + 1 , len ( ip_sets ) ) : if ip_sets [ l_cursor ] & ip_sets [ r_cursor ] : l_range = ip_ranges [ l_cursor ] r_range = ip_ranges [ r_cursor ] LOG . info ( _ ( "Found overlapping ranges: %(l_range)s and " "%(r_range)s" ) , { 'l_range' : l_range , 'r_range' : r_range } ) raise n_exc_ext . OverlappingAllocationPools ( pool_1 = l_range , pool_2 = r_range , subnet_cidr = subnet_cidr )
4224	def _load_keyring_class ( keyring_name ) : module_name , sep , class_name = keyring_name . rpartition ( '.' ) __import__ ( module_name ) module = sys . modules [ module_name ] return getattr ( module , class_name )
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : continue targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
5783	def read_exactly ( self , num_bytes ) : output = b'' remaining = num_bytes while remaining > 0 : output += self . read ( remaining ) remaining = num_bytes - len ( output ) return output
6485	def do_search ( request , course_id = None ) : SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights loss = tf . divide ( tf . reduce_sum ( losses ) , tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
2212	def touch ( fpath , mode = 0o666 , dir_fd = None , verbose = 0 , ** kwargs ) : if verbose : print ( 'Touching file {}' . format ( fpath ) ) if six . PY2 : with open ( fpath , 'a' ) : os . utime ( fpath , None ) else : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fpath , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fpath , dir_fd = None if os . supports_fd else dir_fd , ** kwargs ) return fpath
3855	async def _sync_all_conversations ( client ) : conv_states = [ ] sync_timestamp = None request = hangouts_pb2 . SyncRecentConversationsRequest ( request_header = client . get_request_header ( ) , max_conversations = CONVERSATIONS_PER_REQUEST , max_events_per_conversation = 1 , sync_filter = [ hangouts_pb2 . SYNC_FILTER_INBOX , hangouts_pb2 . SYNC_FILTER_ARCHIVED , ] ) for _ in range ( MAX_CONVERSATION_PAGES ) : logger . info ( 'Requesting conversations page %s' , request . last_event_timestamp ) response = await client . sync_recent_conversations ( request ) conv_states = list ( response . conversation_state ) + conv_states sync_timestamp = parsers . from_timestamp ( response . response_header . current_server_time ) if response . continuation_end_timestamp == 0 : logger . info ( 'Reached final conversations page' ) break else : request . last_event_timestamp = response . continuation_end_timestamp else : logger . warning ( 'Exceeded maximum number of conversation pages' ) logger . info ( 'Synced %s total conversations' , len ( conv_states ) ) return conv_states , sync_timestamp
3865	async def leave ( self ) : is_group_conversation = ( self . _conversation . type == hangouts_pb2 . CONVERSATION_TYPE_GROUP ) try : if is_group_conversation : await self . _client . remove_user ( hangouts_pb2 . RemoveUserRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , ) ) else : await self . _client . delete_conversation ( hangouts_pb2 . DeleteConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , delete_upper_bound_timestamp = parsers . to_timestamp ( datetime . datetime . now ( tz = datetime . timezone . utc ) ) ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to leave conversation: {}' . format ( e ) ) raise
11830	def child_node ( self , problem , action ) : "Fig. 3.10" next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
10094	def get_template ( self , template_id , version = None , timeout = None ) : if ( version ) : return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version ) , self . HTTP_GET , timeout = timeout ) else : return self . _api_request ( self . TEMPLATES_SPECIFIC_ENDPOINT % template_id , self . HTTP_GET , timeout = timeout )
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
72	def deepcopy ( self ) : bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )
9329	def post ( self , url , headers = None , params = None , ** kwargs ) : if len ( kwargs ) > 1 : raise InvalidArgumentsError ( "Too many extra args ({} > 1)" . format ( len ( kwargs ) ) ) if kwargs : kwarg = next ( iter ( kwargs ) ) if kwarg not in ( "json" , "data" ) : raise InvalidArgumentsError ( "Invalid kwarg: " + kwarg ) resp = self . session . post ( url , headers = headers , params = params , ** kwargs ) resp . raise_for_status ( ) return _to_json ( resp )
1179	def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
4412	def fetch ( self , key : object , default = None ) : return self . _user_data . get ( key , default )
5323	def _interrupt_read ( self ) : data = self . _device . read ( ENDPOINT , REQ_INT_LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
2483	def write_document ( document , out , validate = True ) : if validate : messages = [ ] messages = document . validate ( messages ) if messages : raise InvalidDocumentError ( messages ) writer = Writer ( document , out ) writer . write ( )
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
3061	def positional ( max_positional_args ) : def positional_decorator ( wrapped ) : @ functools . wraps ( wrapped ) def positional_wrapper ( * args , ** kwargs ) : if len ( args ) > max_positional_args : plural_s = '' if max_positional_args != 1 : plural_s = 's' message = ( '{function}() takes at most {args_max} positional ' 'argument{plural} ({args_given} given)' . format ( function = wrapped . __name__ , args_max = max_positional_args , args_given = len ( args ) , plural = plural_s ) ) if positional_parameters_enforcement == POSITIONAL_EXCEPTION : raise TypeError ( message ) elif positional_parameters_enforcement == POSITIONAL_WARNING : logger . warning ( message ) return wrapped ( * args , ** kwargs ) return positional_wrapper if isinstance ( max_positional_args , six . integer_types ) : return positional_decorator else : args , _ , _ , defaults = inspect . getargspec ( max_positional_args ) return positional ( len ( args ) - len ( defaults ) ) ( max_positional_args )
10942	def update_function ( self , param_vals ) : self . opt_obj . update_function ( param_vals ) return self . opt_obj . get_error ( )
2627	def show_summary ( self ) : self . get_instance_state ( ) status_string = "EC2 Summary:\n\tVPC IDs: {}\n\tSubnet IDs: \{}\n\tSecurity Group ID: {}\n\tRunning Instance IDs: {}\n" . format ( self . vpc_id , self . sn_ids , self . sg_id , self . instances ) status_string += "\tInstance States:\n\t\t" self . get_instance_state ( ) for state in self . instance_states . keys ( ) : status_string += "Instance ID: {} State: {}\n\t\t" . format ( state , self . instance_states [ state ] ) status_string += "\n" logger . info ( status_string ) return status_string
8868	def _unique ( self , seq ) : checked = [ ] for e in seq : present = False for c in checked : if str ( c ) == str ( e ) : present = True break if not present : checked . append ( e ) return checked
9052	def covariance ( self ) : r from numpy_sugar . linalg import ddot , sum2diag Q0 = self . _QS [ 0 ] [ 0 ] S0 = self . _QS [ 1 ] return sum2diag ( dot ( ddot ( Q0 , self . v0 * S0 ) , Q0 . T ) , self . v1 )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
652	def sameSegment ( seg1 , seg2 ) : result = True for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print "A synapse with zero permanence encountered" result = False res = sameSynapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
11564	def servo_config ( self , pin , min_pulse = 544 , max_pulse = 2400 ) : self . set_pin_mode ( pin , self . SERVO , self . OUTPUT ) command = [ pin , min_pulse & 0x7f , ( min_pulse >> 7 ) & 0x7f , max_pulse & 0x7f , ( max_pulse >> 7 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . SERVO_CONFIG , command )
11010	def preview ( context ) : config = context . obj pelican ( config , '--verbose' , '--ignore-cache' ) server_proc = None os . chdir ( config [ 'OUTPUT_DIR' ] ) try : try : command = 'python -m http.server ' + str ( PORT ) server_proc = run ( command , bg = True ) time . sleep ( 3 ) click . launch ( 'http://localhost:8000' ) time . sleep ( 5 ) pelican ( config , '--autoreload' ) except Exception : if server_proc is not None : server_proc . kill ( ) raise except KeyboardInterrupt : abort ( context )
2404	def gen_prompt_feats ( self , e_set ) : prompt_toks = nltk . word_tokenize ( e_set . _prompt ) expand_syns = [ ] for word in prompt_toks : synonyms = util_functions . get_wordnet_syns ( word ) expand_syns . append ( synonyms ) expand_syns = list ( chain . from_iterable ( expand_syns ) ) prompt_overlap = [ ] prompt_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 prompt_overlap . append ( len ( [ i for i in j if i in prompt_toks ] ) ) prompt_overlap_prop . append ( prompt_overlap [ len ( prompt_overlap ) - 1 ] / float ( tok_length ) ) expand_overlap = [ ] expand_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 expand_overlap . append ( len ( [ i for i in j if i in expand_syns ] ) ) expand_overlap_prop . append ( expand_overlap [ len ( expand_overlap ) - 1 ] / float ( tok_length ) ) prompt_arr = numpy . array ( ( prompt_overlap , prompt_overlap_prop , expand_overlap , expand_overlap_prop ) ) . transpose ( ) return prompt_arr . copy ( )
11851	def add_walls ( self ) : "Put walls around the entire perimeter of the grid." for x in range ( self . width ) : self . add_thing ( Wall ( ) , ( x , 0 ) ) self . add_thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add_thing ( Wall ( ) , ( 0 , y ) ) self . add_thing ( Wall ( ) , ( self . width - 1 , y ) )
5977	def mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0.0 , 0.0 ) ) : mask = np . full ( shape , True ) centres_arcsec = mask_centres_from_shape_pixel_scale_and_centre ( shape = mask . shape , pixel_scale = pixel_scale , centre = centre ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : y_arcsec = ( y - centres_arcsec [ 0 ] ) * pixel_scale x_arcsec = ( x - centres_arcsec [ 1 ] ) * pixel_scale r_arcsec = np . sqrt ( x_arcsec ** 2 + y_arcsec ** 2 ) if outer_radius_arcsec >= r_arcsec >= inner_radius_arcsec : mask [ y , x ] = False return mask
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( int ( self . num_retries ) >= 0 and v <= self . num_retries and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
4764	def is_equal_to ( self , other , ** kwargs ) : if self . _check_dict_like ( self . val , check_values = False , return_as_bool = True ) and self . _check_dict_like ( other , check_values = False , return_as_bool = True ) : if self . _dict_not_equal ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) : self . _dict_err ( self . val , other , ignore = kwargs . get ( 'ignore' ) , include = kwargs . get ( 'include' ) ) else : if self . val != other : self . _err ( 'Expected <%s> to be equal to <%s>, but was not.' % ( self . val , other ) ) return self
7972	def _remove_timeout_handler ( self , handler ) : if handler not in self . timeout_handlers : return self . timeout_handlers . remove ( handler ) for thread in self . timeout_threads : if thread . method . im_self is handler : thread . stop ( )
5	def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
6014	def load_positions ( positions_path ) : with open ( positions_path ) as f : position_string = f . readlines ( ) positions = [ ] for line in position_string : position_list = ast . literal_eval ( line ) positions . append ( position_list ) return positions
8371	def run ( src , grammar = NODEBOX , format = None , outputfile = None , iterations = 1 , buff = None , window = True , title = None , fullscreen = None , close_window = False , server = False , port = 7777 , show_vars = False , vars = None , namespace = None , run_shell = False , args = [ ] , verbose = False , background_thread = True ) : sys . argv = [ sys . argv [ 0 ] ] + args create_args = [ src , grammar , format , outputfile , iterations , buff , window , title , fullscreen , server , port , show_vars ] create_kwargs = dict ( vars = vars , namespace = namespace ) run_args = [ src ] run_kwargs = dict ( iterations = iterations , frame_limiter = window , verbose = verbose , run_forever = window and not ( close_window or bool ( outputfile ) ) , ) if background_thread : sbot_thread = ShoebotThread ( create_args = create_args , create_kwargs = create_kwargs , run_args = run_args , run_kwargs = run_kwargs , send_sigint = run_shell ) sbot_thread . start ( ) sbot = sbot_thread . sbot else : print ( 'background thread disabled' ) if run_shell : raise ValueError ( 'UI Must run in a separate thread to shell and shell needs main thread' ) sbot_thread = None sbot = create_bot ( * create_args , ** create_kwargs ) sbot . run ( * run_args , ** run_kwargs ) if run_shell : import shoebot . sbio . shell shell = shoebot . sbio . shell . ShoebotCmd ( sbot , trusted = True ) try : shell . cmdloop ( ) except KeyboardInterrupt as e : publish_event ( QUIT_EVENT ) if verbose : raise else : return elif background_thread : try : while sbot_thread . is_alive ( ) : sleep ( 1 ) except KeyboardInterrupt : publish_event ( QUIT_EVENT ) if all ( ( background_thread , sbot_thread ) ) : sbot_thread . join ( ) return sbot
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
3916	def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
5237	def get_interval ( ticker , session ) -> Session : if '_' not in session : session = f'{session}_normal_0_0' interval = Intervals ( ticker = ticker ) ss_info = session . split ( '_' ) return getattr ( interval , f'market_{ss_info.pop(1)}' ) ( * ss_info )
11232	def run_excel_to_html ( ) : parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
9744	def on_packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get_3d_markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
994	def _generateRangeDescription ( self , ranges ) : desc = "" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " return desc
809	def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
9160	def delete_roles_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_roles = request . json with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_role_requests ( cursor , uuid_ , posted_roles ) resp = request . response resp . status_int = 200 return resp
9720	async def release_control ( self ) : cmd = "releasecontrol" return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
4553	def set_colors ( self , colors , pos ) : self . _colors = colors self . _pos = pos end = self . _pos + self . numLEDs if end > len ( self . _colors ) : raise ValueError ( 'Needed %d colors but found %d' % ( end , len ( self . _colors ) ) )
10938	def update_eig_J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc_residuals ( ) for a in range ( min ( [ self . num_eig_dirs , vls . size ] ) ) : stif_dir = vcs [ - ( a + 1 ) ] dl = self . eig_dl _ = self . update_function ( self . param_vals + dl * stif_dir ) res1 = self . calc_residuals ( ) grad_stif = ( res1 - res0 ) / dl self . _rank_1_J_update ( stif_dir , grad_stif ) self . JTJ = np . dot ( self . J , self . J . T ) _ = self . update_function ( self . param_vals )
3589	def set_color ( self , r , g , b ) : command = '\x58\x01\x03\x01\xFF\x00{0}{1}{2}' . format ( chr ( r & 0xFF ) , chr ( g & 0xFF ) , chr ( b & 0xFF ) ) self . _color . write_value ( command )
11609	def multiply ( self , multiplier , axis = None ) : if self . finalized : if multiplier . ndim == 1 : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : sz = len ( multiplier ) multiplier_mat = lil_matrix ( ( sz , sz ) ) multiplier_mat . setdiag ( multiplier ) for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] * multiplier_mat elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices ] else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif multiplier . ndim == 2 : if axis == 0 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices , hid ] elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier ) elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : multiplier_vec = multiplier [ hid , : ] multiplier_vec = multiplier_vec . ravel ( ) self . data [ hid ] . data *= multiplier_vec . repeat ( np . diff ( self . data [ hid ] . indptr ) ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif isinstance ( multiplier , Sparse3DMatrix ) : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier . data [ hid ] ) else : raise RuntimeError ( 'The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
4980	def get ( self , request , enterprise_uuid , course_id ) : enrollment_course_mode = request . GET . get ( 'course_mode' ) enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if not enrollment_course_mode : return redirect ( LMS_DASHBOARD_URL ) enrollment_api_client = EnrollmentApiClient ( ) course_modes = enrollment_api_client . get_course_modes ( course_id ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_customer . uuid ) if not course_modes : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTHCE000' log_message = ( 'No course_modes for course_id {course_id} for enterprise_catalog_uuid ' '{enterprise_catalog_uuid}.' 'The following error was presented to ' 'user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_catalog_uuid = enterprise_catalog_uuid , course_id = course_id , error_code = error_code ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) selected_course_mode = None for course_mode in course_modes : if course_mode [ 'slug' ] == enrollment_course_mode : selected_course_mode = course_mode break if not selected_course_mode : return redirect ( LMS_DASHBOARD_URL ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'course-landing-page-enrollment' , request . user . id , course_id , request . get_full_path ( ) ) DataSharingConsent . objects . update_or_create ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer_user . enterprise_customer , defaults = { 'granted' : True } , ) audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' , 'honor' ] ) if selected_course_mode [ 'slug' ] in audit_modes : enrollment_api_client . enroll_user_in_course ( request . user . username , course_id , selected_course_mode [ 'slug' ] ) return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) premium_flow = LMS_START_PREMIUM_COURSE_FLOW_URL . format ( course_id = course_id ) if enterprise_catalog_uuid : premium_flow += '?catalog={catalog_uuid}' . format ( catalog_uuid = enterprise_catalog_uuid ) return redirect ( premium_flow )
5532	def batch_process ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : list ( self . batch_processor ( zoom , tile , multi , max_chunksize ) )
3583	def _print_tree ( self ) : objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
11908	def to_bipartite_matrix ( A ) : m , n = A . shape return four_blocks ( zeros ( m , m ) , A , A . T , zeros ( n , n ) )
6703	def enter_password_change ( self , username = None , old_password = None ) : from fabric . state import connections from fabric . network import disconnect_all r = self . local_renderer r . genv . user = r . genv . user or username r . pc ( 'Changing password for user {user} via interactive prompts.' ) r . env . old_password = r . env . default_passwords [ self . genv . user ] r . env . new_password = self . env . passwords [ self . genv . user ] if old_password : r . env . old_password = old_password prompts = { '(current) UNIX password: ' : r . env . old_password , 'Enter new UNIX password: ' : r . env . new_password , 'Retype new UNIX password: ' : r . env . new_password , } print ( 'prompts:' , prompts ) r . env . password = r . env . old_password with self . settings ( warn_only = True ) : ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) if ret . return_code in ( 1 , 6 ) or 'hello' in ret : self . genv . password = r . env . old_password elif self . genv . user in self . genv . user_passwords : self . genv . password = r . env . new_password else : self . genv . password = None print ( 'using password:' , self . genv . password ) with self . settings ( prompts = prompts ) : ret = r . _run ( 'echo checking for expired password' ) print ( 'ret:[%s]' % ret ) do_disconnect = 'passwd: password updated successfully' in ret print ( 'do_disconnect:' , do_disconnect ) if do_disconnect : disconnect_all ( ) self . genv . password = r . env . new_password
3207	def delete ( self , batch_id ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _delete ( url = self . _build_path ( batch_id ) )
2891	def connect_outgoing ( self , outgoing_task , outgoing_task_node , sequence_flow_node , is_default ) : self . task . connect_outgoing ( outgoing_task , sequence_flow_node . get ( 'id' ) , sequence_flow_node . get ( 'name' , None ) , self . parser . _parse_documentation ( sequence_flow_node , task_parser = self ) )
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
5883	def post_cleanup ( self ) : parse_tags = [ 'p' ] if self . config . parse_lists : parse_tags . extend ( [ 'ul' , 'ol' ] ) if self . config . parse_headers : parse_tags . extend ( [ 'h1' , 'h2' , 'h3' , 'h4' , 'h5' , 'h6' ] ) target_node = self . article . top_node node = self . add_siblings ( target_node ) for elm in self . parser . getChildren ( node ) : e_tag = self . parser . getTag ( elm ) if e_tag not in parse_tags : if ( self . is_highlink_density ( elm ) or self . is_table_and_no_para_exist ( elm ) or not self . is_nodescore_threshold_met ( node , elm ) ) : self . parser . remove ( elm ) return node
2198	def platform_data_dir ( ) : if LINUX : dpath_ = os . environ . get ( 'XDG_DATA_HOME' , '~/.local/share' ) elif DARWIN : dpath_ = '~/Library/Application Support' elif WIN32 : dpath_ = os . environ . get ( 'APPDATA' , '~/AppData/Roaming' ) else : raise '~/AppData/Local' dpath = normpath ( expanduser ( dpath_ ) ) return dpath
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
12554	def sav_to_pandas_rpy2 ( input_file ) : import pandas . rpy . common as com w = com . robj . r ( 'foreign::read.spss("%s", to.data.frame=TRUE)' % input_file ) return com . convert_robj ( w )
5918	def outfile ( self , p ) : if self . outdir is not None : return os . path . join ( self . outdir , os . path . basename ( p ) ) else : return p
4231	def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
5774	def ecdsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'ec' : raise ValueError ( 'The key specified is not an EC private key' ) return _sign ( private_key , data , hash_algorithm )
5607	def resample_from_array ( in_raster = None , in_affine = None , out_tile = None , in_crs = None , resampling = "nearest" , nodataval = 0 ) : if isinstance ( in_raster , ma . MaskedArray ) : pass if isinstance ( in_raster , np . ndarray ) : in_raster = ma . MaskedArray ( in_raster , mask = in_raster == nodataval ) elif isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_crs = in_raster . crs in_raster = in_raster . data elif isinstance ( in_raster , tuple ) : in_raster = ma . MaskedArray ( data = np . stack ( in_raster ) , mask = np . stack ( [ band . mask if isinstance ( band , ma . masked_array ) else np . where ( band == nodataval , True , False ) for band in in_raster ] ) , fill_value = nodataval ) else : raise TypeError ( "wrong input data type: %s" % type ( in_raster ) ) if in_raster . ndim == 2 : in_raster = ma . expand_dims ( in_raster , axis = 0 ) elif in_raster . ndim == 3 : pass else : raise TypeError ( "input array must have 2 or 3 dimensions" ) if in_raster . fill_value != nodataval : ma . set_fill_value ( in_raster , nodataval ) out_shape = ( in_raster . shape [ 0 ] , ) + out_tile . shape dst_data = np . empty ( out_shape , in_raster . dtype ) in_raster = ma . masked_array ( data = in_raster . filled ( ) , mask = in_raster . mask , fill_value = nodataval ) reproject ( in_raster , dst_data , src_transform = in_affine , src_crs = in_crs if in_crs else out_tile . crs , dst_transform = out_tile . affine , dst_crs = out_tile . crs , resampling = Resampling [ resampling ] ) return ma . MaskedArray ( dst_data , mask = dst_data == nodataval )
12169	def _dispatch ( self , event , listener , * args , ** kwargs ) : if ( asyncio . iscoroutinefunction ( listener ) or isinstance ( listener , functools . partial ) and asyncio . iscoroutinefunction ( listener . func ) ) : return self . _dispatch_coroutine ( event , listener , * args , ** kwargs ) return self . _dispatch_function ( event , listener , * args , ** kwargs )
10431	def selectrowpartialmatch ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) for cell in object_handle . AXRows : if re . search ( row_text , cell . AXChildren [ 0 ] . AXValue ) : if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1 raise LdtpServerException ( u"Unable to select row: %s" % row_text )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
13390	def format_pathname ( pathname , max_length ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( pathname ) > max_length : pathname = "...{}" . format ( pathname [ - ( max_length - 3 ) : ] ) return pathname
5501	def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
3255	def list_granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
10297	def get_undefined_namespace_names ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) and exc . namespace == namespace }
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
3136	def get ( self , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
8217	def trigger_fullscreen_action ( self , fullscreen ) : action = self . action_group . get_action ( 'fullscreen' ) action . set_active ( fullscreen )
3811	def get_request_header ( self ) : if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , ** attributes ) : geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
712	def _launchWorkers ( self , cmdLine , numWorkers ) : self . _workers = [ ] for i in range ( numWorkers ) : stdout = tempfile . NamedTemporaryFile ( delete = False ) stderr = tempfile . NamedTemporaryFile ( delete = False ) p = subprocess . Popen ( cmdLine , bufsize = 1 , env = os . environ , shell = True , stdin = None , stdout = stdout , stderr = stderr ) p . _stderr_file = stderr p . _stdout_file = stdout self . _workers . append ( p )
9890	def _boottime_linux ( ) : global __boottime try : f = open ( '/proc/stat' , 'r' ) for line in f : if line . startswith ( 'btime' ) : __boottime = int ( line . split ( ) [ 1 ] ) if datetime is None : raise NotImplementedError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime ) except ( IOError , IndexError ) : return None
10405	def bond_canonical_statistics ( microcanonical_statistics , convolution_factors , ** kwargs ) : spanning_cluster = ( 'has_spanning_cluster' in microcanonical_statistics . dtype . names ) ret = np . empty ( 1 , dtype = canonical_statistics_dtype ( spanning_cluster ) ) if spanning_cluster : ret [ 'percolation_probability' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'has_spanning_cluster' ] ) ret [ 'max_cluster_size' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'max_cluster_size' ] ) ret [ 'moments' ] = np . sum ( convolution_factors [ : , np . newaxis ] * microcanonical_statistics [ 'moments' ] , axis = 0 , ) return ret
7275	def seek ( self , relative_position ) : self . _player_interface . Seek ( Int64 ( 1000.0 * 1000 * relative_position ) ) self . seekEvent ( self , relative_position )
13378	def preprocess_dict ( d ) : out_env = { } for k , v in d . items ( ) : if not type ( v ) in PREPROCESSORS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) out_env [ k ] = PREPROCESSORS [ type ( v ) ] ( v ) return out_env
6935	def add_cmds_cpdir ( cpdir , cmdpkl , cpfileglob = 'checkplot*.pkl*' , require_cmd_magcolor = True , save_cmd_pngs = False ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
5979	def edge_pixels_from_mask ( mask ) : edge_pixel_total = total_edge_pixels_from_mask ( mask ) edge_pixels = np . zeros ( edge_pixel_total ) edge_index = 0 regular_index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : edge_pixels [ edge_index ] = regular_index edge_index += 1 regular_index += 1 return edge_pixels
3722	def calculate ( self , T , method ) : r if method == CRC : A , B , C , D = self . CRC_coeffs epsilon = A + B * T + C * T ** 2 + D * T ** 3 elif method == CRC_CONSTANT : epsilon = self . CRC_permittivity elif method in self . tabular_data : epsilon = self . interpolate ( T , method ) return epsilon
11325	def extract_oembeds ( self , text , maxwidth = None , maxheight = None , resource_type = None ) : parser = text_parser ( ) urls = parser . extract_urls ( text ) return self . handle_extracted_urls ( urls , maxwidth , maxheight , resource_type )
3966	def case_insensitive_rename ( src , dst ) : temp_dir = tempfile . mkdtemp ( ) shutil . rmtree ( temp_dir ) shutil . move ( src , temp_dir ) shutil . move ( temp_dir , dst )
1933	def function_signature_for_name_and_inputs ( name : str , inputs : Sequence [ Mapping [ str , Any ] ] ) -> str : return name + SolidityMetadata . tuple_signature_for_components ( inputs )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
5516	def append ( self , data , start ) : if self . _limit is not None and self . _limit > 0 : if self . _start is None : self . _start = start if start - self . _start > self . reset_rate : self . _sum -= round ( ( start - self . _start ) * self . _limit ) self . _start = start self . _sum += len ( data )
10758	def writable_stream ( handle ) : if isinstance ( handle , io . IOBase ) and sys . version_info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . UnsupportedOperation , IOError ) : return False else : return True
7310	def without_tz ( request ) : t = Template ( '{% load tz %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
4874	def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
10329	def get_path_effect ( graph , path , relationship_dict ) : causal_effect = [ ] for predecessor , successor in pairwise ( path ) : if pair_has_contradiction ( graph , predecessor , successor ) : return Effect . ambiguous edges = graph . get_edge_data ( predecessor , successor ) edge_key , edge_relation , _ = rank_edges ( edges ) relation = graph [ predecessor ] [ successor ] [ edge_key ] [ RELATION ] if relation not in relationship_dict or relationship_dict [ relation ] == 0 : return Effect . no_effect causal_effect . append ( relationship_dict [ relation ] ) final_effect = reduce ( lambda x , y : x * y , causal_effect ) return Effect . activation if final_effect == 1 else Effect . inhibition
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
9873	def CherryPyWSGIServer ( bind_addr , wsgi_app , numthreads = 10 , server_name = None , max = - 1 , request_queue_size = 5 , timeout = 10 , shutdown_timeout = 5 ) : max_threads = max if max_threads < 0 : max_threads = 0 return Rocket ( bind_addr , 'wsgi' , { 'wsgi_app' : wsgi_app } , min_threads = numthreads , max_threads = max_threads , queue_size = request_queue_size , timeout = timeout )
5207	def format_output ( data : pd . DataFrame , source , col_maps = None ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) if source == 'bdp' : req_cols = [ 'ticker' , 'field' , 'value' ] else : req_cols = [ 'ticker' , 'field' , 'name' , 'value' , 'position' ] if any ( col not in data for col in req_cols ) : return pd . DataFrame ( ) if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) if source == 'bdp' : res = pd . DataFrame ( pd . concat ( [ pd . Series ( { ** { 'ticker' : t } , ** grp . set_index ( 'field' ) . value . to_dict ( ) } ) for t , grp in data . groupby ( 'ticker' ) ] , axis = 1 , sort = False ) ) . transpose ( ) . set_index ( 'ticker' ) else : res = pd . DataFrame ( pd . concat ( [ grp . loc [ : , [ 'name' , 'value' ] ] . set_index ( 'name' ) . transpose ( ) . reset_index ( drop = True ) . assign ( ticker = t ) for ( t , _ ) , grp in data . groupby ( [ 'ticker' , 'position' ] ) ] , sort = False ) ) . reset_index ( drop = True ) . set_index ( 'ticker' ) res . columns . name = None if col_maps is None : col_maps = dict ( ) return res . rename ( columns = lambda vv : col_maps . get ( vv , vv . lower ( ) . replace ( ' ' , '_' ) . replace ( '-' , '_' ) ) ) . apply ( pd . to_numeric , errors = 'ignore' , downcast = 'float' )
743	def writeToFile ( self , f , packed = True ) : schema = self . getSchema ( ) proto = schema . new_message ( ) self . write ( proto ) if packed : proto . write_packed ( f ) else : proto . write ( f )
11146	def get_file_info ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) fileName = os . path . basename ( relativePath ) isRepoFile , fileOnDisk , infoOnDisk , classOnDisk = self . is_repository_file ( relativePath ) if not isRepoFile : return None , "file is not a registered repository file." if not infoOnDisk : return None , "file is a registered repository file but info file missing" fileInfoPath = os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % fileName ) try : with open ( fileInfoPath , 'rb' ) as fd : info = pickle . load ( fd ) except Exception as err : return None , "Unable to read file info from disk (%s)" % str ( err ) return info , ''
6125	def plot_image ( image , plot_origin = True , mask = None , extract_array_from_mask = False , zoom_around_mask = False , should_plot_border = False , positions = None , as_subplot = False , units = 'arcsec' , kpc_per_arcsec = None , figsize = ( 7 , 7 ) , aspect = 'square' , cmap = 'jet' , norm = 'linear' , norm_min = None , norm_max = None , linthresh = 0.05 , linscale = 0.01 , cb_ticksize = 10 , cb_fraction = 0.047 , cb_pad = 0.01 , cb_tick_values = None , cb_tick_labels = None , title = 'Image' , titlesize = 16 , xlabelsize = 16 , ylabelsize = 16 , xyticksize = 16 , mask_pointsize = 10 , position_pointsize = 30 , grid_pointsize = 1 , output_path = None , output_format = 'show' , output_filename = 'image' ) : origin = get_origin ( array = image , plot_origin = plot_origin ) array_plotters . plot_array ( array = image , origin = origin , mask = mask , extract_array_from_mask = extract_array_from_mask , zoom_around_mask = zoom_around_mask , should_plot_border = should_plot_border , positions = positions , as_subplot = as_subplot , units = units , kpc_per_arcsec = kpc_per_arcsec , figsize = figsize , aspect = aspect , cmap = cmap , norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale , cb_ticksize = cb_ticksize , cb_fraction = cb_fraction , cb_pad = cb_pad , cb_tick_values = cb_tick_values , cb_tick_labels = cb_tick_labels , title = title , titlesize = titlesize , xlabelsize = xlabelsize , ylabelsize = ylabelsize , xyticksize = xyticksize , mask_pointsize = mask_pointsize , position_pointsize = position_pointsize , grid_pointsize = grid_pointsize , output_path = output_path , output_format = output_format , output_filename = output_filename )
9367	def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for _ in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )
11601	def save_model ( self , request , obj , form , change ) : obj . author = request . user obj . save ( )
853	def appendRecord ( self , record ) : assert self . _file is not None assert self . _mode == self . _FILE_WRITE_MODE assert isinstance ( record , ( list , tuple ) ) , "unexpected record type: " + repr ( type ( record ) ) assert len ( record ) == self . _fieldCount , "len(record): %s, fieldCount: %s" % ( len ( record ) , self . _fieldCount ) if self . _recordCount == 0 : names , types , specials = zip ( * self . getFields ( ) ) for line in names , types , specials : self . _writer . writerow ( line ) self . _updateSequenceInfo ( record ) line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] self . _writer . writerow ( line ) self . _recordCount += 1
10300	def get_names_including_errors ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_names_including_errors_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
8283	def _curvepoint ( self , t , x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , handles = False ) : mint = 1 - t x01 = x0 * mint + x1 * t y01 = y0 * mint + y1 * t x12 = x1 * mint + x2 * t y12 = y1 * mint + y2 * t x23 = x2 * mint + x3 * t y23 = y2 * mint + y3 * t out_c1x = x01 * mint + x12 * t out_c1y = y01 * mint + y12 * t out_c2x = x12 * mint + x23 * t out_c2y = y12 * mint + y23 * t out_x = out_c1x * mint + out_c2x * t out_y = out_c1y * mint + out_c2y * t if not handles : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y ) else : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y , x01 , y01 , x23 , y23 )
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
2061	def _declare ( self , var ) : if var . name in self . _declarations : raise ValueError ( 'Variable already declared' ) self . _declarations [ var . name ] = var return var
760	def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) numChanged += 1 skipOne = True if numChanged == numNear : break
7689	def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values ( ) pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , ** kwargs )
2965	def _sm_start ( self , * args , ** kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , ** kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
1493	def _get_next_timeout_interval ( self ) : if len ( self . timer_tasks ) == 0 : return sys . maxsize else : next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) return next_timeout_interval
3623	def __pre_delete_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE pre_delete FOR %s' , instance . __class__ ) self . delete_record ( instance )
12985	def getCompressMod ( self ) : if self . compressMode == COMPRESS_MODE_ZLIB : return zlib if self . compressMode == COMPRESS_MODE_BZ2 : return bz2 if self . compressMode == COMPRESS_MODE_LZMA : global _lzmaMod if _lzmaMod is not None : return _lzmaMod try : import lzma _lzmaMod = lzma return _lzmaMod except : try : from backports import lzma _lzmaMod = lzma return _lzmaMod except : pass try : import lzmaffi as lzma _lzmaMod = lzma return _lzmaMod except : pass raise ImportError ( "Requested compress mode is lzma and could not find a module providing lzma support. Tried: 'lzma', 'backports.lzma', 'lzmaffi' and none of these were available. Please install one of these, or to use an unlisted implementation, set IndexedRedis.fields.compressed._lzmaMod to the module (must implement standard python compression interface)" )
4180	def _coeff4 ( N , a0 , a1 , a2 , a3 ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) N1 = N - 1. w = a0 - a1 * cos ( 2. * pi * n / N1 ) + a2 * cos ( 4. * pi * n / N1 ) - a3 * cos ( 6. * pi * n / N1 ) return w
7539	def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) if hetprob > homoa : return True , bestprob else : return False , bestprob
5228	def load_info ( cat ) : res = _load_yaml_ ( f'{PKG_PATH}/markets/{cat}.yml' ) root = os . environ . get ( 'BBG_ROOT' , '' ) . replace ( '\\' , '/' ) if not root : return res for cat , ovrd in _load_yaml_ ( f'{root}/markets/{cat}.yml' ) . items ( ) : if isinstance ( ovrd , dict ) : if cat in res : res [ cat ] . update ( ovrd ) else : res [ cat ] = ovrd if isinstance ( ovrd , list ) and isinstance ( res [ cat ] , list ) : res [ cat ] += ovrd return res
2845	def enumerate_device_serials ( vid = FT232H_VID , pid = FT232H_PID ) : try : ctx = None ctx = ftdi . new ( ) device_list = None count , device_list = ftdi . usb_find_all ( ctx , vid , pid ) if count < 0 : raise RuntimeError ( 'ftdi_usb_find_all returned error {0}: {1}' . format ( count , ftdi . get_error_string ( self . _ctx ) ) ) devices = [ ] while device_list is not None : ret , manufacturer , description , serial = ftdi . usb_get_strings ( ctx , device_list . dev , 256 , 256 , 256 ) if serial is not None : devices . append ( serial ) device_list = device_list . next return devices finally : if device_list is not None : ftdi . list_free ( device_list ) if ctx is not None : ftdi . free ( ctx )
5141	def new_comment ( self , string , start , end , line ) : prefix = line [ : start [ 1 ] ] if prefix . strip ( ) : self . current_block . add ( string , start , end , line ) else : block = Comment ( start [ 0 ] , end [ 0 ] , string ) self . blocks . append ( block ) self . current_block = block
3106	def code_challenge ( verifier ) : digest = hashlib . sha256 ( verifier ) . digest ( ) return base64 . urlsafe_b64encode ( digest ) . rstrip ( b'=' )
624	def neighborhood ( centerIndex , radius , dimensions ) : centerPosition = coordinatesFromIndex ( centerIndex , dimensions ) intervals = [ ] for i , dimension in enumerate ( dimensions ) : left = max ( 0 , centerPosition [ i ] - radius ) right = min ( dimension - 1 , centerPosition [ i ] + radius ) intervals . append ( xrange ( left , right + 1 ) ) coords = numpy . array ( list ( itertools . product ( * intervals ) ) ) return numpy . ravel_multi_index ( coords . T , dimensions )
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
7274	def play_pause ( self ) : self . _player_interface . PlayPause ( ) self . _is_playing = not self . _is_playing if self . _is_playing : self . playEvent ( self ) else : self . pauseEvent ( self )
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
5815	def _read_callback ( connection_id , data_buffer , data_length_pointer ) : self = None try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 bytes_requested = deref ( data_length_pointer ) timeout = socket . gettimeout ( ) error = None data = b'' try : while len ( data ) < bytes_requested : if timeout is not None and timeout > 0.0 : read_ready , _ , _ = select . select ( [ socket ] , [ ] , [ ] , timeout ) if len ( read_ready ) == 0 : raise socket_ . error ( errno . EAGAIN , 'timed out' ) chunk = socket . recv ( bytes_requested - len ( data ) ) data += chunk if chunk == b'' : if len ( data ) == 0 : if timeout is None : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort break except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if self and not self . _done_handshake : if len ( data ) >= 3 and len ( self . _server_hello ) == 0 : valid_record_type = data [ 0 : 1 ] in set ( [ b'\x15' , b'\x16' ] ) valid_protocol_version = data [ 1 : 3 ] in set ( [ b'\x03\x00' , b'\x03\x01' , b'\x03\x02' , b'\x03\x03' , b'\x03\x04' ] ) if not valid_record_type or not valid_protocol_version : self . _server_hello += data + _read_remaining ( socket ) return SecurityConst . errSSLProtocol self . _server_hello += data write_to_buffer ( data_buffer , data ) pointer_set ( data_length_pointer , len ( data ) ) if len ( data ) != bytes_requested : return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : if self : self . _exception = e return SecurityConst . errSSLClosedAbort
12038	def matrixToDicts ( data ) : if "float" in str ( type ( data [ 0 ] ) ) : d = { } for x in range ( len ( data ) ) : d [ data . dtype . names [ x ] ] = data [ x ] return d l = [ ] for y in range ( len ( data ) ) : d = { } for x in range ( len ( data [ y ] ) ) : d [ data . dtype . names [ x ] ] = data [ y ] [ x ] l . append ( d ) return l
10299	def group_errors ( graph : BELGraph ) -> Mapping [ str , List [ int ] ] : warning_summary = defaultdict ( list ) for _ , exc , _ in graph . warnings : warning_summary [ str ( exc ) ] . append ( exc . line_number ) return dict ( warning_summary )
9861	def sync_update_info ( self , * _ ) : loop = asyncio . get_event_loop ( ) task = loop . create_task ( self . update_info ( ) ) loop . run_until_complete ( task )
10779	def _remove_closest_particle ( self , p ) : dp = self . pos - p dist2 = ( dp * dp ) . sum ( axis = 1 ) ind = dist2 . argmin ( ) rp = self . pos [ ind ] . copy ( ) self . pos = np . delete ( self . pos , ind , axis = 0 ) return rp
8787	def set ( self , model , value ) : self . validate ( value ) self . _pop ( model ) value = self . serialize ( value ) model . tags . append ( value )
12855	def subtree ( events ) : stack = 0 for obj in events : if obj [ 'type' ] == ENTER : stack += 1 elif obj [ 'type' ] == EXIT : if stack == 0 : break stack -= 1 yield obj
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
7028	def objectid_search ( gaiaid , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'phot_bp_mean_mag' , 'phot_rp_mean_mag' , 'l' , 'b' , 'parallax, parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns} from {{table}} where " "source_id = {gaiaid}" ) formatted_query = query . format ( columns = ', ' . join ( columns ) , gaiaid = gaiaid ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
9093	def _update_namespace ( self , namespace : Namespace ) -> None : old_entry_identifiers = self . _get_old_entry_identifiers ( namespace ) new_count = 0 skip_count = 0 for model in self . _iterate_namespace_models ( ) : if self . _get_identifier ( model ) in old_entry_identifiers : continue entry = self . _create_namespace_entry_from_model ( model , namespace = namespace ) if entry is None or entry . name is None : skip_count += 1 continue new_count += 1 self . session . add ( entry ) t = time . time ( ) log . info ( 'got %d new entries. skipped %d entries missing names. committing models' , new_count , skip_count ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t )
9775	def logs ( ctx , past , follow , hide_time ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if past : try : response = PolyaxonClient ( ) . job . logs ( user , project_name , _job , stream = False ) get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . job . logs ( user , project_name , _job , message_handler = get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
9670	def normalize ( self , string ) : return '' . join ( [ self . _normalize . get ( x , x ) for x in nfd ( string ) ] )
4150	def plot ( self , filename = None , norm = False , ylim = None , sides = None , ** kargs ) : import pylab from pylab import ylim as plt_ylim _ = self . psd if sides is not None : if sides not in self . _sides_choices : raise errors . SpectrumChoiceError ( sides , self . _sides_choices ) if sides is None or sides == self . sides : frequencies = self . frequencies ( ) psd = self . psd sides = self . sides elif sides is not None : if self . datatype == 'complex' : if sides == 'onesided' : raise ValueError ( "sides cannot be one-sided with complex data" ) logging . debug ( "sides is different from the one provided. Converting PSD" ) frequencies = self . frequencies ( sides = sides ) psd = self . get_converted_psd ( sides ) if len ( psd ) != len ( frequencies ) : raise ValueError ( "PSD length is %s and freq length is %s" % ( len ( psd ) , len ( frequencies ) ) ) if 'ax' in list ( kargs . keys ( ) ) : save_ax = pylab . gca ( ) pylab . sca ( kargs [ 'ax' ] ) rollback = True del kargs [ 'ax' ] else : rollback = False if norm : pylab . plot ( frequencies , 10 * stools . log10 ( psd / max ( psd ) ) , ** kargs ) else : pylab . plot ( frequencies , 10 * stools . log10 ( psd ) , ** kargs ) pylab . xlabel ( 'Frequency' ) pylab . ylabel ( 'Power (dB)' ) pylab . grid ( True ) if ylim : plt_ylim ( ylim ) if sides == 'onesided' : pylab . xlim ( 0 , self . sampling / 2. ) elif sides == 'twosided' : pylab . xlim ( 0 , self . sampling ) elif sides == 'centerdc' : pylab . xlim ( - self . sampling / 2. , self . sampling / 2. ) if filename : pylab . savefig ( filename ) if rollback : pylab . sca ( save_ax ) del psd , frequencies
4095	def AICc ( N , rho , k , norm = True ) : r from numpy import log , array p = k res = log ( rho ) + 2. * ( p + 1 ) / ( N - p - 2 ) return res
3147	def _iterate ( self , url , ** queryparams ) : if 'fields' in queryparams : if 'total_items' not in queryparams [ 'fields' ] . split ( ',' ) : queryparams [ 'fields' ] += ',total_items' queryparams . pop ( "offset" , None ) queryparams . pop ( "count" , None ) result = self . _mc_client . _get ( url = url , offset = 0 , count = 1000 , ** queryparams ) total = result [ 'total_items' ] if total > 1000 : for offset in range ( 1 , int ( total / 1000 ) + 1 ) : result = merge_results ( result , self . _mc_client . _get ( url = url , offset = int ( offset * 1000 ) , count = 1000 , ** queryparams ) ) return result else : return result
7956	def _initiate_starttls ( self , ** kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , ** kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
3274	def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n Error condition: '{}'" . format ( self . err_condition ) return s
13229	def create_jwt ( integration_id , private_key_path ) : integration_id = int ( integration_id ) with open ( private_key_path , 'rb' ) as f : cert_bytes = f . read ( ) now = datetime . datetime . now ( ) expiration_time = now + datetime . timedelta ( minutes = 9 ) payload = { 'iat' : int ( now . timestamp ( ) ) , 'exp' : int ( expiration_time . timestamp ( ) ) , 'iss' : integration_id } return jwt . encode ( payload , cert_bytes , algorithm = 'RS256' )
10460	def _ldtpize_accessible ( self , acc ) : actual_role = self . _get_role ( acc ) label = self . _get_title ( acc ) if re . match ( "AXWindow" , actual_role , re . M | re . U | re . L ) : strip = r"( |\n)" else : strip = r"( |:|\.|_|\n)" if label : label = re . sub ( strip , u"" , label ) role = abbreviated_roles . get ( actual_role , "ukn" ) if self . _ldtp_debug and role == "ukn" : print ( actual_role , acc ) return role , label
13737	def get_context ( request , model = None ) : param_values = get_param_values ( request , model = model ) context = param_values . pop ( 'orb_context' , { } ) if isinstance ( context , ( unicode , str ) ) : context = projex . rest . unjsonify ( context ) has_limit = 'limit' in context or 'limit' in param_values orb_context = orb . Context ( ** context ) used = set ( ) query_context = { } for key in orb . Context . Defaults : if key in param_values : used . add ( key ) query_context [ key ] = param_values . get ( key ) schema_values = { } if model : for key , value in request . matchdict . items ( ) : if model . schema ( ) . column ( key , raise_ = False ) : schema_values [ key ] = value for key , value in param_values . items ( ) : root_key = key . split ( '.' ) [ 0 ] schema_object = model . schema ( ) . column ( root_key , raise_ = False ) or model . schema ( ) . collector ( root_key ) if schema_object : value = param_values . pop ( key ) if isinstance ( schema_object , orb . Collector ) and type ( value ) not in ( tuple , list ) : value = [ value ] schema_values [ key ] = value query_context [ 'scope' ] = { 'request' : request } try : default_context = request . orb_default_context except AttributeError : try : query_context [ 'scope' ] . update ( request . orb_scope ) except AttributeError : pass else : if 'scope' in default_context : query_context [ 'scope' ] . update ( default_context . pop ( 'scope' ) ) for k , v in default_context . items ( ) : query_context . setdefault ( k , v ) orb_context . update ( query_context ) return schema_values , orb_context
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
2580	def checkpoint ( self , tasks = None ) : with self . checkpoint_lock : checkpoint_queue = None if tasks : checkpoint_queue = tasks else : checkpoint_queue = self . tasks checkpoint_dir = '{0}/checkpoint' . format ( self . run_dir ) checkpoint_dfk = checkpoint_dir + '/dfk.pkl' checkpoint_tasks = checkpoint_dir + '/tasks.pkl' if not os . path . exists ( checkpoint_dir ) : try : os . makedirs ( checkpoint_dir ) except FileExistsError : pass with open ( checkpoint_dfk , 'wb' ) as f : state = { 'rundir' : self . run_dir , 'task_count' : self . task_count } pickle . dump ( state , f ) count = 0 with open ( checkpoint_tasks , 'ab' ) as f : for task_id in checkpoint_queue : if not self . tasks [ task_id ] [ 'checkpoint' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None : hashsum = self . tasks [ task_id ] [ 'hashsum' ] if not hashsum : continue t = { 'hash' : hashsum , 'exception' : None , 'result' : None } try : r = self . memoizer . hash_lookup ( hashsum ) . result ( ) except Exception as e : t [ 'exception' ] = e else : t [ 'result' ] = r pickle . dump ( t , f ) count += 1 self . tasks [ task_id ] [ 'checkpoint' ] = True logger . debug ( "Task {} checkpointed" . format ( task_id ) ) self . checkpointed_tasks += count if count == 0 : if self . checkpointed_tasks == 0 : logger . warn ( "No tasks checkpointed so far in this run. Please ensure caching is enabled" ) else : logger . debug ( "No tasks checkpointed in this pass." ) else : logger . info ( "Done checkpointing {} tasks" . format ( count ) ) return checkpoint_dir
10616	def clear ( self ) : self . _compound_masses = self . _compound_masses * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
2365	def RobotFactory ( path , parent = None ) : if os . path . isdir ( path ) : return SuiteFolder ( path , parent ) else : rf = RobotFile ( path , parent ) for table in rf . tables : if isinstance ( table , TestcaseTable ) : rf . __class__ = SuiteFile return rf rf . __class__ = ResourceFile return rf
6609	def failed_runids ( self , runids ) : for i in runids : try : self . clusterprocids_finished . remove ( i ) except ValueError : pass
1242	def _sample_with_priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . _memory ) : return parent left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) if p <= left_p : parent = left else : if left + 1 >= len ( self . _memory ) : raise RuntimeError ( 'Right child is expected to exist.' ) p -= left_p parent = left + 1
1789	def DIV ( cpu , src ) : size = src . size reg_name_h = { 8 : 'DL' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] dividend = Operators . CONCAT ( size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = Operators . ZEXTEND ( src . read ( ) , size * 2 ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) quotient = Operators . UDIV ( dividend , divisor ) MASK = ( 1 << size ) - 1 if isinstance ( quotient , int ) and quotient > MASK : raise DivideByZeroError ( ) remainder = Operators . UREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , size ) )
9502	def intersection ( l1 , l2 ) : if len ( l1 ) == 0 or len ( l2 ) == 0 : return [ ] out = [ ] l2_pos = 0 for l in l1 : while l2_pos < len ( l2 ) and l2 [ l2_pos ] . end < l . start : l2_pos += 1 if l2_pos == len ( l2 ) : break while l2_pos < len ( l2 ) and l . intersects ( l2 [ l2_pos ] ) : out . append ( l . intersection ( l2 [ l2_pos ] ) ) l2_pos += 1 l2_pos = max ( 0 , l2_pos - 1 ) return out
4080	def get_languages ( ) -> set : try : languages = cache [ 'languages' ] except KeyError : languages = LanguageTool . _get_languages ( ) cache [ 'languages' ] = languages return languages
5055	def get_idp_choices ( ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None first = [ ( "" , "-" * 7 ) ] if Registry : return first + [ ( idp . provider_id , idp . name ) for idp in Registry . enabled ( ) ] return None
9588	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } if self . session_id is not None : data . setdefault ( 'session_id' , self . session_id ) data = self . _wrap_el ( data ) res = self . remote_invoker . execute ( command , data ) ret = WebDriverResult . from_object ( res ) ret . raise_for_status ( ) ret . value = self . _unwrap_el ( ret . value ) if not unpack : return ret return ret . value
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
11719	def pipelines ( self ) : if not self . response : return set ( ) elif self . _pipelines is None and self . response : self . _pipelines = set ( ) for group in self . response . payload : for pipeline in group [ 'pipelines' ] : self . _pipelines . add ( pipeline [ 'name' ] ) return self . _pipelines
13836	def _MergeMessageField ( self , tokenizer , message , field ) : is_map_entry = _IsMapEntry ( field ) if tokenizer . TryConsume ( '<' ) : end_token = '>' else : tokenizer . Consume ( '{' ) end_token = '}' if field . label == descriptor . FieldDescriptor . LABEL_REPEATED : if field . is_extension : sub_message = message . Extensions [ field ] . add ( ) elif is_map_entry : sub_message = field . message_type . _concrete_class ( ) else : sub_message = getattr ( message , field . name ) . add ( ) else : if field . is_extension : sub_message = message . Extensions [ field ] else : sub_message = getattr ( message , field . name ) sub_message . SetInParent ( ) while not tokenizer . TryConsume ( end_token ) : if tokenizer . AtEnd ( ) : raise tokenizer . ParseErrorPreviousToken ( 'Expected "%s".' % ( end_token , ) ) self . _MergeField ( tokenizer , sub_message ) if is_map_entry : value_cpptype = field . message_type . fields_by_name [ 'value' ] . cpp_type if value_cpptype == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : value = getattr ( message , field . name ) [ sub_message . key ] value . MergeFrom ( sub_message . value ) else : getattr ( message , field . name ) [ sub_message . key ] = sub_message . value
1241	def _next_position_then_increment ( self ) : start = self . _capacity - 1 position = start + self . _position self . _position = ( self . _position + 1 ) % self . _capacity return position
12839	def init_async ( self , loop ) : super ( PooledAIODatabase , self ) . init_async ( loop ) self . _waiters = collections . deque ( )
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
9085	def update_backend ( use_pypi = False , index = 'dev' , build = True , user = None , version = None ) : get_vars ( ) if value_asbool ( build ) : upload_backend ( index = index , user = user ) with fab . cd ( '{apphome}' . format ( ** AV ) ) : if value_asbool ( use_pypi ) : command = 'bin/pip install --upgrade briefkasten' else : command = 'bin/pip install --upgrade --pre -i {ploy_default_publish_devpi}/briefkasten/{index}/+simple/ briefkasten' . format ( index = index , user = user , ** AV ) if version : command = '%s==%s' % ( command , version ) fab . sudo ( command ) briefkasten_ctl ( 'restart' )
116	def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )
8395	def show_help ( ) : print ( ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
12882	def main ( world_cls , referee_cls , gui_cls , gui_actor_cls , ai_actor_cls , theater_cls = PygletTheater , default_host = DEFAULT_HOST , default_port = DEFAULT_PORT , argv = None ) : import sys , os , docopt , nonstdlib exe_name = os . path . basename ( sys . argv [ 0 ] ) usage = main . __doc__ . format ( ** locals ( ) ) . strip ( ) args = docopt . docopt ( usage , argv or sys . argv [ 1 : ] ) num_guis = int ( args [ '<num_guis>' ] or 1 ) num_ais = int ( args [ '<num_ais>' ] or 0 ) host , port = args [ '--host' ] , int ( args [ '--port' ] ) logging . basicConfig ( format = '%(levelname)s: %(name)s: %(message)s' , level = nonstdlib . verbosity ( args [ '--verbose' ] ) , ) if args [ 'debug' ] : print ( ) game = MultiplayerDebugger ( world_cls , referee_cls , gui_cls , gui_actor_cls , num_guis , ai_actor_cls , num_ais , theater_cls , host , port ) else : game = theater_cls ( ) ai_actors = [ ai_actor_cls ( ) for i in range ( num_ais ) ] if args [ 'sandbox' ] : game . gui = gui_cls ( ) game . initial_stage = UniplayerGameStage ( world_cls ( ) , referee_cls ( ) , gui_actor_cls ( ) , ai_actors ) game . initial_stage . successor = PostgameSplashStage ( ) if args [ 'client' ] : game . gui = gui_cls ( ) game . initial_stage = ClientConnectionStage ( world_cls ( ) , gui_actor_cls ( ) , host , port ) if args [ 'server' ] : game . initial_stage = ServerConnectionStage ( world_cls ( ) , referee_cls ( ) , num_guis , ai_actors , host , port ) game . play ( )
5232	def all_files ( path_name , keyword = '' , ext = '' , full_path = True , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword or ext : keyword = f'*{keyword}*' if keyword else '*' if not ext : ext = '*' files = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/{keyword}.{ext}' ) if os . path . isfile ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : files = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isfile ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : files = filter_by_dates ( files , date_fmt = date_fmt ) return files if full_path else [ f . split ( '/' ) [ - 1 ] for f in files ]
9408	def write_file ( obj , path , oned_as = 'row' , convert_to_float = True ) : data = _encode ( obj , convert_to_float ) try : with _WRITE_LOCK : savemat ( path , data , appendmat = False , oned_as = oned_as , long_field_names = True ) except KeyError : raise Exception ( 'could not save mat file' )
4408	def connected_channel ( self ) : if not self . channel_id : return None return self . _lavalink . bot . get_channel ( int ( self . channel_id ) )
2014	def _top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise StackUnderflow ( ) return self . stack [ n - 1 ]
4841	def get_program_by_uuid ( self , program_uuid ) : return self . _load_data ( self . PROGRAMS_ENDPOINT , resource_id = program_uuid , default = None )
2225	def _update_hasher ( hasher , data , types = True ) : if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : _update_hasher ( hasher , item , types ) for item in iter_ : _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
9106	def dropbox_editor_factory ( request ) : dropbox = dropbox_factory ( request ) if is_equal ( dropbox . editor_token , request . matchdict [ 'editor_token' ] . encode ( 'utf-8' ) ) : return dropbox else : raise HTTPNotFound ( 'invalid editor token' )
1728	def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
8593	def remove_snapshot ( self , snapshot_id ) : response = self . _perform_request ( url = '/snapshots/' + snapshot_id , method = 'DELETE' ) return response
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , ** data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
7117	def smush_config ( sources , initial = None ) : if initial is None : initial = { } config = DotDict ( initial ) for fn in sources : log . debug ( 'Merging %s' , fn ) mod = get_config_module ( fn ) config = mod . update ( config ) log . debug ( 'Current config:\n%s' , json . dumps ( config , indent = 4 , cls = LenientJSONEncoder ) ) return config
6067	def convergence_from_grid ( self , grid ) : surface_density_grid = np . zeros ( grid . shape [ 0 ] ) grid_eta = self . grid_to_elliptical_radii ( grid ) for i in range ( grid . shape [ 0 ] ) : surface_density_grid [ i ] = self . convergence_func ( grid_eta [ i ] ) return surface_density_grid
4725	def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
7522	def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
13446	def messages_from_response ( response ) : messages = [ ] if hasattr ( response , 'context' ) and response . context and 'messages' in response . context : messages = response . context [ 'messages' ] elif hasattr ( response , 'cookies' ) : morsel = response . cookies . get ( 'messages' ) if not morsel : return [ ] from django . contrib . messages . storage . cookie import CookieStorage store = CookieStorage ( FakeRequest ( ) ) messages = store . _decode ( morsel . value ) else : return [ ] return [ ( m . message , m . level ) for m in messages ]
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
7856	def __response ( self , stanza ) : try : d = self . disco_class ( stanza . get_query ( ) ) self . got_it ( d ) except ValueError , e : self . error ( e )
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
2646	def python_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp def decorator ( func ) : def wrapper ( f ) : return PythonApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
7911	def get ( self , key , local_default = None , required = False ) : if key in self . _settings : return self . _settings [ key ] if local_default is not None : return local_default if key in self . _defs : setting_def = self . _defs [ key ] if setting_def . default is not None : return setting_def . default factory = setting_def . factory if factory is None : return None value = factory ( self ) if setting_def . cache is True : setting_def . default = value return value if required : raise KeyError ( key ) return local_default
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
10501	def waitForWindowToDisappear ( self , winName , timeout = 10 ) : callback = AXCallbacks . elemDisappearedCallback retelem = None args = ( retelem , self ) win = self . findFirst ( AXRole = 'AXWindow' , AXTitle = winName ) return self . waitFor ( timeout , 'AXUIElementDestroyed' , callback = callback , args = args , AXRole = 'AXWindow' , AXTitle = winName )
7673	def trim ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'trimming can be performed.' ) if not ( 0 <= start_time <= end_time <= float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_trimmed = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_trimmed . annotations = self . annotations . trim ( start_time , end_time , strict = strict ) if 'trim' not in jam_trimmed . sandbox . keys ( ) : jam_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_trimmed
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
11321	def update_languages ( self ) : language_fields = record_get_field_instances ( self . record , '041' ) language = "eng" record_delete_fields ( self . record , "041" ) for field in language_fields : subs = field_get_subfields ( field ) if 'a' in subs : language = self . get_config_item ( subs [ 'a' ] [ 0 ] , "languages" ) break new_subs = [ ( 'a' , language ) ] record_add_field ( self . record , "041" , subfields = new_subs )
5629	def _get_digest ( self ) : return hmac . new ( self . _secret , request . data , hashlib . sha1 ) . hexdigest ( ) if self . _secret else None
11291	def json ( request , * args , ** kwargs ) : params = dict ( request . GET . items ( ) ) callback = params . pop ( 'callback' , None ) url = params . pop ( 'url' , None ) if not url : return HttpResponseBadRequest ( 'Required parameter missing: URL' ) try : provider = oembed . site . provider_for_url ( url ) if not provider . provides : raise OEmbedMissingEndpoint ( ) except OEmbedMissingEndpoint : raise Http404 ( 'No provider found for %s' % url ) query = dict ( [ ( smart_str ( k ) , smart_str ( v ) ) for k , v in params . items ( ) if v ] ) try : resource = oembed . site . embed ( url , ** query ) except OEmbedException , e : raise Http404 ( 'Error embedding %s: %s' % ( url , str ( e ) ) ) response = HttpResponse ( mimetype = 'application/json' ) json = resource . json if callback : response . write ( '%s(%s)' % ( defaultfilters . force_escape ( callback ) , json ) ) else : response . write ( json ) return response
12389	def parse ( specifiers ) : specifiers = "" . join ( specifiers . split ( ) ) for specifier in specifiers . split ( ',' ) : if len ( specifier ) == 0 : raise ValueError ( "Range: Invalid syntax; missing specifier." ) count = specifier . count ( '-' ) if ( count and specifier [ 0 ] == '-' ) or not count : yield int ( specifier ) , int ( specifier ) continue specifier = list ( map ( int , specifier . split ( '-' ) ) ) if len ( specifier ) == 2 : if specifier [ 0 ] < 0 or specifier [ 1 ] < 0 : raise ValueError ( "Range: Invalid syntax; negative indexing " "not supported in a range specifier." ) if specifier [ 1 ] < specifier [ 0 ] : raise ValueError ( "Range: Invalid syntax; stop is less than start." ) yield tuple ( specifier ) continue raise ValueError ( "Range: Invalid syntax." )
5001	def _get_enterprise_customer_users_batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise customer users from indexes: %s to %s' , start , end ) return User . objects . filter ( pk__in = self . _get_enterprise_customer_user_ids ( ) ) [ start : end ]
13161	def select ( cls , cur , table : str , order_by : str , columns : list = None , where_keys : list = None , limit = 100 , offset = 0 ) : if columns : columns_string = cls . _COMMA . join ( columns ) if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_selective_column_with_condition . format ( columns_string , table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_selective_column . format ( columns_string , table , order_by , limit , offset ) q , t = query , ( ) else : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_all_string_with_condition . format ( table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_all_string . format ( table , order_by , limit , offset ) q , t = query , ( ) yield from cur . execute ( q , t ) return ( yield from cur . fetchall ( ) )
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
9108	def cleanup ( self ) : try : remove ( join ( self . fs_path , u'message' ) ) remove ( join ( self . fs_path , 'dirty.zip.pgp' ) ) except OSError : pass shutil . rmtree ( join ( self . fs_path , u'clean' ) , ignore_errors = True ) shutil . rmtree ( join ( self . fs_path , u'attach' ) , ignore_errors = True )
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams forder = len ( famps ) iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] modelmags = np . median ( mags ) for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
3938	def _parse_sid_response ( res ) : res = json . loads ( list ( ChunkParser ( ) . get_chunks ( res ) ) [ 0 ] ) sid = res [ 0 ] [ 1 ] [ 1 ] gsessionid = res [ 1 ] [ 1 ] [ 0 ] [ 'gsid' ] return ( sid , gsessionid )
11379	def extract_oembeds ( text , args = None ) : resource_type = width = height = None if args : dimensions = args . lower ( ) . split ( 'x' ) if len ( dimensions ) in ( 3 , 1 ) : resource_type = dimensions . pop ( ) if len ( dimensions ) == 2 : width , height = map ( lambda x : int ( x ) , dimensions ) client = OEmbedConsumer ( ) return client . extract ( text , width , height , resource_type )
3816	async def _add_channel_services ( self ) : logger . info ( 'Adding channel services...' ) services = [ "babel" , "babel_presence_last_seen" ] map_list = [ dict ( p = json . dumps ( { "3" : { "1" : { "1" : service } } } ) ) for service in services ] await self . _channel . send_maps ( map_list ) logger . info ( 'Channel services added' )
11096	def select_by_pattern_in_fname ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . fname else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . fname . lower ( ) return self . select_file ( filters , recursive )
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
3146	def _build_path ( self , * args ) : return '/' . join ( chain ( ( self . endpoint , ) , map ( str , args ) ) )
11782	def check_me ( self ) : "Check that my fields make sense." assert len ( self . attrnames ) == len ( self . attrs ) assert self . target in self . attrs assert self . target not in self . inputs assert set ( self . inputs ) . issubset ( set ( self . attrs ) ) map ( self . check_example , self . examples )
1386	def set_packing_plan ( self , packing_plan ) : if not packing_plan : self . packing_plan = None self . id = None else : self . packing_plan = packing_plan self . id = packing_plan . id self . trigger_watches ( )
9639	def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) fd . seek ( next_pos ) if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
13317	def remove ( name_or_path ) : r = resolve ( name_or_path ) r . resolved [ 0 ] . remove ( ) EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
8159	def edit ( self , id , * args , ** kw ) : if args and kw : return if args and type ( args [ 0 ] ) == dict : fields = [ k for k in args [ 0 ] ] v = [ args [ 0 ] [ k ] for k in args [ 0 ] ] if kw : fields = [ k for k in kw ] v = [ kw [ k ] for k in kw ] sql = "update " + self . _name + " set " + "=?, " . join ( fields ) + "=? where " + self . _key + "=" + unicode ( id ) self . _db . _cur . execute ( sql , v ) self . _db . _i += 1 if self . _db . _i >= self . _db . _commit : self . _db . _i = 0 self . _db . _con . commit ( )
486	def _getLogger ( cls , logLevel = None ) : logger = logging . getLogger ( "." . join ( [ 'com.numenta' , _MODULE_NAME , cls . __name__ ] ) ) if logLevel is not None : logger . setLevel ( logLevel ) return logger
908	def handleInputRecord ( self , inputRecord ) : assert inputRecord , "Invalid inputRecord: %r" % inputRecord results = self . __phaseManager . handleInputRecord ( inputRecord ) metrics = self . __metricsMgr . update ( results ) for cb in self . __userCallbacks [ 'postIter' ] : cb ( self . __model ) results . metrics = metrics return results
7945	def _continue_connect ( self ) : try : self . _socket . connect ( self . _dst_addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] == errno . EISCONN : pass elif err . args [ 0 ] in BLOCKING_ERRORS : return None elif self . _dst_addrs : self . _set_state ( "connect" ) return None elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return None else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) raise self . _connected ( )
9274	def filter_excluded_tags ( self , all_tags ) : filtered_tags = copy . deepcopy ( all_tags ) if self . options . exclude_tags : filtered_tags = self . apply_exclude_tags ( filtered_tags ) if self . options . exclude_tags_regex : filtered_tags = self . apply_exclude_tags_regex ( filtered_tags ) return filtered_tags
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
2782	def destroy ( self ) : return self . get_data ( "domains/%s/records/%s" % ( self . domain , self . id ) , type = DELETE , )
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
2453	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True match = self . VERIF_CODE_REGEX . match ( code ) if match : doc . package . verif_code = match . group ( self . VERIF_CODE_CODE_GRP ) if match . group ( self . VERIF_CODE_EXC_FILES_GRP ) is not None : doc . package . verif_exc_files = match . group ( self . VERIF_CODE_EXC_FILES_GRP ) . split ( ',' ) return True else : raise SPDXValueError ( 'Package::VerificationCode' ) else : raise CardinalityError ( 'Package::VerificationCode' )
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
10976	def delete ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_edit ( current_user ) : try : group . delete ( ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'Successfully removed group "%(group_name)s"' , group_name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'You cannot delete the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( ".index" ) )
10590	def report ( self , format = ReportFormat . printout , output_path = None ) : rpt = GlsRpt ( self , output_path ) return rpt . render ( format )
13284	def list_from_document ( cls , doc ) : objs = [ ] for feu in doc . xpath ( '//FEU' ) : detail_els = feu . xpath ( 'event-element-details/event-element-detail' ) for idx , detail in enumerate ( detail_els ) : objs . append ( cls ( feu , detail , id_suffix = idx , number_in_group = len ( detail_els ) ) ) return objs
12286	def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
6130	def get ( self , * args , ** kwargs ) : try : req_func = self . session . get if self . session else requests . get req = req_func ( * args , ** kwargs ) req . raise_for_status ( ) self . failed_last = False return req except requests . exceptions . RequestException as e : self . log_error ( e ) for i in range ( 1 , self . num_retries ) : sleep_time = self . retry_rate * i self . log_function ( "Retrying in %s seconds" % sleep_time ) self . _sleep ( sleep_time ) try : req = requests . get ( * args , ** kwargs ) req . raise_for_status ( ) self . log_function ( "New request successful" ) return req except requests . exceptions . RequestException : self . log_function ( "New request failed" ) if not self . failed_last : self . failed_last = True raise ApiError ( e ) else : raise FatalApiError ( e )
9397	def run ( self ) : print ( 'Oct2Py speed test' ) print ( '*' * 20 ) time . sleep ( 1 ) print ( 'Raw speed: ' ) avg = timeit . timeit ( self . raw_speed , number = 10 ) / 10 print ( ' {0:0.01f} usec per loop' . format ( avg * 1e6 ) ) sides = [ 1 , 10 , 100 , 1000 ] runs = [ 10 , 10 , 10 , 5 ] for ( side , nruns ) in zip ( sides , runs ) : self . array = np . reshape ( np . arange ( side ** 2 ) , ( - 1 ) ) print ( 'Put {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_put , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) print ( 'Get {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_get , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) self . octave . exit ( ) print ( '*' * 20 ) print ( 'Test complete!' )
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
7093	def create_widget ( self ) : self . init_options ( ) MapFragment . newInstance ( self . options ) . then ( self . on_map_fragment_created ) self . widget = FrameLayout ( self . get_context ( ) ) self . map = GoogleMap ( __id__ = bridge . generate_id ( ) )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
12935	def _parse_allele_data ( self ) : pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , ** cln_data ) if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
9	def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break return mask
10972	def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
12473	def add_extension_if_needed ( filepath , ext , check_if_exists = False ) : if not filepath . endswith ( ext ) : filepath += ext if check_if_exists : if not op . exists ( filepath ) : raise IOError ( 'File not found: ' + filepath ) return filepath
12465	def read_config ( filename , args ) : config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
322	def get_top_drawdowns ( returns , top = 10 ) : returns = returns . copy ( ) df_cum = ep . cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 drawdowns = [ ] for t in range ( top ) : peak , valley , recovery = get_max_drawdown_underwater ( underwater ) if not pd . isnull ( recovery ) : underwater . drop ( underwater [ peak : recovery ] . index [ 1 : - 1 ] , inplace = True ) else : underwater = underwater . loc [ : peak ] drawdowns . append ( ( peak , valley , recovery ) ) if ( len ( returns ) == 0 ) or ( len ( underwater ) == 0 ) : break return drawdowns
318	def calc_bootstrap ( func , returns , * args , ** kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , ** kwargs ) else : out [ i ] = func ( returns_i , * args , ** kwargs ) return out
9516	def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
12257	def smooth ( x , rho , penalty , axis = 0 , newshape = None ) : orig_shape = x . shape if newshape is not None : x = x . reshape ( newshape ) n = x . shape [ axis ] lap_op = spdiags ( [ ( 2 + rho / penalty ) * np . ones ( n ) , - 1 * np . ones ( n ) , - 1 * np . ones ( n ) ] , [ 0 , - 1 , 1 ] , n , n , format = 'csc' ) A = penalty * lap_op b = rho * np . rollaxis ( x , axis , 0 ) return np . rollaxis ( spsolve ( A , b ) , axis , 0 ) . reshape ( orig_shape )
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
4252	def org_by_addr ( self , addr ) : valid = ( const . ORG_EDITION , const . ISP_EDITION , const . ASNUM_EDITION , const . ASNUM_EDITION_V6 ) if self . _databaseType not in valid : message = 'Invalid database type, expected Org, ISP or ASNum' raise GeoIPError ( message ) ipnum = util . ip2long ( addr ) return self . _get_org ( ipnum )
1048	def print_tb ( tb , limit = None , file = None ) : if file is None : file = sys . stderr if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit n = 0 while tb is not None and ( limit is None or n < limit ) : f = tb . tb_frame lineno = tb . tb_lineno co = f . f_code filename = co . co_filename name = co . co_name _print ( file , ' File "%s", line %d, in %s' % ( filename , lineno , name ) ) linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : _print ( file , ' ' + line . strip ( ) ) tb = tb . tb_next n = n + 1
3769	def none_and_length_check ( all_inputs , length = None ) : r if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
12623	def dir_match ( regex , wd = os . curdir ) : ls = os . listdir ( wd ) filt = re . compile ( regex ) . match return filter_list ( ls , filt )
12477	def merge ( dict_1 , dict_2 ) : return dict ( ( str ( key ) , dict_1 . get ( key ) or dict_2 . get ( key ) ) for key in set ( dict_2 ) | set ( dict_1 ) )
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
10279	def neurommsig_topology ( graph : BELGraph , nodes : List [ BaseEntity ] ) -> float : nodes = list ( nodes ) number_nodes = len ( nodes ) if number_nodes <= 1 : return 0.0 unnormalized_sum = sum ( u in graph [ v ] for u , v in itt . product ( nodes , repeat = 2 ) if v in graph and u != v ) return unnormalized_sum / ( number_nodes * ( number_nodes - 1.0 ) )
7253	def status ( self , order_id ) : self . logger . debug ( 'Get status of order ' + order_id ) url = '%(base_url)s/order/%(order_id)s' % { 'base_url' : self . base_url , 'order_id' : order_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) . get ( "acquisitions" , { } )
3057	def _load_credentials_file ( credentials_file ) : try : credentials_file . seek ( 0 ) data = json . load ( credentials_file ) except Exception : logger . warning ( 'Credentials file could not be loaded, will ignore and ' 'overwrite.' ) return { } if data . get ( 'file_version' ) != 2 : logger . warning ( 'Credentials file is not version 2, will ignore and ' 'overwrite.' ) return { } credentials = { } for key , encoded_credential in iteritems ( data . get ( 'credentials' , { } ) ) : try : credential_json = base64 . b64decode ( encoded_credential ) credential = client . Credentials . new_from_json ( credential_json ) credentials [ key ] = credential except : logger . warning ( 'Invalid credential {0} in file, ignoring.' . format ( key ) ) return credentials
5308	def hex_to_rgb ( value ) : value = value . lstrip ( '#' ) check_hex ( value ) length = len ( value ) step = int ( length / 3 ) return tuple ( int ( value [ i : i + step ] , 16 ) for i in range ( 0 , length , step ) )
5076	def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
8372	def save_as ( self ) : chooser = ShoebotFileChooserDialog ( _ ( 'Save File' ) , None , Gtk . FileChooserAction . SAVE , ( Gtk . STOCK_SAVE , Gtk . ResponseType . ACCEPT , Gtk . STOCK_CANCEL , Gtk . ResponseType . CANCEL ) ) chooser . set_do_overwrite_confirmation ( True ) chooser . set_transient_for ( self ) saved = chooser . run ( ) == Gtk . ResponseType . ACCEPT if saved : old_filename = self . filename self . source_buffer . filename = chooser . get_filename ( ) if not self . save ( ) : self . filename = old_filename chooser . destroy ( ) return saved
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
2629	def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
9319	def _validate_status ( self ) : if not self . id : msg = "No 'id' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . status : msg = "No 'status' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . total_count is None : msg = "No 'total_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . success_count is None : msg = "No 'success_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . failure_count is None : msg = "No 'failure_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . pending_count is None : msg = "No 'pending_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if len ( self . successes ) != self . success_count : msg = "Found successes={}, but success_count={} in status '{}'" raise ValidationError ( msg . format ( self . successes , self . success_count , self . id ) ) if len ( self . pendings ) != self . pending_count : msg = "Found pendings={}, but pending_count={} in status '{}'" raise ValidationError ( msg . format ( self . pendings , self . pending_count , self . id ) ) if len ( self . failures ) != self . failure_count : msg = "Found failures={}, but failure_count={} in status '{}'" raise ValidationError ( msg . format ( self . failures , self . failure_count , self . id ) ) if ( self . success_count + self . pending_count + self . failure_count != self . total_count ) : msg = ( "(success_count={} + pending_count={} + " "failure_count={}) != total_count={} in status '{}'" ) raise ValidationError ( msg . format ( self . success_count , self . pending_count , self . failure_count , self . total_count , self . id ) )
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
7405	def top ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) self . to ( o )
6019	def from_inverse_noise_map ( cls , pixel_scale , inverse_noise_map ) : noise_map = 1.0 / inverse_noise_map return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
8461	def get_cookiecutter_config ( template , default_config = None , version = None ) : default_config = default_config or { } config_dict = cc_config . get_user_config ( ) repo_dir , _ = cc_repository . determine_repo_dir ( template = template , abbreviations = config_dict [ 'abbreviations' ] , clone_to_dir = config_dict [ 'cookiecutters_dir' ] , checkout = version , no_input = True ) context_file = os . path . join ( repo_dir , 'cookiecutter.json' ) context = cc_generate . generate_context ( context_file = context_file , default_context = { ** config_dict [ 'default_context' ] , ** default_config } ) return repo_dir , cc_prompt . prompt_for_config ( context )
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
1642	def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] match = Match ( r'^(.*[^ ({>]){' , line ) if match : leading_text = match . group ( 1 ) ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) trailing_text = '' if endpos > - 1 : trailing_text = endline [ endpos : ] for offset in xrange ( endlinenum + 1 , min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : trailing_text += clean_lines . elided [ offset ] if ( not Match ( r'^[\s}]*[{.;,)<>\]:]' , trailing_text ) and not _IsType ( clean_lines , nesting_state , leading_text ) ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before {' ) if Search ( r'}else' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before else' ) if Search ( r':\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Semicolon defining empty statement. Use {} instead.' ) elif Search ( r'^\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Line contains only semicolon. If this should be an empty statement, ' 'use {} instead.' ) elif ( Search ( r'\s+;\s*$' , line ) and not Search ( r'\bfor\b' , line ) ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Extra space before last semicolon. If this should be an empty ' 'statement, use {} instead.' )
12247	def create_bucket ( self , * args , ** kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , ** kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
4101	def mdl_eigen ( s , N ) : r import numpy as np kmdl = [ ] n = len ( s ) for k in range ( 0 , n - 1 ) : ak = 1. / ( n - k ) * np . sum ( s [ k + 1 : ] ) gk = np . prod ( s [ k + 1 : ] ** ( 1. / ( n - k ) ) ) kmdl . append ( - ( n - k ) * N * np . log ( gk / ak ) + 0.5 * k * ( 2. * n - k ) * np . log ( N ) ) return kmdl
6816	def optimize_wsgi_processes ( self ) : r = self . local_renderer r . env . wsgi_server_memory_gb = 8 verbose = self . verbose all_sites = list ( self . iter_sites ( site = ALL , setter = self . set_site_specifics ) )
2520	def p_file_lic_info ( self , f_term , predicate ) : for _ , _ , info in self . graph . triples ( ( f_term , predicate , None ) ) : lic = self . handle_lics ( info ) if lic is not None : self . builder . set_file_license_in_file ( self . doc , lic )
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
13322	def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
1152	def warn ( message , category = None , stacklevel = 1 ) : if isinstance ( message , Warning ) : category = message . __class__ if category is None : category = UserWarning assert issubclass ( category , Warning ) try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
11934	def auto_widget ( field ) : info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( ** info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
10578	def get_element_masses ( self ) : result = [ 0 ] * len ( self . material . elements ) for compound in self . material . compounds : c = self . get_compound_mass ( compound ) f = [ c * x for x in emf ( compound , self . material . elements ) ] result = [ v + f [ ix ] for ix , v in enumerate ( result ) ] return result
7859	def make_result_response ( self ) : if self . stanza_type not in ( "set" , "get" ) : raise ValueError ( "Results may only be generated for" " 'set' or 'get' iq" ) stanza = Iq ( stanza_type = "result" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id ) return stanza
4982	def get_available_course_modes ( self , request , course_run_id , enterprise_catalog ) : modes = EnrollmentApiClient ( ) . get_course_modes ( course_run_id ) if not modes : LOGGER . warning ( 'Unable to get course modes for course run id {course_run_id}.' . format ( course_run_id = course_run_id ) ) messages . add_generic_info_message_for_error ( request ) if enterprise_catalog : modes = [ mode for mode in modes if mode [ 'slug' ] in enterprise_catalog . enabled_course_modes ] modes . sort ( key = lambda course_mode : enterprise_catalog . enabled_course_modes . index ( course_mode [ 'slug' ] ) ) if not modes : LOGGER . info ( 'No matching course modes found for course run {course_run_id} in ' 'EnterpriseCustomerCatalog [{enterprise_catalog_uuid}]' . format ( course_run_id = course_run_id , enterprise_catalog_uuid = enterprise_catalog , ) ) messages . add_generic_info_message_for_error ( request ) return modes
8832	def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
2962	def expand_partitions ( containers , partitions ) : all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
7144	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( [ ( address , amount ) ] , priority , payment_id , unlock_time , account = self . index , relay = relay )
6635	def ignores ( self , path ) : test_path = PurePath ( '/' , path ) test_paths = tuple ( [ test_path ] + list ( test_path . parents ) ) for exp in self . ignore_patterns : for tp in test_paths : if tp . match ( exp ) : logger . debug ( '"%s" ignored ("%s" matched "%s")' , path , tp , exp ) return True return False
13408	def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) self . logButton . setObjectName ( "roundButton" ) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
8652	def create_thread ( session , member_ids , context_type , context , message ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } thread_data = { 'members[]' : member_ids , 'context_type' : context_type , 'context' : context , 'message' : message , } response = make_post_request ( session , 'threads' , headers , form_data = thread_data ) json_data = response . json ( ) if response . status_code == 200 : return Thread ( json_data [ 'result' ] ) else : raise ThreadNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
194	def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , "Input images did not fulfill user-defined assertion in AssertLambda." ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , "Input heatmaps did not fulfill user-defined assertion in AssertLambda." ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , "Input keypoints did not fulfill user-defined assertion in AssertLambda." ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , "Input polygons did not fulfill user-defined assertion in AssertLambda." ) return polygons_on_images if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
3919	async def _load ( self ) : try : conv_events = await self . _conversation . get_events ( self . _conversation . events [ 0 ] . id_ ) except ( IndexError , hangups . NetworkError ) : conv_events = [ ] if not conv_events : self . _first_loaded = True if self . _focus_position == self . POSITION_LOADING and conv_events : self . set_focus ( conv_events [ - 1 ] . id_ ) else : self . _modified ( ) self . _refresh_watermarked_events ( ) self . _is_loading = False
13680	def get_translated_data ( self ) : j = { } for k in self . data : d = { } for l in self . data [ k ] : d [ self . translation_keys [ l ] ] = self . data [ k ] [ l ] j [ k ] = d return j
981	def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0
2705	def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
920	def critical ( self , msg , * args , ** kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , ** savgol_kwargs ) : if errs is None : errs = 0.001 * mags finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , ** savgol_kwargs ) subtracted = fmags - smoothed series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] for ind , extrema_index in enumerate ( extrema_indices ) : pass
3164	def all ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = None return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) )
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
9689	def read_bin_particle_density ( self ) : config = [ ] self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 4 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) bpd = self . _calculate_float ( config ) return bpd
2390	def count_list ( the_list ) : count = the_list . count result = [ ( item , count ( item ) ) for item in set ( the_list ) ] result . sort ( ) return result
10079	def _publish_new ( self , id_ = None ) : minter = current_pidstore . minters [ current_app . config [ 'DEPOSIT_PID_MINTER' ] ] id_ = id_ or uuid . uuid4 ( ) record_pid = minter ( id_ , self ) self [ '_deposit' ] [ 'pid' ] = { 'type' : record_pid . pid_type , 'value' : record_pid . pid_value , 'revision_id' : 0 , } data = dict ( self . dumps ( ) ) data [ '$schema' ] = self . record_schema with self . _process_files ( id_ , data ) : record = self . published_record_class . create ( data , id_ = id_ ) return record
816	def MultiIndicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : for i in pos : x [ i ] = 1 else : x [ pos ] = 1 return x
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
4022	def docker_vm_is_running ( ) : running_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'runningvms' ] ) for line in running_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
6872	def given_lc_get_transit_tmids_tstarts_tends ( time , flux , err_flux , blsfit_savpath = None , trapfit_savpath = None , magsarefluxes = True , nworkers = 1 , sigclip = None , extra_maskfrac = 0.03 ) : endp = 1.05 * ( np . nanmax ( time ) - np . nanmin ( time ) ) / 2 blsdict = kbls . bls_parallel_pfind ( time , flux , err_flux , magsarefluxes = magsarefluxes , startp = 0.1 , endp = endp , maxtransitduration = 0.3 , nworkers = nworkers , sigclip = sigclip ) blsd = kbls . bls_stats_singleperiod ( time , flux , err_flux , blsdict [ 'bestperiod' ] , magsarefluxes = True , sigclip = sigclip , perioddeltapercent = 5 ) if blsfit_savpath : make_fit_plot ( blsd [ 'phases' ] , blsd [ 'phasedmags' ] , None , blsd [ 'blsmodel' ] , blsd [ 'period' ] , blsd [ 'epoch' ] , blsd [ 'epoch' ] , blsfit_savpath , magsarefluxes = magsarefluxes ) ingduration_guess = blsd [ 'transitduration' ] * 0.2 transitparams = [ blsd [ 'period' ] , blsd [ 'epoch' ] , blsd [ 'transitdepth' ] , blsd [ 'transitduration' ] , ingduration_guess ] if trapfit_savpath : trapd = traptransit_fit_magseries ( time , flux , err_flux , transitparams , magsarefluxes = magsarefluxes , sigclip = sigclip , plotfit = trapfit_savpath ) tmids , t_starts , t_ends = get_transit_times ( blsd , time , extra_maskfrac , trapd = trapd ) return tmids , t_starts , t_ends
12589	def treefall ( iterable ) : num_elems = len ( iterable ) for i in range ( num_elems , - 1 , - 1 ) : for c in combinations ( iterable , i ) : yield c
1959	def sys_openat ( self , dirfd , buf , flags , mode ) : filename = self . current . read_string ( buf ) dirfd = ctypes . c_int32 ( dirfd ) . value if os . path . isabs ( filename ) or dirfd == self . FCNTL_FDCWD : return self . sys_open ( buf , flags , mode ) try : dir_entry = self . _get_fd ( dirfd ) except FdError as e : logger . info ( "openat: Not valid file descriptor. Returning EBADF" ) return - e . err if not isinstance ( dir_entry , Directory ) : logger . info ( "openat: Not directory descriptor. Returning ENOTDIR" ) return - errno . ENOTDIR dir_path = dir_entry . name filename = os . path . join ( dir_path , filename ) try : f = self . _sys_open_get_file ( filename , flags ) logger . debug ( f"Opening file {filename} for real fd {f.fileno()}" ) except IOError as e : logger . info ( f"Could not open file {filename}. Reason: {e!s}" ) return - e . errno if e . errno is not None else - errno . EINVAL return self . _open ( f )
1277	def tf_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : x , next_iteration , deltas , improvement , last_improvement , estimated_improvement = super ( LineSearch , self ) . tf_step ( x , iteration , deltas , improvement , last_improvement , estimated_improvement ) next_x = [ t + delta for t , delta in zip ( x , deltas ) ] if self . mode == 'linear' : next_deltas = deltas next_estimated_improvement = estimated_improvement + self . estimated_incr elif self . mode == 'exponential' : next_deltas = [ delta * self . parameter for delta in deltas ] next_estimated_improvement = estimated_improvement * self . parameter target_value = self . fn_x ( next_deltas ) next_improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = next_estimated_improvement , y = util . epsilon ) ) return next_x , next_iteration , next_deltas , next_improvement , improvement , next_estimated_improvement
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : lhs += token expression = matched . group ( 2 ) else : operator = token rhs = matched . group ( 2 ) break else : matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) if not ( lhs and operator and rhs ) : return if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
2072	def get_obj_cols ( df ) : obj_cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is_category ( dt ) : obj_cols . append ( df . columns . values [ idx ] ) return obj_cols
10148	def from_schema_mapping ( self , schema_mapping ) : responses = { } for status , response_schema in schema_mapping . items ( ) : response = { } if response_schema . description : response [ 'description' ] = response_schema . description else : raise CorniceSwaggerException ( 'Responses must have a description.' ) for field_schema in response_schema . children : location = field_schema . name if location == 'body' : title = field_schema . __class__ . __name__ if title == 'body' : title = response_schema . __class__ . __name__ + 'Body' field_schema . title = title response [ 'schema' ] = self . definitions . from_schema ( field_schema ) elif location in ( 'header' , 'headers' ) : header_schema = self . type_converter ( field_schema ) headers = header_schema . get ( 'properties' ) if headers : for header in headers . values ( ) : header . pop ( 'title' ) response [ 'headers' ] = headers pointer = response_schema . __class__ . __name__ if self . ref : response = self . _ref ( response , pointer ) responses [ status ] = response return responses
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
9556	def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
5347	def compose_github ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'github_repos' ] ) > 0 ] : if 'github' not in projects [ p ] : projects [ p ] [ 'github' ] = [ ] urls = [ url [ 'url' ] for url in data [ p ] [ 'github_repos' ] if url [ 'url' ] not in projects [ p ] [ 'github' ] ] projects [ p ] [ 'github' ] += urls return projects
1091	def encode_basestring ( s ) : def replace ( match ) : return ESCAPE_DCT [ match . group ( 0 ) ] return '"' + ESCAPE . sub ( replace , s ) + '"'
3444	def load_json_model ( filename ) : if isinstance ( filename , string_types ) : with open ( filename , "r" ) as file_handle : return model_from_dict ( json . load ( file_handle ) ) else : return model_from_dict ( json . load ( filename ) )
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
13750	def one_to_many ( clsname , ** kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , ** kw ) return o2m
6478	def _normalised_numpy ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) points = np . array ( self . points ) - self . minimum points = points * 4.0 / self . extents * self . size . y for x , y in enumerate ( points ) : yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
6166	def my_psd ( x , NFFT = 2 ** 10 , Fs = 1 ) : Px , f = pylab . mlab . psd ( x , NFFT , Fs ) return Px . flatten ( ) , f
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
10026	def create_application_version ( self , version_label , key ) : out ( "Creating application version " + str ( version_label ) + " for " + str ( key ) ) self . ebs . create_application_version ( self . app_name , version_label , s3_bucket = self . aws . bucket , s3_key = self . aws . bucket_path + key )
508	def match ( self , record ) : for field , meta in self . filterDict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : if not record : continue if record [ index ] . find ( category ) != - 1 : return True return False
5878	def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
13898	def IterHashes ( iterator_size , hash_length = 7 ) : if not isinstance ( iterator_size , int ) : raise TypeError ( 'iterator_size must be integer.' ) count = 0 while count != iterator_size : count += 1 yield GetRandomHash ( hash_length )
12167	def _dispatch_coroutine ( self , event , listener , * args , ** kwargs ) : try : coro = listener ( * args , ** kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc ) asyncio . ensure_future ( _try_catch_coro ( self , event , listener , coro ) , loop = self . _loop , )
13824	def ToJsonString ( self ) : if self . seconds < 0 or self . nanos < 0 : result = '-' seconds = - self . seconds + int ( ( 0 - self . nanos ) // 1e9 ) nanos = ( 0 - self . nanos ) % 1e9 else : result = '' seconds = self . seconds + int ( self . nanos // 1e9 ) nanos = self . nanos % 1e9 result += '%d' % seconds if ( nanos % 1e9 ) == 0 : return result + 's' if ( nanos % 1e6 ) == 0 : return result + '.%03ds' % ( nanos / 1e6 ) if ( nanos % 1e3 ) == 0 : return result + '.%06ds' % ( nanos / 1e3 ) return result + '.%09ds' % nanos
10316	def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
7346	async def get_oauth_token ( consumer_key , consumer_secret , callback_uri = "oob" ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . request_token . post ( _suffix = "" , oauth_callback = callback_uri ) return parse_token ( response )
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
2559	def create_reg_message ( self ) : msg = { 'parsl_v' : PARSL_VERSION , 'python_v' : "{}.{}.{}" . format ( sys . version_info . major , sys . version_info . minor , sys . version_info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b_msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b_msg
10039	def pick_coda_from_decimal ( decimal ) : decimal = Decimal ( decimal ) __ , digits , exp = decimal . as_tuple ( ) if exp < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] __ , digits , exp = decimal . normalize ( ) . as_tuple ( ) index = bisect_right ( EXP_INDICES , exp ) - 1 if index < 0 : return DIGIT_CODAS [ digits [ - 1 ] ] else : return EXP_CODAS [ EXP_INDICES [ index ] ]
165	def compute_distance ( self , other , default = None ) : distances = self . compute_pointwise_distances ( other , default = [ ] ) if len ( distances ) == 0 : return default return min ( distances )
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
13167	def insert ( self , before , name , attrs = None , data = None ) : if isinstance ( before , self . __class__ ) : if before . parent != self : raise ValueError ( 'Cannot insert before an element with a different parent.' ) before = before . index before = min ( max ( 0 , before ) , len ( self . _children ) ) elem = self . __class__ ( name , attrs , data , parent = self , index = before ) self . _children . insert ( before , elem ) for idx , c in enumerate ( self . _children ) : c . index = idx return elem
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) protocolID = protocol . split ( " " ) [ 0 ] return protocolID
6971	def _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi_value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi_value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi_value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi_value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi_value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi_value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi_value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi_value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )
5650	def _remove_I_columns ( df ) : all_columns = list ( filter ( lambda el : el [ - 2 : ] == "_I" , df . columns ) ) for column in all_columns : del df [ column ]
6319	def initial_sanity_check ( self ) : self . try_import ( self . project_name ) self . validate_name ( self . project_name ) if os . path . exists ( self . project_name ) : print ( "Directory {} already exist. Aborting." . format ( self . project_name ) ) return False if os . path . exists ( 'manage.py' ) : print ( "A manage.py file already exist in the current directory. Aborting." ) return False return True
9343	def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) with self . pymata . data_lock : for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
8615	def wait_for_completion ( self , response , timeout = 3600 , initial_wait = 5 , scaleup = 10 ) : if not response : return logger = logging . getLogger ( __name__ ) wait_period = initial_wait next_increase = time . time ( ) + wait_period * scaleup if timeout : timeout = time . time ( ) + timeout while True : request = self . get_request ( request_id = response [ 'requestId' ] , status = True ) if request [ 'metadata' ] [ 'status' ] == 'DONE' : break elif request [ 'metadata' ] [ 'status' ] == 'FAILED' : raise PBFailedRequest ( 'Request {0} failed to complete: {1}' . format ( response [ 'requestId' ] , request [ 'metadata' ] [ 'message' ] ) , response [ 'requestId' ] ) current_time = time . time ( ) if timeout and current_time > timeout : raise PBTimeoutError ( 'Timed out waiting for request {0}.' . format ( response [ 'requestId' ] ) , response [ 'requestId' ] ) if current_time > next_increase : wait_period *= 2 next_increase = time . time ( ) + wait_period * scaleup scaleup *= 2 logger . info ( "Request %s is in state '%s'. Sleeping for %i seconds..." , response [ 'requestId' ] , request [ 'metadata' ] [ 'status' ] , wait_period ) time . sleep ( wait_period )
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
13796	def handle_rereduce ( self , reduce_function_names , values ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
8783	def _get_base_network_info ( self , context , network_id , base_net_driver ) : driver_name = base_net_driver . get_name ( ) net_info = { "network_type" : driver_name } LOG . debug ( '_get_base_network_info: %s %s' % ( driver_name , network_id ) ) if driver_name == 'NVP' : LOG . debug ( 'looking up lswitch ids for network %s' % ( network_id ) ) lswitch_ids = base_net_driver . get_lswitch_ids_for_network ( context , network_id ) if not lswitch_ids or len ( lswitch_ids ) > 1 : msg = ( 'lswitch id lookup failed, %s ids found.' % ( len ( lswitch_ids ) ) ) LOG . error ( msg ) raise IronicException ( msg ) lswitch_id = lswitch_ids . pop ( ) LOG . info ( 'found lswitch for network %s: %s' % ( network_id , lswitch_id ) ) net_info [ 'lswitch_id' ] = lswitch_id LOG . debug ( '_get_base_network_info finished: %s %s %s' % ( driver_name , network_id , net_info ) ) return net_info
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
5533	def batch_processor ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : if zoom and tile : raise ValueError ( "use either zoom or tile" ) if tile : yield _run_on_single_tile ( self , tile ) elif multi > 1 : for process_info in _run_with_multiprocessing ( self , list ( _get_zoom_level ( zoom , self ) ) , multi , max_chunksize ) : yield process_info elif multi == 1 : for process_info in _run_without_multiprocessing ( self , list ( _get_zoom_level ( zoom , self ) ) ) : yield process_info
2890	def create_task ( self ) : return self . spec_class ( self . spec , self . get_task_spec_name ( ) , lane = self . get_lane ( ) , description = self . node . get ( 'name' , None ) )
7971	def _run_timeout_threads ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
12567	def create_empty_dataset ( self , ds_name , dtype = np . float32 ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] ds = self . _group . create_dataset ( ds_name , ( 1 , 1 ) , maxshape = None , dtype = dtype ) self . _datasets [ ds_name ] = ds return ds
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : r if verbose is None : verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
13156	def transaction ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : try : yield from c . execute ( 'BEGIN' ) result = ( yield from func ( cls , c , * args , ** kwargs ) ) except Exception : yield from c . execute ( 'ROLLBACK' ) else : yield from c . execute ( 'COMMIT' ) return result return wrapper
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
6846	def is_present ( self , host = None ) : r = self . local_renderer r . env . host = host or self . genv . host_string ret = r . _local ( "getent hosts {host} | awk '{{ print $1 }}'" , capture = True ) or '' if self . verbose : print ( 'ret:' , ret ) ret = ret . strip ( ) if self . verbose : print ( 'Host %s %s present.' % ( r . env . host , 'IS' if bool ( ret ) else 'IS NOT' ) ) ip = ret ret = bool ( ret ) if not ret : return False r . env . ip = ip with settings ( warn_only = True ) : ret = r . _local ( 'ping -c 1 {ip}' , capture = True ) or '' packet_loss = re . findall ( r'([0-9]+)% packet loss' , ret ) ip_accessible = packet_loss and int ( packet_loss [ 0 ] ) < 100 if self . verbose : print ( 'IP %s accessible: %s' % ( ip , ip_accessible ) ) return bool ( ip_accessible )
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
8682	def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type = key_type ) : self . delete ( key_name )
8380	def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y s = self . graph . styles . default self . _ctx . nofill ( ) self . _ctx . nostroke ( ) if s . stroke : self . _ctx . strokewidth ( s . strokewidth ) self . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . _ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass self . _ctx . drawpath ( p ) r = node . __class__ ( None ) . r * 0.75 self . _ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
2285	def predict ( self , df_data , graph = None , ** kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , ** kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , ** kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , ** kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
4242	def _seek_country ( self , ipnum ) : try : offset = 0 seek_depth = 127 if len ( str ( ipnum ) ) > 10 else 31 for depth in range ( seek_depth , - 1 , - 1 ) : if self . _flags & const . MEMORY_CACHE : startIndex = 2 * self . _recordLength * offset endIndex = startIndex + ( 2 * self . _recordLength ) buf = self . _memory [ startIndex : endIndex ] else : startIndex = 2 * self . _recordLength * offset readLength = 2 * self . _recordLength try : self . _lock . acquire ( ) self . _fp . seek ( startIndex , os . SEEK_SET ) buf = self . _fp . read ( readLength ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) x = [ 0 , 0 ] for i in range ( 2 ) : for j in range ( self . _recordLength ) : byte = buf [ self . _recordLength * i + j ] x [ i ] += ord ( byte ) << ( j * 8 ) if ipnum & ( 1 << depth ) : if x [ 1 ] >= self . _databaseSegments : self . _netmask = seek_depth - depth + 1 return x [ 1 ] offset = x [ 1 ] else : if x [ 0 ] >= self . _databaseSegments : self . _netmask = seek_depth - depth + 1 return x [ 0 ] offset = x [ 0 ] except ( IndexError , UnicodeDecodeError ) : pass raise GeoIPError ( 'Corrupt database' )
1017	def _adaptSegment ( self , segUpdate ) : trimSegment = False c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) segment . lastActiveIteration = self . lrnIterationIdx segment . positiveActivations += 1 segment . dutyCycle ( active = True ) lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) else : newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
11647	def fit_transform ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) discard_X = not self . copy and self . negatives_likely vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = discard_X ) vals = vals [ : , None ] self . clip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) if discard_X or vals [ 0 , 0 ] < 0 : del X np . abs ( vals , out = vals ) X = np . dot ( vecs , vals * vecs . T ) del vals , vecs X = Symmetrize ( copy = False ) . fit_transform ( X ) return X
5292	def post ( self , request , * args , ** kwargs ) : form_class = self . get_form_class ( ) form = self . get_form ( form_class ) if form . is_valid ( ) : self . object = form . save ( commit = False ) form_validated = True else : form_validated = False inlines = self . construct_inlines ( ) if all_valid ( inlines ) and form_validated : return self . forms_valid ( form , inlines ) return self . forms_invalid ( form , inlines )
7889	def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) if t == "unavailable" : self . role = "none" self . affiliation = "none" self . room_jid = self . presence . get_from ( ) self . nick = self . room_jid . resource mc = self . presence . get_muc_child ( ) if isinstance ( mc , MucUserX ) : items = mc . get_items ( ) for item in items : if not isinstance ( item , MucItem ) : continue if item . role : self . role = item . role if item . affiliation : self . affiliation = item . affiliation if item . jid : self . real_jid = item . jid if item . nick : self . new_nick = item . nick break
50	def copy ( self , x = None , y = None ) : return self . deepcopy ( x = x , y = y )
3774	def select_valid_methods ( self , T ) : r if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
12574	def smooth_fwhm ( self , fwhm ) : if fwhm != self . _smooth_fwhm : self . _is_data_smooth = False self . _smooth_fwhm = fwhm
3148	def all ( self , workflow_id ) : self . workflow_id = workflow_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'removed-subscribers' ) )
8708	def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
8269	def contains ( self , clr ) : if not isinstance ( clr , Color ) : return False if not isinstance ( clr , _list ) : clr = [ clr ] for clr in clr : if clr . is_grey and not self . grayscale : return ( self . black . contains ( clr ) or self . white . contains ( clr ) ) for r , v in [ ( self . h , clr . h ) , ( self . s , clr . s ) , ( self . b , clr . brightness ) , ( self . a , clr . a ) ] : if isinstance ( r , _list ) : pass elif isinstance ( r , tuple ) : r = [ r ] else : r = [ ( r , r ) ] for min , max in r : if not ( min <= v <= max ) : return False return True
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
2605	def make_hash ( self , task ) : t = [ serialize_object ( task [ 'func_name' ] ) [ 0 ] , serialize_object ( task [ 'fn_hash' ] ) [ 0 ] , serialize_object ( task [ 'args' ] ) [ 0 ] , serialize_object ( task [ 'kwargs' ] ) [ 0 ] , serialize_object ( task [ 'env' ] ) [ 0 ] ] x = b'' . join ( t ) hashedsum = hashlib . md5 ( x ) . hexdigest ( ) return hashedsum
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
3918	def _handle_event ( self , conv_event ) : if not self . _is_scrolling : self . set_focus ( conv_event . id_ ) else : self . _modified ( )
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
11747	def routes_simple ( self ) : routes = [ ] for bundle in self . _registered_bundles : bundle_path = bundle [ 'path' ] for blueprint in bundle [ 'blueprints' ] : bp_path = blueprint [ 'path' ] for child in blueprint [ 'routes' ] : routes . append ( ( child [ 'endpoint' ] , bundle_path + bp_path + child [ 'path' ] , child [ 'methods' ] ) ) return routes
643	def getConfigPaths ( cls ) : configPaths = [ ] if cls . _configPaths is not None : return cls . _configPaths else : if 'NTA_CONF_PATH' in os . environ : configVar = os . environ [ 'NTA_CONF_PATH' ] configPaths = configVar . split ( os . pathsep ) return configPaths
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
10216	def to_jupyter ( graph : BELGraph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render_with_javascript.js' ) , 'rt' ) as f : js_template = Template ( f . read ( ) ) return Javascript ( js_template . render ( ** _get_context ( graph , chart = chart ) ) )
13572	def download ( course , tid = None , dl_all = False , force = False , upgradejava = False , update = False ) : def dl ( id ) : download_exercise ( Exercise . get ( Exercise . tid == id ) , force = force , update_java = upgradejava , update = update ) if dl_all : for exercise in list ( course . exercises ) : dl ( exercise . tid ) elif tid is not None : dl ( int ( tid ) ) else : for exercise in list ( course . exercises ) : if not exercise . is_completed : dl ( exercise . tid ) else : exercise . update_downloaded ( )
8348	def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : return return self . convert_codepoint ( n )
7264	def validate ( method ) : name_error = 'configuration option "{}" is not supported' @ functools . wraps ( method ) def validator ( self , name , * args ) : if name not in self . allowed_opts : raise ValueError ( name_error . format ( name ) ) return method ( self , name , * args ) return validator
4318	def _stat_call ( filepath ) : validate_input_file ( filepath ) args = [ 'sox' , filepath , '-n' , 'stat' ] _ , _ , stat_output = sox ( args ) return stat_output
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
5875	def get_images_bytesize_match ( self , images ) : cnt = 0 max_bytes_size = 15728640 good_images = [ ] for image in images : if cnt > 30 : return good_images src = self . parser . getAttribute ( image , attr = 'src' ) src = self . build_image_path ( src ) src = self . add_schema_if_none ( src ) local_image = self . get_local_image ( src ) if local_image : filesize = local_image . bytes if ( filesize == 0 or filesize > self . images_min_bytes ) and filesize < max_bytes_size : good_images . append ( image ) else : images . remove ( image ) cnt += 1 return good_images if len ( good_images ) > 0 else None
827	def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
8423	def hls_palette ( n_colors = 6 , h = .01 , l = .6 , s = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues -= hues . astype ( int ) palette = [ colorsys . hls_to_rgb ( h_i , l , s ) for h_i in hues ] return palette
3260	def get_layergroup ( self , name , workspace = None ) : layergroups = self . get_layergroups ( names = name , workspaces = workspace ) return self . _return_first_item ( layergroups )
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : raise e
7777	def __make_fn ( self ) : s = [ ] if self . n . prefix : s . append ( self . n . prefix ) if self . n . given : s . append ( self . n . given ) if self . n . middle : s . append ( self . n . middle ) if self . n . family : s . append ( self . n . family ) if self . n . suffix : s . append ( self . n . suffix ) s = u" " . join ( s ) self . content [ "FN" ] = VCardString ( "FN" , s , empty_ok = True )
8322	def sanitize ( self , val ) : if self . type == NUMBER : try : return clamp ( self . min , self . max , float ( val ) ) except ValueError : return 0.0 elif self . type == TEXT : try : return unicode ( str ( val ) , "utf_8" , "replace" ) except : return "" elif self . type == BOOLEAN : if unicode ( val ) . lower ( ) in ( "true" , "1" , "yes" ) : return True else : return False
2637	def parent_callback ( self , parent_fu ) : if parent_fu . done ( ) is True : e = parent_fu . _exception if e : super ( ) . set_exception ( e ) else : super ( ) . set_result ( self . file_obj ) return
1397	def extract_execution_state ( self , topology ) : execution_state = topology . execution_state executionState = { "cluster" : execution_state . cluster , "environ" : execution_state . environ , "role" : execution_state . role , "jobname" : topology . name , "submission_time" : execution_state . submission_time , "submission_user" : execution_state . submission_user , "release_username" : execution_state . release_state . release_username , "release_tag" : execution_state . release_state . release_tag , "release_version" : execution_state . release_state . release_version , "has_physical_plan" : None , "has_tmaster_location" : None , "has_scheduler_location" : None , "extra_links" : [ ] , } for extra_link in self . config . extra_links : link = extra_link . copy ( ) link [ "url" ] = self . config . get_formatted_url ( executionState , link [ EXTRA_LINK_FORMATTER_KEY ] ) executionState [ "extra_links" ] . append ( link ) return executionState
11072	def _to_primary_key ( self , value ) : if value is None : return None if isinstance ( value , self . base_class ) : if not value . _is_loaded : raise exceptions . DatabaseError ( 'Record must be loaded.' ) return value . _primary_key return self . base_class . _to_primary_key ( value )
12240	def booth ( theta ) : x , y = theta A = x + 2 * y - 7 B = 2 * x + y - 5 obj = A ** 2 + B ** 2 grad = np . array ( [ 2 * A + 4 * B , 4 * A + 2 * B ] ) return obj , grad
11941	def mark_all_read ( user ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) backend . inbox_purge ( user )
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
4875	def validate_tpa_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : tpa_client = ThirdPartyAuthApiClient ( ) username = tpa_client . get_username_from_remote_id ( enterprise_customer . identity_provider , value ) user = User . objects . get ( username = username ) return models . EnterpriseCustomerUser . objects . get ( user_id = user . id , enterprise_customer = enterprise_customer ) except ( models . EnterpriseCustomerUser . DoesNotExist , User . DoesNotExist ) : pass return None
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
3177	def create ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list merge field must have a name' ) if 'type' not in data : raise KeyError ( 'The list merge field must have a type' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'merge-fields' ) , data = data ) if response is not None : self . merge_id = response [ 'merge_id' ] else : self . merge_id = None return response
3634	def search ( self , ctype , level = None , category = None , assetId = None , defId = None , min_price = None , max_price = None , min_buy = None , max_buy = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None , start = 0 , page_size = itemsPerPage [ 'transferMarket' ] , fast = False ) : method = 'GET' url = 'transfermarket' if start == 0 : events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer Market Search' ) ] self . pin . send ( events , fast = fast ) params = { 'start' : start , 'num' : page_size , 'type' : ctype , } if level : params [ 'lev' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if defId : params [ 'definitionId' ] = defId if min_price : params [ 'micr' ] = min_price if max_price : params [ 'macr' ] = max_price if min_buy : params [ 'minb' ] = min_buy if max_buy : params [ 'maxb' ] = max_buy if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params , fast = fast ) if start == 0 : events = [ self . pin . event ( 'page_view' , 'Transfer Market Results - List View' ) , self . pin . event ( 'page_view' , 'Item - Detail View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
13863	def ts ( when , tz = None ) : if not when : return None when = totz ( when , tz ) return calendar . timegm ( when . timetuple ( ) )
4184	def window_bohman ( N ) : r x = linspace ( - 1 , 1 , N ) w = ( 1. - abs ( x ) ) * cos ( pi * abs ( x ) ) + 1. / pi * sin ( pi * abs ( x ) ) return w
8443	def ls ( github_user , template = None ) : temple . check . has_env_vars ( temple . constants . GITHUB_API_TOKEN_ENV_VAR ) if template : temple . check . is_git_ssh_path ( template ) search_q = 'user:{} filename:{} {}' . format ( github_user , temple . constants . TEMPLE_CONFIG_FILE , template ) else : search_q = 'user:{} cookiecutter.json in:path' . format ( github_user ) results = _code_search ( search_q , github_user ) return collections . OrderedDict ( sorted ( results . items ( ) ) )
11019	def show_response_messages ( response_json ) : message_type_kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response_json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , ** message_type_kwargs . get ( message [ 'type' ] , { } ) )
1634	def CheckPosixThreading ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] for single_thread_func , multithread_safe_func , pattern in _THREADING_LIST : if Search ( pattern , line ) : error ( filename , linenum , 'runtime/threadsafe_fn' , 2 , 'Consider using ' + multithread_safe_func + '...) instead of ' + single_thread_func + '...) for improved thread safety.' )
4237	def get_attached_devices_2 ( self ) : _LOGGER . info ( "Get attached devices 2" ) success , response = self . _make_request ( SERVICE_DEVICE_INFO , "GetAttachDevice2" ) if not success : return None success , devices_node = _find_node ( response . text , ".//GetAttachDevice2Response/NewAttachDevice" ) if not success : return None xml_devices = devices_node . findall ( "Device" ) devices = [ ] for d in xml_devices : ip = _xml_get ( d , 'IP' ) name = _xml_get ( d , 'Name' ) mac = _xml_get ( d , 'MAC' ) signal = _convert ( _xml_get ( d , 'SignalStrength' ) , int ) link_type = _xml_get ( d , 'ConnectionType' ) link_rate = _xml_get ( d , 'Linkspeed' ) allow_or_block = _xml_get ( d , 'AllowOrBlock' ) device_type = _convert ( _xml_get ( d , 'DeviceType' ) , int ) device_model = _xml_get ( d , 'DeviceModel' ) ssid = _xml_get ( d , 'SSID' ) conn_ap_mac = _xml_get ( d , 'ConnAPMAC' ) devices . append ( Device ( name , ip , mac , link_type , signal , link_rate , allow_or_block , device_type , device_model , ssid , conn_ap_mac ) ) return devices
11722	def app_class ( ) : try : pkg_resources . get_distribution ( 'invenio-files-rest' ) from invenio_files_rest . app import Flask as FlaskBase except pkg_resources . DistributionNotFound : from flask import Flask as FlaskBase class Request ( TrustedHostsMixin , FlaskBase . request_class ) : pass class Flask ( FlaskBase ) : request_class = Request return Flask
2078	def associate_notification_template ( self , job_template , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , job_template , notification_template )
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
13291	def get_variables_by_attributes ( self , ** kwargs ) : vs = [ ] has_value_flag = False for vname in self . variables : var = self . variables [ vname ] for k , v in kwargs . items ( ) : if callable ( v ) : has_value_flag = v ( getattr ( var , k , None ) ) if has_value_flag is False : break elif hasattr ( var , k ) and getattr ( var , k ) == v : has_value_flag = True else : has_value_flag = False break if has_value_flag is True : vs . append ( self . variables [ vname ] ) return vs
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
17	def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )
12603	def duplicated_rows ( df , col_name ) : _check_cols ( df , [ col_name ] ) dups = df [ pd . notnull ( df [ col_name ] ) & df . duplicated ( subset = [ col_name ] ) ] return dups
10814	def add_member ( self , user , state = MembershipState . ACTIVE ) : return Membership . create ( self , user , state )
3474	def check_mass_balance ( self ) : reaction_element_dict = defaultdict ( int ) for metabolite , coefficient in iteritems ( self . _metabolites ) : if metabolite . charge is not None : reaction_element_dict [ "charge" ] += coefficient * metabolite . charge if metabolite . elements is None : raise ValueError ( "No elements found in metabolite %s" % metabolite . id ) for element , amount in iteritems ( metabolite . elements ) : reaction_element_dict [ element ] += coefficient * amount return { k : v for k , v in iteritems ( reaction_element_dict ) if v != 0 }
12022	def check_parent_boundary ( self ) : for line in self . lines : for parent_feature in line [ 'parents' ] : ok = False for parent_line in parent_feature : if parent_line [ 'start' ] <= line [ 'start' ] and line [ 'end' ] <= parent_line [ 'end' ] : ok = True break if not ok : self . add_line_error ( line , { 'message' : 'This feature is not contained within the feature boundaries of parent: {0:s}: {1:s}' . format ( parent_feature [ 0 ] [ 'attributes' ] [ 'ID' ] , ',' . join ( [ '({0:s}, {1:d}, {2:d})' . format ( line [ 'seqid' ] , line [ 'start' ] , line [ 'end' ] ) for line in parent_feature ] ) ) , 'error_type' : 'BOUNDS' , 'location' : 'parent_boundary' } )
5788	def _bcrypt_encrypt ( cipher , key , data , iv , padding ) : key_handle = None try : key_handle = _bcrypt_create_key_handle ( cipher , key ) if iv is None : iv_len = 0 else : iv_len = len ( iv ) flags = 0 if padding is True : flags = BcryptConst . BCRYPT_BLOCK_PADDING out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) iv_buffer = buffer_from_bytes ( iv ) if iv else null ( ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , iv_buffer , iv_len , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) finally : if key_handle : bcrypt . BCryptDestroyKey ( key_handle )
1799	def CMOVO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
3643	def quickSell ( self , item_id ) : method = 'DELETE' url = 'item' if not isinstance ( item_id , ( list , tuple ) ) : item_id = ( item_id , ) item_id = ( str ( i ) for i in item_id ) params = { 'itemIds' : ',' . join ( item_id ) } self . __request__ ( method , url , params = params ) return True
13064	def make_coins ( self , collection , text , subreference = "" , lang = None ) : if lang is None : lang = self . __default_lang__ return "url_ver=Z39.88-2004" "&ctx_ver=Z39.88-2004" "&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" "&rft_id={cid}" "&rft.genre=bookitem" "&rft.btitle={title}" "&rft.edition={edition}" "&rft.au={author}" "&rft.atitle={pages}" "&rft.language={language}" "&rft.pages={pages}" . format ( title = quote ( str ( text . get_title ( lang ) ) ) , author = quote ( str ( text . get_creator ( lang ) ) ) , cid = url_for ( ".r_collection" , objectId = collection . id , _external = True ) , language = collection . lang , pages = quote ( subreference ) , edition = quote ( str ( text . get_description ( lang ) ) ) )
2990	def cross_origin ( * args , ** kwargs ) : _options = kwargs def decorator ( f ) : LOG . debug ( "Enabling %s for cross_origin using options:%s" , f , _options ) if _options . get ( 'automatic_options' , True ) : f . required_methods = getattr ( f , 'required_methods' , set ( ) ) f . required_methods . add ( 'OPTIONS' ) f . provide_automatic_options = False def wrapped_function ( * args , ** kwargs ) : options = get_cors_options ( current_app , _options ) if options . get ( 'automatic_options' ) and request . method == 'OPTIONS' : resp = current_app . make_default_options_response ( ) else : resp = make_response ( f ( * args , ** kwargs ) ) set_cors_headers ( resp , options ) setattr ( resp , FLASK_CORS_EVALUATED , True ) return resp return update_wrapper ( wrapped_function , f ) return decorator
13231	def get_def_macros ( tex_source ) : r macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , ** filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
6173	def single_instance ( func = None , lock_timeout = None , include_args = False ) : if func is None : return partial ( single_instance , lock_timeout = lock_timeout , include_args = include_args ) @ wraps ( func ) def wrapped ( celery_self , * args , ** kwargs ) : timeout = ( lock_timeout or celery_self . soft_time_limit or celery_self . time_limit or celery_self . app . conf . get ( 'CELERYD_TASK_SOFT_TIME_LIMIT' ) or celery_self . app . conf . get ( 'CELERYD_TASK_TIME_LIMIT' ) or ( 60 * 5 ) ) manager_class = _select_manager ( celery_self . backend . __class__ . __name__ ) lock_manager = manager_class ( celery_self , timeout , include_args , args , kwargs ) with lock_manager : ret_value = func ( * args , ** kwargs ) return ret_value return wrapped
6375	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _umlauts ) wlen = len ( word ) - 1 if wlen > 3 : if wlen > 5 : if word [ - 3 : ] == 'nen' : return word [ : - 3 ] if wlen > 4 : if word [ - 2 : ] in { 'en' , 'se' , 'es' , 'er' } : return word [ : - 2 ] if word [ - 1 ] in { 'e' , 'n' , 'r' , 's' } : return word [ : - 1 ] return word
9879	def _random_coincidences ( value_domain , n , n_v ) : n_v_column = n_v . reshape ( - 1 , 1 ) return ( n_v_column . dot ( n_v_column . T ) - np . eye ( len ( value_domain ) ) * n_v_column ) / ( n - 1 )
284	def plot_drawdown_periods ( returns , top = 10 , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) df_drawdowns = timeseries . gen_drawdown_table ( returns , top = top ) df_cum_rets . plot ( ax = ax , ** kwargs ) lim = ax . get_ylim ( ) colors = sns . cubehelix_palette ( len ( df_drawdowns ) ) [ : : - 1 ] for i , ( peak , recovery ) in df_drawdowns [ [ 'Peak date' , 'Recovery date' ] ] . iterrows ( ) : if pd . isnull ( recovery ) : recovery = returns . index [ - 1 ] ax . fill_between ( ( peak , recovery ) , lim [ 0 ] , lim [ 1 ] , alpha = .4 , color = colors [ i ] ) ax . set_ylim ( lim ) ax . set_title ( 'Top %i drawdown periods' % top ) ax . set_ylabel ( 'Cumulative returns' ) ax . legend ( [ 'Portfolio' ] , loc = 'upper left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
5834	def create_ml_configuration ( self , search_template , extract_as_keys , dataset_ids ) : data = { "search_template" : search_template , "extract_as_keys" : extract_as_keys } failure_message = "ML Configuration creation failed" config_job_id = self . _get_success_json ( self . _post_json ( 'v1/descriptors/builders/simple/default/trigger' , data , failure_message = failure_message ) ) [ 'data' ] [ 'result' ] [ 'uid' ] while True : config_status = self . __get_ml_configuration_status ( config_job_id ) print ( 'Configuration status: ' , config_status ) if config_status [ 'status' ] == 'Finished' : ml_config = self . __convert_response_to_configuration ( config_status [ 'result' ] , dataset_ids ) return ml_config time . sleep ( 5 )
11454	def from_source ( cls , source ) : bibrecs = BibRecordPackage ( source ) bibrecs . parse ( ) for bibrec in bibrecs . get_records ( ) : yield cls ( bibrec )
715	def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , "wb" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )
4760	def wait ( timeout = 300 ) : if env ( ) : cij . err ( "cij.ssh.wait: Invalid SSH environment" ) return 1 timeout_backup = cij . ENV . get ( "SSH_CMD_TIMEOUT" ) try : time_start = time . time ( ) cij . ENV [ "SSH_CMD_TIMEOUT" ] = "3" while True : time_current = time . time ( ) if ( time_current - time_start ) > timeout : cij . err ( "cij.ssh.wait: Timeout" ) return 1 status , _ , _ = command ( [ "exit" ] , shell = True , echo = False ) if not status : break cij . info ( "cij.ssh.wait: Time elapsed: %d seconds" % ( time_current - time_start ) ) finally : if timeout_backup is None : del cij . ENV [ "SSH_CMD_TIMEOUT" ] else : cij . ENV [ "SSH_CMD_TIMEOUT" ] = timeout_backup return 0
5080	def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
8904	def _request ( self , endpoint , method = "GET" , lookup = None , data = { } , params = { } , userargs = None , password = None ) : if isinstance ( userargs , dict ) : userargs = "&" . join ( [ "{}={}" . format ( key , val ) for ( key , val ) in iteritems ( userargs ) ] ) if lookup : lookup = lookup + '/' url = urljoin ( urljoin ( self . base_url , endpoint ) , lookup ) auth = ( userargs , password ) if userargs else None resp = requests . request ( method , url , json = data , params = params , auth = auth ) resp . raise_for_status ( ) return resp
5794	def _extract_error ( ) : error_num = errno ( ) try : error_string = os . strerror ( error_num ) except ( ValueError ) : return str_cls ( error_num ) if isinstance ( error_string , str_cls ) : return error_string return _try_decode ( error_string )
33	def reset ( self , ** kwargs ) : if self . was_real_done : obs = self . env . reset ( ** kwargs ) else : obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs
3489	def _sbase_notes_dict ( sbase , notes ) : if notes and len ( notes ) > 0 : tokens = [ '<html xmlns = "http://www.w3.org/1999/xhtml" >' ] + [ "<p>{}: {}</p>" . format ( k , v ) for ( k , v ) in notes . items ( ) ] + [ "</html>" ] _check ( sbase . setNotes ( "\n" . join ( tokens ) ) , "Setting notes on sbase: {}" . format ( sbase ) )
1438	def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
1089	def indexOf ( a , b ) : "Return the first index of b in a." for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )
5272	def _find_lcs ( self , node , stringIdxs ) : nodes = [ self . _find_lcs ( n , stringIdxs ) for ( n , _ ) in node . transition_links if n . generalized_idxs . issuperset ( stringIdxs ) ] if nodes == [ ] : return node deepestNode = max ( nodes , key = lambda n : n . depth ) return deepestNode
8491	def _parse_hosts ( self , hosts ) : if hosts is None : return if isinstance ( hosts , six . string_types ) : hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] hosts = [ host . split ( ':' ) for host in hosts ] hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] return tuple ( hosts )
12815	def _finish ( self , forced = False ) : if hasattr ( self , "_current_file_handle" ) and self . _current_file_handle : self . _current_file_handle . close ( ) if self . _current_deferred : self . _current_deferred . callback ( self . _sent ) self . _current_deferred = None if not forced and self . _deferred : self . _deferred . callback ( self . _sent )
3699	def Tliquidus ( Tms = None , ws = None , xs = None , CASRNs = None , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if none_and_length_check ( [ Tms ] ) : methods . append ( 'Maximum' ) methods . append ( 'Simple' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Maximum' : _Tliq = max ( Tms ) elif Method == 'Simple' : _Tliq = mixing_simple ( xs , Tms ) elif Method == 'None' : return None else : raise Exception ( 'Failure in in function' ) return _Tliq
6097	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( radius , dim . Length ) : radius = dim . Length ( value = radius , unit_length = 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
6093	def grid_angle_to_profile ( self , grid_thetas ) : theta_coordinate_to_profile = np . add ( grid_thetas , - self . phi_radians ) return np . cos ( theta_coordinate_to_profile ) , np . sin ( theta_coordinate_to_profile )
7909	def __presence_available ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_available_presence ( MucPresence ( stanza ) ) return True
323	def gen_drawdown_table ( returns , top = 10 ) : df_cum = ep . cum_returns ( returns , 1.0 ) drawdown_periods = get_top_drawdowns ( returns , top = top ) df_drawdowns = pd . DataFrame ( index = list ( range ( top ) ) , columns = [ 'Net drawdown in %' , 'Peak date' , 'Valley date' , 'Recovery date' , 'Duration' ] ) for i , ( peak , valley , recovery ) in enumerate ( drawdown_periods ) : if pd . isnull ( recovery ) : df_drawdowns . loc [ i , 'Duration' ] = np . nan else : df_drawdowns . loc [ i , 'Duration' ] = len ( pd . date_range ( peak , recovery , freq = 'B' ) ) df_drawdowns . loc [ i , 'Peak date' ] = ( peak . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Valley date' ] = ( valley . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) if isinstance ( recovery , float ) : df_drawdowns . loc [ i , 'Recovery date' ] = recovery else : df_drawdowns . loc [ i , 'Recovery date' ] = ( recovery . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Net drawdown in %' ] = ( ( df_cum . loc [ peak ] - df_cum . loc [ valley ] ) / df_cum . loc [ peak ] ) * 100 df_drawdowns [ 'Peak date' ] = pd . to_datetime ( df_drawdowns [ 'Peak date' ] ) df_drawdowns [ 'Valley date' ] = pd . to_datetime ( df_drawdowns [ 'Valley date' ] ) df_drawdowns [ 'Recovery date' ] = pd . to_datetime ( df_drawdowns [ 'Recovery date' ] ) return df_drawdowns
8855	def setup_mnu_style ( self , editor ) : menu = QtWidgets . QMenu ( 'Styles' , self . menuEdit ) group = QtWidgets . QActionGroup ( self ) self . styles_group = group current_style = editor . syntax_highlighter . color_scheme . name group . triggered . connect ( self . on_style_changed ) for s in sorted ( PYGMENTS_STYLES ) : a = QtWidgets . QAction ( menu ) a . setText ( s ) a . setCheckable ( True ) if s == current_style : a . setChecked ( True ) group . addAction ( a ) menu . addAction ( a ) self . menuEdit . addMenu ( menu )
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
7998	def set_authenticated ( self , me , restart_stream = False ) : with self . lock : self . authenticated = True self . me = me if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . me ) )
8889	def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
1497	def parse_query_string ( self , query ) : if not query : return None if query [ 0 ] == '(' : index = self . find_closing_braces ( query ) if index != len ( query ) - 1 : raise Exception ( "Invalid syntax" ) else : return self . parse_query_string ( query [ 1 : - 1 ] ) start_index = query . find ( "(" ) if start_index < 0 : try : constant = float ( query ) return constant except ValueError : raise Exception ( "Invalid syntax" ) token = query [ : start_index ] if token not in self . operators : raise Exception ( "Invalid token: " + token ) rest_of_the_query = query [ start_index : ] braces_end_index = self . find_closing_braces ( rest_of_the_query ) if braces_end_index != len ( rest_of_the_query ) - 1 : raise Exception ( "Invalid syntax" ) parts = self . get_sub_parts ( rest_of_the_query [ 1 : - 1 ] ) if token == "TS" : return self . operators [ token ] ( parts ) children = [ ] for part in parts : children . append ( self . parse_query_string ( part ) ) node = self . operators [ token ] ( children ) return node
2398	def encode_plus ( s ) : regex = r"\+" pat = re . compile ( regex ) return pat . sub ( "%2B" , s )
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
7562	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmptre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp , 'r' ) as intree : tre = ete3 . Tree ( intree . read ( ) . strip ( ) ) names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] tmptre = tre . write ( format = 9 ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmptre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmptre ) self . _save ( )
3211	def insert ( self , key , obj , future_expiration_minutes = 15 ) : expiration_time = self . _calculate_expiration ( future_expiration_minutes ) self . _CACHE [ key ] = ( expiration_time , obj ) return True
2129	def configure_display ( self , data , kwargs = None , write = False ) : if settings . format != 'human' : return if write : obj , obj_type , res , res_type = self . obj_res ( kwargs ) data [ 'type' ] = kwargs [ 'type' ] data [ obj_type ] = obj data [ res_type ] = res self . set_display_columns ( set_false = [ 'team' if obj_type == 'user' else 'user' ] , set_true = [ 'target_team' if res_type == 'team' else res_type ] ) else : self . set_display_columns ( set_false = [ 'user' , 'team' ] , set_true = [ 'resource_name' , 'resource_type' ] ) if 'results' in data : for i in range ( len ( data [ 'results' ] ) ) : self . populate_resource_columns ( data [ 'results' ] [ i ] ) else : self . populate_resource_columns ( data )
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( "--" ) and len ( arg ) > 2 : break if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )
8280	def _render_closure ( self ) : fillcolor = self . fill strokecolor = self . stroke strokewidth = self . strokewidth def _render ( cairo_ctx ) : transform = self . _call_transform_mode ( self . _transform ) if fillcolor is None and strokecolor is None : return cairo_ctx . set_matrix ( transform ) self . _traverse ( cairo_ctx ) cairo_ctx . set_matrix ( cairo . Matrix ( ) ) if fillcolor is not None and strokecolor is not None : if strokecolor [ 3 ] < 1 : cairo_ctx . push_group ( ) cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) e = cairo_ctx . stroke_extents ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_operator ( cairo . OPERATOR_SOURCE ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) cairo_ctx . pop_group_to_source ( ) cairo_ctx . paint ( ) else : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill_preserve ( ) cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) elif fillcolor is not None : cairo_ctx . set_source_rgba ( * fillcolor ) cairo_ctx . fill ( ) elif strokecolor is not None : cairo_ctx . set_source_rgba ( * strokecolor ) cairo_ctx . set_line_width ( strokewidth ) cairo_ctx . stroke ( ) return _render
7051	def parallel_tfa_lclist ( lclist , templateinfo , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = templateinfo [ 'timecols' ] if magcols is None : magcols = templateinfo [ 'magcols' ] if errcols is None : errcols = templateinfo [ 'errcols' ] outdict = { } for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , templateinfo , lcformat , lcformatdir , interp , sigclip ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( _parallel_tfa_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
3035	def flow_from_clientsecrets ( filename , scope , redirect_uri = None , message = None , cache = None , login_hint = None , device_uri = None , pkce = None , code_verifier = None , prompt = None ) : try : client_type , client_info = clientsecrets . loadfile ( filename , cache = cache ) if client_type in ( clientsecrets . TYPE_WEB , clientsecrets . TYPE_INSTALLED ) : constructor_kwargs = { 'redirect_uri' : redirect_uri , 'auth_uri' : client_info [ 'auth_uri' ] , 'token_uri' : client_info [ 'token_uri' ] , 'login_hint' : login_hint , } revoke_uri = client_info . get ( 'revoke_uri' ) optional = ( 'revoke_uri' , 'device_uri' , 'pkce' , 'code_verifier' , 'prompt' ) for param in optional : if locals ( ) [ param ] is not None : constructor_kwargs [ param ] = locals ( ) [ param ] return OAuth2WebServerFlow ( client_info [ 'client_id' ] , client_info [ 'client_secret' ] , scope , ** constructor_kwargs ) except clientsecrets . InvalidClientSecretsError as e : if message is not None : if e . args : message = ( 'The client secrets were invalid: ' '\n{0}\n{1}' . format ( e , message ) ) sys . exit ( message ) else : raise else : raise UnknownClientSecretsFlowError ( 'This OAuth 2.0 flow is unsupported: {0!r}' . format ( client_type ) )
6068	def tabulate_integral ( self , grid , tabulate_bins ) : eta_min = 1.0e-4 eta_max = 1.05 * np . max ( self . grid_to_elliptical_radii ( grid ) ) minimum_log_eta = np . log10 ( eta_min ) maximum_log_eta = np . log10 ( eta_max ) bin_size = ( maximum_log_eta - minimum_log_eta ) / ( tabulate_bins - 1 ) return eta_min , eta_max , minimum_log_eta , maximum_log_eta , bin_size
7499	def resolve_ambigs ( tmpseq ) : for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : idx , idy = np . where ( tmpseq == ambig ) res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
13789	def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup_logging ( loglevel , verbosity , logfilter )
1806	def SETB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
2554	def setdocument ( self , doc ) : if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
12059	def TK_message ( title , msg ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) tkinter . messagebox . showwarning ( title , msg ) root . destroy ( )
3720	def ionic_strength ( mis , zis ) : r return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
5929	def getLogLevel ( self , section , option ) : return logging . getLevelName ( self . get ( section , option ) . upper ( ) )
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
5801	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : all_purposes = '2.5.29.37.0' ca_path = system_path ( ) output = [ ] with open ( ca_path , 'rb' ) as f : for armor_type , _ , cert_bytes in unarmor ( f . read ( ) , multiple = True ) : if armor_type == 'CERTIFICATE' : if cert_callback : cert_callback ( Certificate . load ( cert_bytes ) , None ) output . append ( ( cert_bytes , set ( ) , set ( ) ) ) elif armor_type == 'TRUSTED CERTIFICATE' : cert , aux = TrustedCertificate . load ( cert_bytes ) reject_all = False trust_oids = set ( ) reject_oids = set ( ) for purpose in aux [ 'trust' ] : if purpose . dotted == all_purposes : trust_oids = set ( [ purpose . dotted ] ) break trust_oids . add ( purpose . dotted ) for purpose in aux [ 'reject' ] : if purpose . dotted == all_purposes : reject_all = True break reject_oids . add ( purpose . dotted ) if reject_all : if cert_callback : cert_callback ( cert , 'explicitly distrusted' ) continue if cert_callback and not callback_only_on_failure : cert_callback ( cert , None ) output . append ( ( cert . dump ( ) , trust_oids , reject_oids ) ) return output
4666	def sign ( self , wifkeys , chain = None ) : if not chain : chain = self . get_default_prefix ( ) self . deriveDigest ( chain ) self . privkeys = [ ] for item in wifkeys : if item not in self . privkeys : self . privkeys . append ( item ) sigs = [ ] for wif in self . privkeys : signature = sign_message ( self . message , wif ) sigs . append ( Signature ( signature ) ) self . data [ "signatures" ] = Array ( sigs ) return self
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
13182	def writerow ( self , observation_data ) : if isinstance ( observation_data , ( list , tuple ) ) : row = observation_data else : row = self . dict_to_row ( observation_data ) self . writer . writerow ( row )
8899	def _dequeue_into_store ( transfersession ) : with connection . cursor ( ) as cursor : DBBackend . _dequeuing_delete_rmcb_records ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_buffered_records ( cursor , transfersession . id ) current_id = InstanceIDModel . get_current_instance_and_increment_counter ( ) DBBackend . _dequeuing_merge_conflict_buffer ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_merge_conflict_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_update_rmcs_last_saved_by ( cursor , current_id , transfersession . id ) DBBackend . _dequeuing_delete_mc_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_mc_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_buffer ( cursor , transfersession . id ) DBBackend . _dequeuing_insert_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_rmcb ( cursor , transfersession . id ) DBBackend . _dequeuing_delete_remaining_buffer ( cursor , transfersession . id ) if getattr ( settings , 'MORANGO_DESERIALIZE_AFTER_DEQUEUING' , True ) : _deserialize_from_store ( transfersession . sync_session . profile )
3758	def UFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'UFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'UFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'UFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'UFL' ] ) elif Method == SUZUKI : return Suzuki_UFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_UFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
11948	def init ( base_level = DEFAULT_BASE_LOGGING_LEVEL , verbose_level = DEFAULT_VERBOSE_LOGGING_LEVEL , logging_config = None ) : if logging_config is None : logging_config = { } logging_config = logging_config or LOGGER log_file = LOGGER [ 'handlers' ] [ 'file' ] [ 'filename' ] log_dir = os . path . dirname ( os . path . expanduser ( log_file ) ) if os . path . isfile ( log_dir ) : sys . exit ( 'file {0} exists - log directory cannot be created ' 'there. please remove the file and try again.' . format ( log_dir ) ) try : if not os . path . exists ( log_dir ) and not len ( log_dir ) == 0 : os . makedirs ( log_dir ) dictconfig . dictConfig ( logging_config ) lgr = logging . getLogger ( 'user' ) lgr . setLevel ( base_level ) return lgr except ValueError as e : sys . exit ( 'could not initialize logger.' ' verify your logger config' ' and permissions to write to {0} ({1})' . format ( log_file , e ) )
2172	def new_state ( self ) : try : self . _state = self . state ( ) log . debug ( "Generated new state %s." , self . _state ) except TypeError : self . _state = self . state log . debug ( "Re-using previously supplied state %s." , self . _state ) return self . _state
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
1072	def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist )
13843	def close ( self ) : try : self . conn . close ( ) self . logger . debug ( "Close connect succeed." ) except pymssql . Error as e : self . unknown ( "Close connect error: %s" % e )
10394	def calculate_average_score_by_annotation ( graph : BELGraph , annotation : str , key : Optional [ str ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) -> Mapping [ str , float ] : candidate_mechanisms = generate_bioprocess_mechanisms ( graph , key = key ) scores : Mapping [ BaseEntity , Tuple ] = calculate_average_scores_on_subgraphs ( subgraphs = candidate_mechanisms , key = key , runs = runs , use_tqdm = use_tqdm , ) subgraph_bp : Mapping [ str , List [ BaseEntity ] ] = defaultdict ( list ) subgraphs : Mapping [ str , BELGraph ] = get_subgraphs_by_annotation ( graph , annotation ) for annotation_value , subgraph in subgraphs . items ( ) : subgraph_bp [ annotation_value ] . extend ( get_nodes_by_function ( subgraph , BIOPROCESS ) ) return { annotation_value : np . average ( scores [ bp ] [ 0 ] for bp in bps ) for annotation_value , bps in subgraph_bp . items ( ) }
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
8386	def amend_filename ( filename , amend ) : base , ext = os . path . splitext ( filename ) amended_name = base + amend + ext return amended_name
220	async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return PlainTextResponse ( "Method Not Allowed" , status_code = 405 ) if path . startswith ( ".." ) : return PlainTextResponse ( "Not Found" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : index_path = os . path . join ( path , "index.html" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ "path" ] . endswith ( "/" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : full_path , stat_result = await self . lookup_path ( "404.html" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( "Not Found" , status_code = 404 )
12619	def check_img_compatibility ( one_img , another_img , only_check_3d = False ) : nd_to_check = None if only_check_3d : nd_to_check = 3 if hasattr ( one_img , 'shape' ) and hasattr ( another_img , 'shape' ) : if not have_same_shape ( one_img , another_img , nd_to_check = nd_to_check ) : msg = 'Shape of the first image: \n{}\n is different from second one: \n{}' . format ( one_img . shape , another_img . shape ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg ) if hasattr ( one_img , 'get_affine' ) and hasattr ( another_img , 'get_affine' ) : if not have_same_affine ( one_img , another_img , only_check_3d = only_check_3d ) : msg = 'Affine matrix of the first image: \n{}\n is different ' 'from second one:\n{}' . format ( one_img . get_affine ( ) , another_img . get_affine ( ) ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg )
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
11331	def progress ( length , ** kwargs ) : quiet = False progress_class = kwargs . pop ( "progress_class" , Progress ) kwargs [ "write_method" ] = istdout . info kwargs [ "width" ] = kwargs . get ( "width" , globals ( ) [ "WIDTH" ] ) kwargs [ "length" ] = length pbar = progress_class ( ** kwargs ) pbar . update ( 0 ) yield pbar pbar . update ( length ) br ( )
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
10971	def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query_by_user ( current_user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per_page = per_page ) requests = Membership . query_requests ( current_user ) . count ( ) invitations = Membership . query_invitations ( current_user ) . count ( ) return render_template ( 'invenio_groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per_page = per_page , q = q )
2145	def request ( self , method , url , * args , ** kwargs ) : import re url = re . sub ( "^/?api/v[0-9]+/" , "" , url ) use_version = not url . startswith ( '/o/' ) url = '%s%s' % ( self . get_prefix ( use_version ) , url . lstrip ( '/' ) ) kwargs . setdefault ( 'auth' , BasicTowerAuth ( settings . username , settings . password , self ) ) headers = kwargs . get ( 'headers' , { } ) if method . upper ( ) in ( 'PATCH' , 'POST' , 'PUT' ) : headers . setdefault ( 'Content-Type' , 'application/json' ) kwargs [ 'headers' ] = headers debug . log ( '%s %s' % ( method , url ) , fg = 'blue' , bold = True ) if method in ( 'POST' , 'PUT' , 'PATCH' ) : debug . log ( 'Data: %s' % kwargs . get ( 'data' , { } ) , fg = 'blue' , bold = True ) if method == 'GET' or kwargs . get ( 'params' , None ) : debug . log ( 'Params: %s' % kwargs . get ( 'params' , { } ) , fg = 'blue' , bold = True ) debug . log ( '' ) if headers . get ( 'Content-Type' , '' ) == 'application/json' : kwargs [ 'data' ] = json . dumps ( kwargs . get ( 'data' , { } ) ) r = self . _make_request ( method , url , args , kwargs ) if r . status_code >= 500 : raise exc . ServerError ( 'The Tower server sent back a server error. ' 'Please try again later.' ) if r . status_code == 401 : raise exc . AuthError ( 'Invalid Tower authentication credentials (HTTP 401).' ) if r . status_code == 403 : raise exc . Forbidden ( "You don't have permission to do that (HTTP 403)." ) if r . status_code == 404 : raise exc . NotFound ( 'The requested object could not be found.' ) if r . status_code == 405 : raise exc . MethodNotAllowed ( "The Tower server says you can't make a request with the " "%s method to that URL (%s)." % ( method , url ) , ) if r . status_code >= 400 : raise exc . BadRequest ( 'The Tower server claims it was sent a bad request.\n\n' '%s %s\nParams: %s\nData: %s\n\nResponse: %s' % ( method , url , kwargs . get ( 'params' , None ) , kwargs . get ( 'data' , None ) , r . content . decode ( 'utf8' ) ) ) r . __class__ = APIResponse return r
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
12230	def unpatch_locals ( depth = 3 ) : for name , locals_dict in traverse_local_prefs ( depth ) : if isinstance ( locals_dict [ name ] , PatchedLocal ) : locals_dict [ name ] = locals_dict [ name ] . val del get_frame_locals ( depth ) [ __PATCHED_LOCALS_SENTINEL ]
2940	def deserialize_condition ( self , workflow , start_node ) : condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
6048	def relocated_grid_stack_from_grid_stack ( self , grid_stack ) : border_grid = grid_stack . regular [ self ] return GridStack ( regular = self . relocated_grid_from_grid_jit ( grid = grid_stack . regular , border_grid = border_grid ) , sub = self . relocated_grid_from_grid_jit ( grid = grid_stack . sub , border_grid = border_grid ) , blurring = None , pix = self . relocated_grid_from_grid_jit ( grid = grid_stack . pix , border_grid = border_grid ) )
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
9077	def make_downloader ( url : str , path : str ) -> Callable [ [ bool ] , str ] : def download_data ( force_download : bool = False ) -> str : if os . path . exists ( path ) and not force_download : log . info ( 'using cached data at %s' , path ) else : log . info ( 'downloading %s to %s' , url , path ) urlretrieve ( url , path ) return path return download_data
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
13628	def parse ( expected , query ) : return dict ( ( key , parser ( query . get ( key , [ ] ) ) ) for key , parser in expected . items ( ) )
9696	def validate_token ( self , request , consumer , token ) : oauth_server , oauth_request = oauth_provider . utils . initialize_server_request ( request ) oauth_server . verify_request ( oauth_request , consumer , token )
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
60	def union ( self , other ) : return BoundingBox ( x1 = min ( self . x1 , other . x1 ) , y1 = min ( self . y1 , other . y1 ) , x2 = max ( self . x2 , other . x2 ) , y2 = max ( self . y2 , other . y2 ) , )
10418	def get_variants_to_controllers ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants_of ( graph , node , modifications ) for controller , variant , data in graph . in_edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL_RELATIONS : rv [ variant ] . add ( controller ) return rv
578	def dictDiffAndReport ( da , db ) : differences = dictDiff ( da , db ) if not differences : return differences if differences [ 'inAButNotInB' ] : print ">>> inAButNotInB: %s" % differences [ 'inAButNotInB' ] if differences [ 'inBButNotInA' ] : print ">>> inBButNotInA: %s" % differences [ 'inBButNotInA' ] for key in differences [ 'differentValues' ] : print ">>> da[%s] != db[%s]" % ( key , key ) print "da[%s] = %r" % ( key , da [ key ] ) print "db[%s] = %r" % ( key , db [ key ] ) return differences
3496	def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
4129	def _autocov ( s , ** kwargs ) : debias = kwargs . pop ( 'debias' , True ) axis = kwargs . get ( 'axis' , - 1 ) if debias : s = _remove_bias ( s , axis ) kwargs [ 'debias' ] = False return _crosscov ( s , s , ** kwargs )
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) self . forum . on_start_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
890	def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]
11568	def open ( self , verbose ) : if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : raise
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
13298	def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
3511	def sample ( model , n , method = "optgp" , thinning = 100 , processes = 1 , seed = None ) : if method == "optgp" : sampler = OptGPSampler ( model , processes , thinning = thinning , seed = seed ) elif method == "achr" : sampler = ACHRSampler ( model , thinning = thinning , seed = seed ) else : raise ValueError ( "method must be 'optgp' or 'achr'!" ) return pandas . DataFrame ( columns = [ rxn . id for rxn in model . reactions ] , data = sampler . sample ( n ) )
9264	def filter_merged_pull_requests ( self , pull_requests ) : if self . options . verbose : print ( "Fetching merge date for pull requests..." ) closed_pull_requests = self . fetcher . fetch_closed_pull_requests ( ) if not pull_requests : return [ ] pulls = copy . deepcopy ( pull_requests ) for pr in pulls : fetched_pr = None for fpr in closed_pull_requests : if fpr [ 'number' ] == pr [ 'number' ] : fetched_pr = fpr if fetched_pr : pr [ 'merged_at' ] = fetched_pr [ 'merged_at' ] closed_pull_requests . remove ( fetched_pr ) for pr in pulls : if not pr . get ( 'merged_at' ) : pulls . remove ( pr ) return pulls
5012	def get_inactive_sap_learners ( self ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) sap_search_student_url = '{sapsf_base_url}/{search_students_path}?$filter={search_filter}' . format ( sapsf_base_url = self . enterprise_configuration . sapsf_base_url . rstrip ( '/' ) , search_students_path = self . global_sap_config . search_student_api_path . rstrip ( '/' ) , search_filter = 'criteria/isActive eq False&$select=studentID' , ) all_inactive_learners = self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners = [ ] , page_size = 500 , start_at = 0 ) return all_inactive_learners
4951	def get_required_query_params ( self , request ) : username = get_request_value ( request , self . REQUIRED_PARAM_USERNAME , '' ) course_id = get_request_value ( request , self . REQUIRED_PARAM_COURSE_ID , '' ) program_uuid = get_request_value ( request , self . REQUIRED_PARAM_PROGRAM_UUID , '' ) enterprise_customer_uuid = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER ) if not ( username and ( course_id or program_uuid ) and enterprise_customer_uuid ) : raise ConsentAPIRequestError ( self . get_missing_params_message ( [ ( "'username'" , bool ( username ) ) , ( "'enterprise_customer_uuid'" , bool ( enterprise_customer_uuid ) ) , ( "one of 'course_id' or 'program_uuid'" , bool ( course_id or program_uuid ) ) , ] ) ) return username , course_id , program_uuid , enterprise_customer_uuid
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
3805	def calculate_P ( self , T , P , method ) : r if method == ELI_HANLEY_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley_dense ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm , Vmg ) elif method == CHUNG_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T , P ) if hasattr ( self . mug , '__call__' ) else self . mug kg = chung_dense ( T , self . MW , self . Tc , self . Vc , self . omega , Cvgm , Vmg , mug , self . dipole ) elif method == STIEL_THODOS_DENSE : kg = self . T_dependent_property ( T ) Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg kg = stiel_thodos_dense ( T , self . MW , self . Tc , self . Pc , self . Vc , self . Zc , Vmg , kg ) elif method == COOLPROP : kg = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kg = self . interpolate_P ( T , P , method ) return kg
7417	def update ( assembly , idict , count ) : data = iter ( open ( os . path . join ( assembly . dirs . outfiles , assembly . name + ".phy" ) , 'r' ) ) ntax , nchar = data . next ( ) . strip ( ) . split ( ) for line in data : tax , seq = line . strip ( ) . split ( ) idict [ tax ] = idict [ tax ] [ 100000 : ] idict [ tax ] += seq [ count : count + 100000 ] del line return idict
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
11275	def check_pid ( pid , debug ) : try : os . kill ( pid , 0 ) if debug > 1 : print ( "Script has a PIDFILE where the process is still running" ) return True except OSError : if debug > 1 : print ( "Script does not appear to be running" ) return False
11551	def get_exception_from_status_and_error_codes ( status_code , error_code , value ) : if status_code == requests . codes . bad_request : exception = BadRequest ( value ) elif status_code == requests . codes . unauthorized : exception = Unauthorized ( value ) elif status_code == requests . codes . forbidden : exception = Unauthorized ( value ) elif status_code in [ requests . codes . not_found , requests . codes . gone ] : exception = NotFound ( value ) elif status_code == requests . codes . method_not_allowed : exception = MethodNotAllowed ( value ) elif status_code >= requests . codes . bad_request : exception = HTTPError ( value ) else : exception = ResponseError ( value ) if error_code == - 100 : exception = InternalError ( value ) elif error_code == - 101 : exception = InvalidToken ( value ) elif error_code == - 105 : exception = UploadFailed ( value ) elif error_code == - 140 : exception = UploadTokenGenerationFailed ( value ) elif error_code == - 141 : exception = InvalidUploadToken ( value ) elif error_code == - 150 : exception = InvalidParameter ( value ) elif error_code == - 151 : exception = InvalidPolicy ( value ) return exception
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
13178	def cache_func ( prefix , method = False ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : cache_args = args if method : cache_args = args [ 1 : ] cache_key = get_cache_key ( prefix , * cache_args , ** kwargs ) cached_value = cache . get ( cache_key ) if cached_value is None : cached_value = func ( * args , ** kwargs ) cache . set ( cache_key , cached_value ) return cached_value return wrapper return decorator
10628	def T ( self , T ) : self . _T = T self . _Hfr = self . _calculate_Hfr ( T )
7365	def run_command ( args , ** kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args , ** kwargs ) process . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "%s took %0.4f seconds" , args [ 0 ] , elapsed_time )
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
13694	def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
4246	def id_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . id_by_addr ( addr )
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
1835	def JRCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . RCX == 0 , target . read ( ) , cpu . PC )
4914	def course_enrollments ( self , request , pk ) : enterprise_customer = self . get_object ( ) serializer = serializers . EnterpriseCustomerCourseEnrollmentsSerializer ( data = request . data , many = True , context = { 'enterprise_customer' : enterprise_customer , 'request_user' : request . user , } ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data , status = HTTP_200_OK ) return Response ( serializer . errors , status = HTTP_400_BAD_REQUEST )
9525	def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
144	def copy ( self , exterior = None , label = None ) : return self . deepcopy ( exterior = exterior , label = label )
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
2893	def get_outgoing_sequence_names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing_sequence_flows_by_id . values ( ) ) ] )
6496	def _get_mappings ( self , doc_type ) : mapping = ElasticSearchEngine . get_mappings ( self . index_name , doc_type ) if not mapping : mapping = self . _es . indices . get_mapping ( index = self . index_name , doc_type = doc_type , ) . get ( self . index_name , { } ) . get ( 'mappings' , { } ) . get ( doc_type , { } ) if mapping : ElasticSearchEngine . set_mappings ( self . index_name , doc_type , mapping ) return mapping
328	def extract_interesting_date_ranges ( returns ) : returns_dupe = returns . copy ( ) returns_dupe . index = returns_dupe . index . map ( pd . Timestamp ) ranges = OrderedDict ( ) for name , ( start , end ) in PERIODS . items ( ) : try : period = returns_dupe . loc [ start : end ] if len ( period ) == 0 : continue ranges [ name ] = period except BaseException : continue return ranges
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
3644	def watchlistDelete ( self , trade_id ) : method = 'DELETE' url = 'watchlist' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeId' : ',' . join ( trade_id ) } self . __request__ ( method , url , params = params ) return True
5603	def read_raster_window ( input_files , tile , indexes = None , resampling = "nearest" , src_nodata = None , dst_nodata = None , gdal_opts = None ) : with rasterio . Env ( ** get_gdal_options ( gdal_opts , is_remote = path_is_remote ( input_files [ 0 ] if isinstance ( input_files , list ) else input_files , s3 = True ) ) ) as env : logger . debug ( "reading %s with GDAL options %s" , input_files , env . options ) return _read_raster_window ( input_files , tile , indexes = indexes , resampling = resampling , src_nodata = src_nodata , dst_nodata = dst_nodata )
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , ** kwargs )
3998	def copy_from_local ( local_path , remote_name , remote_path , demote = True ) : if not os . path . exists ( local_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist' . format ( local_path ) ) temp_identifier = str ( uuid . uuid1 ( ) ) if os . path . isdir ( local_path ) : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_dir_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path ) else : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_file_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : depth } return modeldict
9401	def _parse_error ( self , err ) : self . logger . debug ( err ) stack = err . get ( 'stack' , [ ] ) if not err [ 'message' ] . startswith ( 'parse error:' ) : err [ 'message' ] = 'error: ' + err [ 'message' ] errmsg = 'Octave evaluation error:\n%s' % err [ 'message' ] if not isinstance ( stack , StructArray ) : return errmsg errmsg += '\nerror: called from:' for item in stack [ : - 1 ] : errmsg += '\n %(name)s at line %(line)d' % item try : errmsg += ', column %(column)d' % item except Exception : pass return errmsg
8021	async def websocket_close ( self , message , stream_name ) : if stream_name in self . applications_accepting_frames : self . applications_accepting_frames . remove ( stream_name ) if self . closing : return if not self . applications_accepting_frames : await self . close ( message . get ( "code" ) )
2871	def remove_event_detect ( self , pin ) : self . mraa_gpio . Gpio . isrExit ( self . mraa_gpio . Gpio ( pin ) )
11921	def paginator ( self ) : if not hasattr ( self , '_paginator' ) : if self . pagination_class is None : self . _paginator = None else : self . _paginator = self . pagination_class ( ) return self . _paginator
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] self [ "expiration" ] = None dict . __init__ ( self , { } )
3672	def bubble_at_P ( P , zs , vapor_pressure_eqns , fugacities = None , gammas = None ) : def bubble_P_error ( T ) : Psats = [ VP ( T ) for VP in vapor_pressure_eqns ] Pcalc = bubble_at_T ( zs , Psats , fugacities , gammas ) return P - Pcalc T_bubble = newton ( bubble_P_error , 300 ) return T_bubble
9634	def numeric ( _ , n ) : try : nt = n . as_tuple ( ) except AttributeError : raise TypeError ( 'numeric field requires Decimal value (got %r)' % n ) digits = [ ] if isinstance ( nt . exponent , str ) : ndigits = 0 weight = 0 sign = 0xC000 dscale = 0 else : decdigits = list ( reversed ( nt . digits + ( nt . exponent % 4 ) * ( 0 , ) ) ) weight = 0 while decdigits : if any ( decdigits [ : 4 ] ) : break weight += 1 del decdigits [ : 4 ] while decdigits : digits . insert ( 0 , ndig ( decdigits [ : 4 ] ) ) del decdigits [ : 4 ] ndigits = len ( digits ) weight += nt . exponent // 4 + ndigits - 1 sign = nt . sign * 0x4000 dscale = - min ( 0 , nt . exponent ) data = [ ndigits , weight , sign , dscale ] + digits return ( 'ihhHH%dH' % ndigits , [ 2 * len ( data ) ] + data )
7064	def sqs_delete_item ( queue_url , receipt_handle , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_message ( QueueUrl = queue_url , ReceiptHandle = receipt_handle ) except Exception as e : LOGEXCEPTION ( 'could not delete message with receipt handle: ' '%s from queue: %s' % ( receipt_handle , queue_url ) ) if raiseonfail : raise
807	def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None
12994	def table ( cluster ) : teffs = teff ( cluster ) lums = luminosity ( cluster ) arr = cluster . to_array ( ) i = 0 for row in arr : row [ 'lum' ] [ 0 ] = np . array ( [ lums [ i ] ] , dtype = 'f' ) row [ 'temp' ] [ 0 ] = np . array ( [ teffs [ i ] ] , dtype = 'f' ) i += 1 arr = round_arr_teff_luminosity ( arr ) return arr
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
11267	def walk ( prev , inital_path , * args , ** kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
6841	def supported_locales ( ) : family = distrib_family ( ) if family == 'debian' : return _parse_locales ( '/usr/share/i18n/SUPPORTED' ) elif family == 'arch' : return _parse_locales ( '/etc/locale.gen' ) elif family == 'redhat' : return _supported_locales_redhat ( ) else : raise UnsupportedFamily ( supported = [ 'debian' , 'arch' , 'redhat' ] )
12323	def api_call_action ( func ) : def _inner ( * args , ** kwargs ) : return func ( * args , ** kwargs ) _inner . __name__ = func . __name__ _inner . __doc__ = func . __doc__ return _inner
4238	def get_traffic_meter ( self ) : _LOGGER . info ( "Get traffic meter" ) def parse_text ( text ) : def tofloats ( lst ) : return ( float ( t ) for t in lst ) try : if "/" in text : return tuple ( tofloats ( text . split ( '/' ) ) ) elif ":" in text : hour , mins = tofloats ( text . split ( ':' ) ) return timedelta ( hours = hour , minutes = mins ) else : return float ( text ) except ValueError : return None success , response = self . _make_request ( SERVICE_DEVICE_CONFIG , "GetTrafficMeterStatistics" ) if not success : return None success , node = _find_node ( response . text , ".//GetTrafficMeterStatisticsResponse" ) if not success : return None return { t . tag : parse_text ( t . text ) for t in node }
12552	def write_mhd_file ( filename , data , shape = None , meta_dict = None ) : ext = get_extension ( filename ) fname = op . basename ( filename ) if ext != '.mhd' or ext != '.raw' : mhd_filename = fname + '.mhd' raw_filename = fname + '.raw' elif ext == '.mhd' : mhd_filename = fname raw_filename = remove_ext ( fname ) + '.raw' elif ext == '.raw' : mhd_filename = remove_ext ( fname ) + '.mhd' raw_filename = fname else : raise ValueError ( '`filename` extension {} from {} is not recognised. ' 'Expected .mhd or .raw.' . format ( ext , filename ) ) if meta_dict is None : meta_dict = { } if shape is None : shape = data . shape meta_dict [ 'ObjectType' ] = meta_dict . get ( 'ObjectType' , 'Image' ) meta_dict [ 'BinaryData' ] = meta_dict . get ( 'BinaryData' , 'True' ) meta_dict [ 'BinaryDataByteOrderMSB' ] = meta_dict . get ( 'BinaryDataByteOrderMSB' , 'False' ) meta_dict [ 'ElementType' ] = meta_dict . get ( 'ElementType' , NUMPY_TO_MHD_TYPE [ data . dtype . type ] ) meta_dict [ 'NDims' ] = meta_dict . get ( 'NDims' , str ( len ( shape ) ) ) meta_dict [ 'DimSize' ] = meta_dict . get ( 'DimSize' , ' ' . join ( [ str ( i ) for i in shape ] ) ) meta_dict [ 'ElementDataFile' ] = meta_dict . get ( 'ElementDataFile' , raw_filename ) mhd_filename = op . join ( op . dirname ( filename ) , mhd_filename ) raw_filename = op . join ( op . dirname ( filename ) , raw_filename ) write_meta_header ( mhd_filename , meta_dict ) dump_raw_data ( raw_filename , data ) return mhd_filename , raw_filename
11821	def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
5729	def main ( verbose = True ) : find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) gdbmi = GdbController ( verbose = verbose ) responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) responses = gdbmi . write ( "-file-list-exec-source-files" ) responses = gdbmi . write ( "-break-insert main" ) responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) gdbmi . exit ( )
3392	def prune_unused_reactions ( cobra_model ) : output_model = cobra_model . copy ( ) reactions_to_prune = [ r for r in output_model . reactions if len ( r . metabolites ) == 0 ] output_model . remove_reactions ( reactions_to_prune ) return output_model , reactions_to_prune
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
10657	def amounts ( masses ) : return { compound : amount ( compound , masses [ compound ] ) for compound in masses . keys ( ) }
6626	def availableBranches ( self ) : return [ GithubComponentVersion ( '' , b [ 0 ] , b [ 1 ] , self . name , cache_key = None ) for b in _getBranchHeads ( self . repo ) . items ( ) ]
6769	def install_apt ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : r = self . local_renderer assert self . genv [ ROLE ] apt_req_fqfn = fn or ( self . env . apt_requirments_fn and self . find_template ( self . env . apt_requirments_fn ) ) if not apt_req_fqfn : return [ ] assert os . path . isfile ( apt_req_fqfn ) lines = list ( self . env . apt_packages or [ ] ) for _ in open ( apt_req_fqfn ) . readlines ( ) : if _ . strip ( ) and not _ . strip ( ) . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) : lines . extend ( _pkg . strip ( ) for _pkg in _ . split ( ' ' ) if _pkg . strip ( ) ) if list_only : return lines tmp_fn = r . write_temp_file ( '\n' . join ( lines ) ) apt_req_fqfn = tmp_fn if not self . genv . is_local : r . put ( local_path = tmp_fn , remote_path = tmp_fn ) apt_req_fqfn = self . genv . put_remote_path r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt_req_fqfn )
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
6723	def get_or_create_ec2_key_pair ( name = None , verbose = 1 ) : verbose = int ( verbose ) name = name or env . vm_ec2_keypair_name pem_path = 'roles/%s/%s.pem' % ( env . ROLE , name ) conn = get_ec2_connection ( ) kp = conn . get_key_pair ( name ) if kp : print ( 'Key pair %s already exists.' % name ) else : kp = conn . create_key_pair ( name ) open ( pem_path , 'wb' ) . write ( kp . material ) os . system ( 'chmod 600 %s' % pem_path ) print ( 'Key pair %s created.' % name ) return pem_path
7025	def make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = False , wrap = False , model_over_lc = False ) : plt . close ( 'all' ) plt . figure ( figsize = ( 8 , 4.8 ) ) if model_over_lc : model_z = 100 lc_z = 0 else : model_z = 0 lc_z = 100 if not wrap : plt . plot ( phase , fitmags , linewidth = 3.0 , color = 'red' , zorder = model_z ) plt . plot ( phase , pmags , marker = 'o' , markersize = 1.0 , linestyle = 'none' , rasterized = True , color = 'k' , zorder = lc_z ) plt . gca ( ) . set_xticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) else : plt . plot ( np . concatenate ( [ phase - 1.0 , phase ] ) , np . concatenate ( [ fitmags , fitmags ] ) , linewidth = 3.0 , color = 'red' , zorder = model_z ) plt . plot ( np . concatenate ( [ phase - 1.0 , phase ] ) , np . concatenate ( [ pmags , pmags ] ) , marker = 'o' , markersize = 1.0 , linestyle = 'none' , rasterized = True , color = 'k' , zorder = lc_z ) plt . gca ( ) . set_xlim ( ( - 0.8 , 0.8 ) ) plt . gca ( ) . set_xticks ( [ - 0.8 , - 0.7 , - 0.6 , - 0.5 , - 0.4 , - 0.3 , - 0.2 , - 0.1 , 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 ] ) ymin , ymax = plt . ylim ( ) if not magsarefluxes : plt . gca ( ) . invert_yaxis ( ) plt . ylabel ( 'magnitude' ) else : plt . ylabel ( 'flux' ) plt . xlabel ( 'phase' ) plt . title ( 'period: %.6f, folded at %.6f, fit epoch: %.6f' % ( period , mintime , magseriesepoch ) ) plt . savefig ( plotfit ) plt . close ( )
9375	def get_run_time_period ( run_steps ) : init_ts_start = get_standardized_timestamp ( 'now' , None ) ts_start = init_ts_start ts_end = '0' for run_step in run_steps : if run_step . ts_start and run_step . ts_end : if run_step . ts_start < ts_start : ts_start = run_step . ts_start if run_step . ts_end > ts_end : ts_end = run_step . ts_end if ts_end == '0' : ts_end = None if ts_start == init_ts_start : ts_start = None logger . info ( 'get_run_time_period range returned ' + str ( ts_start ) + ' to ' + str ( ts_end ) ) return ts_start , ts_end
13909	def show_version ( self ) : class ShowVersionAction ( argparse . Action ) : def __init__ ( inner_self , nargs = 0 , ** kw ) : super ( ShowVersionAction , inner_self ) . __init__ ( nargs = nargs , ** kw ) def __call__ ( inner_self , parser , args , value , option_string = None ) : print ( "{parser_name} version: {version}" . format ( parser_name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog_version ) ) return ShowVersionAction
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
11029	def sse_content ( response , handler , ** sse_kwargs ) : raise_for_not_ok_status ( response ) raise_for_header ( response , 'Content-Type' , 'text/event-stream' ) finished , _ = _sse_content_with_protocol ( response , handler , ** sse_kwargs ) return finished
10352	def make_pubmed_gene_group ( entrez_ids : Iterable [ Union [ str , int ] ] ) -> Iterable [ str ] : url = PUBMED_GENE_QUERY_URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez_ids ) ) response = requests . get ( url ) tree = ElementTree . fromstring ( response . content ) for x in tree . findall ( './DocumentSummarySet/DocumentSummary' ) : yield '\n# {}' . format ( x . find ( 'Description' ) . text ) yield 'SET Citation = {{"Other", "PubMed Gene", "{}"}}' . format ( x . attrib [ 'uid' ] ) yield 'SET Evidence = "{}"' . format ( x . find ( 'Summary' ) . text . strip ( ) . replace ( '\n' , '' ) ) yield '\nUNSET Evidence\nUNSET Citation'
12550	def write_meta_header ( filename , meta_dict ) : header = '' for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
12185	def extract ( self , document , selector , debug_offset = '' ) : selected = self . select ( document , selector ) if selected is not None : if isinstance ( selected , ( list , tuple ) ) : if not len ( selected ) : return return [ self . _extract_single ( m ) for m in selected ] else : return self . _extract_single ( selected ) else : if self . DEBUG : print ( debug_offset , "selector did not match anything; return None" ) return None
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
8542	def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( "credentials" , "username" , fallback = None ) if not username : username = input ( "Please enter your username: " ) . strip ( ) while not username : username = input ( "No username specified. Please enter your username: " ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( "credentials" , "username" , username ) self . _save_config ( ) return username
6725	def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
689	def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
7682	def display ( annotation , meta = True , ** kwargs ) : for namespace , func in six . iteritems ( VIZ_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) axes = func ( ann , ** kwargs ) axes . set_title ( annotation . namespace ) if meta : description = pprint_jobject ( annotation . annotation_metadata , indent = 2 ) anchored_box = AnchoredText ( description . strip ( '\n' ) , loc = 2 , frameon = True , bbox_to_anchor = ( 1.02 , 1.0 ) , bbox_transform = axes . transAxes , borderpad = 0.0 ) axes . add_artist ( anchored_box ) axes . figure . subplots_adjust ( right = 0.8 ) return axes except NamespaceError : pass raise NamespaceError ( 'Unable to visualize annotation of namespace="{:s}"' . format ( annotation . namespace ) )
4262	def save_cache ( gallery ) : if hasattr ( gallery , "exifCache" ) : cache = gallery . exifCache else : cache = gallery . exifCache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) if len ( cache ) == 0 : if os . path . exists ( cachePath ) : os . remove ( cachePath ) return try : with open ( cachePath , "wb" ) as cacheFile : pickle . dump ( cache , cacheFile ) logger . debug ( "Stored cache with %d entries" , len ( gallery . exifCache ) ) except Exception as e : logger . warn ( "Could not store cache: %s" , e ) os . remove ( cachePath )
5820	def _cached_path_needs_update ( ca_path , cache_length ) : exists = os . path . exists ( ca_path ) if not exists : return True stats = os . stat ( ca_path ) if stats . st_mtime < time . time ( ) - cache_length * 60 * 60 : return True if stats . st_size == 0 : return True return False
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
3501	def assess_precursors ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff , solver )
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : if callback : callback ( item ) def error_cb ( error_stanza ) : if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
7113	def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator . predict ( x ) y = [ item [ 0 ] for item in y ] y = [ self . _remove_prefix ( label ) for label in y ] if not isinstance ( X , list ) : y = y [ 0 ] return y
151	def from_shapely ( geometry , label = None ) : import shapely . geometry if isinstance ( geometry , shapely . geometry . MultiPolygon ) : return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) elif isinstance ( geometry , shapely . geometry . Polygon ) : return MultiPolygon ( [ Polygon . from_shapely ( geometry , label = label ) ] ) elif isinstance ( geometry , shapely . geometry . collection . GeometryCollection ) : ia . do_assert ( all ( [ isinstance ( poly , shapely . geometry . Polygon ) for poly in geometry . geoms ] ) ) return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) else : raise Exception ( "Unknown datatype '%s'. Expected shapely.geometry.Polygon or " "shapely.geometry.MultiPolygon or " "shapely.geometry.collections.GeometryCollection." % ( type ( geometry ) , ) )
2608	def _nbytes ( buf ) : if isinstance ( buf , memoryview ) : if PY3 : return buf . nbytes else : size = buf . itemsize for dim in buf . shape : size *= dim return size else : return len ( buf )
30	def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables )
8586	def get_attached_cdrom ( self , datacenter_id , server_id , cdrom_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms/%s' % ( datacenter_id , server_id , cdrom_id ) ) return response
12971	def getMultiple ( self , pks , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : return IRQueryableList ( [ self . get ( pks [ 0 ] , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hgetall ( key ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) i = 0 pksLen = len ( pks ) while i < pksLen : if res [ i ] is None : ret . append ( None ) i += 1 continue res [ i ] [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( res [ i ] ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : if not obj : continue self . _doCascadeFetch ( obj ) return ret
12620	def have_same_affine ( one_img , another_img , only_check_3d = False ) : img1 = check_img ( one_img ) img2 = check_img ( another_img ) ndim1 = len ( img1 . shape ) ndim2 = len ( img2 . shape ) if ndim1 < 3 : raise ValueError ( 'Image {} has only {} dimensions, at least 3 dimensions is expected.' . format ( repr_imgs ( img1 ) , ndim1 ) ) if ndim2 < 3 : raise ValueError ( 'Image {} has only {} dimensions, at least 3 dimensions is expected.' . format ( repr_imgs ( img2 ) , ndim1 ) ) affine1 = img1 . get_affine ( ) affine2 = img2 . get_affine ( ) if only_check_3d : affine1 = affine1 [ : 3 , : 3 ] affine2 = affine2 [ : 3 , : 3 ] try : return np . allclose ( affine1 , affine2 ) except ValueError : return False except : raise
1955	def empty_platform ( cls , arch ) : platform = cls ( None ) platform . _init_cpu ( arch ) platform . _init_std_fds ( ) return platform
3793	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . kappa , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . Tcs [ i ]
12954	def _rem_id_from_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_ids_key ( ) , pk )
185	def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )
13531	def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
5072	def get_configuration_value ( val_name , default = None , ** kwargs ) : if kwargs . get ( 'type' ) == 'url' : return get_url ( val_name ) or default if callable ( get_url ) else default return configuration_helpers . get_value ( val_name , default , ** kwargs ) if configuration_helpers else default
5213	def intraday ( ticker , dt , session = '' , ** kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( ** kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( ** kw ) ) return cur_data
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
10638	def get_element_mfr ( self , element ) : result = 0.0 for compound in self . material . compounds : formula = compound . split ( '[' ) [ 0 ] result += self . get_compound_mfr ( compound ) * stoich . element_mass_fraction ( formula , element ) return result
13795	def handle_reduce ( self , reduce_function_names , mapped_docs ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) keys , values = zip ( ( key , value ) for ( ( key , doc_id ) , value ) in mapped_docs ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( keys , values , rereduce = False ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
10064	def process_schema ( value ) : schemas = current_app . extensions [ 'invenio-jsonschemas' ] . schemas try : return schemas [ value ] except KeyError : raise click . BadParameter ( 'Unknown schema {0}. Please use one of:\n {1}' . format ( value , '\n' . join ( schemas . keys ( ) ) ) )
4170	def zpk2ss ( z , p , k ) : import scipy . signal return scipy . signal . zpk2ss ( z , p , k )
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero for what in ( "minimum" , "maximum" ) : if processes > 1 : chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
7006	def plot_training_results ( classifier , classlabels , outfile ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None confmatrix = clfdict [ 'best_confmatrix' ] overall_feature_importances = clfdict [ 'best_classifier' ] . feature_importances_ feature_importances_per_tree = np . array ( [ tree . feature_importances_ for tree in clfdict [ 'best_classifier' ] . estimators_ ] ) stdev_feature_importances = np . std ( feature_importances_per_tree , axis = 0 ) feature_names = np . array ( clfdict [ 'feature_names' ] ) plt . figure ( figsize = ( 6.4 * 3.0 , 4.8 ) ) plt . subplot ( 121 ) classes = np . array ( classlabels ) plt . imshow ( confmatrix , interpolation = 'nearest' , cmap = plt . cm . Blues ) tick_marks = np . arange ( len ( classes ) ) plt . xticks ( tick_marks , classes ) plt . yticks ( tick_marks , classes ) plt . title ( 'evaluation set confusion matrix' ) plt . ylabel ( 'predicted class' ) plt . xlabel ( 'actual class' ) thresh = confmatrix . max ( ) / 2. for i , j in itertools . product ( range ( confmatrix . shape [ 0 ] ) , range ( confmatrix . shape [ 1 ] ) ) : plt . text ( j , i , confmatrix [ i , j ] , horizontalalignment = "center" , color = "white" if confmatrix [ i , j ] > thresh else "black" ) plt . subplot ( 122 ) features = np . array ( feature_names ) sorted_ind = np . argsort ( overall_feature_importances ) [ : : - 1 ] features = features [ sorted_ind ] feature_names = feature_names [ sorted_ind ] overall_feature_importances = overall_feature_importances [ sorted_ind ] stdev_feature_importances = stdev_feature_importances [ sorted_ind ] plt . bar ( np . arange ( 0 , features . size ) , overall_feature_importances , yerr = stdev_feature_importances , width = 0.8 , color = 'grey' ) plt . xticks ( np . arange ( 0 , features . size ) , features , rotation = 90 ) plt . yticks ( [ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ] ) plt . xlim ( - 0.75 , features . size - 1.0 + 0.75 ) plt . ylim ( 0.0 , 0.9 ) plt . ylabel ( 'relative importance' ) plt . title ( 'relative importance of features' ) plt . subplots_adjust ( wspace = 0.1 ) plt . savefig ( outfile , bbox_inches = 'tight' , dpi = 100 ) plt . close ( 'all' ) return outfile
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
3838	async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
2812	def convert_squeeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting squeeze ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert squeeze by multiple dimensions' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) ) : import tensorflow as tf return tf . squeeze ( x , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
11527	def solr_advanced_search ( self , query , token = None , limit = 20 ) : parameters = dict ( ) parameters [ 'query' ] = query parameters [ 'limit' ] = limit if token : parameters [ 'token' ] = token response = self . request ( 'midas.solr.search.advanced' , parameters ) return response
7722	def set_password ( self , password ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "password" : child . unlinkNode ( ) child . freeNode ( ) break if password is not None : self . xmlnode . newTextChild ( self . xmlnode . ns ( ) , "password" , to_utf8 ( password ) )
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
1216	def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
7291	def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
7582	def _call_raxml ( command_list ) : proc = subprocess . Popen ( command_list , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) comm = proc . communicate ( ) return comm
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
3153	def all ( self , list_id , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , ** queryparams )
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
1733	def is_object ( n , last ) : if is_empty_object ( n , last ) : return True if not n . strip ( ) : return False if len ( argsplit ( n , ';' ) ) > 1 : return False cands = argsplit ( n , ',' ) if not cands [ - 1 ] . strip ( ) : return True for cand in cands : cand = cand . strip ( ) kv = argsplit ( cand , ':' ) if len ( kv ) > 2 : kv = kv [ 0 ] , ':' . join ( kv [ 1 : ] ) if len ( kv ) == 2 : k , v = kv if not is_lval ( k . strip ( ) ) : return False v = v . strip ( ) if v . startswith ( 'function' ) : continue if v [ 0 ] == '{' : return False for e in KEYWORD_METHODS : if v . startswith ( e ) and len ( e ) < len ( v ) and v [ len ( e ) ] not in IDENTIFIER_PART : return False elif not ( cand . startswith ( 'set ' ) or cand . startswith ( 'get ' ) ) : return False return True
7273	def set_rate ( self , rate ) : self . _rate = self . _player_interface_property ( 'Rate' , dbus . Double ( rate ) ) return self . _rate
8755	def run ( ) : groups_client = sg_cli . SecurityGroupsClient ( ) xapi_client = xapi . XapiClient ( ) interfaces = set ( ) while True : try : interfaces = xapi_client . get_interfaces ( ) except Exception : LOG . exception ( "Unable to get instances/interfaces from xapi" ) _sleep ( ) continue try : sg_states = groups_client . get_security_group_states ( interfaces ) new_sg , updated_sg , removed_sg = partition_vifs ( xapi_client , interfaces , sg_states ) xapi_client . update_interfaces ( new_sg , updated_sg , removed_sg ) groups_to_ack = [ v for v in new_sg + updated_sg if v . success ] sg_sts_curr = groups_client . get_security_group_states ( interfaces ) groups_to_ack = get_groups_to_ack ( groups_to_ack , sg_states , sg_sts_curr ) ack_groups ( groups_client , groups_to_ack ) except Exception : LOG . exception ( "Unable to get security groups from registry and " "apply them to xapi" ) _sleep ( ) continue _sleep ( )
5268	def _check_input ( self , input ) : if isinstance ( input , str ) : return 'st' elif isinstance ( input , list ) : if all ( isinstance ( item , str ) for item in input ) : return 'gst' raise ValueError ( "String argument should be of type String or" " a list of strings" )
6281	def keyboard_event ( self , key , action , modifier ) : if key == self . keys . ESCAPE : self . close ( ) return if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) self . timeline . key_event ( key , action , modifier )
7431	def _count_PIS ( seqsamp , N ) : counts = [ Counter ( col ) for col in seqsamp . T if not ( "-" in col or "N" in col ) ] pis = [ i . most_common ( 2 ) [ 1 ] [ 1 ] > 1 for i in counts if len ( i . most_common ( 2 ) ) > 1 ] if sum ( pis ) >= N : return sum ( pis ) else : return 0
12653	def call_dcm2nii ( work_dir , arguments = '' ) : if not op . exists ( work_dir ) : raise IOError ( 'Folder {} not found.' . format ( work_dir ) ) cmd_line = 'dcm2nii {0} "{1}"' . format ( arguments , work_dir ) log . info ( cmd_line ) return subprocess . check_call ( cmd_line , shell = True )
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
10929	def do_internal_run ( self , initial_count = 0 , subblock = None , update_derr = True ) : self . _inner_run_counter = initial_count good_step = True n_good_steps = 0 CLOG . debug ( 'Running...' ) _last_residuals = self . calc_residuals ( ) . copy ( ) while ( ( self . _inner_run_counter < self . run_length ) & good_step & ( not self . check_terminate ( ) ) ) : if self . check_Broyden_J ( ) and self . _inner_run_counter != 0 : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) and self . _inner_run_counter != 0 : self . update_eig_J ( ) er0 = 1 * self . error delta_vals = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False , subblock = subblock ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = er1 < er0 if good_step : n_good_steps += 1 CLOG . debug ( '%f\t%f' % ( er0 , er1 ) ) self . update_param_vals ( delta_vals , incremental = True ) self . _last_residuals = _last_residuals . copy ( ) if update_derr : self . _last_error = er0 self . error = er1 _last_residuals = self . calc_residuals ( ) . copy ( ) else : er0_0 = self . update_function ( self . param_vals ) CLOG . debug ( 'Bad step!' ) if np . abs ( er0 - er0_0 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . _inner_run_counter += 1 return n_good_steps
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
7795	def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : return cache . set_fetcher ( None ) finally : self . _lock . release ( )
1343	def onehot_like ( a , index , value = 1 ) : x = np . zeros_like ( a ) x [ index ] = value return x
9730	def get_force ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlate , data , component_position ) force_list = [ ] for _ in range ( plate . force_count ) : component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) force_list . append ( force ) append_components ( ( plate , force_list ) ) return components
12209	def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
9515	def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
5841	def submit_design_run ( self , data_view_id , num_candidates , effort , target = None , constraints = [ ] , sampler = "Default" ) : if effort > 30 : raise CitrinationClientError ( "Parameter effort must be less than 30 to trigger a design run" ) if target is not None : target = target . to_dict ( ) constraint_dicts = [ c . to_dict ( ) for c in constraints ] body = { "num_candidates" : num_candidates , "target" : target , "effort" : effort , "constraints" : constraint_dicts , "sampler" : sampler } url = routes . submit_data_view_design ( data_view_id ) response = self . _post_json ( url , body ) . json ( ) return DesignRun ( response [ "data" ] [ "design_run" ] [ "uid" ] )
12736	def are_connected ( self , body_a , body_b ) : return bool ( ode . areConnected ( self . get_body ( body_a ) . ode_body , self . get_body ( body_b ) . ode_body ) )
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
3041	def access_token_expired ( self ) : if self . invalid : return True if not self . token_expiry : return False now = _UTCNOW ( ) if now >= self . token_expiry : logger . info ( 'access_token is expired. Now: %s, token_expiry: %s' , now , self . token_expiry ) return True return False
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
1895	def _recv ( self ) -> str : buf , left , right = self . __readline_and_count ( ) bufl = [ buf ] while left != right : buf , l , r = self . __readline_and_count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
8724	def from_timestamp ( ts ) : return datetime . datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = pytz . utc )
10626	def _calculate_T ( self , Hfr ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_Hfr ( x [ 0 ] ) - Hfr ) y . append ( self . _calculate_Hfr ( x [ 1 ] ) - Hfr ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_Hfr ( x [ i ] ) - Hfr ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
1287	def process_docstring ( app , what , name , obj , options , lines ) : markdown = "\n" . join ( lines ) rest = m2r ( markdown ) rest . replace ( "\r\n" , "\n" ) del lines [ : ] lines . extend ( rest . split ( "\n" ) )
2561	def recv_result_from_workers ( self ) : info = MPI . Status ( ) result = self . comm . recv ( source = MPI . ANY_SOURCE , tag = RESULT_TAG , status = info ) logger . debug ( "Received result from workers: {}" . format ( result ) ) return result
10092	def _parse_response ( self , response ) : if not self . _raise_errors : return response is_4xx_error = str ( response . status_code ) [ 0 ] == '4' is_5xx_error = str ( response . status_code ) [ 0 ] == '5' content = response . content if response . status_code == 403 : raise AuthenticationError ( content ) elif is_4xx_error : raise APIError ( content ) elif is_5xx_error : raise ServerError ( content ) return response
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
9902	def with_data ( path , data ) : if isinstance ( data , str ) : data = json . loads ( data ) if os . path . exists ( path ) : raise ValueError ( "File exists, not overwriting data. Set the " "'data' attribute on a normally-initialized " "'livejson.File' instance if you really " "want to do this." ) else : f = File ( path ) f . data = data return f
7188	def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_input , syms . suite ) _offset = 0 prefix = '' for _offset , child in enumerate ( body . children ) : if child . type == syms . simple_stmt : stmt = child . children [ 0 ] if stmt . type == syms . expr_stmt : expr = stmt . children if not skip_assignments : break if ( len ( expr ) != 2 or expr [ 0 ] . type != token . NAME or expr [ 1 ] . type != syms . annassign or _eq in expr [ 1 ] . children ) : break elif stmt . type not in ( syms . import_name , syms . import_from , token . STRING ) : break elif child . type == token . INDENT : assert isinstance ( child , Leaf ) prefix = child . value elif child . type != token . NEWLINE : break prefix , child . prefix = child . prefix , prefix return _offset , prefix
9797	def delete ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not click . confirm ( "Are sure you want to delete experiment group `{}`" . format ( _group ) ) : click . echo ( 'Existing without deleting experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . delete_experiment_group ( user , project_name , _group ) GroupManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment group `{}` was delete successfully" . format ( _group ) )
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
11775	def EnsembleLearner ( learners ) : def train ( dataset ) : predictors = [ learner ( dataset ) for learner in learners ] def predict ( example ) : return mode ( predictor ( example ) for predictor in predictors ) return predict return train
1006	def _learnBacktrack ( self ) : numPrevPatterns = len ( self . _prevLrnPatterns ) - 1 if numPrevPatterns <= 0 : if self . verbosity >= 3 : print "lrnBacktrack: No available history to backtrack from" return False badPatterns = [ ] inSequence = False for startOffset in range ( 0 , numPrevPatterns ) : inSequence = self . _learnBacktrackFrom ( startOffset , readOnly = True ) if inSequence : break badPatterns . append ( startOffset ) if not inSequence : if self . verbosity >= 3 : print ( "Failed to lock on. Falling back to start cells on current " "time step." ) self . _prevLrnPatterns = [ ] return False if self . verbosity >= 3 : print ( "Discovered path to current input by using start cells from %d " "steps ago:" % ( numPrevPatterns - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) self . _learnBacktrackFrom ( startOffset , readOnly = False ) for i in range ( numPrevPatterns ) : if i in badPatterns or i <= startOffset : if self . verbosity >= 3 : print ( "Removing useless pattern from history:" , self . _prevLrnPatterns [ 0 ] ) self . _prevLrnPatterns . pop ( 0 ) else : break return numPrevPatterns - startOffset
10959	def set_mem_level ( self , mem_level = 'hi' ) : key = '' . join ( [ c if c in 'mlh' else '' for c in mem_level ] ) if key not in [ 'h' , 'mh' , 'm' , 'ml' , 'm' , 'l' ] : raise ValueError ( 'mem_level must be one of hi, med-hi, med, med-lo, lo.' ) mem_levels = { 'h' : [ np . float64 , np . float64 ] , 'mh' : [ np . float64 , np . float32 ] , 'm' : [ np . float32 , np . float32 ] , 'ml' : [ np . float32 , np . float16 ] , 'l' : [ np . float16 , np . float16 ] } hi_lvl , lo_lvl = mem_levels [ key ] cat_lvls = { 'obj' : lo_lvl , 'ilm' : hi_lvl , 'bkg' : lo_lvl } self . image . float_precision = hi_lvl self . image . image = self . image . image . astype ( lo_lvl ) self . set_image ( self . image ) for cat in cat_lvls . keys ( ) : obj = self . get ( cat ) if hasattr ( obj , 'comps' ) : for c in obj . comps : c . float_precision = lo_lvl else : obj . float_precision = lo_lvl self . _model = self . _model . astype ( hi_lvl ) self . _residuals = self . _model . astype ( hi_lvl ) self . reset ( )
10120	def rectangle ( cls , vertices , ** kwargs ) : bottom_left , top_right = vertices top_left = [ bottom_left [ 0 ] , top_right [ 1 ] ] bottom_right = [ top_right [ 0 ] , bottom_left [ 1 ] ] return cls ( [ bottom_left , bottom_right , top_right , top_left ] , ** kwargs )
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
254	def add_closing_transactions ( positions , transactions ) : closed_txns = transactions [ [ 'symbol' , 'amount' , 'price' ] ] pos_at_end = positions . drop ( 'cash' , axis = 1 ) . iloc [ - 1 ] open_pos = pos_at_end . replace ( 0 , np . nan ) . dropna ( ) end_dt = open_pos . name + pd . Timedelta ( seconds = 1 ) for sym , ending_val in open_pos . iteritems ( ) : txn_sym = transactions [ transactions . symbol == sym ] ending_amount = txn_sym . amount . sum ( ) ending_price = ending_val / ending_amount closing_txn = { 'symbol' : sym , 'amount' : - ending_amount , 'price' : ending_price } closing_txn = pd . DataFrame ( closing_txn , index = [ end_dt ] ) closed_txns = closed_txns . append ( closing_txn ) closed_txns = closed_txns [ closed_txns . amount != 0 ] return closed_txns
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value
5597	def is_on_edge ( self ) : return ( self . left <= self . tile_pyramid . left or self . bottom <= self . tile_pyramid . bottom or self . right >= self . tile_pyramid . right or self . top >= self . tile_pyramid . top )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
1617	def _ShouldPrintError ( category , confidence , linenum ) : if IsErrorSuppressedByNolint ( category , linenum ) : return False if confidence < _cpplint_state . verbose_level : return False is_filtered = False for one_filter in _Filters ( ) : if one_filter . startswith ( '-' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = True elif one_filter . startswith ( '+' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = False else : assert False if is_filtered : return False return True
6804	def init_raspbian_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) device_question = 'SD card present at %s? ' % self . env . sd_device if not yes and not raw_input ( device_question ) . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local_if_missing ( fn = '{raspbian_image_zip}' , cmd = 'wget {raspbian_download_url} -O raspbian_lite_latest.zip' ) r . lenv . img_fn = r . local ( "unzip -l {raspbian_image_zip} | sed -n 4p | awk '{{print $4}}'" , capture = True ) or '$IMG_FN' r . local ( 'echo {img_fn}' ) r . local ( '[ ! -f {img_fn} ] && unzip {raspbian_image_zip} {img_fn} || true' ) r . lenv . img_fn = r . local ( 'readlink -f {img_fn}' , capture = True ) r . local ( 'echo {img_fn}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2} || true' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'time dd bs=4M if={img_fn} of={sd_device}' ) r . run ( 'sync' )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
10999	def psf_slice ( self , zint , size = 11 , zoffset = 0. , getextent = False ) : zint = max ( self . _p2k ( self . _tz ( zint ) ) , 0 ) offset = np . array ( [ zoffset * ( zint > 0 ) , 0 , 0 ] ) scale = [ self . param_dict [ self . zscale ] , 1.0 , 1.0 ] tile = util . Tile ( left = 0 , size = size , centered = True ) vecs = tile . coords ( form = 'flat' ) vecs = [ self . _p2k ( s * i + o ) for i , s , o in zip ( vecs , scale , offset ) ] psf = self . psffunc ( * vecs [ : : - 1 ] , zint = zint , ** self . pack_args ( ) ) . T vec = tile . coords ( form = 'meshed' ) if self . cutoffval is not None and not self . cutbyval : edge = psf > psf . max ( ) * self . cutoffval dd = nd . morphology . distance_transform_edt ( ~ edge ) psf = psf * np . exp ( - dd ** 4 ) psf /= psf . sum ( ) if getextent : size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec if self . cutoffval is not None and self . cutbyval : cutval = self . cutoffval * psf . max ( ) dd = ( psf - cutval ) / cutval dd [ dd > 0 ] = 0. psf = psf * np . exp ( - ( dd / self . cutfallrate ) ** 4 ) psf /= psf . sum ( ) edge = psf > cutval * self . cutedgeval if getextent : size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec return psf , vec
11873	def getJsonFromApi ( view , request ) : jsonText = view ( request ) jsonText = json . loads ( jsonText . content . decode ( 'utf-8' ) ) return jsonText
1092	def sub ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . sub ( repl , string , count )
12309	def auto_add ( repo , autooptions , files ) : mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : relativepath = f . replace ( k + "/" , v ) break count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
11753	def make_tables ( grammar , precedence ) : ACTION = { } GOTO = { } labels = { } def get_label ( closure ) : if closure not in labels : labels [ closure ] = len ( labels ) return labels [ closure ] def resolve_shift_reduce ( lookahead , s_action , r_action ) : s_assoc , s_level = precedence [ lookahead ] r_assoc , r_level = precedence [ r_action [ 1 ] ] if s_level < r_level : return r_action elif s_level == r_level and r_assoc == LEFT : return r_action else : return s_action initial , closures , goto = grammar . closures ( ) for closure in closures : label = get_label ( closure ) for rule in closure : new_action , lookahead = None , rule . lookahead if not rule . at_end : symbol = rule . rhs [ rule . pos ] is_terminal = symbol in grammar . terminals has_goto = symbol in goto [ closure ] if is_terminal and has_goto : next_state = get_label ( goto [ closure ] [ symbol ] ) new_action , lookahead = ( 'shift' , next_state ) , symbol elif rule . production == grammar . start and rule . at_end : new_action = ( 'accept' , ) elif rule . at_end : new_action = ( 'reduce' , rule . production ) if new_action is None : continue prev_action = ACTION . get ( ( label , lookahead ) ) if prev_action is None or prev_action == new_action : ACTION [ label , lookahead ] = new_action else : types = ( prev_action [ 0 ] , new_action [ 0 ] ) if types == ( 'shift' , 'reduce' ) : chosen = resolve_shift_reduce ( lookahead , prev_action , new_action ) elif types == ( 'reduce' , 'shift' ) : chosen = resolve_shift_reduce ( lookahead , new_action , prev_action ) else : raise TableConflictError ( prev_action , new_action ) ACTION [ label , lookahead ] = chosen for symbol in grammar . nonterminals : if symbol in goto [ closure ] : GOTO [ label , symbol ] = get_label ( goto [ closure ] [ symbol ] ) return get_label ( initial ) , ACTION , GOTO
3446	def add_mip_obj ( model ) : if len ( model . variables ) > 1e4 : LOGGER . warning ( "the MIP version of minimal media is extremely slow for" " models that large :(" ) exchange_rxns = find_boundary_types ( model , "exchange" ) big_m = max ( abs ( b ) for r in exchange_rxns for b in r . bounds ) prob = model . problem coefs = { } to_add = [ ] for rxn in exchange_rxns : export = len ( rxn . reactants ) == 1 indicator = prob . Variable ( "ind_" + rxn . id , lb = 0 , ub = 1 , type = "binary" ) if export : vrv = rxn . reverse_variable indicator_const = prob . Constraint ( vrv - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) else : vfw = rxn . forward_variable indicator_const = prob . Constraint ( vfw - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) to_add . extend ( [ indicator , indicator_const ] ) coefs [ indicator ] = 1 model . add_cons_vars ( to_add ) model . solver . update ( ) model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
13474	def _loop ( self ) : while True : try : with uncaught_greenlet_exception_context ( ) : self . _loop_callback ( ) except gevent . GreenletExit : break if self . _stop_event . wait ( self . _interval ) : break self . _clear ( )
4425	def remove ( self , guild_id ) : if guild_id in self . _players : self . _players [ guild_id ] . cleanup ( ) del self . _players [ guild_id ]
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
10821	def query_by_user ( cls , user , ** kwargs ) : return cls . _filter ( cls . query . filter_by ( user_id = user . get_id ( ) ) , ** kwargs )
11596	def _rc_keys ( self , pattern = '*' ) : "Returns a list of keys matching ``pattern``" result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
7359	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , string_types ) : sequence_dict = { "seq" : sequence_dict } elif isinstance ( sequence_dict , ( list , tuple ) ) : sequence_dict = { seq : seq for seq in sequence_dict } peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) peptide_set = set ( [ ] ) peptide_to_name_offset_pairs = defaultdict ( list ) for name , sequence in sequence_dict . items ( ) : for peptide_length in peptide_lengths : for i in range ( len ( sequence ) - peptide_length + 1 ) : peptide = sequence [ i : i + peptide_length ] peptide_set . add ( peptide ) peptide_to_name_offset_pairs [ peptide ] . append ( ( name , i ) ) peptide_list = sorted ( peptide_set ) binding_predictions = self . predict_peptides ( peptide_list ) results = [ ] for binding_prediction in binding_predictions : for name , offset in peptide_to_name_offset_pairs [ binding_prediction . peptide ] : results . append ( binding_prediction . clone_with_updates ( source_sequence_name = name , offset = offset ) ) self . _check_results ( results , peptides = peptide_set , alleles = self . alleles ) return BindingPredictionCollection ( results )
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
9509	def intersection ( self , i ) : if self . intersects ( i ) : return Interval ( max ( self . start , i . start ) , min ( self . end , i . end ) ) else : return None
10412	def summarize_node_filter ( graph : BELGraph , node_filters : NodePredicates ) -> None : passed = count_passed_node_filter ( graph , node_filters ) print ( '{}/{} nodes passed' . format ( passed , graph . number_of_nodes ( ) ) )
3610	def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_delete_request ( endpoint , params , headers , connection = connection )
235	def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures
1560	def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
10615	def clone ( self ) : result = copy . copy ( self ) result . _compound_masses = copy . deepcopy ( self . _compound_masses ) return result
1095	def escape ( pattern ) : "Escape all non-alphanumeric characters in pattern." s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
9856	def get_data ( self , ** kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , ** data )
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
7590	def fields_checker ( fields ) : if isinstance ( fields , int ) : fields = str ( fields ) if isinstance ( fields , str ) : if "," in fields : fields = [ str ( i ) for i in fields . split ( "," ) ] else : fields = [ str ( fields ) ] elif isinstance ( fields , ( tuple , list ) ) : fields = [ str ( i ) for i in fields ] else : raise IPyradWarningExit ( "fields not properly formatted" ) fields = [ i for i in fields if i != '0' ] return fields
5455	def numeric_task_id ( task_id ) : if task_id is not None : if task_id . startswith ( 'task-' ) : return int ( task_id [ len ( 'task-' ) : ] ) else : return int ( task_id )
2879	def serialize_value_list ( self , list_elem , thelist ) : for value in thelist : value_elem = SubElement ( list_elem , 'value' ) self . serialize_value ( value_elem , value ) return list_elem
13315	def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
4436	def destroy ( self ) : self . ws . destroy ( ) self . bot . remove_listener ( self . on_socket_response ) self . hooks . clear ( )
4138	def scale_image ( in_fname , out_fname , max_width , max_height ) : try : from PIL import Image except ImportError : import Image img = Image . open ( in_fname ) width_in , height_in = img . size scale_w = max_width / float ( width_in ) scale_h = max_height / float ( height_in ) if height_in * scale_w <= max_height : scale = scale_w else : scale = scale_h if scale >= 1.0 and in_fname == out_fname : return width_sc = int ( round ( scale * width_in ) ) height_sc = int ( round ( scale * height_in ) ) img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) thumb . paste ( img , pos_insert ) thumb . save ( out_fname ) if os . environ . get ( 'SKLEARN_DOC_OPTIPNG' , False ) : try : subprocess . call ( [ "optipng" , "-quiet" , "-o" , "9" , out_fname ] ) except Exception : warnings . warn ( 'Install optipng to reduce the size of the \ generated images' )
13327	def remove ( name_or_path ) : click . echo ( ) try : r = cpenv . resolve ( name_or_path ) except cpenv . ResolveError as e : click . echo ( e ) return obj = r . resolved [ 0 ] if not isinstance ( obj , cpenv . VirtualEnvironment ) : click . echo ( '{} is a module. Use `cpenv module remove` instead.' ) return click . echo ( format_objects ( [ obj ] ) ) click . echo ( ) user_confirmed = click . confirm ( red ( 'Are you sure you want to remove this environment?' ) ) if user_confirmed : click . echo ( 'Attempting to remove...' , nl = False ) try : obj . remove ( ) except Exception as e : click . echo ( bold_red ( 'FAIL' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'OK!' ) )
4251	def country_name_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . country_name_by_addr ( addr )
4695	def execute ( cmd = None , shell = True , echo = True ) : if echo : cij . emph ( "cij.util.execute: shell: %r, cmd: %r" % ( shell , cmd ) ) rcode = 1 stdout , stderr = ( "" , "" ) if cmd : if shell : cmd = " " . join ( cmd ) proc = Popen ( cmd , stdout = PIPE , stderr = PIPE , shell = shell , close_fds = True ) stdout , stderr = proc . communicate ( ) rcode = proc . returncode if rcode and echo : cij . warn ( "cij.util.execute: stdout: %s" % stdout ) cij . err ( "cij.util.execute: stderr: %s" % stderr ) cij . err ( "cij.util.execute: rcode: %s" % rcode ) return rcode , stdout , stderr
9202	def count_cycles ( series , ndigits = None , left = False , right = False ) : counts = defaultdict ( float ) round_ = _get_round_function ( ndigits ) for low , high , mult in extract_cycles ( series , left = left , right = right ) : delta = round_ ( abs ( high - low ) ) counts [ delta ] += mult return sorted ( counts . items ( ) )
2492	def create_review_node ( self , review ) : review_node = BNode ( ) type_triple = ( review_node , RDF . type , self . spdx_namespace . Review ) self . graph . add ( type_triple ) reviewer_node = Literal ( review . reviewer . to_value ( ) ) self . graph . add ( ( review_node , self . spdx_namespace . reviewer , reviewer_node ) ) reviewed_date_node = Literal ( review . review_date_iso_format ) reviewed_triple = ( review_node , self . spdx_namespace . reviewDate , reviewed_date_node ) self . graph . add ( reviewed_triple ) if review . has_comment : comment_node = Literal ( review . comment ) comment_triple = ( review_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) return review_node
4469	def _pprint ( params , offset = 0 , printer = repr ) : options = np . get_printoptions ( ) np . set_printoptions ( precision = 5 , threshold = 64 , edgeitems = 2 ) params_list = list ( ) this_line_length = offset line_sep = ',\n' + ( 1 + offset // 2 ) * ' ' for i , ( k , v ) in enumerate ( sorted ( six . iteritems ( params ) ) ) : if type ( v ) is float : this_repr = '%s=%s' % ( k , str ( v ) ) else : this_repr = '%s=%s' % ( k , printer ( v ) ) if len ( this_repr ) > 500 : this_repr = this_repr [ : 300 ] + '...' + this_repr [ - 100 : ] if i > 0 : if ( this_line_length + len ( this_repr ) >= 75 or '\n' in this_repr ) : params_list . append ( line_sep ) this_line_length = len ( line_sep ) else : params_list . append ( ', ' ) this_line_length += 2 params_list . append ( this_repr ) this_line_length += len ( this_repr ) np . set_printoptions ( ** options ) lines = '' . join ( params_list ) lines = '\n' . join ( l . rstrip ( ' ' ) for l in lines . split ( '\n' ) ) return lines
2053	def STRD ( cpu , src1 , src2 , dest , offset = None ) : assert src1 . type == 'register' assert src2 . type == 'register' assert dest . type == 'memory' val1 = src1 . read ( ) val2 = src2 . read ( ) writeback = cpu . _compute_writeback ( dest , offset ) cpu . write_int ( dest . address ( ) , val1 , 32 ) cpu . write_int ( dest . address ( ) + 4 , val2 , 32 ) cpu . _cs_hack_ldr_str_writeback ( dest , offset , writeback )
4859	def enterprise_login_required ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : if 'enterprise_uuid' not in kwargs : raise Http404 enterprise_uuid = kwargs [ 'enterprise_uuid' ] enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) if not request . user . is_authenticated : parsed_current_url = urlparse ( request . get_full_path ( ) ) parsed_query_string = parse_qs ( parsed_current_url . query ) parsed_query_string . update ( { 'tpa_hint' : enterprise_customer . identity_provider , FRESH_LOGIN_PARAMETER : 'yes' } ) next_url = '{current_path}?{query_string}' . format ( current_path = quote ( parsed_current_url . path ) , query_string = urlencode ( parsed_query_string , doseq = True ) ) return redirect ( '{login_url}?{params}' . format ( login_url = '/login' , params = urlencode ( { 'next' : next_url } ) ) ) return view ( request , * args , ** kwargs ) return wrapper
2	def conv_only ( convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , ** conv_kwargs ) : def network_fn ( X ) : out = tf . cast ( X , tf . float32 ) / 255. with tf . variable_scope ( "convnet" ) : for num_outputs , kernel_size , stride in convs : out = layers . convolution2d ( out , num_outputs = num_outputs , kernel_size = kernel_size , stride = stride , activation_fn = tf . nn . relu , ** conv_kwargs ) return out return network_fn
2261	def dict_hist ( item_list , weight_list = None , ordered = False , labels = None ) : if labels is None : hist_ = defaultdict ( lambda : 0 ) else : hist_ = { k : 0 for k in labels } if weight_list is None : weight_list = it . repeat ( 1 ) for item , weight in zip ( item_list , weight_list ) : hist_ [ item ] += weight if ordered : getval = op . itemgetter ( 1 ) hist = OrderedDict ( [ ( key , value ) for ( key , value ) in sorted ( hist_ . items ( ) , key = getval ) ] ) else : hist = dict ( hist_ ) return hist
11314	def update_hidden_notes ( self ) : if not self . tag_as_cern : notes = record_get_field_instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag_as_cern = True record_delete_fields ( self . record , tag = "595" )
2118	def convert ( self , value , param , ctx ) : resource = tower_cli . get_resource ( self . resource_name ) if value is None : return None if isinstance ( value , int ) : return value if re . match ( r'^[\d]+$' , value ) : return int ( value ) if value == 'null' : return value try : debug . log ( 'The %s field is given as a name; ' 'looking it up.' % param . name , header = 'details' ) lookup_data = { resource . identity [ - 1 ] : value } rel = resource . get ( ** lookup_data ) except exc . MultipleResults : raise exc . MultipleRelatedError ( 'Cannot look up {0} exclusively by name, because multiple {0} ' 'objects exist with that name.\n' 'Please send an ID. You can get the ID for the {0} you want ' 'with:\n' ' tower-cli {0} list --name "{1}"' . format ( self . resource_name , value ) , ) except exc . TowerCLIError as ex : raise exc . RelatedError ( 'Could not get %s. %s' % ( self . resource_name , str ( ex ) ) ) return rel [ 'id' ]
10408	def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
3165	def get ( self , workflow_id , email_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = subscriber_hash return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' , subscriber_hash ) )
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
2121	def disassociate_success_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'success' ) , parent , child )
489	def close ( self ) : self . _logger . info ( "Closing" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
3263	def get_workspace ( self , name ) : workspaces = self . get_workspaces ( names = name ) return self . _return_first_item ( workspaces )
11348	def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) return ret
4811	def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
10204	def _parse_time ( self , tokens ) : return self . time_parser . parse ( self . parse_keyword ( Keyword . WHERE , tokens ) )
983	def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )
5660	def print_coords ( rows , prefix = '' ) : lat = [ row [ 'lat' ] for row in rows ] lon = [ row [ 'lon' ] for row in rows ] print ( 'COORDS' + '-' * 5 ) print ( "%slat, %slon = %r, %r" % ( prefix , prefix , lat , lon ) ) print ( '-' * 5 )
11476	def _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing = False ) : local_item_name = os . path . basename ( local_file ) item_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) items = children [ 'items' ] for item in items : if item [ 'name' ] == local_item_name : item_id = item [ 'item_id' ] break if item_id is None : new_item = session . communicator . create_item ( session . token , local_item_name , parent_folder_id ) item_id = new_item [ 'item_id' ] return item_id
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
5344	def compose_gerrit ( projects ) : git_projects = [ project for project in projects if 'git' in projects [ project ] ] for project in git_projects : repos = [ repo for repo in projects [ project ] [ 'git' ] if 'gitroot' in repo ] if len ( repos ) > 0 : projects [ project ] [ 'gerrit' ] = [ ] for repo in repos : gerrit_project = repo . replace ( "http://git.eclipse.org/gitroot/" , "" ) gerrit_project = gerrit_project . replace ( ".git" , "" ) projects [ project ] [ 'gerrit' ] . append ( "git.eclipse.org_" + gerrit_project ) return projects
7767	def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
594	def _cacheSequenceInfoType ( self ) : hasReset = self . resetFieldName is not None hasSequenceId = self . sequenceIdFieldName is not None if hasReset and not hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY self . _prevSequenceId = 0 elif not hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY self . _prevSequenceId = None elif hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_BOTH else : self . _sequenceInfoType = self . SEQUENCEINFO_NONE
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
5359	def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
8959	def build ( ctx , dput = '' , opts = '' ) : with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , _ , _ = metadata . groups ( ) ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact_pattern = '{}?{}*' . format ( name , re . sub ( r'[^-_.a-zA-Z0-9]' , '?' , version ) ) changes_files = [ ] for debfile in glob . glob ( '../' + artifact_pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes_files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact_pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes_files ) ) )
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
8538	def run ( self , * args , ** kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key ] = stream setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
10519	def onedown ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
12106	def _launch_process_group ( self , process_commands , streams_path ) : processes = [ ] for cmd , tid in process_commands : job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_path = os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) stderr_path = os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) process = { 'tid' : tid , 'cmd' : cmd , 'stdout' : stdout_path , 'stderr' : stderr_path } processes . append ( process ) json_path = os . path . join ( self . root_directory , self . json_name % ( tid ) ) with open ( json_path , 'w' ) as json_file : json . dump ( processes , json_file , sort_keys = True , indent = 4 ) p = subprocess . Popen ( [ self . script_path , json_path , self . batch_name , str ( len ( processes ) ) , str ( self . max_concurrency ) ] ) if p . wait ( ) != 0 : raise EnvironmentError ( "Script command exit with code: %d" % p . poll ( ) )
7305	def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . document is None : self . document = self . document_type ( ) self . form = MongoModelForm ( model = self . document_type , instance = self . document , form_post_data = self . request . POST ) . get_form ( ) self . form . is_bound = True if self . form . is_valid ( ) : self . document_map_dict = MongoModelForm ( model = self . document_type ) . create_document_dictionary ( self . document_type ) self . new_document = self . document_type self . embedded_list_docs = { } if self . new_document is None : messages . error ( self . request , u"Failed to save document" ) else : self . new_document = self . new_document ( ) for form_key in self . form . cleaned_data . keys ( ) : if form_key == 'id' and hasattr ( self , 'document' ) : self . new_document . id = self . document . id continue self . process_document ( self . new_document , form_key , None ) self . new_document . save ( ) if success_message : messages . success ( self . request , success_message ) return self . form
644	def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : if doForeground and doBackground : return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) else : if doForeground : return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) if doBackground : return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) return input
5957	def merge_ndx ( * args ) : ndxs = [ ] struct = None for fname in args : if fname . endswith ( '.ndx' ) : ndxs . append ( fname ) else : if struct is not None : raise ValueError ( "only one structure file supported" ) struct = fname fd , multi_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'multi_' ) os . close ( fd ) atexit . register ( os . unlink , multi_ndx ) if struct : make_ndx = registry [ 'Make_ndx' ] ( f = struct , n = ndxs , o = multi_ndx ) else : make_ndx = registry [ 'Make_ndx' ] ( n = ndxs , o = multi_ndx ) _ , _ , _ = make_ndx ( input = [ 'q' ] , stdout = False , stderr = False ) return multi_ndx
4427	async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
13689	def add_peer ( self , peer ) : if type ( peer ) == list : for i in peer : check_url ( i ) self . PEERS . extend ( peer ) elif type ( peer ) == str : check_url ( peer ) self . PEERS . append ( peer )
5432	def build_logging_param ( logging_uri , util_class = OutputFileParamUtil ) : if not logging_uri : return job_model . LoggingParam ( None , None ) recursive = not logging_uri . endswith ( '.log' ) oututil = util_class ( '' ) _ , uri , provider = oututil . parse_uri ( logging_uri , recursive ) if '*' in uri . basename : raise ValueError ( 'Wildcards not allowed in logging URI: %s' % uri ) return job_model . LoggingParam ( uri , provider )
7391	def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . ax . set_ylim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . add_axes_and_nodes ( ) self . add_edges ( ) self . ax . axis ( 'off' )
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
8058	def do_play ( self , line ) : if self . pause_speed is None : self . bot . _speed = self . pause_speed self . pause_speed = None self . print_response ( "Play" )
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) return 'NONE'
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
4864	def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
5994	def set_colorbar ( cb_ticksize , cb_fraction , cb_pad , cb_tick_values , cb_tick_labels ) : if cb_tick_values is None and cb_tick_labels is None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad ) elif cb_tick_values is not None and cb_tick_labels is not None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad , ticks = cb_tick_values ) cb . ax . set_yticklabels ( cb_tick_labels ) else : raise exc . PlottingException ( 'Only 1 entry of cb_tick_values or cb_tick_labels was input. You must either supply' 'both the values and labels, or neither.' ) cb . ax . tick_params ( labelsize = cb_ticksize )
9398	def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
6211	def fit ( self , trX , trY , batch_size = 64 , n_epochs = 1 , len_filter = LenFilter ( ) , snapshot_freq = 1 , path = None ) : if len_filter is not None : trX , trY = len_filter . filter ( trX , trY ) trY = standardize_targets ( trY , cost = self . cost ) n = 0. t = time ( ) costs = [ ] for e in range ( n_epochs ) : epoch_costs = [ ] for xmb , ymb in self . iterator . iterXY ( trX , trY ) : c = self . _train ( xmb , ymb ) epoch_costs . append ( c ) n += len ( ymb ) if self . verbose >= 2 : n_per_sec = n / ( time ( ) - t ) n_left = len ( trY ) - n % len ( trY ) time_left = n_left / n_per_sec sys . stdout . write ( "\rEpoch %d Seen %d samples Avg cost %0.4f Time left %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time_left ) ) sys . stdout . flush ( ) costs . extend ( epoch_costs ) status = "Epoch %d Seen %d samples Avg cost %0.4f Time elapsed %d seconds" % ( e , n , np . mean ( epoch_costs [ - 250 : ] ) , time ( ) - t ) if self . verbose >= 2 : sys . stdout . write ( "\r" + status ) sys . stdout . flush ( ) sys . stdout . write ( "\n" ) elif self . verbose == 1 : print ( status ) if path and e % snapshot_freq == 0 : save ( self , "{0}.{1}" . format ( path , e ) ) return costs
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , ** kwargs ) : log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , ** kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
2418	def write_package ( package , out ) : out . write ( '# Package\n\n' ) write_value ( 'PackageName' , package . name , out ) if package . has_optional_field ( 'version' ) : write_value ( 'PackageVersion' , package . version , out ) write_value ( 'PackageDownloadLocation' , package . download_location , out ) if package . has_optional_field ( 'summary' ) : write_text_value ( 'PackageSummary' , package . summary , out ) if package . has_optional_field ( 'source_info' ) : write_text_value ( 'PackageSourceInfo' , package . source_info , out ) if package . has_optional_field ( 'file_name' ) : write_value ( 'PackageFileName' , package . file_name , out ) if package . has_optional_field ( 'supplier' ) : write_value ( 'PackageSupplier' , package . supplier , out ) if package . has_optional_field ( 'originator' ) : write_value ( 'PackageOriginator' , package . originator , out ) if package . has_optional_field ( 'check_sum' ) : write_value ( 'PackageChecksum' , package . check_sum . to_tv ( ) , out ) write_value ( 'PackageVerificationCode' , format_verif_code ( package ) , out ) if package . has_optional_field ( 'description' ) : write_text_value ( 'PackageDescription' , package . description , out ) if isinstance ( package . license_declared , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseDeclared' , u'({0})' . format ( package . license_declared ) , out ) else : write_value ( 'PackageLicenseDeclared' , package . license_declared , out ) if isinstance ( package . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'PackageLicenseConcluded' , u'({0})' . format ( package . conc_lics ) , out ) else : write_value ( 'PackageLicenseConcluded' , package . conc_lics , out ) for lics in sorted ( package . licenses_from_files ) : write_value ( 'PackageLicenseInfoFromFiles' , lics , out ) if package . has_optional_field ( 'license_comment' ) : write_text_value ( 'PackageLicenseComments' , package . license_comment , out ) if isinstance ( package . cr_text , six . string_types ) : write_text_value ( 'PackageCopyrightText' , package . cr_text , out ) else : write_value ( 'PackageCopyrightText' , package . cr_text , out ) if package . has_optional_field ( 'homepage' ) : write_value ( 'PackageHomePage' , package . homepage , out ) for spdx_file in sorted ( package . files ) : write_separators ( out ) write_file ( spdx_file , out )
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
13567	def linspacestep ( start , stop , step = 1 ) : numsteps = _np . int ( ( stop - start ) / step ) return _np . linspace ( start , start + step * numsteps , numsteps + 1 )
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
7565	def memoize ( func ) : class Memodict ( dict ) : def __getitem__ ( self , * key ) : return dict . __getitem__ ( self , key ) def __missing__ ( self , key ) : ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . __getitem__
1474	def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
11437	def _fields_sort_by_indicators ( fields ) : field_dict = { } field_positions_global = [ ] for field in fields : field_dict . setdefault ( field [ 1 : 3 ] , [ ] ) . append ( field ) field_positions_global . append ( field [ 4 ] ) indicators = field_dict . keys ( ) indicators . sort ( ) field_list = [ ] for indicator in indicators : for field in field_dict [ indicator ] : field_list . append ( field [ : 4 ] + ( field_positions_global . pop ( 0 ) , ) ) return field_list
13259	def combine ( self , members , output_file , dimension = None , start_index = None , stop_index = None , stride = None ) : nco = None try : nco = Nco ( ) except BaseException : raise ImportError ( "NCO not found. The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : members = [ m . path for m in members ] options = [ '-4' ] options += [ '-L' , '3' ] options += [ '-h' ] if dimension is not None : if start_index is None : start_index = 0 if stop_index is None : stop_index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start_index , stop_index , stride ) ] nco . ncrcat ( input = members , output = output_file , options = options )
1827	def CALL ( cpu , op0 ) : proc = op0 . read ( ) cpu . push ( cpu . PC , cpu . address_bit_size ) cpu . PC = proc
6483	def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) if not ( 0 < size <= max_page_size ) : raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
11825	def random_boggle ( n = 4 ) : cubes = [ cubes16 [ i % 16 ] for i in range ( n * n ) ] random . shuffle ( cubes ) return map ( random . choice , cubes )
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , ** kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , ** kwargs )
6770	def install_yum ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : assert self . genv [ ROLE ] yum_req_fn = fn or self . find_template ( self . genv . yum_requirments_fn ) if not yum_req_fn : return [ ] assert os . path . isfile ( yum_req_fn ) update = int ( update ) if list_only : return [ _ . strip ( ) for _ in open ( yum_req_fn ) . readlines ( ) if _ . strip ( ) and not _ . strip . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) ] if update : self . sudo_or_dryrun ( 'yum update --assumeyes' ) if package_name : self . sudo_or_dryrun ( 'yum install --assumeyes %s' % package_name ) else : if self . genv . is_local : self . put_or_dryrun ( local_path = yum_req_fn ) yum_req_fn = self . genv . put_remote_fn self . sudo_or_dryrun ( 'yum install --assumeyes $(cat %(yum_req_fn)s)' % yum_req_fn )
10363	def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , ** kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
11932	def find_block ( context , * names ) : block_set = context . render_context [ BLOCK_CONTEXT_KEY ] for name in names : block = block_set . get_block ( name ) if block is not None : return block raise template . TemplateSyntaxError ( 'No widget found for: %r' % ( names , ) )
3105	def code_verifier ( n_bytes = 64 ) : verifier = base64 . urlsafe_b64encode ( os . urandom ( n_bytes ) ) . rstrip ( b'=' ) if len ( verifier ) < 43 : raise ValueError ( "Verifier too short. n_bytes must be > 30." ) elif len ( verifier ) > 128 : raise ValueError ( "Verifier too long. n_bytes must be < 97." ) else : return verifier
9707	def value_from_datadict ( self , * args , ** kwargs ) : value = super ( RichTextWidget , self ) . value_from_datadict ( * args , ** kwargs ) if value is not None : value = self . get_sanitizer ( ) ( value ) return value
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
11847	def delete_thing ( self , thing ) : try : self . things . remove ( thing ) except ValueError , e : print e print " in Environment delete_thing" print " Thing to be removed: %s at %s" % ( thing , thing . location ) print " from list: %s" % [ ( thing , thing . location ) for thing in self . things ] if thing in self . agents : self . agents . remove ( thing )
10009	def check_mro ( self , bases ) : try : self . add_node ( "temp" ) for base in bases : nx . DiGraph . add_edge ( self , base , "temp" ) result = self . get_mro ( "temp" ) [ 1 : ] finally : self . remove_node ( "temp" ) return result
1599	def pipe ( prev_proc , to_cmd ) : stdin = None if prev_proc is None else prev_proc . stdout process = subprocess . Popen ( to_cmd , stdout = subprocess . PIPE , stdin = stdin ) if prev_proc is not None : prev_proc . stdout . close ( ) return process
5906	def create_portable_topology ( topol , struct , ** kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( ** kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( ** mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( ** grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
9655	def run_commands ( commands , settings ) : sprint = settings [ "sprint" ] quiet = settings [ "quiet" ] error = settings [ "error" ] enhanced_errors = True the_shell = None if settings [ "no_enhanced_errors" ] : enhanced_errors = False if "shell" in settings : the_shell = settings [ "shell" ] windows_p = sys . platform == "win32" STDOUT = None STDERR = None if quiet : STDOUT = PIPE STDERR = PIPE commands = commands . rstrip ( ) sprint ( "About to run commands '{}'" . format ( commands ) , level = "verbose" ) if not quiet : sprint ( commands ) if the_shell : tmp = shlex . split ( the_shell ) the_shell = tmp [ 0 ] tmp = tmp [ 1 : ] if enhanced_errors and not windows_p : tmp . append ( "-e" ) tmp . append ( commands ) commands = tmp else : if enhanced_errors and not windows_p : commands = [ "-e" , commands ] p = Popen ( commands , shell = True , stdout = STDOUT , stderr = STDERR , executable = the_shell ) out , err = p . communicate ( ) if p . returncode : if quiet : error ( err . decode ( locale . getpreferredencoding ( ) ) ) error ( "Command failed to run" ) sys . exit ( 1 )
9128	def store_populate ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate ( resource ) _store_helper ( action , session = session ) return action
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
5594	def intersecting ( self , tile ) : return [ self . tile ( * intersecting_tile . id ) for intersecting_tile in self . tile_pyramid . intersecting ( tile ) ]
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
9026	def insert_defs ( self , defs ) : if self . _svg [ "defs" ] is None : self . _svg [ "defs" ] = { } for def_ in defs : for key , value in def_ . items ( ) : if key . startswith ( "@" ) : continue if key not in self . _svg [ "defs" ] : self . _svg [ "defs" ] [ key ] = [ ] if not isinstance ( value , list ) : value = [ value ] self . _svg [ "defs" ] [ key ] . extend ( value )
4028	def create_cookie ( host , path , secure , expires , name , value ) : return http . cookiejar . Cookie ( 0 , name , value , None , False , host , host . startswith ( '.' ) , host . startswith ( '.' ) , path , True , secure , expires , False , None , None , { } )
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
1833	def JCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC )
10966	def set_shape ( self , shape , inner ) : for c in self . comps : c . set_shape ( shape , inner )
1151	def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = "%s: %s: %s\n" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += " %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = "%s:%s" % ( filename , s ) return s
13312	def _pre_activate ( self ) : if 'CPENV_CLEAN_ENV' not in os . environ : if platform == 'win' : os . environ [ 'PROMPT' ] = '$P$G' else : os . environ [ 'PS1' ] = '\\u@\\h:\\w\\$' clean_env_path = utils . get_store_env_tmp ( ) os . environ [ 'CPENV_CLEAN_ENV' ] = clean_env_path utils . store_env ( path = clean_env_path ) else : utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
13417	def syncdb ( args ) : cmd = args and 'syncdb %s' % ' ' . join ( options . args ) or 'syncdb --noinput' call_manage ( cmd ) for fixture in options . paved . django . syncdb . fixtures : call_manage ( "loaddata %s" % fixture )
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
12129	def _build_specs ( self , specs , kwargs , fp_precision ) : if specs is None : overrides = param . ParamOverrides ( self , kwargs , allow_extra_keywords = True ) extra_kwargs = overrides . extra_keywords ( ) kwargs = dict ( [ ( k , v ) for ( k , v ) in kwargs . items ( ) if k not in extra_kwargs ] ) rounded_specs = list ( self . round_floats ( [ extra_kwargs ] , fp_precision ) ) if extra_kwargs == { } : return [ ] , kwargs , True else : return rounded_specs , kwargs , False return list ( self . round_floats ( specs , fp_precision ) ) , kwargs , True
13130	def parse_single_computer ( entry ) : computer = Computer ( dns_hostname = get_field ( entry , 'dNSHostName' ) , description = get_field ( entry , 'description' ) , os = get_field ( entry , 'operatingSystem' ) , group_id = get_field ( entry , 'primaryGroupID' ) ) try : ip = str ( ipaddress . ip_address ( get_field ( entry , 'IPv4' ) ) ) except ValueError : ip = '' if ip : computer . ip = ip elif computer . dns_hostname : computer . ip = resolve_ip ( computer . dns_hostname ) return computer
13272	def generic_masked ( arr , attrs = None , minv = None , maxv = None , mask_nan = True ) : attrs = attrs or { } if 'valid_min' in attrs : minv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_min' ] ) if 'valid_max' in attrs : maxv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_max' ] ) if 'valid_range' in attrs : vr = attrs [ 'valid_range' ] minv = safe_attribute_typing ( arr . dtype , vr [ 0 ] ) maxv = safe_attribute_typing ( arr . dtype , vr [ 1 ] ) try : info = np . iinfo ( arr . dtype ) except ValueError : info = np . finfo ( arr . dtype ) minv = minv if minv is not None else info . min maxv = maxv if maxv is not None else info . max if mask_nan is True : arr = np . ma . fix_invalid ( arr ) return np . ma . masked_outside ( arr , minv , maxv )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
10622	def get_element_mass ( self , element ) : result = numpy . zeros ( 1 ) for compound in self . material . compounds : result += self . get_compound_mass ( compound ) * numpy . array ( stoich . element_mass_fractions ( compound , [ element ] ) ) return result [ 0 ]
8260	def cluster_sort ( self , cmp1 = "hue" , cmp2 = "brightness" , reversed = False , n = 12 ) : sorted = self . sort ( cmp1 ) clusters = ColorList ( ) d = 1.0 i = 0 for j in _range ( len ( sorted ) ) : if getattr ( sorted [ j ] , cmp1 ) < d : clusters . extend ( sorted [ i : j ] . sort ( cmp2 ) ) d -= 1.0 / n i = j clusters . extend ( sorted [ i : ] . sort ( cmp2 ) ) if reversed : _list . reverse ( clusters ) return clusters
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
11094	def n_dir ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_dir ( recursive = True ) : n += 1 return n
13568	def selected_course ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : course = Course . get_selected ( ) return func ( course , * args , ** kwargs ) return inner
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
7601	def get_popular_decks ( self , ** params : keys ) : url = self . api . POPULAR + '/decks' return self . _get_model ( url , ** params )
4991	def post ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , ** kwargs )
3958	def resolve ( cls , all_known_repos , name ) : match = None for repo in all_known_repos : if repo . remote_path == name : return repo if name == repo . short_name : if match is None : match = repo else : raise RuntimeError ( 'Short repo name {} is ambiguous. It matches both {} and {}' . format ( name , match . remote_path , repo . remote_path ) ) if match is None : raise RuntimeError ( 'Short repo name {} does not match any known repos' . format ( name ) ) return match
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
12390	def paginate ( request , response , items ) : header = request . headers . get ( 'Range' ) if not header : return items prefix = RANGE_SPECIFIER + '=' if not header . find ( prefix ) == 0 : raise exceptions . RequestedRangeNotSatisfiable ( ) else : ranges = parse ( header [ len ( prefix ) : ] ) ranges = list ( ranges ) if len ( ranges ) > 1 : raise exceptions . RequestedRangeNotSatisfiable ( 'Multiple ranges in a single request is not yet supported.' ) start , end = ranges [ 0 ] max_length = request . resource . count ( items ) end = min ( end , max_length ) response . status = client . PARTIAL_CONTENT response . headers [ 'Content-Range' ] = '%d-%d/%d' % ( start , end , max_length ) response . headers [ 'Accept-Ranges' ] = RANGE_SPECIFIER items = items [ start : end + 1 ] return items
4813	def _document_frequency ( X ) : if sp . isspmatrix_csr ( X ) : return np . bincount ( X . indices , minlength = X . shape [ 1 ] ) return np . diff ( sp . csc_matrix ( X , copy = False ) . indptr )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
5106	def _current_color ( self , which = 0 ) : if which == 1 : color = self . colors [ 'edge_loop_color' ] elif which == 2 : color = self . colors [ 'vertex_color' ] else : div = self . coloring_sensitivity * self . num_servers + 1. tmp = 1. - min ( self . num_system / div , 1 ) if self . edge [ 0 ] == self . edge [ 1 ] : color = [ i * tmp for i in self . colors [ 'vertex_fill_color' ] ] color [ 3 ] = 1.0 else : color = [ i * tmp for i in self . colors [ 'edge_color' ] ] color [ 3 ] = 1 / 2. return color
969	def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( "self" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
9552	def _apply_value_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , check , code , message , modulus in self . _value_checks : if i % modulus == 0 : fi = self . _field_names . index ( field_name ) if fi < len ( r ) : value = r [ fi ] try : check ( value ) except ValueError : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = True )
2933	def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
9929	def authenticate ( username , password , service = 'login' , encoding = 'utf-8' , resetcred = True ) : if sys . version_info >= ( 3 , ) : if isinstance ( username , str ) : username = username . encode ( encoding ) if isinstance ( password , str ) : password = password . encode ( encoding ) if isinstance ( service , str ) : service = service . encode ( encoding ) @ conv_func def my_conv ( n_messages , messages , p_response , app_data ) : addr = calloc ( n_messages , sizeof ( PamResponse ) ) p_response [ 0 ] = cast ( addr , POINTER ( PamResponse ) ) for i in range ( n_messages ) : if messages [ i ] . contents . msg_style == PAM_PROMPT_ECHO_OFF : pw_copy = strdup ( password ) p_response . contents [ i ] . resp = cast ( pw_copy , c_char_p ) p_response . contents [ i ] . resp_retcode = 0 return 0 handle = PamHandle ( ) conv = PamConv ( my_conv , 0 ) retval = pam_start ( service , username , byref ( conv ) , byref ( handle ) ) if retval != 0 : return False retval = pam_authenticate ( handle , 0 ) auth_success = ( retval == 0 ) if auth_success and resetcred : retval = pam_setcred ( handle , PAM_REINITIALIZE_CRED ) pam_end ( handle , retval ) return auth_success
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
13507	def get_positions ( self ) : url = "/2/positions" data = self . _get_resource ( url ) positions = [ ] for entry in data [ 'positions' ] : positions . append ( self . position_from_json ( entry ) ) return positions
2693	def filter_quotes ( text , is_email = True ) : global DEBUG global PAT_FORWARD , PAT_REPLIED , PAT_UNSUBSC if is_email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) m = PAT_FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT_REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT_UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split_grafs ( lines ) )
13583	def admin_obj_link ( obj , display = '' ) : url = reverse ( 'admin:%s_%s_changelist' % ( obj . _meta . app_label , obj . _meta . model_name ) ) url += '?id__exact=%s' % obj . id text = str ( obj ) if display : text = display return format_html ( '<a href="{}">{}</a>' , url , text )
10280	def bond_task ( perc_graph_result , seeds , ps , convolution_factors_tasks_iterator ) : convolution_factors_tasks = list ( convolution_factors_tasks_iterator ) return reduce ( percolate . hpc . bond_reduce , map ( bond_run , itertools . repeat ( perc_graph_result ) , seeds , itertools . repeat ( ps ) , itertools . repeat ( convolution_factors_tasks ) , ) )
1901	def get_value ( self , constraints , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , ( Bool , BitVec , Array ) ) with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = [ ] result = [ ] for i in range ( expression . index_max ) : subvar = temp_cs . new_bitvec ( expression . value_bits ) var . append ( subvar ) temp_cs . add ( subvar == simplify ( expression [ i ] ) ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) for i in range ( expression . index_max ) : self . _send ( '(get-value (%s))' % var [ i ] . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) result . append ( int ( value , base ) ) return bytes ( result ) temp_cs . add ( var == expression ) self . _reset ( temp_cs ) if not self . _is_sat ( ) : raise SolverError ( 'Model is not available' ) self . _send ( '(get-value (%s))' % var . name ) ret = self . _recv ( ) if not ( ret . startswith ( '((' ) and ret . endswith ( '))' ) ) : raise SolverError ( 'SMTLIB error parsing response: %s' % ret ) if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] if isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "get_value only implemented for Bool and BitVec" )
137	def to_shapely_polygon ( self ) : import shapely . geometry return shapely . geometry . Polygon ( [ ( point [ 0 ] , point [ 1 ] ) for point in self . exterior ] )
6438	def dist ( self , src , tar , weights = 'exponential' , max_length = 8 ) : return self . dist_abs ( src , tar , weights , max_length , True )
8320	def parse_categories ( self , markup ) : categories = [ ] m = re . findall ( self . re [ "category" ] , markup ) for category in m : category = category . split ( "|" ) page = category [ 0 ] . strip ( ) display = u"" if len ( category ) > 1 : display = category [ 1 ] . strip ( ) if not page in categories : categories . append ( page ) return categories
10502	def waitForValueToChange ( self , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXValueChanged' , callback = callback , args = ( retelem , ) )
10167	def get_md_status ( self , line ) : ret = { } splitted = split ( '\W+' , line ) if len ( splitted ) < 7 : ret [ 'available' ] = None ret [ 'used' ] = None ret [ 'config' ] = None else : ret [ 'available' ] = splitted [ - 4 ] ret [ 'used' ] = splitted [ - 3 ] ret [ 'config' ] = splitted [ - 2 ] return ret
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
9193	def _insert_file ( cursor , file , media_type ) : resource_hash = _get_file_sha1 ( file ) cursor . execute ( "SELECT fileid FROM files WHERE sha1 = %s" , ( resource_hash , ) ) try : fileid = cursor . fetchone ( ) [ 0 ] except ( IndexError , TypeError ) : cursor . execute ( "INSERT INTO files (file, media_type) " "VALUES (%s, %s)" "RETURNING fileid" , ( psycopg2 . Binary ( file . read ( ) ) , media_type , ) ) fileid = cursor . fetchone ( ) [ 0 ] return fileid , resource_hash
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
5991	def weighted_regularization_matrix_from_pixel_neighbors ( regularization_weights , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( regularization_weights ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_weight = regularization_weights ** 2.0 for i in range ( pixels ) : for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , neighbor_index ] += regularization_weight [ neighbor_index ] regularization_matrix [ i , neighbor_index ] -= regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , i ] -= regularization_weight [ neighbor_index ] return regularization_matrix
13190	def json_doc_to_xml ( json_obj , lang = 'en' , custom_namespace = None ) : if 'meta' not in json_obj : raise Exception ( "This function requires a conforming Open511 JSON document with a 'meta' section." ) json_obj = dict ( json_obj ) meta = json_obj . pop ( 'meta' ) elem = get_base_open511_element ( lang = lang , version = meta . pop ( 'version' ) ) pagination = json_obj . pop ( 'pagination' , None ) json_struct_to_xml ( json_obj , elem , custom_namespace = custom_namespace ) if pagination : elem . append ( json_struct_to_xml ( pagination , 'pagination' , custom_namespace = custom_namespace ) ) json_struct_to_xml ( meta , elem ) return elem
5029	def transmit_content_metadata ( self , user ) : exporter = self . get_content_metadata_exporter ( user ) transmitter = self . get_content_metadata_transmitter ( ) transmitter . transmit ( exporter . export ( ) )
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
12536	def get_dcm_reader ( store_metadata = True , header_fields = None ) : if not store_metadata : return lambda fpath : fpath if header_fields is None : build_dcm = lambda fpath : DicomFile ( fpath ) else : dicom_header = namedtuple ( 'DicomHeader' , header_fields ) build_dcm = lambda fpath : dicom_header . _make ( DicomFile ( fpath ) . get_attributes ( header_fields ) ) return build_dcm
2210	def parse_requirements ( fname = 'requirements.txt' ) : from os . path import dirname , join , exists import re require_fpath = join ( dirname ( __file__ ) , fname ) def parse_line ( line ) : info = { } if line . startswith ( '-e ' ) : info [ 'package' ] = line . split ( '#egg=' ) [ 1 ] else : pat = '(' + '|' . join ( [ '>=' , '==' , '>' ] ) + ')' parts = re . split ( pat , line , maxsplit = 1 ) parts = [ p . strip ( ) for p in parts ] info [ 'package' ] = parts [ 0 ] if len ( parts ) > 1 : op , rest = parts [ 1 : ] if ';' in rest : version , platform_deps = map ( str . strip , rest . split ( ';' ) ) info [ 'platform_deps' ] = platform_deps else : version = rest info [ 'version' ] = ( op , version ) return info if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as f : packages = [ ] for line in f . readlines ( ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : info = parse_line ( line ) package = info [ 'package' ] if not sys . version . startswith ( '3.4' ) : platform_deps = info . get ( 'platform_deps' ) if platform_deps is not None : package += ';' + platform_deps packages . append ( package ) return packages return [ ]
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
7438	def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) return pd . DataFrame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
8381	def hover ( self , node ) : if self . popup == False : return if self . popup == True or self . popup . node != node : if self . popup_text . has_key ( node . id ) : texts = self . popup_text [ node . id ] else : texts = None self . popup = popup ( self . _ctx , node , texts ) self . popup . draw ( )
8685	def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value
9810	def dashboard ( yes , url ) : dashboard_url = "{}/app" . format ( PolyaxonClient ( ) . api_config . http_host ) if url : click . echo ( dashboard_url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard_url )
2193	def isatty ( self ) : return ( self . redirect is not None and hasattr ( self . redirect , 'isatty' ) and self . redirect . isatty ( ) )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
225	async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
8811	def filter_factory ( global_conf , ** local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
5972	def generate_submit_scripts ( templates , prefix = None , deffnm = 'md' , jobname = 'MD' , budget = None , mdrun_opts = None , walltime = 1.0 , jobarray_string = None , startdir = None , npme = None , ** kwargs ) : if not jobname [ 0 ] . isalpha ( ) : jobname = 'MD_' + jobname wmsg = "To make the jobname legal it must start with a letter: changed to {0!r}" . format ( jobname ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) if prefix is None : prefix = "" if mdrun_opts is not None : mdrun_opts = '"' + str ( mdrun_opts ) + '"' dirname = kwargs . pop ( 'dirname' , os . path . curdir ) wt = Timedelta ( hours = walltime ) walltime = wt . strftime ( "%h:%M:%S" ) wall_hours = wt . ashours def write_script ( template ) : submitscript = os . path . join ( dirname , prefix + os . path . basename ( template ) ) logger . info ( "Setting up queuing system script {submitscript!r}..." . format ( ** vars ( ) ) ) qsystem = detect_queuing_system ( template ) if qsystem is not None and ( qsystem . name == 'Slurm' ) : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-J)' , '((?<=-J\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-t)' , '(?<=-t\s)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] else : cbook . edit_txt ( template , [ ( '^ *DEFFNM=' , '(?<==)(.*)' , deffnm ) , ( '^#.*(-N|job_name)' , '((?<=-N\s)|(?<=job_name\s))\s*\w+' , jobname ) , ( '^#.*(-A|account_no)' , '((?<=-A\s)|(?<=account_no\s))\s*\w+' , budget ) , ( '^#.*(-l walltime|wall_clock_limit)' , '(?<==)(\d+:\d+:\d+)' , walltime ) , ( '^ *WALL_HOURS=' , '(?<==)(.*)' , wall_hours ) , ( '^ *STARTDIR=' , '(?<==)(.*)' , startdir ) , ( '^ *NPME=' , '(?<==)(.*)' , npme ) , ( '^ *MDRUN_OPTS=' , '(?<==)("")' , mdrun_opts ) , ( '^# JOB_ARRAY_PLACEHOLDER' , '^.*$' , jobarray_string ) , ] , newname = submitscript ) ext = os . path . splitext ( submitscript ) [ 1 ] if ext in ( '.sh' , '.csh' , '.bash' ) : os . chmod ( submitscript , 0o755 ) return submitscript return [ write_script ( template ) for template in config . get_templates ( templates ) ]
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
11283	def iter ( self , prev = None ) : if self . next : generator = self . next . iter ( self . func ( prev , * self . args , ** self . kw ) ) else : generator = self . func ( prev , * self . args , ** self . kw ) return generator
8533	def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self . fields , other . fields ) ) )
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
9489	def generate_bytecode_from_obb ( obb : object , previous : bytes ) -> bytes : if isinstance ( obb , pyte . superclasses . _PyteOp ) : return obb . to_bytes ( previous ) elif isinstance ( obb , ( pyte . superclasses . _PyteAugmentedComparator , pyte . superclasses . _PyteAugmentedValidator . _FakeMathematicalOP ) ) : return obb . to_bytes ( previous ) elif isinstance ( obb , pyte . superclasses . _PyteAugmentedValidator ) : obb . validate ( ) return obb . to_load ( ) elif isinstance ( obb , int ) : return obb . to_bytes ( ( obb . bit_length ( ) + 7 ) // 8 , byteorder = "little" ) or b'' elif isinstance ( obb , bytes ) : return obb else : raise TypeError ( "`{}` was not a valid bytecode-encodable item" . format ( obb ) )
4349	def vad ( self , location = 1 , normalize = True , activity_threshold = 7.0 , min_activity_duration = 0.25 , initial_search_buffer = 1.0 , max_gap = 0.25 , initial_pad = 0.0 ) : if location not in [ - 1 , 1 ] : raise ValueError ( "location must be -1 or 1." ) if not isinstance ( normalize , bool ) : raise ValueError ( "normalize muse be a boolean." ) if not is_number ( activity_threshold ) : raise ValueError ( "activity_threshold must be a number." ) if not is_number ( min_activity_duration ) or min_activity_duration < 0 : raise ValueError ( "min_activity_duration must be a positive number" ) if not is_number ( initial_search_buffer ) or initial_search_buffer < 0 : raise ValueError ( "initial_search_buffer must be a positive number" ) if not is_number ( max_gap ) or max_gap < 0 : raise ValueError ( "max_gap must be a positive number." ) if not is_number ( initial_pad ) or initial_pad < 0 : raise ValueError ( "initial_pad must be a positive number." ) effect_args = [ ] if normalize : effect_args . append ( 'norm' ) if location == - 1 : effect_args . append ( 'reverse' ) effect_args . extend ( [ 'vad' , '-t' , '{:f}' . format ( activity_threshold ) , '-T' , '{:f}' . format ( min_activity_duration ) , '-s' , '{:f}' . format ( initial_search_buffer ) , '-g' , '{:f}' . format ( max_gap ) , '-p' , '{:f}' . format ( initial_pad ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'vad' ) return self
9205	def before_constant ( self , constant , key ) : newlines_split = split_on_newlines ( constant ) for c in newlines_split : if is_newline ( c ) : self . current . advance_line ( ) if self . current . line > self . target . line : return self . STOP else : advance_by = len ( c ) if self . is_on_targetted_node ( advance_by ) : self . found_path = deepcopy ( self . current_path ) return self . STOP self . current . advance_columns ( advance_by )
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
1468	def process_tick ( self , tup ) : curtime = int ( time . time ( ) ) window_info = WindowContext ( curtime - self . window_duration , curtime ) tuple_batch = [ ] for ( tup , tm ) in self . current_tuples : tuple_batch . append ( tup ) self . processWindow ( window_info , tuple_batch ) self . _expire ( curtime )
2202	def ensure_app_cache_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_cache_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
384	def parse_darknet_ann_str_to_list ( annotations ) : r annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
13126	def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
5971	def MD ( dirname = 'MD' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'MD_POSRES/md.gro' ) kwargs . setdefault ( 'qname' , 'MD_GMX' ) return _setup_MD ( dirname , ** kwargs )
3246	def get_managed_policies ( group , ** conn ) : managed_policies = list_attached_group_managed_policies ( group [ 'GroupName' ] , ** conn ) managed_policy_names = [ ] for policy in managed_policies : managed_policy_names . append ( policy [ 'PolicyName' ] ) return managed_policy_names
9806	def deploy ( file , manager_path , check , dry_run ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
2300	def predict_undirected_graph ( self , data ) : graph = Graph ( ) for idx_i , i in enumerate ( data . columns ) : for idx_j , j in enumerate ( data . columns [ idx_i + 1 : ] ) : score = self . predict ( data [ i ] . values , data [ j ] . values ) if abs ( score ) > 0.001 : graph . add_edge ( i , j , weight = score ) return graph
12301	def instantiate ( repo , validator_name = None , filename = None , rulesfiles = None ) : default_validators = repo . options . get ( 'validator' , { } ) validators = { } if validator_name is not None : if validator_name in default_validators : validators = { validator_name : default_validators [ validator_name ] } else : validators = { validator_name : { 'files' : [ ] , 'rules' : { } , 'rules-files' : [ ] } } else : validators = default_validators if filename is not None : matching_files = repo . find_matching_files ( [ filename ] ) if len ( matching_files ) == 0 : print ( "Filename could not be found" , filename ) raise Exception ( "Invalid filename pattern" ) for v in validators : validators [ v ] [ 'files' ] = matching_files else : for v in validators : if 'files' not in validators [ v ] : validators [ v ] [ 'files' ] = [ ] elif len ( validators [ v ] [ 'files' ] ) > 0 : matching_files = repo . find_matching_files ( validators [ v ] [ 'files' ] ) validators [ v ] [ 'files' ] = matching_files if rulesfiles is not None : matching_files = repo . find_matching_files ( [ rulesfiles ] ) if len ( matching_files ) == 0 : print ( "Could not find matching rules files ({}) for {}" . format ( rulesfiles , v ) ) raise Exception ( "Invalid rules" ) for v in validators : validators [ v ] [ 'rules-files' ] = matching_files else : for v in validators : if 'rules-files' not in validators [ v ] : validators [ v ] [ 'rules-files' ] = [ ] else : rulesfiles = validators [ v ] [ 'rules-files' ] matching_files = repo . find_matching_files ( rulesfiles ) validators [ v ] [ 'rules-files' ] = matching_files return validators
2889	def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
3674	def draw_3d ( self , width = 300 , height = 500 , style = 'stick' , Hs = True ) : r try : import py3Dmol from IPython . display import display if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol AllChem . EmbedMultipleConfs ( mol ) mb = Chem . MolToMolBlock ( mol ) p = py3Dmol . view ( width = width , height = height ) p . addModel ( mb , 'sdf' ) p . setStyle ( { style : { } } ) p . zoomTo ( ) display ( p . show ( ) ) except : return 'py3Dmol, RDKit, and IPython are required for this feature.'
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
10765	def url ( self ) : if self . id is None : return '' return '{}/{}' . format ( strawpoll . API . _BASE_URL , self . id )
4478	def split_storage ( path , default = 'osfstorage' ) : path = norm_remote_path ( path ) for provider in KNOWN_PROVIDERS : if path . startswith ( provider + '/' ) : if six . PY3 : return path . split ( '/' , maxsplit = 1 ) else : return path . split ( '/' , 1 ) return ( default , path )
11212	def _hash ( secret : bytes , data : bytes , alg : str ) -> bytes : algorithm = get_algorithm ( alg ) return hmac . new ( secret , msg = data , digestmod = algorithm ) . digest ( )
10393	def workflow_all_aggregate ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) : results = { } bioprocess_nodes = list ( get_nodes_by_function ( graph , BIOPROCESS ) ) for bioprocess_node in tqdm ( bioprocess_nodes ) : subgraph = generate_mechanism ( graph , bioprocess_node , key = key ) try : results [ bioprocess_node ] = workflow_aggregate ( graph = subgraph , node = bioprocess_node , key = key , tag = tag , default_score = default_score , runs = runs , aggregator = aggregator ) except Exception : log . exception ( 'could not run on %' , bioprocess_node ) return results
2460	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True if validations . validate_pkg_desc ( text ) : doc . package . description = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Description' ) else : raise CardinalityError ( 'Package::Description' )
3959	def ensure_local_repo ( self ) : if os . path . exists ( self . managed_path ) : logging . debug ( 'Repo {} already exists' . format ( self . remote_path ) ) return logging . info ( 'Initiating clone of local repo {}' . format ( self . remote_path ) ) repo_path_parent = parent_dir ( self . managed_path ) if not os . path . exists ( repo_path_parent ) : os . makedirs ( repo_path_parent ) with git_error_handling ( ) : git . Repo . clone_from ( self . assemble_remote_path ( ) , self . managed_path )
7661	def trim ( self , start_time , end_time , strict = False ) : if end_time <= start_time : raise ParameterError ( 'end_time must be greater than start_time.' ) if self . duration is None : orig_time = start_time orig_duration = end_time - start_time warnings . warn ( "Annotation.duration is not defined, cannot check " "for temporal intersection, assuming the annotation " "is valid between start_time and end_time." ) else : orig_time = self . time orig_duration = self . duration if start_time > ( orig_time + orig_duration ) or ( end_time < orig_time ) : warnings . warn ( 'Time range defined by [start_time,end_time] does not ' 'intersect with the time range spanned by this annotation, ' 'the trimmed annotation will be empty.' ) trim_start = self . time trim_end = trim_start else : trim_start = max ( orig_time , start_time ) trim_end = min ( orig_time + orig_duration , end_time ) ann_trimmed = Annotation ( self . namespace , data = None , annotation_metadata = self . annotation_metadata , sandbox = self . sandbox , time = trim_start , duration = trim_end - trim_start ) for obs in self . data : obs_start = obs . time obs_end = obs_start + obs . duration if obs_start < trim_end and obs_end > trim_start : new_start = max ( obs_start , trim_start ) new_end = min ( obs_end , trim_end ) new_duration = new_end - new_start if ( ( not strict ) or ( new_start == obs_start and new_end == obs_end ) ) : ann_trimmed . append ( time = new_start , duration = new_duration , value = obs . value , confidence = obs . confidence ) if 'trim' not in ann_trimmed . sandbox . keys ( ) : ann_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ] ) else : ann_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ) return ann_trimmed
8077	def ellipsemode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . ellipsemode = mode return self . ellipsemode elif mode is None : return self . ellipsemode else : raise ShoebotError ( _ ( "ellipsemode: invalid input" ) )
580	def getSpec ( cls ) : spec = { "description" : IdentityRegion . __doc__ , "singleNodeOnly" : True , "inputs" : { "in" : { "description" : "The input vector." , "dataType" : "Real32" , "count" : 0 , "required" : True , "regionLevel" : False , "isDefaultInput" : True , "requireSplitterMap" : False } , } , "outputs" : { "out" : { "description" : "A copy of the input vector." , "dataType" : "Real32" , "count" : 0 , "regionLevel" : True , "isDefaultOutput" : True } , } , "parameters" : { "dataWidth" : { "description" : "Size of inputs" , "accessMode" : "Read" , "dataType" : "UInt32" , "count" : 1 , "constraints" : "" } , } , } return spec
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
2298	def predict_proba ( self , x , y = None , ** kwargs ) : if self . clf is None : raise ValueError ( "Model has to be trained before making predictions." ) if x is pandas . Series : input_ = self . featurize_row ( x . iloc [ 0 ] , x . iloc [ 1 ] ) . reshape ( ( 1 , - 1 ) ) elif x is pandas . DataFrame : input_ = np . array ( [ self . featurize_row ( x . iloc [ 0 ] , x . iloc [ 1 ] ) for row in x ] ) elif y is not None : input_ = self . featurize_row ( x , y ) . reshape ( ( 1 , - 1 ) ) else : raise TypeError ( "DataType not understood." ) return self . clf . predict ( input_ )
562	def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )
2858	def bulkread ( self , data = [ ] , lengthR = 'None' , readmode = 1 ) : if ( 1 > lengthR > 65536 ) | ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) if ( lengthR == 'None' ) & ( readmode == 1 ) : lengthR = len ( data ) commandW = 0x10 | ( self . lsbfirst << 3 ) | self . write_clock_ve lengthW = len ( data ) - 1 len_lowW = ( lengthW ) & 0xFF len_highW = ( ( lengthW ) >> 8 ) & 0xFF commandR = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) length = lengthR if lengthR % 2 == 1 : length += 1 length = length / 2 lenremain = lengthR - length len_lowR = ( length - 1 ) & 0xFF len_highR = ( ( length - 1 ) >> 8 ) & 0xFF logger . debug ( 'SPI bulkread with write command {0:2X}.' . format ( commandW ) ) logger . debug ( 'and read command {0:2X}.' . format ( commandR ) ) self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( commandW , len_lowW , len_highW ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data ) ) ) self . _ft232h . _write ( str ( bytearray ( ( commandR , len_lowR , len_highR ) ) ) ) payload1 = self . _ft232h . _poll_read ( length ) self . _ft232h . _write ( str ( bytearray ( ( commandR , len_lowR , len_highR ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
2973	def from_dict ( values ) : try : containers = values [ 'containers' ] parsed_containers = { } for name , container_dict in containers . items ( ) : try : for cnt in BlockadeContainerConfig . from_dict ( name , container_dict ) : if cnt . container_name : cname = cnt . container_name existing = [ c for c in parsed_containers . values ( ) if c . container_name == cname ] if existing : raise BlockadeConfigError ( "Duplicate 'container_name' definition: %s" % ( cname ) ) parsed_containers [ cnt . name ] = cnt except Exception as err : raise BlockadeConfigError ( "Container '%s' config problem: %s" % ( name , err ) ) network = values . get ( 'network' ) if network : defaults = _DEFAULT_NETWORK_CONFIG . copy ( ) defaults . update ( network ) network = defaults else : network = _DEFAULT_NETWORK_CONFIG . copy ( ) return BlockadeConfig ( parsed_containers , network = network ) except KeyError as err : raise BlockadeConfigError ( "Config missing value: " + str ( err ) ) except Exception as err : raise BlockadeConfigError ( "Failed to load config: " + str ( err ) )
4598	def read_from ( self , data , pad = 0 ) : for i in range ( self . BEGIN , self . END + 1 ) : index = self . index ( i , len ( data ) ) yield pad if index is None else data [ index ]
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
5746	def asn ( self , ip , announce_date = None ) : assignations , announce_date , _ = self . run ( ip , announce_date ) return next ( ( assign for assign in assignations if assign is not None ) , None ) , announce_date
13431	def admin_link_move_up ( obj , link_text = 'up' ) : if obj . rank == 1 : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank - 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
8002	def __from_xml ( self , xmlnode ) : self . __logger . debug ( "Converting jabber:iq:register element from XML" ) if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:iq:register element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != REGISTER_NS or xmlnode . name != "query" : raise ValueError ( "XML node is not a jabber:iq:register element" ) for element in xml_element_iter ( xmlnode . children ) : ns = get_node_ns_uri ( element ) if ns == DATAFORM_NS and element . name == "x" and not self . form : self . form = Form ( element ) elif ns != REGISTER_NS : continue name = element . name if name == "instructions" and not self . instructions : self . instructions = from_utf8 ( element . getContent ( ) ) elif name == "registered" : self . registered = True elif name == "remove" : self . remove = True elif name in legacy_fields and not getattr ( self , name ) : value = from_utf8 ( element . getContent ( ) ) if value is None : value = u"" self . __logger . debug ( u"Setting legacy field %r to %r" % ( name , value ) ) setattr ( self , name , value )
11758	def dpll ( clauses , symbols , model ) : "See if the clauses are true in a partial model." unknown_clauses = [ ] for c in clauses : val = pl_true ( c , model ) if val == False : return False if val != True : unknown_clauses . append ( c ) if not unknown_clauses : return model P , value = find_pure_symbol ( symbols , unknown_clauses ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , value = find_unit_clause ( clauses , model ) if P : return dpll ( clauses , removeall ( P , symbols ) , extend ( model , P , value ) ) P , symbols = symbols [ 0 ] , symbols [ 1 : ] return ( dpll ( clauses , symbols , extend ( model , P , True ) ) or dpll ( clauses , symbols , extend ( model , P , False ) ) )
2818	def convert_padding ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting padding...' ) if params [ 'mode' ] == 'constant' : if params [ 'value' ] != 0.0 : raise AssertionError ( 'Cannot convert non-zero padding' ) if names : tf_name = 'PADD' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) padding_name = tf_name padding_layer = keras . layers . ZeroPadding2D ( padding = ( ( params [ 'pads' ] [ 2 ] , params [ 'pads' ] [ 6 ] ) , ( params [ 'pads' ] [ 3 ] , params [ 'pads' ] [ 7 ] ) ) , name = padding_name ) layers [ scope_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) elif params [ 'mode' ] == 'reflect' : def target_layer ( x , pads = params [ 'pads' ] ) : layer = tf . pad ( x , [ [ 0 , 0 ] , [ 0 , 0 ] , [ pads [ 2 ] , pads [ 6 ] ] , [ pads [ 3 ] , pads [ 7 ] ] ] , 'REFLECT' ) return layer lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : from . import geometric as iaa_geometric k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
12943	def diff ( firstObj , otherObj , includeMeta = False ) : if not isIndexedRedisModel ( firstObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( firstObj ) . __name__ , ) ) if not isIndexedRedisModel ( otherObj ) : raise ValueError ( 'Type < %s > does not extend IndexedRedisModel.' % ( type ( otherObj ) . __name__ , ) ) firstObj . validateModel ( ) otherObj . validateModel ( ) if getattr ( firstObj , 'FIELDS' ) != getattr ( otherObj , 'FIELDS' ) : raise ValueError ( 'Cannot compare < %s > and < %s > . Must be same model OR have equal FIELDS.' % ( firstObj . __class__ , otherObj . __class__ ) ) diffFields = { } for thisField in firstObj . FIELDS : thisFieldStr = str ( thisField ) firstVal = object . __getattribute__ ( firstObj , thisFieldStr ) otherVal = object . __getattribute__ ( otherObj , thisFieldStr ) if firstVal != otherVal : diffFields [ thisFieldStr ] = ( ( firstVal , otherVal ) ) if includeMeta : firstPk = firstObj . getPk ( ) otherPk = otherObj . getPk ( ) if firstPk != otherPk : diffFields [ '_id' ] = ( firstPk , otherPk ) return diffFields
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
7489	def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : for line in invcf : if line . split ( ) [ 0 ] == "#CHROM" : names_col = line . split ( ) . index ( "FORMAT" ) + 1 names = line . split ( ) [ names_col : ] LOGGER . debug ( "Got names - %s" , names ) break print ( "wat" ) except Exception : print ( "wat" )
3456	def build_hugo_md ( filename , tag , bump ) : header = [ '+++\n' , 'date = "{}"\n' . format ( date . today ( ) . isoformat ( ) ) , 'title = "{}"\n' . format ( tag ) , 'author = "The COBRApy Team"\n' , 'release = "{}"\n' . format ( bump ) , '+++\n' , '\n' ] with open ( filename , "r" ) as file_h : content = insert_break ( file_h . readlines ( ) ) header . extend ( content ) with open ( filename , "w" ) as file_h : file_h . writelines ( header )
6384	def mean_pairwise_similarity ( collection , metric = sim , mean_func = hmean , symmetric = False ) : if not callable ( mean_func ) : raise ValueError ( 'mean_func must be a function' ) if not callable ( metric ) : raise ValueError ( 'metric must be a function' ) if hasattr ( collection , 'split' ) : collection = collection . split ( ) if not hasattr ( collection , '__iter__' ) : raise ValueError ( 'collection is neither a string nor iterable type' ) elif len ( collection ) < 2 : raise ValueError ( 'collection has fewer than two members' ) collection = list ( collection ) pairwise_values = [ ] for i in range ( len ( collection ) ) : for j in range ( i + 1 , len ( collection ) ) : pairwise_values . append ( metric ( collection [ i ] , collection [ j ] ) ) if symmetric : pairwise_values . append ( metric ( collection [ j ] , collection [ i ] ) ) return mean_func ( pairwise_values )
568	def _matchReportKeys ( reportKeyREs = [ ] , allReportKeys = [ ] ) : matchingReportKeys = [ ] for keyRE in reportKeyREs : matchObj = re . compile ( keyRE ) found = False for keyName in allReportKeys : match = matchObj . match ( keyName ) if match and match . end ( ) == len ( keyName ) : matchingReportKeys . append ( keyName ) found = True if not found : raise _BadKeyError ( keyRE ) return matchingReportKeys
8906	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
3764	def Parachor ( MW , rhol , rhog , sigma ) : r rhol , rhog = rhol * 1000. , rhog * 1000. return sigma ** 0.25 * MW / ( rhol - rhog )
4363	def _spawn_heartbeat ( self ) : self . spawn ( self . _heartbeat ) self . spawn ( self . _heartbeat_timeout )
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
3802	def calculate_P ( self , T , P , method ) : r if method == DIPPR_9G : kl = self . T_dependent_property ( T ) kl = DIPPR9G ( T , P , self . Tc , self . Pc , kl ) elif method == MISSENARD : kl = self . T_dependent_property ( T ) kl = Missenard ( T , P , self . Tc , self . Pc , kl ) elif method == COOLPROP : kl = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kl = self . interpolate_P ( T , P , method ) return kl
11767	def weighted_sample_with_replacement ( seq , weights , n ) : sample = weighted_sampler ( seq , weights ) return [ sample ( ) for s in range ( n ) ]
10764	def _random_token ( self , bits = 128 ) : alphabet = string . ascii_letters + string . digits + '-_' num_letters = int ( math . ceil ( bits / 6.0 ) ) return '' . join ( random . choice ( alphabet ) for i in range ( num_letters ) )
1763	def pop_bytes ( self , nbytes , force = False ) : data = self . read_bytes ( self . STACK , nbytes , force = force ) self . STACK += nbytes return data
6408	def lmean ( nums ) : r if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
13409	def show ( self ) : self . parent . addLayout ( self . _logSelectLayout ) self . menuCount += 1 self . _connectSlots ( )
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
4822	def get_course_details ( self , course_id ) : try : return self . client . course ( course_id ) . get ( ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to retrieve course enrollment details for course [%s] due to: [%s]' , course_id , str ( exc ) ) return { }
9492	def compile_bytecode ( code : list ) -> bytes : bc = b"" for i , op in enumerate ( code ) : try : if isinstance ( op , _PyteOp ) or isinstance ( op , _PyteAugmentedComparator ) : bc_op = op . to_bytes ( bc ) elif isinstance ( op , int ) : bc_op = op . to_bytes ( 1 , byteorder = "little" ) elif isinstance ( op , bytes ) : bc_op = op else : raise CompileError ( "Could not compile code of type {}" . format ( type ( op ) ) ) bc += bc_op except Exception as e : print ( "Fatal compiliation error on operator {i} ({op})." . format ( i = i , op = op ) ) raise e return bc
13285	def clone ( src , dst_path , skip_globals , skip_dimensions , skip_variables ) : if os . path . exists ( dst_path ) : os . unlink ( dst_path ) dst = netCDF4 . Dataset ( dst_path , 'w' ) for attname in src . ncattrs ( ) : if attname not in skip_globals : setattr ( dst , attname , getattr ( src , attname ) ) unlimdim = None unlimdimname = False for dimname , dim in src . dimensions . items ( ) : if dimname in skip_dimensions : continue if dim . isunlimited ( ) : unlimdim = dim unlimdimname = dimname dst . createDimension ( dimname , None ) else : dst . createDimension ( dimname , len ( dim ) ) for varname , ncvar in src . variables . items ( ) : if varname in skip_variables : continue hasunlimdim = False if unlimdimname and unlimdimname in ncvar . dimensions : hasunlimdim = True filler = None if hasattr ( ncvar , '_FillValue' ) : filler = ncvar . _FillValue if ncvar . chunking == "contiguous" : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler ) else : var = dst . createVariable ( varname , ncvar . dtype , ncvar . dimensions , fill_value = filler , chunksizes = ncvar . chunking ( ) ) for attname in ncvar . ncattrs ( ) : if attname == '_FillValue' : continue else : setattr ( var , attname , getattr ( ncvar , attname ) ) nchunk = 1000 if hasunlimdim : if nchunk : start = 0 stop = len ( unlimdim ) step = nchunk if step < 1 : step = 1 for n in range ( start , stop , step ) : nmax = n + nchunk if nmax > len ( unlimdim ) : nmax = len ( unlimdim ) idata = ncvar [ n : nmax ] var [ n : nmax ] = idata else : idata = ncvar [ : ] var [ 0 : len ( unlimdim ) ] = idata else : idata = ncvar [ : ] var [ : ] = idata dst . sync ( ) src . close ( ) dst . close ( )
3932	def _auth_with_code ( session , authorization_code ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'code' : authorization_code , 'grant_type' : 'authorization_code' , 'redirect_uri' : 'urn:ietf:wg:oauth:2.0:oob' , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ] , res [ 'refresh_token' ]
6497	def index ( self , doc_type , sources , ** kwargs ) : try : actions = [ ] for source in sources : self . _check_mappings ( doc_type , source ) id_ = source [ 'id' ] if 'id' in source else None log . debug ( "indexing %s object with id %s" , doc_type , id_ ) action = { "_index" : self . index_name , "_type" : doc_type , "_id" : id_ , "_source" : source } actions . append ( action ) _ , indexing_errors = bulk ( self . _es , actions , ** kwargs ) if indexing_errors : ElasticSearchEngine . log_indexing_error ( indexing_errors ) except Exception as ex : log . exception ( "error while indexing - %s" , str ( ex ) ) raise
2164	def list_facts ( self , pk = None , ** kwargs ) : res = self . get ( pk = pk , ** kwargs ) url = self . endpoint + '%d/%s/' % ( res [ 'id' ] , 'ansible_facts' ) return client . get ( url , params = { } ) . json ( )
11011	def get_collection_endpoint ( cls ) : return cls . Meta . collection_endpoint if cls . Meta . collection_endpoint is not None else cls . __name__ . lower ( ) + "s/"
13752	def _reference_table ( cls , ref_table ) : cols = [ ( sa . Column ( ) , refcol ) for refcol in ref_table . primary_key ] for col , refcol in cols : setattr ( cls , "%s_%s" % ( ref_table . name , refcol . name ) , col ) cls . __table__ . append_constraint ( sa . ForeignKeyConstraint ( * zip ( * cols ) ) )
10179	def delete ( self , start_date = None , end_date = None ) : aggs_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . aggregation_doc_type ) . extra ( _source = False ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : aggs_query = aggs_query . filter ( 'range' , timestamp = range_args ) bookmarks_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range_args : bookmarks_query = bookmarks_query . filter ( 'range' , date = range_args ) def _delete_actions ( ) : for query in ( aggs_query , bookmarks_query ) : affected_indices = set ( ) for doc in query . scan ( ) : affected_indices . add ( doc . meta . index ) yield dict ( _index = doc . meta . index , _op_type = 'delete' , _id = doc . meta . id , _type = doc . meta . doc_type ) current_search_client . indices . flush ( index = ',' . join ( affected_indices ) , wait_if_ongoing = True ) bulk ( self . client , _delete_actions ( ) , refresh = True )
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
12457	def iteritems ( data , ** kwargs ) : return iter ( data . items ( ** kwargs ) ) if IS_PY3 else data . iteritems ( ** kwargs )
9073	def _get_connection ( cls , connection : Optional [ str ] = None ) -> str : return get_connection ( cls . module_name , connection = connection )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
11907	def four_blocks ( topleft , topright , bottomleft , bottomright ) : return vstack ( hstack ( topleft , topright ) , hstack ( bottomleft , bottomright ) )
2324	def predict ( self , x , * args , ** kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , ** kwargs ) else : return self . predict_proba ( x , * args , ** kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , ** kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , ** kwargs )
2963	def get_source_chains ( self , blockade_id ) : result = { } if not blockade_id : raise ValueError ( "invalid blockade_id" ) lines = self . get_chain_rules ( "FORWARD" ) for line in lines : parts = line . split ( ) if len ( parts ) < 4 : continue try : partition_index = parse_partition_index ( blockade_id , parts [ 0 ] ) except ValueError : continue source = parts [ 3 ] if source : result [ source ] = partition_index return result
1513	def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
3557	def find_device ( cls , timeout_sec = TIMEOUT_SEC ) : return get_provider ( ) . find_device ( service_uuids = cls . ADVERTISED , timeout_sec = timeout_sec )
591	def compute ( self , inputs , outputs ) : if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : import hotshot if self . _iterations == 10 : print "\n Collecting and sorting internal node profiling stats generated by hotshot..." stats = hotshot . stats . load ( "hotshot.stats" ) stats . strip_dirs ( ) stats . sort_stats ( 'time' , 'calls' ) stats . print_stats ( ) if self . _profileObj is None : print "\n Preparing to capture profile using hotshot..." if os . path . exists ( 'hotshot.stats' ) : os . remove ( 'hotshot.stats' ) self . _profileObj = hotshot . Profile ( "hotshot.stats" , 1 , 1 ) self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) else : self . _compute ( inputs , outputs )
12470	def copy_w_plus ( src , dst ) : dst_ext = get_extension ( dst ) dst_pre = remove_ext ( dst ) while op . exists ( dst_pre + dst_ext ) : dst_pre += '+' shutil . copy ( src , dst_pre + dst_ext ) return dst_pre + dst_ext
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , ** kw ) : return assertion . test ( subject , expected , * args , ** kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
1954	def symbolic_run_get_cons ( trace ) : m2 = Manticore . linux ( prog , workspace_url = 'mem:' ) f = Follower ( trace ) m2 . verbosity ( VERBOSITY ) m2 . register_plugin ( f ) def on_term_testcase ( mcore , state , stateid , err ) : with m2 . locked_context ( ) as ctx : readdata = [ ] for name , fd , data in state . platform . syscall_trace : if name in ( '_receive' , '_read' ) and fd == 0 : readdata . append ( data ) ctx [ 'readdata' ] = readdata ctx [ 'constraints' ] = list ( state . constraints . constraints ) m2 . subscribe ( 'will_terminate_state' , on_term_testcase ) m2 . run ( ) constraints = m2 . context [ 'constraints' ] datas = m2 . context [ 'readdata' ] return constraints , datas
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
9445	def group_call ( self , call_params ) : path = '/' + self . api_version + '/GroupCall/' method = 'POST' return self . request ( path , method , call_params )
2004	def function_selector ( method_name_and_signature ) : s = sha3 . keccak_256 ( ) s . update ( method_name_and_signature . encode ( ) ) return bytes ( s . digest ( ) [ : 4 ] )
9434	def load_savefile ( input_file , layers = 0 , verbose = False , lazy = False ) : global VERBOSE old_verbose = VERBOSE VERBOSE = verbose __TRACE__ ( '[+] attempting to load {:s}' , ( input_file . name , ) ) header = _load_savefile_header ( input_file ) if __validate_header__ ( header ) : __TRACE__ ( '[+] found valid header' ) if lazy : packets = _generate_packets ( input_file , header , layers ) __TRACE__ ( '[+] created packet generator' ) else : packets = _load_packets ( input_file , header , layers ) __TRACE__ ( '[+] loaded {:d} packets' , ( len ( packets ) , ) ) sfile = pcap_savefile ( header , packets ) __TRACE__ ( '[+] finished loading savefile.' ) else : __TRACE__ ( '[!] invalid savefile' ) sfile = None VERBOSE = old_verbose return sfile
13818	def _ConvertMessage ( value , message ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : _ConvertWrapperMessage ( value , message ) elif full_name in _WKTJSONMETHODS : _WKTJSONMETHODS [ full_name ] [ 1 ] ( value , message ) else : _ConvertFieldValuePair ( value , message )
5658	def _validate_no_null_values ( self ) : for table in DB_TABLE_NAMES : null_not_ok_warning = "Null values in must-have columns in table {table}" . format ( table = table ) null_warn_warning = "Null values in good-to-have columns in table {table}" . format ( table = table ) null_not_ok_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_NOT_OK [ table ] null_warn_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_OK_BUT_WARN [ table ] df = self . gtfs . get_table ( table ) for warning , fields in zip ( [ null_not_ok_warning , null_warn_warning ] , [ null_not_ok_fields , null_warn_fields ] ) : null_unwanted_df = df [ fields ] rows_having_null = null_unwanted_df . isnull ( ) . any ( 1 ) if sum ( rows_having_null ) > 0 : rows_having_unwanted_null = df [ rows_having_null . values ] self . warnings_container . add_warning ( warning , rows_having_unwanted_null , len ( rows_having_unwanted_null ) )
2664	def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
11271	def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
9239	def timestring_to_datetime ( timestring ) : with warnings . catch_warnings ( ) : warnings . filterwarnings ( "ignore" , category = UnicodeWarning ) result = dateutil_parser ( timestring ) return result
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) config . commit ( ) from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
6007	def load_image ( image_path , image_hdu , pixel_scale ) : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = image_path , hdu = image_hdu , pixel_scale = pixel_scale )
4372	def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
9136	def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
1429	def convert_args_dict_to_list ( dict_extra_args ) : list_extra_args = [ ] if 'component_parallelism' in dict_extra_args : list_extra_args += [ "--component_parallelism" , ',' . join ( dict_extra_args [ 'component_parallelism' ] ) ] if 'runtime_config' in dict_extra_args : list_extra_args += [ "--runtime_config" , ',' . join ( dict_extra_args [ 'runtime_config' ] ) ] if 'container_number' in dict_extra_args : list_extra_args += [ "--container_number" , ',' . join ( dict_extra_args [ 'container_number' ] ) ] if 'dry_run' in dict_extra_args and dict_extra_args [ 'dry_run' ] : list_extra_args += [ '--dry_run' ] if 'dry_run_format' in dict_extra_args : list_extra_args += [ '--dry_run_format' , dict_extra_args [ 'dry_run_format' ] ] return list_extra_args
8185	def offset ( self , node ) : x = self . x + node . x - _ctx . WIDTH / 2 y = self . y + node . y - _ctx . HEIGHT / 2 return x , y
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
4383	def is_denied ( self , role , method , resource ) : return ( role , method , resource ) in self . _denied
8313	def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 row_y += cell_h previous_cell_w = cell_w _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
11434	def _tag_matches_pattern ( tag , pattern ) : for char1 , char2 in zip ( tag , pattern ) : if char2 not in ( '%' , char1 ) : return False return True
10806	def validate ( cls , policy ) : return policy in [ cls . PUBLIC , cls . MEMBERS , cls . ADMINS ]
10018	def application_exists ( self ) : response = self . ebs . describe_applications ( application_names = [ self . app_name ] ) return len ( response [ 'DescribeApplicationsResponse' ] [ 'DescribeApplicationsResult' ] [ 'Applications' ] ) > 0
13418	def start ( info ) : cmd = options . paved . django . runserver if cmd == 'runserver_plus' : try : import django_extensions except ImportError : info ( "Could not import django_extensions. Using default runserver." ) cmd = 'runserver' port = options . paved . django . runserver_port if port : cmd = '%s %s' % ( cmd , port ) call_manage ( cmd )
10334	def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
737	def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] with FileRecordStream ( outputFile , write = True , fields = fields ) as o : files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] while not all ( r is None for r in records ) : indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] r = min ( records , key = itemgetter ( * key ) ) o . appendRecord ( r ) index = records . index ( r ) records [ index ] = files [ index ] . getNextRecord ( ) for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )
12336	def pip ( self , package_names , raise_on_error = True ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "pip install -U %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
8774	def _load_worker_plugin_with_module ( self , module , version ) : classes = inspect . getmembers ( module , inspect . isclass ) loaded = 0 for cls_name , cls in classes : if hasattr ( cls , 'versions' ) : if version not in cls . versions : continue else : continue if issubclass ( cls , base_worker . QuarkAsyncPluginBase ) : LOG . debug ( "Loading plugin %s" % cls_name ) plugin = cls ( ) self . plugins . append ( plugin ) loaded += 1 LOG . debug ( "Found %d possible plugins and loaded %d" % ( len ( classes ) , loaded ) )
10694	def hex_to_rgb ( _hex ) : _hex = _hex . strip ( '#' ) n = len ( _hex ) // 3 if len ( _hex ) == 3 : r = int ( _hex [ : n ] * 2 , 16 ) g = int ( _hex [ n : 2 * n ] * 2 , 16 ) b = int ( _hex [ 2 * n : 3 * n ] * 2 , 16 ) else : r = int ( _hex [ : n ] , 16 ) g = int ( _hex [ n : 2 * n ] , 16 ) b = int ( _hex [ 2 * n : 3 * n ] , 16 ) return r , g , b
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
3403	def find_boundary_types ( model , boundary_type , external_compartment = None ) : if not model . boundary : LOGGER . warning ( "There are no boundary reactions in this model. " "Therefore specific types of boundary reactions such " "as 'exchanges', 'demands' or 'sinks' cannot be " "identified." ) return [ ] if external_compartment is None : external_compartment = find_external_compartment ( model ) return model . reactions . query ( lambda r : is_boundary_type ( r , boundary_type , external_compartment ) )
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
11989	def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
10141	def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
2124	def associate_always_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , ** kwargs )
7613	def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . constants . arenas : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/arenas/arena{}.png' . format ( i . arena_id )
3532	def mixpanel ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MixpanelNode ( )
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
2854	def mpsse_gpio ( self ) : level_low = chr ( self . _level & 0xFF ) level_high = chr ( ( self . _level >> 8 ) & 0xFF ) dir_low = chr ( self . _direction & 0xFF ) dir_high = chr ( ( self . _direction >> 8 ) & 0xFF ) return str ( bytearray ( ( 0x80 , level_low , dir_low , 0x82 , level_high , dir_high ) ) )
9189	def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
12335	def apt ( self , package_names , raise_on_error = False ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "apt-get install -y %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
2100	def read ( self , * args , ** kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , ** kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
12383	def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
3002	def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
5340	def __remove_dashboard_menu ( self , kibiter_major ) : logger . info ( "Removing old dashboard menu, if any" ) if kibiter_major == "6" : metadashboard = ".kibana/doc/metadashboard" else : metadashboard = ".kibana/metadashboard/main" menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , metadashboard ) self . grimoire_con . delete ( menu_url )
1369	def create_tar ( tar_filename , files , config_dir , config_files ) : with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config_dir ) : tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config_dir ) for filename in config_files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
5724	def verify_valid_gdb_subprocess ( self ) : if not self . gdb_process : raise NoGdbProcessError ( "gdb process is not attached" ) elif self . gdb_process . poll ( ) is not None : raise NoGdbProcessError ( "gdb process has already finished with return code: %s" % str ( self . gdb_process . poll ( ) ) )
11834	def connect ( self , A , B , distance = 1 ) : self . connect1 ( A , B , distance ) if not self . directed : self . connect1 ( B , A , distance )
4689	def init_aes ( shared_secret , nonce ) : " Shared Secret " ss = hashlib . sha512 ( unhexlify ( shared_secret ) ) . digest ( ) " Seed " seed = bytes ( str ( nonce ) , "ascii" ) + hexlify ( ss ) seed_digest = hexlify ( hashlib . sha512 ( seed ) . digest ( ) ) . decode ( "ascii" ) " AES " key = unhexlify ( seed_digest [ 0 : 64 ] ) iv = unhexlify ( seed_digest [ 64 : 96 ] ) return AES . new ( key , AES . MODE_CBC , iv )
5909	def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
7817	def remove_handler ( self , handler ) : with self . lock : if handler in self . handlers : self . handlers . remove ( handler ) self . _update_handlers ( )
8103	def load_profiles ( self ) : _profiles = { } for name , klass in inspect . getmembers ( profiles ) : if inspect . isclass ( klass ) and name . endswith ( 'Profile' ) and name != 'TuioProfile' : profile = klass ( ) _profiles [ profile . address ] = profile try : setattr ( self , profile . list_label , profile . objs ) except AttributeError : continue self . manager . add ( self . callback , profile . address ) return _profiles
1315	def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
1156	def go_str ( value ) : io = StringIO . StringIO ( ) io . write ( '"' ) for c in value : if c in _ESCAPES : io . write ( _ESCAPES [ c ] ) elif c in _SIMPLE_CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '"' ) return io . getvalue ( )
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , ** ranker_params ) return scores , sim_mat
8034	def write ( self , text , newline = False ) : if not self . isatty : self . fobj . write ( '%s\n' % text ) return msg_len = len ( text ) self . max_len = max ( self . max_len , msg_len ) self . fobj . write ( "\r%-*s" % ( self . max_len , text ) ) if newline or not self . isatty : self . fobj . write ( '\n' ) self . max_len = 0
13553	def _delete_resource ( self , url ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . deleteURL ( url , headers ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
5399	def get_filtered_normalized_events ( self ) : user_image = google_v2_operations . get_action_image ( self . _op , _ACTION_USER_COMMAND ) need_ok = google_v2_operations . is_success ( self . _op ) events = { } for event in google_v2_operations . get_events ( self . _op ) : if self . _filter ( event ) : continue mapped , match = self . _map ( event ) name = mapped [ 'name' ] if name == 'ok' : if not need_ok or 'ok' in events : continue if name == 'pulling-image' : if match . group ( 1 ) != user_image : continue events [ name ] = mapped return sorted ( events . values ( ) , key = operator . itemgetter ( 'start-time' ) )
5782	def select_read ( self , timeout = None ) : if len ( self . _decrypted_bytes ) > 0 : return True read_ready , _ , _ = select . select ( [ self . _socket ] , [ ] , [ ] , timeout ) return len ( read_ready ) > 0
396	def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( "number of actions should equal to number of probabilities." ) return np . random . choice ( action_list , p = probs )
7513	def locichunk ( args ) : data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args hslice = [ start , start + optim ] co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] io5 = h5py . File ( data . clust_database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] for iloc in keep : edg = aedge [ iloc ] args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter_pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter_singles ( * args ) store . append ( outstr ) tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) io5 . close ( ) co5 . close ( ) return samplecov , locuscov , start
9683	def toggle_laser ( self , state ) : a = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 10e-3 ) if state : b = self . cnxn . xfer ( [ 0x02 ] ) [ 0 ] else : b = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x03 else False
2158	def _echo_method ( self , method ) : @ functools . wraps ( method ) def func ( * args , ** kwargs ) : if getattr ( method , 'deprecated' , False ) : debug . log ( 'This method is deprecated in Tower 3.0.' , header = 'warning' ) result = method ( * args , ** kwargs ) color_info = { } if isinstance ( result , dict ) and 'changed' in result : if result [ 'changed' ] : color_info [ 'fg' ] = 'yellow' else : color_info [ 'fg' ] = 'green' format = getattr ( self , '_format_%s' % ( getattr ( method , 'format_freezer' , None ) or settings . format ) ) output = format ( result ) secho ( output , ** color_info ) return func
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
9825	def groups ( ctx , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiment_groups ( username = user , project_name = project_name , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment groups for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiment groups for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiment groups found for project `{}/{}`.' . format ( user , project_name ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiment groups:" ) objects . pop ( 'project' , None ) objects . pop ( 'user' , None ) dict_tabulate ( objects , is_list_dict = True )
5227	def to_str ( data : dict , fmt = '{key}={value}' , sep = ', ' , public_only = True ) -> str : if public_only : keys = list ( filter ( lambda vv : vv [ 0 ] != '_' , data . keys ( ) ) ) else : keys = list ( data . keys ( ) ) return '{' + sep . join ( [ to_str ( data = v , fmt = fmt , sep = sep ) if isinstance ( v , dict ) else fstr ( fmt = fmt , key = k , value = v ) for k , v in data . items ( ) if k in keys ] ) + '}'
2323	def forward ( self , pred , target ) : loss = th . FloatTensor ( [ 0 ] ) for i in range ( 1 , self . moments ) : mk_pred = th . mean ( th . pow ( pred , i ) , 0 ) mk_tar = th . mean ( th . pow ( target , i ) , 0 ) loss . add_ ( th . mean ( ( mk_pred - mk_tar ) ** 2 ) ) return loss
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
7726	def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( "Bad status code" ) self . code = code
6503	def find_matches ( strings , words , length_hoped ) : lower_words = [ w . lower ( ) for w in words ] def has_match ( string ) : lower_string = string . lower ( ) for test_word in lower_words : if test_word in lower_string : return True return False shortened_strings = [ textwrap . wrap ( s ) for s in strings ] short_string_list = list ( chain . from_iterable ( shortened_strings ) ) matches = [ ms for ms in short_string_list if has_match ( ms ) ] cumulative_len = 0 break_at = None for idx , match in enumerate ( matches ) : cumulative_len += len ( match ) if cumulative_len >= length_hoped : break_at = idx break return matches [ 0 : break_at ]
2429	def reset_document ( self ) : self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
3853	def get_conv_name ( conv , truncate = False , show_unread = False ) : num_unread = len ( [ conv_event for conv_event in conv . unread_events if isinstance ( conv_event , hangups . ChatMessageEvent ) and not conv . get_user ( conv_event . user_id ) . is_self ] ) if show_unread and num_unread > 0 : postfix = ' ({})' . format ( num_unread ) else : postfix = '' if conv . name is not None : return conv . name + postfix else : participants = sorted ( ( user for user in conv . users if not user . is_self ) , key = lambda user : user . id_ ) names = [ user . first_name for user in participants ] if not participants : return "Empty Conversation" + postfix if len ( participants ) == 1 : return participants [ 0 ] . full_name + postfix elif truncate and len ( participants ) > 2 : return ( ', ' . join ( names [ : 2 ] + [ '+{}' . format ( len ( names ) - 2 ) ] ) + postfix ) else : return ', ' . join ( names ) + postfix
10946	def do_internal_run ( self ) : if not self . save_J : raise RuntimeError ( 'self.save_J=True required for do_internal_run()' ) if not np . all ( self . _has_saved_J ) : raise RuntimeError ( 'J, JTJ have not been pre-computed. Call do_run_1 or do_run_2' ) self . _do_run ( mode = 'internal' )
1425	def initialize ( self , config , context ) : self . logger . info ( "Initializing PulsarSpout with the following" ) self . logger . info ( "Component-specific config: \n%s" % str ( config ) ) self . logger . info ( "Context: \n%s" % str ( context ) ) self . emit_count = 0 self . ack_count = 0 self . fail_count = 0 if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : self . logger . fatal ( "Need to specify both serviceUrl and topicName" ) self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) self . topic = str ( config [ PulsarSpout . topicName ] ) mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) else : self . acking_timeout = 30000 if PulsarSpout . receiveTimeoutMs in config : self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] else : self . receive_timeout_ms = 10 if PulsarSpout . deserializer in config : self . deserializer = config [ PulsarSpout . deserializer ] if not callable ( self . deserializer ) : self . logger . fatal ( "Pulsar Message Deserializer needs to be callable" ) else : self . deserializer = self . default_deserializer self . logConfFileName = GenerateLogConfig ( context ) self . logger . info ( "Generated LogConf at %s" % self . logConfFileName ) self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) self . logger . info ( "Setup Client with cluster %s" % self . pulsar_cluster ) try : self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , consumer_type = pulsar . ConsumerType . Failover , unacked_messages_timeout_ms = self . acking_timeout ) except Exception as e : self . logger . fatal ( "Pulsar client subscription failed: %s" % str ( e ) ) self . logger . info ( "Subscribed to topic %s" % self . topic )
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : r if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] self . user_methods_P = user_methods_P self . forced_P = forced_P if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
932	def run ( self , inputRecord ) : predictionNumber = self . _numPredictions self . _numPredictions += 1 result = opf_utils . ModelResult ( predictionNumber = predictionNumber , rawInput = inputRecord ) return result
7568	def comp ( seq ) : return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
10662	def element_mass_fraction ( compound , element ) : coeff = stoichiometry_coefficient ( compound , element ) if coeff == 0.0 : return 0.0 formula_mass = molar_mass ( compound ) element_mass = molar_mass ( element ) return coeff * element_mass / formula_mass
12143	async def _push ( self , * args , ** kwargs ) : self . _data . append ( ( args , kwargs ) ) if self . _future is not None : future , self . _future = self . _future , None future . set_result ( True )
8171	def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
4519	def set_project ( self , project ) : def visit ( x ) : set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
7449	def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
13045	def f_i18n_iso ( isocode , lang = "eng" ) : if lang not in flask_nemo . _data . AVAILABLE_TRANSLATIONS : lang = "eng" try : return flask_nemo . _data . ISOCODES [ isocode ] [ lang ] except KeyError : return "Unknown"
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
2475	def set_lic_name ( self , doc , name ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_name_set : self . extr_lic_name_set = True if validations . validate_extr_lic_name ( name ) : self . extr_lic ( doc ) . full_name = name return True else : raise SPDXValueError ( 'ExtractedLicense::Name' ) else : raise CardinalityError ( 'ExtractedLicense::Name' ) else : raise OrderError ( 'ExtractedLicense::Name' )
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
7602	def get_known_tournaments ( self , ** params : tournamentfilter ) : url = self . api . TOURNAMENT + '/known' return self . _get_model ( url , PartialTournament , ** params )
3228	def gce_list_aggregated ( service = None , key_name = 'name' , ** kwargs ) : resp_list = [ ] req = service . aggregatedList ( ** kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
5325	def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
4404	def lock ( self , name , timeout = None , sleep = 0.1 ) : return Lock ( self , name , timeout = timeout , sleep = sleep )
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
12707	def rotation ( self , rotation ) : if isinstance ( rotation , np . ndarray ) : rotation = rotation . ravel ( ) self . ode_body . setRotation ( tuple ( rotation ) )
4115	def lar2rc ( g ) : assert numpy . isrealobj ( g ) , 'Log area ratios not defined for complex reflection coefficients.' return - numpy . tanh ( - numpy . array ( g ) / 2 )
3957	def update_running_containers_from_spec ( compose_config , recreate_containers = True ) : write_composefile ( compose_config , constants . COMPOSEFILE_PATH ) compose_up ( constants . COMPOSEFILE_PATH , 'dusty' , recreate_containers = recreate_containers )
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
9517	def trim_Ns ( self ) : i = 0 while i < len ( self ) and self . seq [ i ] in 'nN' : i += 1 self . seq = self . seq [ i : ] self . qual = self . qual [ i : ] self . seq = self . seq . rstrip ( 'Nn' ) self . qual = self . qual [ : len ( self . seq ) ]
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
12721	def hi_stops ( self , hi_stops ) : _set_params ( self . ode_obj , 'HiStop' , hi_stops , self . ADOF + self . LDOF )
5763	def topological_order_packages ( packages ) : from catkin_pkg . topological_order import _PackageDecorator from catkin_pkg . topological_order import _sort_decorated_packages decorators_by_name = { } for path , package in packages . items ( ) : decorators_by_name [ package . name ] = _PackageDecorator ( package , path ) for decorator in decorators_by_name . values ( ) : decorator . depends_for_topological_order = set ( [ ] ) all_depends = decorator . package . build_depends + decorator . package . buildtool_depends + decorator . package . run_depends + decorator . package . test_depends unique_depend_names = set ( [ d . name for d in all_depends if d . name in decorators_by_name . keys ( ) ] ) for name in unique_depend_names : if name in decorator . depends_for_topological_order : continue decorators_by_name [ name ] . _add_recursive_run_depends ( decorators_by_name , decorator . depends_for_topological_order ) ordered_pkg_tuples = _sort_decorated_packages ( decorators_by_name ) for pkg_path , pkg in ordered_pkg_tuples : if pkg_path is None : raise RuntimeError ( 'Circular dependency in: %s' % pkg ) return ordered_pkg_tuples
13756	def copy_file ( src , dest ) : dir_path = os . path . dirname ( dest ) if not os . path . exists ( dir_path ) : os . makedirs ( dir_path ) shutil . copy2 ( src , dest )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
11147	def is_repository_file ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) if relativePath == '' : return False , False , False , False relaDir , name = os . path . split ( relativePath ) fileOnDisk = os . path . isfile ( os . path . join ( self . __path , relativePath ) ) infoOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % name ) ) classOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileClass % name ) ) cDir = self . __repo [ 'walk_repo' ] if len ( relaDir ) : for dirname in relaDir . split ( os . sep ) : dList = [ d for d in cDir if isinstance ( d , dict ) ] if not len ( dList ) : cDir = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : cDir = None break cDir = cDict [ 0 ] [ dirname ] if cDir is None : return False , fileOnDisk , infoOnDisk , classOnDisk if str ( name ) not in [ str ( i ) for i in cDir ] : return False , fileOnDisk , infoOnDisk , classOnDisk return True , fileOnDisk , infoOnDisk , classOnDisk
4293	def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None if not args . noinput : if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
13261	def get_task_tree ( white_list = None ) : assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) if white_list is not None : white_list = set ( item if isinstance ( item , str ) else item . __qualname__ for item in white_list ) tree = dict ( ( task . qualified_name , task ) for task in _task_list . values ( ) if white_list is None or task . qualified_name in white_list ) plugins = get_plugin_list ( ) for plugin in [ plugin for plugin in plugins . values ( ) if white_list is None or plugin . __qualname__ in white_list ] : tasks = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . isfunction ( func ) and hasattr ( func , "yaz_task_config" ) ] if len ( tasks ) == 0 : continue node = tree for name in plugin . __qualname__ . split ( "." ) : if not name in node : node [ name ] = { } node = node [ name ] for func in tasks : logger . debug ( "Found task %s" , func ) node [ func . __name__ ] = Task ( plugin_class = plugin , func = func , config = func . yaz_task_config ) return tree
2379	def _get_rules ( self , cls ) : result = [ ] for rule_class in cls . __subclasses__ ( ) : rule_name = rule_class . __name__ . lower ( ) if rule_name not in self . _rules : rule = rule_class ( self ) self . _rules [ rule_name ] = rule result . append ( self . _rules [ rule_name ] ) return result
131	def is_partly_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = False )
12308	def get_files_to_commit ( autooptions ) : workingdir = autooptions [ 'working-directory' ] includes = autooptions [ 'track' ] [ 'includes' ] excludes = autooptions [ 'track' ] [ 'excludes' ] includes = r'|' . join ( [ fnmatch . translate ( x ) for x in includes ] ) excludes = r'|' . join ( [ fnmatch . translate ( x ) for x in excludes ] ) or r'$.' matched_files = [ ] for root , dirs , files in os . walk ( workingdir ) : dirs [ : ] = [ d for d in dirs if not re . match ( excludes , d ) ] files = [ f for f in files if not re . match ( excludes , f ) ] files = [ f for f in files if re . match ( includes , f ) ] files = [ os . path . join ( root , f ) for f in files ] matched_files . extend ( files ) return matched_files
8661	def from_config ( cls , cfg , default_fg = DEFAULT_FG_16 , default_bg = DEFAULT_BG_16 , default_fg_hi = DEFAULT_FG_256 , default_bg_hi = DEFAULT_BG_256 , max_colors = 2 ** 24 ) : e = PaletteEntry ( mono = default_fg , foreground = default_fg , background = default_bg , foreground_high = default_fg_hi , background_high = default_bg_hi ) if isinstance ( cfg , str ) : e . foreground_high = cfg if e . allowed ( cfg , 16 ) : e . foreground = cfg else : rgb = AttrSpec ( fg = cfg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( cfg , dict ) : bg = cfg . get ( "bg" , None ) if isinstance ( bg , str ) : e . background_high = bg if e . allowed ( bg , 16 ) : e . background = bg else : rgb = AttrSpec ( fg = bg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) elif isinstance ( bg , dict ) : e . background_high = bg . get ( "hi" , default_bg_hi ) if "lo" in bg : if e . allowed ( bg [ "lo" ] , 16 ) : e . background = bg [ "lo" ] else : rgb = AttrSpec ( fg = bg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) fg = cfg . get ( "fg" , cfg ) if isinstance ( fg , str ) : e . foreground_high = fg if e . allowed ( fg , 16 ) : e . foreground = fg else : rgb = AttrSpec ( fg = fg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( fg , dict ) : e . foreground_high = fg . get ( "hi" , default_fg_hi ) if "lo" in fg : if e . allowed ( fg [ "lo" ] , 16 ) : e . foreground = fg [ "lo" ] else : rgb = AttrSpec ( fg = fg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) return e
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
8339	def _findAll ( self , name , attrs , text , limit , generator , ** kwargs ) : "Iterates over a generator looking for things that match." if isinstance ( name , SoupStrainer ) : strainer = name else : strainer = SoupStrainer ( name , attrs , text , ** kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
13059	def get_inventory ( self ) : if self . _inventory is not None : return self . _inventory self . _inventory = self . resolver . getMetadata ( ) return self . _inventory
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
13396	def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
4582	def construct ( cls , project , * , run = None , name = None , data = None , ** desc ) : from . failed import Failed exception = desc . pop ( '_exception' , None ) if exception : a = Failed ( project . layout , desc , exception ) else : try : a = cls ( project . layout , ** desc ) a . _set_runner ( run or { } ) except Exception as e : if cls . FAIL_ON_EXCEPTION : raise a = Failed ( project . layout , desc , e ) a . name = name a . data = data return a
1304	def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
12408	def _merge ( options , name , bases , default = None ) : result = None for base in bases : if base is None : continue value = getattr ( base , name , None ) if value is None : continue result = utils . cons ( result , value ) value = options . get ( name ) if value is not None : result = utils . cons ( result , value ) return result or default
4420	async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
12790	def create_from_settings ( settings ) : return Connection ( settings [ "url" ] , settings [ "base_url" ] , settings [ "user" ] , settings [ "password" ] , authorizations = settings [ "authorizations" ] , debug = settings [ "debug" ] )
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
10866	def update ( self , params , values ) : params = listify ( params ) values = listify ( values ) for i , p in enumerate ( params ) : if ( p [ - 2 : ] == '-a' ) and ( values [ i ] < 0 ) : values [ i ] = 0.0 super ( PlatonicSpheresCollection , self ) . update ( params , values )
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
11260	def match ( prev , pattern , * args , ** kw ) : to = 'to' in kw and kw . pop ( 'to' ) pattern_obj = re . compile ( pattern , * args , ** kw ) if to is dict : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groupdict ( ) elif to is tuple : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match . groups ( ) elif to is list : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield list ( match . groups ( ) ) else : for data in prev : match = pattern_obj . match ( data ) if match is not None : yield match
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
5877	def get_local_image ( self , src ) : return ImageUtils . store_image ( self . fetcher , self . article . link_hash , src , self . config )
4684	def getPublicKeys ( self , current = False ) : pubkeys = self . store . getPublicKeys ( ) if not current : return pubkeys pubs = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : pubs . append ( pubkey ) return pubs
485	def _getCommonSteadyDBArgsDict ( ) : return dict ( creator = pymysql , host = Configuration . get ( 'nupic.cluster.database.host' ) , port = int ( Configuration . get ( 'nupic.cluster.database.port' ) ) , user = Configuration . get ( 'nupic.cluster.database.user' ) , passwd = Configuration . get ( 'nupic.cluster.database.passwd' ) , charset = 'utf8' , use_unicode = True , setsession = [ 'SET AUTOCOMMIT = 1' ] )
7436	def _tuplecheck ( newvalue , dtype = str ) : if isinstance ( newvalue , list ) : newvalue = tuple ( newvalue ) if isinstance ( newvalue , str ) : newvalue = newvalue . rstrip ( ")" ) . strip ( "(" ) try : newvalue = tuple ( [ dtype ( i . strip ( ) ) for i in newvalue . split ( "," ) ] ) except TypeError : newvalue = tuple ( dtype ( newvalue ) ) except ValueError : LOGGER . info ( "Assembly.tuplecheck() failed to cast to {} - {}" . format ( dtype , newvalue ) ) raise except Exception as inst : LOGGER . info ( inst ) raise SystemExit ( "\nError: Param`{}` is not formatted correctly.\n({})\n" . format ( newvalue , inst ) ) return newvalue
10324	def microcanonical_averages_arrays ( microcanonical_averages ) : ret = dict ( ) for n , microcanonical_average in enumerate ( microcanonical_averages ) : assert n == microcanonical_average [ 'n' ] if n == 0 : num_edges = microcanonical_average [ 'M' ] num_sites = microcanonical_average [ 'N' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_average ) ret [ 'max_cluster_size' ] = np . empty ( num_edges + 1 ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( num_edges + 1 ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( num_edges + 1 , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , num_edges + 1 ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , num_edges + 1 , 2 ) ) ret [ 'max_cluster_size' ] [ n ] = microcanonical_average [ 'max_cluster_size' ] ret [ 'max_cluster_size_ci' ] [ n ] = ( microcanonical_average [ 'max_cluster_size_ci' ] ) if spanning_cluster : ret [ 'spanning_cluster' ] [ n ] = ( microcanonical_average [ 'spanning_cluster' ] ) ret [ 'spanning_cluster_ci' ] [ n ] = ( microcanonical_average [ 'spanning_cluster_ci' ] ) ret [ 'moments' ] [ : , n ] = microcanonical_average [ 'moments' ] ret [ 'moments_ci' ] [ : , n ] = microcanonical_average [ 'moments_ci' ] for key in ret : if 'spanning_cluster' in key : continue ret [ key ] /= num_sites ret [ 'M' ] = num_edges ret [ 'N' ] = num_sites return ret
6350	def _pnums_with_leading_space ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return ' ' + self . _phonetic_number ( phonetic ) prefix = phonetic [ : alt_start ] alt_start += 1 alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for alt in alt_array : result += self . _pnums_with_leading_space ( prefix + alt + suffix ) return result
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
3562	def find_characteristic ( self , uuid ) : for char in self . list_characteristics ( ) : if char . uuid == uuid : return char return None
10336	def bel_to_spia_matrices ( graph : BELGraph ) -> Mapping [ str , pd . DataFrame ] : index_nodes = get_matrix_index ( graph ) spia_matrices = build_spia_matrices ( index_nodes ) for u , v , edge_data in graph . edges ( data = True ) : if isinstance ( u , CentralDogma ) and isinstance ( v , CentralDogma ) : update_spia_matrices ( spia_matrices , u , v , edge_data ) elif isinstance ( u , CentralDogma ) and isinstance ( v , ListAbundance ) : for node in v . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , u , node , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , CentralDogma ) : for node in u . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , node , v , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for sub_member , obj_member in product ( u . members , v . members ) : if isinstance ( sub_member , CentralDogma ) and isinstance ( obj_member , CentralDogma ) : update_spia_matrices ( spia_matrices , sub_member , obj_member , edge_data ) return spia_matrices
9142	def _sortkey ( self , key = 'uri' , language = 'any' ) : if key == 'uri' : return self . uri else : l = label ( self . labels , language , key == 'sortlabel' ) return l . label . lower ( ) if l else ''
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) network . initialize ( ) return network
5576	def load_output_writer ( output_params , readonly = False ) : if not isinstance ( output_params , dict ) : raise TypeError ( "output_params must be a dictionary" ) driver_name = output_params [ "format" ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : _driver = v . load ( ) if all ( [ hasattr ( _driver , attr ) for attr in [ "OutputData" , "METADATA" ] ] ) and ( _driver . METADATA [ "driver_name" ] == driver_name ) : return _driver . OutputData ( output_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
10979	def approve ( group_id , user_id ) : membership = Membership . query . get_or_404 ( ( user_id , group_id ) ) group = membership . group if group . can_edit ( current_user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( 'You cannot approve memberships for the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
2594	def interactive ( f ) : if isinstance ( f , FunctionType ) : mainmod = __import__ ( '__main__' ) f = FunctionType ( f . __code__ , mainmod . __dict__ , f . __name__ , f . __defaults__ , ) f . __module__ = '__main__' return f
9426	def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
5296	def get_end_date ( self , obj ) : obj_date = getattr ( obj , self . get_end_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : pass return obj_date
13554	def create_shift ( self , params = { } ) : url = "/2/shifts/" body = params data = self . _post_resource ( url , body ) shift = self . shift_from_json ( data [ "shift" ] ) return shift
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
7435	def _zbufcountlines ( filename , gzipped ) : if gzipped : cmd1 = [ "gunzip" , "-c" , filename ] else : cmd1 = [ "cat" , filename ] cmd2 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdout = sps . PIPE , stderr = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , stderr = sps . PIPE ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error zbufcountlines {}:" . format ( res ) ) LOGGER . info ( res ) nlines = int ( res . split ( ) [ 0 ] ) return nlines
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
6528	def get_reports ( ) : if not hasattr ( get_reports , '_CACHE' ) : get_reports . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.reports' ) : try : get_reports . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load report "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_reports . _CACHE
3755	def Carcinogen ( CASRN , AvailableMethods = False , Method = None ) : r methods = [ COMBINED , IARC , NTP ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == IARC : if CASRN in IARC_data . index : status = IARC_codes [ IARC_data . at [ CASRN , 'group' ] ] else : status = UNLISTED elif Method == NTP : if CASRN in NTP_data . index : status = NTP_codes [ NTP_data . at [ CASRN , 'Listing' ] ] else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = Carcinogen ( CASRN , Method = method ) else : raise Exception ( 'Failure in in function' ) return status
8817	def get_network ( context , id , fields = None ) : LOG . info ( "get_network %s for tenant %s fields %s" % ( id , context . tenant_id , fields ) ) network = db_api . network_find ( context = context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , id = id , join_subnets = True , scope = db_api . ONE ) if not network : raise n_exc . NetworkNotFound ( net_id = id ) return v . _make_network_dict ( network , fields = fields )
5599	def open ( self , tile , process , ** kwargs ) : return InputTile ( tile , process , kwargs . get ( "resampling" , None ) )
3886	def _add_user_from_conv_part ( self , conv_part ) : user_ = User . from_conv_part_data ( conv_part , self . _self_user . id_ ) existing = self . _user_dict . get ( user_ . id_ ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user_ . name_type . name . lower ( ) , user_ . full_name ) self . _user_dict [ user_ . id_ ] = user_ return user_ else : existing . upgrade_name ( user_ ) return existing
9701	def send ( self , msg ) : slipDriver = sliplib . Driver ( ) slipData = slipDriver . send ( msg ) res = self . _serialPort . write ( slipData ) return res
11237	def sendreturn ( gen , value ) : try : gen . send ( value ) except StopIteration as e : return stopiter_value ( e ) else : raise RuntimeError ( 'generator did not return as expected' )
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : stream_queue . put ( line ) stream_queue . put ( None ) stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True _thread . start ( ) return stream_queue
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
6617	def expand_path_cfg ( path_cfg , alias_dict = { } , overriding_kargs = { } ) : if isinstance ( path_cfg , str ) : return _expand_str ( path_cfg , alias_dict , overriding_kargs ) if isinstance ( path_cfg , dict ) : return _expand_dict ( path_cfg , alias_dict ) return _expand_tuple ( path_cfg , alias_dict , overriding_kargs )
5136	def get_class_traits ( klass ) : source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
7470	def fill_dups_arr ( data ) : duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples_*.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) ) io5 = h5py . File ( data . clust_database , 'r+' ) dfilter = io5 [ "duplicates" ] init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
6193	def add ( self , num_particles , D ) : self . _plist += self . _generate ( num_particles , D , box = self . box , rs = self . rs )
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
12228	def register_admin_models ( admin_site ) : global __MODELS_REGISTRY prefs = get_prefs ( ) for app_label , prefs_items in prefs . items ( ) : model_class = get_pref_model_class ( app_label , prefs_items , get_app_prefs ) if model_class is not None : __MODELS_REGISTRY [ app_label ] = model_class admin_site . register ( model_class , get_pref_model_admin_class ( prefs_items ) )
180	def to_bounding_box ( self ) : from . bbs import BoundingBox if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
12272	def int2fin_reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return "%s%s" % ( n , checksum )
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
142	def from_shapely ( polygon_shapely , label = None ) : import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
9218	def _smixins ( self , name ) : return ( self . _mixins [ name ] if name in self . _mixins else False )
5678	def get_stop_count_data ( self , start_ut , end_ut ) : trips_df = self . get_tripIs_active_in_range ( start_ut , end_ut ) stop_counts = Counter ( ) for row in trips_df . itertuples ( ) : stops_seq = self . get_trip_stop_time_data ( row . trip_I , row . day_start_ut ) for stop_time_row in stops_seq . itertuples ( index = False ) : if ( stop_time_row . dep_time_ut >= start_ut ) and ( stop_time_row . dep_time_ut <= end_ut ) : stop_counts [ stop_time_row . stop_I ] += 1 all_stop_data = self . stops ( ) counts = [ stop_counts [ stop_I ] for stop_I in all_stop_data [ "stop_I" ] . values ] all_stop_data . loc [ : , "count" ] = pd . Series ( counts , index = all_stop_data . index ) return all_stop_data
4072	def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : raise err ( response . headers [ 'Location' ] , ** payload ) raise err ( ** payload )
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
5693	def evaluate ( self , dep_time , first_leg_can_be_walk = True , connection_arrival_time = None ) : walk_labels = list ( ) if first_leg_can_be_walk and self . _walk_to_target_duration != float ( 'inf' ) : if connection_arrival_time is not None : walk_labels . append ( self . _get_label_to_target ( connection_arrival_time ) ) else : walk_labels . append ( self . _get_label_to_target ( dep_time ) ) if dep_time in self . dep_times_to_index : assert ( dep_time != float ( 'inf' ) ) index = self . dep_times_to_index [ dep_time ] labels = self . _label_bags [ index ] pareto_optimal_labels = merge_pareto_frontiers ( labels , walk_labels ) else : pareto_optimal_labels = walk_labels if not first_leg_can_be_walk : pareto_optimal_labels = [ label for label in pareto_optimal_labels if not label . first_leg_is_walk ] return pareto_optimal_labels
12725	def cfms ( self , cfms ) : _set_params ( self . ode_obj , 'CFM' , cfms , self . ADOF + self . LDOF )
13337	def active_env_module_resolver ( resolver , path ) : from . api import get_active_env env = get_active_env ( ) if not env : raise ResolveError mod = env . get_module ( path ) if not mod : raise ResolveError return mod
13414	def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : self . removeLayout ( item . layout ( ) )
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
7825	def feature_uri ( uri ) : def decorator ( class_ ) : if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
106	def avg_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . average , cval = cval , preserve_dtype = preserve_dtype )
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
13713	def run ( self ) : self . log . debug ( 'consumer is running...' ) self . running = True while self . running : self . upload ( ) self . log . debug ( 'consumer exited.' )
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
3540	def status_printer ( ) : last_len = [ 0 ] def p ( s ) : s = next ( spinner ) + ' ' + s len_s = len ( s ) output = '\r' + s + ( ' ' * max ( last_len [ 0 ] - len_s , 0 ) ) sys . stdout . write ( output ) sys . stdout . flush ( ) last_len [ 0 ] = len_s return p
8307	def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
7728	def add_item ( self , item ) : if not isinstance ( item , MucItemBase ) : raise TypeError ( "Bad item type for muc#user" ) item . as_xml ( self . xmlnode )
7593	def get_constants ( self , ** params : keys ) : url = self . api . CONSTANTS return self . _get_model ( url , ** params )
6920	def _autocorr_func2 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 0 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) autocovarfunc = npsum ( products ) / lagindex . size varfunc = npsum ( ( mags [ lagindex ] - magmed ) * ( mags [ lagindex ] - magmed ) ) / mags . size acorr = autocovarfunc / varfunc return acorr
5454	def task_view_generator ( job_descriptor ) : for task_descriptor in job_descriptor . task_descriptors : jd = JobDescriptor ( job_descriptor . job_metadata , job_descriptor . job_params , job_descriptor . job_resources , [ task_descriptor ] ) yield jd
4805	def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
7497	def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) for idx in xrange ( smpchunk . shape [ 0 ] ) : sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
6231	def apply_mesh_programs ( self , mesh_programs = None ) : if not mesh_programs : mesh_programs = [ ColorProgram ( ) , TextureProgram ( ) , FallbackProgram ( ) ] for mesh in self . meshes : for mp in mesh_programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , MeshProgram ) : mesh . mesh_program = mp break else : raise ValueError ( "apply() must return a MeshProgram instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh_program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
9709	def heappop_max ( heap ) : lastelt = heap . pop ( ) if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup_max ( heap , 0 ) return returnitem return lastelt
7561	def get_sampled ( data , totn , node ) : names = sorted ( totn ) cdict = { name : idx for idx , name in enumerate ( names ) } if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = set ( cdict [ i ] for i in down_r . get_leaf_names ( ) ) lendl = set ( cdict [ i ] for i in down_l . get_leaf_names ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = set ( cdict [ i ] for i in up_r . get_leaf_names ( ) ) lenul = set ( cdict [ i ] for i in totn ) - set . union ( lendr , lendl , lenur ) idx = 0 sampled = 0 with h5py . File ( data . database . output , 'r' ) as io5 : end = io5 [ "quartets" ] . shape [ 0 ] while 1 : if idx >= end : break qrts = io5 [ "quartets" ] [ idx : idx + data . _chunksize ] for qrt in qrts : sqrt = set ( qrt ) if all ( [ sqrt . intersection ( i ) for i in [ lendr , lendl , lenur , lenul ] ] ) : sampled += 1 idx += data . _chunksize return sampled
12202	def _from_jsonlines ( cls , lines , selector_handler = None , strict = False , debug = False ) : return cls ( json . loads ( "\n" . join ( [ l for l in lines if not cls . REGEX_COMMENT_LINE . match ( l ) ] ) ) , selector_handler = selector_handler , strict = strict , debug = debug )
12253	def _delete_key_internal ( self , * args , ** kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , ** kwargs )
12289	def bootstrap_datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) package = OrderedDict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise IncompleteParameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
12998	def round_teff_luminosity ( cluster ) : temps = [ round ( t , - 1 ) for t in teff ( cluster ) ] lums = [ round ( l , 3 ) for l in luminosity ( cluster ) ] return temps , lums
6829	def pull ( self , path , use_sudo = False , user = None , force = False ) : if path is None : raise ValueError ( "Path to the working copy is needed to pull from a remote repository." ) options = [ ] if force : options . append ( '--force' ) options = ' ' . join ( options ) cmd = 'git pull %s' % options with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
7545	def calculate_depths ( data , samples , lbview ) : start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
6699	def get_selections ( ) : with settings ( hide ( 'stdout' ) ) : res = run_as_root ( 'dpkg --get-selections' ) selections = dict ( ) for line in res . splitlines ( ) : package , status = line . split ( ) selections . setdefault ( status , list ( ) ) . append ( package ) return selections
7525	def draw ( self , show_tip_labels = True , show_node_support = False , use_edge_lengths = False , orient = "right" , print_args = False , * args , ** kwargs ) : self . _decompose_tree ( orient = orient , use_edge_lengths = use_edge_lengths ) dwargs = { } dwargs [ "show_tip_labels" ] = show_tip_labels dwargs [ "show_node_support" ] = show_node_support dwargs . update ( kwargs ) canvas , axes , panel = tree_panel_plot ( self , print_args , ** dwargs ) return canvas , axes , panel
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
10312	def prepare_c3_time_series ( data : List [ Tuple [ int , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' ) -> str : years , counter = zip ( * data ) years = [ datetime . date ( year , 1 , 1 ) . isoformat ( ) for year in years ] return json . dumps ( [ [ x_axis_label ] + list ( years ) , [ y_axis_label ] + list ( counter ) ] )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
3296	def ref_url_to_path ( self , ref_url ) : return "/" + compat . unquote ( util . lstripstr ( ref_url , self . share_path ) ) . lstrip ( "/" )
6555	def copy ( self ) : return self . __class__ ( self . func , self . configurations , self . variables , self . vartype , name = self . name )
9529	def get_encrypted_field ( base_class ) : assert not isinstance ( base_class , models . Field ) field_name = 'Encrypted' + base_class . __name__ if base_class not in FIELD_CACHE : FIELD_CACHE [ base_class ] = type ( field_name , ( EncryptedMixin , base_class ) , { 'base_class' : base_class , } ) return FIELD_CACHE [ base_class ]
497	def _constructClassificationRecord ( self , inputs ) : allSPColumns = inputs [ "spBottomUpOut" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . classificationVectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
3845	def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
4290	def write ( self , album , media_group ) : from sigal import __url__ as sigal_link file_path = os . path . join ( album . dst_path , media_group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media_group [ 0 ] , 'previous_media' : media_group [ - 1 ] , 'next_media' : media_group [ 1 ] , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , } ) output_file = "%s.html" % file_path with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
363	def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
4693	def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
13083	def chunk ( self , text , reffs ) : if str ( text . id ) in self . chunker : return self . chunker [ str ( text . id ) ] ( text , reffs ) return self . chunker [ "default" ] ( text , reffs )
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
4898	def _remove_failed_items ( self , failed_items , items_to_create , items_to_update , items_to_delete ) : for item in failed_items : content_metadata_id = item [ 'courseID' ] items_to_create . pop ( content_metadata_id , None ) items_to_update . pop ( content_metadata_id , None ) items_to_delete . pop ( content_metadata_id , None )
10581	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) mm_average = 0.0 for compound , molefraction in state [ "x" ] . items ( ) : mm_average += molefraction * mm ( compound ) mm_average /= 1000.0 return mm_average * state [ "P" ] / R / state [ "T" ]
4616	def refresh ( self ) : asset = self . blockchain . rpc . get_asset ( self . identifier ) if not asset : raise AssetDoesNotExistsException ( self . identifier ) super ( Asset , self ) . __init__ ( asset , blockchain_instance = self . blockchain ) if self . full : if "bitasset_data_id" in asset : self [ "bitasset_data" ] = self . blockchain . rpc . get_object ( asset [ "bitasset_data_id" ] ) self [ "dynamic_asset_data" ] = self . blockchain . rpc . get_object ( asset [ "dynamic_asset_data_id" ] )
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
4853	def _get_transmissions ( self ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) return ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) )
198	def Snowflakes ( density = ( 0.005 , 0.075 ) , density_uniformity = ( 0.3 , 0.9 ) , flake_size = ( 0.2 , 0.7 ) , flake_size_uniformity = ( 0.4 , 0.8 ) , angle = ( - 30 , 30 ) , speed = ( 0.007 , 0.03 ) , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) layer = SnowflakesLayer ( density = density , density_uniformity = density_uniformity , flake_size = flake_size , flake_size_uniformity = flake_size_uniformity , angle = angle , speed = speed , blur_sigma_fraction = ( 0.0001 , 0.001 ) ) return meta . SomeOf ( ( 1 , 3 ) , children = [ layer . deepcopy ( ) for _ in range ( 3 ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) return ( pmags - modelmags ) / perrs
10084	def discard ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) _ , record = self . fetch_published ( ) self . model . json = deepcopy ( record . model . json ) self . model . json [ '$schema' ] = self . build_deposit_schema ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
399	def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
5580	def extract_contours ( array , tile , interval = 100 , field = 'elev' , base = 0 ) : import matplotlib . pyplot as plt levels = _get_contour_values ( array . min ( ) , array . max ( ) , interval = interval , base = base ) if not levels : return [ ] contours = plt . contour ( array , levels ) index = 0 out_contours = [ ] for level in range ( len ( contours . collections ) ) : elevation = levels [ index ] index += 1 paths = contours . collections [ level ] . get_paths ( ) for path in paths : out_coords = [ ( tile . left + ( y * tile . pixel_x_size ) , tile . top - ( x * tile . pixel_y_size ) , ) for x , y in zip ( path . vertices [ : , 1 ] , path . vertices [ : , 0 ] ) ] if len ( out_coords ) >= 2 : out_contours . append ( dict ( properties = { field : elevation } , geometry = mapping ( LineString ( out_coords ) ) ) ) return out_contours
5595	def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
2866	def write8 ( self , register , value ) : value = value & 0xFF self . _bus . write_byte_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
8190	def nodes_by_betweenness ( self , treshold = 0.0 ) : nodes = [ ( n . betweenness , n ) for n in self . nodes if n . betweenness > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
3537	def crazy_egg ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return CrazyEggNode ( )
10555	def find_helping_materials ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'helpingmaterial' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
6377	def sim_typo ( src , tar , metric = 'euclidean' , cost = ( 1 , 1 , 0.5 , 0.5 ) , layout = 'QWERTY' ) : return Typo ( ) . sim ( src , tar , metric , cost , layout )
3158	def _post ( self , url , data = None ) : url = urljoin ( self . base_url , url ) try : r = self . _make_request ( ** dict ( method = 'POST' , url = url , json = data , auth = self . auth , timeout = self . timeout , hooks = self . request_hooks , headers = self . request_headers ) ) except requests . exceptions . RequestException as e : raise e else : if r . status_code >= 400 : try : error_data = r . json ( ) except ValueError : error_data = { "response" : r } raise MailChimpError ( error_data ) if r . status_code == 204 : return None return r . json ( )
10957	def _calc_loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . _data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
8408	def expand_range_distinct ( range , expand = ( 0 , 0 , 0 , 0 ) , zero_width = 1 ) : if len ( expand ) == 2 : expand = tuple ( expand ) * 2 lower = expand_range ( range , expand [ 0 ] , expand [ 1 ] , zero_width ) [ 0 ] upper = expand_range ( range , expand [ 2 ] , expand [ 3 ] , zero_width ) [ 1 ] return ( lower , upper )
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = ' ' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
1719	def emit ( self , what , * args ) : if isinstance ( what , basestring ) : return self . exe . emit ( what , * args ) elif isinstance ( what , list ) : self . _emit_statement_list ( what ) else : return getattr ( self , what [ 'type' ] ) ( ** what )
10634	def get_compound_afr ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_mfrs [ index ] )
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
4065	def fields_types ( self , tname , qstring , itemtype ) : template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
13825	def FromJsonString ( self , value ) : if len ( value ) < 1 or value [ - 1 ] != 's' : raise ParseError ( 'Duration must end with letter "s": {0}.' . format ( value ) ) try : pos = value . find ( '.' ) if pos == - 1 : self . seconds = int ( value [ : - 1 ] ) self . nanos = 0 else : self . seconds = int ( value [ : pos ] ) if value [ 0 ] == '-' : self . nanos = int ( round ( float ( '-0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) else : self . nanos = int ( round ( float ( '0{0}' . format ( value [ pos : - 1 ] ) ) * 1e9 ) ) except ValueError : raise ParseError ( 'Couldn\'t parse duration: {0}.' . format ( value ) )
2240	def modname_to_modpath ( modname , hide_init = True , hide_main = False , sys_path = None ) : modpath = _syspath_modname_to_modpath ( modname , sys_path ) if modpath is None : return None modpath = normalize_modpath ( modpath , hide_init = hide_init , hide_main = hide_main ) return modpath
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
5133	def generate_random_graph ( num_vertices = 250 , prob_loop = 0.5 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) for v in g . nodes ( ) : e = ( v , v ) if not g . is_edge ( e ) : if np . random . uniform ( ) < prob_loop : g . add_edge ( * e ) g = set_types_random ( g , ** kwargs ) return g
1484	def start_state_manager_watches ( self ) : Log . info ( "Start state manager watches" ) statemgr_config = StateMgrConfig ( ) statemgr_config . set_state_locations ( configloader . load_state_manager_locations ( self . cluster , state_manager_config_file = self . state_manager_config_file , overrides = { "heron.statemgr.connection.string" : self . state_manager_connection } ) ) try : self . state_managers = statemanagerfactory . get_all_state_managers ( statemgr_config ) for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_packing_plan_watch ( state_manager , new_packing_plan ) : Log . debug ( "State watch triggered for PackingPlan update on shard %s. Existing: %s, New: %s" % ( self . shard , str ( self . packing_plan ) , str ( new_packing_plan ) ) ) if self . packing_plan != new_packing_plan : Log . info ( "PackingPlan change detected on shard %s, relaunching effected processes." % self . shard ) self . update_packing_plan ( new_packing_plan ) Log . info ( "Updating executor processes" ) self . launch ( ) else : Log . info ( "State watch triggered for PackingPlan update but plan not changed so not relaunching." ) for state_manager in self . state_managers : onPackingPlanWatch = functools . partial ( on_packing_plan_watch , state_manager ) state_manager . get_packing_plan ( self . topology_name , onPackingPlanWatch ) Log . info ( "Registered state watch for packing plan changes with state manager %s." % str ( state_manager ) )
6618	def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : new_path_cfg = path_cfg [ 0 ] new_overriding_kargs = path_cfg [ 1 ] . copy ( ) new_overriding_kargs . update ( overriding_kargs ) return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
13078	def make_cache_keys ( self , endpoint , kwargs ) : keys = sorted ( kwargs . keys ( ) ) i18n_cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys ] ) if "lang" in keys : cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys if k != "lang" ] ) else : cache_key = i18n_cache_key return i18n_cache_key , cache_key
989	def add ( reader , writer , column , start , stop , value ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( value ) ( row [ column ] ) + value writer . appendRecord ( row )
3554	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . stopScan ( ) self . _is_scanning = False
11295	def make_request_data ( self , zipcode , city , state ) : data = { 'key' : self . api_key , 'postalcode' : str ( zipcode ) , 'city' : city , 'state' : state } data = ZipTaxClient . _clean_request_data ( data ) return data
8988	def last_consumed_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . consumes_meshes ( ) : return instruction . last_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
1956	def _execve ( self , program , argv , envp ) : argv = [ ] if argv is None else argv envp = [ ] if envp is None else envp logger . debug ( f"Loading {program} as a {self.arch} elf" ) self . load ( program , envp ) self . _arch_specific_init ( ) self . _stack_top = self . current . STACK self . setup_stack ( [ program ] + argv , envp ) nprocs = len ( self . procs ) nfiles = len ( self . files ) assert nprocs > 0 self . running = list ( range ( nprocs ) ) self . timers = [ None ] * nprocs self . rwait = [ set ( ) for _ in range ( nfiles ) ] self . twait = [ set ( ) for _ in range ( nfiles ) ] for proc in self . procs : self . forward_events_from ( proc )
10535	def get_categories ( limit = 20 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) try : res = _pybossa_req ( 'get' , 'category' , params = params ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : raise TypeError except : raise
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
2625	def submit ( self , command = 'sleep 1' , blocksize = 1 , tasks_per_node = 1 , job_name = "parsl.auto" ) : job_name = "parsl.auto.{0}" . format ( time . time ( ) ) wrapped_cmd = self . launcher ( command , tasks_per_node , self . nodes_per_block ) [ instance , * rest ] = self . spin_up_instance ( command = wrapped_cmd , job_name = job_name ) if not instance : logger . error ( "Failed to submit request to EC2" ) return None logger . debug ( "Started instance_id: {0}" . format ( instance . instance_id ) ) state = translate_table . get ( instance . state [ 'Name' ] , "PENDING" ) self . resources [ instance . instance_id ] = { "job_id" : instance . instance_id , "instance" : instance , "status" : state } return instance . instance_id
3536	def gauges ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return GaugesNode ( )
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
11104	def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : with self . locker as r : acquired , code , _ = r if acquired : try : r = func ( self , * args , ** kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
7052	def parallel_tfa_lcdir ( lcdir , templateinfo , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if lcfileglob is None : lcfileglob = dfileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_tfa_lclist ( lclist , templateinfo , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = None , interp = interp , sigclip = sigclip , mintemplatedist_arcmin = mintemplatedist_arcmin , nworkers = nworkers , maxworkertasks = maxworkertasks )
13355	def profil_annuel ( df , func = 'mean' ) : func = _get_funky ( func ) res = df . groupby ( lambda x : x . month ) . aggregate ( func ) res . index = [ cal . month_name [ i ] for i in range ( 1 , 13 ) ] return res
6248	def get_texture ( self , label : str ) -> Union [ moderngl . Texture , moderngl . TextureArray , moderngl . Texture3D , moderngl . TextureCube ] : return self . _project . get_texture ( label )
13180	def _get_column_nums_from_args ( columns ) : nums = [ ] for c in columns : for p in c . split ( ',' ) : p = p . strip ( ) try : c = int ( p ) nums . append ( c ) except ( TypeError , ValueError ) : start , ignore , end = p . partition ( '-' ) try : start = int ( start ) end = int ( end ) except ( TypeError , ValueError ) : raise ValueError ( 'Did not understand %r, expected digit-digit' % c ) inc = 1 if start < end else - 1 nums . extend ( range ( start , end + inc , inc ) ) return [ n - 1 for n in nums ]
11991	def decode_html_entities ( html ) : if not html : return html for entity , char in six . iteritems ( html_entity_map ) : html = html . replace ( entity , char ) return html
