3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
7547	def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".alleles" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + ".loci" ) , 'r' ) longname = max ( len ( x ) for x in data . samples . keys ( ) ) name_padding = 5 writing = [ ] loc = 0 for line in lines : if ">" in line : name , seq = line . split ( " " ) [ 0 ] , line . split ( " " ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) writing . append ( name + "_0" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele1 ) writing . append ( name + "_1" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 if not loc % 10000 : outfile . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] outfile . write ( "\n" . join ( writing ) ) outfile . close ( )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
10363	def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
3663	def calculate_integral_over_T ( self , T1 , T2 , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral_over_T ( T1 , T2 ) elif method == POLING_CONST : return self . POLING_constant * log ( T2 / T1 ) elif method == CRCSTD : return self . CRCSTD_constant * log ( T2 / T1 ) elif method == DADGOSTAR_SHAW : dS = ( Dadgostar_Shaw_integral_over_T ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral_over_T ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dS , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( lambda T : self . calculate ( T , method ) / T , T1 , T2 ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
1102	def restore ( delta , which ) : r try : tag = { 1 : "- " , 2 : "+ " } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( " " , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]
5301	def parse_colors ( path ) : if path . endswith ( ".txt" ) : return parse_rgb_txt_file ( path ) elif path . endswith ( ".json" ) : return parse_json_color_file ( path ) raise TypeError ( "colorful only supports .txt and .json files for colors" )
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
4147	def DaniellPeriodogram ( data , P , NFFT = None , detrend = 'mean' , sampling = 1. , scale_by_freq = True , window = 'hamming' ) : r psd = speriodogram ( data , NFFT = NFFT , detrend = detrend , sampling = sampling , scale_by_freq = scale_by_freq , window = window ) if len ( psd ) % 2 == 1 : datatype = 'real' else : datatype = 'complex' N = len ( psd ) _slice = 2 * P + 1 if datatype == 'real' : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 0 : newN = psd . size / _slice else : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 1 : newN = psd . size / _slice newpsd = np . zeros ( int ( newN ) ) for i in range ( 0 , newpsd . size ) : count = 0 for n in range ( i * _slice - P , i * _slice + P + 1 ) : if n > 0 and n < N : count += 1 newpsd [ i ] += psd [ n ] newpsd [ i ] /= float ( count ) if datatype == 'complex' : freq = np . linspace ( 0 , sampling , len ( newpsd ) ) else : df = 1. / sampling freq = np . linspace ( 0 , sampling / 2. , len ( newpsd ) ) return newpsd , freq
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) self . models_name = self . kwargs . get ( 'models_name' , 'models' ) self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
3537	def crazy_egg ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return CrazyEggNode ( )
13641	def send ( self , use_open_peers = True , queue = True , ** kw ) : if not use_open_peers : ip = kw . get ( 'ip' ) port = kw . get ( 'port' ) peer = 'http://{}:{}' . format ( ip , port ) res = arky . rest . POST . peer . transactions ( peer = peer , transactions = [ self . tx . tx ] ) else : res = arky . core . sendPayload ( self . tx . tx ) if self . tx . success != '0.0%' : self . tx . error = None self . tx . success = True else : self . tx . error = res [ 'messages' ] self . tx . success = False self . tx . tries += 1 self . tx . res = res if queue : self . tx . send = True self . __save ( ) return res
6386	def _sb_r1 ( self , term , r1_prefixes = None ) : vowel_found = False if hasattr ( r1_prefixes , '__iter__' ) : for prefix in r1_prefixes : if term [ : len ( prefix ) ] == prefix : return len ( prefix ) for i in range ( len ( term ) ) : if not vowel_found and term [ i ] in self . _vowels : vowel_found = True elif vowel_found and term [ i ] not in self . _vowels : return i + 1 return len ( term )
12535	def copy_files_to_other_folder ( self , output_folder , rename_files = True , mkdir = True , verbose = False ) : import shutil if not os . path . exists ( output_folder ) : os . mkdir ( output_folder ) if not rename_files : for dcmf in self . items : outf = os . path . join ( output_folder , os . path . basename ( dcmf ) ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf ) else : n_pad = len ( self . items ) + 2 for idx , dcmf in enumerate ( self . items ) : outf = '{number:0{width}d}.dcm' . format ( width = n_pad , number = idx ) outf = os . path . join ( output_folder , outf ) if verbose : print ( '{} -> {}' . format ( dcmf , outf ) ) shutil . copyfile ( dcmf , outf )
1996	def cmp_regs ( cpu , should_print = False ) : differing = False gdb_regs = gdb . getCanonicalRegisters ( ) for name in sorted ( gdb_regs ) : vg = gdb_regs [ name ] if name . endswith ( 'psr' ) : name = 'apsr' v = cpu . read_register ( name . upper ( ) ) if should_print : logger . debug ( f'{name} gdb:{vg:x} mcore:{v:x}' ) if vg != v : if should_print : logger . warning ( '^^ unequal' ) differing = True if differing : logger . debug ( qemu . correspond ( None ) ) return differing
12177	def show_variances ( Y , variances , varianceX , logScale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) varSorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( "chronological variance" ) plt . ylabel ( "original data" ) plot_shaded_data ( X , Y , variances , varianceX ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "time in sweep (sec)" ) plt . plot ( varianceX , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "chunk number" ) plt . title ( "sorted variance" ) plt . plot ( varSorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) label = "%2d-%d percentile" % ( i , i + + PERCENT_STEP ) color = COLORMAP ( i / 100 ) print ( "%s: variance = %.02f - %.02f" % ( label , varLimitLow , varLimitHigh ) ) plt . figure ( 1 ) plt . axhspan ( varLimitLow , varLimitHigh , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunkLow = np . where ( varSorted >= varLimitLow ) [ 0 ] [ 0 ] chunkHigh = np . where ( varSorted >= varLimitHigh ) [ 0 ] [ 0 ] plt . axvspan ( chunkLow , chunkHigh , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if logScale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight_layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( logScale ) ) ) plt . show ( )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
9085	def update_backend ( use_pypi = False , index = 'dev' , build = True , user = None , version = None ) : get_vars ( ) if value_asbool ( build ) : upload_backend ( index = index , user = user ) with fab . cd ( '{apphome}' . format ( ** AV ) ) : if value_asbool ( use_pypi ) : command = 'bin/pip install --upgrade briefkasten' else : command = 'bin/pip install --upgrade --pre -i {ploy_default_publish_devpi}/briefkasten/{index}/+simple/ briefkasten' . format ( index = index , user = user , ** AV ) if version : command = '%s==%s' % ( command , version ) fab . sudo ( command ) briefkasten_ctl ( 'restart' )
11962	def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
3005	def _get_storage_model ( ) : storage_model_settings = getattr ( django . conf . settings , 'GOOGLE_OAUTH2_STORAGE_MODEL' , None ) if storage_model_settings is not None : return ( storage_model_settings [ 'model' ] , storage_model_settings [ 'user_property' ] , storage_model_settings [ 'credentials_property' ] ) else : return None , None , None
5368	def _get_storage_service ( credentials ) : if credentials is None : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
11548	def guess_array_memory_usage ( bam_readers , dtype , use_strand = False ) : ARRAY_COUNT = 5 if not isinstance ( bam_readers , list ) : bam_readers = [ bam_readers ] if isinstance ( dtype , basestring ) : dtype = NUMPY_DTYPES . get ( dtype , None ) use_strand = use_strand + 1 dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = None , force_dtype = False ) if not [ dt for dt in dtypes if dt is not None ] : dtypes = guess_numpy_dtypes_from_idxstats ( bam_readers , default = dtype or numpy . uint64 , force_dtype = True ) elif dtype : dtypes = [ dtype if dt else None for dt in dtypes ] read_groups = [ ] no_read_group = False for bam in bam_readers : rgs = bam . get_read_groups ( ) if rgs : for rg in rgs : if rg not in read_groups : read_groups . append ( rg ) else : no_read_group = True read_groups = len ( read_groups ) + no_read_group max_ref_size = 0 array_byte_overhead = sys . getsizeof ( numpy . zeros ( ( 0 ) , dtype = numpy . uint64 ) ) array_count = ARRAY_COUNT * use_strand * read_groups for bam in bam_readers : for i , ( name , length ) in enumerate ( bam . get_references ( ) ) : if dtypes [ i ] is not None : max_ref_size = max ( max_ref_size , ( length + length * dtypes [ i ] ( ) . nbytes * array_count + ( array_byte_overhead * ( array_count + 1 ) ) ) ) return max_ref_size
1141	def dedent ( text ) : margin = None text = _whitespace_only_re . sub ( '' , text ) indents = _leading_whitespace_re . findall ( text ) for indent in indents : if margin is None : margin = indent elif indent . startswith ( margin ) : pass elif margin . startswith ( indent ) : margin = indent else : for i , ( x , y ) in enumerate ( zip ( margin , indent ) ) : if x != y : margin = margin [ : i ] break else : margin = margin [ : len ( indent ) ] if 0 and margin : for line in text . split ( "\n" ) : assert not line or line . startswith ( margin ) , "line = %r, margin = %r" % ( line , margin ) if margin : text = re . sub ( r'(?m)^' + margin , '' , text ) return text
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
13025	def create_payload ( self , x86_file , x64_file , payload_file ) : sc_x86 = open ( os . path . join ( self . datadir , x86_file ) , 'rb' ) . read ( ) sc_x64 = open ( os . path . join ( self . datadir , x64_file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload_file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc_x86 ) ) ) fp . write ( sc_x86 ) fp . write ( sc_x64 ) fp . close ( )
8030	def groupByContent ( paths ) : handles , results = [ ] , [ ] hList = [ ] for path in paths : try : hList . append ( ( path , open ( path , 'rb' ) , '' ) ) except IOError : pass handles . append ( hList ) while handles : more , done = compareChunks ( handles . pop ( 0 ) ) handles . extend ( more ) results . extend ( done ) return dict ( ( x [ 0 ] , x ) for x in results )
3645	def tradepileDelete ( self , trade_id ) : method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) return True
8718	def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
13161	def select ( cls , cur , table : str , order_by : str , columns : list = None , where_keys : list = None , limit = 100 , offset = 0 ) : if columns : columns_string = cls . _COMMA . join ( columns ) if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_selective_column_with_condition . format ( columns_string , table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_selective_column . format ( columns_string , table , order_by , limit , offset ) q , t = query , ( ) else : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_all_string_with_condition . format ( table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_all_string . format ( table , order_by , limit , offset ) q , t = query , ( ) yield from cur . execute ( q , t ) return ( yield from cur . fetchall ( ) )
1119	def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list
5415	def parse_args ( parser , provider_required_args , argv ) : epilog = 'Provider-required arguments:\n' for provider in provider_required_args : epilog += ' %s: %s\n' % ( provider , provider_required_args [ provider ] ) parser . epilog = epilog args = parser . parse_args ( argv ) for arg in provider_required_args [ args . provider ] : if not args . __getattribute__ ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
12636	def dist_percentile_threshold ( dist_matrix , perc_thr = 0.05 , k = 1 ) : triu_idx = np . triu_indices ( dist_matrix . shape [ 0 ] , k = k ) upper = np . zeros_like ( dist_matrix ) upper [ triu_idx ] = dist_matrix [ triu_idx ] < np . percentile ( dist_matrix [ triu_idx ] , perc_thr ) return upper
11085	def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . _bot . runnable = False return "Shutting down..."
13846	def __get_numbered_paths ( filepath ) : format = '%s (%%d)%s' % splitext_files_only ( filepath ) return map ( lambda n : format % n , itertools . count ( 1 ) )
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
9390	def get_aggregation_timestamp ( self , timestamp , granularity = 'second' ) : if granularity is None or granularity . lower ( ) == 'none' : return int ( timestamp ) , 1 elif granularity == 'hour' : return ( int ( timestamp ) / ( 3600 * 1000 ) ) * 3600 * 1000 , 3600 elif granularity == 'minute' : return ( int ( timestamp ) / ( 60 * 1000 ) ) * 60 * 1000 , 60 else : return ( int ( timestamp ) / 1000 ) * 1000 , 1
10876	def calculate_linescan_ilm_psf ( y , z , polar_angle = 0. , nlpts = 1 , pinhole_width = 1 , use_laggauss = False , ** kwargs ) : if use_laggauss : x_vals , wts = calc_pts_lag ( ) else : x_vals , wts = calc_pts_hg ( ) xg , yg , zg = [ np . zeros ( list ( y . shape ) + [ x_vals . size ] ) for a in range ( 3 ) ] hilm = np . zeros ( xg . shape ) for a in range ( x_vals . size ) : xg [ ... , a ] = x_vals [ a ] yg [ ... , a ] = y . copy ( ) zg [ ... , a ] = z . copy ( ) y_pinhole , wts_pinhole = np . polynomial . hermite . hermgauss ( nlpts ) y_pinhole *= np . sqrt ( 2 ) * pinhole_width wts_pinhole /= np . sqrt ( np . pi ) for yp , wp in zip ( y_pinhole , wts_pinhole ) : rho = np . sqrt ( xg * xg + ( yg - yp ) * ( yg - yp ) ) phi = np . arctan2 ( yg , xg ) hsym , hasym = get_hsym_asym ( rho , zg , get_hdet = False , ** kwargs ) hilm += wp * ( hsym + np . cos ( 2 * ( phi - polar_angle ) ) * hasym ) for a in range ( x_vals . size ) : hilm [ ... , a ] *= wts [ a ] return hilm . sum ( axis = - 1 ) * 2.
2843	def disable_FTDI_driver ( ) : logger . debug ( 'Disabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) _check_running_as_root ( ) subprocess . call ( 'kextunload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . call ( 'kextunload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) _check_running_as_root ( ) subprocess . call ( 'modprobe -r -q ftdi_sio' , shell = True ) subprocess . call ( 'modprobe -r -q usbserial' , shell = True )
6078	def convolve_image ( self , image_array , blurring_array ) : return self . convolve_jit ( image_array , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths , blurring_array , self . blurring_frame_indexes , self . blurring_frame_psfs , self . blurring_frame_lengths )
682	def getSDRforValue ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . numRecords > j encoding = self . fields [ i ] . encodings [ j ] return encoding
11746	def init_app ( self , app ) : if len ( self . _attached_bundles ) == 0 : raise NoBundlesAttached ( "At least one bundle must be attached before initializing Journey" ) for bundle in self . _attached_bundles : processed_bundle = { 'path' : bundle . path , 'description' : bundle . description , 'blueprints' : [ ] } for ( bp , description ) in bundle . blueprints : blueprint = self . _register_blueprint ( app , bp , bundle . path , self . get_bp_path ( bp ) , description ) processed_bundle [ 'blueprints' ] . append ( blueprint ) self . _registered_bundles . append ( processed_bundle )
12631	def group_dicom_files ( dicom_file_paths , header_fields ) : dist = SimpleDicomFileDistance ( field_weights = header_fields ) path_list = dicom_file_paths . copy ( ) path_groups = DefaultOrderedDict ( DicomFileSet ) while len ( path_list ) > 0 : file_path1 = path_list . pop ( ) file_subgroup = [ file_path1 ] dist . set_dicom_file1 ( file_path1 ) j = len ( path_list ) - 1 while j >= 0 : file_path2 = path_list [ j ] dist . set_dicom_file2 ( file_path2 ) if dist . transform ( ) : file_subgroup . append ( file_path2 ) path_list . pop ( j ) j -= 1 path_groups [ file_path1 ] . from_set ( file_subgroup , check_if_dicoms = False ) return path_groups
3414	def model_to_dict ( model , sort = False ) : obj = OrderedDict ( ) obj [ "metabolites" ] = list ( map ( metabolite_to_dict , model . metabolites ) ) obj [ "reactions" ] = list ( map ( reaction_to_dict , model . reactions ) ) obj [ "genes" ] = list ( map ( gene_to_dict , model . genes ) ) obj [ "id" ] = model . id _update_optional ( model , obj , _OPTIONAL_MODEL_ATTRIBUTES , _ORDERED_OPTIONAL_MODEL_KEYS ) if sort : get_id = itemgetter ( "id" ) obj [ "metabolites" ] . sort ( key = get_id ) obj [ "reactions" ] . sort ( key = get_id ) obj [ "genes" ] . sort ( key = get_id ) return obj
2183	def existing_versions ( self ) : import glob pattern = join ( self . dpath , self . fname + '_*' + self . ext ) for fname in glob . iglob ( pattern ) : data_fpath = join ( self . dpath , fname ) yield data_fpath
5003	def handle ( self , * args , ** options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
10693	def rgb_to_hsv ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 _min = min ( r , g , b ) _max = max ( r , g , b ) v = _max delta = _max - _min if _max == 0 : return 0 , 0 , v s = delta / _max if delta == 0 : delta = 1 if r == _max : h = 60 * ( ( ( g - b ) / delta ) % 6 ) elif g == _max : h = 60 * ( ( ( b - r ) / delta ) + 2 ) else : h = 60 * ( ( ( r - g ) / delta ) + 4 ) return round ( h , 3 ) , round ( s , 3 ) , round ( v , 3 )
13480	def _str_replacement ( self , target , replacement ) : self . data = self . data . replace ( target , replacement )
11777	def replicated_dataset ( dataset , weights , n = None ) : "Copy dataset, replicating each example in proportion to its weight." n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
1085	def timetz ( self ) : "Return the time part, with same tzinfo." return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
13880	def MoveFile ( source_filename , target_filename ) : _AssertIsLocal ( source_filename ) _AssertIsLocal ( target_filename ) import shutil shutil . move ( source_filename , target_filename )
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common locals_ [ 'shell' ] = shell for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
11921	def paginator ( self ) : if not hasattr ( self , '_paginator' ) : if self . pagination_class is None : self . _paginator = None else : self . _paginator = self . pagination_class ( ) return self . _paginator
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
2118	def convert ( self , value , param , ctx ) : resource = tower_cli . get_resource ( self . resource_name ) if value is None : return None if isinstance ( value , int ) : return value if re . match ( r'^[\d]+$' , value ) : return int ( value ) if value == 'null' : return value try : debug . log ( 'The %s field is given as a name; ' 'looking it up.' % param . name , header = 'details' ) lookup_data = { resource . identity [ - 1 ] : value } rel = resource . get ( ** lookup_data ) except exc . MultipleResults : raise exc . MultipleRelatedError ( 'Cannot look up {0} exclusively by name, because multiple {0} ' 'objects exist with that name.\n' 'Please send an ID. You can get the ID for the {0} you want ' 'with:\n' ' tower-cli {0} list --name "{1}"' . format ( self . resource_name , value ) , ) except exc . TowerCLIError as ex : raise exc . RelatedError ( 'Could not get %s. %s' % ( self . resource_name , str ( ex ) ) ) return rel [ 'id' ]
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
7700	def verify_roster_set ( self , fix = False , settings = None ) : try : self . _verify ( ( None , u"remove" ) , fix ) except ValueError , err : raise BadRequestProtocolError ( unicode ( err ) ) if self . ask : if fix : self . ask = None else : raise BadRequestProtocolError ( "'ask' in roster set" ) if self . approved : if fix : self . approved = False else : raise BadRequestProtocolError ( "'approved' in roster set" ) if settings is None : settings = XMPPSettings ( ) name_length_limit = settings [ "roster_name_length_limit" ] if self . name and len ( self . name ) > name_length_limit : raise NotAcceptableProtocolError ( u"Roster item name too long" ) group_length_limit = settings [ "roster_group_name_length_limit" ] for group in self . groups : if not group : raise NotAcceptableProtocolError ( u"Roster group name empty" ) if len ( group ) > group_length_limit : raise NotAcceptableProtocolError ( u"Roster group name too long" ) if self . _duplicate_group : raise BadRequestProtocolError ( u"Item group duplicated" )
3494	def total_yield ( input_fluxes , input_elements , output_flux , output_elements ) : carbon_input_flux = sum ( total_components_flux ( flux , components , consumption = True ) for flux , components in zip ( input_fluxes , input_elements ) ) carbon_output_flux = total_components_flux ( output_flux , output_elements , consumption = False ) try : return carbon_output_flux / carbon_input_flux except ZeroDivisionError : return nan
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
3395	def remove_genes ( cobra_model , gene_list , remove_reactions = True ) : gene_set = { cobra_model . genes . get_by_id ( str ( i ) ) for i in gene_list } gene_id_set = { i . id for i in gene_set } remover = _GeneRemover ( gene_id_set ) ast_rules = get_compiled_gene_reaction_rules ( cobra_model ) target_reactions = [ ] for reaction , rule in iteritems ( ast_rules ) : if reaction . gene_reaction_rule is None or len ( reaction . gene_reaction_rule ) == 0 : continue if remove_reactions and not eval_gpr ( rule , gene_id_set ) : target_reactions . append ( reaction ) else : remover . visit ( rule ) new_rule = ast2str ( rule ) if new_rule != reaction . gene_reaction_rule : reaction . gene_reaction_rule = new_rule for gene in gene_set : cobra_model . genes . remove ( gene ) associated_groups = cobra_model . get_associated_groups ( gene ) for group in associated_groups : group . remove_members ( gene ) cobra_model . remove_reactions ( target_reactions )
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
3437	def repair ( self , rebuild_index = True , rebuild_relationships = True ) : if rebuild_index : self . reactions . _generate_index ( ) self . metabolites . _generate_index ( ) self . genes . _generate_index ( ) self . groups . _generate_index ( ) if rebuild_relationships : for met in self . metabolites : met . _reaction . clear ( ) for gene in self . genes : gene . _reaction . clear ( ) for rxn in self . reactions : for met in rxn . _metabolites : met . _reaction . add ( rxn ) for gene in rxn . _genes : gene . _reaction . add ( rxn ) for l in ( self . reactions , self . genes , self . metabolites , self . groups ) : for e in l : e . _model = self
13014	def remove_namespace ( doc , namespace ) : ns = u'{%s}' % namespace nsl = len ( ns ) for elem in doc . getiterator ( ) : if elem . tag . startswith ( ns ) : elem . tag = elem . tag [ nsl : ] elem . attrib [ 'oxmlns' ] = namespace
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
10107	def get_context_data ( self , ** kwargs ) : context = super ( TabView , self ) . get_context_data ( ** kwargs ) context . update ( kwargs ) process_tabs_kwargs = { 'tabs' : self . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : self , } context [ 'tabs' ] = self . _process_tabs ( ** process_tabs_kwargs ) context [ 'current_tab_id' ] = self . tab_id if self . tab_parent is not None : if self . tab_parent not in self . _registry : msg = '%s has no attribute _is_tab' % self . tab_parent . __class__ . __name__ raise ImproperlyConfigured ( msg ) parent = self . tab_parent ( ) process_parents_kwargs = { 'tabs' : parent . get_group_tabs ( ) , 'current_tab' : self , 'group_current_tab' : parent , } context [ 'parent_tabs' ] = self . _process_tabs ( ** process_parents_kwargs ) context [ 'parent_tab_id' ] = parent . tab_id if self . tab_id in self . _children : process_children_kwargs = { 'tabs' : [ t ( ) for t in self . _children [ self . tab_id ] ] , 'current_tab' : self , 'group_current_tab' : None , } context [ 'child_tabs' ] = self . _process_tabs ( ** process_children_kwargs ) return context
6621	def getVector ( self , tree , branchName ) : if ( tree , branchName ) in self . __class__ . addressDict : return self . __class__ . addressDict [ ( tree , branchName ) ] itsVector = self . _getVector ( tree , branchName ) self . __class__ . addressDict [ ( tree , branchName ) ] = itsVector return itsVector
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
3606	def get_async ( self , url , name , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_get_request , args = ( endpoint , params , headers ) , callback = callback )
316	def perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' ) : stats = pd . Series ( ) for stat_func in SIMPLE_STAT_FUNCS : stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = stat_func ( returns ) if positions is not None : stats [ 'Gross leverage' ] = gross_lev ( positions ) . mean ( ) if transactions is not None : stats [ 'Daily turnover' ] = get_turnover ( positions , transactions , turnover_denom ) . mean ( ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : res = stat_func ( returns , factor_returns ) stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = res return stats
4441	async def _find ( self , ctx , * , query ) : if not query . startswith ( 'ytsearch:' ) and not query . startswith ( 'scsearch:' ) : query = 'ytsearch:' + query results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found' ) tracks = results [ 'tracks' ] [ : 10 ] o = '' for index , track in enumerate ( tracks , start = 1 ) : track_title = track [ "info" ] [ "title" ] track_uri = track [ "info" ] [ "uri" ] o += f'`{index}.` [{track_title}]({track_uri})\n' embed = discord . Embed ( color = discord . Color . blurple ( ) , description = o ) await ctx . send ( embed = embed )
9693	def cut ( self , by , from_start = True ) : s , e = copy ( self . start ) , copy ( self . end ) if from_start : e = s + by else : s = e - by return Range ( s , e )
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
8448	def _has_branch ( branch ) : ret = temple . utils . shell ( 'git rev-parse --verify {}' . format ( branch ) , stderr = subprocess . DEVNULL , stdout = subprocess . DEVNULL , check = False ) return ret . returncode == 0
1980	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to read from a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to read to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to read a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( rx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_receive ( cpu , fd , buf , count , rx_bytes )
11966	def _bin_to_dec ( ip , check = True ) : if check and not is_bin ( ip ) : raise ValueError ( '_bin_to_dec: invalid IP: "%s"' % ip ) if isinstance ( ip , int ) : ip = str ( ip ) return int ( str ( ip ) , 2 )
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : stream_queue . put ( line ) stream_queue . put ( None ) stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True _thread . start ( ) return stream_queue
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
6276	def get_loader ( self , meta : ResourceDescription , raise_on_error = False ) -> BaseLoader : for loader in self . _loaders : if loader . name == meta . loader : return loader if raise_on_error : raise ImproperlyConfigured ( "Resource has invalid loader '{}': {}\nAvailiable loaders: {}" . format ( meta . loader , meta , [ loader . name for loader in self . _loaders ] ) )
12918	def delete ( self ) : if len ( self ) == 0 : return 0 mdl = self . getModel ( ) return mdl . deleter . deleteMultiple ( self )
13371	def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
4699	def get_meta ( offset , length , output ) : if env ( ) : cij . err ( "cij.nvme.meta: Invalid NVMe ENV." ) return 1 nvme = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) max_size = 0x40000 with open ( output , "wb" ) as fout : for off in range ( offset , length , max_size ) : size = min ( length - off , max_size ) cmd = [ "nvme get-log" , nvme [ "DEV_PATH" ] , "-i 0xca" , "-o 0x%x" % off , "-l 0x%x" % size , "-b" ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : cij . err ( "cij.nvme.meta: Error get chunk meta" ) return 1 fout . write ( stdout ) return 0
3665	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cplms = [ i ( T ) for i in self . HeatCapacityLiquids ] return mixing_simple ( zs , Cplms ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) Cpl = Laliberte_heat_capacity ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return property_mass_to_molar ( Cpl , MW ) else : raise Exception ( 'Method not valid' )
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
7755	def set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : self . lock . acquire ( ) try : self . _set_response_handlers ( stanza , res_handler , err_handler , timeout_handler , timeout ) finally : self . lock . release ( )
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
8238	def analogous ( clr , angle = 10 , contrast = 0.25 ) : contrast = max ( 0 , min ( contrast , 1.0 ) ) clr = color ( clr ) colors = colorlist ( clr ) for i , j in [ ( 1 , 2.2 ) , ( 2 , 1 ) , ( - 1 , - 0.5 ) , ( - 2 , 1 ) ] : c = clr . rotate_ryb ( angle * i ) t = 0.44 - j * 0.1 if clr . brightness - contrast * j < t : c . brightness = t else : c . brightness = clr . brightness - contrast * j c . saturation -= 0.05 colors . append ( c ) return colors
7387	def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
12174	def genIndex ( folder , forceIDs = [ ] ) : if not os . path . exists ( folder + "/swhlab4/" ) : print ( " !! cannot index if no /swhlab4/" ) return timestart = cm . timethis ( ) files = glob . glob ( folder + "/*.*" ) files . extend ( glob . glob ( folder + "/swhlab4/*.*" ) ) print ( " -- indexing glob took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) files . extend ( genPNGs ( folder , files ) ) files = sorted ( files ) timestart = cm . timethis ( ) d = cm . getIDfileDict ( files ) print ( " -- filedict length:" , len ( d ) ) print ( " -- generating ID dict took %.02f ms" % ( cm . timethis ( timestart ) * 1000 ) ) groups = cm . getABFgroups ( files ) print ( " -- groups length:" , len ( groups ) ) for ID in sorted ( list ( groups . keys ( ) ) ) : overwrite = False for abfID in groups [ ID ] : if abfID in forceIDs : overwrite = True try : htmlABF ( ID , groups [ ID ] , d , folder , overwrite ) except : print ( "~~ HTML GENERATION FAILED!!!" ) menu = expMenu ( groups , folder ) makeSplash ( menu , folder ) makeMenu ( menu , folder ) htmlFrames ( d , folder ) makeMenu ( menu , folder ) makeSplash ( menu , folder )
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
5952	def start_logging ( logfile = "gromacs.log" ) : from . import log log . create ( "gromacs" , logfile = logfile ) logging . getLogger ( "gromacs" ) . info ( "GromacsWrapper %s STARTED logging to %r" , __version__ , logfile )
2881	def get_message_event_definition ( self , messageEventDefinition ) : messageRef = first ( self . xpath ( './/bpmn:messageRef' ) ) message = messageRef . get ( 'name' ) if messageRef is not None else self . node . get ( 'name' ) return MessageEventDefinition ( message )
9311	def amz_cano_querystring ( qs ) : safe_qs_amz_chars = '&=+' safe_qs_unresvd = '-_.~' if PY2 : qs = qs . encode ( 'utf-8' ) safe_qs_amz_chars = safe_qs_amz_chars . encode ( ) safe_qs_unresvd = safe_qs_unresvd . encode ( ) qs = unquote ( qs ) space = b' ' if PY2 else ' ' qs = qs . split ( space ) [ 0 ] qs = quote ( qs , safe = safe_qs_amz_chars ) qs_items = { } for name , vals in parse_qs ( qs , keep_blank_values = True ) . items ( ) : name = quote ( name , safe = safe_qs_unresvd ) vals = [ quote ( val , safe = safe_qs_unresvd ) for val in vals ] qs_items [ name ] = vals qs_strings = [ ] for name , vals in qs_items . items ( ) : for val in vals : qs_strings . append ( '=' . join ( [ name , val ] ) ) qs = '&' . join ( sorted ( qs_strings ) ) if PY2 : qs = unicode ( qs ) return qs
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
7690	def sonify ( annotation , sr = 22050 , duration = None , ** kwargs ) : length = None if duration is None : duration = annotation . duration if duration is not None : length = int ( duration * sr ) if annotation . namespace in SONIFY_MAPPING : ann = coerce_annotation ( annotation , annotation . namespace ) return SONIFY_MAPPING [ annotation . namespace ] ( ann , sr = sr , length = length , ** kwargs ) for namespace , func in six . iteritems ( SONIFY_MAPPING ) : try : ann = coerce_annotation ( annotation , namespace ) return func ( ann , sr = sr , length = length , ** kwargs ) except NamespaceError : pass raise NamespaceError ( 'Unable to sonify annotation of namespace="{:s}"' . format ( annotation . namespace ) )
13831	def _api_call ( self , method_name , * args , ** kwargs ) : params = kwargs . setdefault ( 'params' , { } ) params . update ( { 'key' : self . _apikey } ) if self . _token is not None : params . update ( { 'token' : self . _token } ) http_method = getattr ( requests , method_name ) return http_method ( TRELLO_URL + self . _url , * args , ** kwargs )
4885	def allow_request ( self , request , view ) : service_users = get_service_usernames ( ) if request . user . username in service_users : self . update_throttle_scope ( ) return super ( ServiceUserThrottle , self ) . allow_request ( request , view )
3984	def get_same_container_repos ( app_or_library_name ) : specs = get_expanded_libs_specs ( ) spec = specs . get_app_or_lib ( app_or_library_name ) return get_same_container_repos_from_spec ( spec )
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) packet += HeronProtocol . pack_int ( datasize ) packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename packet += reqid . pack ( ) packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
620	def parseSdr ( s ) : assert isinstance ( s , basestring ) sdr = [ int ( c ) for c in s if c in ( "0" , "1" ) ] if len ( sdr ) != len ( s ) : raise ValueError ( "The provided string %s is malformed. The string should " "have only 0's and 1's." ) return sdr
2944	def accept_message ( self , message ) : assert not self . read_only self . refresh_waiting_tasks ( ) self . do_engine_steps ( ) for my_task in Task . Iterator ( self . task_tree , Task . WAITING ) : my_task . task_spec . accept_message ( my_task , message )
4849	def _serialize_items ( self , channel_metadata_items ) : return json . dumps ( self . _prepare_items_for_transmission ( channel_metadata_items ) , sort_keys = True ) . encode ( 'utf-8' )
11653	def __get_live_version ( self ) : try : import versiontools except ImportError : return None else : return str ( versiontools . Version . from_expression ( self . name ) )
2868	def get_platform_gpio ( ** keywords ) : plat = Platform . platform_detect ( ) if plat == Platform . RASPBERRY_PI : import RPi . GPIO return RPiGPIOAdapter ( RPi . GPIO , ** keywords ) elif plat == Platform . BEAGLEBONE_BLACK : import Adafruit_BBIO . GPIO return AdafruitBBIOAdapter ( Adafruit_BBIO . GPIO , ** keywords ) elif plat == Platform . MINNOWBOARD : import mraa return AdafruitMinnowAdapter ( mraa , ** keywords ) elif plat == Platform . JETSON_NANO : import Jetson . GPIO return RPiGPIOAdapter ( Jetson . GPIO , ** keywords ) elif plat == Platform . UNKNOWN : raise RuntimeError ( 'Could not determine platform.' )
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
5061	def get_enterprise_customer_for_user ( auth_user ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( user_id = auth_user . id ) . enterprise_customer except EnterpriseCustomerUser . DoesNotExist : return None
6736	def reboot_or_dryrun ( * args , ** kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) try : if verbose : print ( 'Reconnecting to:' , env . host_string ) connections . connect ( env . host_string ) with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
4508	def error ( self , fail = True , action = '' ) : e = 'There was an unknown error communicating with the device.' if action : e = 'While %s: %s' % ( action , e ) log . error ( e ) if fail : raise IOError ( e )
1018	def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] )
8468	def parseConfig ( cls , value ) : if 'enabled' in value : value [ 'enabled' ] = bool ( value [ 'enabled' ] ) if 'exclude_paths' in value : value [ 'exclude_paths' ] = [ n . strip ( ) for n in ast . literal_eval ( value [ 'exclude_paths' ] ) ] return value
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
5503	def config ( ctx , key , value , remove , edit ) : conf = ctx . obj [ "conf" ] if not edit and not key : raise click . BadArgumentUsage ( "You have to specify either a key or use --edit." ) if edit : return click . edit ( filename = conf . config_file ) if remove : try : conf . cfg . remove_option ( key [ 0 ] , key [ 1 ] ) except Exception as e : logger . debug ( e ) else : conf . write_config ( ) return if not value : try : click . echo ( conf . cfg . get ( key [ 0 ] , key [ 1 ] ) ) except Exception as e : logger . debug ( e ) return if not conf . cfg . has_section ( key [ 0 ] ) : conf . cfg . add_section ( key [ 0 ] ) conf . cfg . set ( key [ 0 ] , key [ 1 ] , value ) conf . write_config ( )
3500	def assess_component ( model , reaction , side , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] result_key = dict ( reactants = 'produced' , products = 'capacity' ) [ side ] get_components = attrgetter ( side ) with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True simulation_results = { } demand_reactions = { } for component in get_components ( reaction ) : coeff = reaction . metabolites [ component ] demand = m . add_boundary ( component , type = 'demand' ) demand . metabolites [ component ] = coeff demand_reactions [ demand ] = ( component , coeff ) joint_demand = Reaction ( "joint_demand" ) for demand_reaction in demand_reactions : joint_demand += demand_reaction m . add_reactions ( [ joint_demand ] ) m . objective = joint_demand if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True for demand_reaction , ( component , coeff ) in iteritems ( demand_reactions ) : with m : m . objective = demand_reaction flux = _optimize_or_value ( m , solver = solver ) if flux_coefficient_cutoff > flux : simulation_results . update ( { component : { 'required' : flux_coefficient_cutoff / abs ( coeff ) , result_key : flux / abs ( coeff ) } } ) if len ( simulation_results ) == 0 : simulation_results = False return simulation_results
8324	def isString ( s ) : try : return isinstance ( s , unicode ) or isinstance ( s , basestring ) except NameError : return isinstance ( s , str )
11166	def unusedoptions ( self , sections ) : unused = set ( [ ] ) for section in _list ( sections ) : if not self . has_section ( section ) : continue options = self . options ( section ) raw_values = [ self . get ( section , option , raw = True ) for option in options ] for option in options : formatter = "%(" + option + ")s" for raw_value in raw_values : if formatter in raw_value : break else : unused . add ( option ) return list ( unused )
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
7655	def update ( self , ** kwargs ) : for name , value in six . iteritems ( kwargs ) : setattr ( self , name , value )
8320	def parse_categories ( self , markup ) : categories = [ ] m = re . findall ( self . re [ "category" ] , markup ) for category in m : category = category . split ( "|" ) page = category [ 0 ] . strip ( ) display = u"" if len ( category ) > 1 : display = category [ 1 ] . strip ( ) if not page in categories : categories . append ( page ) return categories
2132	def _compare_node_lists ( old , new ) : to_expand = [ ] to_delete = [ ] to_recurse = [ ] old_records = { } new_records = { } for tree_node in old : old_records . setdefault ( tree_node . unified_job_template , [ ] ) old_records [ tree_node . unified_job_template ] . append ( tree_node ) for tree_node in new : new_records . setdefault ( tree_node . unified_job_template , [ ] ) new_records [ tree_node . unified_job_template ] . append ( tree_node ) for ujt_id in old_records : if ujt_id not in new_records : to_delete . extend ( old_records [ ujt_id ] ) continue old_list = old_records [ ujt_id ] new_list = new_records . pop ( ujt_id ) if len ( old_list ) == 1 and len ( new_list ) == 1 : to_recurse . append ( ( old_list [ 0 ] , new_list [ 0 ] ) ) else : to_delete . extend ( old_list ) to_expand . extend ( new_list ) for nodes in new_records . values ( ) : to_expand . extend ( nodes ) return to_expand , to_delete , to_recurse
7294	def create_list_dict ( self , document , list_field , doc_key ) : list_dict = { "_document" : document } if isinstance ( list_field . field , EmbeddedDocumentField ) : list_dict . update ( self . create_document_dictionary ( document = list_field . field . document_type_obj , owner_document = document ) ) list_dict . update ( { "_document_field" : list_field . field , "_key" : doc_key , "_field_type" : ListField , "_widget" : get_widget ( list_field . field ) , "_value" : getattr ( document , doc_key , None ) } ) return list_dict
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
10988	def _translate_particles ( s , max_mem = 1e9 , desc = '' , min_rad = 'calc' , max_rad = 'calc' , invert = 'guess' , rz_order = 0 , do_polish = True ) : if desc is not None : desc_trans = desc + 'translate-particles' desc_burn = desc + 'addsub_burn' desc_polish = desc + 'addsub_polish' else : desc_trans , desc_burn , desc_polish = [ None ] * 3 RLOG . info ( 'Translate Particles:' ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.1 , desc = desc_trans , max_mem = max_mem , include_rad = False , dowarn = False ) opt . burn ( s , mode = 'do-particles' , n_loop = 4 , fractol = 0.05 , desc = desc_trans , max_mem = max_mem , include_rad = True , dowarn = False ) RLOG . info ( 'Start add-subtract' ) addsub . add_subtract ( s , tries = 30 , min_rad = min_rad , max_rad = max_rad , invert = invert ) if desc is not None : states . save ( s , desc = desc + 'translate-addsub' ) if do_polish : RLOG . info ( 'Final Burn:' ) opt . burn ( s , mode = 'burn' , n_loop = 3 , fractol = 3e-4 , desc = desc_burn , max_mem = max_mem , rz_order = rz_order , dowarn = False ) RLOG . info ( 'Final Polish:' ) d = opt . burn ( s , mode = 'polish' , n_loop = 4 , fractol = 3e-4 , desc = desc_polish , max_mem = max_mem , rz_order = rz_order , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
2314	def anm_score ( self , x , y ) : gp = GaussianProcessRegressor ( ) . fit ( x , y ) y_predict = gp . predict ( x ) indepscore = normalized_hsic ( y_predict - y , x ) return indepscore
11960	def is_bits_nm ( nm ) : try : bits = int ( str ( nm ) ) except ValueError : return False if bits > 32 or bits < 0 : return False return True
11797	def nconflicts ( self , var , val , assignment ) : "Return the number of conflicts var=val has with other variables." def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count_if ( conflict , self . neighbors [ var ] )
4131	def get_docstring_and_rest ( filename ) : with open ( filename ) as f : content = f . read ( ) node = ast . parse ( content ) if not isinstance ( node , ast . Module ) : raise TypeError ( "This function only supports modules. " "You provided {0}" . format ( node . __class__ . __name__ ) ) if node . body and isinstance ( node . body [ 0 ] , ast . Expr ) and isinstance ( node . body [ 0 ] . value , ast . Str ) : docstring_node = node . body [ 0 ] docstring = docstring_node . value . s rest = content . split ( '\n' , docstring_node . lineno ) [ - 1 ] return docstring , rest else : raise ValueError ( ( 'Could not find docstring in file "{0}". ' 'A docstring is required by sphinx-gallery' ) . format ( filename ) )
2326	def orient_graph ( self , df_data , graph , nb_runs = 6 , printout = None , ** kwargs ) : if type ( graph ) == nx . DiGraph : edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) ] oriented_edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) not in list ( graph . edges ( ) ) ] for a in edges : if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) : edges . remove ( a ) output = nx . DiGraph ( ) for i in oriented_edges : output . add_edge ( * i ) elif type ( graph ) == nx . Graph : edges = list ( graph . edges ( ) ) output = nx . DiGraph ( ) else : raise TypeError ( "Data type not understood." ) res = [ ] for idx , ( a , b ) in enumerate ( edges ) : weight = self . predict_proba ( df_data [ a ] . values . reshape ( ( - 1 , 1 ) ) , df_data [ b ] . values . reshape ( ( - 1 , 1 ) ) , idx = idx , nb_runs = nb_runs , ** kwargs ) if weight > 0 : output . add_edge ( a , b , weight = weight ) else : output . add_edge ( b , a , weight = abs ( weight ) ) if printout is not None : res . append ( [ str ( a ) + '-' + str ( b ) , weight ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) for node in list ( df_data . columns . values ) : if node not in output . nodes ( ) : output . add_node ( node ) return output
7720	def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : child . unlinkNode ( ) child . freeNode ( ) break if parameters . maxchars and parameters . maxchars < 0 : raise ValueError ( "History parameter maxchars must be positive" ) if parameters . maxstanzas and parameters . maxstanzas < 0 : raise ValueError ( "History parameter maxstanzas must be positive" ) if parameters . maxseconds and parameters . maxseconds < 0 : raise ValueError ( "History parameter maxseconds must be positive" ) hnode = self . xmlnode . newChild ( self . xmlnode . ns ( ) , "history" , None ) if parameters . maxchars is not None : hnode . setProp ( "maxchars" , str ( parameters . maxchars ) ) if parameters . maxstanzas is not None : hnode . setProp ( "maxstanzas" , str ( parameters . maxstanzas ) ) if parameters . maxseconds is not None : hnode . setProp ( "maxseconds" , str ( parameters . maxseconds ) ) if parameters . since is not None : hnode . setProp ( "since" , parameters . since . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) )
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s ' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
9751	def find_fann ( ) : if sys . platform == "win32" : dirs = sys . path for ver in dirs : if os . path . isdir ( ver ) : if find_x ( ver ) : return True raise Exception ( "Couldn't find FANN source libs!" ) else : dirs = [ '/lib' , '/usr/lib' , '/usr/lib64' , '/usr/local/lib' , '/usr/pkg/lib' ] for path in dirs : if os . path . isdir ( path ) : if find_x ( path ) : return True raise Exception ( "Couldn't find FANN source libs!" )
2189	def _rectify_products ( self , product = None ) : products = self . product if product is None else product if products is None : return None if not isinstance ( products , ( list , tuple ) ) : products = [ products ] return products
4790	def is_lower ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . lower ( ) : self . _err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
4754	def runlogs_to_html ( run_root ) : if not os . path . isdir ( run_root ) : return "CANNOT_LOCATE_LOGFILES" hook_enter = [ ] hook_exit = [ ] tcase = [ ] for fpath in glob . glob ( os . sep . join ( [ run_root , "*.log" ] ) ) : if "exit" in fpath : hook_exit . append ( fpath ) continue if "hook" in fpath : hook_enter . append ( fpath ) continue tcase . append ( fpath ) content = "" for fpath in hook_enter + tcase + hook_exit : content += "# BEGIN: run-log from log_fpath: %s\n" % fpath content += open ( fpath , "r" ) . read ( ) content += "# END: run-log from log_fpath: %s\n\n" % fpath return content
8213	def export_svg ( self ) : d = "" if len ( self . _points ) > 0 : d += "M " + str ( self . _points [ 0 ] . x ) + " " + str ( self . _points [ 0 ] . y ) + " " for pt in self . _points : if pt . cmd == MOVETO : d += "M " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == LINETO : d += "L " + str ( pt . x ) + " " + str ( pt . y ) + " " elif pt . cmd == CURVETO : d += "C " d += str ( pt . ctrl1 . x ) + " " + str ( pt . ctrl1 . y ) + " " d += str ( pt . ctrl2 . x ) + " " + str ( pt . ctrl2 . y ) + " " d += str ( pt . x ) + " " + str ( pt . y ) + " " c = "rgb(" c += str ( int ( self . path_color . r * 255 ) ) + "," c += str ( int ( self . path_color . g * 255 ) ) + "," c += str ( int ( self . path_color . b * 255 ) ) + ")" s = '<?xml version="1.0"?>\n' s += '<svg width="' + str ( _ctx . WIDTH ) + 'pt" height="' + str ( _ctx . HEIGHT ) + 'pt">\n' s += '<g>\n' s += '<path d="' + d + '" fill="none" stroke="' + c + '" stroke-width="' + str ( self . strokewidth ) + '" />\n' s += '</g>\n' s += '</svg>\n' f = open ( self . file + ".svg" , "w" ) f . write ( s ) f . close ( )
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
8314	def parse ( self , light = False ) : markup = self . markup self . disambiguation = self . parse_disambiguation ( markup ) self . categories = self . parse_categories ( markup ) self . links = self . parse_links ( markup ) if not light : markup = self . convert_pre ( markup ) markup = self . convert_li ( markup ) markup = self . convert_table ( markup ) markup = replace_entities ( markup ) markup = markup . replace ( "{{Cite" , "{{cite" ) markup = re . sub ( "\{\{ {1,2}cite" , "{{cite" , markup ) self . references , markup = self . parse_references ( markup ) markup = re . sub ( "\n+(\{\{legend)" , "\\1" , markup ) self . images , markup = self . parse_images ( markup ) self . images . extend ( self . parse_gallery_images ( markup ) ) self . paragraphs = self . parse_paragraphs ( markup ) self . tables = self . parse_tables ( markup ) self . translations = self . parse_translations ( markup ) self . important = self . parse_important ( markup )
8595	def create_group ( self , group ) : data = json . dumps ( self . _create_group_dict ( group ) ) response = self . _perform_request ( url = '/um/groups' , method = 'POST' , data = data ) return response
1472	def main ( ) : shell_env = os . environ . copy ( ) shell_env [ "PEX_ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) executor = HeronExecutor ( sys . argv , shell_env ) executor . initialize ( ) start ( executor )
2091	def copy ( self , pk = None , new_name = None , ** kwargs ) : orig = self . read ( pk , fail_on_no_results = True , fail_on_multiple_results = True ) orig = orig [ 'results' ] [ 0 ] self . _pop_none ( kwargs ) newresource = copy ( orig ) newresource . pop ( 'id' ) basename = newresource [ 'name' ] . split ( '@' , 1 ) [ 0 ] . strip ( ) for field in self . fields : if field . multiple and field . name in newresource : newresource [ field . name ] = ( newresource . get ( field . name ) , ) if new_name is None : newresource [ 'name' ] = "%s @ %s" % ( basename , time . strftime ( '%X' ) ) newresource . update ( kwargs ) return self . write ( create_on_missing = True , fail_on_found = True , ** newresource ) else : if kwargs : raise exc . TowerCLIError ( 'Cannot override {} and also use --new-name.' . format ( kwargs . keys ( ) ) ) copy_endpoint = '{}/{}/copy/' . format ( self . endpoint . strip ( '/' ) , pk ) return client . post ( copy_endpoint , data = { 'name' : new_name } ) . json ( )
2040	def RETURN ( self , offset , size ) : data = self . read_buffer ( offset , size ) raise EndTx ( 'RETURN' , data )
6225	def move_state ( self , direction , activate ) : if direction == RIGHT : self . _xdir = POSITIVE if activate else STILL elif direction == LEFT : self . _xdir = NEGATIVE if activate else STILL elif direction == FORWARD : self . _zdir = NEGATIVE if activate else STILL elif direction == BACKWARD : self . _zdir = POSITIVE if activate else STILL elif direction == UP : self . _ydir = POSITIVE if activate else STILL elif direction == DOWN : self . _ydir = NEGATIVE if activate else STILL
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
13223	def dinner ( self , message = "Dinner is served" , shout : bool = False ) : return self . helper . output ( message , shout )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
13452	def imgmin ( self ) : if not hasattr ( self , '_imgmin' ) : imgmin = _np . min ( self . images [ 0 ] ) for img in self . images : imin = _np . min ( img ) if imin > imgmin : imgmin = imin self . _imgmin = imgmin return _np . min ( self . image )
1605	def run_bolts ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) bolt_name = cl_args [ 'bolt' ] if bolt_name : if bolt_name in bolts : bolts = [ bolt_name ] else : Log . error ( 'Unknown bolt: \'%s\'' % bolt_name ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False bolts_result = [ ] for bolt in bolts : try : metrics = tracker_access . get_component_metrics ( bolt , cluster , env , topology , role ) stat , header = to_table ( metrics ) bolts_result . append ( ( bolt , stat , header ) ) except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False for i , ( bolt , stat , header ) in enumerate ( bolts_result ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % bolt ) print ( tabulate ( stat , headers = header ) ) return True
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
11122	def add_directory ( self , relativePath , info = None ) : path = os . path . normpath ( relativePath ) currentDir = self . path currentDict = self if path in ( "" , "." ) : return currentDict save = False for dir in path . split ( os . sep ) : dirPath = os . path . join ( currentDir , dir ) if not os . path . exists ( dirPath ) : os . mkdir ( dirPath ) currentDict = dict . __getitem__ ( currentDict , "directories" ) if currentDict . get ( dir , None ) is None : save = True currentDict [ dir ] = { "directories" : { } , "files" : { } , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "info" : info } currentDict = currentDict [ dir ] currentDir = dirPath if save : self . save ( ) return currentDict
13562	def save ( self , * args , ** kwargs ) : rerank = kwargs . pop ( 'rerank' , True ) if rerank : if not self . id : self . _process_new_rank_obj ( ) elif self . rank == self . _rank_at_load : pass else : self . _process_moved_rank_obj ( ) super ( RankedModel , self ) . save ( * args , ** kwargs )
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : func = getattr ( instance , method_name ) if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : setattr ( module_obj , method_name , func ) else : _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
2571	def send_message ( self ) : start = time . time ( ) message = None if not self . initialized : message = self . construct_start_message ( ) self . initialized = True else : message = self . construct_end_message ( ) self . send_UDP_message ( message ) end = time . time ( ) return end - start
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) result_dir = os . path . dirname ( result_fullpath ) alphatwirl . mkdir_p ( result_dir ) return package_index
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
8873	def isA ( instance , typeList ) : return any ( map ( lambda iType : isinstance ( instance , iType ) , typeList ) )
3403	def find_boundary_types ( model , boundary_type , external_compartment = None ) : if not model . boundary : LOGGER . warning ( "There are no boundary reactions in this model. " "Therefore specific types of boundary reactions such " "as 'exchanges', 'demands' or 'sinks' cannot be " "identified." ) return [ ] if external_compartment is None : external_compartment = find_external_compartment ( model ) return model . reactions . query ( lambda r : is_boundary_type ( r , boundary_type , external_compartment ) )
6896	def pdw_worker ( task ) : frequency = task [ 0 ] times , modmags = task [ 1 ] , task [ 2 ] fold_time = task [ 3 ] j_range = range ( task [ 4 ] ) keep_threshold_1 = task [ 5 ] keep_threshold_2 = task [ 6 ] phasebinsize = task [ 7 ] try : period = 1.0 / frequency phased = phase_magseries ( times , modmags , period , fold_time , wrap = False , sort = True ) if phasebinsize is not None and phasebinsize > 0 : bphased = pwd_phasebin ( phased [ 'phase' ] , phased [ 'mags' ] , binsize = phasebinsize ) phase_sorted = bphased [ 0 ] mod_mag_sorted = bphased [ 1 ] j_range = range ( len ( mod_mag_sorted ) - 1 ) else : phase_sorted = phased [ 'phase' ] mod_mag_sorted = phased [ 'mags' ] rolledmags = nproll ( mod_mag_sorted , 1 ) rolledphases = nproll ( phase_sorted , 1 ) strings = ( ( rolledmags - mod_mag_sorted ) * ( rolledmags - mod_mag_sorted ) + ( rolledphases - phase_sorted ) * ( rolledphases - phase_sorted ) ) strings [ 0 ] = ( ( ( mod_mag_sorted [ 0 ] - mod_mag_sorted [ - 1 ] ) * ( mod_mag_sorted [ 0 ] - mod_mag_sorted [ - 1 ] ) ) + ( ( phase_sorted [ 0 ] - phase_sorted [ - 1 ] + 1 ) * ( phase_sorted [ 0 ] - phase_sorted [ - 1 ] + 1 ) ) ) strlen = npsum ( npsqrt ( strings ) ) if ( keep_threshold_1 < strlen < keep_threshold_2 ) : p_goodflag = True else : p_goodflag = False return ( period , strlen , p_goodflag ) except Exception as e : LOGEXCEPTION ( 'error in DWP' ) return ( period , npnan , False )
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , ** kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( ** params ) else : params [ 'Body' ] = file resp = await self . put_object ( ** params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
3983	def get_same_container_repos_from_spec ( app_or_library_spec ) : repos = set ( ) app_or_lib_repo = get_repo_of_app_or_library ( app_or_library_spec . name ) if app_or_lib_repo is not None : repos . add ( app_or_lib_repo ) for dependent_name in app_or_library_spec [ 'depends' ] [ 'libs' ] : repos . add ( get_repo_of_app_or_library ( dependent_name ) ) return repos
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
1274	def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
5072	def get_configuration_value ( val_name , default = None , ** kwargs ) : if kwargs . get ( 'type' ) == 'url' : return get_url ( val_name ) or default if callable ( get_url ) else default return configuration_helpers . get_value ( val_name , default , ** kwargs ) if configuration_helpers else default
9107	def sanitize_filename ( filename ) : token = generate_drop_id ( ) name , extension = splitext ( filename ) if extension : return '%s%s' % ( token , extension ) else : return token
3778	def T_dependent_property_integral ( self , T1 , T2 ) : r Tavg = 0.5 * ( T1 + T2 ) if self . method : if self . test_method_validity ( Tavg , self . method ) : try : return self . calculate_integral ( T1 , T2 , self . method ) except : pass sorted_valid_methods = self . select_valid_methods ( Tavg ) for method in sorted_valid_methods : try : return self . calculate_integral ( T1 , T2 , method ) except : pass return None
29	def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess
7512	def padnames ( names ) : longname_len = max ( len ( i ) for i in names ) padding = 5 pnames = [ name + " " * ( longname_len - len ( name ) + padding ) for name in names ] snppad = "//" + " " * ( longname_len - 2 + padding ) return np . array ( pnames ) , snppad
9205	def before_constant ( self , constant , key ) : newlines_split = split_on_newlines ( constant ) for c in newlines_split : if is_newline ( c ) : self . current . advance_line ( ) if self . current . line > self . target . line : return self . STOP else : advance_by = len ( c ) if self . is_on_targetted_node ( advance_by ) : self . found_path = deepcopy ( self . current_path ) return self . STOP self . current . advance_columns ( advance_by )
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
9822	def list ( page ) : user = AuthConfigManager . get_value ( 'username' ) if not user : Printer . print_error ( 'Please login first. `polyaxon login --help`' ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_projects ( user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get list of projects.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Projects for current user' ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No projects found for current user' ) objects = list_dicts_to_tabulate ( [ o . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'experiment_groups' , 'experiments' , 'description' , 'num_experiments' , 'num_independent_experiments' , 'num_experiment_groups' , 'num_jobs' , 'num_builds' , 'unique_name' ] ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
2280	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_ccdr ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
3140	def get ( self , folder_id , ** queryparams ) : self . folder_id = folder_id return self . _mc_client . _get ( url = self . _build_path ( folder_id ) , ** queryparams )
8362	def encode ( self , o ) : if isinstance ( o , basestring ) : if isinstance ( o , str ) : _encoding = self . encoding if ( _encoding is not None and not ( _encoding == 'utf-8' ) ) : o = o . decode ( _encoding ) if self . ensure_ascii : return encode_basestring_ascii ( o ) else : return encode_basestring ( o ) chunks = list ( self . iterencode ( o ) ) return '' . join ( chunks )
421	def delete_training_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . TrainLog . delete_many ( kwargs ) logging . info ( "[Database] Delete TrainLog SUCCESS" )
10282	def get_peripheral_predecessor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for v in subgraph : for u , _ , k in graph . in_edges ( v , keys = True ) : if u not in subgraph : yield u , v , k
6220	def create ( self ) : dtype = NP_COMPONENT_DTYPE [ self . component_type . value ] data = numpy . frombuffer ( self . buffer . read ( byte_length = self . byte_length , byte_offset = self . byte_offset ) , count = self . count * self . components , dtype = dtype , ) return dtype , data
2439	def reset_annotations ( self ) : self . annotation_date_set = False self . annotation_comment_set = False self . annotation_type_set = False self . annotation_spdx_id_set = False
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
8076	def rectmode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . rectmode = mode return self . rectmode elif mode is None : return self . rectmode else : raise ShoebotError ( _ ( "rectmode: invalid input" ) )
3553	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
13895	def FindFiles ( dir_ , in_filters = None , out_filters = None , recursive = True , include_root_dir = True , standard_paths = False ) : if in_filters is None : in_filters = [ '*' ] if out_filters is None : out_filters = [ ] result = [ ] for dir_root , directories , filenames in os . walk ( dir_ ) : for i_directory in directories [ : ] : if MatchMasks ( i_directory , out_filters ) : directories . remove ( i_directory ) for filename in directories + filenames : if MatchMasks ( filename , in_filters ) and not MatchMasks ( filename , out_filters ) : result . append ( os . path . join ( dir_root , filename ) ) if not recursive : break if not include_root_dir : dir_prefix = len ( dir_ ) + 1 result = [ file [ dir_prefix : ] for file in result ] if standard_paths : result = map ( StandardizePath , result ) return result
11600	def get_queryset ( self , request ) : qs = super ( GalleryAdmin , self ) . get_queryset ( request ) return qs . annotate ( photo_count = Count ( 'photos' ) )
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
12751	def joint_torques ( self ) : return as_flat_array ( getattr ( j , 'amotor' , j ) . feedback [ - 1 ] [ : j . ADOF ] for j in self . joints )
10971	def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query_by_user ( current_user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per_page = per_page ) requests = Membership . query_requests ( current_user ) . count ( ) invitations = Membership . query_invitations ( current_user ) . count ( ) return render_template ( 'invenio_groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per_page = per_page , q = q )
1792	def INC ( cpu , dest ) : arg0 = dest . read ( ) res = dest . write ( arg0 + 1 ) res &= ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) cpu . AF = ( ( arg0 ^ 1 ) ^ res ) & 0x10 != 0 cpu . ZF = res == 0 cpu . SF = ( res & SIGN_MASK ) != 0 cpu . OF = res == SIGN_MASK cpu . PF = cpu . _calculate_parity_flag ( res )
3905	def add_conversation_tab ( self , conv_id , switch = False ) : conv_widget = self . get_conv_widget ( conv_id ) self . _tabbed_window . set_tab ( conv_widget , switch = switch , title = conv_widget . title )
13575	def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
12552	def write_mhd_file ( filename , data , shape = None , meta_dict = None ) : ext = get_extension ( filename ) fname = op . basename ( filename ) if ext != '.mhd' or ext != '.raw' : mhd_filename = fname + '.mhd' raw_filename = fname + '.raw' elif ext == '.mhd' : mhd_filename = fname raw_filename = remove_ext ( fname ) + '.raw' elif ext == '.raw' : mhd_filename = remove_ext ( fname ) + '.mhd' raw_filename = fname else : raise ValueError ( '`filename` extension {} from {} is not recognised. ' 'Expected .mhd or .raw.' . format ( ext , filename ) ) if meta_dict is None : meta_dict = { } if shape is None : shape = data . shape meta_dict [ 'ObjectType' ] = meta_dict . get ( 'ObjectType' , 'Image' ) meta_dict [ 'BinaryData' ] = meta_dict . get ( 'BinaryData' , 'True' ) meta_dict [ 'BinaryDataByteOrderMSB' ] = meta_dict . get ( 'BinaryDataByteOrderMSB' , 'False' ) meta_dict [ 'ElementType' ] = meta_dict . get ( 'ElementType' , NUMPY_TO_MHD_TYPE [ data . dtype . type ] ) meta_dict [ 'NDims' ] = meta_dict . get ( 'NDims' , str ( len ( shape ) ) ) meta_dict [ 'DimSize' ] = meta_dict . get ( 'DimSize' , ' ' . join ( [ str ( i ) for i in shape ] ) ) meta_dict [ 'ElementDataFile' ] = meta_dict . get ( 'ElementDataFile' , raw_filename ) mhd_filename = op . join ( op . dirname ( filename ) , mhd_filename ) raw_filename = op . join ( op . dirname ( filename ) , raw_filename ) write_meta_header ( mhd_filename , meta_dict ) dump_raw_data ( raw_filename , data ) return mhd_filename , raw_filename
2985	def try_match ( request_origin , maybe_regex ) : if isinstance ( maybe_regex , RegexObject ) : return re . match ( maybe_regex , request_origin ) elif probably_regex ( maybe_regex ) : return re . match ( maybe_regex , request_origin , flags = re . IGNORECASE ) else : try : return request_origin . lower ( ) == maybe_regex . lower ( ) except AttributeError : return request_origin == maybe_regex
4110	def ac2poly ( data ) : a , e , _c = LEVINSON ( data ) a = numpy . insert ( a , 0 , 1 ) return a , e
4715	def trun_emph ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:CONF {" ) for cvar in sorted ( trun [ "conf" ] . keys ( ) ) : cij . emph ( " % 16s: %r" % ( cvar , trun [ "conf" ] [ cvar ] ) ) cij . emph ( "}" ) if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:INFO {" ) cij . emph ( " OUTPUT: %r" % trun [ "conf" ] [ "OUTPUT" ] ) cij . emph ( " yml_fpath: %r" % yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) ) cij . emph ( "}" )
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
9429	def extract ( self , member , path = None , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename if path is None : path = os . getcwd ( ) self . _extract_members ( [ member ] , path , pwd ) return os . path . join ( path , member )
5751	def already_downloaded ( filename ) : cur_file = os . path . join ( c . bview_dir , filename ) old_file = os . path . join ( c . bview_dir , 'old' , filename ) if not os . path . exists ( cur_file ) and not os . path . exists ( old_file ) : return False return True
2633	def scale_in ( self , blocks ) : status = dict ( zip ( self . engines , self . provider . status ( self . engines ) ) ) to_kill = [ engine for engine in status if status [ engine ] == "RUNNING" ] [ : blocks ] if self . provider : r = self . provider . cancel ( to_kill ) else : logger . error ( "No execution provider available" ) r = None return r
1592	def add ( self , stream_id , task_ids , grouping , source_comp_name ) : if stream_id not in self . targets : self . targets [ stream_id ] = [ ] self . targets [ stream_id ] . append ( Target ( task_ids , grouping , source_comp_name ) )
4066	def item_fields ( self ) : if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
3639	def squad ( self , squad_id = 0 , persona_id = None ) : method = 'GET' url = 'squad/%s/user/%s' % ( squad_id , persona_id or self . persona_id ) events = [ self . pin . event ( 'page_view' , 'Hub - Squads' ) ] self . pin . send ( events ) rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Squad Details' ) , self . pin . event ( 'page_view' , 'Squads - Squad Overview' ) ] self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'players' , ( ) ) ]
5018	def add_missing_price_information_message ( request , item ) : messages . warning ( request , _ ( '{strong_start}We could not gather price information for {em_start}{item}{em_end}.{strong_end} ' '{span_start}If you continue to have these issues, please contact ' '{link_start}{platform_name} support{link_end}.{span_end}' ) . format ( item = item , em_start = '<em>' , em_end = '</em>' , link_start = '<a href="{support_link}" target="_blank">' . format ( support_link = get_configuration_value ( 'ENTERPRISE_SUPPORT_URL' , settings . ENTERPRISE_SUPPORT_URL ) , ) , platform_name = get_configuration_value ( 'PLATFORM_NAME' , settings . PLATFORM_NAME ) , link_end = '</a>' , span_start = '<span>' , span_end = '</span>' , strong_start = '<strong>' , strong_end = '</strong>' , ) )
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
6327	def get_count ( self , ngram , corpus = None ) : r if not corpus : corpus = self . ngcorpus if not ngram : return corpus [ None ] if isinstance ( ngram , ( text_type , str ) ) : ngram = text_type ( ngram ) . split ( ) if ngram [ 0 ] in corpus : return self . get_count ( ngram [ 1 : ] , corpus [ ngram [ 0 ] ] ) return 0
4565	def pop_legacy_palette ( kwds , * color_defaults ) : palette = kwds . pop ( 'palette' , None ) if palette : legacy = [ k for k , _ in color_defaults if k in kwds ] if legacy : raise ValueError ( 'Cannot set palette and ' + ', ' . join ( legacy ) ) return palette values = [ kwds . pop ( k , v ) for k , v in color_defaults ] if values and color_defaults [ 0 ] [ 0 ] in ( 'colors' , 'palette' ) : values = values [ 0 ] return make . colors ( values or None )
11504	def delete_folder ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.delete' , parameters ) return response
5448	def make_param ( self , name , raw_uri , disk_size ) : if raw_uri . startswith ( 'https://www.googleapis.com/compute' ) : docker_path = self . _parse_image_uri ( raw_uri ) return job_model . PersistentDiskMountParam ( name , raw_uri , docker_path , disk_size , disk_type = None ) elif raw_uri . startswith ( 'file://' ) : local_path , docker_path = self . _parse_local_mount_uri ( raw_uri ) return job_model . LocalMountParam ( name , raw_uri , docker_path , local_path ) elif raw_uri . startswith ( 'gs://' ) : docker_path = self . _parse_gcs_uri ( raw_uri ) return job_model . GCSMountParam ( name , raw_uri , docker_path ) else : raise ValueError ( 'Mount parameter {} must begin with valid prefix.' . format ( raw_uri ) )
12770	def step ( self , substeps = 2 ) : self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
11638	def write_data ( data , filename ) : name , ext = get_file_extension ( filename ) func = json_write_data if ext == '.json' else yaml_write_data return func ( data , filename )
7632	def list_namespaces ( ) : print ( '{:30s}\t{:40s}' . format ( 'NAME' , 'DESCRIPTION' ) ) print ( '-' * 78 ) for sch in sorted ( __NAMESPACE__ ) : desc = __NAMESPACE__ [ sch ] [ 'description' ] desc = ( desc [ : 44 ] + '..' ) if len ( desc ) > 46 else desc print ( '{:30s}\t{:40s}' . format ( sch , desc ) )
2086	def get ( self , pk ) : try : return next ( s for s in self . list ( ) [ 'results' ] if s [ 'id' ] == pk ) except StopIteration : raise exc . NotFound ( 'The requested object could not be found.' )
11519	def perform_upload ( self , upload_token , filename , ** kwargs ) : parameters = dict ( ) parameters [ 'uploadtoken' ] = upload_token parameters [ 'filename' ] = filename try : create_additional_revision = kwargs [ 'create_additional_revision' ] except KeyError : create_additional_revision = False if not create_additional_revision : parameters [ 'revision' ] = 'head' optional_keys = [ 'mode' , 'folderid' , 'item_id' , 'itemid' , 'revision' ] for key in optional_keys : if key in kwargs : if key == 'item_id' : parameters [ 'itemid' ] = kwargs [ key ] continue if key == 'folder_id' : parameters [ 'folderid' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] file_payload = open ( kwargs . get ( 'filepath' , filename ) , 'rb' ) parameters [ 'length' ] = os . fstat ( file_payload . fileno ( ) ) . st_size response = self . request ( 'midas.upload.perform' , parameters , file_payload ) return response
5801	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : all_purposes = '2.5.29.37.0' ca_path = system_path ( ) output = [ ] with open ( ca_path , 'rb' ) as f : for armor_type , _ , cert_bytes in unarmor ( f . read ( ) , multiple = True ) : if armor_type == 'CERTIFICATE' : if cert_callback : cert_callback ( Certificate . load ( cert_bytes ) , None ) output . append ( ( cert_bytes , set ( ) , set ( ) ) ) elif armor_type == 'TRUSTED CERTIFICATE' : cert , aux = TrustedCertificate . load ( cert_bytes ) reject_all = False trust_oids = set ( ) reject_oids = set ( ) for purpose in aux [ 'trust' ] : if purpose . dotted == all_purposes : trust_oids = set ( [ purpose . dotted ] ) break trust_oids . add ( purpose . dotted ) for purpose in aux [ 'reject' ] : if purpose . dotted == all_purposes : reject_all = True break reject_oids . add ( purpose . dotted ) if reject_all : if cert_callback : cert_callback ( cert , 'explicitly distrusted' ) continue if cert_callback and not callback_only_on_failure : cert_callback ( cert , None ) output . append ( ( cert . dump ( ) , trust_oids , reject_oids ) ) return output
2179	def fetch_request_token ( self , url , realm = None , ** request_kwargs ) : r self . _client . client . realm = " " . join ( realm ) if realm else None token = self . _fetch_token ( url , ** request_kwargs ) log . debug ( "Resetting callback_uri and realm (not needed in next phase)." ) self . _client . client . callback_uri = None self . _client . client . realm = None return token
11234	def translate_array ( self , string , language , level = 3 , retdata = False ) : language = language . lower ( ) assert self . is_built_in ( language ) or language in self . outer_templates , "Sorry, " + language + " is not a supported language." data = phpserialize . loads ( bytes ( string , 'utf-8' ) , array_hook = list , decode_strings = True ) if self . is_built_in ( language ) : self . get_built_in ( language , level , data ) print ( self ) return self . data_structure if retdata else None def loop_print ( iterable , level = 3 ) : retval = '' indentation = ' ' * level if not self . is_iterable ( iterable ) or isinstance ( iterable , str ) : non_iterable = str ( iterable ) return str ( non_iterable ) for item in iterable : if isinstance ( item , tuple ) and len ( item ) == 2 : key = item [ 0 ] val = loop_print ( item [ 1 ] , level = level + 3 ) val = self . translate_val ( language , val ) if language in self . lang_specific_values and val in self . lang_specific_values [ language ] else val key = str ( key ) if isinstance ( key , int ) else '\'' + str ( key ) + '\'' needs_unpacking = hasattr ( item [ 0 ] , '__iter__' ) == False and hasattr ( item [ 1 ] , '__iter__' ) == True if needs_unpacking : retval += self . get_inner_template ( language , 'iterable' , indentation , key , val ) else : val = str ( val ) if val . isdigit ( ) or val in self . lang_specific_values [ language ] . values ( ) else '\'' + str ( val ) + '\'' retval += self . get_inner_template ( language , 'singular' , indentation , key , val ) return retval self . data_structure = self . outer_templates [ language ] % ( loop_print ( data ) ) print ( self ) return self . data_structure if retdata else None
2342	def run ( self , x , y , lr = 0.01 , train_epochs = 1000 , test_epochs = 1000 , idx = 0 , verbose = None , ** kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) running_loss = 0 teloss = 0 for i in range ( train_epochs + test_epochs ) : optim . zero_grad ( ) pred = self . forward ( x ) loss = self . criterion ( pred , y ) running_loss += loss . item ( ) if i < train_epochs : loss . backward ( ) optim . step ( ) else : teloss += running_loss if verbose and not i % 300 : print ( 'Idx:{}; epoch:{}; score:{}' . format ( idx , i , running_loss / 300 ) ) running_loss = 0.0 return teloss / test_epochs
3049	def _get_implicit_credentials ( cls ) : environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
9516	def subseq ( self , start , end ) : return Fastq ( self . id , self . seq [ start : end ] , self . qual [ start : end ] )
8815	def update_interfaces ( self , added_sg , updated_sg , removed_sg ) : if not ( added_sg or updated_sg or removed_sg ) : return with self . sessioned ( ) as session : self . _set_security_groups ( session , added_sg ) self . _unset_security_groups ( session , removed_sg ) combined = added_sg + updated_sg + removed_sg self . _refresh_interfaces ( session , combined )
12758	def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
3108	def locked_get ( self ) : query = { self . key_name : self . key_value } entities = self . model_class . objects . filter ( ** query ) if len ( entities ) > 0 : credential = getattr ( entities [ 0 ] , self . property_name ) if getattr ( credential , 'set_store' , None ) is not None : credential . set_store ( self ) return credential else : return None
5556	def _filter_by_zoom ( element = None , conf_string = None , zoom = None ) : for op_str , op_func in [ ( "=" , operator . eq ) , ( "<=" , operator . le ) , ( ">=" , operator . ge ) , ( "<" , operator . lt ) , ( ">" , operator . gt ) , ] : if conf_string . startswith ( op_str ) : return element if op_func ( zoom , _strip_zoom ( conf_string , op_str ) ) else None
13857	def sendmsg ( self , message , recipient_mobiles = [ ] , url = 'http://services.ambientmobile.co.za/sms' , concatenate_message = True , message_id = str ( time ( ) ) . replace ( "." , "" ) , reply_path = None , allow_duplicates = True , allow_invalid_numbers = True , ) : if not recipient_mobiles or not ( isinstance ( recipient_mobiles , list ) or isinstance ( recipient_mobiles , tuple ) ) : raise AmbientSMSError ( "Missing recipients" ) if not message or not len ( message ) : raise AmbientSMSError ( "Missing message" ) postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXMLList . append ( "<recipients>%s</recipients>" % "" . join ( [ "<mobile>%s</mobile>" % m for m in recipient_mobiles ] ) ) postXMLList . append ( "<msg>%s</msg>" % message ) postXMLList . append ( "<concat>%s</concat>" % ( 1 if concatenate_message else 0 ) ) postXMLList . append ( "<message_id>%s</message_id>" % message_id ) postXMLList . append ( "<allow_duplicates>%s</allow_duplicates>" % ( 1 if allow_duplicates else 0 ) ) postXMLList . append ( "<allow_invalid_numbers>%s</allow_invalid_numbers>" % ( 1 if allow_invalid_numbers else 0 ) ) if reply_path : postXMLList . append ( "<reply_path>%s</reply_path>" % reply_path ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) status = result . get ( "status" , None ) if status and int ( status ) in [ 0 , 1 , 2 ] : return result else : raise AmbientSMSError ( int ( status ) )
645	def generateCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : coincMatrix0 = SM32 ( int ( nCoinc ) , int ( length ) ) theOnes = numpy . array ( [ 1.0 ] * activity , dtype = numpy . float32 ) for rowIdx in xrange ( nCoinc ) : coinc = numpy . array ( random . sample ( xrange ( length ) , activity ) , dtype = numpy . uint32 ) coinc . sort ( ) coincMatrix0 . setRowFromSparse ( rowIdx , coinc , theOnes ) coincMatrix = SM32 ( int ( nCoinc ) , int ( length ) ) coincMatrix . initializeWithFixedNNZR ( activity ) return coincMatrix0
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
9453	def play ( self , call_params ) : path = '/' + self . api_version + '/Play/' method = 'POST' return self . request ( path , method , call_params )
4555	def genVector ( width , height , x_mult = 1 , y_mult = 1 ) : center_x = ( width - 1 ) / 2 center_y = ( height - 1 ) / 2 def length ( x , y ) : dx = math . pow ( x - center_x , 2 * x_mult ) dy = math . pow ( y - center_y , 2 * y_mult ) return int ( math . sqrt ( dx + dy ) ) return [ [ length ( x , y ) for x in range ( width ) ] for y in range ( height ) ]
11998	def _encode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : return data + self . _hmac_generate ( data , algorithm , key ) elif algorithm [ 'type' ] == 'aes' : return self . _aes_encrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . dumps ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_compress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
1221	def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor
3815	async def _on_receive_array ( self , array ) : if array [ 0 ] == 'noop' : pass else : wrapper = json . loads ( array [ 0 ] [ 'p' ] ) if '3' in wrapper : self . _client_id = wrapper [ '3' ] [ '2' ] logger . info ( 'Received new client_id: %r' , self . _client_id ) await self . _add_channel_services ( ) if '2' in wrapper : pblite_message = json . loads ( wrapper [ '2' ] [ '2' ] ) if pblite_message [ 0 ] == 'cbu' : batch_update = hangouts_pb2 . BatchUpdate ( ) pblite . decode ( batch_update , pblite_message , ignore_first_item = True ) for state_update in batch_update . state_update : logger . debug ( 'Received StateUpdate:\n%s' , state_update ) header = state_update . state_update_header self . _active_client_state = header . active_client_state await self . on_state_update . fire ( state_update ) else : logger . info ( 'Ignoring message: %r' , pblite_message [ 0 ] )
11347	def html_to_text ( cls , html ) : s = cls ( ) s . feed ( html ) unescaped_data = s . unescape ( s . get_data ( ) ) return escape_for_xml ( unescaped_data , tags_to_keep = s . mathml_elements )
10739	def log_calls ( function ) : def wrapper ( self , * args , ** kwargs ) : self . log . log ( group = function . __name__ , message = 'Enter' ) function ( self , * args , ** kwargs ) self . log . log ( group = function . __name__ , message = 'Exit' ) return wrapper
5317	def use_style ( self , style_name ) : try : style = getattr ( styles , style_name . upper ( ) ) except AttributeError : raise ColorfulError ( 'the style "{0}" is undefined' . format ( style_name ) ) else : self . colorpalette = style
8299	def decodeOSC ( data ) : table = { "i" : readInt , "f" : readFloat , "s" : readString , "b" : readBlob } decoded = [ ] address , rest = readString ( data ) typetags = "" if address == "#bundle" : time , rest = readLong ( rest ) while len ( rest ) > 0 : length , rest = readInt ( rest ) decoded . append ( decodeOSC ( rest [ : length ] ) ) rest = rest [ length : ] elif len ( rest ) > 0 : typetags , rest = readString ( rest ) decoded . append ( address ) decoded . append ( typetags ) if typetags [ 0 ] == "," : for tag in typetags [ 1 : ] : value , rest = table [ tag ] ( rest ) decoded . append ( value ) else : print "Oops, typetag lacks the magic ," return decoded
8065	def drawdaisy ( x , y , color = '#fefefe' ) : _ctx . push ( ) _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) _ctx . pop ( )
4787	def matches ( self , pattern ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if not isinstance ( pattern , str_types ) : raise TypeError ( 'given pattern arg must be a string' ) if len ( pattern ) == 0 : raise ValueError ( 'given pattern arg must not be empty' ) if re . search ( pattern , self . val ) is None : self . _err ( 'Expected <%s> to match pattern <%s>, but did not.' % ( self . val , pattern ) ) return self
9697	def check_nonce ( self , request , oauth_request ) : oauth_nonce = oauth_request [ 'oauth_nonce' ] oauth_timestamp = oauth_request [ 'oauth_timestamp' ] return check_nonce ( request , oauth_request , oauth_nonce , oauth_timestamp )
12442	def require_accessibility ( self , user , method ) : if method == 'OPTIONS' : return authz = self . meta . authorization if not authz . is_accessible ( user , method , self ) : authz . unaccessible ( )
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
9512	def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
7648	def deprecated ( version , version_removed ) : def __wrapper ( func , * args , ** kwargs ) : code = six . get_function_code ( func ) warnings . warn_explicit ( "{:s}.{:s}\n\tDeprecated as of JAMS version {:s}." "\n\tIt will be removed in JAMS version {:s}." . format ( func . __module__ , func . __name__ , version , version_removed ) , category = DeprecationWarning , filename = code . co_filename , lineno = code . co_firstlineno + 1 ) return func ( * args , ** kwargs ) return decorator ( __wrapper )
13325	def activate ( paths , skip_local , skip_shared ) : if not paths : ctx = click . get_current_context ( ) if cpenv . get_active_env ( ) : ctx . invoke ( info ) return click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples: \n' ' cpenv activate my_env\n' ' cpenv activate ./relative/path/to/my_env\n' ' cpenv activate my_env my_module\n' ) click . echo ( examples ) return if skip_local : cpenv . module_resolvers . remove ( cpenv . resolver . module_resolver ) cpenv . module_resolvers . remove ( cpenv . resolver . active_env_module_resolver ) if skip_shared : cpenv . module_resolvers . remove ( cpenv . resolver . modules_path_resolver ) try : r = cpenv . resolve ( * paths ) except cpenv . ResolveError as e : click . echo ( '\n' + str ( e ) ) return resolved = set ( r . resolved ) active_modules = set ( ) env = cpenv . get_active_env ( ) if env : active_modules . add ( env ) active_modules . update ( cpenv . get_active_modules ( ) ) new_modules = resolved - active_modules old_modules = active_modules & resolved if old_modules and not new_modules : click . echo ( '\nModules already active: ' + bold ( ' ' . join ( [ obj . name for obj in old_modules ] ) ) ) return if env and contains_env ( new_modules ) : click . echo ( '\nUse bold(exit) to leave your active environment first.' ) return click . echo ( '\nResolved the following modules...' ) click . echo ( format_objects ( r . resolved ) ) r . activate ( ) click . echo ( blue ( '\nLaunching subshell...' ) ) modules = sorted ( resolved | active_modules , key = _type_and_name ) prompt = ':' . join ( [ obj . name for obj in modules ] ) shell . launch ( prompt )
13883	def GetFileLines ( filename , newline = None , encoding = None ) : return GetFileContents ( filename , binary = False , encoding = encoding , newline = newline , ) . split ( '\n' )
10497	def doubleClickMouse ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 2 ) self . _postQueuedEvents ( )
6001	def pix_to_sub ( self ) : pix_to_sub = [ [ ] for _ in range ( self . pixels ) ] for regular_pixel , pix_pixel in enumerate ( self . sub_to_pix ) : pix_to_sub [ pix_pixel ] . append ( regular_pixel ) return pix_to_sub
3474	def check_mass_balance ( self ) : reaction_element_dict = defaultdict ( int ) for metabolite , coefficient in iteritems ( self . _metabolites ) : if metabolite . charge is not None : reaction_element_dict [ "charge" ] += coefficient * metabolite . charge if metabolite . elements is None : raise ValueError ( "No elements found in metabolite %s" % metabolite . id ) for element , amount in iteritems ( metabolite . elements ) : reaction_element_dict [ element ] += coefficient * amount return { k : v for k , v in iteritems ( reaction_element_dict ) if v != 0 }
1263	def states ( self ) : screen = self . env . getScreenRGB ( ) return dict ( shape = screen . shape , type = 'int' )
3073	def _load_config ( self , client_secrets_file , client_id , client_secret ) : if client_id and client_secret : self . client_id , self . client_secret = client_id , client_secret return if client_secrets_file : self . _load_client_secrets ( client_secrets_file ) return if 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' in self . app . config : self . _load_client_secrets ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE' ] ) return try : self . client_id , self . client_secret = ( self . app . config [ 'GOOGLE_OAUTH2_CLIENT_ID' ] , self . app . config [ 'GOOGLE_OAUTH2_CLIENT_SECRET' ] ) except KeyError : raise ValueError ( 'OAuth2 configuration could not be found. Either specify the ' 'client_secrets_file or client_id and client_secret or set ' 'the app configuration variables ' 'GOOGLE_OAUTH2_CLIENT_SECRETS_FILE or ' 'GOOGLE_OAUTH2_CLIENT_ID and GOOGLE_OAUTH2_CLIENT_SECRET.' )
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
2235	def _tee_output ( make_proc , stdout = None , stderr = None , backend = 'auto' ) : logged_out = [ ] logged_err = [ ] if backend == 'auto' : backend = 'thread' if backend == 'select' : if not POSIX : raise NotImplementedError ( 'select is only available on posix' ) _proc_iteroutput = _proc_iteroutput_select elif backend == 'thread' : _proc_iteroutput = _proc_iteroutput_thread else : raise ValueError ( 'backend must be select, thread, or auto' ) proc = make_proc ( ) for oline , eline in _proc_iteroutput ( proc ) : if oline : if stdout : stdout . write ( oline ) stdout . flush ( ) logged_out . append ( oline ) if eline : if stderr : stderr . write ( eline ) stderr . flush ( ) logged_err . append ( eline ) return proc , logged_out , logged_err
2965	def _sm_start ( self , * args , ** kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
12701	def get_subfields ( self , datafield , subfield , i1 = None , i2 = None , exception = False ) : if len ( datafield ) != 3 : raise ValueError ( "`datafield` parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subfield have to be 1 char long!" ) if datafield not in self . datafields : if exception : raise KeyError ( datafield + " is not in datafields!" ) return [ ] output = [ ] for datafield in self . datafields [ datafield ] : if subfield not in datafield : continue for sfield in datafield [ subfield ] : if i1 and sfield . i1 != i1 : continue if i2 and sfield . i2 != i2 : continue output . append ( sfield ) if not output and exception : raise KeyError ( subfield + " couldn't be found in subfields!" ) return output
13558	def get_all_images_count ( self ) : self_imgs = self . image_set . count ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) . count ( ) count = self_imgs + u_images return count
9436	def strip_ip ( packet ) : if not isinstance ( packet , IP ) : packet = IP ( packet ) payload = packet . payload return payload
12123	def validate_activatable_models ( ) : for model in get_activatable_models ( ) : activatable_field = next ( ( f for f in model . _meta . fields if f . __class__ == models . BooleanField and f . name == model . ACTIVATABLE_FIELD_NAME ) , None ) if activatable_field is None : raise ValidationError ( ( 'Model {0} is an activatable model. It must define an activatable BooleanField that ' 'has a field name of model.ACTIVATABLE_FIELD_NAME (which defaults to is_active)' . format ( model ) ) ) if not model . ALLOW_CASCADE_DELETE : for field in model . _meta . fields : if field . __class__ in ( models . ForeignKey , models . OneToOneField ) : if field . remote_field . on_delete == models . CASCADE : raise ValidationError ( ( 'Model {0} is an activatable model. All ForeignKey and OneToOneFields ' 'must set on_delete methods to something other than CASCADE (the default). ' 'If you want to explicitely allow cascade deletes, then you must set the ' 'ALLOW_CASCADE_DELETE=True class variable on your model.' ) . format ( model ) )
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
1110	def _dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
614	def _getPredictedField ( options ) : if not options [ 'inferenceArgs' ] or not options [ 'inferenceArgs' ] [ 'predictedField' ] : return None , None predictedField = options [ 'inferenceArgs' ] [ 'predictedField' ] predictedFieldInfo = None includedFields = options [ 'includedFields' ] for info in includedFields : if info [ 'fieldName' ] == predictedField : predictedFieldInfo = info break if predictedFieldInfo is None : raise ValueError ( "Predicted field '%s' does not exist in included fields." % predictedField ) predictedFieldType = predictedFieldInfo [ 'fieldType' ] return predictedField , predictedFieldType
8771	def _add_default_tz_bindings ( self , context , switch , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_add_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_add_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . add ( context , switch , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
3853	def get_conv_name ( conv , truncate = False , show_unread = False ) : num_unread = len ( [ conv_event for conv_event in conv . unread_events if isinstance ( conv_event , hangups . ChatMessageEvent ) and not conv . get_user ( conv_event . user_id ) . is_self ] ) if show_unread and num_unread > 0 : postfix = ' ({})' . format ( num_unread ) else : postfix = '' if conv . name is not None : return conv . name + postfix else : participants = sorted ( ( user for user in conv . users if not user . is_self ) , key = lambda user : user . id_ ) names = [ user . first_name for user in participants ] if not participants : return "Empty Conversation" + postfix if len ( participants ) == 1 : return participants [ 0 ] . full_name + postfix elif truncate and len ( participants ) > 2 : return ( ', ' . join ( names [ : 2 ] + [ '+{}' . format ( len ( names ) - 2 ) ] ) + postfix ) else : return ', ' . join ( names ) + postfix
12000	def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
10859	def update ( self , params , values ) : global_update , particles = self . _update_type ( params ) if global_update : self . set_values ( params , values ) self . initialize ( ) return oldargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( oldargs [ n ] ) , sign = - 1 ) self . set_values ( params , values ) newargs = self . _drawargs ( ) for n in particles : self . _draw_particle ( self . pos [ n ] , * listify ( newargs [ n ] ) , sign = + 1 )
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
12325	def untokenize ( tokens ) : text = '' previous_line = '' last_row = 0 last_column = - 1 last_non_whitespace_token_type = None for ( token_type , token_string , start , end , line ) in tokens : if TOKENIZE_HAS_ENCODING and token_type == tokenize . ENCODING : continue ( start_row , start_column ) = start ( end_row , end_column ) = end if ( last_non_whitespace_token_type != tokenize . COMMENT and start_row > last_row and previous_line . endswith ( ( '\\\n' , '\\\r\n' , '\\\r' ) ) ) : text += previous_line [ len ( previous_line . rstrip ( ' \t\n\r\\' ) ) : ] if start_row > last_row : last_column = 0 if start_column > last_column : text += line [ last_column : start_column ] text += token_string previous_line = line last_row = end_row last_column = end_column if token_type not in WHITESPACE_TOKENS : last_non_whitespace_token_type = token_type return text
9843	def __tokenize ( self , string ) : for m in self . dx_regex . finditer ( string . strip ( ) ) : code = m . lastgroup text = m . group ( m . lastgroup ) tok = Token ( code , text ) if not tok . iscode ( 'WHITESPACE' ) : self . tokens . append ( tok )
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
162	def get_pointwise_inside_image_mask ( self , image ) : if len ( self . coords ) == 0 : return np . zeros ( ( 0 , ) , dtype = bool ) shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] x_within = np . logical_and ( 0 <= self . xx , self . xx < width ) y_within = np . logical_and ( 0 <= self . yy , self . yy < height ) return np . logical_and ( x_within , y_within )
5302	def parse_rgb_txt_file ( path ) : color_dict = { } with open ( path , 'r' ) as rgb_txt : for line in rgb_txt : line = line . strip ( ) if not line or line . startswith ( '!' ) : continue parts = line . split ( ) color_dict [ " " . join ( parts [ 3 : ] ) ] = ( int ( parts [ 0 ] ) , int ( parts [ 1 ] ) , int ( parts [ 2 ] ) ) return color_dict
10251	def remove_highlight_nodes ( graph : BELGraph , nodes : Optional [ Iterable [ BaseEntity ] ] = None ) -> None : for node in graph if nodes is None else nodes : if is_node_highlighted ( graph , node ) : del graph . node [ node ] [ NODE_HIGHLIGHT ]
5839	def submit_predict_request ( self , data_view_id , candidates , prediction_source = 'scalar' , use_prior = True ) : data = { "prediction_source" : prediction_source , "use_prior" : use_prior , "candidates" : candidates } failure_message = "Configuration creation failed" post_url = 'v1/data_views/' + str ( data_view_id ) + '/predict/submit' return self . _get_success_json ( self . _post_json ( post_url , data , failure_message = failure_message ) ) [ 'data' ] [ 'uid' ]
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
13901	def get_db_from_db ( db_string ) : server = get_server_from_db ( db_string ) local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) if local_match : return server [ local_match . groupdict ( ) [ 'database' ] ] elif remote_match : return server [ remote_match . groupdict ( ) [ 'database' ] ] raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
1253	def print_state ( self ) : def tile_string ( value ) : if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
13554	def create_shift ( self , params = { } ) : url = "/2/shifts/" body = params data = self . _post_resource ( url , body ) shift = self . shift_from_json ( data [ "shift" ] ) return shift
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
1597	def format_prefix ( filename , sres ) : try : pwent = pwd . getpwuid ( sres . st_uid ) user = pwent . pw_name except KeyError : user = sres . st_uid try : grent = grp . getgrgid ( sres . st_gid ) group = grent . gr_name except KeyError : group = sres . st_gid return '%s %3d %10s %10s %10d %s' % ( format_mode ( sres ) , sres . st_nlink , user , group , sres . st_size , format_mtime ( sres . st_mtime ) , )
11538	def pin_direction ( self , pin ) : if type ( pin ) is list : return [ self . pin_direction ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_direction ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
13111	def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . _item_dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . _item_dict . get ( key ) return item . value if item else default
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
7201	def is_ordered ( cat_id ) : url = 'https://rda.geobigdata.io/v1/stripMetadata/{}' . format ( cat_id ) auth = Auth ( ) r = _req_with_retries ( auth . gbdx_connection , url ) if r is not None : return r . status_code == 200 return False
3203	def delete ( self , store_id , cart_id , line_id ) : self . store_id = store_id self . cart_id = cart_id self . line_id = line_id return self . _mc_client . _delete ( url = self . _build_path ( store_id , 'carts' , cart_id , 'lines' , line_id ) )
5716	def validate ( self ) : warnings . warn ( 'Property "package.validate" is deprecated.' , UserWarning ) descriptor = self . to_dict ( ) self . profile . validate ( descriptor )
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
13156	def transaction ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . NAMEDTUPLE ) ) as c : try : yield from c . execute ( 'BEGIN' ) result = ( yield from func ( cls , c , * args , ** kwargs ) ) except Exception : yield from c . execute ( 'ROLLBACK' ) else : yield from c . execute ( 'COMMIT' ) return result return wrapper
5411	def build_machine ( network = None , machine_type = None , preemptible = None , service_account = None , boot_disk_size_gb = None , disks = None , accelerators = None , labels = None , cpu_platform = None , nvidia_driver_version = None ) : return { 'network' : network , 'machineType' : machine_type , 'preemptible' : preemptible , 'serviceAccount' : service_account , 'bootDiskSizeGb' : boot_disk_size_gb , 'disks' : disks , 'accelerators' : accelerators , 'labels' : labels , 'cpuPlatform' : cpu_platform , 'nvidiaDriverVersion' : nvidia_driver_version , }
8061	def do_windowed ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( False ) print ( self . response_prompt , file = self . stdout )
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
3230	def service_list ( service = None , key_name = None , ** kwargs ) : resp_list = [ ] req = service . list ( ** kwargs ) while req is not None : resp = req . execute ( ) if key_name and key_name in resp : resp_list . extend ( resp [ key_name ] ) else : resp_list . append ( resp ) if hasattr ( service , 'list_next' ) : req = service . list_next ( previous_request = req , previous_response = resp ) else : req = None return resp_list
9147	def web ( connection , host , port ) : from bio2bel . web . application import create_application app = create_application ( connection = connection ) app . run ( host = host , port = port )
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
277	def plotting_context ( context = 'notebook' , font_scale = 1.5 , rc = None ) : if rc is None : rc = { } rc_default = { 'lines.linewidth' : 1.5 } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . plotting_context ( context = context , font_scale = font_scale , rc = rc )
6317	def image_data ( image ) : data = image . tobytes ( ) components = len ( data ) // ( image . size [ 0 ] * image . size [ 1 ] ) return components , data
5032	def _build_admin_context ( request , customer ) : opts = customer . _meta codename = get_permission_codename ( 'change' , opts ) has_change_permission = request . user . has_perm ( '%s.%s' % ( opts . app_label , codename ) ) return { 'has_change_permission' : has_change_permission , 'opts' : opts }
1262	def import_demonstrations ( self , demonstrations ) : if isinstance ( demonstrations , dict ) : if self . unique_state : demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) if self . unique_action : demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) self . model . import_demo_experience ( ** demonstrations ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for demonstration in demonstrations : if self . unique_state : states [ 'state' ] . append ( demonstration [ 'states' ] ) else : for name , state in states . items ( ) : state . append ( demonstration [ 'states' ] [ name ] ) for name , internal in internals . items ( ) : internal . append ( demonstration [ 'internals' ] [ name ] ) if self . unique_action : actions [ 'action' ] . append ( demonstration [ 'actions' ] ) else : for name , action in actions . items ( ) : action . append ( demonstration [ 'actions' ] [ name ] ) terminal . append ( demonstration [ 'terminal' ] ) reward . append ( demonstration [ 'reward' ] ) self . model . import_demo_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
9506	def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
859	def isTemporal ( inferenceElement ) : if InferenceElement . __temporalInferenceElements is None : InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) return inferenceElement in InferenceElement . __temporalInferenceElements
7738	def check_unassigned ( self , data ) : for char in data : for lookup in self . unassigned : if lookup ( char ) : raise StringprepError ( "Unassigned character: {0!r}" . format ( char ) ) return data
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
3199	def delete ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _delete ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
10599	def create_template ( material , path , show = False ) : file_name = 'dataset-%s.csv' % material . lower ( ) file_path = os . path . join ( path , file_name ) with open ( file_path , 'w' , newline = '' ) as csvfile : writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE_MINIMAL ) writer . writerow ( [ 'Name' , material ] ) writer . writerow ( [ 'Description' , '<Add a data set description ' 'here.>' ] ) writer . writerow ( [ 'Reference' , '<Add a reference to the source of ' 'the data set here.>' ] ) writer . writerow ( [ 'Temperature' , '<parameter 1 name>' , '<parameter 2 name>' , '<parameter 3 name>' ] ) writer . writerow ( [ 'T' , '<parameter 1 display symbol>' , '<parameter 2 display symbol>' , '<parameter 3 display symbol>' ] ) writer . writerow ( [ 'K' , '<parameter 1 units>' , '<parameter 2 units>' , '<parameter 3 units>' ] ) writer . writerow ( [ 'T' , '<parameter 1 symbol>' , '<parameter 2 symbol>' , '<parameter 3 symbol>' ] ) for i in range ( 10 ) : writer . writerow ( [ 100.0 + i * 50 , float ( i ) , 10.0 + i , 100.0 + i ] ) if show is True : webbrowser . open_new ( file_path )
3605	def get ( self , url , name , params = None , headers = None , connection = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_get_request ( endpoint , params , headers , connection = connection )
4280	def watermark ( im , mark , position , opacity = 1 ) : if opacity < 1 : mark = reduce_opacity ( mark , opacity ) if im . mode != 'RGBA' : im = im . convert ( 'RGBA' ) layer = Image . new ( 'RGBA' , im . size , ( 0 , 0 , 0 , 0 ) ) if position == 'tile' : for y in range ( 0 , im . size [ 1 ] , mark . size [ 1 ] ) : for x in range ( 0 , im . size [ 0 ] , mark . size [ 0 ] ) : layer . paste ( mark , ( x , y ) ) elif position == 'scale' : ratio = min ( float ( im . size [ 0 ] ) / mark . size [ 0 ] , float ( im . size [ 1 ] ) / mark . size [ 1 ] ) w = int ( mark . size [ 0 ] * ratio ) h = int ( mark . size [ 1 ] * ratio ) mark = mark . resize ( ( w , h ) ) layer . paste ( mark , ( int ( ( im . size [ 0 ] - w ) / 2 ) , int ( ( im . size [ 1 ] - h ) / 2 ) ) ) else : layer . paste ( mark , position ) return Image . composite ( layer , im , layer )
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
4474	def __serial_transform ( self , jam , steps ) : if six . PY2 : attr = 'next' else : attr = '__next__' pending = len ( steps ) nexts = itertools . cycle ( getattr ( iter ( D . transform ( jam ) ) , attr ) for ( name , D ) in steps ) while pending : try : for next_jam in nexts : yield next_jam ( ) except StopIteration : pending -= 1 nexts = itertools . cycle ( itertools . islice ( nexts , pending ) )
5892	def get_urls ( self ) : urls = patterns ( '' , url ( r'^upload/$' , self . admin_site . admin_view ( self . handle_upload ) , name = 'quill-file-upload' ) , ) return urls + super ( QuillAdmin , self ) . get_urls ( )
2441	def add_annotation_date ( self , doc , annotation_date ) : if len ( doc . annotations ) != 0 : if not self . annotation_date_set : self . annotation_date_set = True date = utils . datetime_from_iso_format ( annotation_date ) if date is not None : doc . annotations [ - 1 ] . annotation_date = date return True else : raise SPDXValueError ( 'Annotation::AnnotationDate' ) else : raise CardinalityError ( 'Annotation::AnnotationDate' ) else : raise OrderError ( 'Annotation::AnnotationDate' )
1489	def save_file ( self , obj ) : try : import StringIO as pystringIO except ImportError : import io as pystringIO if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : raise pickle . PicklingError ( "Cannot pickle files that do not map to an actual file" ) if obj is sys . stdout : return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) if obj is sys . stderr : return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) if obj is sys . stdin : raise pickle . PicklingError ( "Cannot pickle standard input" ) if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : raise pickle . PicklingError ( "Cannot pickle files that map to tty objects" ) if 'r' not in obj . mode : raise pickle . PicklingError ( "Cannot pickle files that are not opened for reading" ) name = obj . name try : fsize = os . stat ( name ) . st_size except OSError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be stat" % name ) if obj . closed : retval = pystringIO . StringIO ( "" ) retval . close ( ) elif not fsize : retval = pystringIO . StringIO ( "" ) try : tmpfile = file ( name ) tst = tmpfile . read ( 1 ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) tmpfile . close ( ) if tst != '' : raise pickle . PicklingError ( "Cannot pickle file %s as it does not appear to map to a physical, real file" % name ) else : try : tmpfile = file ( name ) contents = tmpfile . read ( ) tmpfile . close ( ) except IOError : raise pickle . PicklingError ( "Cannot pickle file %s as it cannot be read" % name ) retval = pystringIO . StringIO ( contents ) curloc = obj . tell ( ) retval . seek ( curloc ) retval . name = name self . save ( retval ) self . memoize ( obj )
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
3247	def get_users ( group , ** conn ) : group_details = get_group_api ( group [ 'GroupName' ] , ** conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
5961	def set_correlparameters ( self , ** kwargs ) : self . ncorrel = kwargs . pop ( 'ncorrel' , self . ncorrel ) or 25000 nstep = kwargs . pop ( 'nstep' , None ) if nstep is None : nstep = len ( self . array [ 0 ] ) / float ( self . ncorrel ) nstep = int ( numpy . ceil ( nstep ) ) kwargs [ 'nstep' ] = nstep self . __correlkwargs . update ( kwargs ) return self . __correlkwargs
3619	def register ( self , model , index_cls = AlgoliaIndex , auto_indexing = None ) : if self . is_registered ( model ) : raise RegistrationError ( '{} is already registered with Algolia engine' . format ( model ) ) if not issubclass ( index_cls , AlgoliaIndex ) : raise RegistrationError ( '{} should be a subclass of AlgoliaIndex' . format ( index_cls ) ) index_obj = index_cls ( model , self . client , self . __settings ) self . __registered_models [ model ] = index_obj if ( isinstance ( auto_indexing , bool ) and auto_indexing ) or self . __auto_indexing : post_save . connect ( self . __post_save_receiver , model ) pre_delete . connect ( self . __pre_delete_receiver , model ) logger . info ( 'REGISTER %s' , model )
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
7544	def run ( data , samples , force , ipyclient ) : data . dirs . consens = os . path . join ( data . dirs . project , data . name + "_consens" ) if not os . path . exists ( data . dirs . consens ) : os . mkdir ( data . dirs . consens ) tmpcons = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcats = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpfile in tmpcons + tmpcats : os . remove ( tmpfile ) samples = get_subsamples ( data , samples , force ) lbview = ipyclient . load_balanced_view ( ) data . cpus = data . _ipcluster [ "cores" ] if not data . cpus : data . cpus = len ( ipyclient . ids ) inst = "" try : samples = calculate_depths ( data , samples , lbview ) lasyncs = make_chunks ( data , samples , lbview ) process_chunks ( data , samples , lasyncs , lbview ) except KeyboardInterrupt as inst : raise inst finally : tmpcons = glob . glob ( os . path . join ( data . dirs . clusts , "tmp_*.[0-9]*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpchunk in tmpcons : os . remove ( tmpchunk ) data . _checkpoint = 0
6014	def load_positions ( positions_path ) : with open ( positions_path ) as f : position_string = f . readlines ( ) positions = [ ] for line in position_string : position_list = ast . literal_eval ( line ) positions . append ( position_list ) return positions
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) config . include ( 'twitcher.config' ) config . include ( 'twitcher.basicauth' ) config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
7488	def make ( data , samples ) : invcffile = os . path . join ( data . dirs . consens , data . name + ".vcf" ) outlocifile = os . path . join ( data . dirs . outfiles , data . name + ".loci" ) importvcf ( invcffile , outlocifile )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : image = common . pad ( image , extrapad , 0 ) s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
11190	def show ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) click . secho ( readme_content )
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
10868	def j2 ( x ) : to_return = 2. / ( x + 1e-15 ) * j1 ( x ) - j0 ( x ) to_return [ x == 0 ] = 0 return to_return
9870	def build_environ ( self , sock_file , conn ) : request = self . read_request_line ( sock_file ) environ = self . base_environ . copy ( ) for k , v in self . read_headers ( sock_file ) . items ( ) : environ [ str ( 'HTTP_' + k ) ] = v environ [ 'REQUEST_METHOD' ] = request [ 'method' ] environ [ 'PATH_INFO' ] = request [ 'path' ] environ [ 'SERVER_PROTOCOL' ] = request [ 'protocol' ] environ [ 'SERVER_PORT' ] = str ( conn . server_port ) environ [ 'REMOTE_PORT' ] = str ( conn . client_port ) environ [ 'REMOTE_ADDR' ] = str ( conn . client_addr ) environ [ 'QUERY_STRING' ] = request [ 'query_string' ] if 'HTTP_CONTENT_LENGTH' in environ : environ [ 'CONTENT_LENGTH' ] = environ [ 'HTTP_CONTENT_LENGTH' ] if 'HTTP_CONTENT_TYPE' in environ : environ [ 'CONTENT_TYPE' ] = environ [ 'HTTP_CONTENT_TYPE' ] self . request_method = environ [ 'REQUEST_METHOD' ] if conn . ssl : environ [ 'wsgi.url_scheme' ] = 'https' environ [ 'HTTPS' ] = 'on' else : environ [ 'wsgi.url_scheme' ] = 'http' if environ . get ( 'HTTP_TRANSFER_ENCODING' , '' ) == 'chunked' : environ [ 'wsgi.input' ] = ChunkedReader ( sock_file ) else : environ [ 'wsgi.input' ] = sock_file return environ
4317	def info ( filepath ) : info_dictionary = { 'channels' : channels ( filepath ) , 'sample_rate' : sample_rate ( filepath ) , 'bitrate' : bitrate ( filepath ) , 'duration' : duration ( filepath ) , 'num_samples' : num_samples ( filepath ) , 'encoding' : encoding ( filepath ) , 'silent' : silent ( filepath ) } return info_dictionary
7735	def set_stringprep_cache_size ( size ) : global _stringprep_cache_size _stringprep_cache_size = size if len ( Profile . cache_items ) > size : remove = Profile . cache_items [ : - size ] for profile , key in remove : try : del profile . cache [ key ] except KeyError : pass Profile . cache_items = Profile . cache_items [ - size : ]
2797	def transfer ( self , new_region_slug ) : return self . get_data ( "images/%s/actions/" % self . id , type = POST , params = { "type" : "transfer" , "region" : new_region_slug } )
7526	def get_quick_depths ( data , sample ) : sample . files . clusters = os . path . join ( data . dirs . clusts , sample . name + ".clustS.gz" ) fclust = data . samples [ sample . name ] . files . clusters clusters = gzip . open ( fclust , 'r' ) pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) depths = [ ] maxlen = [ ] tdepth = 0 tlen = 0 while 1 : try : name , seq = pairdealer . next ( ) except StopIteration : break if name . strip ( ) == seq . strip ( ) : depths . append ( tdepth ) maxlen . append ( tlen ) tlen = 0 tdepth = 0 else : tdepth += int ( name . split ( ";" ) [ - 2 ] [ 5 : ] ) tlen = len ( seq ) clusters . close ( ) return np . array ( maxlen ) , np . array ( depths )
3615	def get_raw_record ( self , instance , update_fields = None ) : tmp = { 'objectID' : self . objectID ( instance ) } if update_fields : if isinstance ( update_fields , str ) : update_fields = ( update_fields , ) for elt in update_fields : key = self . __translate_fields . get ( elt , None ) if key : tmp [ key ] = self . __named_fields [ key ] ( instance ) else : for key , value in self . __named_fields . items ( ) : tmp [ key ] = value ( instance ) if self . geo_field : loc = self . geo_field ( instance ) if isinstance ( loc , tuple ) : tmp [ '_geoloc' ] = { 'lat' : loc [ 0 ] , 'lng' : loc [ 1 ] } elif isinstance ( loc , dict ) : self . _validate_geolocation ( loc ) tmp [ '_geoloc' ] = loc elif isinstance ( loc , list ) : [ self . _validate_geolocation ( geo ) for geo in loc ] tmp [ '_geoloc' ] = loc if self . tags : if callable ( self . tags ) : tmp [ '_tags' ] = self . tags ( instance ) if not isinstance ( tmp [ '_tags' ] , list ) : tmp [ '_tags' ] = list ( tmp [ '_tags' ] ) logger . debug ( 'BUILD %s FROM %s' , tmp [ 'objectID' ] , self . model ) return tmp
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
3662	def calculate_integral ( self , T1 , T2 , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral ( T1 , T2 ) elif method == POLING_CONST : return ( T2 - T1 ) * self . POLING_constant elif method == CRCSTD : return ( T2 - T1 ) * self . CRCSTD_constant elif method == DADGOSTAR_SHAW : dH = ( Dadgostar_Shaw_integral ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dH , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
510	def _updateMinDutyCycles ( self ) : if self . _globalInhibition or self . _inhibitionRadius > self . _numInputs : self . _updateMinDutyCyclesGlobal ( ) else : self . _updateMinDutyCyclesLocal ( )
13446	def messages_from_response ( response ) : messages = [ ] if hasattr ( response , 'context' ) and response . context and 'messages' in response . context : messages = response . context [ 'messages' ] elif hasattr ( response , 'cookies' ) : morsel = response . cookies . get ( 'messages' ) if not morsel : return [ ] from django . contrib . messages . storage . cookie import CookieStorage store = CookieStorage ( FakeRequest ( ) ) messages = store . _decode ( morsel . value ) else : return [ ] return [ ( m . message , m . level ) for m in messages ]
6321	def get_template_dir ( self ) : directory = os . path . dirname ( os . path . abspath ( __file__ ) ) directory = os . path . dirname ( os . path . dirname ( directory ) ) directory = os . path . join ( directory , 'project_template' ) return directory
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
5614	def segmentize_geometry ( geometry , segmentize_value ) : if geometry . geom_type != "Polygon" : raise TypeError ( "segmentize geometry type must be Polygon" ) return Polygon ( LinearRing ( [ p for l in map ( lambda x : LineString ( [ x [ 0 ] , x [ 1 ] ] ) , zip ( geometry . exterior . coords [ : - 1 ] , geometry . exterior . coords [ 1 : ] ) ) for p in [ l . interpolate ( segmentize_value * i ) . coords [ 0 ] for i in range ( int ( l . length / segmentize_value ) ) ] + [ l . coords [ 1 ] ] ] ) )
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
280	def plot_annual_returns ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , ** kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Annual returns" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( ** row ) ) return settings
4728	def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
13450	def field_value ( self , admin_model , instance , field_name ) : _ , _ , value = lookup_field ( field_name , instance , admin_model ) return value
706	def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : if not self . _createCheckpoints : modelCheckpointGUID = None self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( "Running Model. \nmodelParams: %s, \nmodelID=%s, " % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) cpuTimeStart = time . clock ( ) logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( "%s" , e )
11839	def set_board ( self , board = None ) : "Set the board, and find all the words in it." if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
8551	def delete_image ( self , image_id ) : response = self . _perform_request ( url = '/images/' + image_id , method = 'DELETE' ) return response
220	async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return PlainTextResponse ( "Method Not Allowed" , status_code = 405 ) if path . startswith ( ".." ) : return PlainTextResponse ( "Not Found" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : index_path = os . path . join ( path , "index.html" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ "path" ] . endswith ( "/" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : full_path , stat_result = await self . lookup_path ( "404.html" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( "Not Found" , status_code = 404 )
4469	def _pprint ( params , offset = 0 , printer = repr ) : options = np . get_printoptions ( ) np . set_printoptions ( precision = 5 , threshold = 64 , edgeitems = 2 ) params_list = list ( ) this_line_length = offset line_sep = ',\n' + ( 1 + offset // 2 ) * ' ' for i , ( k , v ) in enumerate ( sorted ( six . iteritems ( params ) ) ) : if type ( v ) is float : this_repr = '%s=%s' % ( k , str ( v ) ) else : this_repr = '%s=%s' % ( k , printer ( v ) ) if len ( this_repr ) > 500 : this_repr = this_repr [ : 300 ] + '...' + this_repr [ - 100 : ] if i > 0 : if ( this_line_length + len ( this_repr ) >= 75 or '\n' in this_repr ) : params_list . append ( line_sep ) this_line_length = len ( line_sep ) else : params_list . append ( ', ' ) this_line_length += 2 params_list . append ( this_repr ) this_line_length += len ( this_repr ) np . set_printoptions ( ** options ) lines = '' . join ( params_list ) lines = '\n' . join ( l . rstrip ( ' ' ) for l in lines . split ( '\n' ) ) return lines
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
8940	def _to_pypi ( self , docs_base , release ) : url = None with self . _zipped ( docs_base ) as handle : reply = requests . post ( self . params [ 'url' ] , auth = get_pypi_auth ( ) , allow_redirects = False , files = dict ( content = ( self . cfg . project . name + '.zip' , handle , 'application/zip' ) ) , data = { ':action' : 'doc_upload' , 'name' : self . cfg . project . name } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for POST to {url}" . format ( ** data ) ) return url
10221	def preprocessing_br_projection_excel ( path : str ) -> pd . DataFrame : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) return pd . read_excel ( path , sheetname = 0 , header = 0 )
13237	def next_interval ( self , after = None ) : if after is None : after = timezone . now ( ) after = self . to_timezone ( after ) return next ( self . intervals ( range_start = after ) , None )
9151	def _convert_coordinatelist ( input_obj ) : cdl = pgmagick . CoordinateList ( ) for obj in input_obj : cdl . append ( pgmagick . Coordinate ( obj [ 0 ] , obj [ 1 ] ) ) return cdl
9234	def run ( self ) : if not self . options . project or not self . options . user : print ( "Project and/or user missing. " "For help run:\n pygcgen --help" ) return if not self . options . quiet : print ( "Generating changelog..." ) log = None try : log = self . generator . compound_changelog ( ) except ChangelogGeneratorError as err : print ( "\n\033[91m\033[1m{}\x1b[0m" . format ( err . args [ 0 ] ) ) exit ( 1 ) if not log : if not self . options . quiet : print ( "Empty changelog generated. {} not written." . format ( self . options . output ) ) return if self . options . no_overwrite : out = checkname ( self . options . output ) else : out = self . options . output with codecs . open ( out , "w" , "utf-8" ) as fh : fh . write ( log ) if not self . options . quiet : print ( "Done!" ) print ( "Generated changelog written to {}" . format ( out ) )
2628	def teardown ( self ) : self . shut_down_instance ( self . instances ) self . instances = [ ] try : self . client . delete_internet_gateway ( InternetGatewayId = self . internet_gateway ) self . internet_gateway = None self . client . delete_route_table ( RouteTableId = self . route_table ) self . route_table = None for subnet in list ( self . sn_ids ) : self . client . delete_subnet ( SubnetId = subnet ) self . sn_ids . remove ( subnet ) self . client . delete_security_group ( GroupId = self . sg_id ) self . sg_id = None self . client . delete_vpc ( VpcId = self . vpc_id ) self . vpc_id = None except Exception as e : logger . error ( "{}" . format ( e ) ) raise e self . show_summary ( ) os . remove ( self . config [ 'state_file_path' ] )
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
566	def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) encoder = MultiEncoder ( ) encoder . addEncoder ( "consumption" , consumption_encoder ) encoder . addEncoder ( "timestamp" , time_encoder ) return encoder
12548	def icc_img_to_zscore ( icc , center_image = False ) : vol = read_img ( icc ) . get_data ( ) v2 = vol [ vol != 0 ] if center_image : v2 = detrend ( v2 , axis = 0 ) vstd = np . linalg . norm ( v2 , ord = 2 ) / np . sqrt ( np . prod ( v2 . shape ) - 1 ) eps = np . finfo ( vstd . dtype ) . eps vol /= ( eps + vstd ) return vol
5993	def get_normalization_min_max ( array , norm_min , norm_max ) : if norm_min is None : norm_min = array . min ( ) if norm_max is None : norm_max = array . max ( ) return norm_min , norm_max
11204	def valuestodict ( key ) : dout = { } size = winreg . QueryInfoKey ( key ) [ 1 ] tz_res = None for i in range ( size ) : key_name , value , dtype = winreg . EnumValue ( key , i ) if dtype == winreg . REG_DWORD or dtype == winreg . REG_DWORD_LITTLE_ENDIAN : if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG_SZ : if value . startswith ( '@tzres' ) : tz_res = tz_res or tzres ( ) value = tz_res . name_from_string ( value ) value = value . rstrip ( '\x00' ) dout [ key_name ] = value return dout
3869	async def update_read_timestamp ( self , read_timestamp = None ) : if read_timestamp is None : read_timestamp = ( self . events [ - 1 ] . timestamp if self . events else datetime . datetime . now ( datetime . timezone . utc ) ) if read_timestamp > self . latest_read_timestamp : logger . info ( 'Setting {} latest_read_timestamp from {} to {}' . format ( self . id_ , self . latest_read_timestamp , read_timestamp ) ) state = self . _conversation . self_conversation_state state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( read_timestamp ) ) try : await self . _client . update_watermark ( hangouts_pb2 . UpdateWatermarkRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , last_read_timestamp = parsers . to_timestamp ( read_timestamp ) , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to update read timestamp: {}' . format ( e ) ) raise
7710	def request_roster ( self , version = None ) : processor = self . stanza_processor request = Iq ( stanza_type = "get" ) request . set_payload ( RosterPayload ( version = version ) ) processor . set_response_handlers ( request , self . _get_success , self . _get_error ) processor . send ( request )
12893	def get_power ( self ) : power = ( yield from self . handle_int ( self . API . get ( 'power' ) ) ) return bool ( power )
12102	def _launch_process_group ( self , process_commands , streams_path ) : processes = { } def check_complete_processes ( wait = False ) : result = False for proc in list ( processes ) : if wait : proc . wait ( ) if proc . poll ( ) is not None : self . debug ( "Process %d exited with code %d." % ( processes [ proc ] [ 'tid' ] , proc . poll ( ) ) ) processes [ proc ] [ 'stdout' ] . close ( ) processes [ proc ] [ 'stderr' ] . close ( ) del processes [ proc ] result = True return result for cmd , tid in process_commands : self . debug ( "Starting process %d..." % tid ) job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_handle = open ( os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) , "wb" ) stderr_handle = open ( os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) , "wb" ) proc = subprocess . Popen ( cmd , stdout = stdout_handle , stderr = stderr_handle ) processes [ proc ] = { 'tid' : tid , 'stdout' : stdout_handle , 'stderr' : stderr_handle } if self . max_concurrency : while len ( processes ) >= self . max_concurrency : if not check_complete_processes ( len ( processes ) == 1 ) : time . sleep ( 0.1 ) while len ( processes ) > 0 : if not check_complete_processes ( True ) : time . sleep ( 0.1 )
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : "Add to chart any rules for B that could help extend this edge." B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
5533	def batch_processor ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : if zoom and tile : raise ValueError ( "use either zoom or tile" ) if tile : yield _run_on_single_tile ( self , tile ) elif multi > 1 : for process_info in _run_with_multiprocessing ( self , list ( _get_zoom_level ( zoom , self ) ) , multi , max_chunksize ) : yield process_info elif multi == 1 : for process_info in _run_without_multiprocessing ( self , list ( _get_zoom_level ( zoom , self ) ) ) : yield process_info
9284	def connect ( self , blocking = False , retry = 30 ) : if self . _connected : return while True : try : self . _connect ( ) if not self . skip_login : self . _send_login ( ) break except ( LoginError , ConnectionError ) : if not blocking : raise self . logger . info ( "Retrying connection is %d seconds." % retry ) time . sleep ( retry )
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
9892	def _uptime_minix ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . read ( ) ) f . close ( ) return up except ( IOError , ValueError ) : return None
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
13722	def log ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if re . match ( "file://" , url ) : self . log_file ( url ) elif re . match ( "https://" , url ) or re . match ( "http://" , url ) : self . log_post ( url , credentials , do_verify_certificate ) else : self . log_stdout ( )
7168	def remove_intent ( self , name ) : self . intents . remove ( name ) self . padaos . remove_intent ( name ) self . must_train = True
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3988	def parallel_task_queue ( pool_size = multiprocessing . cpu_count ( ) ) : task_queue = TaskQueue ( pool_size ) yield task_queue task_queue . execute ( )
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
10769	def matlab_formatter ( level , vertices , codes = None ) : vertices = numpy_formatter ( level , vertices , codes ) if codes is not None : level = level [ 0 ] headers = np . vstack ( ( [ v . shape [ 0 ] for v in vertices ] , [ level ] * len ( vertices ) ) ) . T vertices = np . vstack ( list ( it . __next__ ( ) for it in itertools . cycle ( ( iter ( headers ) , iter ( vertices ) ) ) ) ) return vertices
11123	def remove_directory ( self , relativePath , removeFromSystem = False ) : relativePath = os . path . normpath ( relativePath ) parentDirInfoDict , errorMessage = self . get_parent_directory_info ( relativePath ) assert parentDirInfoDict is not None , errorMessage path , name = os . path . split ( relativePath ) if dict . __getitem__ ( parentDirInfoDict , 'directories' ) . get ( name , None ) is None : raise Exception ( "'%s' is not a registered directory in repository relative path '%s'" % ( name , path ) ) if removeFromSystem : for rp in self . walk_files_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isfile ( ap ) : continue if not os . path . exists ( ap ) : continue if os . path . isfile ( ap ) : os . remove ( ap ) for rp in self . walk_directories_relative_path ( relativePath = relativePath ) : ap = os . path . join ( self . __path , relativePath , rp ) if not os . path . isdir ( ap ) : continue if not os . path . exists ( ap ) : continue if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) dict . __getitem__ ( parentDirInfoDict , 'directories' ) . pop ( name , None ) ap = os . path . join ( self . __path , relativePath ) if not os . path . isdir ( ap ) : if not len ( os . listdir ( ap ) ) : os . rmdir ( ap ) self . save ( )
11317	def update_reportnumbers ( self ) : report_037_fields = record_get_field_instances ( self . record , '037' ) for field in report_037_fields : subs = field_get_subfields ( field ) for val in subs . get ( "a" , [ ] ) : if "arXiv" not in val : record_delete_field ( self . record , tag = "037" , field_position_global = field [ 4 ] ) new_subs = [ ( code , val [ 0 ] ) for code , val in subs . items ( ) ] record_add_field ( self . record , "088" , subfields = new_subs ) break
10743	def declaration ( function ) : function , name = _strip_function ( function ) if not function . __code__ . co_code in [ empty_function . __code__ . co_code , doc_string_only_function . __code__ . co_code ] : raise ValueError ( 'Declaration requires empty function definition' ) def not_implemented_function ( * args , ** kwargs ) : raise ValueError ( 'Argument \'{}\' did not specify how \'{}\' should act on it' . format ( args [ 0 ] , name ) ) not_implemented_function . __qualname__ = not_implemented_function . __name__ return default ( not_implemented_function , name = name )
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
10010	def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
10959	def set_mem_level ( self , mem_level = 'hi' ) : key = '' . join ( [ c if c in 'mlh' else '' for c in mem_level ] ) if key not in [ 'h' , 'mh' , 'm' , 'ml' , 'm' , 'l' ] : raise ValueError ( 'mem_level must be one of hi, med-hi, med, med-lo, lo.' ) mem_levels = { 'h' : [ np . float64 , np . float64 ] , 'mh' : [ np . float64 , np . float32 ] , 'm' : [ np . float32 , np . float32 ] , 'ml' : [ np . float32 , np . float16 ] , 'l' : [ np . float16 , np . float16 ] } hi_lvl , lo_lvl = mem_levels [ key ] cat_lvls = { 'obj' : lo_lvl , 'ilm' : hi_lvl , 'bkg' : lo_lvl } self . image . float_precision = hi_lvl self . image . image = self . image . image . astype ( lo_lvl ) self . set_image ( self . image ) for cat in cat_lvls . keys ( ) : obj = self . get ( cat ) if hasattr ( obj , 'comps' ) : for c in obj . comps : c . float_precision = lo_lvl else : obj . float_precision = lo_lvl self . _model = self . _model . astype ( hi_lvl ) self . _residuals = self . _model . astype ( hi_lvl ) self . reset ( )
846	def _getDistances ( self , inputPattern , partitionId = None ) : if not self . _finishedLearning : self . finishLearning ( ) self . _finishedLearning = True if self . _vt is not None and len ( self . _vt ) > 0 : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) sparseInput = self . _sparsifyVector ( inputPattern ) dist = self . _calcDistance ( sparseInput ) if self . _specificIndexTraining : dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf if partitionId is not None : dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf return dist
11357	def format_arxiv_id ( arxiv_id ) : if arxiv_id and "/" not in arxiv_id and "arXiv" not in arxiv_id : return "arXiv:%s" % ( arxiv_id , ) elif arxiv_id and '.' not in arxiv_id and arxiv_id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv_id [ 6 : ] else : return arxiv_id
6338	def dist_tversky ( src , tar , qval = 2 , alpha = 1 , beta = 1 , bias = None ) : return Tversky ( ) . dist ( src , tar , qval , alpha , beta , bias )
12567	def create_empty_dataset ( self , ds_name , dtype = np . float32 ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] ds = self . _group . create_dataset ( ds_name , ( 1 , 1 ) , maxshape = None , dtype = dtype ) self . _datasets [ ds_name ] = ds return ds
1878	def MOVSS ( cpu , dest , src ) : if dest . type == 'register' and src . type == 'register' : assert dest . size == 128 and src . size == 128 dest . write ( dest . read ( ) & ~ 0xffffffff | src . read ( ) & 0xffffffff ) elif dest . type == 'memory' : assert src . type == 'register' dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : assert src . type == 'memory' and dest . type == 'register' assert src . size == 32 and dest . size == 128 dest . write ( Operators . ZEXTEND ( src . read ( ) , 128 ) )
3794	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . m , self . Tc = self . ais [ i ] , self . ms [ i ] , self . Tcs [ i ]
8237	def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
7564	def get_client ( cluster_id , profile , engines , timeout , cores , quiet , spacer , ** kwargs ) : save_stdout = sys . stdout save_stderr = sys . stderr sys . stdout = cStringIO . StringIO ( ) sys . stderr = cStringIO . StringIO ( ) connection_string = "{}establishing parallel connection:" . format ( spacer ) try : if profile not in [ None , "default" ] : args = { 'profile' : profile , "timeout" : timeout } else : clusterargs = [ cluster_id , profile , timeout ] argnames = [ "cluster_id" , "profile" , "timeout" ] args = { key : value for key , value in zip ( argnames , clusterargs ) } ipyclient = ipp . Client ( ** args ) sys . stdout = save_stdout sys . stderr = save_stderr if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : if not quiet : print ( connection_string ) for _ in range ( 6000 ) : initid = len ( ipyclient ) time . sleep ( 0.01 ) if ( engines == "MPI" ) or ( "ipyrad-cli-" in cluster_id ) : if cores : time . sleep ( 0.1 ) if initid == cores : break if initid : time . sleep ( 3 ) if len ( ipyclient ) == initid : break else : if cores : if initid == cores : break else : if initid : break except KeyboardInterrupt as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst except IOError as inst : sys . stdout = save_stdout sys . stderr = save_stderr if "ipyrad-cli-" in cluster_id : raise IPyradWarningExit ( NO_IPCLUSTER_CLI ) else : raise IPyradWarningExit ( NO_IPCLUSTER_API ) except ( ipp . TimeoutError , ipp . NoEnginesRegistered ) as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst except Exception as inst : sys . stdout = save_stdout sys . stderr = save_stderr raise inst finally : sys . stdout = save_stdout sys . stderr = save_stderr return ipyclient
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
12961	def count ( self ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : return conn . scard ( self . _get_ids_key ( ) ) if numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] return conn . scard ( self . _get_key_for_index ( filterFieldName , filterValue ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] return len ( conn . sinter ( indexKeys ) ) notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : return len ( conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) pks = pipeline . execute ( ) [ 1 ] return len ( pks )
2469	def set_file_copyright ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_copytext_set : self . file_copytext_set = True if validations . validate_file_cpyright ( text ) : if isinstance ( text , string_types ) : self . file ( doc ) . copyright = str_from_text ( text ) else : self . file ( doc ) . copyright = text return True else : raise SPDXValueError ( 'File::CopyRight' ) else : raise CardinalityError ( 'File::CopyRight' ) else : raise OrderError ( 'File::CopyRight' )
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
13715	def next ( self ) : queue = self . queue items = [ ] item = self . next_item ( ) if item is None : return items items . append ( item ) while len ( items ) < self . upload_size and not queue . empty ( ) : item = self . next_item ( ) if item : items . append ( item ) return items
11820	def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings
1683	def PrintErrorCounts ( self ) : for category , count in sorted ( iteritems ( self . errors_by_category ) ) : self . PrintInfo ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error_count > 0 : self . PrintInfo ( 'Total errors found: %d\n' % self . error_count )
3359	def insert ( self , index , object ) : self . _check ( object . id ) list . insert ( self , index , object ) _dict = self . _dict for i , j in iteritems ( _dict ) : if j >= index : _dict [ i ] = j + 1 _dict [ object . id ] = index
4856	def _delete_transmissions ( self , content_metadata_item_ids ) : ContentMetadataItemTransmission = apps . get_model ( 'integrated_channel' , 'ContentMetadataItemTransmission' ) ContentMetadataItemTransmission . objects . filter ( enterprise_customer = self . enterprise_configuration . enterprise_customer , integrated_channel_code = self . enterprise_configuration . channel_code ( ) , content_id__in = content_metadata_item_ids ) . delete ( )
2861	def _transaction_end ( self ) : self . _command . append ( '\x87' ) self . _ft232h . _write ( '' . join ( self . _command ) ) return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
9784	def stop ( ctx , yes ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _build ) ) : click . echo ( 'Existing without stopping build job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . build_job . stop ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job is being stopped." )
10733	def to_bool ( option , value ) : if type ( value ) is str : if value . lower ( ) == 'true' : value = True elif value . lower ( ) == 'false' : value = False return ( option , value )
13092	def load_targets ( self ) : ldap_services = [ ] if self . ldap : ldap_services = self . search . get_services ( ports = [ 389 ] , up = True ) self . ldap_strings = [ "ldap://{}" . format ( service . address ) for service in ldap_services ] self . services = self . search . get_services ( tags = [ 'smb_signing_disabled' ] ) self . ips = [ str ( service . address ) for service in self . services ]
10076	def commit ( self , * args , ** kwargs ) : return super ( Deposit , self ) . commit ( * args , ** kwargs )
223	def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { "REQUEST_METHOD" : scope [ "method" ] , "SCRIPT_NAME" : scope . get ( "root_path" , "" ) , "PATH_INFO" : scope [ "path" ] , "QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , "SERVER_PROTOCOL" : f"HTTP/{scope['http_version']}" , "wsgi.version" : ( 1 , 0 ) , "wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , "wsgi.input" : io . BytesIO ( body ) , "wsgi.errors" : sys . stdout , "wsgi.multithread" : True , "wsgi.multiprocess" : True , "wsgi.run_once" : False , } server = scope . get ( "server" ) or ( "localhost" , 80 ) environ [ "SERVER_NAME" ] = server [ 0 ] environ [ "SERVER_PORT" ] = server [ 1 ] if scope . get ( "client" ) : environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] for name , value in scope . get ( "headers" , [ ] ) : name = name . decode ( "latin1" ) if name == "content-length" : corrected_name = "CONTENT_LENGTH" elif name == "content-type" : corrected_name = "CONTENT_TYPE" else : corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) value = value . decode ( "latin1" ) if corrected_name in environ : value = environ [ corrected_name ] + "," + value environ [ corrected_name ] = value return environ
405	def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , ** kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
11302	def provider_for_url ( self , url ) : for provider , regex in self . get_registry ( ) . items ( ) : if re . match ( regex , url ) is not None : return provider raise OEmbedMissingEndpoint ( 'No endpoint matches URL: %s' % url )
8798	def get_security_group_states ( self , interfaces ) : LOG . debug ( "Getting security groups from Redis for {0}" . format ( interfaces ) ) interfaces = tuple ( interfaces ) vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in interfaces ] sec_grp_all = self . get_fields_all ( vif_keys ) ret = { } for vif , group in zip ( interfaces , sec_grp_all ) : if group : ret [ vif ] = { SECURITY_GROUP_ACK : None , SECURITY_GROUP_HASH_ATTR : [ ] } temp_ack = group [ SECURITY_GROUP_ACK ] . lower ( ) temp_rules = group [ SECURITY_GROUP_HASH_ATTR ] if temp_rules : temp_rules = json . loads ( temp_rules ) ret [ vif ] [ SECURITY_GROUP_HASH_ATTR ] = temp_rules [ "rules" ] if "true" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = True elif "false" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = False else : ret . pop ( vif , None ) LOG . debug ( "Skipping bad ack value %s" % temp_ack ) return ret
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
12727	def stop_erps ( self , stop_erps ) : _set_params ( self . ode_obj , 'StopERP' , stop_erps , self . ADOF + self . LDOF )
4631	def point ( self ) : string = unhexlify ( self . unCompressed ( ) ) return ecdsa . VerifyingKey . from_string ( string [ 1 : ] , curve = ecdsa . SECP256k1 ) . pubkey . point
2258	def argsort ( indexable , key = None , reverse = False ) : if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
12497	def as_ndarray ( arr , copy = False , dtype = None , order = 'K' ) : if order not in ( 'C' , 'F' , 'A' , 'K' , None ) : raise ValueError ( "Invalid value for 'order': {}" . format ( str ( order ) ) ) if isinstance ( arr , np . memmap ) : if dtype is None : if order in ( 'K' , 'A' , None ) : ret = np . array ( np . asarray ( arr ) , copy = True ) else : ret = np . array ( np . asarray ( arr ) , copy = True , order = order ) else : if order in ( 'K' , 'A' , None ) : ret = np . asarray ( arr ) . astype ( dtype ) else : ret = _asarray ( np . array ( arr , copy = True ) , dtype = dtype , order = order ) elif isinstance ( arr , np . ndarray ) : ret = _asarray ( arr , dtype = dtype , order = order ) if np . may_share_memory ( ret , arr ) and copy : ret = ret . T . copy ( ) . T if ret . flags [ 'F_CONTIGUOUS' ] else ret . copy ( ) elif isinstance ( arr , ( list , tuple ) ) : if order in ( "A" , "K" ) : ret = np . asarray ( arr , dtype = dtype ) else : ret = np . asarray ( arr , dtype = dtype , order = order ) else : raise ValueError ( "Type not handled: {}" . format ( arr . __class__ ) ) return ret
8291	def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
8491	def _parse_hosts ( self , hosts ) : if hosts is None : return if isinstance ( hosts , six . string_types ) : hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] hosts = [ host . split ( ':' ) for host in hosts ] hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] return tuple ( hosts )
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( ** querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
3734	def load_included_indentifiers ( self , file_name ) : self . restrict_identifiers = True included_identifiers = set ( ) with open ( file_name ) as f : [ included_identifiers . add ( int ( line ) ) for line in f ] self . included_identifiers = included_identifiers
10304	def min_tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a or not b : return 0.0 return len ( a & b ) / min ( len ( a ) , len ( b ) )
331	def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
6667	def check_version ( ) : global CHECK_VERSION if not CHECK_VERSION : return CHECK_VERSION = 0 from six . moves . urllib . request import urlopen try : response = urlopen ( "https://pypi.org/pypi/burlap/json" ) data = json . loads ( response . read ( ) . decode ( ) ) remote_release = sorted ( tuple ( map ( int , _ . split ( '.' ) ) ) for _ in data [ 'releases' ] . keys ( ) ) [ - 1 ] remote_release_str = '.' . join ( map ( str , remote_release ) ) local_release = VERSION local_release_str = '.' . join ( map ( str , local_release ) ) if remote_release > local_release : print ( '\033[93m' ) print ( "You are using burlap version %s, however version %s is available." % ( local_release_str , remote_release_str ) ) print ( "You should consider upgrading via the 'pip install --upgrade burlap' command." ) print ( '\033[0m' ) except Exception as exc : print ( '\033[93m' ) print ( "Unable to check for updated burlap version: %s" % exc ) print ( '\033[0m' )
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if not entity : entity = self . model_class ( ** filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
7416	def loci2migrate ( name , locifile , popdict , mindict = 1 ) : outfile = open ( name + ".migrate" , 'w' ) infile = open ( locifile , 'r' ) if isinstance ( mindict , int ) : mindict = { pop : mindict for pop in popdict } else : mindict = mindict keep = [ ] MINS = zip ( taxa . keys ( ) , minhits ) loci = infile . read ( ) . strip ( ) . split ( "|" ) [ : - 1 ] for loc in loci : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] GG = [ ] for group , mins in MINS : GG . append ( sum ( [ i in samps for i in taxa [ group ] ] ) >= int ( mins ) ) if all ( GG ) : keep . append ( loc ) print >> outfile , len ( taxa ) , len ( keep ) , "( npops nloci for data set" , data . name + ".loci" , ")" done = 0 for group in taxa : if not done : loclens = [ len ( loc . split ( "\n" ) [ 1 ] . split ( ) [ - 1 ] . replace ( "x" , "n" ) . replace ( "n" , "" ) ) for loc in keep ] print >> outfile , " " . join ( map ( str , loclens ) ) done += 1 indslist = [ ] for loc in keep : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] inds = sum ( [ i in samps for i in taxa [ group ] ] ) indslist . append ( inds ) print >> outfile , " " . join ( map ( str , indslist ) ) , group for loc in range ( len ( keep ) ) : seqs = [ i . split ( ) [ - 1 ] for i in keep [ loc ] . split ( "\n" ) if i . split ( ) [ 0 ] . replace ( ">" , "" ) in taxa [ group ] ] for i in range ( len ( seqs ) ) : print >> outfile , group [ 0 : 8 ] + "_" + str ( i ) + ( " " * ( 10 - len ( group [ 0 : 8 ] + "_" + str ( i ) ) ) ) + seqs [ i ] . replace ( "x" , "n" ) . replace ( "n" , "" ) outfile . close ( )
1576	def make_shell_endpoint ( topologyInfo , instance_id ) : pplan = topologyInfo [ "physical_plan" ] stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] return "http://%s:%d" % ( host , shell_port )
5021	def get_enterprise_customer_from_catalog_id ( catalog_id ) : try : return str ( EnterpriseCustomerCatalog . objects . get ( pk = catalog_id ) . enterprise_customer . uuid ) except EnterpriseCustomerCatalog . DoesNotExist : return None
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
11881	def scanProcessForMapping ( pid , searchPortion , isExactMatch = False , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e with open ( '/proc/%d/maps' % ( pid , ) , 'r' ) as f : contents = f . read ( ) lines = contents . split ( '\n' ) matchedMappings = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor == searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) == searchIn . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor in searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) in searchIn . lower ( ) ) for line in lines : portion = ' ' . join ( line . split ( ' ' ) [ 5 : ] ) . lstrip ( ) if isMatch ( searchPortion , portion ) : matchedMappings . append ( '\t' + line ) if len ( matchedMappings ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'matchedMappings' : matchedMappings , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
2181	def parse_authorization_response ( self , url ) : log . debug ( "Parsing token from query part of url %s" , url ) token = dict ( urldecode ( urlparse ( url ) . query ) ) log . debug ( "Updating internal client token attribute." ) self . _populate_attributes ( token ) self . token = token return token
7204	def set ( self , ** kwargs ) : for port_name , port_value in kwargs . items ( ) : if hasattr ( port_value , 'value' ) : port_value = port_value . value self . inputs . __setattr__ ( port_name , port_value )
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
1140	def fill ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . fill ( text )
5187	def inventory ( self , ** kwargs ) : inventory = self . _query ( 'inventory' , ** kwargs ) for inv in inventory : yield Inventory ( node = inv [ 'certname' ] , time = inv [ 'timestamp' ] , environment = inv [ 'environment' ] , facts = inv [ 'facts' ] , trusted = inv [ 'trusted' ] )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
12626	def recursive_find_search ( folder_path , regex = '' ) : outlist = [ ] for root , dirs , files in os . walk ( folder_path ) : outlist . extend ( [ op . join ( root , f ) for f in files if re . search ( regex , f ) ] ) return outlist
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
13510	def pyflakes ( ) : packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
3809	def serialize_formula ( formula ) : r charge = charge_from_formula ( formula ) element_dict = nested_formula_parser ( formula ) base = atoms_to_Hill ( element_dict ) if charge == 0 : pass elif charge > 0 : if charge == 1 : base += '+' else : base += '+' + str ( charge ) elif charge < 0 : if charge == - 1 : base += '-' else : base += str ( charge ) return base
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
11651	def transform ( self , X ) : self . _check_fitted ( ) M = self . smoothness dim = self . dim_ inds = self . inds_ do_check = self . do_bounds_check X = as_features ( X ) if X . dim != dim : msg = "model fit for dimension {} but got dim {}" raise ValueError ( msg . format ( dim , X . dim ) ) Xt = np . empty ( ( len ( X ) , self . inds_ . shape [ 0 ] ) ) Xt . fill ( np . nan ) if self . basis == 'cosine' : coefs = ( np . pi * np . arange ( M + 1 ) ) [ ... , : ] for i , bag in enumerate ( X ) : if do_check : if np . min ( bag ) < 0 or np . max ( bag ) > 1 : raise ValueError ( "Bag {} not in [0, 1]" . format ( i ) ) phi = coefs * bag [ ... , np . newaxis ] np . cos ( phi , out = phi ) phi [ : , : , 1 : ] *= np . sqrt ( 2 ) B = reduce ( op . mul , ( phi [ : , i , inds [ : , i ] ] for i in xrange ( dim ) ) ) Xt [ i , : ] = np . mean ( B , axis = 0 ) else : raise ValueError ( "unknown basis '{}'" . format ( self . basis ) ) return Xt
4541	def _on_index ( self , old_index ) : if self . animation : log . debug ( '%s: %s' , self . __class__ . __name__ , self . current_animation . title ) self . frames = self . animation . generate_frames ( False )
2779	def get_data ( self , url , headers = dict ( ) , params = dict ( ) , render_json = True ) : url = urljoin ( self . end_point , url ) response = requests . get ( url , headers = headers , params = params , timeout = self . get_timeout ( ) ) if render_json : return response . json ( ) return response . content
3558	def discover ( cls , device , timeout_sec = TIMEOUT_SEC ) : device . discover ( cls . SERVICES , cls . CHARACTERISTICS , timeout_sec )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
11635	def oauth2_access_parser ( self , raw_access ) : parsed_access = json . loads ( raw_access . content . decode ( 'utf-8' ) ) self . access_token = parsed_access [ 'access_token' ] self . token_type = parsed_access [ 'token_type' ] self . refresh_token = parsed_access [ 'refresh_token' ] self . guid = parsed_access [ 'xoauth_yahoo_guid' ] credentials = { 'access_token' : self . access_token , 'token_type' : self . token_type , 'refresh_token' : self . refresh_token , 'guid' : self . guid } return credentials
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( int ( self . num_retries ) >= 0 and v <= self . num_retries and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
10547	def find_taskruns ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'taskrun' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : return res except : raise
7911	def get ( self , key , local_default = None , required = False ) : if key in self . _settings : return self . _settings [ key ] if local_default is not None : return local_default if key in self . _defs : setting_def = self . _defs [ key ] if setting_def . default is not None : return setting_def . default factory = setting_def . factory if factory is None : return None value = factory ( self ) if setting_def . cache is True : setting_def . default = value return value if required : raise KeyError ( key ) return local_default
3692	def a_alpha_and_derivatives ( self , T , full = True , quick = True ) : r return TWU_a_alpha_common ( T , self . Tc , self . omega , self . a , full = full , quick = quick , method = 'SRK' )
2657	def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
10899	def update ( self , value = 0 ) : self . _deltas . append ( time . time ( ) ) self . value = value self . _percent = 100.0 * self . value / self . num if self . bar : self . _bars = self . _bar_symbol * int ( np . round ( self . _percent / 100. * self . _barsize ) ) if ( len ( self . _deltas ) < 2 ) or ( self . _deltas [ - 1 ] - self . _deltas [ - 2 ] ) > 1e-1 : self . _estimate_time ( ) self . _draw ( ) if self . value == self . num : self . end ( )
3217	def get_subnets ( vpc , ** conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
13139	def to_json ( self ) : if self . subreference is not None : return { "source" : self . objectId , "selector" : { "type" : "FragmentSelector" , "conformsTo" : "http://ontology-dts.org/terms/subreference" , "value" : self . subreference } } else : return { "source" : self . objectId }
3877	async def _on_event ( self , event_ ) : conv_id = event_ . conversation_id . id try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for event notification: %s' , conv_id ) else : self . _sync_timestamp = parsers . from_timestamp ( event_ . timestamp ) conv_event = conv . add_event ( event_ ) if conv_event is not None : await self . on_event . fire ( conv_event ) await conv . on_event . fire ( conv_event )
4045	def num_collectionitems ( self , collection ) : query = "/{t}/{u}/collections/{c}/items" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _totals ( query )
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
8937	def get_pypi_auth ( configfile = '~/.pypirc' ) : pypi_cfg = ConfigParser ( ) if pypi_cfg . read ( os . path . expanduser ( configfile ) ) : try : user = pypi_cfg . get ( 'pypi' , 'username' ) pwd = pypi_cfg . get ( 'pypi' , 'password' ) return user , pwd except ConfigError : notify . warning ( "No PyPI credentials in '{}'," " will fall back to '~/.netrc'..." . format ( configfile ) ) return None
7359	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , string_types ) : sequence_dict = { "seq" : sequence_dict } elif isinstance ( sequence_dict , ( list , tuple ) ) : sequence_dict = { seq : seq for seq in sequence_dict } peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) peptide_set = set ( [ ] ) peptide_to_name_offset_pairs = defaultdict ( list ) for name , sequence in sequence_dict . items ( ) : for peptide_length in peptide_lengths : for i in range ( len ( sequence ) - peptide_length + 1 ) : peptide = sequence [ i : i + peptide_length ] peptide_set . add ( peptide ) peptide_to_name_offset_pairs [ peptide ] . append ( ( name , i ) ) peptide_list = sorted ( peptide_set ) binding_predictions = self . predict_peptides ( peptide_list ) results = [ ] for binding_prediction in binding_predictions : for name , offset in peptide_to_name_offset_pairs [ binding_prediction . peptide ] : results . append ( binding_prediction . clone_with_updates ( source_sequence_name = name , offset = offset ) ) self . _check_results ( results , peptides = peptide_set , alleles = self . alleles ) return BindingPredictionCollection ( results )
2101	def log ( s , header = '' , file = sys . stderr , nl = 1 , ** kwargs ) : if not settings . verbose : return if header : word_arr = s . split ( ' ' ) multi = [ ] word_arr . insert ( 0 , '%s:' % header . upper ( ) ) i = 0 while i < len ( word_arr ) : to_add = [ '***' ] count = 3 while count <= 79 : count += len ( word_arr [ i ] ) + 1 if count <= 79 : to_add . append ( word_arr [ i ] ) i += 1 if i == len ( word_arr ) : break if len ( to_add ) == 1 : to_add . append ( word_arr [ i ] ) i += 1 if i != len ( word_arr ) : count -= len ( word_arr [ i ] ) + 1 to_add . append ( '*' * ( 78 - count ) ) multi . append ( ' ' . join ( to_add ) ) s = '\n' . join ( multi ) lines = len ( multi ) else : lines = 1 if isinstance ( nl , int ) and nl > lines : s += '\n' * ( nl - lines ) return secho ( s , file = file , ** kwargs )
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
11340	def set_target_celsius ( self , celsius , mode = config . SCHEDULE_HOLD ) : temperature = celsius_to_nuheat ( celsius ) self . set_target_temperature ( temperature , mode )
2705	def collect_phrases ( sent , ranks , spacy_nlp ) : tail = 0 last_idx = sent [ 0 ] . idx - 1 phrase = [ ] while tail < len ( sent ) : w = sent [ tail ] if ( w . word_id > 0 ) and ( w . root in ranks ) and ( ( w . idx - last_idx ) == 1 ) : rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] , ids = w . word_id , pos = w . pos . lower ( ) , count = 1 ) phrase . append ( rl ) else : for text , p in enumerate_chunks ( phrase , spacy_nlp ) : if p : id_list = [ rl . ids for rl in p ] rank_list = [ rl . rank for rl in p ] np_rl = RankedLexeme ( text = text , rank = rank_list , ids = id_list , pos = "np" , count = 1 ) if DEBUG : print ( np_rl ) yield np_rl phrase = [ ] last_idx = w . idx tail += 1
711	def _iterModels ( modelIDs ) : class ModelInfoIterator ( object ) : __CACHE_LIMIT = 1000 debug = False def __init__ ( self , modelIDs ) : self . __modelIDs = tuple ( modelIDs ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __init__; numModelIDs=%s" % len ( self . __modelIDs ) ) self . __nextIndex = 0 self . __modelCache = collections . deque ( ) return def __iter__ ( self ) : return self def next ( self ) : return self . __getNext ( ) def __getNext ( self ) : if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: __getNext(); modelCacheLen=%s" % ( len ( self . __modelCache ) ) ) if not self . __modelCache : self . __fillCache ( ) if not self . __modelCache : raise StopIteration ( ) return self . __modelCache . popleft ( ) def __fillCache ( self ) : assert ( not self . __modelCache ) numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 if self . __nextIndex >= numModelIDs : return idRange = self . __nextIndex + self . __CACHE_LIMIT if idRange > numModelIDs : idRange = numModelIDs lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] self . __nextIndex += ( idRange - self . __nextIndex ) infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) assert len ( infoList ) == len ( lookupIDs ) , "modelsInfo returned %s elements; expected %s." % ( len ( infoList ) , len ( lookupIDs ) ) for rawInfo in infoList : modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) self . __modelCache . append ( modelInfo ) assert len ( self . __modelCache ) == len ( lookupIDs ) , "Added %s elements to modelCache; expected %s." % ( len ( self . __modelCache ) , len ( lookupIDs ) ) if self . debug : _emit ( Verbosity . DEBUG , "MODELITERATOR: Leaving __fillCache(); modelCacheLen=%s" % ( len ( self . __modelCache ) , ) ) return ModelInfoIterator ( modelIDs )
6570	def last_arg_decorator ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : if signature_matches ( func , args , kwargs ) : return func ( * args , ** kwargs ) else : return lambda last : func ( * ( args + ( last , ) ) , ** kwargs ) return decorator
8225	def _key_pressed ( self , key , keycode ) : self . _namespace [ 'key' ] = key self . _namespace [ 'keycode' ] = keycode self . _namespace [ 'keydown' ] = True
6412	def agmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) : m_a , m_g = ( m_a + m_g ) / 2 , ( m_a * m_g ) ** ( 1 / 2 ) return m_a
10800	def _newcall ( self , rvecs ) : sigma = 1 * self . filter_size out = self . _eval_firstorder ( rvecs , self . d , sigma ) ondata = self . _eval_firstorder ( self . x , self . d , sigma ) for i in range ( self . iterations ) : out += self . _eval_firstorder ( rvecs , self . d - ondata , sigma ) ondata += self . _eval_firstorder ( self . x , self . d - ondata , sigma ) sigma *= self . damp return out
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
8959	def build ( ctx , dput = '' , opts = '' ) : with io . open ( 'debian/changelog' , encoding = 'utf-8' ) as changes : metadata = re . match ( r'^([^ ]+) \(([^)]+)\) ([^;]+); urgency=(.+)$' , changes . readline ( ) . rstrip ( ) ) if not metadata : notify . failure ( 'Badly formatted top entry in changelog' ) name , version , _ , _ = metadata . groups ( ) ctx . run ( 'dpkg-buildpackage {} {}' . format ( ctx . rituals . deb . build . opts , opts ) ) if not os . path . exists ( 'dist' ) : os . makedirs ( 'dist' ) artifact_pattern = '{}?{}*' . format ( name , re . sub ( r'[^-_.a-zA-Z0-9]' , '?' , version ) ) changes_files = [ ] for debfile in glob . glob ( '../' + artifact_pattern ) : shutil . move ( debfile , 'dist' ) if debfile . endswith ( '.changes' ) : changes_files . append ( os . path . join ( 'dist' , os . path . basename ( debfile ) ) ) ctx . run ( 'ls -l dist/{}' . format ( artifact_pattern ) ) if dput : ctx . run ( 'dput {} {}' . format ( dput , ' ' . join ( changes_files ) ) )
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
4213	def pass_from_pipe ( cls ) : is_pipe = not sys . stdin . isatty ( ) return is_pipe and cls . strip_last_newline ( sys . stdin . read ( ) )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
12141	def load_dframe ( self , dframe ) : filename_series = dframe [ self . key ] loaded_data = filename_series . map ( self . filetype . data ) keys = [ list ( el . keys ( ) ) for el in loaded_data . values ] for key in set ( ) . union ( * keys ) : key_exists = key in dframe . columns if key_exists : self . warning ( "Appending '_data' suffix to data key %r to avoid" "overwriting existing metadata with the same name." % key ) suffix = '_data' if key_exists else '' dframe [ key + suffix ] = loaded_data . map ( lambda x : x . get ( key , np . nan ) ) return dframe
9031	def _expand_produced_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_consumed ( ) : return row = mesh . consuming_row position = Point ( row_position . x - mesh . index_in_consuming_row + mesh_index , row_position . y + INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
12587	def all_childnodes_to_nifti1img ( h5group ) : child_nodes = [ ] def append_parent_if_dataset ( name , obj ) : if isinstance ( obj , h5py . Dataset ) : if name . split ( '/' ) [ - 1 ] == 'data' : child_nodes . append ( obj . parent ) vols = [ ] h5group . visititems ( append_parent_if_dataset ) for c in child_nodes : vols . append ( hdfgroup_to_nifti1image ( c ) ) return vols
7446	def _step5func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 5: Consensus base calling " ) samples = _get_samples ( self , samples ) if not self . _samples_precheck ( samples , 5 , force ) : raise IPyradError ( FIRST_RUN_4 ) elif not force : if all ( [ i . stats . state >= 5 for i in samples ] ) : print ( CONSENS_EXIST . format ( len ( samples ) ) ) return assemble . consens_se . run ( self , samples , force , ipyclient )
7465	def _parse_01 ( ofiles , individual = False ) : cols = [ ] dats = [ ] for ofile in ofiles : with open ( ofile ) as infile : dat = infile . read ( ) lastbits = dat . split ( ".mcmc.txt\n\n" ) [ 1 : ] results = lastbits [ 0 ] . split ( "\n\n" ) [ 0 ] . split ( ) shape = ( ( ( len ( results ) - 3 ) / 4 ) , 4 ) dat = np . array ( results [ 3 : ] ) . reshape ( shape ) cols . append ( dat [ : , 3 ] . astype ( float ) ) if not individual : cols = np . array ( cols ) cols = cols . sum ( axis = 0 ) / len ( ofiles ) dat [ : , 3 ] = cols . astype ( str ) df = pd . DataFrame ( dat [ : , 1 : ] ) df . columns = [ "delim" , "prior" , "posterior" ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) df [ "nspecies" ] = nspecies return df else : res = [ ] for i in xrange ( len ( cols ) ) : x = dat x [ : , 3 ] = cols [ i ] . astype ( str ) x = pd . DataFrame ( x [ : , 1 : ] ) x . columns = [ 'delim' , 'prior' , 'posterior' ] nspecies = 1 + np . array ( [ list ( i ) for i in dat [ : , 1 ] ] , dtype = int ) . sum ( axis = 1 ) x [ "nspecies" ] = nspecies res . append ( x ) return res
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
161	def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
9126	def _make_session ( connection : Optional [ str ] = None ) -> Session : if connection is None : connection = get_global_connection ( ) engine = create_engine ( connection ) create_all ( engine ) session_cls = sessionmaker ( bind = engine ) session = session_cls ( ) return session
1923	def binary_arch ( binary ) : with open ( binary , 'rb' ) as f : elffile = ELFFile ( f ) if elffile [ 'e_machine' ] == 'EM_X86_64' : return True else : return False
6445	def _cond_x ( self , word , suffix_len ) : return word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 : - suffix_len ] == 'u' and word [ - suffix_len - 1 ] == 'e' )
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
31	def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )
7928	def stop ( self ) : with self . lock : for dummy in self . threads : self . queue . put ( None )
6083	def deflections_of_galaxies_from_sub_grid ( sub_grid , galaxies ) : if galaxies : return sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( sub_grid ) , galaxies ) ) else : return np . full ( ( sub_grid . shape [ 0 ] , 2 ) , 0.0 )
5762	def write_groovy_script_and_configs ( filename , content , job_configs , view_configs = None ) : with open ( filename , 'w' ) as h : h . write ( content ) if view_configs : view_config_dir = os . path . join ( os . path . dirname ( filename ) , 'view_configs' ) if not os . path . isdir ( view_config_dir ) : os . makedirs ( view_config_dir ) for config_name , config_body in view_configs . items ( ) : config_filename = os . path . join ( view_config_dir , config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body ) job_config_dir = os . path . join ( os . path . dirname ( filename ) , 'job_configs' ) if not os . path . isdir ( job_config_dir ) : os . makedirs ( job_config_dir ) format_str = '%0' + str ( len ( str ( len ( job_configs ) ) ) ) + 'd' i = 0 for config_name , config_body in job_configs . items ( ) : i += 1 config_filename = os . path . join ( job_config_dir , format_str % i + ' ' + config_name ) with open ( config_filename , 'w' ) as config_fh : config_fh . write ( config_body )
11619	def _setup ( ) : s = str . split if sys . version_info < ( 3 , 0 ) : s = unicode . split def pop_all ( some_dict , some_list ) : for scheme in some_list : some_dict . pop ( scheme ) global SCHEMES SCHEMES = copy . deepcopy ( sanscript . SCHEMES ) pop_all ( SCHEMES , [ sanscript . ORIYA , sanscript . BENGALI , sanscript . GUJARATI ] ) SCHEMES [ HK ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ HK ] [ 'consonants' ] + s ( ) } ) SCHEMES [ ITRANS ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ ITRANS ] [ 'consonants' ] + s ( ) } ) pop_all ( SCHEMES [ ITRANS ] . synonym_map , s ( ) ) SCHEMES [ OPTITRANS ] . update ( { 'vowels' : s ( ) + s ( ) , 'marks' : s ( ) + s ( ) , 'consonants' : sanscript . SCHEMES [ OPTITRANS ] [ 'consonants' ] + s ( ) } ) pop_all ( SCHEMES [ OPTITRANS ] . synonym_map , s ( ) )
4407	async def listen ( self ) : while not self . _shutdown : try : data = json . loads ( await self . _ws . recv ( ) ) except websockets . ConnectionClosed as error : log . warning ( 'Disconnected from Lavalink: {}' . format ( str ( error ) ) ) for g in self . _lavalink . players . _players . copy ( ) . keys ( ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( g ) ) await ws . voice_state ( int ( g ) , None ) self . _lavalink . players . clear ( ) if self . _shutdown : break if await self . _attempt_reconnect ( ) : return log . warning ( 'Unable to reconnect to Lavalink!' ) break op = data . get ( 'op' , None ) log . debug ( 'Received WebSocket data {}' . format ( str ( data ) ) ) if not op : return log . debug ( 'Received WebSocket message without op {}' . format ( str ( data ) ) ) if op == 'event' : log . debug ( 'Received event of type {}' . format ( data [ 'type' ] ) ) player = self . _lavalink . players [ int ( data [ 'guildId' ] ) ] event = None if data [ 'type' ] == 'TrackEndEvent' : event = TrackEndEvent ( player , data [ 'track' ] , data [ 'reason' ] ) elif data [ 'type' ] == 'TrackExceptionEvent' : event = TrackExceptionEvent ( player , data [ 'track' ] , data [ 'error' ] ) elif data [ 'type' ] == 'TrackStuckEvent' : event = TrackStuckEvent ( player , data [ 'track' ] , data [ 'thresholdMs' ] ) if event : await self . _lavalink . dispatch_event ( event ) elif op == 'playerUpdate' : await self . _lavalink . update_state ( data ) elif op == 'stats' : self . _lavalink . stats . _update ( data ) await self . _lavalink . dispatch_event ( StatsUpdateEvent ( self . _lavalink . stats ) ) log . debug ( 'Closing WebSocket...' ) await self . _ws . close ( )
13697	def try_read_file ( s ) : try : with open ( s , 'r' ) as f : data = f . read ( ) except FileNotFoundError : return s except EnvironmentError as ex : print_err ( '\nFailed to read file: {}\n {}' . format ( s , ex ) ) return None return data
11255	def attrdict ( prev , attr_names ) : if isinstance ( attr_names , dict ) : for obj in prev : attr_values = dict ( ) for name in attr_names . keys ( ) : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) else : attr_values [ name ] = attr_names [ name ] yield attr_values else : for obj in prev : attr_values = dict ( ) for name in attr_names : if hasattr ( obj , name ) : attr_values [ name ] = getattr ( obj , name ) yield attr_values
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : if product in enzymes : continue if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : if reactant in enzymes : continue if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
739	def cPrint ( self , level , message , * args , ** kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
4573	def hsv2rgb_spectrum ( hsv ) : h , s , v = hsv return hsv2rgb_raw ( ( ( h * 192 ) >> 8 , s , v ) )
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
9888	def _call_multi_fortran_z_attr ( self , names , data_types , num_elems , entry_nums , attr_nums , var_names , input_type_code , func , data_offset = None ) : idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : max_num = num_elems [ idx ] . max ( ) sub_num_elems = num_elems [ idx ] sub_names = np . array ( names ) [ idx ] sub_var_names = np . array ( var_names ) [ idx ] sub_entry_nums = entry_nums [ idx ] sub_attr_nums = attr_nums [ idx ] status , data = func ( self . fname , sub_attr_nums , sub_entry_nums , len ( sub_attr_nums ) , max_num , len ( self . fname ) ) if ( status == 0 ) . all ( ) : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset self . _process_return_multi_z_attr ( data , sub_names , sub_var_names , sub_num_elems ) else : idx , = np . where ( status != 0 ) raise IOError ( fortran_cdf . statusreporter ( status [ idx ] [ 0 ] ) )
8979	def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
2237	def import_module_from_path ( modpath , index = - 1 ) : import os if not os . path . exists ( modpath ) : import re import zipimport pat = '(.zip[' + re . escape ( os . path . sep ) + '/:])' parts = re . split ( pat , modpath , flags = re . IGNORECASE ) if len ( parts ) > 2 : archivepath = '' . join ( parts [ : - 1 ] ) [ : - 1 ] internal = parts [ - 1 ] modname = os . path . splitext ( internal ) [ 0 ] modname = os . path . normpath ( modname ) if os . path . exists ( archivepath ) : zimp_file = zipimport . zipimporter ( archivepath ) module = zimp_file . load_module ( modname ) return module raise IOError ( 'modpath={} does not exist' . format ( modpath ) ) else : module = _custom_import_modpath ( modpath ) return module
2996	def marketNewsDF ( count = 10 , token = '' , version = '' ) : df = pd . DataFrame ( marketNews ( count , token , version ) ) _toDatetime ( df ) _reindex ( df , 'datetime' ) return df
2622	def spin_up_instance ( self , command , job_name ) : command = Template ( template_string ) . substitute ( jobname = job_name , user_script = command , linger = str ( self . linger ) . lower ( ) , worker_init = self . worker_init ) instance_type = self . instance_type subnet = self . sn_ids [ 0 ] ami_id = self . image_id total_instances = len ( self . instances ) if float ( self . spot_max_bid ) > 0 : spot_options = { 'MarketType' : 'spot' , 'SpotOptions' : { 'MaxPrice' : str ( self . spot_max_bid ) , 'SpotInstanceType' : 'one-time' , 'InstanceInterruptionBehavior' : 'terminate' } } else : spot_options = { } if total_instances > self . max_nodes : logger . warn ( "Exceeded instance limit ({}). Cannot continue\n" . format ( self . max_nodes ) ) return [ None ] try : tag_spec = [ { "ResourceType" : "instance" , "Tags" : [ { 'Key' : 'Name' , 'Value' : job_name } ] } ] instance = self . ec2 . create_instances ( MinCount = 1 , MaxCount = 1 , InstanceType = instance_type , ImageId = ami_id , KeyName = self . key_name , SubnetId = subnet , SecurityGroupIds = [ self . sg_id ] , TagSpecifications = tag_spec , InstanceMarketOptions = spot_options , InstanceInitiatedShutdownBehavior = 'terminate' , IamInstanceProfile = { 'Arn' : self . iam_instance_profile_arn } , UserData = command ) except ClientError as e : print ( e ) logger . error ( e . response ) return [ None ] except Exception as e : logger . error ( "Request for EC2 resources failed : {0}" . format ( e ) ) return [ None ] self . instances . append ( instance [ 0 ] . id ) logger . info ( "Started up 1 instance {} . Instance type:{}" . format ( instance [ 0 ] . id , instance_type ) ) return instance
2609	def _extract_buffers ( obj , threshold = MAX_BYTES ) : buffers = [ ] if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : nbytes = _nbytes ( buf ) if nbytes > threshold : obj . buffers [ i ] = None buffers . append ( buf ) elif isinstance ( buf , memoryview ) : obj . buffers [ i ] = buf . tobytes ( ) elif isinstance ( buf , buffer ) : obj . buffers [ i ] = bytes ( buf ) return buffers
6305	def runnable_effects ( self ) -> List [ Type [ Effect ] ] : return [ cls for cls in self . effect_classes if cls . runnable ]
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
3452	def find_essential_genes ( model , threshold = None , processes = None ) : if threshold is None : threshold = model . slim_optimize ( error_value = None ) * 1E-02 deletions = single_gene_deletion ( model , method = 'fba' , processes = processes ) essential = deletions . loc [ deletions [ 'growth' ] . isna ( ) | ( deletions [ 'growth' ] < threshold ) , : ] . index return { model . genes . get_by_id ( g ) for ids in essential for g in ids }
6150	def fir_remez_lpf ( f_pass , f_stop , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = lowpass_order ( f_pass , f_stop , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
7225	def delete ( self , project_id ) : self . logger . debug ( 'Deleting project by id: ' + project_id ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project_id } r = self . gbdx_connection . delete ( url ) r . raise_for_status ( )
2966	def _sm_to_pain ( self , * args , ** kwargs ) : _logger . info ( "Starting chaos for blockade %s" % self . _blockade_name ) self . _do_blockade_event ( ) millisec = random . randint ( self . _run_min_time , self . _run_max_time ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
3668	def K_value ( P = None , Psat = None , phi_l = None , phi_g = None , gamma = None , Poynting = 1 ) : r try : if gamma : if phi_l : return gamma * Psat * phi_l * Poynting / ( phi_g * P ) return gamma * Psat * Poynting / P elif phi_l : return phi_l / phi_g return Psat / P except TypeError : raise Exception ( 'Input must consist of one set from (P, Psat, phi_l, \phi_g, gamma), (P, Psat, gamma), (phi_l, phi_g), (P, Psat)' )
10271	def is_unweighted_source ( graph : BELGraph , node : BaseEntity , key : str ) -> bool : return graph . in_degree ( node ) == 0 and key not in graph . nodes [ node ]
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
1776	def AND ( cpu , dest , src ) : if src . size == 64 and src . type == 'immediate' and dest . size == 64 : arg1 = Operators . SEXTEND ( src . read ( ) , 32 , 64 ) else : arg1 = src . read ( ) res = dest . write ( dest . read ( ) & arg1 ) cpu . _calculate_logic_flags ( dest . size , res )
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
12981	def string ( html , start_on = None , ignore = ( ) , use_short = True , ** queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , ** queries )
13794	def handle_map_doc ( self , document ) : for function in sorted ( self . functions . values ( ) , key = lambda x : x [ 0 ] ) : try : yield [ list ( function ( document ) ) ] except Exception , exc : yield [ ] self . log ( repr ( exc ) )
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
9539	def number_range_inclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) < min or type ( v ) > max : raise ValueError ( v ) return checker
9799	def stop ( ctx , yes , pending ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not yes and not click . confirm ( "Are sure you want to stop experiments " "in group `{}`" . format ( _group ) ) : click . echo ( 'Existing without stopping experiments in group.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment_group . stop ( user , project_name , _group , pending = pending ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiments in group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments in group are being stopped." )
9673	def resolve ( self , context , quiet = True ) : try : obj = context for level in self . levels : if isinstance ( obj , dict ) : obj = obj [ level ] elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : obj = obj [ int ( level ) ] else : if callable ( getattr ( obj , level ) ) : try : obj = getattr ( obj , level ) ( ) except KeyError : obj = getattr ( obj , level ) else : display = 'get_%s_display' % level obj = getattr ( obj , display ) ( ) if hasattr ( obj , display ) else getattr ( obj , level ) if not obj : break return obj except Exception as e : if quiet : return '' else : raise e
5076	def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
9887	def _read_all_attribute_info ( self ) : num = copy . deepcopy ( self . _num_attrs ) fname = copy . deepcopy ( self . fname ) out = fortran_cdf . inquire_all_attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max_gentries = out [ 3 ] max_rentries = out [ 4 ] max_zentries = out [ 5 ] attr_nums = out [ 6 ] global_attrs_info = { } var_attrs_info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max_gentries , max_rentries , max_zentries , attr_nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max_gentry' ] = gentry nug [ 'max_rentry' ] = rentry nug [ 'max_zentry' ] = zentry nug [ 'attr_num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global_attrs_info [ name ] = nug elif scope == 2 : var_attrs_info [ name ] = nug self . global_attrs_info = global_attrs_info self . var_attrs_info = var_attrs_info else : raise IOError ( fortran_cdf . statusreporter ( status ) )
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
12668	def matrix_to_4dvolume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 2 : raise ValueError ( "X must be a 2-dimensional array" ) if mask . sum ( ) != arr . shape [ 0 ] : raise ValueError ( 'Expected arr of shape ({}, samples). Got {}.' . format ( mask . sum ( ) , arr . shape ) ) data = np . zeros ( mask . shape + ( arr . shape [ 1 ] , ) , dtype = arr . dtype , order = order ) data [ mask , : ] = arr return data
2303	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_gies ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
1420	def _get_scheduler_location_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_scheduler_location_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_scheduler_location ( data , stats ) : if data : scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) callback ( scheduler_location ) else : callback ( None ) return isWatching
917	def info ( self , msg , * args , ** kwargs ) : self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
2698	def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
873	def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
2116	def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : return file_obj . read ( ) return file_obj return value
13578	def update ( course = False ) : if course : with Spinner . context ( msg = "Updated course metadata." , waitmsg = "Updating course metadata." ) : for course in api . get_courses ( ) : old = None try : old = Course . get ( Course . tid == course [ "id" ] ) except peewee . DoesNotExist : old = None if old : old . details_url = course [ "details_url" ] old . save ( ) continue Course . create ( tid = course [ "id" ] , name = course [ "name" ] , details_url = course [ "details_url" ] ) else : selected = Course . get_selected ( ) print ( "Updating exercise data." ) for exercise in api . get_exercises ( selected ) : old = None try : old = Exercise . byid ( exercise [ "id" ] ) except peewee . DoesNotExist : old = None if old is not None : old . name = exercise [ "name" ] old . course = selected . id old . is_attempted = exercise [ "attempted" ] old . is_completed = exercise [ "completed" ] old . deadline = exercise . get ( "deadline" ) old . is_downloaded = os . path . isdir ( old . path ( ) ) old . return_url = exercise [ "return_url" ] old . zip_url = exercise [ "zip_url" ] old . submissions_url = exercise [ "exercise_submissions_url" ] old . save ( ) download_exercise ( old , update = True ) else : ex = Exercise . create ( tid = exercise [ "id" ] , name = exercise [ "name" ] , course = selected . id , is_attempted = exercise [ "attempted" ] , is_completed = exercise [ "completed" ] , deadline = exercise . get ( "deadline" ) , return_url = exercise [ "return_url" ] , zip_url = exercise [ "zip_url" ] , submissions_url = exercise [ ( "exercise_" "submissions_" "url" ) ] ) ex . is_downloaded = os . path . isdir ( ex . path ( ) ) ex . save ( )
2026	def BYTE ( self , offset , value ) : offset = Operators . ITEBV ( 256 , offset < 32 , ( 31 - offset ) * 8 , 256 ) return Operators . ZEXTEND ( Operators . EXTRACT ( value , offset , 8 ) , 256 )
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
12217	def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
3834	async def send_offnetwork_invitation ( self , send_offnetwork_invitation_request ) : response = hangouts_pb2 . SendOffnetworkInvitationResponse ( ) await self . _pb_request ( 'devices/sendoffnetworkinvitation' , send_offnetwork_invitation_request , response ) return response
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
13877	def CopyDirectory ( source_dir , target_dir , override = False ) : _AssertIsLocal ( source_dir ) _AssertIsLocal ( target_dir ) if override and IsDir ( target_dir ) : DeleteDirectory ( target_dir , skip_on_error = False ) import shutil shutil . copytree ( source_dir , target_dir )
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
4068	def update_items ( self , payload ) : to_send = [ self . check_items ( [ p ] ) [ 0 ] for p in payload ] headers = { } headers . update ( self . default_headers ( ) ) for chunk in chunks ( to_send , 50 ) : req = requests . post ( url = self . endpoint + "/{t}/{u}/items/" . format ( t = self . library_type , u = self . library_id ) , headers = headers , data = json . dumps ( chunk ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
10465	def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
8326	def setup ( self , parent = None , previous = None ) : self . parent = parent self . previous = previous self . next = None self . previousSibling = None self . nextSibling = None if self . parent and self . parent . contents : self . previousSibling = self . parent . contents [ - 1 ] self . previousSibling . nextSibling = self
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
2380	def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
710	def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath
5494	def expand_mentions ( text , embed_names = True ) : if embed_names : mention_format = "@<{name} {url}>" else : mention_format = "@<{url}>" def handle_mention ( match ) : source = get_source_by_name ( match . group ( 1 ) ) if source is None : return "@{0}" . format ( match . group ( 1 ) ) return mention_format . format ( name = source . nick , url = source . url ) return short_mention_re . sub ( handle_mention , text )
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
9301	def retrieve ( self , cursor ) : assert isinstance ( cursor , dict ) , "expected cursor type 'dict'" query = self . get_query ( ) assert isinstance ( query , peewee . Query ) query return query . get ( ** cursor )
6852	def configure ( self , reboot = 1 ) : r = self . local_renderer for ip , hostname in self . iter_hostnames ( ) : self . vprint ( 'ip/hostname:' , ip , hostname ) r . genv . host_string = ip r . env . hostname = hostname with settings ( warn_only = True ) : r . sudo ( 'echo "{hostname}" > /etc/hostname' ) r . sudo ( 'echo "127.0.0.1 {hostname}" | cat - /etc/hosts > /tmp/out && mv /tmp/out /etc/hosts' ) r . sudo ( r . env . set_hostname_command ) if r . env . auto_reboot and int ( reboot ) : r . reboot ( )
11332	def err ( format_msg , * args , ** kwargs ) : exc_info = kwargs . pop ( "exc_info" , False ) stderr . warning ( str ( format_msg ) . format ( * args , ** kwargs ) , exc_info = exc_info )
4911	def ensure_data_exists ( self , request , data , error_message = None ) : if not data : error_message = ( error_message or "Unable to fetch API response from endpoint '{}'." . format ( request . get_full_path ( ) ) ) LOGGER . error ( error_message ) raise NotFound ( error_message )
13117	def search ( self , number = None , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : if number : response = search [ 0 : number ] else : args , _ = self . core_parser . parse_known_args ( ) if args . number : response = search [ 0 : args . number ] else : response = search . scan ( ) return [ hit for hit in response ] except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) return [ ] except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" ) return [ ]
7936	def _set_state ( self , state ) : logger . debug ( " _set_state({0!r})" . format ( state ) ) self . _state = state self . _state_cond . notify ( )
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
3370	def interface_to_str ( interface ) : if isinstance ( interface , ModuleType ) : interface = interface . __name__ return re . sub ( r"optlang.|.interface" , "" , interface )
13772	def _default_json_default ( obj ) : if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time ) ) : return obj . strftime ( default_date_fmt ) else : return str ( obj )
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
7401	def down ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__gt = self . order ) )
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
2428	def set_doc_namespace ( self , doc , namespace ) : if not self . doc_namespace_set : self . doc_namespace_set = True if validations . validate_doc_namespace ( namespace ) : doc . namespace = namespace return True else : raise SPDXValueError ( 'Document::Namespace' ) else : raise CardinalityError ( 'Document::Comment' )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
3050	def from_stream ( credential_filename ) : if credential_filename and os . path . isfile ( credential_filename ) : try : return _get_application_default_credential_from_file ( credential_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : extra_help = ( ' (provided as parameter to the ' 'from_stream() method)' ) _raise_exception_for_reading_json ( credential_filename , extra_help , error ) else : raise ApplicationDefaultCredentialsError ( 'The parameter passed to the from_stream() ' 'method should point to a file.' )
2097	def cancel ( self , pk = None , fail_if_not_running = False , ** kwargs ) : if not pk : existing_data = self . get ( ** kwargs ) pk = existing_data [ 'id' ] cancel_endpoint = '%s%s/cancel/' % ( self . endpoint , pk ) try : client . post ( cancel_endpoint ) changed = True except exc . MethodNotAllowed : changed = False if fail_if_not_running : raise exc . TowerCLIError ( 'Job not running.' ) return { 'status' : 'canceled' , 'changed' : changed }
3997	def copy_between_containers ( source_name , source_path , dest_name , dest_path ) : if not container_path_exists ( source_name , source_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( source_path , source_name ) ) temp_path = os . path . join ( tempfile . mkdtemp ( ) , str ( uuid . uuid1 ( ) ) ) with _cleanup_path ( temp_path ) : copy_to_local ( temp_path , source_name , source_path , demote = False ) copy_from_local ( temp_path , dest_name , dest_path , demote = False )
12530	def open_volume_file ( filepath ) : if not op . exists ( filepath ) : raise IOError ( 'Could not find file {}.' . format ( filepath ) ) def open_nifti_file ( filepath ) : return NiftiImage ( filepath ) def open_mhd_file ( filepath ) : return MedicalImage ( filepath ) vol_data , hdr_data = load_raw_data_with_mhd ( filepath ) return vol_data , hdr_data def open_mha_file ( filepath ) : raise NotImplementedError ( 'This function has not been implemented yet.' ) def _load_file ( filepath , loader ) : return loader ( filepath ) filext_loader = { 'nii' : open_nifti_file , 'mhd' : open_mhd_file , 'mha' : open_mha_file , } ext = get_extension ( filepath ) loader = None for e in filext_loader : if ext in e : loader = filext_loader [ e ] if loader is None : raise ValueError ( 'Could not find a loader for file {}.' . format ( filepath ) ) return _load_file ( filepath , loader )
12491	def as_float_array ( X , copy = True , force_all_finite = True ) : if isinstance ( X , np . matrix ) or ( not isinstance ( X , np . ndarray ) and not sp . issparse ( X ) ) : return check_array ( X , [ 'csr' , 'csc' , 'coo' ] , dtype = np . float64 , copy = copy , force_all_finite = force_all_finite , ensure_2d = False ) elif sp . issparse ( X ) and X . dtype in [ np . float32 , np . float64 ] : return X . copy ( ) if copy else X elif X . dtype in [ np . float32 , np . float64 ] : return X . copy ( 'F' if X . flags [ 'F_CONTIGUOUS' ] else 'C' ) if copy else X else : return X . astype ( np . float32 if X . dtype == np . int32 else np . float64 )
1163	def wait ( self , timeout = None ) : with self . __cond : if not self . __flag : self . __cond . wait ( timeout ) return self . __flag
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) lenul = tots - ( lendr + lendl + lenur ) return lendr * lendl * lenur * lenul
2374	def append ( self , row ) : if len ( row ) == 0 : return if ( row [ 0 ] != "" and ( not row [ 0 ] . lstrip ( ) . startswith ( "#" ) ) ) : self . _children . append ( self . _childClass ( self . parent , row . linenumber , row [ 0 ] ) ) if len ( row . cells ) > 1 : row [ 0 ] = "" self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells ) elif len ( self . _children ) == 0 : self . comments . append ( row ) else : if len ( row . cells ) > 0 : self . _children [ - 1 ] . append ( row . linenumber , row . raw_text , row . cells )
6273	def load ( self , meta : ResourceDescription ) -> Any : self . _check_meta ( meta ) self . resolve_loader ( meta ) return meta . loader_cls ( meta ) . load ( )
2490	def create_file_node ( self , doc_file ) : file_node = URIRef ( 'http://www.spdx.org/files#{id}' . format ( id = str ( doc_file . spdx_id ) ) ) type_triple = ( file_node , RDF . type , self . spdx_namespace . File ) self . graph . add ( type_triple ) name_triple = ( file_node , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) self . graph . add ( name_triple ) if doc_file . has_optional_field ( 'comment' ) : comment_triple = ( file_node , RDFS . comment , Literal ( doc_file . comment ) ) self . graph . add ( comment_triple ) if doc_file . has_optional_field ( 'type' ) : ftype = self . spdx_namespace [ self . FILE_TYPES [ doc_file . type ] ] ftype_triple = ( file_node , self . spdx_namespace . fileType , ftype ) self . graph . add ( ftype_triple ) self . graph . add ( ( file_node , self . spdx_namespace . checksum , self . create_checksum_node ( doc_file . chk_sum ) ) ) conc_lic_node = self . license_or_special ( doc_file . conc_lics ) conc_lic_triple = ( file_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) license_info_nodes = map ( self . license_or_special , doc_file . licenses_in_file ) for lic in license_info_nodes : triple = ( file_node , self . spdx_namespace . licenseInfoInFile , lic ) self . graph . add ( triple ) if doc_file . has_optional_field ( 'license_comment' ) : comment_triple = ( file_node , self . spdx_namespace . licenseComments , Literal ( doc_file . license_comment ) ) self . graph . add ( comment_triple ) cr_text_node = self . to_special_value ( doc_file . copyright ) cr_text_triple = ( file_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) if doc_file . has_optional_field ( 'notice' ) : notice_triple = ( file_node , self . spdx_namespace . noticeText , doc_file . notice ) self . graph . add ( notice_triple ) contrib_nodes = map ( lambda c : Literal ( c ) , doc_file . contributors ) contrib_triples = [ ( file_node , self . spdx_namespace . fileContributor , node ) for node in contrib_nodes ] for triple in contrib_triples : self . graph . add ( triple ) return file_node
9109	def _create_encrypted_zip ( self , source = 'dirty' , fs_target_dir = None ) : backup_recipients = [ r for r in self . editors if checkRecipient ( self . gpg_context , r ) ] if not backup_recipients : self . status = u'500 no valid keys at all' return self . status fs_backup = join ( self . fs_path , '%s.zip' % source ) if fs_target_dir is None : fs_backup_pgp = join ( self . fs_path , '%s.zip.pgp' % source ) else : fs_backup_pgp = join ( fs_target_dir , '%s.zip.pgp' % self . drop_id ) fs_source = dict ( dirty = self . fs_dirty_attachments , clean = self . fs_cleansed_attachments ) with ZipFile ( fs_backup , 'w' , ZIP_STORED ) as backup : if exists ( join ( self . fs_path , 'message' ) ) : backup . write ( join ( self . fs_path , 'message' ) , arcname = 'message' ) for fs_attachment in fs_source [ source ] : backup . write ( fs_attachment , arcname = split ( fs_attachment ) [ - 1 ] ) with open ( fs_backup , "rb" ) as backup : self . gpg_context . encrypt_file ( backup , backup_recipients , always_trust = True , output = fs_backup_pgp ) remove ( fs_backup ) return fs_backup_pgp
1741	def add_inputs ( self , xs ) : states = [ ] cur = self for x in xs : cur = cur . add_input ( x ) states . append ( cur ) return states
7554	def store_all ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) i = 0 while i < self . params . nquartets : dat = np . array ( list ( itertools . islice ( qiter , self . _chunksize ) ) ) end = min ( self . params . nquartets , dat . shape [ 0 ] + i ) fillsets [ i : end ] = dat [ : end - i ] i += self . _chunksize print ( min ( i , self . params . nquartets ) )
1306	def GetConsoleTitle ( ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleTitleW ( values , MAX_PATH ) return values . value
10549	def get_results ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'result' , params = params ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
4372	def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
4601	def main ( ) : if not _curses : if os . name == 'nt' : raise ValueError ( 'curses is not supported under Windows' ) raise ValueError ( 'Your platform does not support curses.' ) try : driver = next ( iter ( Curses . DRIVERS ) ) except : raise ValueError ( 'No Curses driver in project' ) _curses . wrapper ( driver . run_in_curses )
7081	def send_email ( sender , subject , content , email_recipient_list , email_address_list , email_user = None , email_pass = None , email_server = None ) : if not email_user : email_user = EMAIL_USER if not email_pass : email_pass = EMAIL_PASSWORD if not email_server : email_server = EMAIL_SERVER if not email_server and email_user and email_pass : raise ValueError ( "no email server address and " "credentials available, can't continue" ) msg_text = EMAIL_TEMPLATE . format ( sender = sender , hostname = socket . gethostname ( ) , activity_time = '%sZ' % datetime . utcnow ( ) . isoformat ( ) , activity_report = content ) email_sender = '%s <%s>' % ( sender , EMAIL_USER ) email_recipients = [ ( '%s <%s>' % ( x , y ) ) for ( x , y ) in zip ( email_recipient_list , email_address_list ) ] email_msg = MIMEText ( msg_text ) email_msg [ 'From' ] = email_sender email_msg [ 'To' ] = ', ' . join ( email_recipients ) email_msg [ 'Message-Id' ] = make_msgid ( ) email_msg [ 'Subject' ] = '[%s on %s] %s' % ( sender , socket . gethostname ( ) , subject ) email_msg [ 'Date' ] = formatdate ( time . time ( ) ) try : server = smtplib . SMTP ( EMAIL_SERVER , 587 ) server_ehlo_response = server . ehlo ( ) if server . has_extn ( 'STARTTLS' ) : try : tls_start_response = server . starttls ( ) tls_ehlo_response = server . ehlo ( ) login_response = server . login ( EMAIL_USER , EMAIL_PASSWORD ) send_response = ( server . sendmail ( email_sender , email_address_list , email_msg . as_string ( ) ) ) except Exception as e : print ( 'script email sending failed with error: %s' % e ) send_response = None if send_response is not None : print ( 'script email sent successfully' ) quit_response = server . quit ( ) return True else : quit_response = server . quit ( ) return False else : print ( 'email server does not support STARTTLS,' ' bailing out...' ) quit_response = server . quit ( ) return False except Exception as e : print ( 'sending email failed with error: %s' % e ) returnval = False quit_response = server . quit ( ) return returnval
9261	def filter_wo_labels ( self , all_issues ) : issues_wo_labels = [ ] if not self . options . add_issues_wo_labels : for issue in all_issues : if not issue [ 'labels' ] : issues_wo_labels . append ( issue ) return issues_wo_labels
11468	def rm ( self , filename ) : try : self . _ftp . delete ( filename ) except error_perm : try : current_folder = self . _ftp . pwd ( ) self . cd ( filename ) except error_perm : print ( '550 Delete operation failed %s ' 'does not exist!' % ( filename , ) ) else : self . cd ( current_folder ) print ( '550 Delete operation failed %s ' 'is a folder. Use rmdir function ' 'to delete it.' % ( filename , ) )
4012	def configure_nfs_server ( ) : repos_for_export = get_all_repos ( active_only = True , include_specs_repo = False ) current_exports = _get_current_exports ( ) needed_exports = _get_exports_for_repos ( repos_for_export ) _ensure_managed_repos_dir_exists ( ) if not needed_exports . difference ( current_exports ) : if not _server_is_running ( ) : _restart_server ( ) return _write_exports_config ( needed_exports ) _restart_server ( )
10113	def filter_rows_as_dict ( fname , filter_ , ** kw ) : filter_ = DictFilter ( filter_ ) rewrite ( fname , filter_ , ** kw ) return filter_ . removed
5399	def get_filtered_normalized_events ( self ) : user_image = google_v2_operations . get_action_image ( self . _op , _ACTION_USER_COMMAND ) need_ok = google_v2_operations . is_success ( self . _op ) events = { } for event in google_v2_operations . get_events ( self . _op ) : if self . _filter ( event ) : continue mapped , match = self . _map ( event ) name = mapped [ 'name' ] if name == 'ok' : if not need_ok or 'ok' in events : continue if name == 'pulling-image' : if match . group ( 1 ) != user_image : continue events [ name ] = mapped return sorted ( events . values ( ) , key = operator . itemgetter ( 'start-time' ) )
9685	def read_firmware ( self ) : self . cnxn . xfer ( [ 0x12 ] ) sleep ( 10e-3 ) self . firmware [ 'major' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] self . firmware [ 'minor' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] self . firmware [ 'version' ] = float ( '{}.{}' . format ( self . firmware [ 'major' ] , self . firmware [ 'minor' ] ) ) sleep ( 0.1 ) return self . firmware
583	def _addRecordToKNN ( self , record ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
9028	def instructions ( self ) : x = self . x y = self . y result = [ ] for instruction in self . _row . instructions : instruction_in_grid = InstructionInGrid ( instruction , Point ( x , y ) ) x += instruction_in_grid . width result . append ( instruction_in_grid ) return result
2546	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True doc . annotations [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'AnnotationComment' ) else : raise OrderError ( 'AnnotationComment' )
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
12377	def make_response ( self , data = None ) : if data is not None : data = self . prepare ( data ) self . response . write ( data , serialize = True )
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
5248	def custom_req ( session , request ) : while ( session . tryNextEvent ( ) ) : pass print ( "Sending Request:\n %s" % request ) session . sendRequest ( request ) messages = [ ] while ( True ) : ev = session . nextEvent ( 500 ) for msg in ev : print ( "Message Received:\n %s" % msg ) messages . append ( msg ) if ev . eventType ( ) == blpapi . Event . RESPONSE : break return messages
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
592	def _compute ( self , inputs , outputs ) : if self . _sfdr is None : raise RuntimeError ( "Spatial pooler has not been initialized" ) if not self . topDownMode : self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 rfOutput = self . _doBottomUpCompute ( rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , resetSignal = resetSignal ) outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat else : topDownIn = inputs . get ( 'topDownIn' , None ) spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut if temporalTopDownOut is not None : outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut outputs [ 'anomalyScore' ] [ : ] = 0
3497	def total_components_flux ( flux , components , consumption = True ) : direction = 1 if consumption else - 1 c_flux = [ elem * flux * direction for elem in components ] return sum ( [ flux for flux in c_flux if flux > 0 ] )
8888	def fit ( self , X ) : X = iter2array ( X , dtype = ReactionContainer ) self . _train_signatures = { self . __get_signature ( x ) for x in X } return self
11430	def record_order_subfields ( rec , tag = None ) : if rec is None : return rec if tag is None : tags = rec . keys ( ) for tag in tags : record_order_subfields ( rec , tag ) elif tag in rec : for i in xrange ( len ( rec [ tag ] ) ) : field = rec [ tag ] [ i ] ordered_subfields = sorted ( field [ 0 ] , key = lambda subfield : subfield [ 0 ] ) rec [ tag ] [ i ] = ( ordered_subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] , field [ 4 ] )
4125	def data_two_freqs ( N = 200 ) : nn = arange ( N ) xx = cos ( 0.257 * pi * nn ) + sin ( 0.2 * pi * nn ) + 0.01 * randn ( nn . size ) return xx
4727	def s20_to_gen ( self , pugrp , punit , chunk , sectr ) : cmd = [ "nvm_addr s20_to_gen" , self . envs [ "DEV_PATH" ] , "%d %d %d %d" % ( pugrp , punit , chunk , sectr ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.s20_to_gen: cmd fail" ) return int ( re . findall ( r"val: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
4015	def consume ( consumer_id ) : global _consumers consumer = _consumers [ consumer_id ] client = get_docker_client ( ) try : status = client . inspect_container ( consumer . container_id ) [ 'State' ] [ 'Status' ] except Exception as e : status = 'unknown' new_logs = client . logs ( consumer . container_id , stdout = True , stderr = True , stream = False , timestamps = False , since = calendar . timegm ( consumer . offset . timetuple ( ) ) ) updated_consumer = Consumer ( consumer . container_id , datetime . utcnow ( ) ) _consumers [ str ( consumer_id ) ] = updated_consumer response = jsonify ( { 'logs' : new_logs , 'status' : status } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) raise e self . _updateType ( )
2606	def check_memo ( self , task_id , task ) : if not self . memoize or not task [ 'memoize' ] : task [ 'hashsum' ] = None return None , None hashsum = self . make_hash ( task ) present = False result = None if hashsum in self . memo_lookup_table : present = True result = self . memo_lookup_table [ hashsum ] logger . info ( "Task %s using result from cache" , task_id ) task [ 'hashsum' ] = hashsum return present , result
1958	def _open ( self , f ) : if None in self . files : fd = self . files . index ( None ) self . files [ fd ] = f else : fd = len ( self . files ) self . files . append ( f ) return fd
11632	def codepointsInNamelist ( namFilename , unique_glyphs = False , cache = None ) : key = 'charset' if not unique_glyphs else 'ownCharset' internals_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) target = os . path . join ( internals_dir , namFilename ) result = readNamelist ( target , unique_glyphs , cache ) return result [ key ]
11612	def report_read_counts ( self , filename , grp_wise = False , reorder = 'as-is' , notes = None ) : expected_read_counts = self . probability . sum ( axis = APM . Axis . READ ) if grp_wise : lname = self . probability . gname expected_read_counts = expected_read_counts * self . grp_conv_mat else : lname = self . probability . lname total_read_counts = expected_read_counts . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_read_counts . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( expected_read_counts , total_read_counts ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
11889	def get_lights ( self ) : now = datetime . datetime . now ( ) if ( now - self . _last_updated ) < datetime . timedelta ( seconds = UPDATE_INTERVAL_SECONDS ) : return self . _bulbs else : self . _last_updated = now light_data = self . get_data ( ) _LOGGER . debug ( "got: %s" , light_data ) if not light_data : return [ ] if self . _bulbs : for bulb in self . _bulbs : try : values = light_data [ bulb . zid ] bulb . _online , bulb . _red , bulb . _green , bulb . _blue , bulb . _level = values except KeyError : pass else : for light_id in light_data : self . _bulbs . append ( Bulb ( self , light_id , * light_data [ light_id ] ) ) return self . _bulbs
3093	def _create_flow ( self , request_handler ) : if self . flow is None : redirect_uri = request_handler . request . relative_url ( self . _callback_path ) self . flow = client . OAuth2WebServerFlow ( self . _client_id , self . _client_secret , self . _scope , redirect_uri = redirect_uri , user_agent = self . _user_agent , auth_uri = self . _auth_uri , token_uri = self . _token_uri , revoke_uri = self . _revoke_uri , ** self . _kwargs )
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
13142	def recover_triples_from_mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
7915	def list_all ( cls , basic = None ) : if basic is None : return [ s for s in cls . _defs ] else : return [ s . name for s in cls . _defs . values ( ) if s . basic == basic ]
2247	def _make_signature_key ( args , kwargs ) : kwitems = kwargs . items ( ) if ( sys . version_info . major , sys . version_info . minor ) < ( 3 , 7 ) : kwitems = sorted ( kwitems ) kwitems = tuple ( kwitems ) try : key = _hashable ( args ) , _hashable ( kwitems ) except TypeError : raise TypeError ( 'Signature is not hashable: args={} kwargs{}' . format ( args , kwargs ) ) return key
2357	def is_element_displayed ( self , strategy , locator ) : return self . driver_adapter . is_element_displayed ( strategy , locator , root = self . root )
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
2590	def stage_in ( self , file , executor ) : if file . scheme == 'ftp' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _ftp_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'http' or file . scheme == 'https' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _http_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_in_app = self . _globus_stage_in_app ( ) app_fut = stage_in_app ( globus_ep , outputs = [ file ] ) return app_fut . _outputs [ 0 ] else : raise Exception ( 'Staging in with unknown file scheme {} is not supported' . format ( file . scheme ) )
5862	def validate ( self , ml_template ) : data = { "ml_template" : ml_template } failure_message = "ML template validation invoke failed" res = self . _get_success_json ( self . _post_json ( 'ml_templates/validate' , data , failure_message = failure_message ) ) [ 'data' ] if res [ 'valid' ] : return 'OK' return res [ 'reason' ]
12933	def as_dict ( self , * args , ** kwargs ) : self_as_dict = super ( ClinVarAllele , self ) . as_dict ( * args , ** kwargs ) self_as_dict [ 'hgvs' ] = self . hgvs self_as_dict [ 'clnalleleid' ] = self . clnalleleid self_as_dict [ 'clnsig' ] = self . clnsig self_as_dict [ 'clndn' ] = self . clndn self_as_dict [ 'clndisdb' ] = self . clndisdb self_as_dict [ 'clnvi' ] = self . clnvi return self_as_dict
8604	def update_user ( self , user_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/users/%s' % user_id , method = 'PUT' , data = json . dumps ( data ) ) return response
12874	def satisfies ( guard ) : i = peek ( ) if ( i is EndOfFile ) or ( not guard ( i ) ) : fail ( [ "<satisfies predicate " + _fun_to_str ( guard ) + ">" ] ) next ( ) return i
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
3628	def pad_cells ( table ) : col_sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell_num , cell in enumerate ( row ) : row [ cell_num ] = pad_to ( cell , col_sizes [ cell_num ] ) return table
6129	def build_sdist ( sdist_directory , config_settings ) : backend = _build_backend ( ) try : return backend . build_sdist ( sdist_directory , config_settings ) except getattr ( backend , 'UnsupportedOperation' , _DummyException ) : raise GotUnsupportedOperation ( traceback . format_exc ( ) )
6067	def convergence_from_grid ( self , grid ) : surface_density_grid = np . zeros ( grid . shape [ 0 ] ) grid_eta = self . grid_to_elliptical_radii ( grid ) for i in range ( grid . shape [ 0 ] ) : surface_density_grid [ i ] = self . convergence_func ( grid_eta [ i ] ) return surface_density_grid
7311	def is_valid_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return True except ValueError as e : return False
11544	def analog_reference ( self , pin = None ) : if pin is None : return self . _analog_reference ( None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _analog_reference ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
2400	def get_good_pos_ngrams ( self ) : if ( os . path . isfile ( NGRAM_PATH ) ) : good_pos_ngrams = pickle . load ( open ( NGRAM_PATH , 'rb' ) ) elif os . path . isfile ( ESSAY_CORPUS_PATH ) : essay_corpus = open ( ESSAY_CORPUS_PATH ) . read ( ) essay_corpus = util_functions . sub_chars ( essay_corpus ) good_pos_ngrams = util_functions . regenerate_good_tokens ( essay_corpus ) pickle . dump ( good_pos_ngrams , open ( NGRAM_PATH , 'wb' ) ) else : good_pos_ngrams = [ 'NN PRP' , 'NN PRP .' , 'NN PRP . DT' , 'PRP .' , 'PRP . DT' , 'PRP . DT NNP' , '. DT' , '. DT NNP' , '. DT NNP NNP' , 'DT NNP' , 'DT NNP NNP' , 'DT NNP NNP NNP' , 'NNP NNP' , 'NNP NNP NNP' , 'NNP NNP NNP NNP' , 'NNP NNP NNP .' , 'NNP NNP .' , 'NNP NNP . TO' , 'NNP .' , 'NNP . TO' , 'NNP . TO NNP' , '. TO' , '. TO NNP' , '. TO NNP NNP' , 'TO NNP' , 'TO NNP NNP' ] return set ( good_pos_ngrams )
12728	def axes ( self , axes ) : self . lmotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
6552	def check ( self , solution ) : return self . func ( * ( solution [ v ] for v in self . variables ) )
1753	def get_argument_values ( self , model , prefix_args ) : spec = inspect . getfullargspec ( model ) if spec . varargs : logger . warning ( "ABI: A vararg model must be a unary function." ) nargs = len ( spec . args ) - len ( prefix_args ) if inspect . ismethod ( model ) : nargs -= 1 def resolve_argument ( arg ) : if isinstance ( arg , str ) : return self . _cpu . read_register ( arg ) else : return self . _cpu . read_int ( arg ) descriptors = self . get_arguments ( ) argument_iter = map ( resolve_argument , descriptors ) from . . models import isvariadic if isvariadic ( model ) : arguments = prefix_args + ( argument_iter , ) else : arguments = prefix_args + tuple ( islice ( argument_iter , nargs ) ) return arguments
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
4594	def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
2402	def gen_bag_feats ( self , e_set ) : if ( hasattr ( self , '_stem_dict' ) ) : sfeats = self . _stem_dict . transform ( e_set . _clean_stem_text ) nfeats = self . _normal_dict . transform ( e_set . _text ) bag_feats = numpy . concatenate ( ( sfeats . toarray ( ) , nfeats . toarray ( ) ) , axis = 1 ) else : raise util_functions . InputError ( self , "Dictionaries must be initialized prior to generating bag features." ) return bag_feats . copy ( )
9184	def _node_to_model ( tree_or_item , metadata = None , parent = None , lucent_id = cnxepub . TRANSLUCENT_BINDER_ID ) : if 'contents' in tree_or_item : tree = tree_or_item binder = cnxepub . TranslucentBinder ( metadata = tree ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder , lucent_id = lucent_id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) result = binder else : item = tree_or_item result = cnxepub . DocumentPointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
11934	def auto_widget ( field ) : info = { 'widget' : field . field . widget . __class__ . __name__ , 'field' : field . field . __class__ . __name__ , 'name' : field . name , } return [ fmt . format ( ** info ) for fmt in ( '{field}_{widget}_{name}' , '{field}_{name}' , '{widget}_{name}' , '{field}_{widget}' , '{name}' , '{widget}' , '{field}' , ) ]
2471	def set_file_atrificat_of_project ( self , doc , symbol , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_artifact ( symbol , value ) else : raise OrderError ( 'File::Artificat' )
4003	def registry_from_image ( image_name ) : if '/' not in image_name : return constants . PUBLIC_DOCKER_REGISTRY prefix = image_name . split ( '/' ) [ 0 ] if '.' not in prefix : return constants . PUBLIC_DOCKER_REGISTRY return prefix
8	def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : placeholder = observation_placeholder ( ob_space , batch_size , name ) return placeholder , encode_observation ( ob_space , placeholder )
10398	def score_leaves ( self ) -> Set [ BaseEntity ] : leaves = set ( self . iter_leaves ( ) ) if not leaves : log . warning ( 'no leaves.' ) return set ( ) for leaf in leaves : self . graph . nodes [ leaf ] [ self . tag ] = self . calculate_score ( leaf ) log . log ( 5 , 'chomping %s' , leaf ) return leaves
12856	def merge_text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
13636	def contentEncoding ( requestHeaders , encoding = None ) : if encoding is None : encoding = b'utf-8' headers = _splitHeaders ( requestHeaders . getRawHeaders ( b'Content-Type' , [ ] ) ) if headers : return headers [ 0 ] [ 1 ] . get ( b'charset' , encoding ) return encoding
5145	def _merge_config ( self , config , templates ) : if not templates : return config if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
368	def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] else : h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg return x [ h_offset : h_end , w_offset : w_end ]
9634	def numeric ( _ , n ) : try : nt = n . as_tuple ( ) except AttributeError : raise TypeError ( 'numeric field requires Decimal value (got %r)' % n ) digits = [ ] if isinstance ( nt . exponent , str ) : ndigits = 0 weight = 0 sign = 0xC000 dscale = 0 else : decdigits = list ( reversed ( nt . digits + ( nt . exponent % 4 ) * ( 0 , ) ) ) weight = 0 while decdigits : if any ( decdigits [ : 4 ] ) : break weight += 1 del decdigits [ : 4 ] while decdigits : digits . insert ( 0 , ndig ( decdigits [ : 4 ] ) ) del decdigits [ : 4 ] ndigits = len ( digits ) weight += nt . exponent // 4 + ndigits - 1 sign = nt . sign * 0x4000 dscale = - min ( 0 , nt . exponent ) data = [ ndigits , weight , sign , dscale ] + digits return ( 'ihhHH%dH' % ndigits , [ 2 * len ( data ) ] + data )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
1493	def _get_next_timeout_interval ( self ) : if len ( self . timer_tasks ) == 0 : return sys . maxsize else : next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) return next_timeout_interval
8479	def potential ( self , value ) : if value : self . _potential = True else : self . _potential = False
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
4538	def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote_plus ( address ) try : error = NO_PROJECT_ERROR if not self . project : raise ValueError error = BAD_ADDRESS_ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD_GETTER_ERROR result = method ( self , ed ) else : error = BAD_SETTER_ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print_exc ( ) msg = '%s\n%s' % ( error . format ( ** locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
6342	def docs_of_words ( self ) : r return [ [ words for sents in doc for words in sents ] for doc in self . corpus ]
10101	def create_snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
3916	def _get_date_str ( timestamp , datetimefmt , show_date = False ) : fmt = '' if show_date : fmt += '\n' + datetimefmt . get ( 'date' , '' ) + '\n' fmt += datetimefmt . get ( 'time' , '' ) return timestamp . astimezone ( tz = None ) . strftime ( fmt )
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
6245	def set_time ( self , value : float ) : if value < 0 : value = 0 self . controller . row = self . rps * value
4251	def country_name_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . country_name_by_addr ( addr )
4882	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . get_or_create ( name = 'SAP_USE_ENTERPRISE_ENROLLMENT_PAGE' , defaults = { 'active' : False } )
12103	def summary ( self ) : print ( "Type: %s" % self . __class__ . __name__ ) print ( "Batch Name: %r" % self . batch_name ) if self . tag : print ( "Tag: %s" % self . tag ) print ( "Root directory: %r" % self . get_root_directory ( ) ) print ( "Maximum concurrency: %s" % self . max_concurrency ) if self . description : print ( "Description: %s" % self . description )
6477	def render ( self , stream ) : encoding = self . option . encoding or self . term . encoding or "utf8" if self . option . color : ramp = self . color_ramp ( self . size . y ) [ : : - 1 ] else : ramp = None if self . cycle >= 1 and self . lines : stream . write ( self . term . csi ( 'cuu' , self . lines ) ) zero = int ( self . null / 4 ) lines = 0 for y in range ( self . screen . size . y ) : if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) for x in range ( self . screen . size . x ) : point = Point ( ( x , y ) ) if point in self . screen : value = self . screen [ point ] if isinstance ( value , int ) : stream . write ( chr ( self . base + value ) . encode ( encoding ) ) else : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( self . term . csi_wrap ( value . encode ( encoding ) , 'bold' ) ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'smul' ) ) if ramp : stream . write ( ramp [ y ] ) else : stream . write ( b' ' ) if y == zero and self . size . y > 1 : stream . write ( self . term . csi ( 'rmul' ) ) if ramp : stream . write ( self . term . csi ( 'sgr0' ) ) stream . write ( b'\n' ) lines += 1 stream . flush ( ) self . cycle = self . cycle + 1 self . lines = lines
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
10307	def calculate_global_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : r universe = set ( itt . chain . from_iterable ( dict_of_sets . values ( ) ) ) universe_size = len ( universe ) result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = 1.0 - len ( dict_of_sets [ x ] | dict_of_sets [ y ] ) / universe_size for x in dict_of_sets : result [ x ] [ x ] = 1.0 - len ( x ) / universe_size return dict ( result )
9265	def fetch_and_filter_tags ( self ) : self . all_tags = self . fetcher . get_all_tags ( ) self . filtered_tags = self . get_filtered_tags ( self . all_tags ) self . fetch_tags_dates ( )
10617	def get_compound_mass ( self , compound ) : if compound in self . material . compounds : return self . _compound_masses [ self . material . get_compound_index ( compound ) ] else : return 0.0
12729	def axes ( self , axes ) : self . amotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
6678	def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
3634	def search ( self , ctype , level = None , category = None , assetId = None , defId = None , min_price = None , max_price = None , min_buy = None , max_buy = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None , start = 0 , page_size = itemsPerPage [ 'transferMarket' ] , fast = False ) : method = 'GET' url = 'transfermarket' if start == 0 : events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer Market Search' ) ] self . pin . send ( events , fast = fast ) params = { 'start' : start , 'num' : page_size , 'type' : ctype , } if level : params [ 'lev' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if defId : params [ 'definitionId' ] = defId if min_price : params [ 'micr' ] = min_price if max_price : params [ 'macr' ] = max_price if min_buy : params [ 'minb' ] = min_buy if max_buy : params [ 'maxb' ] = max_buy if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params , fast = fast ) if start == 0 : events = [ self . pin . event ( 'page_view' , 'Transfer Market Results - List View' ) , self . pin . event ( 'page_view' , 'Item - Detail View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
1781	def AAM ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AH = Operators . UDIV ( cpu . AL , imm ) cpu . AL = Operators . UREM ( cpu . AL , imm ) cpu . _calculate_logic_flags ( 8 , cpu . AL )
3622	def __post_save_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE post_save FOR %s' , instance . __class__ ) self . save_record ( instance , ** kwargs )
10607	def run ( self ) : self . prepare_to_run ( ) for i in range ( 0 , self . period_count ) : for e in self . entities : e . run ( self . clock ) self . clock . tick ( )
1953	def input_from_cons ( constupl , datas ) : ' solve bytes in |datas| based on ' def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
4390	def adsSyncReadStateReqEx ( port , address ) : sync_read_state_request = _adsDLL . AdsSyncReadStateReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state = ctypes . c_int ( ) ads_state_pointer = ctypes . pointer ( ads_state ) device_state = ctypes . c_int ( ) device_state_pointer = ctypes . pointer ( device_state ) error_code = sync_read_state_request ( port , ams_address_pointer , ads_state_pointer , device_state_pointer ) if error_code : raise ADSError ( error_code ) return ( ads_state . value , device_state . value )
3773	def set_user_methods ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . T_cached = None
3682	def logP ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in CRClogPDict . index : methods . append ( CRC ) if CASRN in SyrresDict2 . index : methods . append ( SYRRES ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : return float ( CRClogPDict . at [ CASRN , 'logP' ] ) elif Method == SYRRES : return float ( SyrresDict2 . at [ CASRN , 'logP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
10122	def _kwargs ( self ) : return dict ( color = self . color , velocity = self . velocity , colors = self . colors )
5006	def handle_enterprise_logistration ( backend , user , ** kwargs ) : request = backend . strategy . request enterprise_customer = get_enterprise_customer_for_running_pipeline ( request , { 'backend' : backend . name , 'kwargs' : kwargs } ) if enterprise_customer is None : return enterprise_customer_user , _ = EnterpriseCustomerUser . objects . update_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enterprise_customer_user . update_session ( request )
11213	def decode ( secret : Union [ str , bytes ] , token : Union [ str , bytes ] , alg : str = default_alg ) -> Tuple [ dict , dict ] : secret = util . to_bytes ( secret ) token = util . to_bytes ( token ) pre_signature , signature_segment = token . rsplit ( b'.' , 1 ) header_b64 , payload_b64 = pre_signature . split ( b'.' ) try : header_json = util . b64_decode ( header_b64 ) header = json . loads ( util . from_bytes ( header_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidHeaderError ( 'Invalid header' ) try : payload_json = util . b64_decode ( payload_b64 ) payload = json . loads ( util . from_bytes ( payload_json ) ) except ( json . decoder . JSONDecodeError , UnicodeDecodeError , ValueError ) : raise InvalidPayloadError ( 'Invalid payload' ) if not isinstance ( header , dict ) : raise InvalidHeaderError ( 'Invalid header: {}' . format ( header ) ) if not isinstance ( payload , dict ) : raise InvalidPayloadError ( 'Invalid payload: {}' . format ( payload ) ) signature = util . b64_decode ( signature_segment ) calculated_signature = _hash ( secret , pre_signature , alg ) if not compare_signature ( signature , calculated_signature ) : raise InvalidSignatureError ( 'Invalid signature' ) return header , payload
12857	def with_peer ( events ) : stack = [ ] for obj in events : if obj [ 'type' ] == ENTER : stack . append ( obj ) yield obj , None elif obj [ 'type' ] == EXIT : yield obj , stack . pop ( ) else : yield obj , None
208	def draw_on_image ( self , image , alpha = 0.75 , cmap = "jet" , resize = "heatmaps" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "heatmaps" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = "cubic" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == "heatmaps" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix
11980	def set_ip ( self , ip ) : self . set ( ip = ip , netmask = self . _nm )
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
10593	def get_date ( date ) : if type ( date ) is str : return datetime . strptime ( date , '%Y-%m-%d' ) . date ( ) else : return date
1864	def SARX ( cpu , dest , src , count ) : OperandSize = dest . size count = count . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = count & countMask tempDest = value = src . read ( ) sign = value & ( 1 << ( OperandSize - 1 ) ) while tempCount != 0 : cpu . CF = ( value & 0x1 ) != 0 value = ( value >> 1 ) | sign tempCount = tempCount - 1 res = dest . write ( value )
410	def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] inputs = self . _to_bc_h_w ( inputs , input_shape ) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) coords = tf . expand_dims ( grid_offset , 0 ) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals
6311	def from_single ( cls , meta : ProgramDescription , source : str ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , source ) if GEOMETRY_SHADER in source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , source , ) if FRAGMENT_SHADER in source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , source , ) if TESS_CONTROL_SHADER in source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , source , ) if TESS_EVALUATION_SHADER in source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_evaluation_shader , source , ) return instance
17	def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )
3635	def bid ( self , trade_id , bid , fast = False ) : method = 'PUT' url = 'trade/%s/bid' % trade_id if not fast : rc = self . tradeStatus ( trade_id ) [ 0 ] if rc [ 'currentBid' ] >= bid or self . credits < bid : return False data = { 'bid' : bid } try : rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } , fast = fast ) [ 'auctionInfo' ] [ 0 ] except PermissionDenied : return False if rc [ 'bidState' ] == 'highest' or ( rc [ 'tradeState' ] == 'closed' and rc [ 'bidState' ] == 'buyNow' ) : return True else : return False
12684	def query ( self , input = '' , params = { } ) : payload = { 'input' : input , 'appid' : self . appid } for key , value in params . items ( ) : if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
1015	def _getCellForNewSegment ( self , colIdx ) : if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) if len ( candidateCellIdxs ) > 0 : candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print "Cell [%d,%d] chosen for new segment, # of segs is %d" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx candidateSegment = None candidateSegmentDC = 1.0 for i in xrange ( minIdx , maxIdx + 1 ) : for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s if self . verbosity >= 5 : print ( "Deleting segment #%d for cell[%d,%d] to make room for new " "segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx
9595	def execute_async_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_ASYNC_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
9802	def get ( keys ) : _config = GlobalConfigManager . get_config_or_default ( ) if not keys : return print_values = { } for key in keys : if hasattr ( _config , key ) : print_values [ key ] = getattr ( _config , key ) else : click . echo ( 'Key `{}` is not recognised.' . format ( key ) ) dict_tabulate ( print_values , )
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
9829	def edges ( self ) : return [ self . delta [ d , d ] * numpy . arange ( self . shape [ d ] + 1 ) + self . origin [ d ] - 0.5 * self . delta [ d , d ] for d in range ( self . rank ) ]
4691	def decode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Encryption " raw = bytes ( message , "ascii" ) cleartext = aes . decrypt ( unhexlify ( raw ) ) " Checksum " checksum = cleartext [ 0 : 4 ] message = cleartext [ 4 : ] message = _unpad ( message , 16 ) " Verify checksum " check = hashlib . sha256 ( message ) . digest ( ) [ 0 : 4 ] if check != checksum : raise ValueError ( "checksum verification failure" ) return message . decode ( "utf8" )
2205	def userhome ( username = None ) : if username is None : if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : if sys . platform . startswith ( 'win32' ) : if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : if sys . platform . startswith ( 'win32' ) : c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
4035	def cleanwrap ( func ) : def enc ( self , * args , ** kwargs ) : return ( func ( self , item , ** kwargs ) for item in args ) return enc
12492	def indexable ( * iterables ) : result = [ ] for X in iterables : if sp . issparse ( X ) : result . append ( X . tocsr ( ) ) elif hasattr ( X , "__getitem__" ) or hasattr ( X , "iloc" ) : result . append ( X ) elif X is None : result . append ( X ) else : result . append ( np . array ( X ) ) check_consistent_length ( * result ) return result
10582	def set_parent_path ( self , value ) : self . _parent_path = value self . path = value + r'/' + self . name self . _update_childrens_parent_path ( )
9646	def get_attributes ( var ) : is_valid = partial ( is_valid_in_template , var ) return list ( filter ( is_valid , dir ( var ) ) )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
4613	def blocks ( self , start = None , stop = None ) : self . block_interval = self . get_block_interval ( ) if not start : start = self . get_current_block_num ( ) while True : if stop : head_block = stop else : head_block = self . get_current_block_num ( ) for blocknum in range ( start , head_block + 1 ) : block = self . wait_for_and_get_block ( blocknum ) block . update ( { "block_num" : blocknum } ) yield block start = head_block + 1 if stop and start > stop : return time . sleep ( self . block_interval )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
1337	def softmax ( logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) return e / np . sum ( e )
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
4670	def setKeys ( self , loadkeys ) : log . debug ( "Force setting of private keys. Not using the wallet database!" ) if isinstance ( loadkeys , dict ) : loadkeys = list ( loadkeys . values ( ) ) elif not isinstance ( loadkeys , ( list , set ) ) : loadkeys = [ loadkeys ] for wif in loadkeys : pub = self . publickey_from_wif ( wif ) self . store . add ( str ( wif ) , pub )
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
5492	def write_config ( self ) : with open ( self . config_file , "w" ) as config_file : self . cfg . write ( config_file )
6536	def output_error ( msg ) : click . echo ( click . style ( msg , fg = 'red' ) , err = True )
9368	def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) alias = self . _field [ 1 : ] self . _alias = alias return self
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
8284	def _segment_lengths ( self , relative = False , n = 20 ) : lengths = [ ] first = True for el in self . _get_elements ( ) : if first is True : close_x , close_y = el . x , el . y first = False elif el . cmd == MOVETO : close_x , close_y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . _linelength ( x0 , y0 , close_x , close_y ) ) elif el . cmd == LINETO : lengths . append ( self . _linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y lengths . append ( self . _curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : return map ( lambda l : l / length , lengths ) except ZeroDivisionError : return [ 0.0 ] * len ( lengths ) else : return lengths
9944	def link_file ( self , path , prefixed_path , source_storage ) : if prefixed_path in self . symlinked_files : return self . log ( "Skipping '%s' (already linked earlier)" % path ) if not self . delete_file ( path , prefixed_path , source_storage ) : return source_path = source_storage . path ( path ) if self . dry_run : self . log ( "Pretending to link '%s'" % source_path , level = 1 ) else : self . log ( "Linking '%s'" % source_path , level = 1 ) full_path = self . storage . path ( prefixed_path ) try : os . makedirs ( os . path . dirname ( full_path ) ) except OSError : pass try : if os . path . lexists ( full_path ) : os . unlink ( full_path ) os . symlink ( source_path , full_path ) except AttributeError : import platform raise CommandError ( "Symlinking is not supported by Python %s." % platform . python_version ( ) ) except NotImplementedError : import platform raise CommandError ( "Symlinking is not supported in this " "platform (%s)." % platform . platform ( ) ) except OSError as e : raise CommandError ( e ) if prefixed_path not in self . symlinked_files : self . symlinked_files . append ( prefixed_path )
6778	def get_component_order ( component_names ) : assert isinstance ( component_names , ( tuple , list ) ) component_dependences = { } for _name in component_names : deps = set ( manifest_deployers_befores . get ( _name , [ ] ) ) deps = deps . intersection ( component_names ) component_dependences [ _name ] = deps component_order = list ( topological_sort ( component_dependences . items ( ) ) ) return component_order
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
11957	def is_oct ( ip ) : try : dec = int ( str ( ip ) , 8 ) except ( TypeError , ValueError ) : return False if dec > 0o37777777777 or dec < 0 : return False return True
1736	def parse_num ( source , start , charset ) : while start < len ( source ) and source [ start ] in charset : start += 1 return start
7675	def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , ** kwargs ) string = re . sub ( r'[{}"]' , '' , string ) string = re . sub ( r',\n' , '\n' , string ) string = re . sub ( r'^\s*$' , '' , string ) return string
6507	def excerpt ( self ) : if "content" not in self . _results_fields : return None match_phrases = [ self . _match_phrase ] if six . PY2 : separate_phrases = [ phrase . decode ( 'utf-8' ) for phrase in shlex . split ( self . _match_phrase . encode ( 'utf-8' ) ) ] else : separate_phrases = [ phrase for phrase in shlex . split ( self . _match_phrase ) ] if len ( separate_phrases ) > 1 : match_phrases . extend ( separate_phrases ) else : match_phrases = separate_phrases matches = SearchResultProcessor . find_matches ( SearchResultProcessor . strings_in_dictionary ( self . _results_fields [ "content" ] ) , match_phrases , DESIRED_EXCERPT_LENGTH ) excerpt_text = ELLIPSIS . join ( matches ) for match_word in match_phrases : excerpt_text = SearchResultProcessor . decorate_matches ( excerpt_text , match_word ) return excerpt_text
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : fmt = '' if l > 1 : fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : if num_bytes < 4 : fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) fd . write ( struct . pack ( fmt , * data ) )
2431	def build_tool ( self , doc , entity ) : match = self . tool_re . match ( entity ) if match and validations . validate_tool_name ( match . group ( self . TOOL_NAME_GROUP ) ) : name = match . group ( self . TOOL_NAME_GROUP ) return creationinfo . Tool ( name ) else : raise SPDXValueError ( 'Failed to extract tool name' )
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
2642	def submit ( self , func , * args , ** kwargs ) : task_id = uuid . uuid4 ( ) logger . debug ( "Pushing function {} to queue with args {}" . format ( func , args ) ) self . tasks [ task_id ] = Future ( ) fn_buf = pack_apply_message ( func , args , kwargs , buffer_threshold = 1024 * 1024 , item_threshold = 1024 ) msg = { "task_id" : task_id , "buffer" : fn_buf } self . outgoing_q . put ( msg ) return self . tasks [ task_id ]
10708	def delete_vacation ( _id ) : arequest = requests . delete ( VACATIONS_URL + "/" + _id , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "Failed to delete vacation. " + status_code ) return False return True
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
3611	def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
12956	def _rem_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_key_for_index ( indexedField , val ) , pk )
5571	def execute ( mp ) : with mp . open ( "file1" , resampling = "bilinear" ) as raster_file : if raster_file . is_empty ( ) : return "empty" dem = raster_file . read ( ) return dem
5912	def combine ( self , name_all = None , out_ndx = None , operation = '|' , defaultgroups = False ) : if not operation in ( '|' , '&' , False ) : raise ValueError ( "Illegal operation {0!r}, only '|' (OR) and '&' (AND) or False allowed." . format ( operation ) ) if name_all is None and operation : name_all = self . name_all or operation . join ( self . indexfiles ) if out_ndx is None : out_ndx = self . output if defaultgroups : fd , default_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'default__' ) try : self . make_ndx ( o = default_ndx , input = [ 'q' ] ) except : utilities . unlink_gmx ( default_ndx ) raise ndxfiles = [ default_ndx ] else : ndxfiles = [ ] ndxfiles . extend ( self . indexfiles . values ( ) ) if operation : try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'combined__' ) operation = ' ' + operation . strip ( ) + ' ' cmd = [ operation . join ( [ '"{0!s}"' . format ( gname ) for gname in self . indexfiles ] ) , '' , 'q' ] rc , out , err = self . make_ndx ( n = ndxfiles , o = tmp_ndx , input = cmd ) if self . _is_empty_group ( out ) : warnings . warn ( "No atoms found for {cmd!r}" . format ( ** vars ( ) ) , category = BadParameterWarning ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] name_cmd = [ "name {0:d} {1!s}" . format ( last [ 'nr' ] , name_all ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = out_ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) if defaultgroups : utilities . unlink_gmx ( default_ndx ) else : rc , out , err = self . make_ndx ( n = ndxfiles , o = out_ndx , input = [ '' , 'q' ] ) return name_all , out_ndx
13758	def split_path ( path ) : result_parts = [ ] while path != "/" : parts = os . path . split ( path ) if parts [ 1 ] == path : result_parts . insert ( 0 , parts [ 1 ] ) break elif parts [ 0 ] == path : result_parts . insert ( 0 , parts [ 0 ] ) break else : path = parts [ 0 ] result_parts . insert ( 0 , parts [ 1 ] ) return result_parts
5080	def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
8960	def clean ( _dummy_ctx , docs = False , backups = False , bytecode = False , dist = False , all = False , venv = False , tox = False , extra = '' ) : cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) venv_dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/_build/' , 'doc/_build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/__pycache__/' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project_root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv_dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv_dirs ] ) fileset = antglob . FileSet ( cfg . project_root , patterns ) for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project_root , name ) ) else : os . unlink ( os . path . join ( cfg . project_root , name ) )
9503	def remove_contained_in_list ( l ) : i = 0 l . sort ( ) while i < len ( l ) - 1 : if l [ i + 1 ] . contains ( l [ i ] ) : l . pop ( i ) elif l [ i ] . contains ( l [ i + 1 ] ) : l . pop ( i + 1 ) else : i += 1
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
9640	def require_template_debug ( f ) : def _ ( * args , ** kwargs ) : TEMPLATE_DEBUG = getattr ( settings , 'TEMPLATE_DEBUG' , False ) return f ( * args , ** kwargs ) if TEMPLATE_DEBUG else '' return _
7851	def remove_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = 'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = "d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( expr ) if not l : return for f in l : f . unlinkNode ( ) f . freeNode ( )
5109	def set_num_servers ( self , n ) : if not isinstance ( n , numbers . Integral ) and n is not infty : the_str = "n must be an integer or infinity.\n{0}" raise TypeError ( the_str . format ( str ( self ) ) ) elif n <= 0 : the_str = "n must be a positive integer or infinity.\n{0}" raise ValueError ( the_str . format ( str ( self ) ) ) else : self . num_servers = n
7243	def geotiff ( self , ** kwargs ) : if 'proj' not in kwargs : kwargs [ 'proj' ] = self . proj return to_geotiff ( self , ** kwargs )
10600	def _url ( self , endpoint , url_data = None , parameters = None ) : try : url = '%s/%s' % ( self . base_url , self . endpoints [ endpoint ] ) except KeyError : raise EndPointDoesNotExist ( endpoint ) if url_data : url = url % url_data if parameters : url = '%s?%s' % ( url , urllib . urlencode ( parameters , True ) ) return url
5962	def parse ( self , stride = None ) : if stride is None : stride = self . stride self . corrupted_lineno = [ ] irow = 0 with utilities . openany ( self . real_filename ) as xvg : rows = [ ] ncol = None for lineno , line in enumerate ( xvg ) : line = line . strip ( ) if len ( line ) == 0 : continue if "label" in line and "xaxis" in line : self . xaxis = line . split ( '"' ) [ - 2 ] if "label" in line and "yaxis" in line : self . yaxis = line . split ( '"' ) [ - 2 ] if line . startswith ( "@ legend" ) : if not "legend" in self . metadata : self . metadata [ "legend" ] = [ ] self . metadata [ "legend" ] . append ( line . split ( "legend " ) [ - 1 ] ) if line . startswith ( "@ s" ) and "subtitle" not in line : name = line . split ( "legend " ) [ - 1 ] . replace ( '"' , '' ) . strip ( ) self . names . append ( name ) if line . startswith ( ( '#' , '@' ) ) : continue if line . startswith ( '&' ) : raise NotImplementedError ( '{0!s}: Multi-data not supported, only simple NXY format.' . format ( self . real_filename ) ) try : row = [ float ( el ) for el in line . split ( ) ] except : if self . permissive : self . logger . warn ( "%s: SKIPPING unparsable line %d: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue self . logger . error ( "%s: Cannot parse line %d: %r" , self . real_filename , lineno + 1 , line ) raise if ncol is not None and len ( row ) != ncol : if self . permissive : self . logger . warn ( "%s: SKIPPING line %d with wrong number of columns: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue errmsg = "{0!s}: Wrong number of columns in line {1:d}: {2!r}" . format ( self . real_filename , lineno + 1 , line ) self . logger . error ( errmsg ) raise IOError ( errno . ENODATA , errmsg , self . real_filename ) if irow % stride == 0 : ncol = len ( row ) rows . append ( row ) irow += 1 try : self . __array = numpy . array ( rows ) . transpose ( ) except : self . logger . error ( "%s: Failed reading XVG file, possibly data corrupted. " "Check the last line of the file..." , self . real_filename ) raise finally : del rows
1265	def sanity_check_actions ( actions_spec ) : actions = copy . deepcopy ( actions_spec ) is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) for name , action in actions . items ( ) : if 'type' not in action : action [ 'type' ] = 'int' if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( "Action requires value 'num_actions' set!" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( "Action requires both values 'min_value' and 'max_value' set!" ) if 'shape' not in action : action [ 'shape' ] = ( ) if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
7222	def get ( self , recipe_id ) : self . logger . debug ( 'Retrieving recipe by id: ' + recipe_id ) url = '%(base_url)s/recipe/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
5507	def _image_name_from_url ( url ) : find = r'https?://|[^\w]' replace = '_' return re . sub ( find , replace , url ) . strip ( '_' )
5955	def find_executables ( path ) : execs = [ ] for exe in os . listdir ( path ) : fullexe = os . path . join ( path , exe ) if ( os . access ( fullexe , os . X_OK ) and not os . path . isdir ( fullexe ) and exe not in [ 'GMXRC' , 'GMXRC.bash' , 'GMXRC.csh' , 'GMXRC.zsh' , 'demux.pl' , 'xplor2gmx.pl' ] ) : execs . append ( exe ) return execs
5880	def get_siblings_content ( self , current_sibling , baselinescore_siblings_para ) : if current_sibling . tag == 'p' and self . parser . getText ( current_sibling ) : tmp = current_sibling if tmp . tail : tmp = deepcopy ( tmp ) tmp . tail = '' return [ tmp ] else : potential_paragraphs = self . parser . getElementsByTag ( current_sibling , tag = 'p' ) if potential_paragraphs is None : return None paragraphs = list ( ) for first_paragraph in potential_paragraphs : text = self . parser . getText ( first_paragraph ) if text : word_stats = self . stopwords_class ( language = self . get_language ( ) ) . get_stopword_count ( text ) paragraph_score = word_stats . get_stopword_count ( ) sibling_baseline_score = float ( .30 ) high_link_density = self . is_highlink_density ( first_paragraph ) score = float ( baselinescore_siblings_para * sibling_baseline_score ) if score < paragraph_score and not high_link_density : para = self . parser . createElement ( tag = 'p' , text = text , tail = None ) paragraphs . append ( para ) return paragraphs
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
9743	async def reboot ( ip_address ) : _ , protocol = await asyncio . get_event_loop ( ) . create_datagram_endpoint ( QRebootProtocol , local_addr = ( ip_address , 0 ) , allow_broadcast = True , reuse_address = True , ) LOG . info ( "Sending reboot on %s" , ip_address ) protocol . send_reboot ( )
8878	def find_libname ( self , name ) : names = [ "{}.lib" , "lib{}.lib" , "{}lib.lib" ] names = [ n . format ( name ) for n in names ] dirs = self . get_library_dirs ( ) for d in dirs : for n in names : if exists ( join ( d , n ) ) : return n [ : - 4 ] msg = "Could not find the {} library." . format ( name ) raise ValueError ( msg )
2427	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True if validations . validate_doc_comment ( comment ) : doc . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'Document::Comment' ) else : raise CardinalityError ( 'Document::Comment' )
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
2143	def ordered_dump ( data , Dumper = yaml . Dumper , ** kws ) : class OrderedDumper ( Dumper ) : pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items ( ) ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , None , OrderedDumper , ** kws )
540	def __createModelCheckpoint ( self ) : if self . _model is None or self . _modelCheckpointGUID is None : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) predictions = StringIO . StringIO ( ) self . _predictionLogger . checkpoint ( checkpointSink = predictions , maxRows = int ( Configuration . get ( 'nupic.model.checkpoint.maxPredictionRows' ) ) ) self . _model . save ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : str ( self . _modelCheckpointGUID ) } , ignoreUnchanged = True ) self . _logger . info ( "Checkpointed Hypersearch Model: modelID: %r, " "checkpointID: %r" , self . _modelID , checkpointID ) return
5888	def __crawl ( self , crawl_candidate ) : def crawler_wrapper ( parser , parsers_lst , crawl_candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl_candidate ) except ( UnicodeDecodeError , ValueError ) as ex : if parsers_lst : parser = parsers_lst . pop ( 0 ) return crawler_wrapper ( parser , parsers_lst , crawl_candidate ) else : raise ex return article parsers = list ( self . config . available_parsers ) parsers . remove ( self . config . parser_class ) return crawler_wrapper ( self . config . parser_class , parsers , crawl_candidate )
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
6424	def sim ( self , src , tar , qval = 2 ) : r return super ( self . __class__ , self ) . sim ( src , tar , qval , 1 , 1 )
5274	def find ( self , y ) : node = self . root while True : edge = self . _edgeLabel ( node , node . parent ) if edge . startswith ( y ) : return node . idx i = 0 while ( i < len ( edge ) and edge [ i ] == y [ 0 ] ) : y = y [ 1 : ] i += 1 if i != 0 : if i == len ( edge ) and y != '' : pass else : return - 1 node = node . _get_transition_link ( y [ 0 ] ) if not node : return - 1
12341	def _set_path ( self , path ) : "Set self.path, self.dirname and self.basename." import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
11707	def generate_gamete ( self , egg_or_sperm_word ) : p_rate_of_mutation = [ 0.9 , 0.1 ] should_use_mutant_pool = ( npchoice ( [ 0 , 1 ] , 1 , p = p_rate_of_mutation ) [ 0 ] == 1 ) if should_use_mutant_pool : pool = tokens . secondary_tokens else : pool = tokens . primary_tokens return get_matches ( egg_or_sperm_word , pool , 23 )
1232	def import_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
12903	def _parse_genotype ( self , vcf_fields ) : format_col = vcf_fields [ 8 ] . split ( ':' ) genome_data = vcf_fields [ 9 ] . split ( ':' ) try : gt_idx = format_col . index ( 'GT' ) except ValueError : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome_data [ gt_idx ] ) if x != '.' ]
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
4816	def create_n_gram_df ( df , n_pad ) : n_pad_2 = int ( ( n_pad - 1 ) / 2 ) for i in range ( n_pad_2 ) : df [ 'char-{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( i + 1 ) df [ 'type-{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( i + 1 ) df [ 'char{}' . format ( i + 1 ) ] = df [ 'char' ] . shift ( - i - 1 ) df [ 'type{}' . format ( i + 1 ) ] = df [ 'type' ] . shift ( - i - 1 ) return df [ n_pad_2 : - n_pad_2 ]
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
6359	def sim ( self , src , tar ) : r if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsstr ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
5169	def __netjson_protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
4194	def plot_window ( self ) : from pylab import plot , xlim , grid , title , ylabel , axis x = linspace ( 0 , 1 , self . N ) xlim ( 0 , 1 ) plot ( x , self . data ) grid ( True ) title ( '%s Window (%s points)' % ( self . name . capitalize ( ) , self . N ) ) ylabel ( 'Amplitude' ) axis ( [ 0 , 1 , 0 , 1.1 ] )
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
2959	def _state_delete ( self ) : try : os . remove ( self . _state_file ) except OSError as err : if err . errno not in ( errno . EPERM , errno . ENOENT ) : raise try : os . rmdir ( self . _state_dir ) except OSError as err : if err . errno not in ( errno . ENOTEMPTY , errno . ENOENT ) : raise
1530	def monitor ( self ) : def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : for topology , callbacks in watchers . items ( ) : file_path = os . path . join ( path , topology ) data = "" if os . path . exists ( file_path ) : with open ( os . path . join ( path , topology ) ) as f : data = f . read ( ) if topology not in directory or data != directory [ topology ] : proto_object = ProtoClass ( ) proto_object . ParseFromString ( data ) for callback in callbacks : callback ( proto_object ) directory [ topology ] = data while not self . monitoring_thread_stop_signal : topologies_path = self . get_topologies_path ( ) topologies = [ ] if os . path . isdir ( topologies_path ) : topologies = list ( filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , os . listdir ( topologies_path ) ) ) if set ( topologies ) != set ( self . topologies_directory ) : for callback in self . topologies_watchers : callback ( topologies ) self . topologies_directory = topologies trigger_watches_based_on_files ( self . topology_watchers , topologies_path , self . topologies_directory , Topology ) execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) trigger_watches_based_on_files ( self . execution_state_watchers , execution_state_path , self . execution_state_directory , ExecutionState ) packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) trigger_watches_based_on_files ( self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) trigger_watches_based_on_files ( self . pplan_watchers , pplan_path , self . pplan_directory , PhysicalPlan ) tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) trigger_watches_based_on_files ( self . tmaster_watchers , tmaster_path , self . tmaster_directory , TMasterLocation ) scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) trigger_watches_based_on_files ( self . scheduler_location_watchers , scheduler_location_path , self . scheduler_location_directory , SchedulerLocation ) self . event . wait ( timeout = 5 )
2265	def dict_isect ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict common_keys = set . intersection ( * map ( set , args ) ) first_dict = args [ 0 ] return dictclass ( ( k , first_dict [ k ] ) for k in common_keys )
12373	def get_first ( ) : client = po . connect ( ) all_droplets = client . droplets . list ( ) id = all_droplets [ 0 ] [ 'id' ] return client . droplets . get ( id )
7759	def check_events ( self ) : if self . event_dispatcher . flush ( ) is QUIT : self . _quit = True return True return False
360	def load_folder_list ( path = "" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]
9320	def _validate_collection ( self ) : if not self . _id : msg = "No 'id' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _title : msg = "No 'title' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_read is None : msg = "No 'can_read' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _can_write is None : msg = "No 'can_write' in Collection for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _id not in self . url : msg = "The collection '{}' does not match the url for queries '{}'" raise ValidationError ( msg . format ( self . _id , self . url ) )
12814	def startProducing ( self , consumer ) : self . _consumer = consumer self . _current_deferred = defer . Deferred ( ) self . _sent = 0 self . _paused = False if not hasattr ( self , "_chunk_headers" ) : self . _build_chunk_headers ( ) if self . _data : block = "" for field in self . _data : block += self . _chunk_headers [ field ] block += self . _data [ field ] block += "\r\n" self . _send_to_consumer ( block ) if self . _files : self . _files_iterator = self . _files . iterkeys ( ) self . _files_sent = 0 self . _files_length = len ( self . _files ) self . _current_file_path = None self . _current_file_handle = None self . _current_file_length = None self . _current_file_sent = 0 result = self . _produce ( ) if result : return result else : return defer . succeed ( None ) return self . _current_deferred
1630	def CheckForHeaderGuard ( filename , clean_lines , error ) : raw_lines = clean_lines . lines_without_raw_strings for i in raw_lines : if Search ( r'//\s*NOLINT\(build/header_guard\)' , i ) : return for i in raw_lines : if Search ( r'^\s*#pragma\s+once' , i ) : return cppvar = GetHeaderGuardCPPVariable ( filename ) ifndef = '' ifndef_linenum = 0 define = '' endif = '' endif_linenum = 0 for linenum , line in enumerate ( raw_lines ) : linesplit = line . split ( ) if len ( linesplit ) >= 2 : if not ifndef and linesplit [ 0 ] == '#ifndef' : ifndef = linesplit [ 1 ] ifndef_linenum = linenum if not define and linesplit [ 0 ] == '#define' : define = linesplit [ 1 ] if line . startswith ( '#endif' ) : endif = line endif_linenum = linenum if not ifndef or not define or ifndef != define : error ( filename , 0 , 'build/header_guard' , 5 , 'No #ifndef header guard found, suggested CPP variable is: %s' % cppvar ) return if ifndef != cppvar : error_level = 0 if ifndef != cppvar + '_' : error_level = 5 ParseNolintSuppressions ( filename , raw_lines [ ifndef_linenum ] , ifndef_linenum , error ) error ( filename , ifndef_linenum , 'build/header_guard' , error_level , '#ifndef header guard has wrong style, please use: %s' % cppvar ) ParseNolintSuppressions ( filename , raw_lines [ endif_linenum ] , endif_linenum , error ) match = Match ( r'#endif\s*//\s*' + cppvar + r'(_)?\b' , endif ) if match : if match . group ( 1 ) == '_' : error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif // %s"' % cppvar ) return no_single_line_comments = True for i in xrange ( 1 , len ( raw_lines ) - 1 ) : line = raw_lines [ i ] if Match ( r'^(?:(?:\'(?:\.|[^\'])*\')|(?:"(?:\.|[^"])*")|[^\'"])*//' , line ) : no_single_line_comments = False break if no_single_line_comments : match = Match ( r'#endif\s*/\*\s*' + cppvar + r'(_)?\s*\*/' , endif ) if match : if match . group ( 1 ) == '_' : error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif /* %s */"' % cppvar ) return error ( filename , endif_linenum , 'build/header_guard' , 5 , '#endif line should be "#endif // %s"' % cppvar )
1877	def MOVSD ( cpu , dest , src ) : assert dest . type != 'memory' or src . type != 'memory' value = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) if dest . size > src . size : value = Operators . ZEXTEND ( value , dest . size ) dest . write ( value )
4531	def construct ( cls , project , ** desc ) : return cls ( project . drivers , maker = project . maker , ** desc )
11466	def ls ( self , folder = '' ) : current_folder = self . _ftp . pwd ( ) self . cd ( folder ) contents = [ ] self . _ftp . retrlines ( 'LIST' , lambda a : contents . append ( a ) ) files = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( '-' ) , contents ) folders = filter ( lambda a : a . split ( ) [ 0 ] . startswith ( 'd' ) , contents ) files = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , files ) folders = map ( lambda a : ' ' . join ( a . split ( ) [ 8 : ] ) , folders ) self . _ftp . cwd ( current_folder ) return files , folders
1116	def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . _prefix [ 1 ] next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id="difflib_chg_%s_%d"' % ( toprefix , num_chg ) num_chg += 1 next_href [ last ] = '<a href="#difflib_chg_%s_%d">n</a>' % ( toprefix , num_chg ) else : in_change = False if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next_href [ 0 ] = '<a href="#difflib_chg_%s_0">f</a>' % toprefix next_href [ last ] = '<a href="#difflib_chg_%s_top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id
7652	def match_query ( string , query ) : if six . callable ( query ) : return query ( string ) elif ( isinstance ( query , six . string_types ) and isinstance ( string , six . string_types ) ) : return re . match ( query , string ) is not None else : return query == string
13509	def sloccount ( ) : setup = options . get ( 'setup' ) packages = options . get ( 'packages' ) if setup else None if packages : dirs = [ x for x in packages if '.' not in x ] else : dirs = [ '.' ] ls = [ ] for d in dirs : ls += list ( path ( d ) . walkfiles ( ) ) files = ' ' . join ( ls ) param = options . paved . pycheck . sloccount . param sh ( 'sloccount {param} {files} | tee sloccount.sc' . format ( param = param , files = files ) )
11339	def set_target_fahrenheit ( self , fahrenheit , mode = config . SCHEDULE_HOLD ) : temperature = fahrenheit_to_nuheat ( fahrenheit ) self . set_target_temperature ( temperature , mode )
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
5587	def extract_subset ( self , input_data_tiles = None , out_tile = None ) : if self . METADATA [ "data_type" ] == "raster" : mosaic = create_mosaic ( input_data_tiles ) return extract_from_array ( in_raster = prepare_array ( mosaic . data , nodata = self . nodata , dtype = self . output_params [ "dtype" ] ) , in_affine = mosaic . affine , out_tile = out_tile ) elif self . METADATA [ "data_type" ] == "vector" : return [ feature for feature in list ( chain . from_iterable ( [ features for _ , features in input_data_tiles ] ) ) if shape ( feature [ "geometry" ] ) . intersects ( out_tile . bbox ) ]
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
4616	def refresh ( self ) : asset = self . blockchain . rpc . get_asset ( self . identifier ) if not asset : raise AssetDoesNotExistsException ( self . identifier ) super ( Asset , self ) . __init__ ( asset , blockchain_instance = self . blockchain ) if self . full : if "bitasset_data_id" in asset : self [ "bitasset_data" ] = self . blockchain . rpc . get_object ( asset [ "bitasset_data_id" ] ) self [ "dynamic_asset_data" ] = self . blockchain . rpc . get_object ( asset [ "dynamic_asset_data_id" ] )
582	def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . saved_states ) == 0 : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'. Model has no saved records." ) startID = self . saved_states [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . saved_states ) if end is None else max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . saved_states ) } ) recordsToDelete = [ ] for state in self . saved_states [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . saved_states [ clippedEnd : ] : self . _updateState ( state ) return { 'status' : 'success' }
8590	def reboot_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/reboot' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
4420	async def stop ( self ) : await self . _lavalink . ws . send ( op = 'stop' , guildId = self . guild_id ) self . current = None
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
4150	def plot ( self , filename = None , norm = False , ylim = None , sides = None , ** kargs ) : import pylab from pylab import ylim as plt_ylim _ = self . psd if sides is not None : if sides not in self . _sides_choices : raise errors . SpectrumChoiceError ( sides , self . _sides_choices ) if sides is None or sides == self . sides : frequencies = self . frequencies ( ) psd = self . psd sides = self . sides elif sides is not None : if self . datatype == 'complex' : if sides == 'onesided' : raise ValueError ( "sides cannot be one-sided with complex data" ) logging . debug ( "sides is different from the one provided. Converting PSD" ) frequencies = self . frequencies ( sides = sides ) psd = self . get_converted_psd ( sides ) if len ( psd ) != len ( frequencies ) : raise ValueError ( "PSD length is %s and freq length is %s" % ( len ( psd ) , len ( frequencies ) ) ) if 'ax' in list ( kargs . keys ( ) ) : save_ax = pylab . gca ( ) pylab . sca ( kargs [ 'ax' ] ) rollback = True del kargs [ 'ax' ] else : rollback = False if norm : pylab . plot ( frequencies , 10 * stools . log10 ( psd / max ( psd ) ) , ** kargs ) else : pylab . plot ( frequencies , 10 * stools . log10 ( psd ) , ** kargs ) pylab . xlabel ( 'Frequency' ) pylab . ylabel ( 'Power (dB)' ) pylab . grid ( True ) if ylim : plt_ylim ( ylim ) if sides == 'onesided' : pylab . xlim ( 0 , self . sampling / 2. ) elif sides == 'twosided' : pylab . xlim ( 0 , self . sampling ) elif sides == 'centerdc' : pylab . xlim ( - self . sampling / 2. , self . sampling / 2. ) if filename : pylab . savefig ( filename ) if rollback : pylab . sca ( save_ax ) del psd , frequencies
13088	def write_config ( self , initialize_indices = False ) : if not os . path . exists ( self . config_dir ) : os . mkdir ( self . config_dir ) with open ( self . config_file , 'w' ) as configfile : self . config . write ( configfile ) if initialize_indices : index = self . get ( 'jackal' , 'index' ) from jackal import Host , Range , Service , User , Credential , Log from jackal . core import create_connection create_connection ( self ) Host . init ( index = "{}-hosts" . format ( index ) ) Range . init ( index = "{}-ranges" . format ( index ) ) Service . init ( index = "{}-services" . format ( index ) ) User . init ( index = "{}-users" . format ( index ) ) Credential . init ( index = "{}-creds" . format ( index ) ) Log . init ( index = "{}-log" . format ( index ) )
13435	def cut ( self , line ) : result = [ ] line = self . line ( line ) for i , field in enumerate ( self . positions ) : try : index = _setup_index ( field ) try : result += line [ index ] except IndexError : result . append ( self . invalid_pos ) except ValueError : result . append ( str ( field ) ) except TypeError : result . extend ( self . _cut_range ( line , int ( field [ 0 ] ) , i ) ) return '' . join ( result )
548	def __deleteOutputCache ( self , modelID ) : if modelID == self . _modelID and self . _predictionLogger is not None : self . _predictionLogger . close ( ) del self . __predictionCache self . _predictionLogger = None self . __predictionCache = None
11604	def check_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if isinstance ( start , int ) or isinstance ( end , int ) : if isinstance ( start , int ) and not ( 0 <= start < length ) : continue elif isinstance ( start , int ) and isinstance ( end , int ) and not ( start <= end ) : continue elif start is None and end == 0 : continue result . append ( ( start , end ) ) return result
5577	def load_input_reader ( input_params , readonly = False ) : logger . debug ( "find input reader with params %s" , input_params ) if not isinstance ( input_params , dict ) : raise TypeError ( "input_params must be a dictionary" ) if "abstract" in input_params : driver_name = input_params [ "abstract" ] [ "format" ] elif "path" in input_params : if os . path . splitext ( input_params [ "path" ] ) [ 1 ] : input_file = input_params [ "path" ] driver_name = driver_from_file ( input_file ) else : logger . debug ( "%s is a directory" , input_params [ "path" ] ) driver_name = "TileDirectory" else : raise MapcheteDriverError ( "invalid input parameters %s" % input_params ) for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "driver_name" ] == driver_name ) : return v . load ( ) . InputData ( input_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
8472	def setup ( ) : if not os . path . isdir ( AtomShieldsScanner . CHECKERS_DIR ) : os . makedirs ( AtomShieldsScanner . CHECKERS_DIR ) if not os . path . isdir ( AtomShieldsScanner . REPORTS_DIR ) : os . makedirs ( AtomShieldsScanner . REPORTS_DIR ) for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "checkers" ) , "*.py" ) : AtomShieldsScanner . installChecker ( f ) for f in AtomShieldsScanner . _getFiles ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , "reports" ) , "*.py" ) : AtomShieldsScanner . installReport ( f ) AtomShieldsScanner . _executeMassiveMethod ( path = AtomShieldsScanner . CHECKERS_DIR , method = "install" , args = { } ) config_dir = os . path . dirname ( AtomShieldsScanner . CONFIG_PATH ) if not os . path . isdir ( config_dir ) : os . makedirs ( config_dir )
4693	def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
150	def deepcopy ( self ) : polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
8471	def _debug ( message , color = None , attrs = None ) : if attrs is None : attrs = [ ] if color is not None : print colored ( message , color , attrs = attrs ) else : if len ( attrs ) > 0 : print colored ( message , "white" , attrs = attrs ) else : print message
5480	def cancel ( batch_fn , cancel_fn , ops ) : canceled_ops = [ ] error_messages = [ ] max_batch = 256 total_ops = len ( ops ) for first_op in range ( 0 , total_ops , max_batch ) : batch_canceled , batch_messages = _cancel_batch ( batch_fn , cancel_fn , ops [ first_op : first_op + max_batch ] ) canceled_ops . extend ( batch_canceled ) error_messages . extend ( batch_messages ) return canceled_ops , error_messages
1351	def make_response ( self , status ) : response = { constants . RESPONSE_KEY_STATUS : status , constants . RESPONSE_KEY_VERSION : constants . API_VERSION , constants . RESPONSE_KEY_EXECUTION_TIME : 0 , constants . RESPONSE_KEY_MESSAGE : "" , } return response
13326	def create ( name_or_path , config ) : if not name_or_path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv create my_env\n' ' cpenv create ./relative/path/to/my_env\n' ' cpenv create my_env --config ./relative/path/to/config\n' ' cpenv create my_env --config git@github.com:user/config.git\n' ) click . echo ( examples ) return click . echo ( blue ( 'Creating a new virtual environment ' + name_or_path ) ) try : env = cpenv . create ( name_or_path , config ) except Exception as e : click . echo ( bold_red ( 'FAILED TO CREATE ENVIRONMENT!' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'Successfully created environment!' ) ) click . echo ( blue ( 'Launching subshell' ) ) cpenv . activate ( env ) shell . launch ( env . name )
13071	def r_first_passage ( self , objectId ) : collection , reffs = self . get_reffs ( objectId = objectId , export_collection = True ) first , _ = reffs [ 0 ] return redirect ( url_for ( ".r_passage_semantic" , objectId = objectId , subreference = first , semantic = self . semantic ( collection ) ) )
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
6491	def _process_exclude_dictionary ( exclude_dictionary ) : not_properties = [ ] for exclude_property in exclude_dictionary : exclude_values = exclude_dictionary [ exclude_property ] if not isinstance ( exclude_values , list ) : exclude_values = [ exclude_values ] not_properties . extend ( [ { "term" : { exclude_property : exclude_value } } for exclude_value in exclude_values ] ) if not not_properties : return { } return { "not" : { "filter" : { "or" : not_properties } } }
4386	def router_function ( fn ) : @ wraps ( fn ) def wrapper ( * args , ** kwargs ) : if platform_is_windows ( ) : raise RuntimeError ( "Router interface is not available on Win32 systems.\n" "Configure AMS routes using the TwinCAT router service." ) return fn ( * args , ** kwargs ) return wrapper
1514	def wait_for_job_to_start ( single_master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise RuntimeError ( ) except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
8429	def cmap_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) def _cmap_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _cmap_pal
13764	def RegisterMessage ( self , message ) : desc = message . DESCRIPTOR self . _symbols [ desc . full_name ] = message if desc . file . name not in self . _symbols_by_file : self . _symbols_by_file [ desc . file . name ] = { } self . _symbols_by_file [ desc . file . name ] [ desc . full_name ] = message self . pool . AddDescriptor ( desc ) return message
2732	def get_records ( self , params = None ) : if params is None : params = { } records = [ ] data = self . get_data ( "domains/%s/records/" % self . name , type = GET , params = params ) for record_data in data [ 'domain_records' ] : record = Record ( domain_name = self . name , ** record_data ) record . token = self . token records . append ( record ) return records
8889	def _self_referential_fk ( klass_model ) : for f in klass_model . _meta . concrete_fields : if f . related_model : if issubclass ( klass_model , f . related_model ) : return f . attname return None
4568	def dumps ( data , use_yaml = None , safe = True , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML if use_yaml : dumps = yaml . safe_dump if safe else yaml . dump else : dumps = json . dumps kwds . update ( indent = 4 , sort_keys = True ) if not safe : kwds . update ( default = repr ) return dumps ( data , ** kwds )
5515	def setlocale ( name ) : with LOCALE_LOCK : old_locale = locale . setlocale ( locale . LC_ALL ) try : yield locale . setlocale ( locale . LC_ALL , name ) finally : locale . setlocale ( locale . LC_ALL , old_locale )
12944	def save ( self , cascadeSave = True ) : saver = IndexedRedisSave ( self . __class__ ) return saver . save ( self , cascadeSave = cascadeSave )
8206	def overlap ( self , x1 , y1 , x2 , y2 , r = 5 ) : if abs ( x2 - x1 ) < r and abs ( y2 - y1 ) < r : return True else : return False
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
7685	def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , ** kwargs )
5626	def write_json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put_object ( Key = key , Body = json . dumps ( params , sort_keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort_keys = True , indent = 4 )
1847	def JZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC )
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
13240	def includes ( self , query_date , query_time = None ) : if self . start_date and query_date < self . start_date : return False if self . end_date and query_date > self . end_date : return False if query_date . weekday ( ) not in self . weekdays : return False if not query_time : return True if query_time >= self . period . start and query_time <= self . period . end : return True return False
10261	def _collapse_variants_by_function ( graph : BELGraph , func : str ) -> None : for parent_node , variant_node , data in graph . edges ( data = True ) : if data [ RELATION ] == HAS_VARIANT and parent_node . function == func : collapse_pair ( graph , from_node = variant_node , to_node = parent_node )
7667	def search ( self , ** kwargs ) : results = AnnotationArray ( ) for annotation in self : if annotation . search ( ** kwargs ) : results . append ( annotation ) return results
2605	def make_hash ( self , task ) : t = [ serialize_object ( task [ 'func_name' ] ) [ 0 ] , serialize_object ( task [ 'fn_hash' ] ) [ 0 ] , serialize_object ( task [ 'args' ] ) [ 0 ] , serialize_object ( task [ 'kwargs' ] ) [ 0 ] , serialize_object ( task [ 'env' ] ) [ 0 ] ] x = b'' . join ( t ) hashedsum = hashlib . md5 ( x ) . hexdigest ( ) return hashedsum
7927	def reorder_srv ( records ) : records = list ( records ) records . sort ( ) ret = [ ] tmp = [ ] for rrecord in records : if not tmp or rrecord . priority == tmp [ 0 ] . priority : tmp . append ( rrecord ) continue ret += shuffle_srv ( tmp ) tmp = [ rrecord ] if tmp : ret += shuffle_srv ( tmp ) return ret
4877	def validate_course_run_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) if not enterprise_customer . catalog_contains_course ( value ) : raise serializers . ValidationError ( 'The course run id {course_run_id} is not in the catalog ' 'for Enterprise Customer {enterprise_customer}' . format ( course_run_id = value , enterprise_customer = enterprise_customer . name , ) ) return value
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
5210	def bdp ( tickers , flds , ** kwargs ) : logger = logs . get_logger ( bdp , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( ** kwargs ) logger . info ( f'loading reference data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . ref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for r , snap in data . iterrows ( ) : subset = [ r ] data_file = storage . ref_file ( ticker = snap . ticker , fld = snap . field , ext = 'pkl' , ** kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( data . iloc [ subset ] ) files . create_folder ( data_file , is_file = True ) data . iloc [ subset ] . to_pickle ( data_file ) return qry_data
3337	def join_uri ( uri , * segments ) : sub = "/" . join ( segments ) if not sub : return uri return uri . rstrip ( "/" ) + "/" + sub
1340	def binarize ( x , values , threshold = None , included_in = 'upper' ) : lower , upper = values if threshold is None : threshold = ( lower + upper ) / 2. x = x . copy ( ) if included_in == 'lower' : x [ x <= threshold ] = lower x [ x > threshold ] = upper elif included_in == 'upper' : x [ x < threshold ] = lower x [ x >= threshold ] = upper else : raise ValueError ( 'included_in must be "lower" or "upper"' ) return x
2808	def convert_matmul ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting matmul ...' ) if names == 'short' : tf_name = 'MMUL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) == 1 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) elif len ( inputs ) == 2 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) else : raise AssertionError ( 'Cannot convert matmul layer' )
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : r if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
10430	def multiselect ( self , window_name , object_name , row_text_list , partial_match = False ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) object_handle . activate ( ) selected = False try : window = self . _get_front_most_window ( ) except ( IndexError , ) : window = self . _get_any_window ( ) for row_text in row_text_list : selected = False for cell in object_handle . AXRows : parent_cell = cell cell = self . _getfirstmatchingchild ( cell , "(AXTextField|AXStaticText)" ) if not cell : continue if re . match ( row_text , cell . AXValue ) : selected = True if not parent_cell . AXSelected : x , y , width , height = self . _getobjectsize ( parent_cell ) window . clickMouseButtonLeftWithMods ( ( x + width / 2 , y + height / 2 ) , [ '<command_l>' ] ) self . wait ( 0.5 ) else : pass break if not selected : raise LdtpServerException ( u"Unable to select row: %s" % row_text ) if not selected : raise LdtpServerException ( u"Unable to select any row" ) return 1
9006	def to_svg ( self , converter = None ) : if converter is None : from knittingpattern . convert . InstructionSVGCache import default_svg_cache converter = default_svg_cache ( ) return converter . to_svg ( self )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
5859	def __prune_search_template ( self , extract_as_keys , search_template ) : data = { "extract_as_keys" : extract_as_keys , "search_template" : search_template } failure_message = "Failed to prune a search template" return self . _get_success_json ( self . _post_json ( 'v1/search_templates/prune-to-extract-as' , data , failure_message = failure_message ) ) [ 'data' ]
12819	def _file_size ( self , field ) : size = 0 try : handle = open ( self . _files [ field ] , "r" ) size = os . fstat ( handle . fileno ( ) ) . st_size handle . close ( ) except : size = 0 self . _file_lengths [ field ] = size return self . _file_lengths [ field ]
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
9840	def __array ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'type' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: type was "%s", not a string.' % tok . text ) self . currentobject [ 'type' ] = tok . value ( ) elif tok . equals ( 'rank' ) : tok = self . __consume ( ) try : self . currentobject [ 'rank' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: rank was "%s", not an integer.' % tok . text ) elif tok . equals ( 'items' ) : tok = self . __consume ( ) try : self . currentobject [ 'size' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: items was "%s", not an integer.' % tok . text ) elif tok . equals ( 'data' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: data was "%s", not a string.' % tok . text ) if tok . text != 'follows' : raise NotImplementedError ( 'array: Only the "data follows header" format is supported.' ) if not self . currentobject [ 'size' ] : raise DXParseError ( "array: missing number of items" ) self . currentobject [ 'array' ] = [ ] while len ( self . currentobject [ 'array' ] ) < self . currentobject [ 'size' ] : self . currentobject [ 'array' ] . extend ( self . dxfile . readline ( ) . strip ( ) . split ( ) ) elif tok . equals ( 'attribute' ) : attribute = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'string' ) : raise DXParseError ( 'array: "string" expected.' ) value = self . __consume ( ) . value ( ) else : raise DXParseError ( 'array: ' + str ( tok ) + ' not recognized.' )
4222	def disable ( ) : root = platform . config_root ( ) try : os . makedirs ( root ) except OSError : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( ** locals ( ) ) raise RuntimeError ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
11506	def create_item ( self , token , name , parent_id , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name parameters [ 'parentid' ] = parent_id optional_keys = [ 'description' , 'uuid' , 'privacy' ] for key in optional_keys : if key in kwargs : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.item.create' , parameters ) return response
9384	def parse_xml_jtl ( self , granularity ) : data = defaultdict ( list ) processed_data = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for input_file in self . infile_list : logger . info ( 'Processing : %s' , input_file ) timestamp_format = None tree = ElementTree . parse ( input_file ) samples = tree . findall ( './httpSample' ) + tree . findall ( './sample' ) for sample in samples : if not timestamp_format or timestamp_format == 'unknown' : timestamp_format = naarad . utils . detect_timestamp_format ( sample . get ( 'ts' ) ) if timestamp_format == 'unknown' : continue ts = naarad . utils . get_standardized_timestamp ( sample . get ( 'ts' ) , timestamp_format ) if ts == - 1 : continue ts = naarad . utils . reconcile_timezones ( ts , self . timezone , self . graph_timezone ) aggregate_timestamp , averaging_factor = self . get_aggregation_timestamp ( ts , granularity ) self . aggregate_count_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , aggregate_timestamp ) self . aggregate_values_over_time ( processed_data , sample , [ self . _sanitize_label ( sample . get ( 'lb' ) ) , 'Overall_Summary' ] , [ 't' , 'by' ] , aggregate_timestamp ) logger . info ( 'Finished parsing : %s' , input_file ) logger . info ( 'Processing metrics for output to csv' ) self . average_values_for_plot ( processed_data , data , averaging_factor ) logger . info ( 'Writing time series csv' ) for csv in data . keys ( ) : self . csv_files . append ( csv ) with open ( csv , 'w' ) as csvf : csvf . write ( '\n' . join ( sorted ( data [ csv ] ) ) ) logger . info ( 'Processing raw data for stats' ) self . calculate_key_stats ( processed_data ) return True
10680	def Cp_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : c = ( self . _B_mag * ( 2 * tau ** 3 + 2 * tau ** 9 / 3 + 2 * tau ** 15 / 5 ) ) / self . _D_mag else : c = ( 2 * tau ** - 5 + 2 * tau ** - 15 / 3 + 2 * tau ** - 25 / 5 ) / self . _D_mag result = R * math . log ( self . beta0_mag + 1 ) * c return result
3670	def Wilson ( xs , params ) : r gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
11082	def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( ** value ) return value
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
5969	def em_schedule ( ** kwargs ) : mdrunner = kwargs . pop ( 'mdrunner' , None ) integrators = kwargs . pop ( 'integrators' , [ 'l-bfgs' , 'steep' ] ) kwargs . pop ( 'integrator' , None ) nsteps = kwargs . pop ( 'nsteps' , [ 100 , 1000 ] ) outputs = [ 'em{0:03d}_{1!s}.pdb' . format ( i , integrator ) for i , integrator in enumerate ( integrators ) ] outputs [ - 1 ] = kwargs . pop ( 'output' , 'em.pdb' ) files = { 'struct' : kwargs . pop ( 'struct' , None ) } for i , integrator in enumerate ( integrators ) : struct = files [ 'struct' ] logger . info ( "[em %d] energy minimize with %s for maximum %d steps" , i , integrator , nsteps [ i ] ) kwargs . update ( { 'struct' : struct , 'output' : outputs [ i ] , 'integrator' : integrator , 'nsteps' : nsteps [ i ] } ) if not integrator == 'l-bfgs' : kwargs [ 'mdrunner' ] = mdrunner else : kwargs [ 'mdrunner' ] = None logger . warning ( "[em %d] Not using mdrunner for L-BFGS because it cannot " "do parallel runs." , i ) files = energy_minimize ( ** kwargs ) return files
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
13455	def _parse_args ( args ) : parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
8542	def _get_username ( self , username = None , use_config = True , config_filename = None ) : if not username and use_config : if self . _config is None : self . _read_config ( config_filename ) username = self . _config . get ( "credentials" , "username" , fallback = None ) if not username : username = input ( "Please enter your username: " ) . strip ( ) while not username : username = input ( "No username specified. Please enter your username: " ) . strip ( ) if 'credendials' not in self . _config : self . _config . add_section ( 'credentials' ) self . _config . set ( "credentials" , "username" , username ) self . _save_config ( ) return username
10894	def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
11386	def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , ** kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
13885	def CreateFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , create_dir = True , encoding = None , binary = False ) : if binary : if isinstance ( contents , six . text_type ) : raise TypeError ( 'contents must be str (bytes) when binary=True' ) else : if not isinstance ( contents , six . text_type ) : raise TypeError ( 'contents must be unicode when binary=False' ) contents = _HandleContentsEol ( contents , eol_style ) encoding = encoding or sys . getfilesystemencoding ( ) contents = contents . encode ( encoding ) binary = True if create_dir : dirname = os . path . dirname ( filename ) if dirname : CreateDirectory ( dirname ) from six . moves . urllib . parse import urlparse filename_url = urlparse ( filename ) if _UrlIsLocal ( filename_url ) : with open ( filename , 'wb' ) as oss : oss . write ( contents ) elif filename_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( filename_url . scheme ) return filename
11760	def retract ( self , sentence ) : "Remove the sentence's clauses from the KB." for c in conjuncts ( to_cnf ( sentence ) ) : if c in self . clauses : self . clauses . remove ( c )
8155	def create_table ( self , name , fields = [ ] , key = "id" ) : for f in fields : if f == key : fields . remove ( key ) sql = "create table " + name + " " sql += "(" + key + " integer primary key" for f in fields : sql += ", " + f + " varchar(255)" sql += ")" self . _cur . execute ( sql ) self . _con . commit ( ) self . index ( name , key , unique = True ) self . connect ( self . _name )
9657	def get_direct_ancestors ( G , list_of_nodes ) : parents = [ ] for item in list_of_nodes : anc = G . predecessors ( item ) for one in anc : parents . append ( one ) return parents
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
11977	def get_bits ( self ) : return _convert ( self . _ip , notation = NM_BITS , inotation = IP_DOT , _check = False , _isnm = self . _isnm )
915	def normalize ( lx ) : lx = numpy . asarray ( lx ) base = lx . max ( ) x = numpy . exp ( lx - base ) result = x / x . sum ( ) conventional = ( numpy . exp ( lx ) / numpy . exp ( lx ) . sum ( ) ) assert similar ( result , conventional ) return result
6467	def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } subnet_list = db_api . subnet_find ( context = context . elevated ( ) , ** filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
5850	def matched_file_count ( self , dataset_id , glob = "." , is_dir = False ) : list_result = self . list_files ( dataset_id , glob , is_dir ) return len ( list_result )
7445	def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 4: Joint estimation of error rate and heterozygosity" ) samples = _get_samples ( self , samples ) if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return assemble . jointestimate . run ( self , samples , force , ipyclient )
8131	def merge ( self , layers ) : layers . sort ( ) if layers [ 0 ] == 0 : del layers [ 0 ] self . flatten ( layers )
5404	def _get_delocalization_env ( self , outputs , user_project ) : non_empty_outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT_COUNT' : str ( len ( non_empty_outputs ) ) } for idx , var in enumerate ( non_empty_outputs ) : env [ 'OUTPUT_{}' . format ( idx ) ] = var . name env [ 'OUTPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT_SRC_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
8613	def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method = 'DELETE' ) return response
6339	def lcsseq ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) for i , src_char in enumerate ( src ) : for j , tar_char in enumerate ( tar ) : if src_char == tar_char : lengths [ i + 1 , j + 1 ] = lengths [ i , j ] + 1 else : lengths [ i + 1 , j + 1 ] = max ( lengths [ i + 1 , j ] , lengths [ i , j + 1 ] ) result = '' i , j = len ( src ) , len ( tar ) while i != 0 and j != 0 : if lengths [ i , j ] == lengths [ i - 1 , j ] : i -= 1 elif lengths [ i , j ] == lengths [ i , j - 1 ] : j -= 1 else : result = src [ i - 1 ] + result i -= 1 j -= 1 return result
13151	def log_update ( entity , update ) : p = { 'on' : entity , 'update' : update } _log ( TYPE_CODES . UPDATE , p )
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
797	def jobUpdateResults ( self , jobID , results ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), ' ' results=%%s ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ results , jobID ] )
4252	def org_by_addr ( self , addr ) : valid = ( const . ORG_EDITION , const . ISP_EDITION , const . ASNUM_EDITION , const . ASNUM_EDITION_V6 ) if self . _databaseType not in valid : message = 'Invalid database type, expected Org, ISP or ASNum' raise GeoIPError ( message ) ipnum = util . ip2long ( addr ) return self . _get_org ( ipnum )
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
7086	def _single_true ( iterable ) : iterator = iter ( iterable ) has_true = any ( iterator ) has_another_true = any ( iterator ) return has_true and not has_another_true
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
8641	def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
11842	def ModelBasedVacuumAgent ( ) : "An agent that keeps track of what locations are clean or dirty." model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
8929	def pylint ( ctx , skip_tests = False , skip_root = False , reports = False ) : cfg = config . load ( ) add_dir2pypath ( cfg . project_root ) if not os . path . exists ( cfg . testjoin ( '__init__.py' ) ) : add_dir2pypath ( cfg . testjoin ( ) ) namelist = set ( ) for package in cfg . project . get ( 'packages' , [ ] ) : if '.' not in package : namelist . add ( cfg . srcjoin ( package ) ) for module in cfg . project . get ( 'py_modules' , [ ] ) : namelist . add ( module + '.py' ) if not skip_tests : test_py = antglob . FileSet ( cfg . testdir , '**/*.py' ) test_py = [ cfg . testjoin ( i ) for i in test_py ] if test_py : namelist |= set ( test_py ) if not skip_root : root_py = antglob . FileSet ( '.' , '*.py' ) if root_py : namelist |= set ( root_py ) namelist = set ( [ i [ len ( os . getcwd ( ) ) + 1 : ] if i . startswith ( os . getcwd ( ) + os . sep ) else i for i in namelist ] ) cmd = 'pylint' cmd += ' "{}"' . format ( '" "' . join ( sorted ( namelist ) ) ) cmd += ' --reports={0}' . format ( 'y' if reports else 'n' ) for cfgfile in ( '.pylintrc' , 'pylint.rc' , 'pylint.cfg' , 'project.d/pylint.cfg' ) : if os . path . exists ( cfgfile ) : cmd += ' --rcfile={0}' . format ( cfgfile ) break try : shell . run ( cmd , report_error = False , runner = ctx . run ) notify . info ( "OK - No problems found by pylint." ) except exceptions . Failure as exc : if exc . result . return_code & 32 : notify . error ( "Usage error, bad arguments in {}?!" . format ( repr ( cmd ) ) ) raise else : bits = { 1 : "fatal" , 2 : "error" , 4 : "warning" , 8 : "refactor" , 16 : "convention" , } notify . warning ( "Some messages of type {} issued by pylint." . format ( ", " . join ( [ text for bit , text in bits . items ( ) if exc . result . return_code & bit ] ) ) ) if exc . result . return_code & 3 : notify . error ( "Exiting due to fatal / error message." ) raise
6258	def update ( self , aspect_ratio = None , fov = None , near = None , far = None ) : self . aspect_ratio = aspect_ratio or self . aspect_ratio self . fov = fov or self . fov self . near = near or self . near self . far = far or self . far self . matrix = Matrix44 . perspective_projection ( self . fov , self . aspect_ratio , self . near , self . far )
8013	async def _create_upstream_applications ( self ) : loop = asyncio . get_event_loop ( ) for steam_name , ApplicationsCls in self . applications . items ( ) : application = ApplicationsCls ( self . scope ) upstream_queue = asyncio . Queue ( ) self . application_streams [ steam_name ] = upstream_queue self . application_futures [ steam_name ] = loop . create_task ( application ( upstream_queue . get , partial ( self . dispatch_downstream , steam_name = steam_name ) ) )
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
8371	def run ( src , grammar = NODEBOX , format = None , outputfile = None , iterations = 1 , buff = None , window = True , title = None , fullscreen = None , close_window = False , server = False , port = 7777 , show_vars = False , vars = None , namespace = None , run_shell = False , args = [ ] , verbose = False , background_thread = True ) : sys . argv = [ sys . argv [ 0 ] ] + args create_args = [ src , grammar , format , outputfile , iterations , buff , window , title , fullscreen , server , port , show_vars ] create_kwargs = dict ( vars = vars , namespace = namespace ) run_args = [ src ] run_kwargs = dict ( iterations = iterations , frame_limiter = window , verbose = verbose , run_forever = window and not ( close_window or bool ( outputfile ) ) , ) if background_thread : sbot_thread = ShoebotThread ( create_args = create_args , create_kwargs = create_kwargs , run_args = run_args , run_kwargs = run_kwargs , send_sigint = run_shell ) sbot_thread . start ( ) sbot = sbot_thread . sbot else : print ( 'background thread disabled' ) if run_shell : raise ValueError ( 'UI Must run in a separate thread to shell and shell needs main thread' ) sbot_thread = None sbot = create_bot ( * create_args , ** create_kwargs ) sbot . run ( * run_args , ** run_kwargs ) if run_shell : import shoebot . sbio . shell shell = shoebot . sbio . shell . ShoebotCmd ( sbot , trusted = True ) try : shell . cmdloop ( ) except KeyboardInterrupt as e : publish_event ( QUIT_EVENT ) if verbose : raise else : return elif background_thread : try : while sbot_thread . is_alive ( ) : sleep ( 1 ) except KeyboardInterrupt : publish_event ( QUIT_EVENT ) if all ( ( background_thread , sbot_thread ) ) : sbot_thread . join ( ) return sbot
10262	def _collapse_edge_passing_predicates ( graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : for u , v , _ in filter_edges ( graph , edge_predicates = edge_predicates ) : collapse_pair ( graph , survivor = u , victim = v )
12152	def html_single_plot ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_plot.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' for fname in filesByType [ 'plot' ] : html += self . htmlFor ( fname ) print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
801	def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : assert len ( fields ) >= 1 , "fields is empty" with ConnectionFactory . get ( ) as conn : dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] dbFieldStr = ", " . join ( dbFields ) query = 'SELECT model_id, {fields} from {models}' ' WHERE job_id=%s AND model_checkpoint_id IS NOT NULL' . format ( fields = dbFieldStr , models = self . modelsTableName ) conn . cursor . execute ( query , [ jobID ] ) rows = conn . cursor . fetchall ( ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
3567	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_started . clear ( ) self . _adapter . StartDiscovery ( ) if not self . _scan_started . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to start scanning!' )
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
5687	def stop ( self , stop_I ) : return pd . read_sql_query ( "SELECT * FROM stops WHERE stop_I={stop_I}" . format ( stop_I = stop_I ) , self . conn )
4193	def plot_frequencies ( self , mindB = None , maxdB = None , norm = True ) : from pylab import plot , title , xlim , grid , ylim , xlabel , ylabel self . compute_response ( norm = norm ) plot ( self . frequencies , self . response ) title ( "ENBW=%2.1f" % ( self . enbw ) ) ylabel ( 'Frequency response (dB)' ) xlabel ( 'Fraction of sampling frequency' ) xlim ( - 0.5 , 0.5 ) y0 , y1 = ylim ( ) if mindB : y0 = mindB if maxdB is not None : y1 = maxdB else : y1 = max ( self . response ) ylim ( y0 , y1 ) grid ( True )
2175	def refresh_token ( self , token_url , refresh_token = None , body = "" , auth = None , timeout = None , headers = None , verify = True , proxies = None , ** kwargs ) : if not token_url : raise ValueError ( "No token endpoint set for auto_refresh." ) if not is_secure_transport ( token_url ) : raise InsecureTransportError ( ) refresh_token = refresh_token or self . token . get ( "refresh_token" ) log . debug ( "Adding auto refresh key word arguments %s." , self . auto_refresh_kwargs ) kwargs . update ( self . auto_refresh_kwargs ) body = self . _client . prepare_refresh_body ( body = body , refresh_token = refresh_token , scope = self . scope , ** kwargs ) log . debug ( "Prepared refresh token request body %s" , body ) if headers is None : headers = { "Accept" : "application/json" , "Content-Type" : ( "application/x-www-form-urlencoded;charset=UTF-8" ) , } r = self . post ( token_url , data = dict ( urldecode ( body ) ) , auth = auth , timeout = timeout , headers = headers , verify = verify , withhold_token = True , proxies = proxies , ) log . debug ( "Request to refresh token completed with status %s." , r . status_code ) log . debug ( "Response headers were %s and content %s." , r . headers , r . text ) log . debug ( "Invoking %d token response hooks." , len ( self . compliance_hook [ "refresh_token_response" ] ) , ) for hook in self . compliance_hook [ "refresh_token_response" ] : log . debug ( "Invoking hook %s." , hook ) r = hook ( r ) self . token = self . _client . parse_request_body_response ( r . text , scope = self . scope ) if not "refresh_token" in self . token : log . debug ( "No new refresh token given. Re-using old." ) self . token [ "refresh_token" ] = refresh_token return self . token
13227	def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , ** kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , ** kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , ** kwargs ) return _decorator_helper return _decorator
3831	async def search_entities ( self , search_entities_request ) : response = hangouts_pb2 . SearchEntitiesResponse ( ) await self . _pb_request ( 'contacts/searchentities' , search_entities_request , response ) return response
10181	def _events_process ( event_types = None , eager = False ) : event_types = event_types or list ( current_stats . enabled_events ) if eager : process_events . apply ( ( event_types , ) , throw = True ) click . secho ( 'Events processed successfully.' , fg = 'green' ) else : process_events . delay ( event_types ) click . secho ( 'Events processing task sent...' , fg = 'yellow' )
1662	def FilesBelongToSameModule ( filename_cc , filename_h ) : fileinfo_cc = FileInfo ( filename_cc ) if not fileinfo_cc . Extension ( ) . lstrip ( '.' ) in GetNonHeaderExtensions ( ) : return ( False , '' ) fileinfo_h = FileInfo ( filename_h ) if not fileinfo_h . Extension ( ) . lstrip ( '.' ) in GetHeaderExtensions ( ) : return ( False , '' ) filename_cc = filename_cc [ : - ( len ( fileinfo_cc . Extension ( ) ) ) ] matched_test_suffix = Search ( _TEST_FILE_SUFFIX , fileinfo_cc . BaseName ( ) ) if matched_test_suffix : filename_cc = filename_cc [ : - len ( matched_test_suffix . group ( 1 ) ) ] filename_cc = filename_cc . replace ( '/public/' , '/' ) filename_cc = filename_cc . replace ( '/internal/' , '/' ) filename_h = filename_h [ : - ( len ( fileinfo_h . Extension ( ) ) ) ] if filename_h . endswith ( '-inl' ) : filename_h = filename_h [ : - len ( '-inl' ) ] filename_h = filename_h . replace ( '/public/' , '/' ) filename_h = filename_h . replace ( '/internal/' , '/' ) files_belong_to_same_module = filename_cc . endswith ( filename_h ) common_path = '' if files_belong_to_same_module : common_path = filename_cc [ : - len ( filename_h ) ] return files_belong_to_same_module , common_path
1816	def SETNO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) )
12898	def set_mute ( self , value = False ) : mute = ( yield from self . handle_set ( self . API . get ( 'mute' ) , int ( value ) ) ) return bool ( mute )
10376	def calculate_concordance ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , use_ambiguous : bool = False ) -> float : correct , incorrect , ambiguous , _ = calculate_concordance_helper ( graph , key , cutoff = cutoff ) try : return correct / ( correct + incorrect + ( ambiguous if use_ambiguous else 0 ) ) except ZeroDivisionError : return - 1.0
8941	def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( ** url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( ** data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
5149	def _add_file ( self , tar , name , contents , mode = DEFAULT_FILE_MODE ) : byte_contents = BytesIO ( contents . encode ( 'utf8' ) ) info = tarfile . TarInfo ( name = name ) info . size = len ( contents ) info . mtime = 0 info . type = tarfile . REGTYPE info . mode = int ( mode , 8 ) tar . addfile ( tarinfo = info , fileobj = byte_contents )
6495	def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( ** self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
7622	def tempo ( ref , est , ** kwargs ) : r ref = coerce_annotation ( ref , 'tempo' ) est = coerce_annotation ( est , 'tempo' ) ref_tempi = np . asarray ( [ o . value for o in ref ] ) ref_weight = ref . data [ 0 ] . confidence est_tempi = np . asarray ( [ o . value for o in est ] ) return mir_eval . tempo . evaluate ( ref_tempi , ref_weight , est_tempi , ** kwargs )
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
13717	def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
8732	def divide_timedelta_float ( td , divisor ) : dsm = [ getattr ( td , attr ) for attr in ( 'days' , 'seconds' , 'microseconds' ) ] dsm = map ( lambda elem : elem / divisor , dsm ) return datetime . timedelta ( * dsm )
11192	def item ( proto_dataset_uri , input_file , relpath_in_dataset ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( proto_dataset_uri , config_path = CONFIG_PATH ) if relpath_in_dataset == "" : relpath_in_dataset = os . path . basename ( input_file ) proto_dataset . put_item ( input_file , relpath_in_dataset )
6765	def load_table ( self , table_name , src , dst = 'localhost' , name = None , site = None ) : r = self . database_renderer ( name = name , site = site ) r . env . table_name = table_name r . run ( 'psql --user={dst_db_user} --host={dst_db_host} --command="DROP TABLE IF EXISTS {table_name} CASCADE;"' ) r . run ( 'pg_dump -t {table_name} --user={dst_db_user} --host={dst_db_host} | psql --user={src_db_user} --host={src_db_host}' )
111	def warn_deprecated ( msg , stacklevel = 2 ) : import warnings warnings . warn ( msg , category = DeprecationWarning , stacklevel = stacklevel )
3661	def calculate ( self , T , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate ( T ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate ( T ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate ( T ) elif method == COOLPROP : return CoolProp_T_dependent_property ( T , self . CASRN , 'CPMOLAR' , 'l' ) elif method == POLING_CONST : return self . POLING_constant elif method == CRCSTD : return self . CRCSTD_constant elif method == ROWLINSON_POLING : Cpgm = self . Cpgm ( T ) if hasattr ( self . Cpgm , '__call__' ) else self . Cpgm return Rowlinson_Poling ( T , self . Tc , self . omega , Cpgm ) elif method == ROWLINSON_BONDI : Cpgm = self . Cpgm ( T ) if hasattr ( self . Cpgm , '__call__' ) else self . Cpgm return Rowlinson_Bondi ( T , self . Tc , self . omega , Cpgm ) elif method == DADGOSTAR_SHAW : Cp = Dadgostar_Shaw ( T , self . similarity_variable ) return property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : return self . interpolate ( T , method ) else : raise Exception ( 'Method not valid' )
503	def _labelListToCategoryNumber ( self , labelList ) : categoryNumber = 0 for label in labelList : categoryNumber += self . _labelToCategoryNumber ( label ) return categoryNumber
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
8007	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) stanza = Presence ( stanza_type = "error" , from_jid = self . from_jid , to_jid = self . to_jid , stanza_id = self . stanza_id , status = self . _status , show = self . _show , priority = self . _priority , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : stanza . add_payload ( payload ) return stanza
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
3859	def update_conversation ( self , conversation ) : new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
11407	def record_get_field_instances ( rec , tag = "" , ind1 = " " , ind2 = " " ) : if not rec : return [ ] if not tag : return rec . items ( ) else : out = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if '%' in tag : for field_tag in rec : if _tag_matches_pattern ( field_tag , tag ) : for possible_field_instance in rec [ field_tag ] : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) else : for possible_field_instance in rec . get ( tag , [ ] ) : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) return out
352	def load_celebA_dataset ( path = 'data' ) : data_dir = 'celebA' filename , drive_id = "img_align_celeba.zip" , "0B7EVK8r0v71pZjFTYXZWM3FlRnM" save_path = os . path . join ( path , filename ) image_path = os . path . join ( path , data_dir ) if os . path . exists ( image_path ) : logging . info ( '[*] {} already exists' . format ( save_path ) ) else : exists_or_mkdir ( path ) download_file_from_google_drive ( drive_id , save_path ) zip_dir = '' with zipfile . ZipFile ( save_path ) as zf : zip_dir = zf . namelist ( ) [ 0 ] zf . extractall ( path ) os . remove ( save_path ) os . rename ( os . path . join ( path , zip_dir ) , image_path ) data_files = load_file_list ( path = image_path , regx = '\\.jpg' , printable = False ) for i , _v in enumerate ( data_files ) : data_files [ i ] = os . path . join ( image_path , data_files [ i ] ) return data_files
11447	def _login ( self , session , get_request = False ) : req = session . post ( self . _login_url , data = self . _logindata ) if _LOGIN_ERROR_STRING in req . text or req . status_code == 403 or req . url == _LOGIN_URL : err_mess = "YesssSMS: login failed, username or password wrong" if _LOGIN_LOCKED_MESS in req . text : err_mess += ", page says: " + _LOGIN_LOCKED_MESS_ENG self . _suspended = True raise self . AccountSuspendedError ( err_mess ) raise self . LoginError ( err_mess ) self . _suspended = False return ( session , req ) if get_request else session
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
6839	def distrib_release ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : return run ( 'lsb_release -r --short' ) elif kernel == SUNOS : return run ( 'uname -v' )
6142	def DSP_capture_add_samples ( self , new_data ) : self . capture_sample_count += len ( new_data ) if self . Tcapture > 0 : self . data_capture = np . hstack ( ( self . data_capture , new_data ) ) if ( self . Tcapture > 0 ) and ( len ( self . data_capture ) > self . Ncapture ) : self . data_capture = self . data_capture [ - self . Ncapture : ]
3952	def read_adc ( self , channel , gain = 1 , data_rate = None ) : assert 0 <= channel <= 3 , 'Channel must be a value within 0-3!' return self . _read ( channel + 0x04 , gain , data_rate , ADS1x15_CONFIG_MODE_SINGLE )
11298	def get_all_text ( node ) : if node . nodeType == node . TEXT_NODE : return node . data else : text_string = "" for child_node in node . childNodes : text_string += get_all_text ( child_node ) return text_string
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
4434	async def update_state ( self , data ) : guild_id = int ( data [ 'guildId' ] ) if guild_id in self . players : player = self . players . get ( guild_id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position_timestamp = data [ 'state' ] [ 'time' ]
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
2961	def __write ( self , containers , initialize = True ) : path = self . _state_file self . _assure_dir ( ) try : flags = os . O_WRONLY | os . O_CREAT if initialize : flags |= os . O_EXCL with os . fdopen ( os . open ( path , flags ) , "w" ) as f : yaml . safe_dump ( self . __base_state ( containers ) , f ) except OSError as err : if err . errno == errno . EEXIST : raise AlreadyInitializedError ( "Path %s exists. " "You may need to destroy a previous blockade." % path ) raise except Exception : self . _state_delete ( ) raise
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( ** pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
5532	def batch_process ( self , zoom = None , tile = None , multi = cpu_count ( ) , max_chunksize = 1 ) : list ( self . batch_processor ( zoom , tile , multi , max_chunksize ) )
1890	def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
875	def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )
1240	def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
10150	def generate ( self , title = None , version = None , base_path = None , info = None , swagger = None , ** kwargs ) : title = title or self . api_title version = version or self . api_version info = info or self . swagger . get ( 'info' , { } ) swagger = swagger or self . swagger base_path = base_path or self . base_path swagger = swagger . copy ( ) info . update ( title = title , version = version ) swagger . update ( swagger = '2.0' , info = info , basePath = base_path ) paths , tags = self . _build_paths ( ) if tags : swagger . setdefault ( 'tags' , [ ] ) tag_names = { t [ 'name' ] for t in swagger [ 'tags' ] } for tag in tags : if tag [ 'name' ] not in tag_names : swagger [ 'tags' ] . append ( tag ) if paths : swagger . setdefault ( 'paths' , { } ) merge_dicts ( swagger [ 'paths' ] , paths ) definitions = self . definitions . definition_registry if definitions : swagger . setdefault ( 'definitions' , { } ) merge_dicts ( swagger [ 'definitions' ] , definitions ) parameters = self . parameters . parameter_registry if parameters : swagger . setdefault ( 'parameters' , { } ) merge_dicts ( swagger [ 'parameters' ] , parameters ) responses = self . responses . response_registry if responses : swagger . setdefault ( 'responses' , { } ) merge_dicts ( swagger [ 'responses' ] , responses ) return swagger
7391	def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . ax . set_ylim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . add_axes_and_nodes ( ) self . add_edges ( ) self . ax . axis ( 'off' )
420	def save_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . ValidLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] valid log: " + _log )
8292	def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list
9416	def to_value ( cls , instance ) : if not isinstance ( instance , OctaveUserClass ) or not instance . _attrs : return dict ( ) dtype = [ ] values = [ ] for attr in instance . _attrs : dtype . append ( ( str ( attr ) , object ) ) values . append ( getattr ( instance , attr ) ) struct = np . array ( [ tuple ( values ) ] , dtype ) return MatlabObject ( struct , instance . _name )
4686	def encrypt ( self , message ) : if not message : return None nonce = str ( random . getrandbits ( 64 ) ) try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( self . from_account [ "options" ] [ "memo_key" ] ) except KeyNotFound : raise MissingKeyError ( "Memo private key {} for {} could not be found" . format ( self . from_account [ "options" ] [ "memo_key" ] , self . from_account [ "name" ] ) ) if not memo_wif : raise MissingKeyError ( "Memo key for %s missing!" % self . from_account [ "name" ] ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix enc = memo . encode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( self . to_account [ "options" ] [ "memo_key" ] , prefix = self . chain_prefix ) , nonce , message , ) return { "message" : enc , "nonce" : nonce , "from" : self . from_account [ "options" ] [ "memo_key" ] , "to" : self . to_account [ "options" ] [ "memo_key" ] , }
12800	def is_text ( self ) : return self . type in [ self . _TYPE_PASTE , self . _TYPE_TEXT , self . _TYPE_TWEET ]
5750	def downloadURL ( url , filename ) : path_temp_bviewfile = os . path . join ( c . raw_data , c . bview_dir , 'tmp' , filename ) path_bviewfile = os . path . join ( c . raw_data , c . bview_dir , filename ) try : f = urlopen ( url ) except : return False if f . getcode ( ) != 200 : publisher . warning ( '{} unavailable, code: {}' . format ( url , f . getcode ( ) ) ) return False try : with open ( path_temp_bviewfile , 'w' ) as outfile : outfile . write ( f . read ( ) ) os . rename ( path_temp_bviewfile , path_bviewfile ) except : os . remove ( path_temp_bviewfile ) return False return True
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp if rstart_buff == rend_buff : rend_buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust
11233	def get_inner_template ( self , language , template_type , indentation , key , val ) : inner_templates = { 'php' : { 'iterable' : '%s%s => array \n%s( \n%s%s),\n' % ( indentation , key , indentation , val , indentation ) , 'singular' : '%s%s => %s, \n' % ( indentation , key , val ) } , 'javascript' : { 'iterable' : '%s%s : {\n%s\n%s},\n' % ( indentation , key , val , indentation ) , 'singular' : '%s%s: %s,\n' % ( indentation , key , val ) } , 'ocaml' : { 'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % ( indentation , key , val , indentation ) , 'singular' : '%s(%s, %s);\n' % ( indentation , key , val ) } } return inner_templates [ language ] [ template_type ]
9221	def mix ( self , color1 , color2 , weight = 50 , * args ) : if color1 and color2 : if isinstance ( weight , string_types ) : weight = float ( weight . strip ( '%' ) ) weight = ( ( weight / 100.0 ) * 2 ) - 1 rgb1 = self . _hextorgb ( color1 ) rgb2 = self . _hextorgb ( color2 ) alpha = 0 w1 = ( ( ( weight if weight * alpha == - 1 else weight + alpha ) / ( 1 + weight * alpha ) ) + 1 ) w1 = w1 / 2.0 w2 = 1 - w1 rgb = [ rgb1 [ 0 ] * w1 + rgb2 [ 0 ] * w2 , rgb1 [ 1 ] * w1 + rgb2 [ 1 ] * w2 , rgb1 [ 2 ] * w1 + rgb2 [ 2 ] * w2 , ] return self . _rgbatohex ( rgb ) raise ValueError ( 'Illegal color values' )
773	def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
11059	def run ( self , start = True ) : if not self . is_setup : raise NotSetupError self . webserver . start ( ) first_connect = True try : while self . runnable : if self . reconnect_needed : if not self . sc . rtm_connect ( with_team_state = start ) : return False self . reconnect_needed = False if first_connect : first_connect = False self . plugins . connect ( ) try : events = self . sc . rtm_read ( ) except AttributeError : self . log . exception ( 'Something has failed in the slack rtm library. This is fatal.' ) self . runnable = False events = [ ] except : self . log . exception ( 'Unhandled exception in rtm_read()' ) self . reconnect_needed = True events = [ ] for e in events : try : self . _handle_event ( e ) except KeyboardInterrupt : self . runnable = False except : self . log . exception ( 'Unhandled exception in event handler' ) sleep ( 0.1 ) except KeyboardInterrupt : pass except : self . log . exception ( 'Unhandled exception' )
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
490	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) self . _conn . _ping_check ( ) connWrap = ConnectionWrapper ( dbConn = self . _conn , cursor = self . _conn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
11033	def parse ( self , value : str , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , ) -> typing . Any : if type_ is bool : return type_ ( value . lower ( ) in self . TRUE_STRINGS ) try : if isinstance ( type_ , type ) and issubclass ( type_ , ( list , tuple , set , frozenset ) ) : return type_ ( self . parse ( v . strip ( " " ) , subtype ) for v in value . split ( "," ) if value . strip ( " " ) ) return type_ ( value ) except ValueError as e : raise ConfigError ( * e . args )
6759	def get_package_list ( self ) : os_version = self . os_version self . vprint ( 'os_version:' , os_version ) req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
1528	def pick_unused_port ( self ) : s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) s . bind ( ( '127.0.0.1' , 0 ) ) _ , port = s . getsockname ( ) s . close ( ) return port
5028	def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
10033	def join_phonemes ( * args ) : if len ( args ) == 1 : args = args [ 0 ] if len ( args ) == 2 : args += ( CODAS [ 0 ] , ) try : onset , nucleus , coda = args except ValueError : raise TypeError ( 'join_phonemes() takes at most 3 arguments' ) offset = ( ( ONSETS . index ( onset ) * NUM_NUCLEUSES + NUCLEUSES . index ( nucleus ) ) * NUM_CODAS + CODAS . index ( coda ) ) return unichr ( FIRST_HANGUL_OFFSET + offset )
2331	def normal_noise ( points ) : return np . random . rand ( 1 ) * np . random . randn ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
4368	def error ( self , error_name , error_message , msg_id = None , quiet = False ) : self . socket . error ( error_name , error_message , endpoint = self . ns_name , msg_id = msg_id , quiet = quiet )
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) width , height = g [ 2 ] , g [ 3 ] return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
6882	def describe_lcc_csv ( lcdict , returndesc = False ) : metadata_lines = [ ] coldef_lines = [ ] if 'lcformat' in lcdict and 'lcc-csv' in lcdict [ 'lcformat' ] . lower ( ) : metadata = lcdict [ 'metadata' ] metakeys = lcdict [ 'objectinfo' ] . keys ( ) coldefs = lcdict [ 'coldefs' ] for mk in metakeys : metadata_lines . append ( '%20s | %s' % ( mk , metadata [ mk ] [ 'desc' ] ) ) for ck in lcdict [ 'columns' ] : coldef_lines . append ( 'column %02d | %8s | numpy dtype: %3s | %s' % ( coldefs [ ck ] [ 'colnum' ] , ck , coldefs [ ck ] [ 'dtype' ] , coldefs [ ck ] [ 'desc' ] ) ) desc = LCC_CSVLC_DESCTEMPLATE . format ( objectid = lcdict [ 'objectid' ] , metadata_desc = '\n' . join ( metadata_lines ) , metadata = pformat ( lcdict [ 'objectinfo' ] ) , columndefs = '\n' . join ( coldef_lines ) ) print ( desc ) if returndesc : return desc else : LOGERROR ( "this lcdict is not from an LCC CSV, can't figure it out..." ) return None
9568	def get_chat_id ( self , message ) : if message . chat . type == 'private' : return message . user . id return message . chat . id
3442	def to_json ( model , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC return json . dumps ( obj , allow_nan = False , ** kwargs )
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) tags = [ t for t in tags if t ] return tags
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
2601	def start ( self ) : if self . mode == "manual" : return if self . ipython_dir != '~/.ipython' : self . ipython_dir = os . path . abspath ( os . path . expanduser ( self . ipython_dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython_dir , "{0}.controller.out" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython_dir , "{0}.controller.err" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython_dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython_dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public_ip ) if self . public_ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port_range is not None : opts += [ '--HubFactory.hb={0},{1}' . format ( self . hb_ping , self . hb_pong ) , '--HubFactory.control={0},{1}' . format ( self . control_client , self . control_engine ) , '--HubFactory.mux={0},{1}' . format ( self . mux_client , self . mux_engine ) , '--HubFactory.task={0},{1}' . format ( self . task_client , self . task_engine ) ] logger . debug ( "Starting ipcontroller with '{}'" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec_fn = os . setsid ) except FileNotFoundError : msg = "Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env" logger . error ( msg ) raise ControllerError ( msg ) except Exception as e : msg = "IPPController failed to start: {0}" . format ( e ) logger . error ( msg ) raise ControllerError ( msg )
10804	def resolve_admin_type ( admin ) : if admin is current_user or isinstance ( admin , UserMixin ) : return 'User' else : return admin . __class__ . __name__
8961	def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( "python setup.py build" ) if docs : for doc_path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc_path , 'conf.py' ) ) : break else : doc_path = None if doc_path : ctx . run ( "invoke docs" ) else : notify . warning ( "Cannot find either a 'docs' or 'doc' Sphinx directory!" )
10268	def main ( output ) : from hbp_knowledge import get_graph graph = get_graph ( ) text = to_html ( graph ) print ( text , file = output )
791	def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' completion_reason=%%s, ' ' completion_msg=%%s, ' ' end_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of jobID=%s to " "completed, but this job could not be found or " "belongs to some other CJM" % ( jobID ) )
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
4021	def _start_docker_vm ( ) : is_running = docker_vm_is_running ( ) if not is_running : log_to_client ( 'Starting docker-machine VM {}' . format ( constants . VM_MACHINE_NAME ) ) _apply_nat_dns_host_resolver ( ) _apply_nat_net_less_greedy_subnet ( ) check_and_log_output_and_error_demoted ( [ 'docker-machine' , 'start' , constants . VM_MACHINE_NAME ] , quiet_on_success = True ) return is_running
1363	def get_argument_offset ( self ) : try : offset = self . get_argument ( constants . PARAM_OFFSET ) return offset except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
8674	def export_keys ( output_path , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Exporting stash to {0}...' . format ( output_path ) ) stash . export ( output_path = output_path ) click . echo ( 'Export complete!' ) except GhostError as ex : sys . exit ( ex )
3460	def single_reaction_deletion ( model , reaction_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'reaction' , element_lists = _element_lists ( model . reactions , reaction_list ) , method = method , solution = solution , processes = processes , ** kwargs )
8389	def write ( self , text , hashline = b"# {}" ) : u if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
5478	def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
3305	def _run_gevent ( app , config , mode ) : import gevent import gevent . monkey gevent . monkey . patch_all ( ) from gevent . pywsgi import WSGIServer server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "keyfile" : None , "certfile" : None , } protocol = "http" server_args . update ( config . get ( "server_args" , { } ) ) dav_server = WSGIServer ( server_args [ "bind_addr" ] , app ) _logger . info ( "Running {}" . format ( dav_server ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) try : gevent . spawn ( dav_server . serve_forever ( ) ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
8571	def get_nic ( self , datacenter_id , server_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s?depth=%s' % ( datacenter_id , server_id , nic_id , str ( depth ) ) ) return response
2399	def initialize_dictionaries ( self , e_set , max_feats2 = 200 ) : if ( hasattr ( e_set , '_type' ) ) : if ( e_set . _type == "train" ) : nvocab = util_functions . get_vocab ( e_set . _text , e_set . _score , max_feats2 = max_feats2 ) svocab = util_functions . get_vocab ( e_set . _clean_stem_text , e_set . _score , max_feats2 = max_feats2 ) self . _normal_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = nvocab ) self . _stem_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = svocab ) self . dict_initialized = True self . _mean_spelling_errors = sum ( e_set . _spelling_errors ) / float ( len ( e_set . _spelling_errors ) ) self . _spell_errors_per_character = sum ( e_set . _spelling_errors ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) self . _grammar_errors_per_character = ( sum ( good_pos_tags ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ) bag_feats = self . gen_bag_feats ( e_set ) f_row_sum = numpy . sum ( bag_feats [ : , : ] ) self . _mean_f_prop = f_row_sum / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ret = "ok" else : raise util_functions . InputError ( e_set , "needs to be an essay set of the train type." ) else : raise util_functions . InputError ( e_set , "wrong input. need an essay set object" ) return ret
9760	def resources ( ctx , job , gpu ) : def get_experiment_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment . resources ( user , project_name , _experiment , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_resources ( ) : try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . experiment_job . resources ( user , project_name , _experiment , _job , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_resources ( ) else : get_experiment_resources ( )
6139	def get_is_sim_running ( self ) : sim_info = self . simulation_info ( ) try : progress_info = sim_info [ 'simulation_info_progress' ] ret = progress_info [ 'simulation_progress_is_running' ] except KeyError : ret = False return ret
2947	def deserialize ( cls , serializer , wf_spec , s_state , ** kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , ** kwargs )
1052	def print_stack ( f = None , limit = None , file = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back print_list ( extract_stack ( f , limit ) , file )
7926	def shuffle_srv ( records ) : if not records : return [ ] ret = [ ] while len ( records ) > 1 : weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 thres = random . random ( ) * weight_sum weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 if thres < weight_sum : records . remove ( rrecord ) ret . append ( rrecord ) break ret . append ( records [ 0 ] ) return ret
1079	def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . _year if month is None : month = self . _month if day is None : day = self . _day return date . __new__ ( type ( self ) , year , month , day )
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
3042	def get_access_token ( self , http = None ) : if not self . access_token or self . access_token_expired : if not http : http = transport . get_http_object ( ) self . refresh ( http ) return AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) )
6512	def _set ( self , name , gender , country_values ) : if '+' in name : for replacement in [ '' , ' ' , '-' ] : self . _set ( name . replace ( '+' , replacement ) , gender , country_values ) else : if name not in self . names : self . names [ name ] = { } self . names [ name ] [ gender ] = country_values
5690	def read_data_as_dataframe ( self , travel_impedance_measure , from_stop_I = None , to_stop_I = None , statistic = None ) : to_select = [ ] where_clauses = [ ] to_select . append ( "from_stop_I" ) to_select . append ( "to_stop_I" ) if from_stop_I is not None : where_clauses . append ( "from_stop_I=" + str ( int ( from_stop_I ) ) ) if to_stop_I is not None : where_clauses . append ( "to_stop_I=" + str ( int ( to_stop_I ) ) ) where_clause = "" if len ( where_clauses ) > 0 : where_clause = " WHERE " + " AND " . join ( where_clauses ) if not statistic : to_select . extend ( [ "min" , "mean" , "median" , "max" ] ) else : to_select . append ( statistic ) to_select_clause = "," . join ( to_select ) if not to_select_clause : to_select_clause = "*" sql = "SELECT " + to_select_clause + " FROM " + travel_impedance_measure + where_clause + ";" df = pd . read_sql ( sql , self . conn ) return df
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
1010	def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , minNumSyns ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold nSegsRemoved , nSynsRemoved = 0 , 0 segsToDel = [ ] for segment in segList : synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] if len ( synsToDel ) == len ( segment . syns ) : segsToDel . append ( segment ) else : if len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) nSynsRemoved += 1 if len ( segment . syns ) < minNumSyns : segsToDel . append ( segment ) nSegsRemoved += len ( segsToDel ) for seg in segsToDel : self . _cleanUpdatesList ( colIdx , cellIdx , seg ) self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) nSynsRemoved += len ( seg . syns ) return nSegsRemoved , nSynsRemoved
3153	def all ( self , list_id , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , ** queryparams )
9187	def includeme ( config ) : settings = config . registry . settings session_factory = SignedCookieSessionFactory ( settings [ 'session_key' ] ) config . set_session_factory ( session_factory )
3119	def get_prep_value ( self , value ) : if value is None : return None else : return encoding . smart_text ( base64 . b64encode ( jsonpickle . encode ( value ) . encode ( ) ) )
7521	def write_str ( data , sidx , pnames ) : start = time . time ( ) tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : snparr = io5 [ "snparr" ] bisarr = io5 [ "bisarr" ] bend = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( bend ) : bend = bend . min ( ) else : bend = bisarr . shape [ 1 ] send = np . where ( np . all ( snparr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( send ) : send = send . min ( ) else : send = snparr . shape [ 1 ] out1 = open ( data . outfiles . str , 'w' ) out2 = open ( data . outfiles . ustr , 'w' ) numdict = { 'A' : '0' , 'T' : '1' , 'G' : '2' , 'C' : '3' , 'N' : '-9' , '-' : '-9' } if data . paramsdict [ "max_alleles_consens" ] > 1 : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 1 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) else : for idx , name in enumerate ( pnames ) : out1 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in snparr [ idx , : send ] ] ) ) ) out2 . write ( "{}\t\t\t\t\t{}\n" . format ( name , "\t" . join ( [ numdict [ DUCT [ i ] [ 0 ] ] for i in bisarr [ idx , : bend ] ] ) ) ) out1 . close ( ) out2 . close ( ) LOGGER . debug ( "finished writing str in: %s" , time . time ( ) - start )
2561	def recv_result_from_workers ( self ) : info = MPI . Status ( ) result = self . comm . recv ( source = MPI . ANY_SOURCE , tag = RESULT_TAG , status = info ) logger . debug ( "Received result from workers: {}" . format ( result ) ) return result
12840	async def async_connect ( self ) : if self . _waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . _waiters or self . max_connections and ( len ( self . _in_use ) >= self . max_connections ) : waiter = asyncio . Future ( loop = self . _loop ) self . _waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . _waiters . remove ( waiter ) self . connect ( ) return self . _state . conn
769	def _addResults ( self , results ) : if self . __isTemporal : shiftedInferences = self . __inferenceShifter . shift ( results ) . inferences self . __currentResult = copy . deepcopy ( results ) self . __currentResult . inferences = shiftedInferences self . __currentInference = shiftedInferences else : self . __currentResult = copy . deepcopy ( results ) self . __currentInference = copy . deepcopy ( results . inferences ) self . __currentGroundTruth = copy . deepcopy ( results )
10776	def finalize ( self , result = None ) : if not self . settings_path : return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
11620	def to_utf8 ( y ) : out = [ ] for x in y : if x < 0x080 : out . append ( x ) elif x < 0x0800 : out . append ( ( x >> 6 ) | 0xC0 ) out . append ( ( x & 0x3F ) | 0x80 ) elif x < 0x10000 : out . append ( ( x >> 12 ) | 0xE0 ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) else : out . append ( ( x >> 18 ) | 0xF0 ) out . append ( ( x >> 12 ) & 0x3F ) out . append ( ( ( x >> 6 ) & 0x3F ) | 0x80 ) out . append ( ( x & 0x3F ) | 0x80 ) return '' . join ( map ( chr , out ) )
7729	def get_muc_child ( self ) : if self . muc_child : return self . muc_child if not self . xmlnode . children : return None n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ( n . name , ns_uri ) == ( "x" , MUC_NS ) : self . muc_child = MucX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "x" , MUC_USER_NS ) : self . muc_child = MucUserX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_ADMIN_NS ) : self . muc_child = MucAdminQuery ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_OWNER_NS ) : self . muc_child = MucOwnerX ( n ) return self . muc_child n = n . next
9528	def pbkdf2 ( password , salt , iterations , dklen = 0 , digest = None ) : if digest is None : digest = settings . CRYPTOGRAPHY_DIGEST if not dklen : dklen = digest . digest_size password = force_bytes ( password ) salt = force_bytes ( salt ) kdf = PBKDF2HMAC ( algorithm = digest , length = dklen , salt = salt , iterations = iterations , backend = settings . CRYPTOGRAPHY_BACKEND ) return kdf . derive ( password )
1482	def get_commands_to_run ( self ) : if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) commands . update ( self . _get_heron_support_processes ( ) ) return commands
7986	def registration_success ( self , stanza ) : _unused = stanza self . lock . acquire ( ) try : self . state_change ( "registered" , self . registration_form ) if ( 'FORM_TYPE' in self . registration_form and self . registration_form [ 'FORM_TYPE' ] . value == 'jabber:iq:register' ) : if 'username' in self . registration_form : self . my_jid = JID ( self . registration_form [ 'username' ] . value , self . my_jid . domain , self . my_jid . resource ) if 'password' in self . registration_form : self . password = self . registration_form [ 'password' ] . value self . registration_callback = None self . _post_connect ( ) finally : self . lock . release ( )
5444	def parse_uri ( self , raw_uri , recursive ) : if recursive : raw_uri = directory_fmt ( raw_uri ) file_provider = self . parse_file_provider ( raw_uri ) self . _validate_paths_or_fail ( raw_uri , recursive ) uri , docker_uri = self . rewrite_uris ( raw_uri , file_provider ) uri_parts = job_model . UriParts ( directory_fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker_uri , uri_parts , file_provider
13501	def update_time ( sender , ** kwargs ) : comment = kwargs [ 'instance' ] if comment . content_type . app_label == "happenings" and comment . content_type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object_pk ) item . save ( )
8862	def quick_doc ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_definitions ( ) except jedi . NotFoundError : return [ ] else : ret_val = [ d . docstring ( ) for d in definitions ] return ret_val
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
948	def run ( self ) : self . __logger . debug ( "run(): Starting task <%s>" , self . __task [ 'taskLabel' ] ) if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) self . __model . resetSequenceStates ( ) self . __taskDriver . setup ( ) while True : try : next ( iterTracker ) except StopIteration : break try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) periodic . tick ( ) self . _getAndEmitExperimentMetrics ( final = True ) self . __taskDriver . finalize ( ) self . __model . resetSequenceStates ( )
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
9641	def _display_details ( var_data ) : meta_keys = ( key for key in list ( var_data . keys ( ) ) if key . startswith ( 'META_' ) ) for key in meta_keys : display_key = key [ 5 : ] . capitalize ( ) pprint ( '{0}: {1}' . format ( display_key , var_data . pop ( key ) ) ) pprint ( var_data )
8037	def get_summarizer ( self , name ) : if name in self . summarizers : pass elif name == 'lexrank' : from . import lexrank self . summarizers [ name ] = lexrank . summarize elif name == 'mcp' : from . import mcp_summ self . summarizers [ name ] = mcp_summ . summarize return self . summarizers [ name ]
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
13202	def format_abstract ( self , format = 'html5' , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : if self . abstract is None : return None abstract_latex = self . _prep_snippet_for_pandoc ( self . abstract ) output_text = convert_lsstdoc_tex ( abstract_latex , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
10819	def get ( cls , group , user ) : try : m = cls . query . filter_by ( user_id = user . get_id ( ) , group = group ) . one ( ) return m except Exception : return None
6106	def masses_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
12616	def is_img ( obj ) : try : get_data = getattr ( obj , 'get_data' ) get_affine = getattr ( obj , 'get_affine' ) return isinstance ( get_data , collections . Callable ) and isinstance ( get_affine , collections . Callable ) except AttributeError : return False
2778	def remove_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = DELETE , params = { "forwarding_rules" : rules_dict } )
11205	def name_from_string ( self , tzname_str ) : if not tzname_str . startswith ( '@' ) : return tzname_str name_splt = tzname_str . split ( ',-' ) try : offset = int ( name_splt [ 1 ] ) except : raise ValueError ( "Malformed timezone string." ) return self . load_name ( offset )
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
1324	def threadFunc ( root ) : th = threading . currentThread ( ) auto . Logger . WriteLine ( '\nThis is running in a new thread. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan ) time . sleep ( 2 ) auto . InitializeUIAutomationInCurrentThread ( ) auto . GetConsoleWindow ( ) . CaptureToImage ( 'console_newthread.png' ) newRoot = auto . GetRootControl ( ) auto . EnumAndLogControl ( newRoot , 1 ) auto . UninitializeUIAutomationInCurrentThread ( ) auto . Logger . WriteLine ( '\nThread exits. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan )
7174	def _train_and_save ( obj , cache , data , print_updates ) : obj . train ( data ) if print_updates : print ( 'Regenerated ' + obj . name + '.' ) obj . save ( cache )
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
10477	def _leftMouseDragged ( self , stopCoord , strCoord , speed ) : appPid = self . _getPid ( ) if strCoord == ( 0 , 0 ) : loc = AppKit . NSEvent . mouseLocation ( ) strCoord = ( loc . x , Quartz . CGDisplayPixelsHigh ( 0 ) - loc . y ) appPid = self . _getPid ( ) pressLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDown , strCoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , pressLeftButton ) time . sleep ( 5 ) speed = round ( 1 / float ( speed ) , 2 ) xmoved = stopCoord [ 0 ] - strCoord [ 0 ] ymoved = stopCoord [ 1 ] - strCoord [ 1 ] if ymoved == 0 : raise ValueError ( 'Not support horizontal moving' ) else : k = abs ( ymoved / xmoved ) if xmoved != 0 : for xpos in range ( int ( abs ( xmoved ) ) ) : if xmoved > 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] + xpos * k ) elif xmoved > 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] + xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved < 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] - xpos * k ) elif xmoved < 0 and ymoved > 0 : currcoord = ( strCoord [ 0 ] - xpos , strCoord [ 1 ] + xpos * k ) dragLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseDragged , currcoord , Quartz . kCGMouseButtonLeft ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , dragLeftButton ) time . sleep ( speed ) else : raise ValueError ( 'Not support vertical moving' ) upLeftButton = Quartz . CGEventCreateMouseEvent ( None , Quartz . kCGEventLeftMouseUp , stopCoord , Quartz . kCGMouseButtonLeft ) time . sleep ( 5 ) Quartz . CGEventPost ( Quartz . CoreGraphics . kCGHIDEventTap , upLeftButton )
6236	def set_time ( self , value : float ) : if value < 0 : value = 0 mixer . music . set_pos ( value )
12210	def cache_result ( func ) : def cache_set ( key , value ) : cache . set ( key , value , AVATAR_CACHE_TIMEOUT ) return value def cached_func ( user , size ) : prefix = func . __name__ cached_funcs . add ( prefix ) key = get_cache_key ( user , size , prefix = prefix ) return cache . get ( key ) or cache_set ( key , func ( user , size ) ) return cached_func
10684	def S_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : s = 1 - ( self . _B_mag * ( 2 * tau ** 3 / 3 + 2 * tau ** 9 / 27 + 2 * tau ** 15 / 75 ) ) / self . _D_mag else : s = ( 2 * tau ** - 5 / 5 + 2 * tau ** - 15 / 45 + 2 * tau ** - 25 / 125 ) / self . _D_mag return - R * math . log ( self . beta0_mag + 1 ) * s
4137	def save_figures ( image_path , fig_count , gallery_conf ) : figure_list = [ ] fig_managers = matplotlib . _pylab_helpers . Gcf . get_all_fig_managers ( ) for fig_mngr in fig_managers : fig = plt . figure ( fig_mngr . num ) kwargs = { } to_rgba = matplotlib . colors . colorConverter . to_rgba for attr in [ 'facecolor' , 'edgecolor' ] : fig_attr = getattr ( fig , 'get_' + attr ) ( ) default_attr = matplotlib . rcParams [ 'figure.' + attr ] if to_rgba ( fig_attr ) != to_rgba ( default_attr ) : kwargs [ attr ] = fig_attr current_fig = image_path . format ( fig_count + fig_mngr . num ) fig . savefig ( current_fig , ** kwargs ) figure_list . append ( current_fig ) if gallery_conf . get ( 'find_mayavi_figures' , False ) : from mayavi import mlab e = mlab . get_engine ( ) last_matplotlib_fig_num = len ( figure_list ) total_fig_num = last_matplotlib_fig_num + len ( e . scenes ) mayavi_fig_nums = range ( last_matplotlib_fig_num , total_fig_num ) for scene , mayavi_fig_num in zip ( e . scenes , mayavi_fig_nums ) : current_fig = image_path . format ( mayavi_fig_num ) mlab . savefig ( current_fig , figure = scene ) scale_image ( current_fig , current_fig , 850 , 999 ) figure_list . append ( current_fig ) mlab . close ( all = True ) return figure_list
738	def compute ( self , activeColumns , learn = True ) : bottomUpInput = numpy . zeros ( self . numberOfCols , dtype = dtype ) bottomUpInput [ list ( activeColumns ) ] = 1 super ( TemporalMemoryShim , self ) . compute ( bottomUpInput , enableLearn = learn , enableInference = True ) predictedState = self . getPredictedState ( ) self . predictiveCells = set ( numpy . flatnonzero ( predictedState ) )
3350	def _generate_index ( self ) : self . _dict = { v . id : k for k , v in enumerate ( self ) }
868	def resetCustomConfig ( cls ) : _getLogger ( ) . info ( "Resetting all custom configuration properties; " "caller=%r" , traceback . format_stack ( ) ) super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = True )
10003	def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
12394	def try_delegation ( method ) : @ functools . wraps ( method ) def delegator ( self , * args , ** kwargs ) : if self . try_delegation : inst = getattr ( self , 'inst' , None ) if inst is not None : method_name = ( self . delegator_prefix or '' ) + method . __name__ func = getattr ( inst , method_name , None ) if func is not None : return func ( * args , ** kwargs ) return method ( self , * args , ** kwargs ) return delegator
10319	def _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) : ret = dict ( ) runs = max_cluster_size . size sqrt_n = np . sqrt ( runs ) max_cluster_size_sample_mean = max_cluster_size . mean ( ) ret [ 'max_cluster_size' ] = max_cluster_size_sample_mean max_cluster_size_sample_std = max_cluster_size . std ( ddof = 1 ) if max_cluster_size_sample_std : old_settings = np . seterr ( all = 'raise' ) ret [ 'max_cluster_size_ci' ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = max_cluster_size_sample_mean , scale = max_cluster_size_sample_std / sqrt_n ) np . seterr ( ** old_settings ) else : ret [ 'max_cluster_size_ci' ] = ( max_cluster_size_sample_mean * np . ones ( 2 ) ) return ret
9587	def isarray ( array , test , dim = 2 ) : if dim > 1 : return all ( isarray ( array [ i ] , test , dim - 1 ) for i in range ( len ( array ) ) ) return all ( test ( i ) for i in array )
3811	def get_request_header ( self ) : if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
2169	def get_command ( self , ctx , name ) : if name in misc . __all__ : return getattr ( misc , name ) try : resource = tower_cli . get_resource ( name ) return ResSubcommand ( resource ) except ImportError : pass secho ( 'No such command: %s.' % name , fg = 'red' , bold = True ) sys . exit ( 2 )
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
13186	def download_observations ( observer_code ) : page_number = 1 observations = [ ] while True : logger . info ( 'Downloading page %d...' , page_number ) response = requests . get ( WEBOBS_RESULTS_URL , params = { 'obscode' : observer_code , 'num_results' : 200 , 'obs_types' : 'all' , 'page' : page_number , } ) logger . debug ( response . request . url ) parser = WebObsResultsParser ( response . text ) observations . extend ( parser . get_observations ( ) ) if '>Next</a>' not in response . text : break page_number += 1 return observations
12167	def _dispatch_coroutine ( self , event , listener , * args , ** kwargs ) : try : coro = listener ( * args , ** kwargs ) except Exception as exc : if event == self . LISTENER_ERROR_EVENT : raise return self . emit ( self . LISTENER_ERROR_EVENT , event , listener , exc ) asyncio . ensure_future ( _try_catch_coro ( self , event , listener , coro ) , loop = self . _loop , )
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
11722	def app_class ( ) : try : pkg_resources . get_distribution ( 'invenio-files-rest' ) from invenio_files_rest . app import Flask as FlaskBase except pkg_resources . DistributionNotFound : from flask import Flask as FlaskBase class Request ( TrustedHostsMixin , FlaskBase . request_class ) : pass class Flask ( FlaskBase ) : request_class = Request return Flask
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : total = 0 for i in pos : total += counts [ i ] total = float ( total ) for i in pos : x [ i ] = counts [ i ] / total else : x [ pos ] = 1 return x
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : r if self . method : if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
12028	def headerHTML ( header , fname ) : html = "<html><body><code>" html += "<h2>%s</h2>" % ( fname ) html += pprint . pformat ( header , indent = 1 ) html = html . replace ( "\n" , '<br>' ) . replace ( " " , "&nbsp;" ) html = html . replace ( r"\x00" , "" ) html += "</code></body></html>" print ( "saving header file:" , fname ) f = open ( fname , 'w' ) f . write ( html ) f . close ( ) webbrowser . open ( fname )
10568	def filter_google_songs ( songs , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : matched_songs = [ ] filtered_songs = [ ] if include_filters or exclude_filters : for song in songs : if _check_filters ( song , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) : matched_songs . append ( song ) else : filtered_songs . append ( song ) else : matched_songs += songs return matched_songs , filtered_songs
4800	def is_directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . _err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
11425	def record_match_subfields ( rec , tag , ind1 = " " , ind2 = " " , sub_key = None , sub_value = '' , sub_key2 = None , sub_value2 = '' , case_sensitive = True ) : if sub_key is None : raise TypeError ( "None object passed for parameter sub_key." ) if sub_key2 is not None and sub_value2 is '' : raise TypeError ( "Parameter sub_key2 defined but sub_value2 is None, " + "function requires a value for comparrison." ) ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if not case_sensitive : sub_value = sub_value . lower ( ) sub_value2 = sub_value2 . lower ( ) for field in record_get_field_instances ( rec , tag , ind1 , ind2 ) : subfields = dict ( field_get_subfield_instances ( field ) ) if not case_sensitive : for k , v in subfields . iteritems ( ) : subfields [ k ] = v . lower ( ) if sub_key in subfields : if sub_value is '' : return field [ 4 ] else : if sub_value == subfields [ sub_key ] : if sub_key2 is None : return field [ 4 ] else : if sub_key2 in subfields : if sub_value2 == subfields [ sub_key2 ] : return field [ 4 ] return False
9474	def BFS ( self , root = None ) : if not root : root = self . root ( ) queue = deque ( ) queue . append ( root ) nodes = [ ] while len ( queue ) > 0 : x = queue . popleft ( ) nodes . append ( x ) for child in x . children ( ) : queue . append ( child ) return nodes
13253	async def download_metadata_yaml ( session , github_url ) : metadata_yaml_url = _build_metadata_yaml_url ( github_url ) async with session . get ( metadata_yaml_url ) as response : response . raise_for_status ( ) yaml_data = await response . text ( ) return yaml . safe_load ( yaml_data )
4588	def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
7305	def process_post_form ( self , success_message = None ) : if not hasattr ( self , 'document' ) or self . document is None : self . document = self . document_type ( ) self . form = MongoModelForm ( model = self . document_type , instance = self . document , form_post_data = self . request . POST ) . get_form ( ) self . form . is_bound = True if self . form . is_valid ( ) : self . document_map_dict = MongoModelForm ( model = self . document_type ) . create_document_dictionary ( self . document_type ) self . new_document = self . document_type self . embedded_list_docs = { } if self . new_document is None : messages . error ( self . request , u"Failed to save document" ) else : self . new_document = self . new_document ( ) for form_key in self . form . cleaned_data . keys ( ) : if form_key == 'id' and hasattr ( self , 'document' ) : self . new_document . id = self . document . id continue self . process_document ( self . new_document , form_key , None ) self . new_document . save ( ) if success_message : messages . success ( self . request , success_message ) return self . form
6283	def set_default_viewport ( self ) : expected_height = int ( self . buffer_width / self . aspect_ratio ) blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
11432	def _check_field_validity ( field ) : if type ( field ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Field of type '%s' should be either " "a list or a tuple." % type ( field ) ) if len ( field ) != 5 : raise InvenioBibRecordFieldError ( "Field of length '%d' should have 5 " "elements." % len ( field ) ) if type ( field [ 0 ] ) not in ( list , tuple ) : raise InvenioBibRecordFieldError ( "Subfields of type '%s' should be " "either a list or a tuple." % type ( field [ 0 ] ) ) if type ( field [ 1 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 1 of type '%s' should be " "a string." % type ( field [ 1 ] ) ) if type ( field [ 2 ] ) is not str : raise InvenioBibRecordFieldError ( "Indicator 2 of type '%s' should be " "a string." % type ( field [ 2 ] ) ) if type ( field [ 3 ] ) is not str : raise InvenioBibRecordFieldError ( "Controlfield value of type '%s' " "should be a string." % type ( field [ 3 ] ) ) if type ( field [ 4 ] ) is not int : raise InvenioBibRecordFieldError ( "Global position of type '%s' should " "be an int." % type ( field [ 4 ] ) ) for subfield in field [ 0 ] : if ( type ( subfield ) not in ( list , tuple ) or len ( subfield ) != 2 or type ( subfield [ 0 ] ) is not str or type ( subfield [ 1 ] ) is not str ) : raise InvenioBibRecordFieldError ( "Subfields are malformed. " "Should a list of tuples of 2 strings." )
12011	def extractFile ( self , filename ) : files = [ x for x in self . tableOfContents if x [ 'filename' ] == filename ] if len ( files ) == 0 : raise FileNotFoundException ( ) fileRecord = files [ 0 ] metaheadroom = 1024 request = urllib2 . Request ( self . zipURI ) start = fileRecord [ 'filestart' ] end = fileRecord [ 'filestart' ] + fileRecord [ 'compressedsize' ] + metaheadroom request . headers [ 'Range' ] = "bytes=%s-%s" % ( start , end ) handle = urllib2 . urlopen ( request ) return_range = handle . headers . get ( 'Content-Range' ) if return_range != "bytes %d-%d/%s" % ( start , end , self . filesize ) : raise Exception ( "Ranged requests are not supported for this URI" ) filedata = handle . read ( ) zip_n = unpack ( "H" , filedata [ 26 : 28 ] ) [ 0 ] zip_m = unpack ( "H" , filedata [ 28 : 30 ] ) [ 0 ] has_data_descriptor = bool ( unpack ( "H" , filedata [ 6 : 8 ] ) [ 0 ] & 8 ) comp_size = unpack ( "I" , filedata [ 18 : 22 ] ) [ 0 ] if comp_size == 0 and has_data_descriptor : comp_size = fileRecord [ 'compressedsize' ] elif comp_size != fileRecord [ 'compressedsize' ] : raise Exception ( "Something went wrong. Directory and file header disagree of compressed file size" ) raw_zip_data = filedata [ 30 + zip_n + zip_m : 30 + zip_n + zip_m + comp_size ] uncompressed_data = "" compression_method = unpack ( "H" , filedata [ 8 : 10 ] ) [ 0 ] if compression_method == 0 : return raw_zip_data dec = zlib . decompressobj ( - zlib . MAX_WBITS ) for chunk in raw_zip_data : rv = dec . decompress ( chunk ) if rv : uncompressed_data = uncompressed_data + rv return uncompressed_data
13333	def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
11064	def _ignore_event ( self , message ) : if hasattr ( message , 'subtype' ) and message . subtype in self . ignored_events : return True return False
113	def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )
12419	def capture_stderr ( ) : stderr = sys . stderr try : capture_out = StringIO ( ) sys . stderr = capture_out yield capture_out finally : sys . stderr = stderr
7508	def _finalize_stats ( self , ipyclient ) : print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) if self . params . nboots : self . _compute_tree_stats ( ipyclient ) print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get_ascii ( show_internal = True , attributes = [ "dist" , "name" ] ) ) print ( "" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) print ( qtre . get_ascii ( ) ) print ( "" ) docslink = "https://toytree.readthedocs.io/" citelink = "https://ipyrad.readthedocs.io/tetrad.html" print ( LINKS . format ( docslink , citelink ) )
7096	def init_info_window_adapter ( self ) : adapter = self . adapter if adapter : return adapter = GoogleMap . InfoWindowAdapter ( ) adapter . getInfoContents . connect ( self . on_info_window_contents_requested ) adapter . getInfoWindow . connect ( self . on_info_window_requested ) self . map . setInfoWindowAdapter ( adapter )
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
10931	def update_param_vals ( self , new_vals , incremental = False ) : self . _last_vals = self . param_vals . copy ( ) if incremental : self . param_vals += new_vals else : self . param_vals = new_vals . copy ( ) self . _fresh_JTJ = False
9111	def size_attachments ( self ) : total_size = 0 for attachment in self . fs_cleansed_attachments : total_size += stat ( attachment ) . st_size return total_size
13268	def _gmlv2_to_geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ',' ) ] elif tag == 'LineString' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:innerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' , 'MultiCurve' ) : if tag == 'MultiCurve' : single_type = 'LineString' member_tag = 'curveMember' else : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
6567	def fulladder_gate ( variables , vartype = dimod . BINARY , name = 'FULL_ADDER' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 , 0 , 0 ) , ( 0 , 0 , 1 , 1 , 0 ) , ( 0 , 1 , 0 , 1 , 0 ) , ( 0 , 1 , 1 , 0 , 1 ) , ( 1 , 0 , 0 , 1 , 0 ) , ( 1 , 0 , 1 , 0 , 1 ) , ( 1 , 1 , 0 , 0 , 1 ) , ( 1 , 1 , 1 , 1 , 1 ) ] ) else : configs = frozenset ( [ ( - 1 , - 1 , - 1 , - 1 , - 1 ) , ( - 1 , - 1 , + 1 , + 1 , - 1 ) , ( - 1 , + 1 , - 1 , + 1 , - 1 ) , ( - 1 , + 1 , + 1 , - 1 , + 1 ) , ( + 1 , - 1 , - 1 , + 1 , - 1 ) , ( + 1 , - 1 , + 1 , - 1 , + 1 ) , ( + 1 , + 1 , - 1 , - 1 , + 1 ) , ( + 1 , + 1 , + 1 , + 1 , + 1 ) ] ) def func ( in1 , in2 , in3 , sum_ , carry ) : total = ( in1 > 0 ) + ( in2 > 0 ) + ( in3 > 0 ) if total == 0 : return ( sum_ <= 0 ) and ( carry <= 0 ) elif total == 1 : return ( sum_ > 0 ) and ( carry <= 0 ) elif total == 2 : return ( sum_ <= 0 ) and ( carry > 0 ) elif total == 3 : return ( sum_ > 0 ) and ( carry > 0 ) else : raise ValueError ( "func recieved unexpected values" ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : r Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
6130	def get ( self , * args , ** kwargs ) : try : req_func = self . session . get if self . session else requests . get req = req_func ( * args , ** kwargs ) req . raise_for_status ( ) self . failed_last = False return req except requests . exceptions . RequestException as e : self . log_error ( e ) for i in range ( 1 , self . num_retries ) : sleep_time = self . retry_rate * i self . log_function ( "Retrying in %s seconds" % sleep_time ) self . _sleep ( sleep_time ) try : req = requests . get ( * args , ** kwargs ) req . raise_for_status ( ) self . log_function ( "New request successful" ) return req except requests . exceptions . RequestException : self . log_function ( "New request failed" ) if not self . failed_last : self . failed_last = True raise ApiError ( e ) else : raise FatalApiError ( e )
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
11779	def SyntheticRestaurant ( n = 20 ) : "Generate a DataSet with n examples." def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
7258	def search_address ( self , address , filters = None , startDate = None , endDate = None , types = None ) : lat , lng = self . get_address_coords ( address ) return self . search_point ( lat , lng , filters = filters , startDate = startDate , endDate = endDate , types = types )
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
6314	def load ( self ) : self . create_effect_classes ( ) self . _add_resource_descriptions_to_pools ( self . create_external_resources ( ) ) self . _add_resource_descriptions_to_pools ( self . create_resources ( ) ) for meta , resource in resources . textures . load_pool ( ) : self . _textures [ meta . label ] = resource for meta , resource in resources . programs . load_pool ( ) : self . _programs [ meta . label ] = resource for meta , resource in resources . scenes . load_pool ( ) : self . _scenes [ meta . label ] = resource for meta , resource in resources . data . load_pool ( ) : self . _data [ meta . label ] = resource self . create_effect_instances ( ) self . post_load ( )
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
5344	def compose_gerrit ( projects ) : git_projects = [ project for project in projects if 'git' in projects [ project ] ] for project in git_projects : repos = [ repo for repo in projects [ project ] [ 'git' ] if 'gitroot' in repo ] if len ( repos ) > 0 : projects [ project ] [ 'gerrit' ] = [ ] for repo in repos : gerrit_project = repo . replace ( "http://git.eclipse.org/gitroot/" , "" ) gerrit_project = gerrit_project . replace ( ".git" , "" ) projects [ project ] [ 'gerrit' ] . append ( "git.eclipse.org_" + gerrit_project ) return projects
13397	def get_reference_to_class ( cls , class_or_class_name ) : if isinstance ( class_or_class_name , type ) : return class_or_class_name elif isinstance ( class_or_class_name , string_types ) : if ":" in class_or_class_name : mod_name , class_name = class_or_class_name . split ( ":" ) if not mod_name in sys . modules : __import__ ( mod_name ) mod = sys . modules [ mod_name ] return mod . __dict__ [ class_name ] else : return cls . load_class_from_locals ( class_or_class_name ) else : msg = "Unexpected Type '%s'" % type ( class_or_class_name ) raise InternalCashewException ( msg )
11009	def subscribe ( self , event , bet_ids ) : if not self . _subscriptions . get ( event ) : self . _subscriptions [ event ] = set ( ) self . _subscriptions [ event ] = self . _subscriptions [ event ] . union ( bet_ids )
1375	def parse_override_config ( namespace ) : overrides = dict ( ) for config in namespace : kv = config . split ( "=" ) if len ( kv ) != 2 : raise Exception ( "Invalid config property format (%s) expected key=value" % config ) if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : overrides [ kv [ 0 ] ] = True elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : overrides [ kv [ 0 ] ] = False else : overrides [ kv [ 0 ] ] = kv [ 1 ] return overrides
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
6184	def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
3072	def init_app ( self , app , scopes = None , client_secrets_file = None , client_id = None , client_secret = None , authorize_callback = None , storage = None , ** kwargs ) : self . app = app self . authorize_callback = authorize_callback self . flow_kwargs = kwargs if storage is None : storage = dictionary_storage . DictionaryStorage ( session , key = _CREDENTIALS_KEY ) self . storage = storage if scopes is None : scopes = app . config . get ( 'GOOGLE_OAUTH2_SCOPES' , _DEFAULT_SCOPES ) self . scopes = scopes self . _load_config ( client_secrets_file , client_id , client_secret ) app . register_blueprint ( self . _create_blueprint ( ) )
4551	def draw_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : draw_line ( setter , x0 , y0 , x1 , y1 , color , aa ) draw_line ( setter , x1 , y1 , x2 , y2 , color , aa ) draw_line ( setter , x2 , y2 , x0 , y0 , color , aa )
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
4817	def _dictfetchall ( self , cursor ) : columns = [ col [ 0 ] for col in cursor . description ] return [ dict ( zip ( columns , row ) ) for row in cursor . fetchall ( ) ]
1193	def task_done ( self ) : self . all_tasks_done . acquire ( ) try : unfinished = self . unfinished_tasks - 1 if unfinished <= 0 : if unfinished < 0 : raise ValueError ( 'task_done() called too many times' ) self . all_tasks_done . notify_all ( ) self . unfinished_tasks = unfinished finally : self . all_tasks_done . release ( )
13342	def concatenate ( tup , axis = 0 ) : from distob import engine if len ( tup ) is 0 : raise ValueError ( 'need at least one array to concatenate' ) first = tup [ 0 ] others = tup [ 1 : ] if ( hasattr ( first , 'concatenate' ) and hasattr ( type ( first ) , '__array_interface__' ) ) : return first . concatenate ( others , axis ) arrays = [ ] for ar in tup : if isinstance ( ar , DistArray ) : if axis == ar . _distaxis : arrays . extend ( ar . _subarrays ) else : arrays . append ( gather ( ar ) ) elif isinstance ( ar , RemoteArray ) : arrays . append ( ar ) elif isinstance ( ar , Remote ) : arrays . append ( _remote_to_array ( ar ) ) elif hasattr ( type ( ar ) , '__array_interface__' ) : arrays . append ( ar ) else : arrays . append ( np . array ( ar ) ) if all ( isinstance ( ar , np . ndarray ) for ar in arrays ) : return np . concatenate ( arrays , axis ) total_length = 0 commonshape = list ( arrays [ 0 ] . shape ) commonshape [ axis ] = None for ar in arrays : total_length += ar . shape [ axis ] shp = list ( ar . shape ) shp [ axis ] = None if shp != commonshape : raise ValueError ( 'incompatible shapes for concatenation' ) blocksize = ( ( total_length - 1 ) // engine . nengines ) + 1 rarrays = [ ] for ar in arrays : if isinstance ( ar , DistArray ) : rarrays . extend ( ar . _subarrays ) elif isinstance ( ar , RemoteArray ) : rarrays . append ( ar ) else : da = _scatter_ndarray ( ar , axis , blocksize ) for ra in da . _subarrays : rarrays . append ( ra ) del da del arrays eid = rarrays [ 0 ] . _id . engine if all ( ra . _id . engine == eid for ra in rarrays ) : if eid == engine . eid : return concatenate ( [ gather ( r ) for r in rarrays ] , axis ) else : return call ( concatenate , rarrays , axis ) else : return DistArray ( rarrays , axis )
6631	def set ( self , path , value = None , filename = None ) : if filename is None : config = self . _firstConfig ( ) [ 1 ] else : config = self . configs [ filename ] path = _splitPath ( path ) for el in path [ : - 1 ] : if el in config : config = config [ el ] else : config [ el ] = OrderedDict ( ) config = config [ el ] config [ path [ - 1 ] ] = value
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
11859	def sum_out ( var , factors , bn ) : "Eliminate var from all factors by summing over its values." result , var_factors = [ ] , [ ] for f in factors : ( var_factors if var in f . vars else result ) . append ( f ) result . append ( pointwise_product ( var_factors , bn ) . sum_out ( var , bn ) ) return result
12902	def _set_range ( self , start , stop , value , value_len ) : assert stop >= start and value_len >= 0 range_len = stop - start if range_len < value_len : self . _insert_zeros ( stop , stop + value_len - range_len ) self . _copy_to_range ( start , value , value_len ) elif range_len > value_len : self . _del_range ( stop - ( range_len - value_len ) , stop ) self . _copy_to_range ( start , value , value_len ) else : self . _copy_to_range ( start , value , value_len )
5838	def _data_analysis ( self , data_view_id ) : failure_message = "Error while retrieving data analysis for data view {}" . format ( data_view_id ) return self . _get_success_json ( self . _get ( routes . data_analysis ( data_view_id ) , failure_message = failure_message ) )
3765	def Joule_Thomson ( T , V , Cp , dV_dT = None , beta = None ) : r if dV_dT : return ( T * dV_dT - V ) / Cp elif beta : return V / Cp * ( beta * T - 1. ) else : raise Exception ( 'Either dV_dT or beta is needed' )
11709	def instance ( self , counter = None , pipeline_counter = None ) : pipeline_counter = pipeline_counter or self . pipeline_counter pipeline_instance = None if not pipeline_counter : pipeline_instance = self . server . pipeline ( self . pipeline_name ) . instance ( ) self . pipeline_counter = int ( pipeline_instance [ 'counter' ] ) if not counter : if pipeline_instance is None : pipeline_instance = ( self . server . pipeline ( self . pipeline_name ) . instance ( pipeline_counter ) ) for stages in pipeline_instance [ 'stages' ] : if stages [ 'name' ] == self . stage_name : return self . instance ( counter = int ( stages [ 'counter' ] ) , pipeline_counter = pipeline_counter ) return self . _get ( '/instance/{pipeline_counter:d}/{counter:d}' . format ( pipeline_counter = pipeline_counter , counter = counter ) )
2170	def command ( method = None , ** kwargs ) : def actual_decorator ( method ) : method . _cli_command = True method . _cli_command_attrs = kwargs return method if method and isinstance ( method , types . FunctionType ) : return actual_decorator ( method ) else : return actual_decorator
12987	def keep_kwargs_partial ( func , * args , ** keywords ) : def newfunc ( * fargs , ** fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , ** newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
1959	def sys_openat ( self , dirfd , buf , flags , mode ) : filename = self . current . read_string ( buf ) dirfd = ctypes . c_int32 ( dirfd ) . value if os . path . isabs ( filename ) or dirfd == self . FCNTL_FDCWD : return self . sys_open ( buf , flags , mode ) try : dir_entry = self . _get_fd ( dirfd ) except FdError as e : logger . info ( "openat: Not valid file descriptor. Returning EBADF" ) return - e . err if not isinstance ( dir_entry , Directory ) : logger . info ( "openat: Not directory descriptor. Returning ENOTDIR" ) return - errno . ENOTDIR dir_path = dir_entry . name filename = os . path . join ( dir_path , filename ) try : f = self . _sys_open_get_file ( filename , flags ) logger . debug ( f"Opening file {filename} for real fd {f.fileno()}" ) except IOError as e : logger . info ( f"Could not open file {filename}. Reason: {e!s}" ) return - e . errno if e . errno is not None else - errno . EINVAL return self . _open ( f )
11801	def restore ( self , removals ) : "Undo a supposition and all inferences from it." for B , b in removals : self . curr_domains [ B ] . append ( b )
2781	def create ( self ) : input_params = { "type" : self . type , "data" : self . data , "name" : self . name , "priority" : self . priority , "port" : self . port , "ttl" : self . ttl , "weight" : self . weight , "flags" : self . flags , "tags" : self . tags } data = self . get_data ( "domains/%s/records" % ( self . domain ) , type = POST , params = input_params , ) if data : self . id = data [ 'domain_record' ] [ 'id' ]
2818	def convert_padding ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting padding...' ) if params [ 'mode' ] == 'constant' : if params [ 'value' ] != 0.0 : raise AssertionError ( 'Cannot convert non-zero padding' ) if names : tf_name = 'PADD' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) padding_name = tf_name padding_layer = keras . layers . ZeroPadding2D ( padding = ( ( params [ 'pads' ] [ 2 ] , params [ 'pads' ] [ 6 ] ) , ( params [ 'pads' ] [ 3 ] , params [ 'pads' ] [ 7 ] ) ) , name = padding_name ) layers [ scope_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) elif params [ 'mode' ] == 'reflect' : def target_layer ( x , pads = params [ 'pads' ] ) : layer = tf . pad ( x , [ [ 0 , 0 ] , [ 0 , 0 ] , [ pads [ 2 ] , pads [ 6 ] ] , [ pads [ 3 ] , pads [ 7 ] ] ] , 'REFLECT' ) return layer lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
10159	def fresh_cookies ( ctx , mold = '' ) : mold = mold or "https://github.com/Springerle/py-generic-project.git" tmpdir = os . path . join ( tempfile . gettempdir ( ) , "cc-upgrade-pygments-markdown-lexer" ) if os . path . isdir ( '.git' ) : pass if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore_patterns ( ".git" , ".svn" , "*~" , ) ) else : ctx . run ( "git clone {} {}" . format ( mold , tmpdir ) ) shutil . copy2 ( "project.d/cookiecutter.json" , tmpdir ) with pushd ( '..' ) : ctx . run ( "cookiecutter --no-input {}" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( "git status" )
2022	def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
11969	def _bits_to_dec ( nm , check = True ) : if check and not is_bits_nm ( nm ) : raise ValueError ( '_bits_to_dec: invalid netmask: "%s"' % nm ) bits = int ( str ( nm ) ) return VALID_NETMASKS [ bits ]
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : if credentials is not None : self . credentials = credentials else : self . credentials = { } if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
2335	def clr ( M , ** kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] return R
10660	def mass_fractions ( amounts ) : m = masses ( amounts ) m_total = sum ( m . values ( ) ) return { compound : m [ compound ] / m_total for compound in m . keys ( ) }
7398	def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) bib [ - 1 ] [ key ] = value return bib
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
8492	def main ( ) : parser = argparse . ArgumentParser ( description = "Helper for working with " "pyconfigs" ) target_group = parser . add_mutually_exclusive_group ( ) target_group . add_argument ( '-f' , '--filename' , help = "parse an individual file or directory" , metavar = 'F' ) target_group . add_argument ( '-m' , '--module' , help = "parse a package or module, recursively looking inside it" , metavar = 'M' ) parser . add_argument ( '-v' , '--view-call' , help = "show the actual pyconfig call made (default: show namespace)" , action = 'store_true' ) parser . add_argument ( '-l' , '--load-configs' , help = "query the currently set value for each key found" , action = 'store_true' ) key_group = parser . add_mutually_exclusive_group ( ) key_group . add_argument ( '-a' , '--all' , help = "show keys which don't have defaults set" , action = 'store_true' ) key_group . add_argument ( '-k' , '--only-keys' , help = "show a list of discovered keys without values" , action = 'store_true' ) parser . add_argument ( '-n' , '--natural-sort' , help = "sort by filename and line (default: alphabetical by key)" , action = 'store_true' ) parser . add_argument ( '-s' , '--source' , help = "show source annotations (implies --natural-sort)" , action = 'store_true' ) parser . add_argument ( '-c' , '--color' , help = "toggle output colors (default: %s)" % bool ( pygments ) , action = 'store_const' , default = bool ( pygments ) , const = ( not bool ( pygments ) ) ) args = parser . parse_args ( ) if args . color and not pygments : _error ( "Pygments is required for color output.\n" " pip install pygments" ) if args . module : _handle_module ( args ) if args . filename : _handle_file ( args )
9047	def multivariate_normal ( random , mean , cov ) : from numpy . linalg import cholesky L = cholesky ( cov ) return L @ random . randn ( L . shape [ 0 ] ) + mean
8680	def list ( self , key_name = None , max_suggestions = 100 , cutoff = 0.5 , locked_only = False , key_type = None ) : self . _assert_valid_stash ( ) key_list = [ k for k in self . _storage . list ( ) if k [ 'name' ] != 'stored_passphrase' and ( k . get ( 'lock' ) if locked_only else True ) ] if key_type : types = ( 'secret' , None ) if key_type == 'secret' else [ key_type ] key_list = [ k for k in key_list if k . get ( 'type' ) in types ] key_list = [ k [ 'name' ] for k in key_list ] if key_name : if key_name . startswith ( '~' ) : key_list = difflib . get_close_matches ( key_name . lstrip ( '~' ) , key_list , max_suggestions , cutoff ) else : key_list = [ k for k in key_list if key_name in k ] audit ( storage = self . _storage . db_path , action = 'LIST' + ( '[LOCKED]' if locked_only else '' ) , message = json . dumps ( dict ( ) ) ) return key_list
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
3109	def locked_put ( self , credentials ) : entity , _ = self . model_class . objects . get_or_create ( ** { self . key_name : self . key_value } ) setattr ( entity , self . property_name , credentials ) entity . save ( )
1297	def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) repo . __reset_repository ( )
154	def max_item ( self ) : if self . is_empty ( ) : raise ValueError ( "Tree is empty" ) node = self . _root while node . right is not None : node = node . right return node . key , node . value
814	def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d
538	def __runTaskMainLoop ( self , numIters , learningOffAt = None ) : self . _model . resetSequenceStates ( ) self . _currentRecordIndex = - 1 while True : if self . _isKilled : break if self . _isCanceled : break if self . _isInterrupted . isSet ( ) : self . __setAsOrphaned ( ) break if self . _isMature : if not self . _isBestModel : self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED break else : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF if learningOffAt is not None and self . _currentRecordIndex == learningOffAt : self . _model . disableLearning ( ) try : inputRecord = self . _inputSource . getNextRecordDict ( ) if self . _currentRecordIndex < 0 : self . _inputSource . setTimeout ( 10 ) except Exception , e : raise utils . JobFailException ( ErrorCodes . streamReading , str ( e . args ) , traceback . format_exc ( ) ) if inputRecord is None : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF break if inputRecord : self . _currentRecordIndex += 1 result = self . _model . run ( inputRecord = inputRecord ) result . metrics = self . __metricMgr . update ( result ) if not result . metrics : result . metrics = self . __metricMgr . getMetrics ( ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) result . sensorInput . dataEncodings = None self . _writePrediction ( result ) self . _periodic . tick ( ) if numIters >= 0 and self . _currentRecordIndex >= numIters - 1 : break else : raise ValueError ( "Got an empty record from FileSource: %r" % inputRecord )
7317	def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
10208	def record_view_event_builder ( event , sender_app , pid = None , record = None , ** kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , record_id = str ( record . id ) , pid_type = pid . pid_type , pid_value = str ( pid . pid_value ) , referrer = request . referrer , ** get_user ( ) ) ) return event
5554	def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
7602	def get_known_tournaments ( self , ** params : tournamentfilter ) : url = self . api . TOURNAMENT + '/known' return self . _get_model ( url , PartialTournament , ** params )
13782	def _ConvertEnumDescriptor ( self , enum_proto , package = None , file_desc = None , containing_type = None , scope = None ) : if package : enum_name = '.' . join ( ( package , enum_proto . name ) ) else : enum_name = enum_proto . name if file_desc is None : file_name = None else : file_name = file_desc . name values = [ self . _MakeEnumValueDescriptor ( value , index ) for index , value in enumerate ( enum_proto . value ) ] desc = descriptor . EnumDescriptor ( name = enum_proto . name , full_name = enum_name , filename = file_name , file = file_desc , values = values , containing_type = containing_type , options = enum_proto . options ) scope [ '.%s' % enum_name ] = desc self . _enum_descriptors [ enum_name ] = desc return desc
6943	def jhk_to_bmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , BJHK , BJH , BJK , BHK , BJ , BH , BK )
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
11055	def rm_back_refs ( obj ) : for ref in _collect_refs ( obj ) : ref [ 'value' ] . _remove_backref ( ref [ 'field_instance' ] . _backref_field_name , obj , ref [ 'field_name' ] , strict = False )
6727	def get_name ( ) : if env . vm_type == EC2 : for instance in get_all_running_ec2_instances ( ) : if env . host_string == instance . public_dns_name : name = instance . tags . get ( env . vm_name_tag ) return name else : raise NotImplementedError
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : for statement in table . rows : if statement [ 0 ] != "" : yield statement
8259	def _sorted_copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) _list . sort ( sorted , comparison ) if reversed : _list . reverse ( sorted ) return sorted
10542	def find_tasks ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'task' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : raise
2362	def t_heredoc ( self , t ) : r'<<\S+\r?\n' t . lexer . is_tabbed = False self . _init_heredoc ( t ) t . lexer . begin ( 'heredoc' )
7984	def registration_form_received ( self , stanza ) : self . lock . acquire ( ) try : self . __register = Register ( stanza . get_query ( ) ) self . registration_callback ( stanza , self . __register . get_form ( ) ) finally : self . lock . release ( )
7449	def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
10500	def waitForCreation ( self , timeout = 10 , notification = 'AXCreated' ) : callback = AXCallbacks . returnElemCallback retelem = None args = ( retelem , ) return self . waitFor ( timeout , notification , callback = callback , args = args )
12705	def center_of_mass ( bodies ) : x = np . zeros ( 3. ) t = 0. for b in bodies : m = b . mass x += b . body_to_world ( m . c ) * m . mass t += m . mass return x / t
377	def pixel_value_scale ( im , val = 0.9 , clip = None , is_random = False ) : clip = clip if clip is not None else ( - np . inf , np . inf ) if is_random : scale = 1 + np . random . uniform ( - val , val ) im = im * scale else : im = im * val if len ( clip ) == 2 : im = np . clip ( im , clip [ 0 ] , clip [ 1 ] ) else : raise Exception ( "clip : tuple of 2 numbers" ) return im
498	def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )
8208	def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) network . initialize ( ) return network
10463	def menuitemenabled ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) if menu_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
4127	def plot ( self , ** kargs ) : from pylab import plot , linspace , xlabel , ylabel , grid time = linspace ( 1 * self . dt , self . N * self . dt , self . N ) plot ( time , self . data , ** kargs ) xlabel ( 'Time' ) ylabel ( 'Amplitude' ) grid ( True )
1993	def load_state ( self , state_id , delete = True ) : return self . _store . load_state ( f'{self._prefix}{state_id:08x}{self._suffix}' , delete = delete )
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
9518	def count_sequences ( infile ) : seq_reader = sequences . file_reader ( infile ) n = 0 for seq in seq_reader : n += 1 return n
215	def deepcopy ( self ) : return HeatmapsOnImage ( self . get_arr ( ) , shape = self . shape , min_value = self . min_value , max_value = self . max_value )
1552	def _get_base_component ( self ) : comp = topology_pb2 . Component ( ) comp . name = self . name comp . spec = topology_pb2 . ComponentObjectSpec . Value ( "PYTHON_CLASS_NAME" ) comp . class_name = self . python_class_path comp . config . CopyFrom ( self . _get_comp_config ( ) ) return comp
13817	def _ConvertFieldValuePair ( js , message ) : names = [ ] message_descriptor = message . DESCRIPTOR for name in js : try : field = message_descriptor . fields_by_camelcase_name . get ( name , None ) if not field : raise ParseError ( 'Message type "{0}" has no field named "{1}".' . format ( message_descriptor . full_name , name ) ) if name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" fields.' . format ( message . DESCRIPTOR . full_name , name ) ) names . append ( name ) if field . containing_oneof is not None : oneof_name = field . containing_oneof . name if oneof_name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" ' 'oneof fields.' . format ( message . DESCRIPTOR . full_name , oneof_name ) ) names . append ( oneof_name ) value = js [ name ] if value is None : message . ClearField ( field . name ) continue if _IsMapEntry ( field ) : message . ClearField ( field . name ) _ConvertMapFieldValue ( value , message , field ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : message . ClearField ( field . name ) if not isinstance ( value , list ) : raise ParseError ( 'repeated field {0} must be in [] which is ' '{1}.' . format ( name , value ) ) if field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : for item in value : sub_message = getattr ( message , field . name ) . add ( ) if ( item is None and sub_message . DESCRIPTOR . full_name != 'google.protobuf.Value' ) : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) _ConvertMessage ( item , sub_message ) else : for item in value : if item is None : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) getattr ( message , field . name ) . append ( _ConvertScalarFieldValue ( item , field ) ) elif field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : sub_message = getattr ( message , field . name ) _ConvertMessage ( value , sub_message ) else : setattr ( message , field . name , _ConvertScalarFieldValue ( value , field ) ) except ParseError as e : if field and field . containing_oneof is None : raise ParseError ( 'Failed to parse {0} field: {1}' . format ( name , e ) ) else : raise ParseError ( str ( e ) ) except ValueError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) ) except TypeError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) )
5203	def delete_connection ( ) : if _CON_SYM_ in globals ( ) : con = globals ( ) . pop ( _CON_SYM_ ) if not getattr ( con , '_session' ) . start ( ) : con . stop ( )
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
3623	def __pre_delete_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE pre_delete FOR %s' , instance . __class__ ) self . delete_record ( instance )
10510	def imagecapture ( self , window_name = None , out_file = None , x = 0 , y = 0 , width = None , height = None ) : if not out_file : out_file = tempfile . mktemp ( '.png' , 'ldtp_' ) else : out_file = os . path . expanduser ( out_file ) if _ldtp_windows_env : if width == None : width = - 1 if height == None : height = - 1 if window_name == None : window_name = '' data = self . _remote_imagecapture ( window_name , x , y , width , height ) f = open ( out_file , 'wb' ) f . write ( b64decode ( data ) ) f . close ( ) return out_file
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
2424	def set_doc_data_lics ( self , doc , lics ) : if not self . doc_data_lics_set : self . doc_data_lics_set = True if validations . validate_data_lics ( lics ) : doc . data_license = document . License . from_identifier ( lics ) return True else : raise SPDXValueError ( 'Document::DataLicense' ) else : raise CardinalityError ( 'Document::DataLicense' )
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
7153	def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '✔' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
8212	def draw_freehand ( self ) : if _ctx . _ns [ "mousedown" ] : x , y = mouse ( ) if self . show_grid : x , y = self . grid . snap ( x , y ) if self . freehand_move == True : cmd = MOVETO self . freehand_move = False else : cmd = LINETO pt = PathElement ( ) if cmd != MOVETO : pt . freehand = True else : pt . freehand = False pt . cmd = cmd pt . x = x pt . y = y pt . ctrl1 = Point ( x , y ) pt . ctrl2 = Point ( x , y ) self . _points . append ( pt ) r = 4 _ctx . nofill ( ) _ctx . stroke ( self . handle_color ) _ctx . oval ( pt . x - r , pt . y - r , r * 2 , r * 2 ) _ctx . fontsize ( 9 ) _ctx . fill ( self . handle_color ) _ctx . text ( " (" + str ( int ( pt . x ) ) + ", " + str ( int ( pt . y ) ) + ")" , pt . x + r , pt . y ) self . _dirty = True else : self . freehand_move = True if self . _dirty : self . _points [ - 1 ] . freehand = False self . export_svg ( ) self . _dirty = False
12948	def reload ( self , cascadeObjects = True ) : _id = self . _id if not _id : raise KeyError ( 'Object has never been saved! Cannot reload.' ) currentData = self . asDict ( False , forStorage = False ) newDataObj = self . objects . get ( _id ) if not newDataObj : raise KeyError ( 'Object with id=%d is not in database. Cannot reload.' % ( _id , ) ) newData = newDataObj . asDict ( False , forStorage = False ) if currentData == newData and not self . foreignFields : return [ ] updatedFields = { } for thisField , newValue in newData . items ( ) : defaultValue = thisField . getDefaultValue ( ) currentValue = currentData . get ( thisField , defaultValue ) fieldIsUpdated = False if currentValue != newValue : fieldIsUpdated = True elif cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : if currentValue . isFetched ( ) : oldObjs = currentValue . getObjs ( ) newObjs = newValue . getObjs ( ) if oldObjs != newObjs : fieldIsUpdated = True else : for i in range ( len ( oldObjs ) ) : if not oldObjs [ i ] . hasSameValues ( newObjs [ i ] , cascadeObjects = True ) : fieldIsUpdated = True break if fieldIsUpdated is True : updatedFields [ thisField ] = ( currentValue , newValue ) setattr ( self , thisField , newValue ) self . _origData [ thisField ] = newDataObj . _origData [ thisField ] return updatedFields
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
9700	def worker ( wrapped , dkwargs , hash_value = None , * args , ** kwargs ) : if "event" not in dkwargs : msg = "djwebhooks.decorators.redis_hook requires an 'event' argument in the decorator." raise TypeError ( msg ) event = dkwargs [ 'event' ] if "owner" not in kwargs : msg = "djwebhooks.senders.redis_callable requires an 'owner' argument in the decorated function." raise TypeError ( msg ) owner = kwargs [ 'owner' ] if "identifier" not in kwargs : msg = "djwebhooks.senders.orm_callable requires an 'identifier' argument in the decorated function." raise TypeError ( msg ) identifier = kwargs [ 'identifier' ] senderobj = DjangoRQSenderable ( wrapped , dkwargs , hash_value , WEBHOOK_ATTEMPTS , * args , ** kwargs ) senderobj . webhook_target = WebhookTarget . objects . get ( event = event , owner = owner , identifier = identifier ) senderobj . url = senderobj . webhook_target . target_url senderobj . payload = senderobj . get_payload ( ) senderobj . payload [ 'owner' ] = getattr ( kwargs [ 'owner' ] , WEBHOOK_OWNER_FIELD ) senderobj . payload [ 'event' ] = dkwargs [ 'event' ] return senderobj . send ( )
13763	def _wrap_color ( self , code , text , format = None , style = None ) : color = None if code [ : 3 ] == self . bg . PREFIX : color = self . bg . COLORS . get ( code , None ) if not color : color = self . fg . COLORS . get ( code , None ) if not color : raise Exception ( 'Color code not found' ) if format and format not in self . formats : raise Exception ( 'Color format not found' ) fmt = "0;" if format == 'bold' : fmt = "1;" elif format == 'underline' : fmt = "4;" parts = color . split ( '[' ) color = '{0}[{1}{2}' . format ( parts [ 0 ] , fmt , parts [ 1 ] ) if self . has_colors and self . colors_enabled : st = '' if style : st = self . st . COLORS . get ( style , '' ) return "{0}{1}{2}{3}" . format ( st , color , text , self . st . COLORS [ 'reset_all' ] ) else : return text
2958	def _assure_dir ( self ) : try : os . makedirs ( self . _state_dir ) except OSError as err : if err . errno != errno . EEXIST : raise
3454	def add_SBO ( model ) : for r in model . reactions : if r . annotation . get ( "sbo" ) : continue if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
8770	def _lswitch_select_open ( self , context , switches = None , ** kwargs ) : if switches is not None : for res in switches [ "results" ] : count = res [ "_relations" ] [ "LogicalSwitchStatus" ] [ "lport_count" ] if ( self . limits [ 'max_ports_per_switch' ] == 0 or count < self . limits [ 'max_ports_per_switch' ] ) : return res [ "uuid" ] return None
4463	def jam_pack ( jam , ** kwargs ) : if not hasattr ( jam . sandbox , 'muda' ) : jam . sandbox . muda = jams . Sandbox ( history = [ ] , state = [ ] , version = dict ( muda = version , librosa = librosa . __version__ , jams = jams . __version__ , pysoundfile = psf . __version__ ) ) elif not isinstance ( jam . sandbox . muda , jams . Sandbox ) : jam . sandbox . muda = jams . Sandbox ( ** jam . sandbox . muda ) jam . sandbox . muda . update ( ** kwargs ) return jam
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
740	def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )
10320	def _microcanonical_average_moments ( moments , alpha ) : ret = dict ( ) runs = moments . shape [ 0 ] sqrt_n = np . sqrt ( runs ) moments_sample_mean = moments . mean ( axis = 0 ) ret [ 'moments' ] = moments_sample_mean moments_sample_std = moments . std ( axis = 0 , ddof = 1 ) ret [ 'moments_ci' ] = np . empty ( ( 5 , 2 ) ) for k in range ( 5 ) : if moments_sample_std [ k ] : old_settings = np . seterr ( all = 'raise' ) ret [ 'moments_ci' ] [ k ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = moments_sample_mean [ k ] , scale = moments_sample_std [ k ] / sqrt_n ) np . seterr ( ** old_settings ) else : ret [ 'moments_ci' ] [ k ] = ( moments_sample_mean [ k ] * np . ones ( 2 ) ) return ret
13589	def json_post_required ( * decorator_args ) : def decorator ( method ) : @ wraps ( method ) def wrapper ( * args , ** kwargs ) : field = decorator_args [ 0 ] if len ( decorator_args ) == 2 : request_name = decorator_args [ 1 ] else : request_name = field request = args [ 0 ] if request . method != 'POST' : logger . error ( 'POST required for this url' ) raise Http404 ( 'only POST allowed for this url' ) if field not in request . POST : s = 'Expected field named %s in POST' % field logger . error ( s ) raise Http404 ( s ) setattr ( request , request_name , json . loads ( request . POST [ field ] ) ) return method ( * args , ** kwargs ) return wrapper return decorator
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
5736	def cleanup ( self ) : if self . subscription : logger . info ( "Deleting worker subscription..." ) self . subscriber_client . delete_subscription ( self . subscription )
4439	async def _playnow ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue and not player . is_playing : return await ctx . invoke ( self . _play , query = query ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' results = await self . bot . lavalink . get_tracks ( query ) if not results or not results [ 'tracks' ] : return await ctx . send ( 'Nothing found!' ) tracks = results [ 'tracks' ] track = tracks . pop ( 0 ) if results [ 'loadType' ] == 'PLAYLIST_LOADED' : for _track in tracks : player . add ( requester = ctx . author . id , track = _track ) await player . play_now ( requester = ctx . author . id , track = track )
11937	def create_message ( self , level , msg_text , extra_tags = '' , date = None , url = None ) : if not date : now = timezone . now ( ) else : now = date r = now . isoformat ( ) if now . microsecond : r = r [ : 23 ] + r [ 26 : ] if r . endswith ( '+00:00' ) : r = r [ : - 6 ] + 'Z' fingerprint = r + msg_text msg_id = hashlib . sha256 ( fingerprint . encode ( 'ascii' , 'ignore' ) ) . hexdigest ( ) return Message ( id = msg_id , message = msg_text , level = level , tags = extra_tags , date = r , url = url )
3321	def delete ( self , token ) : self . _lock . acquire_write ( ) try : lock = self . _dict . get ( token ) _logger . debug ( "delete {}" . format ( lock_string ( lock ) ) ) if lock is None : return False key = "URL2TOKEN:{}" . format ( lock . get ( "root" ) ) if key in self . _dict : tokList = self . _dict [ key ] if len ( tokList ) > 1 : tokList . remove ( token ) self . _dict [ key ] = tokList else : del self . _dict [ key ] del self . _dict [ token ] self . _flush ( ) finally : self . _lock . release ( ) return True
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
3548	def _descriptor_changed ( self , descriptor ) : desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
2260	def group_items ( items , groupids ) : r if callable ( groupids ) : keyfunc = groupids pair_list = ( ( keyfunc ( item ) , item ) for item in items ) else : pair_list = zip ( groupids , items ) groupid_to_items = defaultdict ( list ) for key , item in pair_list : groupid_to_items [ key ] . append ( item ) return groupid_to_items
8344	def renderContents ( self , encoding = DEFAULT_OUTPUT_ENCODING , prettyPrint = False , indentLevel = 0 ) : s = [ ] for c in self : text = None if isinstance ( c , NavigableString ) : text = c . __str__ ( encoding ) elif isinstance ( c , Tag ) : s . append ( c . __str__ ( encoding , prettyPrint , indentLevel ) ) if text and prettyPrint : text = text . strip ( ) if text : if prettyPrint : s . append ( " " * ( indentLevel - 1 ) ) s . append ( text ) if prettyPrint : s . append ( "\n" ) return '' . join ( s )
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
3264	def md_link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadataType" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
13261	def get_task_tree ( white_list = None ) : assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) if white_list is not None : white_list = set ( item if isinstance ( item , str ) else item . __qualname__ for item in white_list ) tree = dict ( ( task . qualified_name , task ) for task in _task_list . values ( ) if white_list is None or task . qualified_name in white_list ) plugins = get_plugin_list ( ) for plugin in [ plugin for plugin in plugins . values ( ) if white_list is None or plugin . __qualname__ in white_list ] : tasks = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . isfunction ( func ) and hasattr ( func , "yaz_task_config" ) ] if len ( tasks ) == 0 : continue node = tree for name in plugin . __qualname__ . split ( "." ) : if not name in node : node [ name ] = { } node = node [ name ] for func in tasks : logger . debug ( "Found task %s" , func ) node [ func . __name__ ] = Task ( plugin_class = plugin , func = func , config = func . yaz_task_config ) return tree
9652	def get_sha ( a_file , settings = None ) : if settings : error = settings [ "error" ] else : error = ERROR_FN try : BLOCKSIZE = 65536 hasher = hashlib . sha1 ( ) with io . open ( a_file , "rb" ) as fh : buf = fh . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = fh . read ( BLOCKSIZE ) the_hash = hasher . hexdigest ( ) except IOError : errmes = "File '{}' could not be read! Exiting!" . format ( a_file ) error ( errmes ) sys . exit ( 1 ) except : errmes = "Unspecified error returning sha1 hash. Exiting!" error ( errmes ) sys . exit ( 1 ) return the_hash
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
5906	def create_portable_topology ( topol , struct , ** kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( ** kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( ** mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( ** grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
11270	def safe_substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . safe_substitute ( data )
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
3777	def calculate_integral ( self , T1 , T2 , method ) : r return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
13152	def log_error ( error , result ) : p = { 'error' : error , 'result' : result } _log ( TYPE_CODES . ERROR , p )
2491	def add_file_dependencies_helper ( self , doc_file ) : subj_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) ) ) if len ( subj_triples ) != 1 : raise InvalidDocumentError ( 'Could not find dependency subject {0}' . format ( doc_file . name ) ) subject_node = subj_triples [ 0 ] [ 0 ] for dependency in doc_file . dependencies : dep_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( dependency ) ) ) ) if len ( dep_triples ) == 1 : dep_node = dep_triples [ 0 ] [ 0 ] dep_triple = ( subject_node , self . spdx_namespace . fileDependency , dep_node ) self . graph . add ( dep_triple ) else : print ( 'Warning could not resolve file dependency {0} -> {1}' . format ( doc_file . name , dependency ) )
9254	def issue_line_with_user ( self , line , issue ) : if not issue . get ( "pull_request" ) or not self . options . author : return line if not issue . get ( "user" ) : line += u" (Null user)" elif self . options . username_as_tag : line += u" (@{0})" . format ( issue [ "user" ] [ "login" ] ) else : line += u" ([{0}]({1}))" . format ( issue [ "user" ] [ "login" ] , issue [ "user" ] [ "html_url" ] ) return line
6282	def cursor_event ( self , x , y , dx , dy ) : self . sys_camera . rot_state ( x , y )
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
11507	def item_get ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.get' , parameters ) return response
12889	def handle_set ( self , item , value ) : doc = yield from self . call ( 'SET/{}' . format ( item ) , dict ( value = value ) ) if doc is None : return None return doc . status == 'FS_OK'
5819	def _map_oids ( oids ) : new_oids = set ( ) for oid in oids : if oid in _oid_map : new_oids |= _oid_map [ oid ] return oids | new_oids
7382	def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
5124	def show_type ( self , edge_type , ** kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , ** kwargs ) self . _update_all_colors ( )
10679	def S ( self , T ) : result = 0.0 if T < self . Tmax : lT = T else : lT = self . Tmax Tref = self . Tmin for c , e in zip ( self . _coefficients , self . _exponents ) : e_modified = e - 1.0 if e_modified == - 1.0 : result += c * math . log ( lT / Tref ) else : e_mod = e_modified + 1.0 result += c * ( lT ** e_mod - Tref ** e_mod ) / e_mod return result
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
9659	def get_levels ( G ) : levels = [ ] ends = get_sinks ( G ) levels . append ( ends ) while get_direct_ancestors ( G , ends ) : ends = get_direct_ancestors ( G , ends ) levels . append ( ends ) levels . reverse ( ) return levels
1982	def sync ( f ) : def new_function ( self , * args , ** kw ) : self . _lock . acquire ( ) try : return f ( self , * args , ** kw ) finally : self . _lock . release ( ) return new_function
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
4488	def _iter_children ( self , url , kind , klass , recurse = None ) : children = self . _follow_next ( url ) while children : child = children . pop ( ) kind_ = child [ 'attributes' ] [ 'kind' ] if kind_ == kind : yield klass ( child , self . session ) elif recurse is not None : url = self . _get_attribute ( child , * recurse ) children . extend ( self . _follow_next ( url ) )
8147	def hue ( self , img1 , img2 ) : import colorsys p1 = list ( img1 . getdata ( ) ) p2 = list ( img2 . getdata ( ) ) for i in range ( len ( p1 ) ) : r1 , g1 , b1 , a1 = p1 [ i ] r1 = r1 / 255.0 g1 = g1 / 255.0 b1 = b1 / 255.0 h1 , s1 , v1 = colorsys . rgb_to_hsv ( r1 , g1 , b1 ) r2 , g2 , b2 , a2 = p2 [ i ] r2 = r2 / 255.0 g2 = g2 / 255.0 b2 = b2 / 255.0 h2 , s2 , v2 = colorsys . rgb_to_hsv ( r2 , g2 , b2 ) r3 , g3 , b3 = colorsys . hsv_to_rgb ( h2 , s1 , v1 ) r3 = int ( r3 * 255 ) g3 = int ( g3 * 255 ) b3 = int ( b3 * 255 ) p1 [ i ] = ( r3 , g3 , b3 , a1 ) img = Image . new ( "RGBA" , img1 . size , 255 ) img . putdata ( p1 ) return img
10143	def decrypt_files ( file_link ) : if ENCRYPTION_DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed_link = re . findall ( r'(.*/(.*))#(.{30})' , file_link ) [ 0 ] req = urllib . request . Request ( parsed_link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) ' ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) file_response = urllib . request . urlopen ( req ) file_to_decrypt = file_response . read ( ) decrypt_r , decrypt_w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt_r ) decrypt_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass_fds = ( decrypt_r , ) ) os . close ( decrypt_r ) open ( decrypt_w , 'w' ) . write ( parsed_link [ 2 ] ) decrypted_data , stderr = decrypt_output . communicate ( file_to_decrypt ) with open ( parsed_link [ 1 ] , 'wb' ) as decrypted_file : decrypted_file . write ( decrypted_data ) return parsed_link [ 1 ] + ' is decrypted and saved.' except IndexError : return 'Please enter valid link.'
8917	def _get_param ( self , param , allowed_values = None , optional = False ) : request_params = self . _request_params ( ) if param in request_params : value = request_params [ param ] . lower ( ) if allowed_values is not None : if value in allowed_values : self . params [ param ] = value else : raise OWSInvalidParameterValue ( "%s %s is not supported" % ( param , value ) , value = param ) elif optional : self . params [ param ] = None else : raise OWSMissingParameterValue ( 'Parameter "%s" is missing' % param , value = param ) return self . params [ param ]
129	def find_closest_point_index ( self , x , y , return_distance = False ) : ia . do_assert ( len ( self . exterior ) > 0 ) distances = [ ] for x2 , y2 in self . exterior : d = ( x2 - x ) ** 2 + ( y2 - y ) ** 2 distances . append ( d ) distances = np . sqrt ( distances ) closest_idx = np . argmin ( distances ) if return_distance : return closest_idx , distances [ closest_idx ] return closest_idx
12604	def duplicated ( values : Sequence ) : vals = pd . Series ( values ) return vals [ vals . duplicated ( ) ]
1291	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . demo_memory = Replay ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , include_next_states = True , capacity = self . demo_memory_capacity , scope = 'demo-replay' , summary_labels = self . summary_labels ) self . fn_import_demo_experience = tf . make_template ( name_ = 'import-demo-experience' , func_ = self . tf_import_demo_experience , custom_getter_ = custom_getter ) self . fn_demo_loss = tf . make_template ( name_ = 'demo-loss' , func_ = self . tf_demo_loss , custom_getter_ = custom_getter ) self . fn_combined_loss = tf . make_template ( name_ = 'combined-loss' , func_ = self . tf_combined_loss , custom_getter_ = custom_getter ) self . fn_demo_optimization = tf . make_template ( name_ = 'demo-optimization' , func_ = self . tf_demo_optimization , custom_getter_ = custom_getter ) return custom_getter
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( ** self . parent . data )
7532	def trackjobs ( func , results , spacer ) : LOGGER . info ( "inside trackjobs of %s" , func ) asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] start = time . time ( ) while 1 : ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
818	def grow ( self , rows , cols ) : if not self . hist_ : self . hist_ = SparseMatrix ( rows , cols ) self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) self . colSums_ = numpy . zeros ( cols , dtype = dtype ) self . hack_ = None else : oldRows = self . hist_ . nRows ( ) oldCols = self . hist_ . nCols ( ) nextRows = max ( oldRows , rows ) nextCols = max ( oldCols , cols ) if ( oldRows < nextRows ) or ( oldCols < nextCols ) : self . hist_ . resize ( nextRows , nextCols ) if oldRows < nextRows : oldSums = self . rowSums_ self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None if oldCols < nextCols : oldSums = self . colSums_ self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) self . colSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
12225	def convertGribToTiff ( listeFile , listParam , listLevel , liststep , grid , startDate , endDate , outFolder ) : dicoValues = { } for l in listeFile : grbs = pygrib . open ( l ) grbs . seek ( 0 ) index = 1 for j in range ( len ( listLevel ) , 0 , - 1 ) : for i in range ( len ( listParam ) - 1 , - 1 , - 1 ) : grb = grbs [ index ] p = grb . name . replace ( ' ' , '_' ) if grb . level != 0 : l = str ( grb . level ) + '_' + grb . typeOfLevel else : l = grb . typeOfLevel if p + '_' + l not in dicoValues . keys ( ) : dicoValues [ p + '_' + l ] = [ ] dicoValues [ p + '_' + l ] . append ( grb . values ) shape = grb . values . shape lat , lon = grb . latlons ( ) geoparam = ( lon . min ( ) , lat . max ( ) , grid , grid ) index += 1 nbJour = ( endDate - startDate ) . days + 1 for s in range ( 0 , ( len ( liststep ) * nbJour - len ( listeFile ) ) ) : for k in dicoValues . keys ( ) : dicoValues [ k ] . append ( np . full ( shape , np . nan ) ) for i in range ( len ( dicoValues . keys ( ) ) - 1 , - 1 , - 1 ) : dictParam = dict ( ( k , dicoValues [ dicoValues . keys ( ) [ i ] ] [ k ] ) for k in range ( 0 , len ( dicoValues [ dicoValues . keys ( ) [ i ] ] ) ) ) sorted ( dictParam . items ( ) , key = lambda x : x [ 0 ] ) outputImg = outFolder + '/' + dicoValues . keys ( ) [ i ] + '_' + startDate . strftime ( '%Y%M%d' ) + '_' + endDate . strftime ( '%Y%M%d' ) + '.tif' writeTiffFromDicoArray ( dictParam , outputImg , shape , geoparam ) for f in listeFile : os . remove ( f )
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
3636	def club ( self , sort = 'desc' , ctype = 'player' , defId = '' , start = 0 , count = None , page_size = itemsPerPage [ 'club' ] , level = None , category = None , assetId = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None ) : method = 'GET' url = 'club' if count : page_size = count params = { 'sort' : sort , 'type' : ctype , 'defId' : defId , 'start' : start , 'count' : page_size } if level : params [ 'level' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params ) if start == 0 : if ctype == 'player' : pgid = 'Club - Players - List View' elif ctype == 'staff' : pgid = 'Club - Staff - List View' elif ctype in ( 'item' , 'kit' , 'ball' , 'badge' , 'stadium' ) : pgid = 'Club - Club Items - List View' events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) , self . pin . event ( 'page_view' , pgid ) ] if rc [ 'itemData' ] : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( { 'itemData' : i } ) for i in rc [ 'itemData' ] ]
9558	def _apply_unique_checks ( self , i , r , unique_sets , summarize = False , context = None ) : for key , code , message in self . _unique_checks : value = None values = unique_sets [ key ] if isinstance ( key , basestring ) : fi = self . _field_names . index ( key ) if fi >= len ( r ) : continue value = r [ fi ] else : value = [ ] for f in key : fi = self . _field_names . index ( f ) if fi >= len ( r ) : break value . append ( r [ fi ] ) value = tuple ( value ) if value in values : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'key' ] = key p [ 'value' ] = value if context is not None : p [ 'context' ] = context yield p values . add ( value )
2194	def encoding ( self ) : if self . redirect is not None : return self . redirect . encoding else : return super ( TeeStringIO , self ) . encoding
10244	def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
10400	def done_chomping ( self ) -> bool : return self . tag in self . graph . nodes [ self . target_node ]
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
8009	def execute ( self ) : self . executed_at = now ( ) self . save ( ) with transaction . atomic ( ) : ret = BillingAgreement . execute ( self . id ) ret . user = self . user ret . save ( ) self . executed_agreement = ret self . save ( ) return ret
1724	def execute ( self , js = None , use_compilation_plan = False ) : try : cache = self . __dict__ [ 'cache' ] except KeyError : cache = self . __dict__ [ 'cache' ] = { } hashkey = hashlib . md5 ( js . encode ( 'utf-8' ) ) . digest ( ) try : compiled = cache [ hashkey ] except KeyError : code = translate_js ( js , '' , use_compilation_plan = use_compilation_plan ) compiled = cache [ hashkey ] = compile ( code , '<EvalJS snippet>' , 'exec' ) exec ( compiled , self . _context )
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
11715	def console_output ( self , instance = None ) : if instance is None : instance = self . instance ( ) for stage in instance [ 'stages' ] : for job in stage [ 'jobs' ] : if job [ 'result' ] not in self . final_results : continue artifact = self . artifact ( instance [ 'counter' ] , stage [ 'name' ] , job [ 'name' ] , stage [ 'counter' ] ) output = artifact . get ( 'cruise-output/console.log' ) yield ( { 'pipeline' : self . name , 'pipeline_counter' : instance [ 'counter' ] , 'stage' : stage [ 'name' ] , 'stage_counter' : stage [ 'counter' ] , 'job' : job [ 'name' ] , 'job_result' : job [ 'result' ] , } , output . body )
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
2184	def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
5036	def _handle_singular ( cls , enterprise_customer , manage_learners_form ) : form_field_value = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . EMAIL_OR_USERNAME ] email = email_or_username__to__email ( form_field_value ) try : validate_email_to_link ( email , form_field_value , ValidationMessages . INVALID_EMAIL_OR_USERNAME , True ) except ValidationError as exc : manage_learners_form . add_error ( ManageLearnersForm . Fields . EMAIL_OR_USERNAME , exc ) else : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) return [ email ]
1752	def _reg_name ( self , reg_id ) : if reg_id >= X86_REG_ENDING : logger . warning ( "Trying to get register name for a non-register" ) return None cs_reg_name = self . cpu . instruction . reg_name ( reg_id ) if cs_reg_name is None or cs_reg_name . lower ( ) == '(invalid)' : return None return self . cpu . _regfile . _alias ( cs_reg_name . upper ( ) )
8218	def do_fullscreen ( self , widget ) : self . fullscreen ( ) self . is_fullscreen = True while Gtk . events_pending ( ) : Gtk . main_iteration ( ) self . bot . _screen_width = Gdk . Screen . width ( ) self . bot . _screen_height = Gdk . Screen . height ( ) self . bot . _screen_ratio = self . bot . _screen_width / self . bot . _screen_height
3339	def is_equal_or_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and ( childUri . rstrip ( "/" ) + "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
187	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image
12158	def abfSort ( IDs ) : IDs = list ( IDs ) monO = [ ] monN = [ ] monD = [ ] good = [ ] for ID in IDs : if ID is None : continue if 'o' in ID : monO . append ( ID ) elif 'n' in ID : monN . append ( ID ) elif 'd' in ID : monD . append ( ID ) else : good . append ( ID ) return sorted ( good ) + sorted ( monO ) + sorted ( monN ) + sorted ( monD )
11912	def fail ( message = None , exit_status = None ) : print ( 'Error:' , message , file = sys . stderr ) sys . exit ( exit_status or 1 )
6422	def _synoname_strip_punct ( self , word ) : stripped = '' for char in word : if char not in set ( ',-./:;"&\'()!{|}?$%*+<=>[\\]^_`~' ) : stripped += char return stripped . strip ( )
6362	def encode ( self , word , max_length = - 1 ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
11239	def bug_info ( exc_type , exc_value , exc_trace ) : if hasattr ( sys , 'ps1' ) or not sys . stderr . isatty ( ) : sys . __excepthook__ ( exc_type , exc_value , exc_trace ) else : import ipdb traceback . print_exception ( exc_type , exc_value , exc_trace ) print ipdb . post_mortem ( exc_trace )
6419	def dist ( self , src , tar , probs = None ) : if src == tar : return 0.0 if probs is None : self . _coder . train ( src + tar ) else : self . _coder . set_probs ( probs ) src_comp = self . _coder . encode ( src ) [ 1 ] tar_comp = self . _coder . encode ( tar ) [ 1 ] concat_comp = self . _coder . encode ( src + tar ) [ 1 ] concat_comp2 = self . _coder . encode ( tar + src ) [ 1 ] return ( min ( concat_comp , concat_comp2 ) - min ( src_comp , tar_comp ) ) / max ( src_comp , tar_comp )
3394	def find_gene_knockout_reactions ( cobra_model , gene_list , compiled_gene_reaction_rules = None ) : potential_reactions = set ( ) for gene in gene_list : if isinstance ( gene , string_types ) : gene = cobra_model . genes . get_by_id ( gene ) potential_reactions . update ( gene . _reaction ) gene_set = { str ( i ) for i in gene_list } if compiled_gene_reaction_rules is None : compiled_gene_reaction_rules = { r : parse_gpr ( r . gene_reaction_rule ) [ 0 ] for r in potential_reactions } return [ r for r in potential_reactions if not eval_gpr ( compiled_gene_reaction_rules [ r ] , gene_set ) ]
12455	def error_handler ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : try : return func ( * args , ** kwargs ) except BaseException as err : if BOOTSTRAPPER_TEST_KEY in os . environ : raise if ERROR_HANDLER_DISABLED : return True return save_traceback ( err ) return wrapper
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
2311	def b_fit_score ( self , x , y ) : x = np . reshape ( minmax_scale ( x ) , ( - 1 , 1 ) ) y = np . reshape ( minmax_scale ( y ) , ( - 1 , 1 ) ) poly = PolynomialFeatures ( degree = self . degree ) poly_x = poly . fit_transform ( x ) poly_x [ : , 1 ] = 0 poly_x [ : , 2 ] = 0 regressor = LinearRegression ( ) regressor . fit ( poly_x , y ) y_predict = regressor . predict ( poly_x ) error = mean_squared_error ( y_predict , y ) return error
7543	def chunk_clusters ( data , sample ) : num = 0 optim = int ( ( sample . stats . clusters_total // data . cpus ) + ( sample . stats . clusters_total % data . cpus ) ) chunkslist = [ ] with gzip . open ( sample . files . clusters , 'rb' ) as clusters : pairdealer = itertools . izip ( * [ iter ( clusters ) ] * 2 ) done = 0 while not done : done , chunk = clustdealer ( pairdealer , optim ) chunkhandle = os . path . join ( data . dirs . clusts , "tmp_" + str ( sample . name ) + "." + str ( num * optim ) ) if chunk : chunkslist . append ( ( optim , chunkhandle ) ) with open ( chunkhandle , 'wb' ) as outchunk : outchunk . write ( "//\n//\n" . join ( chunk ) + "//\n//\n" ) num += 1 return chunkslist
2324	def predict ( self , x , * args , ** kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , ** kwargs ) else : return self . predict_proba ( x , * args , ** kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , ** kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , ** kwargs )
2360	def t_stringdollar_rbrace ( self , t ) : r'\}' t . lexer . braces -= 1 if t . lexer . braces == 0 : t . lexer . begin ( 'string' )
13686	def embed_data ( request ) : result = _EmbedDataFixture ( request ) result . delete_data_dir ( ) result . create_data_dir ( ) yield result result . delete_data_dir ( )
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
6156	def stereo_FM ( x , fs = 2.4e6 , file_name = 'test.wav' ) : N1 = 10 b = signal . firwin ( 64 , 2 * 200e3 / float ( fs ) ) y = signal . lfilter ( b , 1 , x ) z = ss . downsample ( y , N1 ) z_bb = discrim ( z ) b12 = signal . firwin ( 128 , 2 * 12e3 / ( float ( fs ) / N1 ) ) y_lpr = signal . lfilter ( b12 , 1 , z_bb ) b19 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 19 - 5 , 19 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) z_bb19 = signal . lfilter ( b19 , 1 , z_bb ) theta , phi_error = pilot_PLL ( z_bb19 , 19000 , fs / N1 , 2 , 10 , 0.707 ) b38 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 38 - 5 , 38 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) x_lmr = signal . lfilter ( b38 , 1 , z_bb ) x_lmr = 2 * np . sqrt ( 2 ) * np . cos ( 2 * theta ) * x_lmr y_lmr = signal . lfilter ( b12 , 1 , x_lmr ) y_left = y_lpr + y_lmr y_right = y_lpr - y_lmr N2 = 5 fs2 = float ( fs ) / ( N1 * N2 ) y_left_DN2 = ss . downsample ( y_left , N2 ) y_right_DN2 = ss . downsample ( y_right , N2 ) a_de = np . exp ( - 2.1 * 1e3 * 2 * np . pi / fs2 ) z_left = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_left_DN2 ) z_right = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_right_DN2 ) z_out = np . hstack ( ( np . array ( [ z_left ] ) . T , ( np . array ( [ z_right ] ) . T ) ) ) ss . to_wav ( file_name , 48000 , z_out / 2 ) print ( 'Done!' ) return z_bb , theta , y_lpr , y_lmr , z_out
3202	def delete ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _delete ( url = self . _build_path ( campaign_id ) )
1214	def _int_to_pos ( self , flat_position ) : return flat_position % self . env . action_space . screen_shape [ 0 ] , flat_position % self . env . action_space . screen_shape [ 1 ]
7302	def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
5646	def createcolorbar ( cmap , norm ) : cax , kw = matplotlib . colorbar . make_axes ( matplotlib . pyplot . gca ( ) ) c = matplotlib . colorbar . ColorbarBase ( cax , cmap = cmap , norm = norm ) return c
6413	def ghmean ( nums ) : m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_h , 12 ) != round ( m_g , 12 ) : m_g , m_h = ( m_g * m_h ) ** ( 1 / 2 ) , ( 2 * m_g * m_h ) / ( m_g + m_h ) return m_g
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
886	def punishPredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells ) : self . _punishPredictedColumn ( self . connections , columnMatchingSegments , prevActiveCells , self . predictedSegmentDecrement )
3774	def select_valid_methods ( self , T ) : r if self . forced : considered_methods = list ( self . user_methods ) else : considered_methods = list ( self . all_methods ) if self . user_methods : [ considered_methods . remove ( i ) for i in self . user_methods ] preferences = sorted ( [ self . ranked_methods . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods [ i ] for i in preferences ] if self . user_methods : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods ) ] sorted_valid_methods = [ ] for method in sorted_methods : if self . test_method_validity ( T , method ) : sorted_valid_methods . append ( method ) return sorted_valid_methods
5049	def proxied_get ( self , * args , ** kwargs ) : original_kwargs = kwargs . copy ( ) if 'course_id' in kwargs : try : course_run_key = str ( CourseKey . from_string ( kwargs [ 'course_id' ] ) ) except InvalidKeyError : pass else : try : return self . get ( * args , ** kwargs ) except DataSharingConsent . DoesNotExist : kwargs [ 'course_id' ] = parse_course_key ( course_run_key ) try : return self . get ( * args , ** kwargs ) except DataSharingConsent . DoesNotExist : return ProxyDataSharingConsent ( ** original_kwargs )
13148	def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : base_file_names = data [ 'docnames' ] except KeyError : base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
7660	def validate ( self , strict = True ) : ann_schema = schema . namespace_array ( self . namespace ) valid = True try : jsonschema . validate ( self . __json_light__ ( data = False ) , schema . JAMS_SCHEMA ) data_ser = [ serialize_obj ( obs ) for obs in self . data ] jsonschema . validate ( data_ser , ann_schema ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
10810	def get_by_name ( cls , name ) : try : return cls . query . filter_by ( name = name ) . one ( ) except NoResultFound : return None
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
11514	def search_item_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbyname' , parameters ) return response [ 'items' ]
3706	def COSTALD ( T , Tc , Vc , omega ) : r Tr = T / Tc V_delta = ( - 0.296123 + 0.386914 * Tr - 0.0427258 * Tr ** 2 - 0.0480645 * Tr ** 3 ) / ( Tr - 1.00001 ) V_0 = 1 - 1.52816 * ( 1 - Tr ) ** ( 1 / 3. ) + 1.43907 * ( 1 - Tr ) ** ( 2 / 3. ) - 0.81446 * ( 1 - Tr ) + 0.190454 * ( 1 - Tr ) ** ( 4 / 3. ) return Vc * V_0 * ( 1 - omega * V_delta )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
5691	def _check_dep_time_is_valid ( self , dep_time ) : assert dep_time <= self . _min_dep_time , "Labels should be entered in decreasing order of departure time." dep_time_index = self . dep_times_to_index [ dep_time ] if self . _min_dep_time < float ( 'inf' ) : min_dep_index = self . dep_times_to_index [ self . _min_dep_time ] assert min_dep_index == dep_time_index or ( min_dep_index == dep_time_index - 1 ) , "dep times should be ordered sequentially" else : assert dep_time_index is 0 , "first dep_time index should be zero (ensuring that all connections are properly handled)" self . _min_dep_time = dep_time
2987	def get_app_kwarg_dict ( appInstance = None ) : app = ( appInstance or current_app ) app_config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors_' , '' ) : app_config . get ( k ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None }
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
8611	def get_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) ) return response
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
9288	def _connect ( self ) : self . logger . info ( "Attempting connection to %s:%s" , self . server [ 0 ] , self . server [ 1 ] ) try : self . _open_socket ( ) peer = self . sock . getpeername ( ) self . logger . info ( "Connected to %s" , str ( peer ) ) self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . sock . setsockopt ( socket . SOL_SOCKET , socket . SO_KEEPALIVE , 1 ) banner = self . sock . recv ( 512 ) if is_py3 : banner = banner . decode ( 'latin-1' ) if banner [ 0 ] == "#" : self . logger . debug ( "Banner: %s" , banner . rstrip ( ) ) else : raise ConnectionError ( "invalid banner from server" ) except ConnectionError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except ( socket . error , socket . timeout ) as e : self . close ( ) self . logger . error ( "Socket error: %s" % str ( e ) ) if str ( e ) == "timed out" : raise ConnectionError ( "no banner from server" ) else : raise ConnectionError ( e ) self . _connected = True
5544	def clip_array_with_vector ( array , array_affine , geometries , inverted = False , clip_buffer = 0 ) : buffered_geometries = [ ] for feature in geometries : feature_geom = to_shape ( feature [ "geometry" ] ) if feature_geom . is_empty : continue if feature_geom . geom_type == "GeometryCollection" : buffered_geom = unary_union ( [ g . buffer ( clip_buffer ) for g in feature_geom ] ) else : buffered_geom = feature_geom . buffer ( clip_buffer ) if not buffered_geom . is_empty : buffered_geometries . append ( buffered_geom ) if buffered_geometries : if array . ndim == 2 : return ma . masked_array ( array , geometry_mask ( buffered_geometries , array . shape , array_affine , invert = inverted ) ) elif array . ndim == 3 : mask = geometry_mask ( buffered_geometries , ( array . shape [ 1 ] , array . shape [ 2 ] ) , array_affine , invert = inverted ) return ma . masked_array ( array , mask = np . stack ( ( mask for band in array ) ) ) else : fill = False if inverted else True return ma . masked_array ( array , mask = np . full ( array . shape , fill , dtype = bool ) )
13853	def run ( self ) : if not self . device : return try : data = "" while ( self . do_run ) : try : if ( self . device . inWaiting ( ) > 1 ) : l = self . device . readline ( ) [ : - 2 ] l = l . decode ( "UTF-8" ) if ( l == "[" ) : data = "[" elif ( l == "]" ) and ( len ( data ) > 4 ) and ( data [ 0 ] == "[" ) : data = data + "]" self . store . register_json ( data ) self . age ( ) elif ( l [ 0 : 3 ] == " {" ) : data = data + " " + l else : sleep ( 1 ) self . age ( ) except ( UnicodeDecodeError , ValueError ) : data = "" self . age ( ) except serial . serialutil . SerialException : print ( "Could not connect to the serial line at " + self . device_name )
7835	def __from_xml ( self , xmlnode ) : self . fields = [ ] self . reported_fields = [ ] self . items = [ ] self . title = None self . instructions = None if ( xmlnode . type != "element" or xmlnode . name != "x" or xmlnode . ns ( ) . content != DATAFORM_NS ) : raise ValueError ( "Not a form: " + xmlnode . serialize ( ) ) self . type = xmlnode . prop ( "type" ) if not self . type in self . allowed_types : raise BadRequestProtocolError ( "Bad form type: %r" % ( self . type , ) ) child = xmlnode . children while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "title" : self . title = from_utf8 ( child . getContent ( ) ) elif child . name == "instructions" : self . instructions = from_utf8 ( child . getContent ( ) ) elif child . name == "field" : self . fields . append ( Field . _new_from_xml ( child ) ) elif child . name == "item" : self . items . append ( Item . _new_from_xml ( child ) ) elif child . name == "reported" : self . __get_reported ( child ) child = child . next
8138	def contrast ( self , value = 1.0 ) : c = ImageEnhance . Contrast ( self . img ) self . img = c . enhance ( value )
6521	def directories ( self , filters = None , containing = None ) : filters = compile_masks ( filters or [ r'.*' ] ) contains = compile_masks ( containing ) for dirname , files in iteritems ( self . _found ) : relpath = text_type ( Path ( dirname ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : if not contains or self . _contains ( files , contains ) : yield dirname
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
12960	def _filter ( filterObj , ** kwargs ) : for key , value in kwargs . items ( ) : if key . endswith ( '__ne' ) : notFilter = True key = key [ : - 4 ] else : notFilter = False if key not in filterObj . indexedFields : raise ValueError ( 'Field "' + key + '" is not in INDEXED_FIELDS array. Filtering is only supported on indexed fields.' ) if notFilter is False : filterObj . filters . append ( ( key , value ) ) else : filterObj . notFilters . append ( ( key , value ) ) return filterObj
8364	def _output_file ( self , frame ) : if self . buff : return self . buff elif self . multifile : return self . file_root + "_%03d" % frame + self . file_ext else : return self . filename
5897	def get_meta_image_url ( request , image ) : rendition = image . get_rendition ( filter = 'original' ) return request . build_absolute_uri ( rendition . url )
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
12580	def to_file ( self , outpath ) : if not self . has_mask ( ) and not self . is_smoothed ( ) : save_niigz ( outpath , self . img ) else : save_niigz ( outpath , self . get_data ( masked = True , smoothed = True ) , self . get_header ( ) , self . get_affine ( ) )
11792	def lcv ( var , assignment , csp ) : "Least-constraining-values heuristic." return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
4448	def delete_document ( self , doc_id , conn = None ) : if conn is None : conn = self . redis return conn . execute_command ( self . DEL_CMD , self . index_name , doc_id )
8250	def blend ( self , clr , factor = 0.5 ) : r = self . r * ( 1 - factor ) + clr . r * factor g = self . g * ( 1 - factor ) + clr . g * factor b = self . b * ( 1 - factor ) + clr . b * factor a = self . a * ( 1 - factor ) + clr . a * factor return Color ( r , g , b , a , mode = "rgb" )
1934	def get_constructor_arguments ( self ) -> str : item = self . _constructor_abi_item return '()' if item is None else self . tuple_signature_for_components ( item [ 'inputs' ] )
643	def getConfigPaths ( cls ) : configPaths = [ ] if cls . _configPaths is not None : return cls . _configPaths else : if 'NTA_CONF_PATH' in os . environ : configVar = os . environ [ 'NTA_CONF_PATH' ] configPaths = configVar . split ( os . pathsep ) return configPaths
553	def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] if self . _priorStateJSON is None : swarms = dict ( ) if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , "The field '%s' " " specified in the fixedFields list is not present in this " " model." % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( "Unsupported search type: %s" % ( self . _hsObj . _searchType ) ) self . _state = dict ( lastUpdateTime = time . time ( ) , lastGoodSprint = None , searchOver = False , activeSwarms = swarms . keys ( ) , swarms = swarms , sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , blackListedEncoders = [ ] , ) self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
2772	def load ( self ) : data = self . get_data ( 'load_balancers/%s' % self . id , type = GET ) load_balancer = data [ 'load_balancer' ] for attr in load_balancer . keys ( ) : if attr == 'health_check' : health_check = HealthCheck ( ** load_balancer [ 'health_check' ] ) setattr ( self , attr , health_check ) elif attr == 'sticky_sessions' : sticky_ses = StickySesions ( ** load_balancer [ 'sticky_sessions' ] ) setattr ( self , attr , sticky_ses ) elif attr == 'forwarding_rules' : rules = list ( ) for rule in load_balancer [ 'forwarding_rules' ] : rules . append ( ForwardingRule ( ** rule ) ) setattr ( self , attr , rules ) else : setattr ( self , attr , load_balancer [ attr ] ) return self
3555	def power_on ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) if factory is None : factory = count ( ) while len ( nodes ) < stop : G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
8895	def _with_error_handling ( resp , error , mode , response_format ) : def safe_parse ( r ) : try : return APIWrapper . _parse_resp ( r , response_format ) except ( ValueError , SyntaxError ) as ex : log . error ( ex ) r . parsed = None return r if isinstance ( error , requests . HTTPError ) : if resp . status_code == 400 : resp = safe_parse ( resp ) if resp . parsed is not None : parsed_resp = resp . parsed messages = [ ] if response_format == 'xml' and parsed_resp . find ( './ValidationErrors' ) is not None : messages = [ e . find ( './Message' ) . text for e in parsed_resp . findall ( './ValidationErrors/ValidationErrorDto' ) ] elif response_format == 'json' and 'ValidationErrors' in parsed_resp : messages = [ e [ 'Message' ] for e in parsed_resp [ 'ValidationErrors' ] ] error = requests . HTTPError ( '%s: %s' % ( error , '\n\t' . join ( messages ) ) , response = resp ) elif resp . status_code == 429 : error = requests . HTTPError ( '%sToo many requests in the last minute.' % error , response = resp ) if STRICT == mode : raise error elif GRACEFUL == mode : if isinstance ( error , EmptyResponse ) : log . warning ( error ) resp . parsed = None return resp elif isinstance ( error , requests . HTTPError ) : if resp . status_code == 429 : log . warning ( error ) return safe_parse ( resp ) else : raise error else : raise error else : log . error ( error ) return safe_parse ( resp )
3642	def sell ( self , item_id , bid , buy_now , duration = 3600 , fast = False ) : method = 'POST' url = 'auctionhouse' data = { 'buyNowPrice' : buy_now , 'startingBid' : bid , 'duration' : duration , 'itemData' : { 'id' : item_id } } rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } ) if not fast : self . tradeStatus ( rc [ 'id' ] ) return rc [ 'id' ]
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
12738	def create_bodies ( self , translate = ( 0 , 1 , 0 ) , size = 0.1 ) : stack = [ ( 'root' , 0 , self . root [ 'position' ] + translate ) ] while stack : name , depth , end = stack . pop ( ) for child in self . hierarchy . get ( name , ( ) ) : stack . append ( ( child , depth + 1 , end + self . bones [ child ] . end ) ) if name not in self . bones : continue bone = self . bones [ name ] body = self . world . create_body ( 'box' , name = bone . name , density = self . density , lengths = ( size , size , bone . length ) ) body . color = self . color x , y , z = end - bone . direction * bone . length / 2 body . position = x , z , y u = bone . direction v = np . cross ( u , [ 0 , 1 , 0 ] ) l = np . linalg . norm ( v ) if l > 0 : v /= l rot = np . vstack ( [ np . cross ( u , v ) , v , u ] ) . T swizzle = [ [ 1 , 0 , 0 ] , [ 0 , 0 , 1 ] , [ 0 , - 1 , 0 ] ] body . rotation = np . dot ( swizzle , rot ) self . bodies . append ( body )
12144	def figureStimulus ( abf , sweeps = [ 0 ] ) : stimuli = [ 2.31250 , 2.35270 ] for sweep in sweeps : abf . setsweep ( sweep ) for stimulus in stimuli : S1 = int ( abf . pointsPerSec * stimulus ) S2 = int ( abf . pointsPerSec * ( stimulus + 0.001 ) ) abf . sweepY [ S1 : S2 ] = np . nan I1 = int ( abf . pointsPerSec * 2.2 ) I2 = int ( abf . pointsPerSec * 2.6 ) baseline = np . average ( abf . sweepY [ int ( abf . pointsPerSec * 2.0 ) : int ( abf . pointsPerSec * 2.2 ) ] ) Ys = lowPassFilter ( abf . sweepY [ I1 : I2 ] ) - baseline Xs = abf . sweepX2 [ I1 : I1 + len ( Ys ) ] . flatten ( ) plt . plot ( Xs , Ys , alpha = .5 , lw = 2 ) return
8882	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
6395	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) key = '' for char in self . _consonants : if char in word : key += char for char in word : if char not in self . _consonants and char not in key : key += char return key
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
5496	def from_file ( cls , file , * args , ** kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , ** kwargs ) except OSError as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
5579	def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( ** current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
2537	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True doc . package . source_info = text return True else : raise CardinalityError ( 'Package::SourceInfo' )
6806	def init_raspbian_vm ( self ) : r = self . local_renderer r . comment ( 'Installing system packages.' ) r . sudo ( 'add-apt-repository ppa:linaro-maintainers/tools' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install libsdl-dev qemu-system' ) r . comment ( 'Download image.' ) r . local ( 'wget https://downloads.raspberrypi.org/raspbian_lite_latest' ) r . local ( 'unzip raspbian_lite_latest.zip' ) r . comment ( 'Find start of the Linux ext4 partition.' ) r . local ( "parted -s 2016-03-18-raspbian-jessie-lite.img unit B print | " "awk '/^Number/{{p=1;next}}; p{{gsub(/[^[:digit:]]/, " ", $2); print $2}}' | sed -n 2p" , assign_to = 'START' ) r . local ( 'mkdir -p {raspbian_mount_point}' ) r . sudo ( 'mount -v -o offset=$START -t ext4 {raspbian_image} $MNT' ) r . comment ( 'Comment out everything in ld.so.preload' ) r . local ( "sed -i 's/^/#/g' {raspbian_mount_point}/etc/ld.so.preload" ) r . comment ( 'Comment out entries containing /dev/mmcblk in fstab.' ) r . local ( "sed -i '/mmcblk/ s?^?#?' /etc/fstab" ) r . sudo ( 'umount {raspbian_mount_point}' ) r . comment ( 'Download kernel.' ) r . local ( 'wget https://github.com/dhruvvyas90/qemu-rpi-kernel/blob/master/{raspbian_kernel}?raw=true' ) r . local ( 'mv {raspbian_kernel} {libvirt_images_dir}' ) r . comment ( 'Creating libvirt machine.' ) r . local ( 'virsh define libvirt-raspbian.xml' ) r . comment ( 'You should now be able to boot the VM by running:' ) r . comment ( '' ) r . comment ( ' qemu-system-arm -kernel {libvirt_boot_dir}/{raspbian_kernel} ' '-cpu arm1176 -m 256 -M versatilepb -serial stdio -append "root=/dev/sda2 rootfstype=ext4 rw" ' '-hda {libvirt_images_dir}/{raspbian_image}' ) r . comment ( '' ) r . comment ( 'Or by running virt-manager.' )
2249	def memoize_property ( fget ) : while hasattr ( fget , 'fget' ) : fget = fget . fget attr_name = '_' + fget . __name__ @ functools . wraps ( fget ) def fget_memoized ( self ) : if not hasattr ( self , attr_name ) : setattr ( self , attr_name , fget ( self ) ) return getattr ( self , attr_name ) return property ( fget_memoized )
2595	def use_pickle ( ) : from . import serialize serialize . pickle = serialize . _stdlib_pickle can_map [ FunctionType ] = _original_can_map [ FunctionType ]
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3933	def _make_token_request ( session , token_request_data ) : try : r = session . post ( OAUTH2_TOKEN_REQUEST_URL , data = token_request_data ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Token request failed: {}' . format ( e ) ) else : res = r . json ( ) if 'error' in res : raise GoogleAuthError ( 'Token request error: {!r}' . format ( res [ 'error' ] ) ) return res
5528	def open ( config , mode = "continue" , zoom = None , bounds = None , single_input_file = None , with_cache = False , debug = False ) : return Mapchete ( MapcheteConfig ( config , mode = mode , zoom = zoom , bounds = bounds , single_input_file = single_input_file , debug = debug ) , with_cache = with_cache )
2236	def timestamp ( method = 'iso8601' ) : if method == 'iso8601' : tz_hour = time . timezone // 3600 utc_offset = str ( tz_hour ) if tz_hour < 0 else '+' + str ( tz_hour ) stamp = time . strftime ( '%Y-%m-%dT%H%M%S' ) + utc_offset return stamp else : raise ValueError ( 'only iso8601 is accepted for now' )
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
13505	def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
9256	def issues_to_log ( self , issues , pull_requests ) : log = "" sections_a , issues_a = self . parse_by_sections ( issues , pull_requests ) for section , s_issues in sections_a . items ( ) : log += self . generate_sub_section ( s_issues , section ) log += self . generate_sub_section ( issues_a , self . options . issue_prefix ) return log
3291	def get_href ( self ) : safe = "/" + "!*'()," + "$-_|." return compat . quote ( self . provider . mount_path + self . provider . share_path + self . get_preferred_path ( ) , safe = safe , )
5983	def output_subplot_array ( output_path , output_filename , output_format ) : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : raise exc . PlottingException ( 'You cannot output a subplots with format .fits' )
13519	def prop_power ( self , propulsion_eff = 0.7 , sea_margin = 0.2 ) : PP = ( 1 + sea_margin ) * self . resistance ( ) * self . speed / propulsion_eff return PP
13211	def _parse_revision_date ( self ) : r doc_datetime = None if not self . is_draft : date_command = LatexCommand ( 'date' , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( date_command . parse ( self . _tex ) ) command_content = parsed [ 'content' ] . strip ( ) except StopIteration : command_content = None self . _logger . warning ( 'lsstdoc has no date command' ) if command_content is not None and command_content != r'\today' : try : doc_datetime = datetime . datetime . strptime ( command_content , '%Y-%m-%d' ) project_tz = timezone ( 'US/Pacific' ) localized_datetime = project_tz . localize ( doc_datetime ) doc_datetime = localized_datetime . astimezone ( pytz . utc ) self . _revision_datetime_source = 'tex' except ValueError : self . _logger . warning ( 'Could not parse a datetime from ' 'lsstdoc date command: %r' , command_content ) if doc_datetime is None : content_extensions = ( 'tex' , 'bib' , 'pdf' , 'png' , 'jpg' ) try : doc_datetime = get_content_commit_date ( content_extensions , root_dir = self . _root_dir ) self . _revision_datetime_source = 'git' except RuntimeError : self . _logger . warning ( 'Could not get a datetime from the Git ' 'repository at %r' , self . _root_dir ) if doc_datetime is None : doc_datetime = pytz . utc . localize ( datetime . datetime . now ( ) ) self . _revision_datetime_source = 'now' self . _datetime = doc_datetime
11817	def get_value ( self , context , default ) : if default is None : settings = self . setting_model . objects . as_dict ( ) else : settings = self . setting_model . objects . as_dict ( default = default ) return settings
12329	def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
10246	def count_confidences ( graph : BELGraph ) -> typing . Counter [ str ] : return Counter ( ( 'None' if ANNOTATIONS not in data or 'Confidence' not in data [ ANNOTATIONS ] else list ( data [ ANNOTATIONS ] [ 'Confidence' ] ) [ 0 ] ) for _ , _ , data in graph . edges ( data = True ) if CITATION in data )
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
12411	def close ( self ) : self . require_not_closed ( ) if not self . streaming or self . asynchronous : if 'Content-Length' not in self . headers : self . headers [ 'Content-Length' ] = self . tell ( ) self . flush ( ) self . _closed = True
5280	def sklearn2pmml ( pipeline , pmml , user_classpath = [ ] , with_repr = False , debug = False , java_encoding = "UTF-8" ) : if debug : java_version = _java_version ( java_encoding ) if java_version is None : java_version = ( "java" , "N/A" ) print ( "python: {0}" . format ( platform . python_version ( ) ) ) print ( "sklearn: {0}" . format ( sklearn . __version__ ) ) print ( "sklearn.externals.joblib: {0}" . format ( joblib . __version__ ) ) print ( "pandas: {0}" . format ( pandas . __version__ ) ) print ( "sklearn_pandas: {0}" . format ( sklearn_pandas . __version__ ) ) print ( "sklearn2pmml: {0}" . format ( __version__ ) ) print ( "{0}: {1}" . format ( java_version [ 0 ] , java_version [ 1 ] ) ) if not isinstance ( pipeline , PMMLPipeline ) : raise TypeError ( "The pipeline object is not an instance of " + PMMLPipeline . __name__ + ". Use the 'sklearn2pmml.make_pmml_pipeline(obj)' utility function to translate a regular Scikit-Learn estimator or pipeline to a PMML pipeline" ) estimator = pipeline . _final_estimator cmd = [ "java" , "-cp" , os . pathsep . join ( _classpath ( user_classpath ) ) , "org.jpmml.sklearn.Main" ] dumps = [ ] try : if with_repr : pipeline . repr_ = repr ( pipeline ) if hasattr ( estimator , "download_mojo" ) : estimator_mojo = estimator . download_mojo ( ) dumps . append ( estimator_mojo ) estimator . _mojo_path = estimator_mojo pipeline_pkl = _dump ( pipeline , "pipeline" ) cmd . extend ( [ "--pkl-pipeline-input" , pipeline_pkl ] ) dumps . append ( pipeline_pkl ) cmd . extend ( [ "--pmml-output" , pmml ] ) if debug : print ( "Executing command:\n{0}" . format ( " " . join ( cmd ) ) ) try : process = Popen ( cmd , stdout = PIPE , stderr = PIPE , bufsize = 1 ) except OSError : raise RuntimeError ( "Java is not installed, or the Java executable is not on system path" ) output , error = process . communicate ( ) retcode = process . poll ( ) if debug or retcode : if ( len ( output ) > 0 ) : print ( "Standard output:\n{0}" . format ( _decode ( output , java_encoding ) ) ) else : print ( "Standard output is empty" ) if ( len ( error ) > 0 ) : print ( "Standard error:\n{0}" . format ( _decode ( error , java_encoding ) ) ) else : print ( "Standard error is empty" ) if retcode : raise RuntimeError ( "The JPMML-SkLearn conversion application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams" ) finally : if debug : print ( "Preserved joblib dump file(s): {0}" . format ( " " . join ( dumps ) ) ) else : for dump in dumps : os . remove ( dump )
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
114	def pool ( self ) : if self . _pool is None : processes = self . processes if processes is not None and processes < 0 : try : processes = multiprocessing . cpu_count ( ) - abs ( processes ) processes = max ( processes , 1 ) except ( ImportError , NotImplementedError ) : processes = None self . _pool = multiprocessing . Pool ( processes , initializer = _Pool_initialize_worker , initargs = ( self . augseq , self . seed ) , maxtasksperchild = self . maxtasksperchild ) return self . _pool
3992	def get_nginx_configuration_spec ( port_spec_dict , docker_bridge_ip ) : nginx_http_config , nginx_stream_config = "" , "" for port_spec in port_spec_dict [ 'nginx' ] : if port_spec [ 'type' ] == 'http' : nginx_http_config += _nginx_http_spec ( port_spec , docker_bridge_ip ) elif port_spec [ 'type' ] == 'stream' : nginx_stream_config += _nginx_stream_spec ( port_spec , docker_bridge_ip ) return { 'http' : nginx_http_config , 'stream' : nginx_stream_config }
8057	def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
13875	def CopyFiles ( source_dir , target_dir , create_target_dir = False , md5_check = False ) : import fnmatch if IsDir ( source_dir ) : source_mask = '*' else : source_dir , source_mask = os . path . split ( source_dir ) if not IsDir ( target_dir ) : if create_target_dir : CreateDirectory ( target_dir ) else : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( target_dir ) filenames = ListFiles ( source_dir ) if filenames is None : return for i_filename in filenames : if md5_check and i_filename . endswith ( '.md5' ) : continue if fnmatch . fnmatch ( i_filename , source_mask ) : source_path = source_dir + '/' + i_filename target_path = target_dir + '/' + i_filename if IsDir ( source_path ) : CopyFiles ( source_path , target_path , create_target_dir = True , md5_check = md5_check ) else : CopyFile ( source_path , target_path , md5_check = md5_check )
12656	def dictify ( a_named_tuple ) : return dict ( ( s , getattr ( a_named_tuple , s ) ) for s in a_named_tuple . _fields )
13210	def _load_bib_db ( self ) : r command = LatexCommand ( 'bibliography' , { 'name' : 'bib_names' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) bib_names = [ n . strip ( ) for n in parsed [ 'bib_names' ] . split ( ',' ) ] except StopIteration : self . _logger . warning ( 'lsstdoc has no bibliography command' ) bib_names = [ ] custom_bib_names = [ n for n in bib_names if n not in KNOWN_LSSTTEXMF_BIB_NAMES ] custom_bibs = [ ] for custom_bib_name in custom_bib_names : custom_bib_path = os . path . join ( os . path . join ( self . _root_dir ) , custom_bib_name + '.bib' ) if not os . path . exists ( custom_bib_path ) : self . _logger . warning ( 'Could not find bibliography %r' , custom_bib_path ) continue with open ( custom_bib_path , 'r' ) as file_handle : custom_bibs . append ( file_handle . read ( ) ) if len ( custom_bibs ) > 0 : custom_bibtex = '\n\n' . join ( custom_bibs ) else : custom_bibtex = None db = get_bibliography ( bibtex = custom_bibtex ) self . _bib_db = db
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
13850	def ensure_dir_exists ( func ) : "wrap a function that returns a dir, making sure it exists" @ functools . wraps ( func ) def make_if_not_present ( ) : dir = func ( ) if not os . path . isdir ( dir ) : os . makedirs ( dir ) return dir return make_if_not_present
10909	def missing_particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : s = init . create_two_particle_state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
4208	def lpc ( x , N = None ) : m = len ( x ) if N is None : N = m - 1 elif N > m - 1 : x . resize ( N + 1 ) X = fft ( x , 2 ** nextpow2 ( 2. * len ( x ) - 1 ) ) R = real ( ifft ( abs ( X ) ** 2 ) ) R = R / ( m - 1. ) a , e , ref = LEVINSON ( R , N ) return a , e
13329	def remove ( path ) : r = cpenv . resolve ( path ) if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
9919	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = True ) except models . EmailAddress . DoesNotExist : return None token = models . PasswordResetToken . objects . create ( email = email ) token . send ( ) return token
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
13789	def marv ( ctx , config , loglevel , logfilter , verbosity ) : if config is None : cwd = os . path . abspath ( os . path . curdir ) while cwd != os . path . sep : config = os . path . join ( cwd , 'marv.conf' ) if os . path . exists ( config ) : break cwd = os . path . dirname ( cwd ) else : config = '/etc/marv/marv.conf' if not os . path . exists ( config ) : config = None ctx . obj = config setup_logging ( loglevel , verbosity , logfilter )
4245	def _gethostbyname ( self , hostname ) : if self . _databaseType in const . IPV6_EDITIONS : response = socket . getaddrinfo ( hostname , 0 , socket . AF_INET6 ) family , socktype , proto , canonname , sockaddr = response [ 0 ] address , port , flow , scope = sockaddr return address else : return socket . gethostbyname ( hostname )
1602	def parse_topo_loc ( cl_args ) : try : topo_loc = cl_args [ 'cluster/[role]/[env]' ] . split ( '/' ) topo_name = cl_args [ 'topology-name' ] topo_loc . append ( topo_name ) if len ( topo_loc ) != 4 : raise return topo_loc except Exception : Log . error ( 'Invalid topology location' ) raise
12495	def column_or_1d ( y , warn = False ) : shape = np . shape ( y ) if len ( shape ) == 1 : return np . ravel ( y ) if len ( shape ) == 2 and shape [ 1 ] == 1 : if warn : warnings . warn ( "A column-vector y was passed when a 1d array was" " expected. Please change the shape of y to " "(n_samples, ), for example using ravel()." , DataConversionWarning , stacklevel = 2 ) return np . ravel ( y ) raise ValueError ( "bad input shape {0}" . format ( shape ) )
13573	def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get_selected ( ) if sel . course . tid != course . tid : sel = None except NoExerciseSelected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . DoesNotExist : print ( "There are no more exercises in this course." ) return False sel . set_select ( ) list_all ( single = sel )
3095	def http ( self , * args , ** kwargs ) : return self . credentials . authorize ( transport . get_http_object ( * args , ** kwargs ) )
5156	def _get_install_context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
152	def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
5190	def send_direct_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . DirectOperate ( command , index , callback , config )
11736	def _validate_schema ( obj ) : if obj is not None and not isinstance ( obj , Schema ) : raise IncompatibleSchema ( 'Schema must be of type {0}' . format ( Schema ) ) return obj
9810	def dashboard ( yes , url ) : dashboard_url = "{}/app" . format ( PolyaxonClient ( ) . api_config . http_host ) if url : click . echo ( dashboard_url ) sys . exit ( 0 ) if not yes : click . confirm ( 'Dashboard page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( dashboard_url )
6004	def setup_random_seed ( seed ) : if seed == - 1 : seed = np . random . randint ( 0 , int ( 1e9 ) ) np . random . seed ( seed )
10424	def infer_missing_two_way_edges ( graph ) : for u , v , k , d in graph . edges ( data = True , keys = True ) : if d [ RELATION ] in TWO_WAY_RELATIONS : infer_missing_backwards_edge ( graph , u , v , k )
13739	def _keep_alive_thread ( self ) : while True : with self . _lock : if self . connected ( ) : self . _ws . ping ( ) else : self . disconnect ( ) self . _thread = None return sleep ( 30 )
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , ** kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
1624	def CloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if ( line [ pos ] not in '({[<' ) or Match ( r'<[<=]' , line [ pos : ] ) : return ( line , clean_lines . NumLines ( ) , - 1 ) ( end_pos , stack ) = FindEndOfExpressionInLine ( line , pos , [ ] ) if end_pos > - 1 : return ( line , linenum , end_pos ) while stack and linenum < clean_lines . NumLines ( ) - 1 : linenum += 1 line = clean_lines . elided [ linenum ] ( end_pos , stack ) = FindEndOfExpressionInLine ( line , 0 , stack ) if end_pos > - 1 : return ( line , linenum , end_pos ) return ( line , clean_lines . NumLines ( ) , - 1 )
2065	def inverse_transform ( self , X_in ) : X = X_in . copy ( deep = True ) X = util . convert_input ( X ) if self . _dim is None : raise ValueError ( 'Must train encoder before it can be used to inverse_transform data' ) if X . shape [ 1 ] != self . _dim : if self . drop_invariant : raise ValueError ( "Unexpected input dimension %d, the attribute drop_invariant should " "set as False when transform data" % ( X . shape [ 1 ] , ) ) else : raise ValueError ( 'Unexpected input dimension %d, expected %d' % ( X . shape [ 1 ] , self . _dim , ) ) if not self . cols : return X if self . return_df else X . values if self . handle_unknown == 'value' : for col in self . cols : if any ( X [ col ] == - 1 ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category -1 when encode %s" % ( col , ) ) if self . handle_unknown == 'return_nan' and self . handle_missing == 'return_nan' : for col in self . cols : if X [ col ] . isnull ( ) . any ( ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category nan when encode %s" % ( col , ) ) for switch in self . mapping : column_mapping = switch . get ( 'mapping' ) inverse = pd . Series ( data = column_mapping . index , index = column_mapping . get_values ( ) ) X [ switch . get ( 'col' ) ] = X [ switch . get ( 'col' ) ] . map ( inverse ) . astype ( switch . get ( 'data_type' ) ) return X if self . return_df else X . values
7423	def ref_muscle_chunker ( data , sample ) : LOGGER . info ( 'entering ref_muscle_chunker' ) regions = bedtools_merge ( data , sample ) if len ( regions ) > 0 : get_overlapping_reads ( data , sample , regions ) else : msg = "No reads mapped to reference sequence - {}" . format ( sample . name ) LOGGER . warn ( msg )
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
2201	def ensure_app_config_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_config_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
2484	def create_checksum_node ( self , chksum ) : chksum_node = BNode ( ) type_triple = ( chksum_node , RDF . type , self . spdx_namespace . Checksum ) self . graph . add ( type_triple ) algorithm_triple = ( chksum_node , self . spdx_namespace . algorithm , Literal ( chksum . identifier ) ) self . graph . add ( algorithm_triple ) value_triple = ( chksum_node , self . spdx_namespace . checksumValue , Literal ( chksum . value ) ) self . graph . add ( value_triple ) return chksum_node
5430	def _local_uri_rewriter ( raw_uri ) : raw_path , filename = os . path . split ( raw_uri ) prefix_replacements = [ ( 'file:///' , '/' ) , ( '~/' , os . getenv ( 'HOME' ) ) , ( './' , '' ) , ( 'file:/' , '/' ) ] normed_path = raw_path for prefix , replacement in prefix_replacements : if normed_path . startswith ( prefix ) : normed_path = os . path . join ( replacement , normed_path [ len ( prefix ) : ] ) normed_uri = directory_fmt ( os . path . abspath ( normed_path ) ) normed_uri = os . path . join ( normed_uri , filename ) docker_rewrites = [ ( r'/\.\.' , '/_dotdot_' ) , ( r'^\.\.' , '_dotdot_' ) , ( r'^~/' , '_home_/' ) , ( r'^file:/' , '' ) ] docker_path = os . path . normpath ( raw_path ) for pattern , replacement in docker_rewrites : docker_path = re . sub ( pattern , replacement , docker_path ) docker_path = docker_path . lstrip ( './' ) docker_path = directory_fmt ( 'file/' + docker_path ) + filename return normed_uri , docker_path
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
3910	def _rename ( self , name , callback ) : self . _coroutine_queue . put ( self . _conversation . rename ( name ) ) callback ( )
3633	def cardInfo ( self , resource_id ) : base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
2725	def __get_ssh_keys_id_or_fingerprint ( ssh_keys , token , name ) : ssh_keys_id = list ( ) for ssh_key in ssh_keys : if type ( ssh_key ) in [ int , type ( 2 ** 64 ) ] : ssh_keys_id . append ( int ( ssh_key ) ) elif type ( ssh_key ) == SSHKey : ssh_keys_id . append ( ssh_key . id ) elif type ( ssh_key ) in [ type ( u'' ) , type ( '' ) ] : regexp_of_fingerprint = '([0-9a-fA-F]{2}:){15}[0-9a-fA-F]' match = re . match ( regexp_of_fingerprint , ssh_key ) if match is not None and match . end ( ) == len ( ssh_key ) - 1 : ssh_keys_id . append ( ssh_key ) else : key = SSHKey ( ) key . token = token results = key . load_by_pub_key ( ssh_key ) if results is None : key . public_key = ssh_key key . name = "SSH Key %s" % name key . create ( ) else : key = results ssh_keys_id . append ( key . id ) else : raise BadSSHKeyFormat ( "Droplet.ssh_keys should be a list of IDs, public keys" + " or fingerprints." ) return ssh_keys_id
10144	def from_schema ( self , schema_node , base_name = None ) : return self . _ref_recursive ( self . type_converter ( schema_node ) , self . ref , base_name )
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
13250	def get_authoryear_from_entry ( entry , paren = False ) : def _format_last ( person ) : return ' ' . join ( [ n . strip ( '{}' ) for n in person . last_names ] ) if len ( entry . persons [ 'author' ] ) > 0 : persons = entry . persons [ 'author' ] elif len ( entry . persons [ 'editor' ] ) > 0 : persons = entry . persons [ 'editor' ] else : raise AuthorYearError try : year = entry . fields [ 'year' ] except KeyError : raise AuthorYearError if paren and len ( persons ) == 1 : template = '{author} ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif not paren and len ( persons ) == 1 : template = '{author} {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) == 2 : template = '{author1} and {author2} ({year})' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) == 2 : template = '{author1} and {author2} {year}' return template . format ( author1 = _format_last ( persons [ 0 ] ) , author2 = _format_last ( persons [ 1 ] ) , year = year ) elif not paren and len ( persons ) > 2 : template = '{author} et al {year}' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year ) elif paren and len ( persons ) > 2 : template = '{author} et al ({year})' return template . format ( author = _format_last ( persons [ 0 ] ) , year = year )
1366	def validateInterval ( self , startTime , endTime ) : start = int ( startTime ) end = int ( endTime ) if start > end : raise Exception ( "starttime is greater than endtime." )
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
5808	def parse_tls_records ( data ) : pointer = 0 data_len = len ( data ) while pointer < data_len : if data [ pointer : pointer + 1 ] == b'\x14' : break length = int_from_bytes ( data [ pointer + 3 : pointer + 5 ] ) yield ( data [ pointer : pointer + 1 ] , data [ pointer + 1 : pointer + 3 ] , data [ pointer + 5 : pointer + 5 + length ] ) pointer += 5 + length
10062	def deposit_links_factory ( pid ) : links = default_links_factory ( pid ) def _url ( name , ** kwargs ) : endpoint = '.{0}_{1}' . format ( current_records_rest . default_endpoint_prefixes [ pid . pid_type ] , name , ) return url_for ( endpoint , pid_value = pid . pid_value , _external = True , ** kwargs ) links [ 'files' ] = _url ( 'files' ) ui_endpoint = current_app . config . get ( 'DEPOSIT_UI_ENDPOINT' ) if ui_endpoint is not None : links [ 'html' ] = ui_endpoint . format ( host = request . host , scheme = request . scheme , pid_value = pid . pid_value , ) deposit_cls = Deposit if 'pid_value' in request . view_args : deposit_cls = request . view_args [ 'pid_value' ] . data [ 1 ] . __class__ for action in extract_actions_from_class ( deposit_cls ) : links [ action ] = _url ( 'actions' , action = action ) return links
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
11987	async def trigger ( self , event , data = None , socket_id = None ) : json_data = json . dumps ( data , cls = self . pusher . encoder ) query_string = self . signed_query ( event , json_data , socket_id ) signed_path = "%s?%s" % ( self . path , query_string ) pusher = self . pusher absolute_url = pusher . get_absolute_path ( signed_path ) response = await pusher . http . post ( absolute_url , data = json_data , headers = [ ( 'Content-Type' , 'application/json' ) ] ) response . raise_for_status ( ) return response . status_code == 202
10797	def users ( ) : from invenio_groups . models import Group , Membership , PrivacyPolicy , SubscriptionPolicy admin = accounts . datastore . create_user ( email = 'admin@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) reader = accounts . datastore . create_user ( email = 'reader@inveniosoftware.org' , password = encrypt_password ( '123456' ) , active = True , ) admins = Group . create ( name = 'admins' , admins = [ admin ] ) for i in range ( 10 ) : Group . create ( name = 'group-{0}' . format ( i ) , admins = [ admin ] ) Membership . create ( admins , reader ) db . session . commit ( )
9562	def _apply_skips ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for skip in self . _skips : try : result = skip ( r ) if result is True : yield True except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( skip . __name__ , skip . __doc__ ) if context is not None : p [ 'context' ] = context yield p
11854	def scanner ( self , j , word ) : "For each edge expecting a word of this category here, extend the edge." for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
8445	def ls ( github_user , template , long_format ) : github_urls = temple . ls . ls ( github_user , template = template ) for ssh_path , info in github_urls . items ( ) : if long_format : print ( ssh_path , '-' , info [ 'description' ] or '(no project description found)' ) else : print ( ssh_path )
7424	def check_insert_size ( data , sample ) : cmd1 = [ ipyrad . bins . samtools , "stats" , sample . files . mapped_reads ] cmd2 = [ "grep" , "SN" ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , res ) avg_insert = 0 stdv_insert = 0 avg_len = 0 for line in res . split ( "\n" ) : if "insert size average" in line : avg_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) elif "insert size standard deviation" in line : stdv_insert = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) + 0.1 elif "average length" in line : avg_len = float ( line . split ( ":" ) [ - 1 ] . strip ( ) ) LOGGER . debug ( "avg {} stdv {} avg_len {}" . format ( avg_insert , stdv_insert , avg_len ) ) if all ( [ avg_insert , stdv_insert , avg_len ] ) : if stdv_insert < 5 : stdv_insert = 5. if ( 2 * avg_len ) < avg_insert : hack = avg_insert + ( 3 * np . math . ceil ( stdv_insert ) ) - ( 2 * avg_len ) else : hack = ( avg_insert - avg_len ) + ( 3 * np . math . ceil ( stdv_insert ) ) LOGGER . info ( "stdv: hacked insert size is %s" , hack ) data . _hackersonly [ "max_inner_mate_distance" ] = int ( np . math . ceil ( hack ) ) else : data . _hackersonly [ "max_inner_mate_distance" ] = 300 LOGGER . debug ( "inner mate distance for {} - {}" . format ( sample . name , data . _hackersonly [ "max_inner_mate_distance" ] ) )
2290	def create_graph_from_data ( self , data ) : warnings . warn ( "An exhaustive search of the causal structure of CGNN without" " skeleton is super-exponential in the number of variables." ) nb_vars = len ( list ( data . columns ) ) data = scale ( data . values ) . astype ( 'float32' ) candidates = [ np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) for i in itertools . product ( [ 0 , 1 ] , repeat = nb_vars * nb_vars ) if ( np . trace ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) == 0 and nx . is_directed_acyclic_graph ( nx . DiGraph ( np . reshape ( np . array ( i ) , ( nb_vars , nb_vars ) ) ) ) ) ] warnings . warn ( "A total of {} graphs will be evaluated." . format ( len ( candidates ) ) ) scores = [ parallel_graph_evaluation ( data , i , nh = self . nh , nb_runs = self . nb_runs , gpu = self . gpu , nb_jobs = self . nb_jobs , lr = self . lr , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose ) for i in candidates ] final_candidate = candidates [ scores . index ( min ( scores ) ) ] output = np . zeros ( final_candidate . shape ) for ( i , j ) , x in np . ndenumerate ( final_candidate ) : if x > 0 : cand = final_candidate cand [ i , j ] = 0 output [ i , j ] = min ( scores ) - scores [ candidates . index ( cand ) ] return nx . DiGraph ( candidates [ output ] , { idx : i for idx , i in enumerate ( data . columns ) } )
4624	def _get_encrypted_masterpassword ( self ) : if not self . unlocked ( ) : raise WalletLocked aes = AESCipher ( self . password ) return "{}${}" . format ( self . _derive_checksum ( self . masterkey ) , aes . encrypt ( self . masterkey ) )
13127	def get_pipe ( self ) : lines = [ ] for line in sys . stdin : try : lines . append ( self . line_to_object ( line . strip ( ) ) ) except ValueError : pass except KeyError : pass return lines
7009	def _fourier_residual ( fourierparams , phase , mags ) : f = _fourier_func ( fourierparams , phase , mags ) residual = mags - f return residual
6662	def generate_csr ( self , domain = '' , r = None ) : r = r or self . local_renderer r . env . domain = domain or r . env . domain role = self . genv . ROLE or ALL site = self . genv . SITE or self . genv . default_site print ( 'self.genv.default_site:' , self . genv . default_site , file = sys . stderr ) print ( 'site.csr0:' , site , file = sys . stderr ) ssl_dst = 'roles/%s/ssl' % ( role , ) print ( 'ssl_dst:' , ssl_dst ) if not os . path . isdir ( ssl_dst ) : os . makedirs ( ssl_dst ) for site , site_data in self . iter_sites ( ) : print ( 'site.csr1:' , site , file = sys . stderr ) assert r . env . domain , 'No SSL domain defined.' r . env . ssl_base_dst = '%s/%s' % ( ssl_dst , r . env . domain . replace ( '*.' , '' ) ) r . env . ssl_csr_year = date . today ( ) . year r . local ( 'openssl req -nodes -newkey rsa:{ssl_length} ' '-subj "/C={ssl_country}/ST={ssl_state}/L={ssl_city}/O={ssl_organization}/CN={ssl_domain}" ' '-keyout {ssl_base_dst}.{ssl_csr_year}.key -out {ssl_base_dst}.{ssl_csr_year}.csr' )
6722	def list_instances ( show = 1 , name = None , group = None , release = None , except_release = None ) : from burlap . common import shelf , OrderedDict , get_verbose verbose = get_verbose ( ) require ( 'vm_type' , 'vm_group' ) assert env . vm_type , 'No VM type specified.' env . vm_type = ( env . vm_type or '' ) . lower ( ) _name = name _group = group _release = release if verbose : print ( 'name=%s, group=%s, release=%s' % ( _name , _group , _release ) ) env . vm_elastic_ip_mappings = shelf . get ( 'vm_elastic_ip_mappings' ) data = type ( env ) ( ) if env . vm_type == EC2 : if verbose : print ( 'Checking EC2...' ) for instance in get_all_running_ec2_instances ( ) : name = instance . tags . get ( env . vm_name_tag ) group = instance . tags . get ( env . vm_group_tag ) release = instance . tags . get ( env . vm_release_tag ) if env . vm_group and env . vm_group != group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match env.vm_group "%s".' ) % ( instance . public_dns_name , group , env . vm_group ) ) continue if _group and group != _group : if verbose : print ( ( 'Skipping instance %s because its group "%s" ' 'does not match local group "%s".' ) % ( instance . public_dns_name , group , _group ) ) continue if _name and name != _name : if verbose : print ( ( 'Skipping instance %s because its name "%s" ' 'does not match name "%s".' ) % ( instance . public_dns_name , name , _name ) ) continue if _release and release != _release : if verbose : print ( ( 'Skipping instance %s because its release "%s" ' 'does not match release "%s".' ) % ( instance . public_dns_name , release , _release ) ) continue if except_release and release == except_release : continue if verbose : print ( 'Adding instance %s (%s).' % ( name , instance . public_dns_name ) ) data . setdefault ( name , type ( env ) ( ) ) data [ name ] [ 'id' ] = instance . id data [ name ] [ 'public_dns_name' ] = instance . public_dns_name if verbose : print ( 'Public DNS: %s' % instance . public_dns_name ) if env . vm_elastic_ip_mappings and name in env . vm_elastic_ip_mappings : data [ name ] [ 'ip' ] = env . vm_elastic_ip_mappings [ name ] else : data [ name ] [ 'ip' ] = socket . gethostbyname ( instance . public_dns_name ) if int ( show ) : pprint ( data , indent = 4 ) return data elif env . vm_type == KVM : pass else : raise NotImplementedError
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
9965	def _to_attrdict ( self , attrs = None ) : result = self . _baseattrs for attr in attrs : if hasattr ( self , attr ) : result [ attr ] = getattr ( self , attr ) . _to_attrdict ( attrs ) return result
6909	def xieta_from_radecl ( inra , indecl , incenterra , incenterdecl , deg = True ) : if deg : ra = np . radians ( inra ) decl = np . radians ( indecl ) centerra = np . radians ( incenterra ) centerdecl = np . radians ( incenterdecl ) else : ra = inra decl = indecl centerra = incenterra centerdecl = incenterdecl cdecc = np . cos ( centerdecl ) sdecc = np . sin ( centerdecl ) crac = np . cos ( centerra ) srac = np . sin ( centerra ) uu = np . cos ( decl ) * np . cos ( ra ) vv = np . cos ( decl ) * np . sin ( ra ) ww = np . sin ( decl ) uun = uu * cdecc * crac + vv * cdecc * srac + ww * sdecc vvn = - uu * srac + vv * crac wwn = - uu * sdecc * crac - vv * sdecc * srac + ww * cdecc denom = vvn * vvn + wwn * wwn aunn = np . zeros_like ( uun ) aunn [ uun >= 1.0 ] = 0.0 aunn [ uun < 1.0 ] = np . arccos ( uun ) xi , eta = np . zeros_like ( aunn ) , np . zeros_like ( aunn ) xi [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 eta [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 sdenom = np . sqrt ( denom ) xi [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * vvn / sdenom eta [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * wwn / sdenom if deg : return np . degrees ( xi ) , np . degrees ( eta ) else : return xi , eta
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
10910	def name_globals ( s , remove_params = None ) : all_params = s . params for p in s . param_particle ( np . arange ( s . obj_get_positions ( ) . shape [ 0 ] ) ) : all_params . remove ( p ) if remove_params is not None : for p in set ( remove_params ) : all_params . remove ( p ) return all_params
2163	def list ( self , group = None , host_filter = None , ** kwargs ) : if group : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'groups__in' , group ) , ) if host_filter : kwargs [ 'query' ] = kwargs . get ( 'query' , ( ) ) + ( ( 'host_filter' , host_filter ) , ) return super ( Resource , self ) . list ( ** kwargs )
4276	def get_albums ( self , path ) : for name in self . albums [ path ] . subdirs : subdir = os . path . normpath ( join ( path , name ) ) yield subdir , self . albums [ subdir ] for subname , album in self . get_albums ( subdir ) : yield subname , self . albums [ subdir ]
6493	def get_mappings ( cls , index_name , doc_type ) : return cache . get ( cls . get_cache_item_name ( index_name , doc_type ) , { } )
1777	def TEST ( cpu , src1 , src2 ) : temp = src1 . read ( ) & src2 . read ( ) cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 cpu . ZF = temp == 0 cpu . PF = cpu . _calculate_parity_flag ( temp ) cpu . CF = False cpu . OF = False
5213	def intraday ( ticker , dt , session = '' , ** kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( ** kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( ** kw ) ) return cur_data
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : if element [ 'required' ] is True : content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : break if element_start is None and element [ 'required' ] is False : continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
11790	def revise ( csp , Xi , Xj , removals ) : "Return true if we remove a value." revised = False for x in csp . curr_domains [ Xi ] [ : ] : if every ( lambda y : not csp . constraints ( Xi , x , Xj , y ) , csp . curr_domains [ Xj ] ) : csp . prune ( Xi , x , removals ) revised = True return revised
5909	def parse_groups ( output ) : groups = [ ] for line in output . split ( '\n' ) : m = NDXGROUP . match ( line ) if m : d = m . groupdict ( ) groups . append ( { 'name' : d [ 'GROUPNAME' ] , 'nr' : int ( d [ 'GROUPNUMBER' ] ) , 'natoms' : int ( d [ 'NATOMS' ] ) } ) return groups
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
8080	def relmoveto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relmoveto ( x , y )
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
12263	def _load_meta ( self , size , md5 ) : if not hasattr ( self , 'local_hashes' ) : self . local_hashes = { } self . size = int ( size ) if ( re . match ( '^[a-fA-F0-9]{32}$' , md5 ) ) : self . md5 = md5
4957	def parse_csv ( file_stream , expected_columns = None ) : reader = unicodecsv . DictReader ( file_stream , encoding = "utf-8" ) if expected_columns and set ( expected_columns ) - set ( reader . fieldnames ) : raise ValidationError ( ValidationMessages . MISSING_EXPECTED_COLUMNS . format ( expected_columns = ", " . join ( expected_columns ) , actual_columns = ", " . join ( reader . fieldnames ) ) ) for row in reader : yield row
7990	def transport_connected ( self ) : with self . lock : if self . initiator : if self . _output_state is None : self . _initiate ( )
7164	def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cache ) self . padaos . add_intent ( name , lines ) self . must_train = True
13229	def create_jwt ( integration_id , private_key_path ) : integration_id = int ( integration_id ) with open ( private_key_path , 'rb' ) as f : cert_bytes = f . read ( ) now = datetime . datetime . now ( ) expiration_time = now + datetime . timedelta ( minutes = 9 ) payload = { 'iat' : int ( now . timestamp ( ) ) , 'exp' : int ( expiration_time . timestamp ( ) ) , 'iss' : integration_id } return jwt . encode ( payload , cert_bytes , algorithm = 'RS256' )
9163	def lookup_api_key_info ( ) : info = { } with db_connect ( ) as conn : with conn . cursor ( ) as cursor : cursor . execute ( ALL_KEY_INFO_SQL_STMT ) for row in cursor . fetchall ( ) : id , key , name , groups = row user_id = "api_key:{}" . format ( id ) info [ key ] = dict ( id = id , user_id = user_id , name = name , groups = groups ) return info
900	def mmPrettyPrintTraces ( traces , breakOnResets = None ) : assert len ( traces ) > 0 , "No traces found" table = PrettyTable ( [ "#" ] + [ trace . prettyPrintTitle ( ) for trace in traces ] ) for i in xrange ( len ( traces [ 0 ] . data ) ) : if breakOnResets and breakOnResets . data [ i ] : table . add_row ( [ "<reset>" ] * ( len ( traces ) + 1 ) ) table . add_row ( [ i ] + [ trace . prettyPrintDatum ( trace . data [ i ] ) for trace in traces ] ) return table . get_string ( ) . encode ( "utf-8" )
333	def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax
6042	def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
2915	def cancel ( self ) : if self . _is_finished ( ) : for child in self . children : child . cancel ( ) return self . _set_state ( self . CANCELLED ) self . _drop_children ( ) self . task_spec . _on_cancel ( self )
11292	def consume_json ( request ) : client = OEmbedConsumer ( ) urls = request . GET . getlist ( 'urls' ) width = request . GET . get ( 'width' ) height = request . GET . get ( 'height' ) template_dir = request . GET . get ( 'template_dir' ) output = { } ctx = RequestContext ( request ) for url in urls : try : provider = oembed . site . provider_for_url ( url ) except OEmbedMissingEndpoint : oembeds = None rendered = None else : oembeds = url rendered = client . parse_text ( url , width , height , context = ctx , template_dir = template_dir ) output [ url ] = { 'oembeds' : oembeds , 'rendered' : rendered , } return HttpResponse ( simplejson . dumps ( output ) , mimetype = 'application/json' )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
827	def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = ", " . join ( key for key in obj . keys ( ) if not key . startswith ( "_" ) ) raise ValueError ( "Unknown field name '%s' in input record. Known fields are '%s'.\n" "This could be because input headers are mislabeled, or because " "input data rows do not contain a value for '%s'." % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )
11890	def set_brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . _zid , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
4561	def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
11879	def scanProcessForCwd ( pid , searchPortion , isExactMatch = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e cwd = getProcessCwd ( pid ) if not cwd : return None isMatch = False if isExactMatch is True : if searchPortion == cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] == cwd : isMatch = True else : if searchPortion in cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] in cwd : isMatch = True if not isMatch : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'cwd' : cwd , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
4780	def is_less_than ( self , other ) : self . _validate_compareable ( other ) if self . val >= other : if type ( self . val ) is datetime . datetime : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) else : self . _err ( 'Expected <%s> to be less than <%s>, but was not.' % ( self . val , other ) ) return self
10390	def workflow ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , minimum_nodes : int = 1 , ) -> List [ 'Runner' ] : subgraph = generate_mechanism ( graph , node , key = key ) if subgraph . number_of_nodes ( ) <= minimum_nodes : return [ ] runners = multirun ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) return list ( runners )
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
8616	def _b ( s , encoding = 'utf-8' ) : if six . PY2 : if isinstance ( s , str ) : return s elif isinstance ( s , unicode ) : return s . encode ( encoding ) else : if isinstance ( s , bytes ) : return s elif isinstance ( s , str ) : return s . encode ( encoding ) raise TypeError ( "Invalid argument %r for _b()" % ( s , ) )
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
4585	def animated_gif_to_colorlists ( image , container = list ) : deprecated . deprecated ( 'util.gif.animated_gif_to_colorlists' ) from PIL import ImageSequence it = ImageSequence . Iterator ( image ) return [ image_to_colorlist ( i , container ) for i in it ]
4057	def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) ) self . url_params = None return items
3221	def get_gcp_client ( ** kwargs ) : return _gcp_client ( project = kwargs [ 'project' ] , mod_name = kwargs [ 'mod_name' ] , pkg_name = kwargs . get ( 'pkg_name' , 'google.cloud' ) , key_file = kwargs . get ( 'key_file' , None ) , http_auth = kwargs . get ( 'http' , None ) , user_agent = kwargs . get ( 'user_agent' , None ) )
12710	def relative_offset_to_world ( self , offset ) : return np . array ( self . body_to_world ( offset * self . dimensions / 2 ) )
11283	def iter ( self , prev = None ) : if self . next : generator = self . next . iter ( self . func ( prev , * self . args , ** self . kw ) ) else : generator = self . func ( prev , * self . args , ** self . kw ) return generator
8828	def sg_gather_associated_ports ( context , group ) : if not group : return None if not hasattr ( group , "ports" ) or len ( group . ports ) <= 0 : return [ ] return group . ports
4972	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
13598	def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
9195	def get_publication ( request ) : publication_id = request . matchdict [ 'id' ] state , messages = check_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'state' : state , 'messages' : messages , } return response_data
2634	def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
13411	def removeLogbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . logList : if len ( logs ) == 0 or logs == "All" : del self . logList [ type ] else : for logbook in logs : if logbook in self . logList [ type ] : self . logList [ type ] . remove ( logbook ) self . changeLogType ( )
8296	def render ( self , size , frame , drawqueue ) : r_context = self . create_rcontext ( size , frame ) drawqueue . render ( r_context ) self . rendering_finished ( size , frame , r_context ) return r_context
7790	def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) self . _items_list . sort ( ) if item . state == 'purged' : self . _purged += 1 if self . _purged > 0.25 * self . max_items : self . purge_items ( ) return state finally : self . _lock . release ( )
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
6613	def receive_finished ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . poll ( )
13684	def post ( self , url , params = { } , files = None ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . post ( self . host + url , data = params , files = files ) return self . json_parse ( response . content ) except RequestException as e : return self . json_parse ( e . args )
4557	def contains ( x ) : if isinstance ( x , str ) : x = canonical_name ( x ) return x in _TO_COLOR_USER or x in _TO_COLOR else : x = tuple ( x ) return x in _TO_NAME_USER or x in _TO_NAME
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
4610	def recoverPubkeyParameter ( message , digest , signature , pubkey ) : if not isinstance ( message , bytes ) : message = bytes ( message , "utf-8" ) for i in range ( 0 , 4 ) : if SECP256K1_MODULE == "secp256k1" : sig = pubkey . ecdsa_recoverable_deserialize ( signature , i ) p = secp256k1 . PublicKey ( pubkey . ecdsa_recover ( message , sig ) ) if p . serialize ( ) == pubkey . serialize ( ) : return i elif SECP256K1_MODULE == "cryptography" and not isinstance ( pubkey , PublicKey ) : p = recover_public_key ( digest , signature , i , message ) p_comp = hexlify ( compressedPubkey ( p ) ) pubkey_comp = hexlify ( compressedPubkey ( pubkey ) ) if p_comp == pubkey_comp : return i else : p = recover_public_key ( digest , signature , i ) p_comp = hexlify ( compressedPubkey ( p ) ) p_string = hexlify ( p . to_string ( ) ) if isinstance ( pubkey , PublicKey ) : pubkey_string = bytes ( repr ( pubkey ) , "ascii" ) else : pubkey_string = hexlify ( pubkey . to_string ( ) ) if p_string == pubkey_string or p_comp == pubkey_string : return i
1860	def LODS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base size = dest . size arg0 = cpu . read_int ( src_addr , size ) dest . write ( arg0 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment )
12881	def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
1061	def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
3465	def flux ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . forward_variable . primal - self . reverse_variable . primal except AttributeError : raise RuntimeError ( "reaction '{}' is not part of a model" . format ( self . id ) ) except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
5732	def parse_response ( gdb_mi_text ) : stream = StringStream ( gdb_mi_text , debug = _DEBUG ) if _GDB_MI_NOTIFY_RE . match ( gdb_mi_text ) : token , message , payload = _get_notify_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "notify" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_RESULT_RE . match ( gdb_mi_text ) : token , message , payload = _get_result_msg_and_payload ( gdb_mi_text , stream ) return { "type" : "result" , "message" : message , "payload" : payload , "token" : token , } elif _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) : return { "type" : "console" , "message" : None , "payload" : _GDB_MI_CONSOLE_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_LOG_RE . match ( gdb_mi_text ) : return { "type" : "log" , "message" : None , "payload" : _GDB_MI_LOG_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) : return { "type" : "target" , "message" : None , "payload" : _GDB_MI_TARGET_OUTPUT_RE . match ( gdb_mi_text ) . groups ( ) [ 0 ] , } elif response_is_finished ( gdb_mi_text ) : return { "type" : "done" , "message" : None , "payload" : None } else : return { "type" : "output" , "message" : None , "payload" : gdb_mi_text }
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
11754	def parse_definite_clause ( s ) : "Return the antecedents and the consequent of a definite clause." assert is_definite_clause ( s ) if is_symbol ( s . op ) : return [ ] , s else : antecedent , consequent = s . args return conjuncts ( antecedent ) , consequent
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False copy_logs_cmd = textwrap . dedent ( ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) body = textwrap . dedent ( ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
3168	def replicate ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/replicate' ) )
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
2349	def open ( self ) : if self . seed_url : self . driver_adapter . open ( self . seed_url ) self . wait_for_page_to_load ( ) return self raise UsageError ( "Set a base URL or URL_TEMPLATE to open this page." )
4172	def _kaiser ( n , beta ) : from scipy . special import iv as besselI m = n - 1 k = arange ( 0 , m ) k = 2. * beta / m * sqrt ( k * ( m - k ) ) w = besselI ( 0 , k ) / besselI ( 0 , beta ) return w
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
6737	def get_component_settings ( prefixes = None ) : prefixes = prefixes or [ ] assert isinstance ( prefixes , ( tuple , list ) ) , 'Prefixes must be a sequence type, not %s.' % type ( prefixes ) data = { } for name in prefixes : name = name . lower ( ) . strip ( ) for k in sorted ( env ) : if k . startswith ( '%s_' % name ) : new_k = k [ len ( name ) + 1 : ] data [ new_k ] = env [ k ] return data
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
11833	def make_undirected ( self ) : "Make a digraph into an undirected graph by adding symmetric edges." for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
10422	def count_annotation_values_filtered ( graph : BELGraph , annotation : str , source_predicate : Optional [ NodePredicate ] = None , target_predicate : Optional [ NodePredicate ] = None , ) -> Counter : if source_predicate and target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) and target_predicate ( graph , v ) ) elif source_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and source_predicate ( graph , u ) ) elif target_predicate : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) and target_predicate ( graph , u ) ) else : return Counter ( data [ ANNOTATIONS ] [ annotation ] for u , v , data in graph . edges ( data = True ) if edge_has_annotation ( data , annotation ) )
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
6308	def load_resource_module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
13563	def repack ( self ) : items = self . grouped_filter ( ) . order_by ( 'rank' ) . select_for_update ( ) for count , item in enumerate ( items ) : item . rank = count + 1 item . save ( rerank = False )
13137	def _unicode ( string ) : for encoding in [ 'utf-8' , 'latin1' ] : try : result = unicode ( string , encoding ) return result except UnicodeDecodeError : pass result = unicode ( string , 'utf-8' , 'replace' ) return result
3325	def _generate_lock ( self , principal , lock_type , lock_scope , lock_depth , lock_owner , path , timeout ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT elif timeout < 0 : timeout = - 1 lock_dict = { "root" : path , "type" : lock_type , "scope" : lock_scope , "depth" : lock_depth , "owner" : lock_owner , "timeout" : timeout , "principal" : principal , } self . storage . create ( path , lock_dict ) return lock_dict
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
9698	def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
4179	def window_bartlett_hann ( N ) : r if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) a0 = 0.62 a1 = 0.48 a2 = 0.38 win = a0 - a1 * abs ( n / ( N - 1. ) - 0.5 ) - a2 * cos ( 2 * pi * n / ( N - 1. ) ) return win
13828	def read ( readme ) : extend = os . path . splitext ( readme ) [ 1 ] if ( extend == '.rst' ) : import codecs return codecs . open ( readme , 'r' , 'utf-8' ) . read ( ) elif ( extend == '.md' ) : import pypandoc return pypandoc . convert ( readme , 'rst' )
3666	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpsms = [ i ( T ) for i in self . HeatCapacitySolids ] return mixing_simple ( zs , Cpsms ) else : raise Exception ( 'Method not valid' )
2220	def lookup ( self , data ) : for func in self . lazy_init : func ( ) for type , func in self . func_registry . items ( ) : if isinstance ( data , type ) : return func
9864	def get_home ( self , home_id ) : if home_id not in self . _all_home_ids : _LOGGER . error ( "Could not find any Tibber home with id: %s" , home_id ) return None if home_id not in self . _homes . keys ( ) : self . _homes [ home_id ] = TibberHome ( home_id , self ) return self . _homes [ home_id ]
8486	def get ( self , name , default , allow_default = True ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) if name not in self . settings : if not allow_default : raise LookupError ( 'No setting "{name}"' . format ( name = name ) ) self . settings [ name ] = default return self . settings [ name ]
8694	def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main ( )
1699	def log ( self ) : from heronpy . streamlet . impl . logbolt import LogStreamlet log_streamlet = LogStreamlet ( self ) self . _add_child ( log_streamlet ) return
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
9994	def set_attr ( self , name , value ) : if not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'" % name ) if name in self . namespace : if name in self . refs : if name in self . self_refs : self . new_ref ( name , value ) else : raise KeyError ( "Ref '%s' cannot be changed" % name ) elif name in self . cells : if self . cells [ name ] . is_scalar ( ) : self . cells [ name ] . set_value ( ( ) , value ) else : raise AttributeError ( "Cells '%s' is not a scalar." % name ) else : raise ValueError else : self . new_ref ( name , value )
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , ** kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , ** kwargs ) if isinstance ( filename , string_types ) : libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
11013	def lint ( context ) : config = context . obj try : run ( 'flake8 {dir} --exclude={exclude}' . format ( dir = config [ 'CWD' ] , exclude = ',' . join ( EXCLUDE ) , ) ) except SubprocessError : context . exit ( 1 )
7220	def to_geotiff ( arr , path = './output.tif' , proj = None , spec = None , bands = None , ** kwargs ) : assert has_rasterio , "To create geotiff images please install rasterio" try : img_md = arr . rda . metadata [ "image" ] x_size = img_md [ "tileXSize" ] y_size = img_md [ "tileYSize" ] except ( AttributeError , KeyError ) : x_size = kwargs . get ( "chunk_size" , 256 ) y_size = kwargs . get ( "chunk_size" , 256 ) try : tfm = kwargs [ 'transform' ] if 'transform' in kwargs else arr . affine except : tfm = None dtype = arr . dtype . name if arr . dtype . name != 'int8' else 'uint8' if spec is not None and spec . lower ( ) == 'rgb' : if bands is None : bands = arr . _rgb_bands if not arr . options . get ( 'dra' ) : from gbdxtools . rda . interface import RDA rda = RDA ( ) dra = rda . HistogramDRA ( arr ) arr = dra . aoi ( bbox = arr . bounds ) arr = arr [ bands , ... ] . astype ( np . uint8 ) dtype = 'uint8' else : if bands is not None : arr = arr [ bands , ... ] meta = { 'width' : arr . shape [ 2 ] , 'height' : arr . shape [ 1 ] , 'count' : arr . shape [ 0 ] , 'dtype' : dtype , 'driver' : 'GTiff' , 'transform' : tfm } if proj is not None : meta [ "crs" ] = { 'init' : proj } if "tiled" in kwargs and kwargs [ "tiled" ] : meta . update ( blockxsize = x_size , blockysize = y_size , tiled = "yes" ) with rasterio . open ( path , "w" , ** meta ) as dst : writer = rio_writer ( dst ) result = store ( arr , writer , compute = False ) result . compute ( scheduler = threaded_get ) return path
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
3833	async def modify_otr_status ( self , modify_otr_status_request ) : response = hangouts_pb2 . ModifyOTRStatusResponse ( ) await self . _pb_request ( 'conversations/modifyotrstatus' , modify_otr_status_request , response ) return response
6173	def single_instance ( func = None , lock_timeout = None , include_args = False ) : if func is None : return partial ( single_instance , lock_timeout = lock_timeout , include_args = include_args ) @ wraps ( func ) def wrapped ( celery_self , * args , ** kwargs ) : timeout = ( lock_timeout or celery_self . soft_time_limit or celery_self . time_limit or celery_self . app . conf . get ( 'CELERYD_TASK_SOFT_TIME_LIMIT' ) or celery_self . app . conf . get ( 'CELERYD_TASK_TIME_LIMIT' ) or ( 60 * 5 ) ) manager_class = _select_manager ( celery_self . backend . __class__ . __name__ ) lock_manager = manager_class ( celery_self , timeout , include_args , args , kwargs ) with lock_manager : ret_value = func ( * args , ** kwargs ) return ret_value return wrapped
11305	def autodiscover ( self , url ) : headers , response = fetch_url ( url ) if headers [ 'content-type' ] . split ( ';' ) [ 0 ] in ( 'application/json' , 'text/javascript' ) : provider_data = json . loads ( response ) return self . store_providers ( provider_data )
4603	def copy ( self ) : return self . __class__ ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain_instance = self . blockchain , )
9141	def filter_labels_by_language ( labels , language , broader = False ) : if language == 'any' : return labels if broader : language = tags . tag ( language ) . language . format return [ l for l in labels if tags . tag ( l . language ) . language . format == language ] else : language = tags . tag ( language ) . format return [ l for l in labels if tags . tag ( l . language ) . format == language ]
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
756	def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
12271	def get_schema ( self , filename ) : table_set = self . read_file ( filename ) if table_set is None : return [ ] row_set = table_set . tables [ 0 ] offset , headers = headers_guess ( row_set . sample ) row_set . register_processor ( headers_processor ( headers ) ) row_set . register_processor ( offset_processor ( offset + 1 ) ) types = type_guess ( row_set . sample , strict = True ) sample = next ( row_set . sample ) clean = lambda v : str ( v ) if not isinstance ( v , str ) else v schema = [ ] for i , h in enumerate ( headers ) : schema . append ( [ h , str ( types [ i ] ) , clean ( sample [ i ] . value ) ] ) return schema
372	def shift ( x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if is_random : tx = np . random . uniform ( - hrg , hrg ) * h ty = np . random . uniform ( - wrg , wrg ) * w else : tx , ty = hrg * h , wrg * w translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
10081	def publish ( self , pid = None , id_ = None ) : pid = pid or self . pid if not pid . is_registered ( ) : raise PIDInvalidAction ( ) self [ '_deposit' ] [ 'status' ] = 'published' if self [ '_deposit' ] . get ( 'pid' ) is None : self . _publish_new ( id_ = id_ ) else : record = self . _publish_edited ( ) record . commit ( ) self . commit ( ) return self
7492	def random_product ( iter1 , iter2 ) : pool1 = tuple ( iter1 ) pool2 = tuple ( iter2 ) ind1 = random . sample ( pool1 , 2 ) ind2 = random . sample ( pool2 , 2 ) return tuple ( ind1 + ind2 )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
13539	def get_locations ( self ) : url = "/2/locations" data = self . _get_resource ( url ) locations = [ ] for entry in data [ 'locations' ] : locations . append ( self . location_from_json ( entry ) ) return locations
6006	def load_ccd_data_from_fits ( image_path , pixel_scale , image_hdu = 0 , resized_ccd_shape = None , resized_ccd_origin_pixels = None , resized_ccd_origin_arcsec = None , psf_path = None , psf_hdu = 0 , resized_psf_shape = None , renormalize_psf = True , noise_map_path = None , noise_map_hdu = 0 , noise_map_from_image_and_background_noise_map = False , convert_noise_map_from_weight_map = False , convert_noise_map_from_inverse_noise_map = False , background_noise_map_path = None , background_noise_map_hdu = 0 , convert_background_noise_map_from_weight_map = False , convert_background_noise_map_from_inverse_noise_map = False , poisson_noise_map_path = None , poisson_noise_map_hdu = 0 , poisson_noise_map_from_image = False , convert_poisson_noise_map_from_weight_map = False , convert_poisson_noise_map_from_inverse_noise_map = False , exposure_time_map_path = None , exposure_time_map_hdu = 0 , exposure_time_map_from_single_value = None , exposure_time_map_from_inverse_noise_map = False , background_sky_map_path = None , background_sky_map_hdu = 0 , convert_from_electrons = False , gain = None , convert_from_adus = False , lens_name = None ) : image = load_image ( image_path = image_path , image_hdu = image_hdu , pixel_scale = pixel_scale ) background_noise_map = load_background_noise_map ( background_noise_map_path = background_noise_map_path , background_noise_map_hdu = background_noise_map_hdu , pixel_scale = pixel_scale , convert_background_noise_map_from_weight_map = convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map = convert_background_noise_map_from_inverse_noise_map ) if background_noise_map is not None : inverse_noise_map = 1.0 / background_noise_map else : inverse_noise_map = None exposure_time_map = load_exposure_time_map ( exposure_time_map_path = exposure_time_map_path , exposure_time_map_hdu = exposure_time_map_hdu , pixel_scale = pixel_scale , shape = image . shape , exposure_time = exposure_time_map_from_single_value , exposure_time_map_from_inverse_noise_map = exposure_time_map_from_inverse_noise_map , inverse_noise_map = inverse_noise_map ) poisson_noise_map = load_poisson_noise_map ( poisson_noise_map_path = poisson_noise_map_path , poisson_noise_map_hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale , convert_poisson_noise_map_from_weight_map = convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map = convert_poisson_noise_map_from_inverse_noise_map , image = image , exposure_time_map = exposure_time_map , poisson_noise_map_from_image = poisson_noise_map_from_image , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) noise_map = load_noise_map ( noise_map_path = noise_map_path , noise_map_hdu = noise_map_hdu , pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_noise_map_from_weight_map = convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map = convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map = noise_map_from_image_and_background_noise_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) psf = load_psf ( psf_path = psf_path , psf_hdu = psf_hdu , pixel_scale = pixel_scale , renormalize = renormalize_psf ) background_sky_map = load_background_sky_map ( background_sky_map_path = background_sky_map_path , background_sky_map_hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) image = CCDData ( image = image , pixel_scale = pixel_scale , psf = psf , noise_map = noise_map , background_noise_map = background_noise_map , poisson_noise_map = poisson_noise_map , exposure_time_map = exposure_time_map , background_sky_map = background_sky_map , gain = gain , name = lens_name ) if resized_ccd_shape is not None : image = image . new_ccd_data_with_resized_arrays ( new_shape = resized_ccd_shape , new_centre_pixels = resized_ccd_origin_pixels , new_centre_arcsec = resized_ccd_origin_arcsec ) if resized_psf_shape is not None : image = image . new_ccd_data_with_resized_psf ( new_shape = resized_psf_shape ) if convert_from_electrons : image = image . new_ccd_data_converted_from_electrons ( ) elif convert_from_adus : image = image . new_ccd_data_converted_from_adus ( gain = gain ) return image
625	def _neighbors ( coordinate , radius ) : ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) return numpy . array ( list ( itertools . product ( * ranges ) ) )
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass dx , dy = 0 , 0 if s . align == 2 : dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
11734	def get_resample_data ( self ) : if self . data is not None : if self . _pvpc_mean_daily is None : self . _pvpc_mean_daily = self . data [ 'data' ] . resample ( 'D' ) . mean ( ) if self . _pvpc_mean_monthly is None : self . _pvpc_mean_monthly = self . data [ 'data' ] . resample ( 'MS' ) . mean ( ) return self . _pvpc_mean_daily , self . _pvpc_mean_monthly
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
11461	def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
3835	async def set_active_client ( self , set_active_client_request ) : response = hangouts_pb2 . SetActiveClientResponse ( ) await self . _pb_request ( 'clients/setactiveclient' , set_active_client_request , response ) return response
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
13337	def active_env_module_resolver ( resolver , path ) : from . api import get_active_env env = get_active_env ( ) if not env : raise ResolveError mod = env . get_module ( path ) if not mod : raise ResolveError return mod
6980	def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeffs [ 1 ] * npsin ( 2 * MPI * xcc ) + coeffs [ 2 ] * npcos ( 2 * MPI * xcc ) + coeffs [ 3 ] * npsin ( 2 * MPI * ycc ) + coeffs [ 4 ] * npcos ( 2 * MPI * ycc ) + coeffs [ 5 ] * npsin ( 4 * MPI * xcc ) + coeffs [ 6 ] * npcos ( 4 * MPI * xcc ) + coeffs [ 7 ] * npsin ( 4 * MPI * ycc ) + coeffs [ 8 ] * npcos ( 4 * MPI * ycc ) + coeffs [ 9 ] * bgv + coeffs [ 10 ] * bge ) return epdf
5641	def remove_dangling_shapes ( db_conn ) : db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL = "SELECT trips.trip_I, shape_id, min(shape_break) as min_shape_break, max(shape_break) as max_shape_break FROM trips, stop_times WHERE trips.trip_I=stop_times.trip_I GROUP BY trips.trip_I" trip_min_max_shape_seqs = pandas . read_sql ( SELECT_MIN_MAX_SHAPE_BREAKS_BY_TRIP_I_SQL , db_conn ) rows = [ ] for row in trip_min_max_shape_seqs . itertuples ( ) : shape_id , min_shape_break , max_shape_break = row . shape_id , row . min_shape_break , row . max_shape_break if min_shape_break is None or max_shape_break is None : min_shape_break = float ( '-inf' ) max_shape_break = float ( '-inf' ) rows . append ( ( shape_id , min_shape_break , max_shape_break ) ) DELETE_SQL_BASE = "DELETE FROM shapes WHERE shape_id=? AND (seq<? OR seq>?)" db_conn . executemany ( DELETE_SQL_BASE , rows ) remove_dangling_shapes_references ( db_conn )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
10128	def update ( self , dt ) : self . translate ( dt * self . velocity ) self . rotate ( dt * self . angular_velocity )
9096	def upload_bel_namespace ( self , update : bool = False ) -> Namespace : if not self . is_populated ( ) : self . populate ( ) namespace = self . _get_default_namespace ( ) if namespace is None : log . info ( 'making namespace for %s' , self . _get_namespace_name ( ) ) return self . _make_namespace ( ) if update : self . _update_namespace ( namespace ) return namespace
957	def _genLoggingFilePath ( ) : appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' appLogDir = os . path . abspath ( os . path . join ( os . environ [ 'NTA_LOG_DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , appName ) ) appLogFileName = '%s-%s-%s.log' % ( appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( appLogDir , appLogFileName )
6428	def encode ( self , word , lang = 'en' ) : if lang == 'es' : return self . _phonetic_spanish . encode ( self . _spanish_metaphone . encode ( word ) ) word = self . _soundex . encode ( self . _metaphone . encode ( word ) ) word = word [ 0 ] . translate ( self . _trans ) + word [ 1 : ] return word
13733	def value_to_python_log_level ( config_val , evar ) : if not config_val : config_val = evar . default_val config_val = config_val . upper ( ) return logging . _checkLevel ( config_val )
6713	def install_setuptools ( python_cmd = 'python' , use_sudo = True ) : setuptools_version = package_version ( 'setuptools' , python_cmd ) distribute_version = package_version ( 'distribute' , python_cmd ) if setuptools_version is None : _install_from_scratch ( python_cmd , use_sudo ) else : if distribute_version is None : _upgrade_from_setuptools ( python_cmd , use_sudo ) else : _upgrade_from_distribute ( python_cmd , use_sudo )
191	def blend_alpha ( image_fg , image_bg , alpha , eps = 1e-2 ) : assert image_fg . shape == image_bg . shape assert image_fg . dtype . kind == image_bg . dtype . kind assert image_fg . dtype . name not in [ "float128" ] assert image_bg . dtype . name not in [ "float128" ] input_was_2d = ( len ( image_fg . shape ) == 2 ) if input_was_2d : image_fg = np . atleast_3d ( image_fg ) image_bg = np . atleast_3d ( image_bg ) input_was_bool = False if image_fg . dtype . kind == "b" : input_was_bool = True image_fg = image_fg . astype ( np . float32 ) image_bg = image_bg . astype ( np . float32 ) alpha = np . array ( alpha , dtype = np . float64 ) if alpha . size == 1 : pass else : if alpha . ndim == 2 : assert alpha . shape == image_fg . shape [ 0 : 2 ] alpha = alpha . reshape ( ( alpha . shape [ 0 ] , alpha . shape [ 1 ] , 1 ) ) elif alpha . ndim == 3 : assert alpha . shape == image_fg . shape or alpha . shape == image_fg . shape [ 0 : 2 ] + ( 1 , ) else : alpha = alpha . reshape ( ( 1 , 1 , - 1 ) ) if alpha . shape [ 2 ] != image_fg . shape [ 2 ] : alpha = np . tile ( alpha , ( 1 , 1 , image_fg . shape [ 2 ] ) ) if not input_was_bool : if np . all ( alpha >= 1.0 - eps ) : return np . copy ( image_fg ) elif np . all ( alpha <= eps ) : return np . copy ( image_bg ) assert 0 <= alpha . item ( 0 ) <= 1.0 dt_images = iadt . get_minimal_dtype ( [ image_fg , image_bg ] ) isize = dt_images . itemsize * 2 isize = max ( isize , 4 ) dt_blend = np . dtype ( "f%d" % ( isize , ) ) if alpha . dtype != dt_blend : alpha = alpha . astype ( dt_blend ) if image_fg . dtype != dt_blend : image_fg = image_fg . astype ( dt_blend ) if image_bg . dtype != dt_blend : image_bg = image_bg . astype ( dt_blend ) image_blend = image_bg + alpha * ( image_fg - image_bg ) if input_was_bool : image_blend = image_blend > 0.5 else : image_blend = iadt . restore_dtypes_ ( image_blend , dt_images , clip = False , round = True ) if input_was_2d : return image_blend [ : , : , 0 ] return image_blend
11838	def result ( self , state , row ) : "Place the next queen at the given row." col = state . index ( None ) new = state [ : ] new [ col ] = row return new
11226	def dump_nparray ( self , obj , class_name = numpy_ndarray_class_name ) : return { "$" + class_name : self . _json_convert ( obj . tolist ( ) ) }
10218	def plot_summary_axes ( graph : BELGraph , lax , rax , logx = True ) : ntc = count_functions ( graph ) etc = count_relations ( graph ) df = pd . DataFrame . from_dict ( dict ( ntc ) , orient = 'index' ) df_ec = pd . DataFrame . from_dict ( dict ( etc ) , orient = 'index' ) df . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = lax ) lax . set_title ( 'Number of nodes: {}' . format ( graph . number_of_nodes ( ) ) ) df_ec . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = rax ) rax . set_title ( 'Number of edges: {}' . format ( graph . number_of_edges ( ) ) )
12317	def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } return result
4770	def is_length ( self , length ) : if type ( length ) is not int : raise TypeError ( 'given arg must be an int' ) if length < 0 : raise ValueError ( 'given arg must be a positive int' ) if len ( self . val ) != length : self . _err ( 'Expected <%s> to be of length <%d>, but was <%d>.' % ( self . val , length , len ( self . val ) ) ) return self
4808	def generate_best_dataset ( best_path , output_path = 'cleaned_data' , create_val = False ) : if not os . path . isdir ( output_path ) : os . mkdir ( output_path ) if not os . path . isdir ( os . path . join ( output_path , 'train' ) ) : os . makedirs ( os . path . join ( output_path , 'train' ) ) if not os . path . isdir ( os . path . join ( output_path , 'test' ) ) : os . makedirs ( os . path . join ( output_path , 'test' ) ) if not os . path . isdir ( os . path . join ( output_path , 'val' ) ) and create_val : os . makedirs ( os . path . join ( output_path , 'val' ) ) for article_type in article_types : files = glob ( os . path . join ( best_path , article_type , '*.txt' ) ) files_train , files_test = train_test_split ( files , random_state = 0 , test_size = 0.1 ) if create_val : files_train , files_val = train_test_split ( files_train , random_state = 0 , test_size = 0.1 ) val_words = generate_words ( files_val ) val_df = create_char_dataframe ( val_words ) val_df . to_csv ( os . path . join ( output_path , 'val' , 'df_best_{}_val.csv' . format ( article_type ) ) , index = False ) train_words = generate_words ( files_train ) test_words = generate_words ( files_test ) train_df = create_char_dataframe ( train_words ) test_df = create_char_dataframe ( test_words ) train_df . to_csv ( os . path . join ( output_path , 'train' , 'df_best_{}_train.csv' . format ( article_type ) ) , index = False ) test_df . to_csv ( os . path . join ( output_path , 'test' , 'df_best_{}_test.csv' . format ( article_type ) ) , index = False ) print ( "Save {} to CSV file" . format ( article_type ) )
12571	def get ( self , key ) : node = self . get_node ( key ) if node is None : raise KeyError ( 'No object named %s in the file' % key ) if hasattr ( node , 'attrs' ) : if 'pandas_type' in node . attrs : return self . _read_group ( node ) return self . _read_array ( node )
437	def data_to_tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( "%s exists" % filename ) return print ( "Converting data into %s ..." % filename ) writer = tf . python_io . TFRecordWriter ( filename ) for index , img in enumerate ( images ) : img_raw = img . tobytes ( ) label = int ( labels [ index ] ) example = tf . train . Example ( features = tf . train . Features ( feature = { "label" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , } ) ) writer . write ( example . SerializeToString ( ) ) writer . close ( )
3648	def applyConsumable ( self , item_id , resource_id ) : method = 'POST' url = 'item/resource/%s' % resource_id data = { 'apply' : [ { 'id' : item_id } ] } self . __request__ ( method , url , data = json . dumps ( data ) )
8271	def _save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
7190	def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) n . parent = None if prefix is not None : n . prefix = prefix return n
10172	def _get_oldest_event_timestamp ( self ) : query_events = Search ( using = self . client , index = self . event_index ) [ 0 : 1 ] . sort ( { 'timestamp' : { 'order' : 'asc' } } ) result = query_events . execute ( ) if len ( result ) == 0 : return None return parser . parse ( result [ 0 ] [ 'timestamp' ] )
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8983	def get_instruction_id ( self , instruction_or_id ) : if isinstance ( instruction_or_id , tuple ) : return _InstructionId ( instruction_or_id ) return _InstructionId ( instruction_or_id . type , instruction_or_id . hex_color )
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
1997	def sync_svc ( state ) : syscall = state . cpu . R7 name = linux_syscalls . armv7 [ syscall ] logger . debug ( f"Syncing syscall: {name}" ) try : if 'mmap' in name : returned = gdb . getR ( 'R0' ) logger . debug ( f"Syncing mmap ({returned:x})" ) state . cpu . write_register ( 'R0' , returned ) if 'exit' in name : return except ValueError : for reg in state . cpu . canonical_registers : print ( f'{reg}: {state.cpu.read_register(reg):x}' ) raise
2956	def load ( self ) : try : with open ( self . _state_file ) as f : state = yaml . safe_load ( f ) self . _containers = state [ 'containers' ] except ( IOError , OSError ) as err : if err . errno == errno . ENOENT : raise NotInitializedError ( "No blockade exists in this context" ) raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) ) except Exception as err : raise InconsistentStateError ( "Failed to load Blockade state: " + str ( err ) )
4288	def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
10709	def _authenticate ( self ) : auth_url = BASE_URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant_type' : 'password' } arequest = requests . post ( auth_url , data = payload , headers = BASIC_HEADERS ) status = arequest . status_code if status != 200 : _LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) _LOGGER . debug ( str ( response ) ) self . token = response . get ( "access_token" ) self . refresh_token = response . get ( "refresh_token" ) _auth = HEADERS . get ( "Authorization" ) _auth = _auth % self . token HEADERS [ "Authorization" ] = _auth _LOGGER . info ( "Authentication was successful, token set." ) return True
8849	def setup_actions ( self ) : self . actionOpen . triggered . connect ( self . on_open ) self . actionNew . triggered . connect ( self . on_new ) self . actionSave . triggered . connect ( self . on_save ) self . actionSave_as . triggered . connect ( self . on_save_as ) self . actionQuit . triggered . connect ( QtWidgets . QApplication . instance ( ) . quit ) self . tabWidget . current_changed . connect ( self . on_current_tab_changed ) self . tabWidget . last_tab_closed . connect ( self . on_last_tab_closed ) self . actionAbout . triggered . connect ( self . on_about ) self . actionRun . triggered . connect ( self . on_run ) self . interactiveConsole . process_finished . connect ( self . on_process_finished ) self . actionConfigure_run . triggered . connect ( self . on_configure_run )
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
3561	def advertised ( self ) : uuids = [ ] try : uuids = self . _props . Get ( _INTERFACE , 'UUIDs' ) except dbus . exceptions . DBusException as ex : if ex . get_dbus_name ( ) != 'org.freedesktop.DBus.Error.InvalidArgs' : raise ex return [ uuid . UUID ( str ( x ) ) for x in uuids ]
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , ** kwargs ) return decorator
2591	def stage_out ( self , file , executor ) : if file . scheme == 'http' or file . scheme == 'https' : raise Exception ( 'HTTP/HTTPS file staging out is not supported' ) elif file . scheme == 'ftp' : raise Exception ( 'FTP file staging out is not supported' ) elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_out_app = self . _globus_stage_out_app ( ) return stage_out_app ( globus_ep , inputs = [ file ] ) else : raise Exception ( 'Staging out with unknown file scheme {} is not supported' . format ( file . scheme ) )
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
6196	def compact_name ( self , hashsize = 6 ) : s = self . compact_name_core ( hashsize , t_max = True ) s += "_ID%d-%d" % ( self . ID , self . EID ) return s
6597	def receive ( self ) : ret = self . communicationChannel . receive_all ( ) self . nruns -= len ( ret ) if self . nruns > 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} more expected' . format ( len ( ret ) , self . nruns ) ) elif self . nruns < 0 : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too many results received: {} results received, {} too many' . format ( len ( ret ) , - self . nruns ) ) return ret
282	def plot_holdings ( returns , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . copy ( ) . drop ( 'cash' , axis = 'columns' ) df_holdings = positions . replace ( 0 , np . nan ) . count ( axis = 1 ) df_holdings_by_month = df_holdings . resample ( '1M' ) . mean ( ) df_holdings . plot ( color = 'steelblue' , alpha = 0.6 , lw = 0.5 , ax = ax , ** kwargs ) df_holdings_by_month . plot ( color = 'orangered' , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_holdings . values . mean ( ) , color = 'steelblue' , ls = '--' , lw = 3 ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) leg = ax . legend ( [ 'Daily holdings' , 'Average daily holdings, by month' , 'Average daily holdings, overall' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_title ( 'Total holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
4156	def arma2psd ( A = None , B = None , rho = 1. , T = 1. , NFFT = 4096 , sides = 'default' , norm = False ) : r if NFFT is None : NFFT = 4096 if A is None and B is None : raise ValueError ( "Either AR or MA model must be provided" ) psd = np . zeros ( NFFT , dtype = complex ) if A is not None : ip = len ( A ) den = np . zeros ( NFFT , dtype = complex ) den [ 0 ] = 1. + 0j for k in range ( 0 , ip ) : den [ k + 1 ] = A [ k ] denf = fft ( den , NFFT ) if B is not None : iq = len ( B ) num = np . zeros ( NFFT , dtype = complex ) num [ 0 ] = 1. + 0j for k in range ( 0 , iq ) : num [ k + 1 ] = B [ k ] numf = fft ( num , NFFT ) if A is not None and B is not None : psd = rho / T * abs ( numf ) ** 2. / abs ( denf ) ** 2. elif A is not None : psd = rho / T / abs ( denf ) ** 2. elif B is not None : psd = rho / T * abs ( numf ) ** 2. psd = np . real ( psd ) if sides != 'default' : from . import tools assert sides in [ 'centerdc' ] if sides == 'centerdc' : psd = tools . twosided_2_centerdc ( psd ) if norm == True : psd /= max ( psd ) return psd
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , ** kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , ** kwargs ) YX = GNNYX . run ( target , inputx , ** kwargs ) return [ XY , YX ]
10628	def T ( self , T ) : self . _T = T self . _Hfr = self . _calculate_Hfr ( T )
13689	def add_peer ( self , peer ) : if type ( peer ) == list : for i in peer : check_url ( i ) self . PEERS . extend ( peer ) elif type ( peer ) == str : check_url ( peer ) self . PEERS . append ( peer )
5416	def get_dstat_provider_args ( provider , project ) : provider_name = get_provider_name ( provider ) args = [ ] if provider_name == 'google' : args . append ( '--project %s' % project ) elif provider_name == 'google-v2' : args . append ( '--project %s' % project ) elif provider_name == 'local' : pass elif provider_name == 'test-fails' : pass else : assert False , 'Provider %s needs get_dstat_provider_args support' % provider args . insert ( 0 , '--provider %s' % provider_name ) return ' ' . join ( args )
6132	def toJSON ( self ) : return { "id" : self . id , "compile" : self . compile , "position" : self . position , "version" : self . version }
7162	def format_answers ( self , fmt = 'obj' ) : fmts = ( 'obj' , 'array' , 'plain' ) if fmt not in fmts : eprint ( "Error: '{}' not in {}" . format ( fmt , fmts ) ) return def stringify ( val ) : if type ( val ) in ( list , tuple ) : return ', ' . join ( str ( e ) for e in val ) return val if fmt == 'obj' : return json . dumps ( self . answers ) elif fmt == 'array' : answers = [ [ k , v ] for k , v in self . answers . items ( ) ] return json . dumps ( answers ) elif fmt == 'plain' : answers = '\n' . join ( '{}: {}' . format ( k , stringify ( v ) ) for k , v in self . answers . items ( ) ) return answers
2397	def histogram ( ratings , min_rating = None , max_rating = None ) : ratings = [ int ( r ) for r in ratings ] if min_rating is None : min_rating = min ( ratings ) if max_rating is None : max_rating = max ( ratings ) num_ratings = int ( max_rating - min_rating + 1 ) hist_ratings = [ 0 for x in range ( num_ratings ) ] for r in ratings : hist_ratings [ r - min_rating ] += 1 return hist_ratings
7750	def __try_handlers ( self , handler_list , stanza , stanza_type = None ) : if stanza_type is None : stanza_type = stanza . stanza_type payload = stanza . get_all_payload ( ) classes = [ p . __class__ for p in payload ] keys = [ ( p . __class__ , p . handler_key ) for p in payload ] for handler in handler_list : type_filter = handler . _pyxmpp_stanza_handled [ 1 ] class_filter = handler . _pyxmpp_payload_class_handled extra_filter = handler . _pyxmpp_payload_key if type_filter != stanza_type : continue if class_filter : if extra_filter is None and class_filter not in classes : continue if extra_filter and ( class_filter , extra_filter ) not in keys : continue response = handler ( stanza ) if self . _process_handler_result ( response ) : return True return False
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
3055	def from_string ( cls , key , password = 'notasecret' ) : key = _helpers . _from_bytes ( key ) marker_id , key_bytes = pem . readPemBlocksFromFile ( six . StringIO ( key ) , _PKCS1_MARKER , _PKCS8_MARKER ) if marker_id == 0 : pkey = rsa . key . PrivateKey . load_pkcs1 ( key_bytes , format = 'DER' ) elif marker_id == 1 : key_info , remaining = decoder . decode ( key_bytes , asn1Spec = _PKCS8_SPEC ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) pkey_info = key_info . getComponentByName ( 'privateKey' ) pkey = rsa . key . PrivateKey . load_pkcs1 ( pkey_info . asOctets ( ) , format = 'DER' ) else : raise ValueError ( 'No key could be detected.' ) return cls ( pkey )
11042	def request ( self , method , url = None , ** kwargs ) : url = self . _compose_url ( url , kwargs ) kwargs . setdefault ( 'timeout' , self . _timeout ) d = self . _client . request ( method , url , reactor = self . _reactor , ** kwargs ) d . addCallback ( self . _log_request_response , method , url , kwargs ) d . addErrback ( self . _log_request_error , url ) return d
3727	def Vc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Vc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Vc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Vc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Vc = float ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) elif Method == PSRK : _Vc = float ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) elif Method == MATTHEWS : _Vc = float ( _crit_Matthews . at [ CASRN , 'Vc' ] ) elif Method == CRC : _Vc = float ( _crit_CRC . at [ CASRN , 'Vc' ] ) elif Method == YAWS : _Vc = float ( _crit_Yaws . at [ CASRN , 'Vc' ] ) elif Method == SURF : _Vc = third_property ( CASRN = CASRN , V = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Vc
9603	def from_object ( cls , obj ) : return cls ( obj . get ( 'sessionId' , None ) , obj . get ( 'status' , 0 ) , obj . get ( 'value' , None ) )
5932	def scale_impropers ( mol , impropers , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_impropers = [ ] for im in mol . impropers : atypes = ( im . atom1 . get_atomtype ( ) , im . atom2 . get_atomtype ( ) , im . atom3 . get_atomtype ( ) , im . atom4 . get_atomtype ( ) ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] if im . gromacs [ 'param' ] != [ ] : for p in im . gromacs [ 'param' ] : p [ 'kpsi' ] *= scale new_impropers . append ( im ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , im . gromacs [ 'func' ] ) if ( key in impropers ) : for i , imt in enumerate ( impropers [ key ] ) : imA = copy . deepcopy ( im ) param = copy . deepcopy ( imt . gromacs [ 'param' ] ) if not impropers [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kpsi' ] *= scale imA . gromacs [ 'param' ] = param if i == 0 : imA . comment = "; banned lines {0} found={1}\n ; parameters for types {2}-{3}-{4}-{5}-9 at LINE({6})\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if imt . line in banned_lines else 0 , imt . atype1 , imt . atype2 , imt . atype3 , imt . atype4 , imt . line ) new_impropers . append ( imA ) break mol . impropers = new_impropers return mol
2138	def list ( self , root = False , ** kwargs ) : if kwargs . get ( 'parent' , None ) : self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs . pop ( 'parent' ) if root and not kwargs . get ( 'inventory' , None ) : raise exc . UsageError ( 'The --root option requires specifying an inventory also.' ) if root : inventory_id = kwargs [ 'inventory' ] r = client . get ( '/inventories/%d/root_groups/' % inventory_id ) return r . json ( ) return super ( Resource , self ) . list ( ** kwargs )
12397	def gen_method_keys ( self , * args , ** kwargs ) : token = args [ 0 ] for mro_type in type ( token ) . __mro__ [ : - 1 ] : name = mro_type . __name__ yield name
9529	def get_encrypted_field ( base_class ) : assert not isinstance ( base_class , models . Field ) field_name = 'Encrypted' + base_class . __name__ if base_class not in FIELD_CACHE : FIELD_CACHE [ base_class ] = type ( field_name , ( EncryptedMixin , base_class ) , { 'base_class' : base_class , } ) return FIELD_CACHE [ base_class ]
12949	def copyModel ( mdl ) : copyNum = _modelCopyMap [ mdl ] _modelCopyMap [ mdl ] += 1 mdlCopy = type ( mdl . __name__ + '_Copy' + str ( copyNum ) , mdl . __bases__ , copy . deepcopy ( dict ( mdl . __dict__ ) ) ) mdlCopy . FIELDS = [ field . copy ( ) for field in mdl . FIELDS ] mdlCopy . INDEXED_FIELDS = [ str ( idxField ) for idxField in mdl . INDEXED_FIELDS ] mdlCopy . validateModel ( ) return mdlCopy
6077	def hyper_noise_from_contributions ( self , noise_map , contributions ) : return self . noise_factor * ( noise_map * contributions ) ** self . noise_power
10945	def _do_run ( self , mode = '1' ) : for a in range ( len ( self . particle_groups ) ) : group = self . particle_groups [ a ] lp = LMParticles ( self . state , group , ** self . _kwargs ) if mode == 'internal' : lp . J , lp . JTJ , lp . _dif_tile = self . _load_j_diftile ( a ) if mode == '1' : lp . do_run_1 ( ) if mode == '2' : lp . do_run_2 ( ) if mode == 'internal' : lp . do_internal_run ( ) self . stats . append ( lp . get_termination_stats ( get_cos = self . get_cos ) ) if self . save_J and ( mode != 'internal' ) : self . _dump_j_diftile ( a , lp . J , lp . _dif_tile ) self . _has_saved_J [ a ] = True
8737	def get ( self , query , responseformat = "geojson" , verbosity = "body" , build = True ) : if build : full_query = self . _construct_ql_query ( query , responseformat = responseformat , verbosity = verbosity ) else : full_query = query if self . debug : logging . getLogger ( ) . info ( query ) r = self . _get_from_overpass ( full_query ) content_type = r . headers . get ( "content-type" ) if self . debug : print ( content_type ) if content_type == "text/csv" : result = [ ] reader = csv . reader ( StringIO ( r . text ) , delimiter = "\t" ) for row in reader : result . append ( row ) return result elif content_type in ( "text/xml" , "application/xml" , "application/osm3s+xml" ) : return r . text elif content_type == "application/json" : response = json . loads ( r . text ) if not build : return response if "elements" not in response : raise UnknownOverpassError ( "Received an invalid answer from Overpass." ) overpass_remark = response . get ( "remark" , None ) if overpass_remark and overpass_remark . startswith ( "runtime error" ) : raise ServerRuntimeError ( overpass_remark ) if responseformat is not "geojson" : return response return self . _as_geojson ( response [ "elements" ] )
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
13465	def __register_library ( self , module_name : str , attr : str , fallback : str = None ) : try : module = importlib . import_module ( module_name ) except ImportError : if fallback is not None : module = importlib . import_module ( fallback ) self . __logger . warn ( module_name + " not available: Replaced with " + fallback ) else : self . __logger . warn ( module_name + " not available: No Replacement Specified" ) if not attr in dir ( self . __sketch ) : setattr ( self . __sketch , attr , module ) else : self . __logger . warn ( attr + " could not be imported as it's label is already used in the sketch" )
7104	def transform ( self , transformer ) : self . transformers . append ( transformer ) from languageflow . transformer . tagged import TaggedTransformer if isinstance ( transformer , TaggedTransformer ) : self . X , self . y = transformer . transform ( self . sentences ) if isinstance ( transformer , TfidfVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , CountVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , NumberRemover ) : self . X = transformer . transform ( self . X ) if isinstance ( transformer , MultiLabelBinarizer ) : self . y = transformer . fit_transform ( self . y )
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
5031	def get ( self , request , template_id , view_type ) : template = get_object_or_404 ( EnrollmentNotificationEmailTemplate , pk = template_id ) if view_type not in self . view_type_contexts : return HttpResponse ( status = 404 ) base_context = self . view_type_contexts [ view_type ] . copy ( ) base_context . update ( { 'user_name' : self . get_user_name ( request ) } ) return HttpResponse ( template . render_html_template ( base_context ) , content_type = 'text/html' )
4707	def power_on ( self , interval = 200 ) : if self . __power_on_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_ON" ) return 1 return self . __press ( self . __power_on_port , interval = interval )
7605	def get_clan ( self , tag : crtag , timeout : int = None ) : url = self . api . CLAN + '/' + tag return self . _get_model ( url , FullClan , timeout = timeout )
1648	def CheckAltTokens ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] if Match ( r'^\s*#' , line ) : return if line . find ( '/*' ) >= 0 or line . find ( '*/' ) >= 0 : return for match in _ALT_TOKEN_REPLACEMENT_PATTERN . finditer ( line ) : error ( filename , linenum , 'readability/alt_tokens' , 2 , 'Use operator %s instead of %s' % ( _ALT_TOKEN_REPLACEMENT [ match . group ( 1 ) ] , match . group ( 1 ) ) )
9190	def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
994	def _generateRangeDescription ( self , ranges ) : desc = "" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " return desc
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
7814	def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : raise http . exceptions . MethodNotAllowed ( allowed )
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
109	def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : blockade_id = "default" return blockade_id
9463	def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
9778	def login ( token , username , password ) : auth_client = PolyaxonClient ( ) . auth if username : if not password : password = click . prompt ( 'Please enter your password' , type = str , hide_input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = CredentialsConfig ( username = username , password = password ) try : access_code = auth_client . login ( credentials = credentials ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not login.' ) Printer . print_error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access_code : Printer . print_error ( "Failed to login" ) return else : if not token : token_url = "{}/app/token" . format ( auth_client . config . http_host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token_url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide_input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon_cli/commands/auth" ) return access_code = token . strip ( " " ) try : AuthConfigManager . purge ( ) user = PolyaxonClient ( ) . auth . get_user ( token = access_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access_token = AccessTokenConfig ( username = user . username , token = access_code ) AuthConfigManager . set_config ( access_token ) Printer . print_success ( "Login successful" ) server_version = get_server_version ( ) current_version = get_current_version ( ) log_handler = get_log_handler ( ) CliConfigManager . reset ( check_count = 0 , current_version = current_version , min_version = server_version . min_version , log_handler = log_handler )
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
3002	def start ( self ) : if self . extra_args : sys . exit ( '{} takes no extra arguments' . format ( self . name ) ) else : if self . _toggle_value : nbextensions . install_nbextension_python ( _pkg_name , overwrite = True , symlink = False , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) else : nbextensions . uninstall_nbextension_python ( _pkg_name , user = self . user , sys_prefix = self . sys_prefix , prefix = None , nbextensions_dir = None , logger = None ) self . toggle_nbextension_python ( _pkg_name ) self . toggle_server_extension_python ( _pkg_name )
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
4422	async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
4095	def AICc ( N , rho , k , norm = True ) : r from numpy import log , array p = k res = log ( rho ) + 2. * ( p + 1 ) / ( N - p - 2 ) return res
8430	def cmap_d_pal ( name = None , lut = None ) : colormap = get_cmap ( name , lut ) if not isinstance ( colormap , mcolors . ListedColormap ) : raise ValueError ( "For a discrete palette, cmap must be of type " "matplotlib.colors.ListedColormap" ) ncolors = len ( colormap . colors ) def _cmap_d_pal ( n ) : if n > ncolors : raise ValueError ( "cmap `{}` has {} colors you requested {} " "colors." . format ( name , ncolors , n ) ) if ncolors < 256 : return [ mcolors . rgb2hex ( c ) for c in colormap . colors [ : n ] ] else : idx = np . linspace ( 0 , ncolors - 1 , n ) . round ( ) . astype ( int ) return [ mcolors . rgb2hex ( colormap . colors [ i ] ) for i in idx ] return _cmap_d_pal
12909	def from_json ( cls , fh ) : if isinstance ( fh , str ) : return cls ( json . loads ( fh ) ) else : return cls ( json . load ( fh ) )
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
5064	def update_query_parameters ( url , query_parameters ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) url_params = parse_qs ( query_string ) url_params . update ( query_parameters ) return urlunsplit ( ( scheme , netloc , path , urlencode ( sorted ( url_params . items ( ) ) , doseq = True ) , fragment ) , )
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
6729	def deploy_code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local_renderer if self . env . exclusions : r . env . exclusions_str = ' ' . join ( "--exclude='%s'" % _ for _ in self . env . exclusions ) r . local ( r . env . rsync_command ) r . sudo ( 'chown -R {rsync_chown_user}:{rsync_chown_group} {rsync_dst_dir}' )
11016	def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
2639	def cancel ( self , job_ids ) : statuses = [ ] for job_id in job_ids : try : self . delete_instance ( job_id ) statuses . append ( True ) self . provisioned_blocks -= 1 except Exception : statuses . append ( False ) return statuses
12504	def smooth_img ( imgs , fwhm , ** kwargs ) : if hasattr ( imgs , "__iter__" ) and not isinstance ( imgs , string_types ) : single_img = False else : single_img = True imgs = [ imgs ] ret = [ ] for img in imgs : img = check_niimg ( img ) affine = img . get_affine ( ) filtered = _smooth_array ( img . get_data ( ) , affine , fwhm = fwhm , ensure_finite = True , copy = True , ** kwargs ) ret . append ( new_img_like ( img , filtered , affine , copy_header = True ) ) if single_img : return ret [ 0 ] else : return ret
4831	def get_course_certificate ( self , course_id , username ) : return self . client . certificates ( username ) . courses ( course_id ) . get ( )
8348	def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : return return self . convert_codepoint ( n )
4501	def project ( * descs , root_file = None ) : load . ROOT_FILE = root_file desc = merge . merge ( merge . DEFAULT_PROJECT , * descs ) path = desc . get ( 'path' , '' ) if root_file : project_path = os . path . dirname ( root_file ) if path : path += ':' + project_path else : path = project_path with load . extender ( path ) : desc = recurse . recurse ( desc ) project = construct . construct ( ** desc ) project . desc = desc return project
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
5462	def _emit_search_criteria ( user_ids , job_ids , task_ids , labels ) : print ( 'Delete running jobs:' ) print ( ' user:' ) print ( ' %s\n' % user_ids ) print ( ' job-id:' ) print ( ' %s\n' % job_ids ) if task_ids : print ( ' task-id:' ) print ( ' %s\n' % task_ids ) if labels : print ( ' labels:' ) print ( ' %s\n' % repr ( labels ) )
5293	def get_context_data ( self , ** kwargs ) : context = { } inlines_names = self . get_inlines_names ( ) if inlines_names : context . update ( zip ( inlines_names , kwargs . get ( 'inlines' , [ ] ) ) ) if 'formset' in kwargs : context [ inlines_names [ 0 ] ] = kwargs [ 'formset' ] context . update ( kwargs ) return super ( NamedFormsetsMixin , self ) . get_context_data ( ** context )
10534	def delete_project ( project_id ) : try : res = _pybossa_req ( 'delete' , 'project' , project_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
10332	def average_node_annotation ( graph : BELGraph , key : str , annotation : str = 'Subgraph' , aggregator : Optional [ Callable [ [ Iterable [ X ] ] , X ] ] = None , ) -> Mapping [ str , X ] : if aggregator is None : def aggregator ( x ) : return sum ( x ) / len ( x ) result = { } for subgraph , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) : values = [ graph . nodes [ node ] [ key ] for node in nodes if key in graph . nodes [ node ] ] result [ subgraph ] = aggregator ( values ) return result
10114	def dump_grid ( grid ) : header = 'ver:%s' % dump_str ( str ( grid . _version ) , version = grid . _version ) if bool ( grid . metadata ) : header += ' ' + dump_meta ( grid . metadata , version = grid . _version ) columns = dump_columns ( grid . column , version = grid . _version ) rows = dump_rows ( grid ) return '\n' . join ( [ header , columns ] + rows + [ '' ] )
7017	def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objectid , ** kwargs ) except Exception as e : LOGEXCEPTION ( 'failed LC concatenation for %s in %s' % ( objectid , lcbasedir ) ) return None
1532	def get_execution_state ( self , topologyName , callback = None ) : if callback : self . execution_state_watchers [ topologyName ] . append ( callback ) else : execution_state_path = self . get_execution_state_path ( topologyName ) with open ( execution_state_path ) as f : data = f . read ( ) executionState = ExecutionState ( ) executionState . ParseFromString ( data ) return executionState
5102	def get_edge_type ( self , edge_type ) : edges = [ ] for e in self . edges ( ) : if self . adj [ e [ 0 ] ] [ e [ 1 ] ] . get ( 'edge_type' ) == edge_type : edges . append ( e ) return edges
12707	def rotation ( self , rotation ) : if isinstance ( rotation , np . ndarray ) : rotation = rotation . ravel ( ) self . ode_body . setRotation ( tuple ( rotation ) )
6996	def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = False , ignoreinitfail = False , magsarefluxes = False , verbose = True ) : if errs is None : errs = npfull_like ( mags , 0.005 ) stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip , magsarefluxes = magsarefluxes ) nzind = npnonzero ( serrs ) stimes , smags , serrs = stimes [ nzind ] , smags [ nzind ] , serrs [ nzind ] phase , pmags , perrs , ptimes , mintime = ( get_phased_quantities ( stimes , smags , serrs , period ) ) nobs = len ( phase ) nknots = int ( npfloor ( knotfraction * nobs ) ) nknots = maxknots if nknots > maxknots else nknots splineknots = nplinspace ( phase [ 0 ] + 0.01 , phase [ - 1 ] - 0.01 , num = nknots ) phase_diffs_ind = npdiff ( phase ) > 0.0 incphase_ind = npconcatenate ( ( nparray ( [ True ] ) , phase_diffs_ind ) ) phase , pmags , perrs = ( phase [ incphase_ind ] , pmags [ incphase_ind ] , perrs [ incphase_ind ] ) spl = LSQUnivariateSpline ( phase , pmags , t = splineknots , w = 1.0 / perrs ) fitmags = spl ( phase ) fitchisq = npsum ( ( ( fitmags - pmags ) * ( fitmags - pmags ) ) / ( perrs * perrs ) ) fitredchisq = fitchisq / ( len ( pmags ) - nknots - 1 ) if verbose : LOGINFO ( 'spline fit done. nknots = %s, ' 'chisq = %.5f, reduced chisq = %.5f' % ( nknots , fitchisq , fitredchisq ) ) if not magsarefluxes : fitmagminind = npwhere ( fitmags == npmax ( fitmags ) ) else : fitmagminind = npwhere ( fitmags == npmin ( fitmags ) ) if len ( fitmagminind [ 0 ] ) > 1 : fitmagminind = ( fitmagminind [ 0 ] [ 0 ] , ) magseriesepoch = ptimes [ fitmagminind ] returndict = { 'fittype' : 'spline' , 'fitinfo' : { 'nknots' : nknots , 'fitmags' : fitmags , 'fitepoch' : magseriesepoch } , 'fitchisq' : fitchisq , 'fitredchisq' : fitredchisq , 'fitplotfile' : None , 'magseries' : { 'times' : ptimes , 'phase' : phase , 'mags' : pmags , 'errs' : perrs , 'magsarefluxes' : magsarefluxes } , } if plotfit and isinstance ( plotfit , str ) : make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = magsarefluxes ) returndict [ 'fitplotfile' ] = plotfit return returndict
6217	def prepare_attrib_mapping ( self , primitive ) : buffer_info = [ ] for name , accessor in primitive . attributes . items ( ) : info = VBOInfo ( * accessor . info ( ) ) info . attributes . append ( ( name , info . components ) ) if buffer_info and buffer_info [ - 1 ] . buffer_view == info . buffer_view : if buffer_info [ - 1 ] . interleaves ( info ) : buffer_info [ - 1 ] . merge ( info ) continue buffer_info . append ( info ) return buffer_info
9378	def is_valid_file ( filename ) : if os . path . exists ( filename ) : if not os . path . getsize ( filename ) : logger . warning ( '%s : file is empty.' , filename ) return False else : logger . warning ( '%s : file does not exist.' , filename ) return False return True
6197	def numeric_params ( self ) : nparams = dict ( D = ( self . diffusion_coeff . mean ( ) , 'Diffusion coefficient (m^2/s)' ) , np = ( self . num_particles , 'Number of simulated particles' ) , t_step = ( self . t_step , 'Simulation time-step (s)' ) , t_max = ( self . t_max , 'Simulation total time (s)' ) , ID = ( self . ID , 'Simulation ID (int)' ) , EID = ( self . EID , 'IPython Engine ID (int)' ) , pico_mol = ( self . concentration ( ) * 1e12 , 'Particles concentration (pM)' ) ) return nparams
7677	def hierarchy ( annotation , ** kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , ** kwargs )
11745	def closure ( self , rules ) : closure = set ( ) todo = set ( rules ) while todo : rule = todo . pop ( ) closure . add ( rule ) if rule . at_end : continue symbol = rule . rhs [ rule . pos ] for production in self . nonterminals [ symbol ] : for first in self . first ( rule . rest ) : if EPSILON in production . rhs : new_rule = DottedRule ( production , 1 , first ) else : new_rule = DottedRule ( production , 0 , first ) if new_rule not in closure : todo . add ( new_rule ) return frozenset ( closure )
9878	def _coincidences ( value_counts , value_domain , dtype = np . float64 ) : value_counts_matrices = value_counts . reshape ( value_counts . shape + ( 1 , ) ) pairable = np . maximum ( np . sum ( value_counts , axis = 1 ) , 2 ) diagonals = np . tile ( np . eye ( len ( value_domain ) ) , ( len ( value_counts ) , 1 , 1 ) ) * value_counts . reshape ( ( value_counts . shape [ 0 ] , 1 , value_counts . shape [ 1 ] ) ) unnormalized_coincidences = value_counts_matrices * value_counts_matrices . transpose ( ( 0 , 2 , 1 ) ) - diagonals return np . sum ( np . divide ( unnormalized_coincidences , ( pairable - 1 ) . reshape ( ( - 1 , 1 , 1 ) ) , dtype = dtype ) , axis = 0 )
6039	def xticks ( self ) : return np . linspace ( np . min ( self [ : , 1 ] ) , np . max ( self [ : , 1 ] ) , 4 )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
5004	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'degreed' kwargs [ 'model_name' ] = 'DegreedLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'degreed_user_email' super ( DegreedLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
7323	def sendmail ( message , sender , recipients , config_filename ) : if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
4141	def _arburg2 ( X , order ) : x = np . array ( X ) N = len ( x ) if order <= 0. : raise ValueError ( "order must be > 0" ) rho = sum ( abs ( x ) ** 2. ) / N den = rho * 2. * N ef = np . zeros ( N , dtype = complex ) eb = np . zeros ( N , dtype = complex ) for j in range ( 0 , N ) : ef [ j ] = x [ j ] eb [ j ] = x [ j ] a = np . zeros ( 1 , dtype = complex ) a [ 0 ] = 1 ref = np . zeros ( order , dtype = complex ) temp = 1. E = np . zeros ( order + 1 ) E [ 0 ] = rho for m in range ( 0 , order ) : efp = ef [ 1 : ] ebp = eb [ 0 : - 1 ] num = - 2. * np . dot ( ebp . conj ( ) . transpose ( ) , efp ) den = np . dot ( efp . conj ( ) . transpose ( ) , efp ) den += np . dot ( ebp , ebp . conj ( ) . transpose ( ) ) ref [ m ] = num / den ef = efp + ref [ m ] * ebp eb = ebp + ref [ m ] . conj ( ) . transpose ( ) * efp a . resize ( len ( a ) + 1 ) a = a + ref [ m ] * np . flipud ( a ) . conjugate ( ) E [ m + 1 ] = ( 1 - ref [ m ] . conj ( ) . transpose ( ) * ref [ m ] ) * E [ m ] return a , E [ - 1 ] , ref
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : data = fd . read ( num_bytes ) mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) if is_name : fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
3078	def email ( self ) : if not self . credentials : return None try : return self . credentials . id_token [ 'email' ] except KeyError : current_app . logger . error ( 'Invalid id_token {0}' . format ( self . credentials . id_token ) )
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) with self . pymata . data_lock : for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
13756	def copy_file ( src , dest ) : dir_path = os . path . dirname ( dest ) if not os . path . exists ( dir_path ) : os . makedirs ( dir_path ) shutil . copy2 ( src , dest )
1121	def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )
12589	def treefall ( iterable ) : num_elems = len ( iterable ) for i in range ( num_elems , - 1 , - 1 ) : for c in combinations ( iterable , i ) : yield c
7186	def maybe_replace_any_if_equal ( name , expected , actual ) : is_equal = expected == actual if not is_equal and Config . replace_any : actual_str = minimize_whitespace ( str ( actual ) ) if actual_str and actual_str [ 0 ] in { '"' , "'" } : actual_str = actual_str [ 1 : - 1 ] is_equal = actual_str in { 'Any' , 'typing.Any' , 't.Any' } if not is_equal : expected_annotation = minimize_whitespace ( str ( expected ) ) actual_annotation = minimize_whitespace ( str ( actual ) ) raise ValueError ( f"incompatible existing {name}. " + f"Expected: {expected_annotation!r}, actual: {actual_annotation!r}" ) return expected or actual
12446	def resource ( ** kwargs ) : def inner ( function ) : name = kwargs . pop ( 'name' , None ) if name is None : name = utils . dasherize ( function . __name__ ) methods = kwargs . pop ( 'methods' , None ) if isinstance ( methods , six . string_types ) : methods = methods , handler = ( function , methods ) if name not in _resources : _handlers [ name ] = [ ] from armet import resources kwargs [ 'name' ] = name class LightweightResource ( resources . Resource ) : Meta = type ( str ( 'Meta' ) , ( ) , kwargs ) def route ( self , request , response ) : for handler , methods in _handlers [ name ] : if methods is None or request . method in methods : return handler ( request , response ) resources . Resource . route ( self ) _resources [ name ] = LightweightResource _handlers [ name ] . append ( handler ) return _resources [ name ] return inner
7373	def get_error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error
2889	def parse_node ( self ) : try : self . task = self . create_task ( ) self . task . documentation = self . parser . _parse_documentation ( self . node , xpath = self . xpath , task_parser = self ) boundary_event_nodes = self . process_xpath ( './/bpmn:boundaryEvent[@attachedToRef="%s"]' % self . get_id ( ) ) if boundary_event_nodes : parent_task = _BoundaryEventParent ( self . spec , '%s.BoundaryEventParent' % self . get_id ( ) , self . task , lane = self . task . lane ) self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = parent_task parent_task . connect_outgoing ( self . task , '%s.FromBoundaryEventParent' % self . get_id ( ) , None , None ) for boundary_event in boundary_event_nodes : b = self . process_parser . parse_node ( boundary_event ) parent_task . connect_outgoing ( b , '%s.FromBoundaryEventParent' % boundary_event . get ( 'id' ) , None , None ) else : self . process_parser . parsed_nodes [ self . node . get ( 'id' ) ] = self . task children = [ ] outgoing = self . process_xpath ( './/bpmn:sequenceFlow[@sourceRef="%s"]' % self . get_id ( ) ) if len ( outgoing ) > 1 and not self . handles_multiple_outgoing ( ) : raise ValidationException ( 'Multiple outgoing flows are not supported for ' 'tasks of type' , node = self . node , filename = self . process_parser . filename ) for sequence_flow in outgoing : target_ref = sequence_flow . get ( 'targetRef' ) target_node = one ( self . process_xpath ( './/*[@id="%s"]' % target_ref ) ) c = self . process_parser . parse_node ( target_node ) children . append ( ( c , target_node , sequence_flow ) ) if children : default_outgoing = self . node . get ( 'default' ) if not default_outgoing : ( c , target_node , sequence_flow ) = children [ 0 ] default_outgoing = sequence_flow . get ( 'id' ) for ( c , target_node , sequence_flow ) in children : self . connect_outgoing ( c , target_node , sequence_flow , sequence_flow . get ( 'id' ) == default_outgoing ) return parent_task if boundary_event_nodes else self . task except ValidationException : raise except Exception as ex : exc_info = sys . exc_info ( ) tb = "" . join ( traceback . format_exception ( exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] ) ) LOG . error ( "%r\n%s" , ex , tb ) raise ValidationException ( "%r" % ( ex ) , node = self . node , filename = self . process_parser . filename )
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
2667	def write_then_readinto ( self , out_buffer , in_buffer , * , out_start = 0 , out_end = None , in_start = 0 , in_end = None , stop = True ) : if out_end is None : out_end = len ( out_buffer ) if in_end is None : in_end = len ( in_buffer ) if hasattr ( self . i2c , 'writeto_then_readfrom' ) : if self . _debug : print ( "i2c_device.writeto_then_readfrom.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) self . i2c . writeto_then_readfrom ( self . device_address , out_buffer , in_buffer , out_start = out_start , out_end = out_end , in_start = in_start , in_end = in_end , stop = stop ) if self . _debug : print ( "i2c_device.writeto_then_readfrom.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] ) else : self . write ( out_buffer , start = out_start , end = out_end , stop = stop ) if self . _debug : print ( "i2c_device.write_then_readinto.write.out_buffer:" , [ hex ( i ) for i in out_buffer [ out_start : out_end ] ] ) self . readinto ( in_buffer , start = in_start , end = in_end ) if self . _debug : print ( "i2c_device.write_then_readinto.readinto.in_buffer:" , [ hex ( i ) for i in in_buffer [ in_start : in_end ] ] )
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
11004	def psffunc ( self , x , y , z , ** kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_pinhole_psf else : func = psfcalc . calculate_pinhole_psf x0 , y0 = [ psfcalc . vec_to_halfvec ( v ) for v in [ x , y ] ] vls = psfcalc . wrap_and_calc_psf ( x0 , y0 , z , func , ** kwargs ) return vls / vls . sum ( )
13503	def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
6065	def density_between_circular_annuli_in_angular_units ( self , inner_annuli_radius , outer_annuli_radius ) : annuli_area = ( np . pi * outer_annuli_radius ** 2.0 ) - ( np . pi * inner_annuli_radius ** 2.0 ) return ( self . mass_within_circle_in_units ( radius = outer_annuli_radius ) - self . mass_within_circle_in_units ( radius = inner_annuli_radius ) ) / annuli_area
11124	def move_directory ( self , relativePath , relativeDestination , replace = False , verbose = True ) : relativePath = os . path . normpath ( relativePath ) relativeDestination = os . path . normpath ( relativeDestination ) filesInfo = list ( self . walk_files_info ( relativePath = relativePath ) ) dirsPath = list ( self . walk_directories_relative_path ( relativePath = relativePath ) ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage self . remove_directory ( relativePath = relativePath , removeFromSystem = False ) self . add_directory ( relativeDestination ) for RP , info in filesInfo : source = os . path . join ( self . __path , relativePath , RP ) destination = os . path . join ( self . __path , relativeDestination , RP ) newDirRP , fileName = os . path . split ( os . path . join ( relativeDestination , RP ) ) dirInfoDict = self . add_directory ( newDirRP ) if os . path . isfile ( destination ) : if replace : os . remove ( destination ) if verbose : warnings . warn ( "file '%s' is copied replacing existing one in destination '%s'." % ( fileName , newDirRP ) ) else : if verbose : warnings . warn ( "file '%s' is not copied because the same file exists in destination '%s'." % ( fileName , destination ) ) continue os . rename ( source , destination ) dict . __getitem__ ( dirInfoDict , "files" ) [ fileName ] = info self . save ( )
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
5841	def submit_design_run ( self , data_view_id , num_candidates , effort , target = None , constraints = [ ] , sampler = "Default" ) : if effort > 30 : raise CitrinationClientError ( "Parameter effort must be less than 30 to trigger a design run" ) if target is not None : target = target . to_dict ( ) constraint_dicts = [ c . to_dict ( ) for c in constraints ] body = { "num_candidates" : num_candidates , "target" : target , "effort" : effort , "constraints" : constraint_dicts , "sampler" : sampler } url = routes . submit_data_view_design ( data_view_id ) response = self . _post_json ( url , body ) . json ( ) return DesignRun ( response [ "data" ] [ "design_run" ] [ "uid" ] )
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , ** kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
11578	def send_command ( self , command ) : send_message = "" for i in command : send_message += chr ( i ) for data in send_message : self . pymata . transport . write ( data )
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
2182	def rebuild_auth ( self , prepared_request , response ) : if "Authorization" in prepared_request . headers : prepared_request . headers . pop ( "Authorization" , True ) prepared_request . prepare_auth ( self . auth ) return
11385	def body ( self ) : if not hasattr ( self , '_body' ) : self . _body = inspect . getsource ( self . module ) return self . _body
5422	def _resolve_task_logging ( job_metadata , job_resources , task_descriptors ) : if not job_resources . logging : return for task_descriptor in task_descriptors : logging_uri = provider_base . format_logging_uri ( job_resources . logging . uri , job_metadata , task_descriptor . task_metadata ) logging_path = job_model . LoggingParam ( logging_uri , job_resources . logging . file_provider ) if task_descriptor . task_resources : task_descriptor . task_resources = task_descriptor . task_resources . _replace ( logging_path = logging_path ) else : task_descriptor . task_resources = job_model . Resources ( logging_path = logging_path )
7868	def _expire_item ( self , key ) : ( timeout , callback ) = self . _timeouts [ key ] now = time . time ( ) if timeout <= now : item = dict . pop ( self , key ) del self . _timeouts [ key ] if callback : try : callback ( key , item ) except TypeError : try : callback ( key ) except TypeError : callback ( ) return None else : return timeout - now
1292	def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : return self . demo_memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
3900	def dir_maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OSError as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )
6134	def from_file ( cls , fpath , position = 1 , file_id = None ) : if file_id is None : file_id = fpath with open ( fpath ) as f : code = f . read ( ) file_content = str ( code ) file_metadata = FileMetadata ( file_id , position ) return cls ( file_metadata , file_content )
1362	def get_argument_query ( self ) : try : query = self . get_argument ( constants . PARAM_QUERY ) return query except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
8557	def delete_lan ( self , datacenter_id , lan_id ) : response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'DELETE' ) return response
2079	def disassociate_notification_template ( self , job_template , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , job_template , notification_template )
5214	def earning ( ticker , by = 'Geo' , typ = 'Revenue' , ccy = None , level = None , ** kwargs ) -> pd . DataFrame : ovrd = 'G' if by [ 0 ] . upper ( ) == 'G' else 'P' new_kw = dict ( raw = True , Product_Geo_Override = ovrd ) header = bds ( tickers = ticker , flds = 'PG_Bulk_Header' , ** new_kw , ** kwargs ) if ccy : kwargs [ 'Eqy_Fund_Crncy' ] = ccy if level : kwargs [ 'PG_Hierarchy_Level' ] = level data = bds ( tickers = ticker , flds = f'PG_{typ}' , ** new_kw , ** kwargs ) return assist . format_earning ( data = data , header = header )
1203	def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) A_conjugate = self . fn_x ( conjugate ) if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual
13886	def ReplaceInFile ( filename , old , new , encoding = None ) : contents = GetFileContents ( filename , encoding = encoding ) contents = contents . replace ( old , new ) CreateFile ( filename , contents , encoding = encoding ) return contents
7994	def _send ( self , stanza ) : self . fix_out_stanza ( stanza ) element = stanza . as_xml ( ) self . _write_element ( element )
10522	def oneleft ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , ** kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , ** kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
13404	def prettify ( self , elem ) : from xml . etree import ElementTree from re import sub rawString = ElementTree . tostring ( elem , 'utf-8' ) parsedString = sub ( r'(?=<[^/].*>)' , '\n' , rawString ) return parsedString [ 1 : ]
9953	def _get_node ( name : str , args : str ) : obj = get_object ( name ) args = ast . literal_eval ( args ) if not isinstance ( args , tuple ) : args = ( args , ) return obj . node ( * args )
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
8927	def dist ( ctx , devpi = False , egg = False , wheel = False , auto = True ) : config . load ( ) cmd = [ "python" , "setup.py" , "sdist" ] if auto : egg = sys . version_info . major == 2 try : import wheel as _ wheel = True except ImportError : wheel = False if egg : cmd . append ( "bdist_egg" ) if wheel : cmd . append ( "bdist_wheel" ) ctx . run ( "invoke clean --all build --docs test check" ) ctx . run ( ' ' . join ( cmd ) ) if devpi : ctx . run ( "devpi upload dist/*" )
1940	def get_func_signature ( self , hsh : bytes ) -> Optional [ str ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) return self . _function_signatures_by_selector . get ( hsh )
12562	def get_unique_nonzeros ( arr ) : rois = np . unique ( arr ) rois = rois [ np . nonzero ( rois ) ] rois . sort ( ) return rois
12988	def remote_jupyter_proxy_url ( port ) : base_url = os . environ [ 'EXTERNAL_URL' ] host = urllib . parse . urlparse ( base_url ) . netloc if port is None : return host service_url_path = os . environ [ 'JUPYTERHUB_SERVICE_PREFIX' ] proxy_url_path = 'proxy/%d' % port user_url = urllib . parse . urljoin ( base_url , service_url_path ) full_url = urllib . parse . urljoin ( user_url , proxy_url_path ) return full_url
4944	def get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) return DataSharingConsent . objects . proxied_get ( username = username , course_id = course_id , enterprise_customer__uuid = enterprise_customer_uuid )
6728	def respawn ( name = None , group = None ) : if name is None : name = get_name ( ) delete ( name = name , group = group ) instance = get_or_create ( name = name , group = group ) env . host_string = instance . public_dns_name
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
10576	def _create_element_list_ ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
9574	def read_file_header ( fd , endian ) : fields = [ ( 'description' , 's' , 116 ) , ( 'subsystem_offset' , 's' , 8 ) , ( 'version' , 'H' , 2 ) , ( 'endian_test' , 's' , 2 ) ] hdict = { } for name , fmt , num_bytes in fields : data = fd . read ( num_bytes ) hdict [ name ] = unpack ( endian , fmt , data ) hdict [ 'description' ] = hdict [ 'description' ] . strip ( ) v_major = hdict [ 'version' ] >> 8 v_minor = hdict [ 'version' ] & 0xFF hdict [ '__version__' ] = '%d.%d' % ( v_major , v_minor ) return hdict
3307	def _run_cheroot ( app , config , mode ) : assert mode == "cheroot" try : from cheroot import server , wsgi except ImportError : _logger . error ( "*" * 78 ) _logger . error ( "ERROR: Could not import Cheroot." ) _logger . error ( "Try `pip install cheroot` or specify another server using the --server option." ) _logger . error ( "*" * 78 ) raise server_name = "WsgiDAV/{} {} Python/{}" . format ( __version__ , wsgi . Server . version , util . PYTHON_VERSION ) wsgi . Server . version = server_name ssl_certificate = _get_checked_path ( config . get ( "ssl_certificate" ) , config ) ssl_private_key = _get_checked_path ( config . get ( "ssl_private_key" ) , config ) ssl_certificate_chain = _get_checked_path ( config . get ( "ssl_certificate_chain" ) , config ) ssl_adapter = config . get ( "ssl_adapter" , "builtin" ) protocol = "http" if ssl_certificate and ssl_private_key : ssl_adapter = server . get_ssl_adapter_class ( ssl_adapter ) wsgi . Server . ssl_adapter = ssl_adapter ( ssl_certificate , ssl_private_key , ssl_certificate_chain ) protocol = "https" _logger . info ( "SSL / HTTPS enabled. Adapter: {}" . format ( ssl_adapter ) ) elif ssl_certificate or ssl_private_key : raise RuntimeError ( "Option 'ssl_certificate' and 'ssl_private_key' must be used together." ) _logger . info ( "Running {}" . format ( server_name ) ) _logger . info ( "Serving on {}://{}:{} ..." . format ( protocol , config [ "host" ] , config [ "port" ] ) ) server_args = { "bind_addr" : ( config [ "host" ] , config [ "port" ] ) , "wsgi_app" : app , "server_name" : server_name , } server_args . update ( config . get ( "server_args" , { } ) ) server = wsgi . Server ( ** server_args ) startup_event = config . get ( "startup_event" ) if startup_event : def _patched_tick ( ) : server . tick = org_tick _logger . info ( "wsgi.Server is ready" ) startup_event . set ( ) org_tick ( ) org_tick = server . tick server . tick = _patched_tick try : server . start ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) finally : server . stop ( ) return
2358	def registerDriver ( iface , driver , class_implements = [ ] ) : for class_item in class_implements : classImplements ( class_item , iface ) component . provideAdapter ( factory = driver , adapts = [ iface ] , provides = IDriver )
1042	def compare ( left , right , compare_locs = False ) : if type ( left ) != type ( right ) : return False if isinstance ( left , ast . AST ) : for field in left . _fields : if not compare ( getattr ( left , field ) , getattr ( right , field ) ) : return False if compare_locs : for loc in left . _locs : if getattr ( left , loc ) != getattr ( right , loc ) : return False return True elif isinstance ( left , list ) : if len ( left ) != len ( right ) : return False for left_elt , right_elt in zip ( left , right ) : if not compare ( left_elt , right_elt ) : return False return True else : return left == right
10480	def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3873	async def leave_conversation ( self , conv_id ) : logger . info ( 'Leaving conversation: {}' . format ( conv_id ) ) await self . _conv_dict [ conv_id ] . leave ( ) del self . _conv_dict [ conv_id ]
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
10229	def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
635	def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
11080	def get_user ( self , username ) : if hasattr ( self . _bot , 'user_manager' ) : user = self . _bot . user_manager . get_by_username ( username ) if user : return user user = SlackUser . get_user ( self . _bot . sc , username ) self . _bot . user_manager . set ( user ) return user return SlackUser . get_user ( self . _bot . sc , username )
9008	def get_index_in_row ( self ) : expected_index = self . _cached_index_in_row instructions = self . _row . instructions if expected_index is not None and 0 <= expected_index < len ( instructions ) and instructions [ expected_index ] is self : return expected_index for index , instruction_in_row in enumerate ( instructions ) : if instruction_in_row is self : self . _cached_index_in_row = index return index return None
3649	def messages ( self ) : method = 'GET' url = 'activeMessage' rc = self . __request__ ( method , url ) return rc [ 'activeMessage' ]
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
11951	def _import_config ( config_file ) : jocker_lgr . debug ( 'config file is: {0}' . format ( config_file ) ) try : jocker_lgr . debug ( 'importing config...' ) with open ( config_file , 'r' ) as c : return yaml . safe_load ( c . read ( ) ) except IOError as ex : jocker_lgr . error ( str ( ex ) ) raise RuntimeError ( 'cannot access config file' ) except yaml . parser . ParserError as ex : jocker_lgr . error ( 'invalid yaml file: {0}' . format ( ex ) ) raise RuntimeError ( 'invalid yaml file' )
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
1280	def hard_wrap ( self ) : self . linebreak = re . compile ( r'^ *\n(?!\s*$)' ) self . text = re . compile ( r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)' )
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
5940	def _build_arg_list ( self , ** kwargs ) : arglist = [ ] for flag , value in kwargs . items ( ) : flag = str ( flag ) if flag . startswith ( '_' ) : flag = flag [ 1 : ] if not flag . startswith ( '-' ) : flag = '-' + flag if value is True : arglist . append ( flag ) elif value is False : if flag . startswith ( '-no' ) : arglist . append ( '-' + flag [ 3 : ] ) else : arglist . append ( '-no' + flag [ 1 : ] ) elif value is None : pass else : try : arglist . extend ( [ flag ] + value ) except TypeError : arglist . extend ( [ flag , value ] ) return list ( map ( str , arglist ) )
94	def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = "RGB" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img
7522	def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) obj . _origData [ thisField ] = newValue
1293	def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )
1403	def getTopologyInfo ( self , topologyName , cluster , role , environ ) : for ( topology_name , _ ) , topologyInfo in self . topologyInfos . items ( ) : executionState = topologyInfo [ "execution_state" ] if ( topologyName == topology_name and cluster == executionState [ "cluster" ] and environ == executionState [ "environ" ] ) : if not role or executionState . get ( "role" ) == role : return topologyInfo if role is not None : Log . info ( "Could not find topology info for topology: %s," "cluster: %s, role: %s, and environ: %s" , topologyName , cluster , role , environ ) else : Log . info ( "Could not find topology info for topology: %s," "cluster: %s and environ: %s" , topologyName , cluster , environ ) raise Exception ( "No topology found" )
5889	def smart_unicode ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : return force_unicode ( string , encoding , strings_only , errors )
10141	def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( ** self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
8996	def example ( self , relative_path ) : example_path = os . path . join ( "examples" , relative_path ) return self . relative_file ( __file__ , example_path )
204	def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )
12660	def copy ( configfile = '' , destpath = '' , overwrite = False , sub_node = '' ) : log . info ( 'Running {0} {1} {2}' . format ( os . path . basename ( __file__ ) , whoami ( ) , locals ( ) ) ) assert ( os . path . isfile ( configfile ) ) if os . path . exists ( destpath ) : if os . listdir ( destpath ) : raise FolderAlreadyExists ( 'Folder {0} already exists. Please clean ' 'it or change destpath.' . format ( destpath ) ) else : log . info ( 'Creating folder {0}' . format ( destpath ) ) path ( destpath ) . makedirs_p ( ) from boyle . files . file_tree_map import FileTreeMap file_map = FileTreeMap ( ) try : file_map . from_config_file ( configfile ) except Exception as e : raise FileTreeMapError ( str ( e ) ) if sub_node : sub_map = file_map . get_node ( sub_node ) if not sub_map : raise FileTreeMapError ( 'Could not find sub node ' '{0}' . format ( sub_node ) ) file_map . _filetree = { } file_map . _filetree [ sub_node ] = sub_map try : file_map . copy_to ( destpath , overwrite = overwrite ) except Exception as e : raise FileTreeMapError ( str ( e ) )
4034	def ib64_patched ( self , attrsD , contentparams ) : if attrsD . get ( "mode" , "" ) == "base64" : return 0 if self . contentparams [ "type" ] . startswith ( "text/" ) : return 0 if self . contentparams [ "type" ] . endswith ( "+xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/xml" ) : return 0 if self . contentparams [ "type" ] . endswith ( "/json" ) : return 0 return 0
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
13386	def upstream_url ( self , uri ) : "Returns the URL to the upstream data source for the given URI based on configuration" return self . application . options . upstream + self . request . uri
12747	def load ( self , source , ** kwargs ) : if hasattr ( source , 'endswith' ) and source . lower ( ) . endswith ( '.asf' ) : self . load_asf ( source , ** kwargs ) else : self . load_skel ( source , ** kwargs )
3068	def wrap_http_for_auth ( credentials , http ) : orig_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if not credentials . access_token : _LOGGER . info ( 'Attempting refresh to obtain ' 'initial access_token' ) credentials . _refresh ( orig_request_method ) headers = _initialize_headers ( headers ) credentials . apply ( headers ) _apply_user_agent ( headers , credentials . user_agent ) body_stream_position = None if all ( getattr ( body , stream_prop , None ) for stream_prop in _STREAM_PROPERTIES ) : body_stream_position = body . tell ( ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) max_refresh_attempts = 2 for refresh_attempt in range ( max_refresh_attempts ) : if resp . status not in REFRESH_STATUS_CODES : break _LOGGER . info ( 'Refreshing due to a %s (attempt %s/%s)' , resp . status , refresh_attempt + 1 , max_refresh_attempts ) credentials . _refresh ( orig_request_method ) credentials . apply ( headers ) if body_stream_position is not None : body . seek ( body_stream_position ) resp , content = request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) return resp , content http . request = new_request http . request . credentials = credentials
11591	def _rc_sunion ( self , src , * args ) : args = list_or_args ( src , args ) src_set = self . smembers ( args . pop ( 0 ) ) if src_set is not set ( [ ] ) : for key in args : src_set . update ( self . smembers ( key ) ) return src_set
3861	def add_event ( self , event_ ) : conv_event = self . _wrap_event ( event_ ) if conv_event . id_ not in self . _events_dict : self . _events . append ( conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return None return conv_event
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
1784	def CMP ( cpu , src1 , src2 ) : arg0 = src1 . read ( ) arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 )
6275	def resolve_loader ( self , meta : ResourceDescription ) : meta . loader_cls = self . get_loader ( meta , raise_on_error = True )
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
11880	def scanAllProcessesForCwd ( searchPortion , isExactMatch = False ) : pids = getAllRunningPids ( ) cwdResults = [ scanProcessForCwd ( pid , searchPortion , isExactMatch ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if cwdResults [ i ] is not None : ret [ pids [ i ] ] = cwdResults [ i ] return ret
12579	def mask_and_flatten ( self ) : self . _check_for_mask ( ) return self . get_data ( smoothed = True , masked = True , safe_copy = False ) [ self . get_mask_indices ( ) ] , self . get_mask_indices ( ) , self . mask . shape
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
64	def is_out_of_image ( self , image , fully = True , partly = False ) : if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully
3672	def bubble_at_P ( P , zs , vapor_pressure_eqns , fugacities = None , gammas = None ) : def bubble_P_error ( T ) : Psats = [ VP ( T ) for VP in vapor_pressure_eqns ] Pcalc = bubble_at_T ( zs , Psats , fugacities , gammas ) return P - Pcalc T_bubble = newton ( bubble_P_error , 300 ) return T_bubble
9504	def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
3191	def update ( self , folder_id , data ) : if 'name' not in data : raise KeyError ( 'The template folder must have a name' ) self . folder_id = folder_id return self . _mc_client . _patch ( url = self . _build_path ( folder_id ) , data = data )
10336	def bel_to_spia_matrices ( graph : BELGraph ) -> Mapping [ str , pd . DataFrame ] : index_nodes = get_matrix_index ( graph ) spia_matrices = build_spia_matrices ( index_nodes ) for u , v , edge_data in graph . edges ( data = True ) : if isinstance ( u , CentralDogma ) and isinstance ( v , CentralDogma ) : update_spia_matrices ( spia_matrices , u , v , edge_data ) elif isinstance ( u , CentralDogma ) and isinstance ( v , ListAbundance ) : for node in v . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , u , node , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , CentralDogma ) : for node in u . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , node , v , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for sub_member , obj_member in product ( u . members , v . members ) : if isinstance ( sub_member , CentralDogma ) and isinstance ( obj_member , CentralDogma ) : update_spia_matrices ( spia_matrices , sub_member , obj_member , edge_data ) return spia_matrices
5377	def _build_pipeline_input_file_param ( cls , var_name , docker_path ) : path , filename = os . path . split ( docker_path ) if '*' in filename : return cls . _build_pipeline_file_param ( var_name , path + '/' ) else : return cls . _build_pipeline_file_param ( var_name , docker_path )
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
3752	def STEL ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] : _STEL = ( _OntarioExposureLimits [ CASRN ] [ "STEL (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _STEL = None else : raise Exception ( 'Failure in in function' ) return _STEL
4358	def send_packet ( self , pkt ) : self . put_client_msg ( packet . encode ( pkt , self . json_dumps ) )
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
9443	def call ( self , call_params ) : path = '/' + self . api_version + '/Call/' method = 'POST' return self . request ( path , method , call_params )
7718	def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) self . xmlnode . freeNode ( ) self . xmlnode = None
7754	def process_stanza ( self , stanza ) : self . fix_in_stanza ( stanza ) to_jid = stanza . to_jid if not self . process_all_stanzas and to_jid and ( to_jid != self . me and to_jid . bare ( ) != self . me . bare ( ) ) : return self . route_stanza ( stanza ) try : if isinstance ( stanza , Iq ) : if self . process_iq ( stanza ) : return True elif isinstance ( stanza , Message ) : if self . process_message ( stanza ) : return True elif isinstance ( stanza , Presence ) : if self . process_presence ( stanza ) : return True except ProtocolError , err : typ = stanza . stanza_type if typ != 'error' and ( typ != 'result' or stanza . stanza_type != 'iq' ) : response = stanza . make_error_response ( err . xmpp_name ) self . send ( response ) err . log_reported ( ) else : err . log_ignored ( ) return logger . debug ( "Unhandled %r stanza: %r" % ( stanza . stanza_type , stanza . serialize ( ) ) ) return False
13867	def truncate ( when , unit , week_start = mon ) : if is_datetime ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( round ( when . microsecond / 1000.0 ) ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) elif unit == hour : return when . replace ( minute = 0 , second = 0 , microsecond = 0 ) elif unit == day : return when . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == week : weekday = prevweekday ( when , week_start ) return when . replace ( year = weekday . year , month = weekday . month , day = weekday . day , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == month : return when . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif unit == year : return when . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) elif is_date ( when ) : if unit == week : return prevweekday ( when , week_start ) elif unit == month : return when . replace ( day = 1 ) elif unit == year : return when . replace ( month = 1 , day = 1 ) elif is_time ( when ) : if unit == millisecond : return when . replace ( microsecond = int ( when . microsecond / 1000.0 ) * 1000 ) elif unit == second : return when . replace ( microsecond = 0 ) elif unit == minute : return when . replace ( second = 0 , microsecond = 0 ) return when
10503	def waitForFocusedWindowToChange ( self , nextWinName , timeout = 10 ) : callback = AXCallbacks . returnElemCallback retelem = None return self . waitFor ( timeout , 'AXFocusedWindowChanged' , AXTitle = nextWinName )
8497	def _output ( calls , args ) : if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) return keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' )
3493	def production_envelope ( model , reactions , objective = None , carbon_sources = None , points = 20 , threshold = None ) : reactions = model . reactions . get_by_any ( reactions ) objective = model . solver . objective if objective is None else objective data = dict ( ) if carbon_sources is None : c_input = find_carbon_sources ( model ) else : c_input = model . reactions . get_by_any ( carbon_sources ) if c_input is None : data [ 'carbon_source' ] = None elif hasattr ( c_input , 'id' ) : data [ 'carbon_source' ] = c_input . id else : data [ 'carbon_source' ] = ', ' . join ( rxn . id for rxn in c_input ) threshold = normalize_cutoff ( model , threshold ) size = points ** len ( reactions ) for direction in ( 'minimum' , 'maximum' ) : data [ 'flux_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'carbon_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) data [ 'mass_yield_{}' . format ( direction ) ] = full ( size , nan , dtype = float ) grid = pd . DataFrame ( data ) with model : model . objective = objective objective_reactions = list ( sutil . linear_reaction_coefficients ( model ) ) if len ( objective_reactions ) != 1 : raise ValueError ( 'cannot calculate yields for objectives with ' 'multiple reactions' ) c_output = objective_reactions [ 0 ] min_max = fva ( model , reactions , fraction_of_optimum = 0 ) min_max [ min_max . abs ( ) < threshold ] = 0.0 points = list ( product ( * [ linspace ( min_max . at [ rxn . id , "minimum" ] , min_max . at [ rxn . id , "maximum" ] , points , endpoint = True ) for rxn in reactions ] ) ) tmp = pd . DataFrame ( points , columns = [ rxn . id for rxn in reactions ] ) grid = pd . concat ( [ grid , tmp ] , axis = 1 , copy = False ) add_envelope ( model , reactions , grid , c_input , c_output , threshold ) return grid
11095	def select_by_ext ( self , ext , recursive = True ) : ext = [ ext . strip ( ) . lower ( ) for ext in ensure_list ( ext ) ] def filters ( p ) : return p . suffix . lower ( ) in ext return self . select_file ( filters , recursive )
7870	def _decode_error ( self ) : error_qname = self . _ns_prefix + "error" for child in self . _element : if child . tag == error_qname : self . _error = StanzaErrorElement ( child ) return raise BadRequestProtocolError ( "Error element missing in" " an error stanza" )
8688	def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
5152	def merge_list ( list1 , list2 , identifiers = None ) : identifiers = identifiers or [ ] dict_map = { 'list1' : OrderedDict ( ) , 'list2' : OrderedDict ( ) } counter = 1 for list_ in [ list1 , list2 ] : container = dict_map [ 'list{0}' . format ( counter ) ] for el in list_ : key = id ( el ) if isinstance ( el , dict ) : for id_key in identifiers : if id_key in el : key = el [ id_key ] break container [ key ] = deepcopy ( el ) counter += 1 merged = merge_config ( dict_map [ 'list1' ] , dict_map [ 'list2' ] ) return list ( merged . values ( ) )
8129	def suggest_spelling ( q , wait = 10 , asynchronous = False , cached = False ) : return YahooSpelling ( q , wait , asynchronous , cached )
11984	async def copy_storage_object ( self , source_bucket , source_key , bucket , key ) : info = await self . head_object ( Bucket = source_bucket , Key = source_key ) size = info [ 'ContentLength' ] if size > MULTI_PART_SIZE : result = await _multipart_copy ( self , source_bucket , source_key , bucket , key , size ) else : result = await self . copy_object ( Bucket = bucket , Key = key , CopySource = _source_string ( source_bucket , source_key ) ) return result
9637	def format ( self , record ) : data = record . _raw . copy ( ) data [ 'time' ] = data [ 'time' ] . isoformat ( ) if data . get ( 'traceback' ) : data [ 'traceback' ] = self . formatException ( data [ 'traceback' ] ) return json . dumps ( data )
2938	def deserialize_assign_list ( self , workflow , start_node ) : assignments = [ ] for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'assign' : assignments . append ( self . deserialize_assign ( workflow , node ) ) else : _exc ( 'Unknown node: %s' % node . nodeName ) return assignments
5689	def get_day_start_ut_span ( self ) : cur = self . conn . cursor ( ) first_day_start_ut , last_day_start_ut = cur . execute ( "SELECT min(day_start_ut), max(day_start_ut) FROM days;" ) . fetchone ( ) return first_day_start_ut , last_day_start_ut
7193	def histogram_match ( self , use_bands , blm_source = None , ** kwargs ) : assert has_rio , "To match image histograms please install rio_hist" data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) if 0 in data : data = np . ma . masked_values ( data , 0 ) bounds = self . _reproject ( box ( * self . bounds ) , from_proj = self . proj , to_proj = "EPSG:4326" ) . bounds if blm_source == 'browse' : from gbdxtools . images . browse_image import BrowseImage ref = BrowseImage ( self . cat_id , bbox = bounds ) . read ( ) else : from gbdxtools . images . tms_image import TmsImage tms = TmsImage ( zoom = self . _calc_tms_zoom ( self . affine [ 0 ] ) , bbox = bounds , ** kwargs ) ref = np . rollaxis ( tms . read ( ) , 0 , 3 ) out = np . dstack ( [ rio_match ( data [ : , : , idx ] , ref [ : , : , idx ] . astype ( np . double ) / 255.0 ) for idx in range ( data . shape [ - 1 ] ) ] ) if 'stretch' in kwargs or 'gamma' in kwargs : return self . _histogram_stretch ( out , ** kwargs ) else : return out
2089	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the record.' , header = 'details' ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** kwargs ) return response [ 'results' ] [ 0 ]
12289	def bootstrap_datapackage ( repo , force = False , options = None , noinput = False ) : print ( "Bootstrapping datapackage" ) tsprefix = datetime . now ( ) . date ( ) . isoformat ( ) package = OrderedDict ( [ ( 'title' , '' ) , ( 'description' , '' ) , ( 'username' , repo . username ) , ( 'reponame' , repo . reponame ) , ( 'name' , str ( repo ) ) , ( 'title' , "" ) , ( 'description' , "" ) , ( 'keywords' , [ ] ) , ( 'resources' , [ ] ) , ( 'creator' , getpass . getuser ( ) ) , ( 'createdat' , datetime . now ( ) . isoformat ( ) ) , ( 'remote-url' , repo . remoteurl ) ] ) if options is not None : package [ 'title' ] = options [ 'title' ] package [ 'description' ] = options [ 'description' ] else : if noinput : raise IncompleteParameters ( "Option field with title and description" ) for var in [ 'title' , 'description' ] : value = '' while value in [ '' , None ] : value = input ( 'Your Repo ' + var . title ( ) + ": " ) if len ( value ) == 0 : print ( "{} cannot be empty. Please re-enter." . format ( var . title ( ) ) ) package [ var ] = value ( handle , filename ) = tempfile . mkstemp ( ) with open ( filename , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) repo . package = package return filename
3660	def _coeff_ind_from_T ( self , T ) : if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
13639	def bind ( mod_path , with_path = None ) : if with_path : if os . path . isdir ( with_path ) : sys . path . insert ( 0 , with_path ) else : sys . path . insert ( 0 , with_path . rsplit ( '/' , 2 ) [ 0 ] ) pass mod = importlib . import_module ( mod_path ) settings = Settings ( ) for v in dir ( mod ) : if v [ 0 ] == '_' or type ( getattr ( mod , v ) ) . __name__ == 'module' : continue setattr ( settings , v , getattr ( mod , v ) ) pass Settings . _path = mod_path Settings . _wrapped = settings return settings
7827	def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : func . _pyxmpp_stream_element_handled = element_name func . _pyxmpp_usage_restriction = usage_restriction return func return decorator
11794	def mac ( csp , var , value , assignment , removals ) : "Maintain arc consistency." return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
482	def createAndStartSwarm ( client , clientInfo = "" , clientKey = "" , params = "" , minimumWorkers = None , maximumWorkers = None , alreadyRunning = False ) : if minimumWorkers is None : minimumWorkers = Configuration . getInt ( "nupic.hypersearch.minWorkersPerSwarm" ) if maximumWorkers is None : maximumWorkers = Configuration . getInt ( "nupic.hypersearch.maxWorkersPerSwarm" ) return ClientJobsDAO . get ( ) . jobInsert ( client = client , cmdLine = "$HYPERSEARCH" , clientInfo = clientInfo , clientKey = clientKey , alreadyRunning = alreadyRunning , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = ClientJobsDAO . JOB_TYPE_HS )
2386	def create_model_path ( model_path ) : if not model_path . startswith ( "/" ) and not model_path . startswith ( "models/" ) : model_path = "/" + model_path if not model_path . startswith ( "models" ) : model_path = "models" + model_path if not model_path . endswith ( ".p" ) : model_path += ".p" return model_path
1981	def sys_transmit ( self , cpu , fd , buf , count , tx_bytes ) : if issymbolic ( fd ) : logger . info ( "Ask to write to a symbolic file descriptor!!" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 0 ) if issymbolic ( buf ) : logger . info ( "Ask to write to a symbolic buffer" ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 1 ) if issymbolic ( count ) : logger . info ( "Ask to write a symbolic number of bytes " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 2 ) if issymbolic ( tx_bytes ) : logger . info ( "Ask to return size to a symbolic address " ) cpu . PC = cpu . PC - cpu . instruction . size raise SymbolicSyscallArgument ( cpu , 3 ) return super ( ) . sys_transmit ( cpu , fd , buf , count , tx_bytes )
4872	def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
7134	def run_cell ( self , cell ) : globals = self . ipy_shell . user_global_ns locals = self . ipy_shell . user_ns globals . update ( { "__ipy_scope__" : None , } ) try : with redirect_stdout ( self . stdout ) : self . run ( cell , globals , locals ) except : self . code_error = True if self . options . debug : raise BdbQuit finally : self . finalize ( )
2507	def get_extr_lics_xref ( self , extr_lic ) : xrefs = list ( self . graph . triples ( ( extr_lic , RDFS . seeAlso , None ) ) ) return map ( lambda xref_triple : xref_triple [ 2 ] , xrefs )
732	def _generate ( self ) : n = self . _n w = self . _w assert type ( w ) is int , "List for w not supported" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . _patterns [ i ] = pattern
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
7767	def _stream_authenticated ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . setup_stanza_handlers ( handlers , "post-auth" )
12595	def get_aad_token ( endpoint , no_verify ) : from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
1423	def copy ( self , new_object ) : new_object . classdesc = self . classdesc for name in self . classdesc . fields_names : new_object . __setattr__ ( name , getattr ( self , name ) )
7484	def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . realpath ( data . paramsdict [ "project_dir" ] ) , data . name + "_edits" ) if not os . path . exists ( data . dirs . edits ) : os . makedirs ( data . dirs . edits ) subsamples = choose_samples ( samples , force ) if int ( data . paramsdict [ "filter_adapters" ] ) == 3 : if not data . _hackersonly [ "p3_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p3_adapters_extra" ] . append ( poly ) if not data . _hackersonly [ "p5_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p5_adapters_extra" ] . append ( poly ) else : data . _hackersonly [ "p5_adapters_extra" ] = [ ] data . _hackersonly [ "p3_adapters_extra" ] = [ ] subsamples = concat_reads ( data , subsamples , ipyclient ) lbview = ipyclient . load_balanced_view ( targets = ipyclient . ids [ : : 2 ] ) run_cutadapt ( data , subsamples , lbview ) assembly_cleanup ( data )
3033	def credentials_from_clientsecrets_and_code ( filename , scope , code , message = None , redirect_uri = 'postmessage' , http = None , cache = None , device_uri = None ) : flow = flow_from_clientsecrets ( filename , scope , message = message , cache = cache , redirect_uri = redirect_uri , device_uri = device_uri ) credentials = flow . step2_exchange ( code , http = http ) return credentials
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : r if verbose is None : verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
8657	def _clean ( zipcode , valid_length = _valid_zipcode_length ) : zipcode = zipcode . split ( "-" ) [ 0 ] if len ( zipcode ) != valid_length : raise ValueError ( 'Invalid format, zipcode must be of the format: "#####" or "#####-####"' ) if _contains_nondigits ( zipcode ) : raise ValueError ( 'Invalid characters, zipcode may only contain digits and "-".' ) return zipcode
3157	def create ( self , list_id , data ) : return self . _mc_client . _post ( url = self . _build_path ( list_id , 'segments' ) , data = data )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
5216	def fut_ticker ( gen_ticker : str , dt , freq : str , log = logs . LOG_LEVEL ) -> str : logger = logs . get_logger ( fut_ticker , level = log ) dt = pd . Timestamp ( dt ) t_info = gen_ticker . split ( ) asset = t_info [ - 1 ] if asset in [ 'Index' , 'Curncy' , 'Comdty' ] : ticker = ' ' . join ( t_info [ : - 1 ] ) prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , asset elif asset == 'Equity' : ticker = t_info [ 0 ] prefix , idx , postfix = ticker [ : - 1 ] , int ( ticker [ - 1 ] ) - 1 , ' ' . join ( t_info [ 1 : ] ) else : logger . error ( f'unkonwn asset type for ticker: {gen_ticker}' ) return '' month_ext = 4 if asset == 'Comdty' else 2 months = pd . date_range ( start = dt , periods = max ( idx + month_ext , 3 ) , freq = freq ) logger . debug ( f'pulling expiry dates for months: {months}' ) def to_fut ( month ) : return prefix + const . Futures [ month . strftime ( '%b' ) ] + month . strftime ( '%y' ) [ - 1 ] + ' ' + postfix fut = [ to_fut ( m ) for m in months ] logger . debug ( f'trying futures: {fut}' ) try : fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e1 : logger . error ( f'error downloading futures contracts (1st trial) {e1}:\n{fut}' ) try : fut = fut [ : - 1 ] logger . debug ( f'trying futures (2nd trial): {fut}' ) fut_matu = bdp ( tickers = fut , flds = 'last_tradeable_dt' , cache = True ) except Exception as e2 : logger . error ( f'error downloading futures contracts (2nd trial) {e2}:\n{fut}' ) return '' sub_fut = fut_matu [ pd . DatetimeIndex ( fut_matu . last_tradeable_dt ) > dt ] logger . debug ( f'futures full chain:\n{fut_matu.to_string()}' ) logger . debug ( f'getting index {idx} from:\n{sub_fut.to_string()}' ) return sub_fut . index . values [ idx ]
2685	def fate ( name ) : return cached_download ( 'http://fate.ffmpeg.org/fate-suite/' + name , os . path . join ( 'fate-suite' , name . replace ( '/' , os . path . sep ) ) )
8001	def fix_in_stanza ( self , stanza ) : StreamBase . fix_in_stanza ( self , stanza ) if not self . initiator : if stanza . from_jid != self . peer : stanza . set_from ( self . peer )
10954	def model_to_data ( self , sigma = 0.0 ) : im = self . model . copy ( ) im += sigma * np . random . randn ( * im . shape ) self . set_image ( util . NullImage ( image = im ) )
796	def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT %s FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % ( dbFieldsStr , self . jobsTableName ) conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows
3127	def update ( self , template_id , data ) : if 'name' not in data : raise KeyError ( 'The template must have a name' ) if 'html' not in data : raise KeyError ( 'The template must have html' ) self . template_id = template_id return self . _mc_client . _patch ( url = self . _build_path ( template_id ) , data = data )
9818	def upgrade ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . upgrade_on_kubernetes ( ) elif self . is_docker_compose : self . upgrade_on_docker_compose ( ) elif self . is_docker : self . upgrade_on_docker ( ) elif self . is_heroku : self . upgrade_on_heroku ( )
6658	def _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained = False , memory_limit = None , test_mode = False ) : if not memory_constrained : return np . sum ( ( np . dot ( inbag - 1 , pred_centered . T ) / n_trees ) ** 2 , 0 ) if not memory_limit : raise ValueError ( 'If memory_constrained=True, must provide' , 'memory_limit.' ) chunk_size = int ( ( memory_limit * 1e6 ) / ( 8.0 * X_train . shape [ 0 ] ) ) if chunk_size == 0 : min_limit = 8.0 * X_train . shape [ 0 ] / 1e6 raise ValueError ( 'memory_limit provided is too small.' + 'For these dimensions, memory_limit must ' + 'be greater than or equal to %.3e' % min_limit ) chunk_edges = np . arange ( 0 , X_test . shape [ 0 ] + chunk_size , chunk_size ) inds = range ( X_test . shape [ 0 ] ) chunks = [ inds [ chunk_edges [ i ] : chunk_edges [ i + 1 ] ] for i in range ( len ( chunk_edges ) - 1 ) ] if test_mode : print ( 'Number of chunks: %d' % ( len ( chunks ) , ) ) V_IJ = np . concatenate ( [ np . sum ( ( np . dot ( inbag - 1 , pred_centered [ chunk ] . T ) / n_trees ) ** 2 , 0 ) for chunk in chunks ] ) return V_IJ
12776	def forward_dynamics ( self , torques , start = 0 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , torque in enumerate ( torques ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) self . skeleton . add_torques ( torque ) self . ode_world . step ( self . dt ) yield self . ode_contactgroup . empty ( )
5105	def poisson_random_measure ( t , rate , rate_max ) : scale = 1.0 / rate_max t = t + exponential ( scale ) while rate_max * uniform ( ) > rate ( t ) : t = t + exponential ( scale ) return t
1197	def nested ( * managers ) : warn ( "With-statements now directly support multiple context managers" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]
13458	def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
11887	def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : _LOGGER . error ( "Error receiving: %s" , error ) return "" buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
11795	def min_conflicts ( csp , max_steps = 100000 ) : csp . current = current = { } for var in csp . vars : val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) for i in range ( max_steps ) : conflicted = csp . conflicted_vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) return None
11487	def _search_folder_for_item_or_folder ( name , folder_id ) : session . token = verify_credentials ( ) children = session . communicator . folder_children ( session . token , folder_id ) for folder in children [ 'folders' ] : if folder [ 'name' ] == name : return False , folder [ 'folder_id' ] for item in children [ 'items' ] : if item [ 'name' ] == name : return True , item [ 'item_id' ] return False , - 1
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
9654	def take_shas_of_all_files ( G , settings ) : global ERROR_FN sprint = settings [ "sprint" ] error = settings [ "error" ] ERROR_FN = error sha_dict = { } all_files = [ ] for target in G . nodes ( data = True ) : sprint ( "About to take shas of files in target '{}'" . format ( target [ 0 ] ) , level = "verbose" ) if 'dependencies' in target [ 1 ] : sprint ( "It has dependencies" , level = "verbose" ) deplist = [ ] for dep in target [ 1 ] [ 'dependencies' ] : glist = glob . glob ( dep ) if glist : for oneglob in glist : deplist . append ( oneglob ) else : deplist . append ( dep ) target [ 1 ] [ 'dependencies' ] = list ( deplist ) for dep in target [ 1 ] [ 'dependencies' ] : sprint ( " - {}" . format ( dep ) , level = "verbose" ) all_files . append ( dep ) if 'output' in target [ 1 ] : sprint ( "It has outputs" , level = "verbose" ) for out in acts . get_all_outputs ( target [ 1 ] ) : sprint ( " - {}" . format ( out ) , level = "verbose" ) all_files . append ( out ) if len ( all_files ) : sha_dict [ 'files' ] = { } extant_files = [ ] for item in all_files : if item not in extant_files and os . path . isfile ( item ) : extant_files . append ( item ) pool = Pool ( ) results = pool . map ( get_sha , extant_files ) pool . close ( ) pool . join ( ) for fn , sha in zip ( extant_files , results ) : sha_dict [ 'files' ] [ fn ] = { 'sha' : sha } return sha_dict sprint ( "No dependencies" , level = "verbose" )
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
4009	def _start_http_server ( ) : logging . info ( 'Starting HTTP server at {}:{}' . format ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread = threading . Thread ( target = http_server . app . run , args = ( constants . DAEMON_HTTP_BIND_IP , constants . DAEMON_HTTP_BIND_PORT ) ) thread . daemon = True thread . start ( )
7023	def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes = base64 . b64decode ( b64str ) if writetostrio : outobj = StrIO ( filebytes ) return outobj else : with open ( outfpath , 'wb' ) as outfd : outfd . write ( filebytes ) if os . path . exists ( outfpath ) : return outfpath else : LOGERROR ( 'could not write output file: %s' % outfpath ) return None except Exception as e : LOGEXCEPTION ( 'failed while trying to convert ' 'b64 string to file %s' % outfpath ) return None
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
6304	def find_effect_class ( self , path ) -> Type [ Effect ] : package_name , class_name = parse_package_string ( path ) if package_name : package = self . get_package ( package_name ) return package . find_effect_class ( class_name , raise_for_error = True ) for package in self . packages : effect_cls = package . find_effect_class ( class_name ) if effect_cls : return effect_cls raise EffectError ( "No effect class '{}' found in any packages" . format ( class_name ) )
10303	def tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , ** kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
13119	def count ( self , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
2407	def extract_features_and_generate_model_predictors ( predictor_set , algorithm = util_functions . AlgorithmTypes . regression ) : if ( algorithm not in [ util_functions . AlgorithmTypes . regression , util_functions . AlgorithmTypes . classification ] ) : algorithm = util_functions . AlgorithmTypes . regression f = predictor_extractor . PredictorExtractor ( ) f . initialize_dictionaries ( predictor_set ) train_feats = f . gen_feats ( predictor_set ) clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , predictor_set . _target ) try : set_score = numpy . asarray ( predictor_set . _target , dtype = numpy . int ) clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score = predictor_set . _target set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
4651	def appendWif ( self , wif ) : if wif : try : self . privatekey_class ( wif ) self . wifs . add ( wif ) except Exception : raise InvalidWifError
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
3059	def _get_backend ( filename ) : filename = os . path . abspath ( filename ) with _backends_lock : if filename not in _backends : _backends [ filename ] = _MultiprocessStorageBackend ( filename ) return _backends [ filename ]
12062	def stats_first ( abf ) : msg = "" for sweep in range ( abf . sweeps ) : for AP in abf . APs [ sweep ] : for key in sorted ( AP . keys ( ) ) : if key [ - 1 ] is "I" or key [ - 2 : ] in [ "I1" , "I2" ] : continue msg += "%s = %s\n" % ( key , AP [ key ] ) return msg
4966	def clean ( self ) : cleaned_data = super ( ManageLearnersForm , self ) . clean ( ) email_or_username = self . data . get ( self . Fields . EMAIL_OR_USERNAME , None ) bulk_upload_csv = self . files . get ( self . Fields . BULK_UPLOAD , None ) if not email_or_username and not bulk_upload_csv : raise ValidationError ( ValidationMessages . NO_FIELDS_SPECIFIED ) if email_or_username and bulk_upload_csv : raise ValidationError ( ValidationMessages . BOTH_FIELDS_SPECIFIED ) if email_or_username : mode = self . Modes . MODE_SINGULAR else : mode = self . Modes . MODE_BULK cleaned_data [ self . Fields . MODE ] = mode cleaned_data [ self . Fields . NOTIFY ] = self . clean_notify ( ) self . _validate_course ( ) self . _validate_program ( ) if self . data . get ( self . Fields . PROGRAM , None ) and self . data . get ( self . Fields . COURSE , None ) : raise ValidationError ( ValidationMessages . COURSE_AND_PROGRAM_ERROR ) return cleaned_data
6671	def is_file ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isfile ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -f "%(path)s" ]' % locals ( ) ) . succeeded
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) return wrapper
11621	def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : self . curr_script = n self . delta = n * DELTA return
7533	def concat_multiple_edits ( data , sample ) : if len ( sample . files . edits ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . edits ] conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concatedit.fq.gz" ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd1 , res1 ) conc2 = 0 if os . path . exists ( str ( sample . files . edits [ 0 ] [ 1 ] ) ) : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . edits ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concatedit.fq.gz" ) with gzip . open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd2 , res2 ) sample . files . edits = [ ( conc1 , conc2 ) ] return sample . files . edits
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
6639	def runScript ( self , scriptname , additional_environment = None ) : import subprocess import shlex command = self . getScript ( scriptname ) if command is None : logger . debug ( '%s has no script %s' , self , scriptname ) return 0 if not len ( command ) : logger . error ( "script %s of %s is empty" , scriptname , self . getName ( ) ) return 1 env = os . environ . copy ( ) if additional_environment is not None : env . update ( additional_environment ) errcode = 0 child = None try : logger . debug ( 'running script: %s' , command ) child = subprocess . Popen ( command , cwd = self . path , env = env ) child . wait ( ) if child . returncode : logger . error ( "script %s (from %s) exited with non-zero status %s" , scriptname , self . getName ( ) , child . returncode ) errcode = child . returncode child = None finally : if child is not None : tryTerminate ( child ) return errcode
12148	def convertImages ( self ) : exts = [ '.jpg' , '.png' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_jpg_" + fname if not fname2 in self . files2 : self . log . info ( "copying over [%s]" % fname2 ) shutil . copy ( os . path . join ( self . folder1 , fname ) , os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname ) exts = [ '.tif' , '.tiff' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_tif_" + fname + ".jpg" if not fname2 in self . files2 : self . log . info ( "converting micrograph [%s]" % fname2 ) imaging . TIF_to_jpg ( os . path . join ( self . folder1 , fname ) , saveAs = os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname )
1651	def _ClassifyInclude ( fileinfo , include , is_system ) : is_cpp_h = include in _CPP_HEADERS if is_system and os . path . splitext ( include ) [ 1 ] in [ '.hpp' , '.hxx' , '.h++' ] : is_system = False if is_system : if is_cpp_h : return _CPP_SYS_HEADER else : return _C_SYS_HEADER target_dir , target_base = ( os . path . split ( _DropCommonSuffixes ( fileinfo . RepositoryName ( ) ) ) ) include_dir , include_base = os . path . split ( _DropCommonSuffixes ( include ) ) target_dir_pub = os . path . normpath ( target_dir + '/../public' ) target_dir_pub = target_dir_pub . replace ( '\\' , '/' ) if target_base == include_base and ( include_dir == target_dir or include_dir == target_dir_pub ) : return _LIKELY_MY_HEADER target_first_component = _RE_FIRST_COMPONENT . match ( target_base ) include_first_component = _RE_FIRST_COMPONENT . match ( include_base ) if ( target_first_component and include_first_component and target_first_component . group ( 0 ) == include_first_component . group ( 0 ) ) : return _POSSIBLE_MY_HEADER return _OTHER_HEADER
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
3891	def html ( tag ) : return ( HTML_START . format ( tag = tag ) , HTML_END . format ( tag = tag ) )
8810	def delete_segment_allocation_range ( context , sa_id ) : LOG . info ( "delete_segment_allocation_range %s for tenant %s" % ( sa_id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : sa_range = db_api . segment_allocation_range_find ( context , id = sa_id , scope = db_api . ONE ) if not sa_range : raise q_exc . SegmentAllocationRangeNotFound ( segment_allocation_range_id = sa_id ) _delete_segment_allocation_range ( context , sa_range )
10171	def post ( self , ** kwargs ) : data = request . get_json ( force = False ) if data is None : data = { } result = { } for query_name , config in data . items ( ) : if config is None or not isinstance ( config , dict ) or ( set ( config . keys ( ) ) != { 'stat' , 'params' } and set ( config . keys ( ) ) != { 'stat' } ) : raise InvalidRequestInputError ( 'Invalid Input. It should be of the form ' '{ STATISTIC_NAME: { "stat": STAT_TYPE, ' '"params": STAT_PARAMS \}}' ) stat = config [ 'stat' ] params = config . get ( 'params' , { } ) try : query_cfg = current_stats . queries [ stat ] except KeyError : raise UnknownQueryError ( stat ) permission = current_stats . permission_factory ( stat , params ) if permission is not None and not permission . can ( ) : message = ( 'You do not have a permission to query the ' 'statistic "{}" with those ' 'parameters' . format ( stat ) ) if current_user . is_authenticated : abort ( 403 , message ) abort ( 401 , message ) try : query = query_cfg . query_class ( ** query_cfg . query_config ) result [ query_name ] = query . run ( ** params ) except ValueError as e : raise InvalidRequestInputError ( e . args [ 0 ] ) except NotFoundError as e : return None return self . make_response ( result )
13649	def get_fuel_prices_within_radius ( self , latitude : float , longitude : float , radius : int , fuel_type : str , brands : Optional [ List [ str ] ] = None ) -> List [ StationPrice ] : if brands is None : brands = [ ] response = requests . post ( '{}/prices/nearby' . format ( API_URL_BASE ) , json = { 'fueltype' : fuel_type , 'latitude' : latitude , 'longitude' : longitude , 'radius' : radius , 'brand' : brands , } , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) data = response . json ( ) stations = { station [ 'code' ] : Station . deserialize ( station ) for station in data [ 'stations' ] } station_prices = [ ] for serialized_price in data [ 'prices' ] : price = Price . deserialize ( serialized_price ) station_prices . append ( StationPrice ( price = price , station = stations [ price . station_code ] ) ) return station_prices
101	def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) if not img_np . flags [ "WRITEABLE" ] : try : img_np . setflags ( write = True ) except ValueError as ex : if "cannot set WRITEABLE flag to True of this array" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np
3721	def ion_balance_proportional ( anion_charges , cation_charges , zs , n_anions , n_cations , balance_error , method ) : anion_zs = zs [ 0 : n_anions ] cation_zs = zs [ n_anions : n_cations + n_anions ] anion_balance_error = sum ( [ zi * ci for zi , ci in zip ( anion_zs , anion_charges ) ] ) cation_balance_error = sum ( [ zi * ci for zi , ci in zip ( cation_zs , cation_charges ) ] ) if method == 'proportional insufficient ions increase' : if balance_error < 0 : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] else : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] elif method == 'proportional excess ions decrease' : if balance_error < 0 : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] else : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] elif method == 'proportional cation adjustment' : multiplier = - anion_balance_error / cation_balance_error cation_zs = [ i * multiplier for i in cation_zs ] elif method == 'proportional anion adjustment' : multiplier = - cation_balance_error / anion_balance_error anion_zs = [ i * multiplier for i in anion_zs ] else : raise Exception ( 'Allowable methods are %s' % charge_balance_methods ) z_water = 1. - sum ( anion_zs ) - sum ( cation_zs ) return anion_zs , cation_zs , z_water
9459	def send_digits ( self , call_params ) : path = '/' + self . api_version + '/SendDigits/' method = 'POST' return self . request ( path , method , call_params )
12994	def table ( cluster ) : teffs = teff ( cluster ) lums = luminosity ( cluster ) arr = cluster . to_array ( ) i = 0 for row in arr : row [ 'lum' ] [ 0 ] = np . array ( [ lums [ i ] ] , dtype = 'f' ) row [ 'temp' ] [ 0 ] = np . array ( [ teffs [ i ] ] , dtype = 'f' ) i += 1 arr = round_arr_teff_luminosity ( arr ) return arr
3981	def _get_expanded_active_specs ( specs ) : _filter_active ( constants . CONFIG_BUNDLES_KEY , specs ) _filter_active ( 'apps' , specs ) _expand_libs_in_apps ( specs ) _filter_active ( 'libs' , specs ) _filter_active ( 'services' , specs ) _add_active_assets ( specs )
198	def Snowflakes ( density = ( 0.005 , 0.075 ) , density_uniformity = ( 0.3 , 0.9 ) , flake_size = ( 0.2 , 0.7 ) , flake_size_uniformity = ( 0.4 , 0.8 ) , angle = ( - 30 , 30 ) , speed = ( 0.007 , 0.03 ) , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) layer = SnowflakesLayer ( density = density , density_uniformity = density_uniformity , flake_size = flake_size , flake_size_uniformity = flake_size_uniformity , angle = angle , speed = speed , blur_sigma_fraction = ( 0.0001 , 0.001 ) ) return meta . SomeOf ( ( 1 , 3 ) , children = [ layer . deepcopy ( ) for _ in range ( 3 ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
1305	def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
7228	def paint ( self ) : snippet = { 'fill-extrusion-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-extrusion-color' : VectorStyle . get_style_value ( self . color ) , 'fill-extrusion-base' : VectorStyle . get_style_value ( self . base ) , 'fill-extrusion-height' : VectorStyle . get_style_value ( self . height ) } if self . translate : snippet [ 'fill-extrusion-translate' ] = self . translate return snippet
1698	def union ( self , other_streamlet ) : from heronpy . streamlet . impl . unionbolt import UnionStreamlet union_streamlet = UnionStreamlet ( self , other_streamlet ) self . _add_child ( union_streamlet ) other_streamlet . _add_child ( union_streamlet ) return union_streamlet
3907	def _on_event ( self , conv_event ) : conv = self . _conv_list . get ( conv_event . conversation_id ) user = conv . get_user ( conv_event . user_id ) show_notification = all ( ( isinstance ( conv_event , hangups . ChatMessageEvent ) , not user . is_self , not conv . is_quiet , ) ) if show_notification : self . add_conversation_tab ( conv_event . conversation_id ) if self . _discreet_notifications : notification = DISCREET_NOTIFICATION else : notification = notifier . Notification ( user . full_name , get_conv_name ( conv ) , conv_event . text ) self . _notifier . send ( notification )
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
8075	def rect ( self , x , y , width , height , roundness = 0.0 , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) path . rect ( x , y , width , height , roundness , self . rectmode ) if draw : path . draw ( ) return path
12721	def hi_stops ( self , hi_stops ) : _set_params ( self . ode_obj , 'HiStop' , hi_stops , self . ADOF + self . LDOF )
2967	def _sm_stop_from_no_pain ( self , * args , ** kwargs ) : _logger . info ( "Stopping chaos for blockade %s" % self . _blockade_name ) self . _timer . cancel ( )
12045	def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
6797	def get_media_timestamp ( self , last_timestamp = None ) : r = self . local_renderer _latest_timestamp = - 1e9999999999999999 for path in self . iter_static_paths ( ) : path = r . env . static_root + '/' + path self . vprint ( 'checking timestamp of path:' , path ) if not os . path . isfile ( path ) : continue _latest_timestamp = max ( _latest_timestamp , get_last_modified_timestamp ( path ) or _latest_timestamp ) if last_timestamp is not None and _latest_timestamp > last_timestamp : break self . vprint ( 'latest_timestamp:' , _latest_timestamp ) return _latest_timestamp
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
1930	def add ( self , name : str , default = None , description : str = None ) : if name in self . _vars : raise ConfigError ( f"{self.name}.{name} already defined." ) if name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default ) self . _vars [ name ] = v
1413	def _get_packing_plan_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_packing_plan_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_packing_plan ( data , stats ) : if data : packing_plan = PackingPlan ( ) packing_plan . ParseFromString ( data ) callback ( packing_plan ) else : callback ( None ) return isWatching
712	def _launchWorkers ( self , cmdLine , numWorkers ) : self . _workers = [ ] for i in range ( numWorkers ) : stdout = tempfile . NamedTemporaryFile ( delete = False ) stderr = tempfile . NamedTemporaryFile ( delete = False ) p = subprocess . Popen ( cmdLine , bufsize = 1 , env = os . environ , shell = True , stdin = None , stdout = stdout , stderr = stderr ) p . _stderr_file = stderr p . _stdout_file = stdout self . _workers . append ( p )
9258	def find_issues_to_add ( all_issues , tag_name ) : filtered = [ ] for issue in all_issues : if issue . get ( "milestone" ) : if issue [ "milestone" ] [ "title" ] == tag_name : iss = copy . deepcopy ( issue ) filtered . append ( iss ) return filtered
8351	def handle_pi ( self , text ) : if text [ : 3 ] == "xml" : text = u"xml version='1.0' encoding='%SOUP-ENCODING%'" self . _toStringSubclass ( text , ProcessingInstruction )
9600	def wait_for ( self , timeout = 10000 , interval = 1000 , asserter = lambda x : x ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for ( driver ) : asserter ( driver ) return driver return _wait_for ( self )
3749	def calculate_P ( self , T , P , method ) : r if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
5548	def validate_values ( config , values ) : if not isinstance ( config , dict ) : raise TypeError ( "config must be a dictionary" ) for value , vtype in values : if value not in config : raise ValueError ( "%s not given" % value ) if not isinstance ( config [ value ] , vtype ) : raise TypeError ( "%s must be %s" % ( value , vtype ) ) return True
7503	def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ _byteify ( item , ignore_dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore_dicts : return { _byteify ( key , ignore_dicts = True ) : _byteify ( value , ignore_dicts = True ) for key , value in data . iteritems ( ) } return data
6857	def create_user ( name , password , host = 'localhost' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , ** kwargs ) puts ( "Created MySQL user '%s'." % name )
7997	def set_peer_authenticated ( self , peer , restart_stream = False ) : with self . lock : self . peer_authenticated = True self . peer = peer if restart_stream : self . _restart_stream ( ) self . event ( AuthenticatedEvent ( self . peer ) )
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
6859	def create_database ( name , owner = None , owner_host = 'localhost' , charset = 'utf8' , collate = 'utf8_general_ci' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE DATABASE %(name)s CHARACTER SET %(charset)s COLLATE %(collate)s;" % { 'name' : name , 'charset' : charset , 'collate' : collate } , ** kwargs ) if owner : query ( "GRANT ALL PRIVILEGES ON %(name)s.* TO '%(owner)s'@'%(owner_host)s' WITH GRANT OPTION;" % { 'name' : name , 'owner' : owner , 'owner_host' : owner_host } , ** kwargs ) puts ( "Created MySQL database '%s'." % name )
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
298	def plot_turnover ( returns , transactions , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_turnover = txn . get_turnover ( positions , transactions ) df_turnover_by_month = df_turnover . resample ( "M" ) . mean ( ) df_turnover . plot ( color = 'steelblue' , alpha = 1.0 , lw = 0.5 , ax = ax , ** kwargs ) df_turnover_by_month . plot ( color = 'orangered' , alpha = 0.5 , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_turnover . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . legend ( [ 'Daily turnover' , 'Average daily turnover, by month' , 'Average daily turnover, net' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( 'Daily turnover' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylim ( ( 0 , 2 ) ) ax . set_ylabel ( 'Turnover' ) ax . set_xlabel ( '' ) return ax
7127	def find_and_patch_entry ( soup , entry ) : link = soup . find ( "a" , { "class" : "headerlink" } , href = "#" + entry . anchor ) tag = soup . new_tag ( "a" ) tag [ "name" ] = APPLE_REF_TEMPLATE . format ( entry . type , entry . name ) if link : link . parent . insert ( 0 , tag ) return True elif entry . anchor . startswith ( "module-" ) : soup . h1 . parent . insert ( 0 , tag ) return True else : return False
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
8172	def limit ( self , max = 30 ) : if abs ( self . vx ) > max : self . vx = self . vx / abs ( self . vx ) * max if abs ( self . vy ) > max : self . vy = self . vy / abs ( self . vy ) * max if abs ( self . vz ) > max : self . vz = self . vz / abs ( self . vz ) * max
12618	def get_shape ( img ) : if hasattr ( img , 'shape' ) : shape = img . shape else : shape = img . get_data ( ) . shape return shape
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
5366	def replace_print ( fileobj = sys . stderr ) : printer = _Printer ( fileobj ) previous_stdout = sys . stdout sys . stdout = printer try : yield printer finally : sys . stdout = previous_stdout
2853	def mpsse_read_gpio ( self ) : self . _write ( '\x81\x83' ) data = self . _poll_read ( 2 ) low_byte = ord ( data [ 0 ] ) high_byte = ord ( data [ 1 ] ) logger . debug ( 'Read MPSSE GPIO low byte = {0:02X} and high byte = {1:02X}' . format ( low_byte , high_byte ) ) return ( high_byte << 8 ) | low_byte
6324	def train ( self , text ) : r text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) counts = Counter ( text ) counts [ '\x00' ] = 1 tot_letters = sum ( counts . values ( ) ) tot = 0 self . _probs = { } prev = Fraction ( 0 ) for char , count in sorted ( counts . items ( ) , key = lambda x : ( x [ 1 ] , x [ 0 ] ) , reverse = True ) : follow = Fraction ( tot + count , tot_letters ) self . _probs [ char ] = ( prev , follow ) prev = follow tot = tot + count
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
2990	def cross_origin ( * args , ** kwargs ) : _options = kwargs def decorator ( f ) : LOG . debug ( "Enabling %s for cross_origin using options:%s" , f , _options ) if _options . get ( 'automatic_options' , True ) : f . required_methods = getattr ( f , 'required_methods' , set ( ) ) f . required_methods . add ( 'OPTIONS' ) f . provide_automatic_options = False def wrapped_function ( * args , ** kwargs ) : options = get_cors_options ( current_app , _options ) if options . get ( 'automatic_options' ) and request . method == 'OPTIONS' : resp = current_app . make_default_options_response ( ) else : resp = make_response ( f ( * args , ** kwargs ) ) set_cors_headers ( resp , options ) setattr ( resp , FLASK_CORS_EVALUATED , True ) return resp return update_wrapper ( wrapped_function , f ) return decorator
11598	def prepare ( self ) : attributes , elements = OrderedDict ( ) , [ ] nsmap = dict ( [ self . meta . namespace ] ) for name , item in self . _items . items ( ) : if isinstance ( item , Attribute ) : attributes [ name ] = item . prepare ( self ) elif isinstance ( item , Element ) : nsmap . update ( [ item . namespace ] ) elements . append ( item ) return attributes , elements , nsmap
9899	def _data ( self ) : if self . is_caching : return self . cache with open ( self . path , "r" ) as f : return json . load ( f )
292	def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
10204	def _parse_time ( self , tokens ) : return self . time_parser . parse ( self . parse_keyword ( Keyword . WHERE , tokens ) )
5975	def anumb_to_atom ( self , anumb ) : assert isinstance ( anumb , int ) , "anumb must be integer" if not self . _anumb_to_atom : if self . atoms : for atom in self . atoms : self . _anumb_to_atom [ atom . number ] = atom return self . _anumb_to_atom [ anumb ] else : self . logger ( "no atoms in the molecule" ) return False else : if anumb in self . _anumb_to_atom : return self . _anumb_to_atom [ anumb ] else : self . logger ( "no such atom number ({0:d}) in the molecule" . format ( anumb ) ) return False
11131	def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
1438	def update_received_packet ( self , received_pkt_size_bytes ) : self . update_count ( self . RECEIVED_PKT_COUNT ) self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes )
638	def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]
6937	def parallel_update_objectinfo_cplist ( cplist , liststartindex = None , maxobjects = None , nworkers = NCPUS , fast_mode = False , findercmap = 'gray_r' , finderconvolve = None , deredden_object = True , custom_bandpasses = None , gaia_submit_timeout = 10.0 , gaia_submit_tries = 3 , gaia_max_timeout = 180.0 , gaia_mirror = None , complete_query_later = True , lclistpkl = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , plotdpi = 100 , findercachedir = '~/.astrobase/stamp-cache' , verbose = True ) : if sys . platform == 'darwin' : import requests requests . get ( 'http://captive.apple.com/hotspot-detect.html' ) if ( liststartindex is not None ) and ( maxobjects is None ) : cplist = cplist [ liststartindex : ] elif ( liststartindex is None ) and ( maxobjects is not None ) : cplist = cplist [ : maxobjects ] elif ( liststartindex is not None ) and ( maxobjects is not None ) : cplist = ( cplist [ liststartindex : liststartindex + maxobjects ] ) tasks = [ ( x , { 'fast_mode' : fast_mode , 'findercmap' : findercmap , 'finderconvolve' : finderconvolve , 'deredden_object' : deredden_object , 'custom_bandpasses' : custom_bandpasses , 'gaia_submit_timeout' : gaia_submit_timeout , 'gaia_submit_tries' : gaia_submit_tries , 'gaia_max_timeout' : gaia_max_timeout , 'gaia_mirror' : gaia_mirror , 'complete_query_later' : complete_query_later , 'lclistpkl' : lclistpkl , 'nbrradiusarcsec' : nbrradiusarcsec , 'maxnumneighbors' : maxnumneighbors , 'plotdpi' : plotdpi , 'findercachedir' : findercachedir , 'verbose' : verbose } ) for x in cplist ] resultfutures = [ ] results = [ ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( cp_objectinfo_worker , tasks ) results = [ x for x in resultfutures ] executor . shutdown ( ) return results
8854	def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
6455	def dist_mlipns ( src , tar , threshold = 0.25 , max_mismatches = 2 ) : return MLIPNS ( ) . dist ( src , tar , threshold , max_mismatches )
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
10645	def create ( dataset , symbol , degree ) : x_vals = dataset . data [ 'T' ] . tolist ( ) y_vals = dataset . data [ symbol ] . tolist ( ) coeffs = np . polyfit ( x_vals , y_vals , degree ) result = PolynomialModelT ( dataset . material , dataset . names_dict [ symbol ] , symbol , dataset . display_symbols_dict [ symbol ] , dataset . units_dict [ symbol ] , None , [ dataset . name ] , coeffs ) result . state_schema [ 'T' ] [ 'min' ] = float ( min ( x_vals ) ) result . state_schema [ 'T' ] [ 'max' ] = float ( max ( x_vals ) ) return result
12392	def read ( self , deserialize = False , format = None ) : if deserialize : data , _ = self . deserialize ( format = format ) return data content = self . _read ( ) if not content : return '' if type ( content ) is six . binary_type : content = content . decode ( self . encoding ) return content
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
3735	def CoolProp_T_dependent_property ( T , CASRN , prop , phase ) : r if not has_CoolProp : raise Exception ( 'CoolProp library is not installed' ) if CASRN not in coolprop_dict : raise Exception ( 'CASRN not in list of supported fluids' ) Tc = coolprop_fluids [ CASRN ] . Tc T = float ( T ) if phase == 'l' : if T > Tc : raise Exception ( 'For liquid properties, must be under the critical temperature.' ) if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) in [ u'liquid' , u'supercritical_liquid' ] : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'Q' , 0 , CASRN ) elif phase == 'g' : if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) == 'gas' : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : if T < Tc : return PropsSI ( prop , 'T' , T , 'Q' , 1 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : raise Exception ( 'Error in CoolProp property function' )
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , ** kw ) : return assertion . test ( subject , expected , * args , ** kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
5020	def validate_image_size ( image ) : config = get_app_config ( ) valid_max_image_size_in_bytes = config . valid_max_image_size * 1024 if config and not image . size <= valid_max_image_size_in_bytes : raise ValidationError ( _ ( "The logo image file size must be less than or equal to %s KB." ) % config . valid_max_image_size )
274	def get_symbol_rets ( symbol , start = None , end = None ) : return SETTINGS [ 'returns_func' ] ( symbol , start = start , end = end )
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) if not self . connected : self . kill ( detach = True ) return
4656	def broadcast ( self ) : if not self . _is_signed ( ) : self . sign ( ) if "operations" not in self or not self [ "operations" ] : log . warning ( "No operations in transaction! Returning" ) return ret = self . json ( ) if self . blockchain . nobroadcast : log . warning ( "Not broadcasting anything!" ) self . clear ( ) return ret try : if self . blockchain . blocking : ret = self . blockchain . rpc . broadcast_transaction_synchronous ( ret , api = "network_broadcast" ) ret . update ( ** ret . get ( "trx" , { } ) ) else : self . blockchain . rpc . broadcast_transaction ( ret , api = "network_broadcast" ) except Exception as e : raise e finally : self . clear ( ) return ret
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
8085	def nostroke ( self ) : c = self . _canvas . strokecolor self . _canvas . strokecolor = None return c
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
5328	def __get_uuids_from_profile_name ( self , profile_name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile_name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
13036	def read_openke_translation ( filename , delimiter = '\t' , entity_first = True ) : result = { } with open ( filename , "r" ) as f : _ = next ( f ) for line in f : line_slice = line . rstrip ( ) . split ( delimiter ) if not entity_first : line_slice = list ( reversed ( line_slice ) ) result [ line_slice [ 0 ] ] = line_slice [ 1 ] return result
1105	def set_seq2 ( self , b ) : if b is self . b : return self . b = b self . matching_blocks = self . opcodes = None self . fullbcount = None self . __chain_b ( )
2229	def hash_file ( fpath , blocksize = 65536 , stride = 1 , hasher = NoParam , hashlen = NoParam , base = NoParam ) : base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) with open ( fpath , 'rb' ) as file : buf = file . read ( blocksize ) if stride > 1 : while len ( buf ) > 0 : hasher . update ( buf ) file . seek ( blocksize * ( stride - 1 ) , 1 ) buf = file . read ( blocksize ) else : while len ( buf ) > 0 : hasher . update ( buf ) buf = file . read ( blocksize ) text = _digest_hasher ( hasher , hashlen , base ) return text
11125	def rename_file ( self , relativePath , name , newName , replace = False , verbose = True ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage assert name in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' is not found in repository relative path '%s'" % ( name , relativePath ) realPath = os . path . join ( self . __path , relativePath , name ) assert os . path . isfile ( realPath ) , "file '%s' is not found in system" % realPath assert newName not in dict . __getitem__ ( dirInfoDict , "files" ) , "file '%s' already exists in repository relative path '%s'" % ( newName , relativePath ) newRealPath = os . path . join ( self . __path , relativePath , newName ) if os . path . isfile ( newRealPath ) : if replace : os . remove ( newRealPath ) if verbose : warnings . warn ( "file '%s' already exists found in system, it is now replaced by '%s' because 'replace' flag is True." % ( newRealPath , realPath ) ) else : raise Exception ( "file '%s' already exists in system but not registered in repository." % newRealPath ) os . rename ( realPath , newRealPath ) dict . __setitem__ ( dict . __getitem__ ( dirInfoDict , "files" ) , newName , dict . __getitem__ ( dirInfoDict , "files" ) . pop ( name ) ) self . save ( )
13454	def _usage ( prog_name = os . path . basename ( sys . argv [ 0 ] ) ) : spacer = ' ' * len ( 'usage: ' ) usage = prog_name + ' -b LIST [-S SEPARATOR] [file ...]\n' + spacer + prog_name + ' -c LIST [-S SEPERATOR] [file ...]\n' + spacer + prog_name + ' -f LIST [-d DELIM] [-e] [-S SEPERATOR] [-s] [file ...]' return "usage: " + usage . rstrip ( )
4629	def from_pubkey ( cls , pubkey , compressed = True , version = 56 , prefix = None ) : pubkey = PublicKey ( pubkey , prefix = prefix or Prefix . prefix ) if compressed : pubkey_plain = pubkey . compressed ( ) else : pubkey_plain = pubkey . uncompressed ( ) sha = hashlib . sha256 ( unhexlify ( pubkey_plain ) ) . hexdigest ( ) rep = hexlify ( ripemd160 ( sha ) ) . decode ( "ascii" ) s = ( "%.2x" % version ) + rep result = s + hexlify ( doublesha256 ( s ) [ : 4 ] ) . decode ( "ascii" ) result = hexlify ( ripemd160 ( result ) ) . decode ( "ascii" ) return cls ( result , prefix = pubkey . prefix )
12393	def use ( ** kwargs ) : config = dict ( use . config ) use . config . update ( kwargs ) return config
6330	def tf ( self , term ) : r if ' ' in term : raise ValueError ( 'tf can only calculate the term frequency of individual words' ) tcount = self . get_count ( term ) if tcount == 0 : return 0.0 return 1 + log10 ( tcount )
42	def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : env = WarpFrame ( env ) env = ClipRewardEnv ( env ) if frame_stack > 1 : env = FrameStack ( env , frame_stack ) if scale : env = ScaledFloatFrame ( env ) return env
11063	def push ( self , message ) : if self . _ignore_event ( message ) : return None , None args = self . _parse_message ( message ) self . log . debug ( "Searching for command using chunks: %s" , args ) cmd , msg_args = self . _find_longest_prefix_command ( args ) if cmd is not None : if message . user is None : self . log . debug ( "Discarded message with no originating user: %s" , message ) return None , None sender = message . user . username if message . channel is not None : sender = "#%s/%s" % ( message . channel . name , sender ) self . log . info ( "Received from %s: %s, args %s" , sender , cmd , msg_args ) f = self . _get_command ( cmd , message . user ) if f : if self . _is_channel_ignored ( f , message . channel ) : self . log . info ( "Channel %s is ignored, discarding command %s" , message . channel , cmd ) return '_ignored_' , "" return cmd , f . execute ( message , msg_args ) return '_unauthorized_' , "Sorry, you are not authorized to run %s" % cmd return None , None
4514	def fillCircle ( self , x0 , y0 , r , color = None ) : md . fill_circle ( self . set , x0 , y0 , r , color )
10815	def invite_by_emails ( self , emails ) : assert emails is None or isinstance ( emails , list ) results = [ ] for email in emails : try : user = User . query . filter_by ( email = email ) . one ( ) results . append ( self . invite ( user ) ) except NoResultFound : results . append ( None ) return results
2370	def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
4711	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.block.env: invalid SSH environment" ) return 1 block = cij . env_to_dict ( PREFIX , REQUIRED ) block [ "DEV_PATH" ] = "/dev/%s" % block [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , block ) return 0
10195	def register_templates ( ) : event_templates = [ current_stats . _events_config [ e ] [ 'templates' ] for e in current_stats . _events_config ] aggregation_templates = [ current_stats . _aggregations_config [ a ] [ 'templates' ] for a in current_stats . _aggregations_config ] return event_templates + aggregation_templates
5950	def check_file_exists ( self , filename , resolve = 'exception' , force = None ) : def _warn ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . warn ( msg ) warnings . warn ( msg ) return True def _raise ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . error ( msg ) raise IOError ( errno . EEXIST , x , msg ) solutions = { 'ignore' : lambda x : False , 'indicate' : lambda x : True , 'warn' : _warn , 'warning' : _warn , 'exception' : _raise , 'raise' : _raise , } if force is True : resolve = 'ignore' elif force is False : resolve = 'exception' if not os . path . isfile ( filename ) : return False else : return solutions [ resolve ] ( filename )
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
6900	def parallel_periodicfeatures_lcdir ( pfpkl_dir , lcbasedir , outdir , pfpkl_glob = 'periodfinding-*.pkl*' , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS , recursive = True , ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None fileglob = pfpkl_glob LOGINFO ( 'searching for periodfinding pickles in %s ...' % pfpkl_dir ) if recursive is False : matching = glob . glob ( os . path . join ( pfpkl_dir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( pfpkl_dir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( pfpkl_dir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s periodfinding pickles, getting periodicfeatures...' % len ( matching ) ) return parallel_periodicfeatures ( matching , lcbasedir , outdir , starfeaturesdir = starfeaturesdir , fourierorder = fourierorder , transitparams = transitparams , ebparams = ebparams , pdiff_threshold = pdiff_threshold , sidereal_threshold = sidereal_threshold , sampling_peak_multiplier = sampling_peak_multiplier , sampling_startp = sampling_startp , sampling_endp = sampling_endp , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , sigclip = sigclip , verbose = verbose , maxobjects = maxobjects , nworkers = nworkers , ) else : LOGERROR ( 'no periodfinding pickles found in %s' % ( pfpkl_dir ) ) return None
9146	def sheet ( connection , skip , file : TextIO ) : from tabulate import tabulate header = [ '' , 'Name' , 'Description' , 'Terms' , 'Relations' ] rows = [ ] for i , ( idx , name , manager ) in enumerate ( _iterate_managers ( connection , skip ) , start = 1 ) : try : if not manager . is_populated ( ) : continue except AttributeError : click . secho ( f'{name} does not implement is_populated' , fg = 'red' ) continue terms , relations = None , None if isinstance ( manager , BELNamespaceManagerMixin ) : terms = manager . _count_model ( manager . namespace_model ) if isinstance ( manager , BELManagerMixin ) : try : relations = manager . count_relations ( ) except TypeError as e : relations = str ( e ) rows . append ( ( i , name , manager . __doc__ . split ( '\n' ) [ 0 ] . strip ( ) . strip ( '.' ) , terms , relations ) ) print ( tabulate ( rows , headers = header , ) )
9223	def away_from_zero_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] >= 3 : p = 10 ** ndigits return float ( math . floor ( ( value * p ) + math . copysign ( 0.5 , value ) ) ) / p else : return round ( value , ndigits )
2884	def connect ( self , callback , * args , ** kwargs ) : if self . is_connected ( callback ) : raise AttributeError ( 'callback is already connected' ) if self . hard_subscribers is None : self . hard_subscribers = [ ] self . hard_subscribers . append ( ( callback , args , kwargs ) )
8531	def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
13028	def exploit ( self ) : search = ServiceSearch ( ) host_search = HostSearch ( ) services = search . get_services ( tags = [ 'MS17-010' ] ) services = [ service for service in services ] if len ( services ) == 0 : print_error ( "No services found that are vulnerable for MS17-010" ) return if self . auto : print_success ( "Found {} services vulnerable for MS17-010" . format ( len ( services ) ) ) for service in services : print_success ( "Exploiting " + str ( service . address ) ) host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) text = self . exploit_single ( str ( service . address ) , system_os ) print_notification ( text ) else : service_list = [ ] for service in services : host = host_search . id_to_object ( str ( service . address ) ) system_os = '' if host . os : system_os = host . os else : system_os = self . detect_os ( str ( service . address ) ) host . os = system_os host . save ( ) service_list . append ( { 'ip' : service . address , 'os' : system_os , 'string' : "{ip} ({os}) {hostname}" . format ( ip = service . address , os = system_os , hostname = host . hostname ) } ) draw_interface ( service_list , self . callback , "Exploiting {ip} with OS: {os}" )
11512	def share_item ( self , token , item_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.share' , parameters ) return response
5935	def irecarray_to_py ( a ) : pytypes = [ pyify ( typestr ) for name , typestr in a . dtype . descr ] def convert_record ( r ) : return tuple ( [ converter ( value ) for converter , value in zip ( pytypes , r ) ] ) return ( convert_record ( r ) for r in a )
10006	def clear_obj ( self , obj ) : removed = self . cellgraph . clear_obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
4952	def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
12711	def add_force ( self , force , relative = False , position = None , relative_position = None ) : b = self . ode_body if relative_position is not None : op = b . addRelForceAtRelPos if relative else b . addForceAtRelPos op ( force , relative_position ) elif position is not None : op = b . addRelForceAtPos if relative else b . addForceAtPos op ( force , position ) else : op = b . addRelForce if relative else b . addForce op ( force )
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
10041	def deposit_minter ( record_uuid , data ) : provider = DepositProvider . create ( object_type = 'rec' , object_uuid = record_uuid , pid_value = uuid . uuid4 ( ) . hex , ) data [ '_deposit' ] = { 'id' : provider . pid . pid_value , 'status' : 'draft' , } return provider . pid
2565	def async_process ( fn ) : def run ( * args , ** kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
12108	def _launch_all ( self , launchers ) : for launcher in launchers : print ( "== Launching %s ==" % launcher . batch_name ) launcher ( ) return True
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
11884	def scanAllProcessesForOpenFile ( searchPortion , isExactMatch = True , ignoreCase = False ) : pids = getAllRunningPids ( ) mappingResults = [ scanProcessForOpenFile ( pid , searchPortion , isExactMatch , ignoreCase ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if mappingResults [ i ] is not None : ret [ pids [ i ] ] = mappingResults [ i ] return ret
13270	def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
12667	def vector_to_volume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 1 : raise ValueError ( "vector must be a 1-dimensional array" ) if arr . ndim == 2 and any ( v == 1 for v in arr . shape ) : log . debug ( 'Got an array of shape {}, flattening for my purposes.' . format ( arr . shape ) ) arr = arr . flatten ( ) volume = np . zeros ( mask . shape [ : 3 ] , dtype = arr . dtype , order = order ) volume [ mask ] = arr return volume
7351	def parse_netchop ( netchop_output ) : line_iterator = iter ( netchop_output . decode ( ) . split ( "\n" ) ) scores = [ ] for line in line_iterator : if "pos" in line and 'AA' in line and 'score' in line : scores . append ( [ ] ) if "----" not in next ( line_iterator ) : raise ValueError ( "Dashes expected" ) line = next ( line_iterator ) while '-------' not in line : score = float ( line . split ( ) [ 3 ] ) scores [ - 1 ] . append ( score ) line = next ( line_iterator ) return scores
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
12437	def stream ( cls , response , sequence ) : iterator = iter ( sequence ) data = { 'chunk' : next ( iterator ) } response . streaming = True def streamer ( ) : while True : if response . asynchronous : yield data [ 'chunk' ] else : response . send ( data [ 'chunk' ] ) yield response . body response . body = None try : data [ 'chunk' ] = next ( iterator ) except StopIteration : break if not response . asynchronous : response . close ( ) return streamer ( )
1601	def chain ( cmd_list ) : command = ' | ' . join ( map ( lambda x : ' ' . join ( x ) , cmd_list ) ) chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) stdout_builder = proc . async_stdout_builder ( chained_proc ) chained_proc . wait ( ) return { 'command' : command , 'stdout' : stdout_builder . result ( ) }
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
2623	def shut_down_instance ( self , instances = None ) : if instances and len ( self . instances ) > 0 : print ( instances ) try : print ( [ i . id for i in instances ] ) except Exception as e : print ( e ) term = self . client . terminate_instances ( InstanceIds = instances ) logger . info ( "Shut down {} instances (ids:{}" . format ( len ( instances ) , str ( instances ) ) ) elif len ( self . instances ) > 0 : instance = self . instances . pop ( ) term = self . client . terminate_instances ( InstanceIds = [ instance ] ) logger . info ( "Shut down 1 instance (id:{})" . format ( instance ) ) else : logger . warn ( "No Instances to shut down.\n" ) return - 1 self . get_instance_state ( ) return term
10201	def register_events ( ) : return [ dict ( event_type = 'file-download' , templates = 'invenio_stats.contrib.file_download' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_file_unique_id ] ) ) , dict ( event_type = 'record-view' , templates = 'invenio_stats.contrib.record_view' , processor_class = EventsIndexer , processor_config = dict ( preprocessors = [ flag_robots , anonymize_user , build_record_unique_id ] ) ) ]
6844	def create_supervisor_services ( self , site ) : self . vprint ( 'create_supervisor_services:' , site ) self . set_site_specifics ( site = site ) r = self . local_renderer if self . verbose : print ( 'r.env:' ) pprint ( r . env , indent = 4 ) self . vprint ( 'r.env.has_worker:' , r . env . has_worker ) if not r . env . has_worker : self . vprint ( 'skipping: no celery worker' ) return if self . name . lower ( ) not in self . genv . services : self . vprint ( 'skipping: celery not enabled' ) return hostname = self . current_hostname target_sites = self . genv . available_sites_by_host . get ( hostname , None ) if target_sites and site not in target_sites : self . vprint ( 'skipping: site not supported on this server' ) return self . render_paths ( ) conf_name = 'celery_%s.conf' % site ret = r . render_to_string ( 'celery/celery_supervisor.template.conf' ) return conf_name , ret
8850	def setup_editor ( self , editor ) : editor . cursorPositionChanged . connect ( self . on_cursor_pos_changed ) try : m = editor . modes . get ( modes . GoToAssignmentsMode ) except KeyError : pass else : assert isinstance ( m , modes . GoToAssignmentsMode ) m . out_of_doc . connect ( self . on_goto_out_of_doc )
3281	def compute_digest_response ( self , realm , user_name , method , uri , nonce , cnonce , qop , nc , environ ) : def md5h ( data ) : return md5 ( compat . to_bytes ( data ) ) . hexdigest ( ) def md5kd ( secret , data ) : return md5h ( secret + ":" + data ) A1 = self . domain_controller . digest_auth_user ( realm , user_name , environ ) if not A1 : return False A2 = method + ":" + uri if qop : res = md5kd ( A1 , nonce + ":" + nc + ":" + cnonce + ":" + qop + ":" + md5h ( A2 ) ) else : res = md5kd ( A1 , nonce + ":" + md5h ( A2 ) ) return res
10119	def circle ( cls , center , radius , n_vertices = 50 , ** kwargs ) : return cls . regular_polygon ( center , radius , n_vertices , ** kwargs )
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
4181	def window_nuttall ( N ) : r a0 = 0.355768 a1 = 0.487396 a2 = 0.144232 a3 = 0.012604 return _coeff4 ( N , a0 , a1 , a2 , a3 )
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
5361	def __execute_initial_load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks_cls = [ TaskPanels , TaskPanelsMenu ] self . execute_tasks ( tasks_cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks_cls = [ TaskInitSortingHat ] self . execute_tasks ( tasks_cls ) logger . info ( "Loading projects" ) tasks_cls = [ TaskProjects ] self . execute_tasks ( tasks_cls ) logger . info ( "Done" ) return
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
11739	def move_dot ( self ) : return self . __class__ ( self . production , self . pos + 1 , self . lookahead )
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
10911	def get_num_px_jtj ( s , nparams , decimate = 1 , max_mem = 1e9 , min_redundant = 20 ) : px_mem = int ( max_mem // 8 // nparams ) px_red = min_redundant * nparams px_dec = s . residuals . size // decimate if px_red > px_mem : raise RuntimeError ( 'Insufficient max_mem for desired redundancy.' ) num_px = np . clip ( px_dec , px_red , px_mem ) . astype ( 'int' ) return num_px
1834	def JECXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ECX == 0 , target . read ( ) , cpu . PC )
6059	def numpy_array_2d_to_fits ( array_2d , file_path , overwrite = False ) : if overwrite and os . path . exists ( file_path ) : os . remove ( file_path ) new_hdr = fits . Header ( ) hdu = fits . PrimaryHDU ( np . flipud ( array_2d ) , new_hdr ) hdu . writeto ( file_path )
13456	def open_s3 ( bucket ) : conn = boto . connect_s3 ( options . paved . s3 . access_id , options . paved . s3 . secret ) try : bucket = conn . get_bucket ( bucket ) except boto . exception . S3ResponseError : bucket = conn . create_bucket ( bucket ) return bucket
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
5615	def read_vector_window ( input_files , tile , validity_check = True ) : if not isinstance ( input_files , list ) : input_files = [ input_files ] return [ feature for feature in chain . from_iterable ( [ _read_vector_window ( path , tile , validity_check = validity_check ) for path in input_files ] ) ]
345	def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) data = data . reshape ( shape ) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) return data logging . info ( "Load or Download {0} > {1}" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test
661	def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : if not sparseForm : outputs = outputs . reshape ( outputsShape ) spOut = SM32 ( outputs ) else : if len ( outputs ) > 0 : assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) regionSize = 15 rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) regionSums = spOut . nNonZerosPerBox ( rows , cols ) ( locations , values ) = regionSums . tolist ( ) values /= float ( regionSize * regionSize ) sat = list ( values ) innerSat = [ ] locationSet = set ( locations ) for ( location , value ) in itertools . izip ( locations , values ) : ( row , col ) = location if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : innerSat . append ( value ) return ( sat , innerSat )
6784	def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
11329	def main ( ) : argparser = ArgumentParser ( ) subparsers = argparser . add_subparsers ( dest = 'selected_subparser' ) all_parser = subparsers . add_parser ( 'all' ) elsevier_parser = subparsers . add_parser ( 'elsevier' ) oxford_parser = subparsers . add_parser ( 'oxford' ) springer_parser = subparsers . add_parser ( 'springer' ) all_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--run-locally' , action = 'store_true' ) elsevier_parser . add_argument ( '--package-name' ) elsevier_parser . add_argument ( '--path' ) elsevier_parser . add_argument ( '--CONSYN' , action = 'store_true' ) elsevier_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--extract-nations' , action = 'store_true' ) oxford_parser . add_argument ( '--dont-empty-ftp' , action = 'store_true' ) oxford_parser . add_argument ( '--package-name' ) oxford_parser . add_argument ( '--path' ) oxford_parser . add_argument ( '--update-credentials' , action = 'store_true' ) oxford_parser . add_argument ( '--extract-nations' , action = 'store_true' ) springer_parser . add_argument ( '--package-name' ) springer_parser . add_argument ( '--path' ) springer_parser . add_argument ( '--update-credentials' , action = 'store_true' ) springer_parser . add_argument ( '--extract-nations' , action = 'store_true' ) settings = Bunch ( vars ( argparser . parse_args ( ) ) ) call_package ( settings )
295	def plot_max_median_position_concentration ( positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) alloc_summary = pos . get_max_median_position_concentration ( positions ) colors = [ 'mediumblue' , 'steelblue' , 'tomato' , 'firebrick' ] alloc_summary . plot ( linewidth = 1 , color = colors , alpha = 0.6 , ax = ax ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Exposure' ) ax . set_title ( 'Long/short max and median position concentration' ) return ax
8728	def strftime ( fmt , t ) : if isinstance ( t , ( time . struct_time , tuple ) ) : t = datetime . datetime ( * t [ : 6 ] ) assert isinstance ( t , ( datetime . datetime , datetime . time , datetime . date ) ) try : year = t . year if year < 1900 : t = t . replace ( year = 1900 ) except AttributeError : year = 1900 subs = ( ( '%Y' , '%04d' % year ) , ( '%y' , '%02d' % ( year % 100 ) ) , ( '%s' , '%03d' % ( t . microsecond // 1000 ) ) , ( '%u' , '%03d' % ( t . microsecond % 1000 ) ) ) def doSub ( s , sub ) : return s . replace ( * sub ) def doSubs ( s ) : return functools . reduce ( doSub , subs , s ) fmt = '%%' . join ( map ( doSubs , fmt . split ( '%%' ) ) ) return t . strftime ( fmt )
5924	def setup ( filename = CONFIGNAME ) : get_configuration ( ) if not os . path . exists ( filename ) : with open ( filename , 'w' ) as configfile : cfg . write ( configfile ) msg = "NOTE: GromacsWrapper created the configuration file \n\t%r\n" " for you. Edit the file to customize the package." % filename print ( msg ) for d in config_directories : utilities . mkdir_p ( d )
6463	def usage_function ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available functions:' ) for function in sorted ( FUNCTION ) : doc = FUNCTION [ function ] . __doc__ . strip ( ) . splitlines ( ) [ 0 ] print ( ' %-12s %s' % ( function + ':' , doc ) ) return 0
13218	def connection_url ( self , name = None ) : return 'postgresql://{user}@{host}:{port}/{dbname}' . format ( ** { k : v for k , v in self . _connect_options ( name ) } )
1065	def getfirstmatchingheader ( self , name ) : name = name . lower ( ) + ':' n = len ( name ) lst = [ ] hit = 0 for line in self . headers : if hit : if not line [ : 1 ] . isspace ( ) : break elif line [ : n ] . lower ( ) == name : hit = 1 if hit : lst . append ( line ) return lst
8337	def findParent ( self , name = None , attrs = { } , ** kwargs ) : r = None l = self . findParents ( name , attrs , 1 ) if l : r = l [ 0 ] return r
369	def crop_multi ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) results = [ ] for data in x : results . append ( data [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] ) return np . asarray ( results ) else : h_offset = ( h - hrg ) / 2 w_offset = ( w - wrg ) / 2 results = [ ] for data in x : results . append ( data [ h_offset : h - h_offset , w_offset : w - w_offset ] ) return np . asarray ( results )
3297	def is_collection ( self , path , environ ) : res = self . get_resource_inst ( path , environ ) return res and res . is_collection
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
11369	def convert_date_to_iso ( value ) : date_formats = [ "%d %b %Y" , "%Y/%m/%d" ] for dformat in date_formats : try : date = datetime . strptime ( value , dformat ) return date . strftime ( "%Y-%m-%d" ) except ValueError : pass return value
12081	def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
5796	def handle_sec_error ( error , exception_class = None ) : if error == 0 : return if error in set ( [ SecurityConst . errSSLClosedNoNotify , SecurityConst . errSSLClosedAbort ] ) : raise TLSDisconnectError ( 'The remote end closed the connection' ) if error == SecurityConst . errSSLClosedGraceful : raise TLSGracefulDisconnectError ( 'The remote end closed the connection' ) cf_error_string = Security . SecCopyErrorMessageString ( error , null ( ) ) output = CFHelpers . cf_string_to_unicode ( cf_error_string ) CoreFoundation . CFRelease ( cf_error_string ) if output is None or output == '' : output = 'OSStatus %s' % error if exception_class is None : exception_class = OSError raise exception_class ( output )
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
8999	def _dump_knitting_pattern ( self , file ) : knitting_pattern_set = self . __on_dump ( ) knitting_pattern = knitting_pattern_set . patterns . at ( 0 ) layout = GridLayout ( knitting_pattern ) builder = AYABPNGBuilder ( * layout . bounding_box ) builder . set_colors_in_grid ( layout . walk_instructions ( ) ) builder . write_to_file ( file )
9136	def get_modules ( ) -> Mapping : modules = { } for entry_point in iter_entry_points ( group = 'bio2bel' , name = None ) : entry = entry_point . name try : modules [ entry ] = entry_point . load ( ) except VersionConflict as exc : log . warning ( 'Version conflict in %s: %s' , entry , exc ) continue except UnknownExtra as exc : log . warning ( 'Unknown extra in %s: %s' , entry , exc ) continue except ImportError as exc : log . exception ( 'Issue with importing module %s: %s' , entry , exc ) continue return modules
10138	def nearest ( self , ver ) : if not isinstance ( ver , Version ) : ver = Version ( ver ) if ver in OFFICIAL_VERSIONS : return ver versions = list ( OFFICIAL_VERSIONS ) versions . sort ( reverse = True ) best = None for candidate in versions : if candidate == ver : return candidate if ( best is None ) and ( candidate < ver ) : warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (older) version supported is %s.' % ( ver , candidate ) ) return candidate if candidate > ver : best = candidate assert best is not None warnings . warn ( 'This version of hszinc does not yet ' 'support version %s, please seek a newer version ' 'or file a bug. Closest (newer) version supported is %s.' % ( ver , best ) ) return best
7903	def set_handlers ( self , priority = 10 ) : self . stream . set_message_handler ( "groupchat" , self . __groupchat_message , None , priority ) self . stream . set_message_handler ( "error" , self . __error_message , None , priority ) self . stream . set_presence_handler ( "available" , self . __presence_available , None , priority ) self . stream . set_presence_handler ( "unavailable" , self . __presence_unavailable , None , priority ) self . stream . set_presence_handler ( "error" , self . __presence_error , None , priority )
1339	def batch_crossentropy ( label , logits ) : assert logits . ndim == 2 logits = logits - np . max ( logits , axis = 1 , keepdims = True ) e = np . exp ( logits ) s = np . sum ( e , axis = 1 ) ces = np . log ( s ) - logits [ : , label ] return ces
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
9229	def fetch_closed_pull_requests ( self ) : pull_requests = [ ] verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching closed pull requests..." ) page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) if self . options . release_branch : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , base = self . options . release_branch ) else : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , ) if rc == 200 : pull_requests . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if verbose > 1 : print ( "\tfetched {} closed pull requests." . format ( len ( pull_requests ) ) ) return pull_requests
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
3941	async def _fetch_channel_sid ( self ) : logger . info ( 'Requesting new gsessionid and SID...' ) self . _sid_param = None self . _gsessionid_param = None res = await self . send_maps ( [ ] ) self . _sid_param , self . _gsessionid_param = _parse_sid_response ( res . body ) logger . info ( 'New SID: {}' . format ( self . _sid_param ) ) logger . info ( 'New gsessionid: {}' . format ( self . _gsessionid_param ) )
3887	def add_observer ( self , callback ) : if callback in self . _observers : raise ValueError ( '{} is already an observer of {}' . format ( callback , self ) ) self . _observers . append ( callback )
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
11665	def _build_indices ( X , flann_args ) : "Builds FLANN indices for each bag." logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( ** flann_args ) idx . build_index ( bag ) return indices
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
3677	def rdkitmol_Hs ( self ) : r if self . __rdkitmol_Hs : return self . __rdkitmol_Hs else : try : self . __rdkitmol_Hs = Chem . AddHs ( self . rdkitmol ) return self . __rdkitmol_Hs except : return None
650	def generateSequences ( nPatterns = 10 , patternLen = 500 , patternActivity = 50 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSimpleSequences = 50 , nHubSequences = 50 ) : patterns = generateCoincMatrix ( nCoinc = nPatterns , length = patternLen , activity = patternActivity ) seqList = generateSimpleSequences ( nCoinc = nPatterns , seqLength = seqLength , nSeq = nSimpleSequences ) + generateHubSequences ( nCoinc = nPatterns , hubs = hubs , seqLength = seqLength , nSeq = nHubSequences ) return ( seqList , patterns )
8525	def log_callback ( wrapped_function ) : def debug_log ( message ) : logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , ** kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , ** kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper
12173	def htmlABF ( ID , group , d , folder , overwrite = False ) : fname = folder + "/swhlab4/%s_index.html" % ID if overwrite is False and os . path . exists ( fname ) : return html = TEMPLATES [ 'abf' ] html = html . replace ( "~ID~" , ID ) html = html . replace ( "~CONTENT~" , htmlABFcontent ( ID , group , d ) ) print ( " <- writing [%s]" % os . path . basename ( fname ) ) with open ( fname , 'w' ) as f : f . write ( html ) return
11093	def n_file ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_file ( recursive = True ) : n += 1 return n
3386	def _random_point ( self ) : idx = np . random . randint ( self . n_warmup , size = min ( 2 , np . ceil ( np . sqrt ( self . n_warmup ) ) ) ) return self . warmup [ idx , : ] . mean ( axis = 0 )
12860	def add_period ( self , p , holiday_obj = None ) : if isinstance ( p , ( list , tuple ) ) : return [ BusinessDate . add_period ( self , pd ) for pd in p ] elif isinstance ( p , str ) : period = BusinessPeriod ( p ) else : period = p res = self res = BusinessDate . add_months ( res , period . months ) res = BusinessDate . add_years ( res , period . years ) res = BusinessDate . add_days ( res , period . days ) if period . businessdays : if holiday_obj : res = BusinessDate . add_business_days ( res , period . businessdays , holiday_obj ) else : res = BusinessDate . add_business_days ( res , period . businessdays , period . holiday ) return res
6281	def keyboard_event ( self , key , action , modifier ) : if key == self . keys . ESCAPE : self . close ( ) return if key == self . keys . SPACE and action == self . keys . ACTION_PRESS : self . timer . toggle_pause ( ) if key == self . keys . D : if action == self . keys . ACTION_PRESS : self . sys_camera . move_right ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_right ( False ) elif key == self . keys . A : if action == self . keys . ACTION_PRESS : self . sys_camera . move_left ( True ) elif action == self . keys . ACTION_RELEASE : self . sys_camera . move_left ( False ) elif key == self . keys . W : if action == self . keys . ACTION_PRESS : self . sys_camera . move_forward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_forward ( False ) elif key == self . keys . S : if action == self . keys . ACTION_PRESS : self . sys_camera . move_backward ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_backward ( False ) elif key == self . keys . Q : if action == self . keys . ACTION_PRESS : self . sys_camera . move_down ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_down ( False ) elif key == self . keys . E : if action == self . keys . ACTION_PRESS : self . sys_camera . move_up ( True ) if action == self . keys . ACTION_RELEASE : self . sys_camera . move_up ( False ) if key == self . keys . X and action == self . keys . ACTION_PRESS : screenshot . create ( ) if key == self . keys . R and action == self . keys . ACTION_PRESS : project . instance . reload_programs ( ) if key == self . keys . RIGHT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) + 10.0 ) if key == self . keys . LEFT and action == self . keys . ACTION_PRESS : self . timer . set_time ( self . timer . get_time ( ) - 10.0 ) self . timeline . key_event ( key , action , modifier )
564	def toDict ( self ) : def items2dict ( items ) : d = { } for k , v in items . items ( ) : d [ k ] = v . __dict__ return d self . invariant ( ) return dict ( description = self . description , singleNodeOnly = self . singleNodeOnly , inputs = items2dict ( self . inputs ) , outputs = items2dict ( self . outputs ) , parameters = items2dict ( self . parameters ) , commands = items2dict ( self . commands ) )
12556	def save_varlist ( filename , varnames , varlist ) : variables = { } for i , vn in enumerate ( varnames ) : variables [ vn ] = varlist [ i ] ExportData . save_variables ( filename , variables )
9537	def match_pattern ( regex ) : prog = re . compile ( regex ) def checker ( v ) : result = prog . match ( v ) if result is None : raise ValueError ( v ) return checker
13027	def detect_os ( self , ip ) : process = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'checker.py' ) , str ( ip ) ] , stdout = subprocess . PIPE ) out = process . stdout . decode ( 'utf-8' ) . split ( '\n' ) system_os = '' for line in out : if line . startswith ( 'Target OS:' ) : system_os = line . replace ( 'Target OS: ' , '' ) break return system_os
13845	def get_unique_pathname ( path , root = '' ) : path = os . path . join ( root , path ) potentialPaths = itertools . chain ( ( path , ) , __get_numbered_paths ( path ) ) potentialPaths = six . moves . filterfalse ( os . path . exists , potentialPaths ) return next ( potentialPaths )
9454	def play_stop ( self , call_params ) : path = '/' + self . api_version + '/PlayStop/' method = 'POST' return self . request ( path , method , call_params )
5562	def effective_bounds ( self ) : return snap_bounds ( bounds = clip_bounds ( bounds = self . init_bounds , clip = self . process_pyramid . bounds ) , pyramid = self . process_pyramid , zoom = min ( self . baselevels [ "zooms" ] ) if self . baselevels else min ( self . init_zoom_levels ) )
9779	def whoami ( ) : try : user = PolyaxonClient ( ) . auth . get_user ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) click . echo ( "\nUsername: {username}, Email: {email}\n" . format ( ** user . to_dict ( ) ) )
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
2668	def sixteen_oscillator_two_stimulated_ensembles_grid ( ) : "Not accurate false due to spikes are observed" parameters = legion_parameters ( ) parameters . teta_x = - 1.1 template_dynamic_legion ( 16 , 2000 , 1500 , conn_type = conn_type . GRID_FOUR , params = parameters , stimulus = [ 1 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
13816	def Parse ( text , message ) : if not isinstance ( text , six . text_type ) : text = text . decode ( 'utf-8' ) try : if sys . version_info < ( 2 , 7 ) : js = json . loads ( text ) else : js = json . loads ( text , object_pairs_hook = _DuplicateChecker ) except ValueError as e : raise ParseError ( 'Failed to load JSON: {0}.' . format ( str ( e ) ) ) _ConvertMessage ( js , message ) return message
6832	def ssh_config ( self , name = '' ) : r = self . local_renderer with self . settings ( hide ( 'running' ) ) : output = r . local ( 'vagrant ssh-config %s' % name , capture = True ) config = { } for line in output . splitlines ( ) [ 1 : ] : key , value = line . strip ( ) . split ( ' ' , 2 ) config [ key ] = value return config
12444	def route ( self , request , response ) : self . require_http_allowed_method ( request ) function = getattr ( self , request . method . lower ( ) , None ) if function is None : raise http . exceptions . NotImplemented ( ) return function ( request , response )
7987	def request_software_version ( stanza_processor , target_jid , callback , error_callback = None ) : stanza = Iq ( to_jid = target_jid , stanza_type = "get" ) payload = VersionPayload ( ) stanza . set_payload ( payload ) def wrapper ( stanza ) : payload = stanza . get_payload ( VersionPayload ) if payload is None : if error_callback : error_callback ( stanza ) else : logger . warning ( "Invalid version query response." ) else : callback ( payload ) stanza_processor . set_response_handlers ( stanza , wrapper , error_callback ) stanza_processor . send ( stanza )
1568	def invoke_hook_bolt_ack ( self , heron_tuple , process_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_ack_info = BoltAckInfo ( heron_tuple = heron_tuple , acking_task_id = self . get_task_id ( ) , process_latency_ms = process_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_ack ( bolt_ack_info )
975	def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : self . _createBucket ( index - 1 ) self . _createBucket ( index )
5549	def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
6456	def sim ( src , tar , method = sim_levenshtein ) : if callable ( method ) : return method ( src , tar ) else : raise AttributeError ( 'Unknown similarity function: ' + str ( method ) )
12299	def register ( self , what , obj ) : name = obj . name version = obj . version enable = obj . enable if enable == 'n' : return key = Key ( name , version ) self . plugins [ what ] [ key ] = obj
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( ** mpl_kwargs ) lines = LineCollection ( ** line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( ** scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( ** animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( ** save_args )
6206	def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
2869	def setup ( self , pin , mode , pull_up_down = PUD_OFF ) : self . rpi_gpio . setup ( pin , self . _dir_mapping [ mode ] , pull_up_down = self . _pud_mapping [ pull_up_down ] )
9901	def _updateType ( self ) : data = self . _data ( ) if isinstance ( data , dict ) and isinstance ( self , ListFile ) : self . __class__ = DictFile elif isinstance ( data , list ) and isinstance ( self , DictFile ) : self . __class__ = ListFile
9475	def add_node ( self , label ) : try : n = self . _nodes [ label ] except KeyError : n = Node ( ) n [ 'label' ] = label self . _nodes [ label ] = n return n
12996	def main ( ) : services = ServiceSearch ( ) argparse = services . argparser argparse . add_argument ( '-f' , '--file' , type = str , help = "File" ) arguments = argparse . parse_args ( ) if not arguments . file : print_error ( "Please provide a file with credentials seperated by ':'" ) sys . exit ( ) services = services . get_services ( search = [ "Tomcat" ] , up = True , tags = [ '!tomcat_brute' ] ) credentials = [ ] with open ( arguments . file , 'r' ) as f : credentials = f . readlines ( ) for service in services : print_notification ( "Checking ip:{} port {}" . format ( service . address , service . port ) ) url = 'http://{}:{}/manager/html' gevent . spawn ( brutefore_passwords , service . address , url . format ( service . address , service . port ) , credentials , service ) service . add_tag ( 'tomcat_brute' ) service . update ( tags = service . tags ) gevent . wait ( ) Logger ( ) . log ( "tomcat_brute" , "Performed tomcat bruteforce scan" , { 'scanned_services' : len ( services ) } )
2541	def set_pkg_desc ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_desc_set : self . package_desc_set = True doc . package . description = text else : raise CardinalityError ( 'Package::Description' )
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
2597	def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
61	def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
4863	def to_representation ( self , instance ) : request = self . context [ 'request' ] enterprise_customer = instance . enterprise_customer representation = super ( EnterpriseCustomerCatalogDetailSerializer , self ) . to_representation ( instance ) paginated_content = instance . get_paginated_content ( request . GET ) count = paginated_content [ 'count' ] search_results = paginated_content [ 'results' ] for item in search_results : content_type = item [ 'content_type' ] marketing_url = item . get ( 'marketing_url' ) if marketing_url : item [ 'marketing_url' ] = utils . update_query_parameters ( marketing_url , utils . get_enterprise_utm_context ( enterprise_customer ) ) if content_type == 'course' : item [ 'enrollment_url' ] = instance . get_course_enrollment_url ( item [ 'key' ] ) if content_type == 'courserun' : item [ 'enrollment_url' ] = instance . get_course_run_enrollment_url ( item [ 'key' ] ) if content_type == 'program' : item [ 'enrollment_url' ] = instance . get_program_enrollment_url ( item [ 'uuid' ] ) previous_url = None next_url = None page = int ( request . GET . get ( 'page' , '1' ) ) request_uri = request . build_absolute_uri ( ) if paginated_content [ 'previous' ] : previous_url = utils . update_query_parameters ( request_uri , { 'page' : page - 1 } ) if paginated_content [ 'next' ] : next_url = utils . update_query_parameters ( request_uri , { 'page' : page + 1 } ) representation [ 'count' ] = count representation [ 'previous' ] = previous_url representation [ 'next' ] = next_url representation [ 'results' ] = search_results return representation
5015	def filter_queryset ( self , request , queryset , view ) : if request . user . is_staff : email = request . query_params . get ( 'email' , None ) username = request . query_params . get ( 'username' , None ) query_parameters = { } if email : query_parameters . update ( email = email ) if username : query_parameters . update ( username = username ) if query_parameters : users = User . objects . filter ( ** query_parameters ) . values_list ( 'id' , flat = True ) queryset = queryset . filter ( user_id__in = users ) else : queryset = queryset . filter ( user_id = request . user . id ) return queryset
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
8415	def min_max ( x , na_rm = False , finite = True ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if na_rm and finite : x = x [ np . isfinite ( x ) ] elif not na_rm and np . any ( np . isnan ( x ) ) : return np . nan , np . nan elif na_rm : x = x [ ~ np . isnan ( x ) ] elif finite : x = x [ ~ np . isinf ( x ) ] if ( len ( x ) ) : return np . min ( x ) , np . max ( x ) else : return float ( '-inf' ) , float ( 'inf' )
7013	def read_hatpi_pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except UnicodeDecodeError : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
4737	def good ( txt ) : print ( "%s# %s%s%s" % ( PR_GOOD_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
1870	def MOVSX ( cpu , op0 , op1 ) : op0 . write ( Operators . SEXTEND ( op1 . read ( ) , op1 . size , op0 . size ) )
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
4506	def find_serial_devices ( self ) : if self . devices is not None : return self . devices self . devices = { } hardware_id = "(?i)" + self . hardware_id for ports in serial . tools . list_ports . grep ( hardware_id ) : port = ports [ 0 ] try : id = self . get_device_id ( port ) ver = self . _get_device_version ( port ) except : log . debug ( 'Error getting device_id for %s, %s' , port , self . baudrate ) if True : raise continue if getattr ( ports , '__len__' , lambda : 0 ) ( ) : log . debug ( 'Multi-port device %s:%s:%s with %s ports found' , self . hardware_id , id , ver , len ( ports ) ) if id < 0 : log . debug ( 'Serial device %s:%s:%s with id %s < 0' , self . hardware_id , id , ver ) else : self . devices [ id ] = port , ver return self . devices
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
365	def affine_transform_keypoints ( coords_list , transform_matrix ) : coords_result_list = [ ] for coords in coords_list : coords = np . asarray ( coords ) coords = coords . transpose ( [ 1 , 0 ] ) coords = np . insert ( coords , 2 , 1 , axis = 0 ) coords_result = np . matmul ( transform_matrix , coords ) coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) coords_result_list . append ( coords_result ) return coords_result_list
9380	def get_standardized_timestamp ( timestamp , ts_format ) : if not timestamp : return None if timestamp == 'now' : timestamp = str ( datetime . datetime . now ( ) ) if not ts_format : ts_format = detect_timestamp_format ( timestamp ) try : if ts_format == 'unknown' : logger . error ( 'Unable to determine timestamp format for : %s' , timestamp ) return - 1 elif ts_format == 'epoch' : ts = int ( timestamp ) * 1000 elif ts_format == 'epoch_ms' : ts = timestamp elif ts_format == 'epoch_fraction' : ts = int ( timestamp [ : 10 ] ) * 1000 + int ( timestamp [ 11 : ] ) elif ts_format in ( '%H:%M:%S' , '%H:%M:%S.%f' ) : date_today = str ( datetime . date . today ( ) ) dt_obj = datetime . datetime . strptime ( date_today + ' ' + timestamp , '%Y-%m-%d ' + ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 else : dt_obj = datetime . datetime . strptime ( timestamp , ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 except ValueError : return - 1 return str ( ts )
4037	def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : if req . status_code == 429 : delay = backoff . delay if delay > 32 : backoff . reset ( ) raise ze . TooManyRetries ( "Continuing to receive HTTP 429 \responses after 62 seconds. You are being rate-limited, try again later" ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
9176	def with_db_cursor ( func ) : @ functools . wraps ( func ) def wrapped ( * args , ** kwargs ) : if 'cursor' in kwargs or func . func_code . co_argcount == len ( args ) : return func ( * args , ** kwargs ) with db_connect ( ) as db_connection : with db_connection . cursor ( ) as cursor : kwargs [ 'cursor' ] = cursor return func ( * args , ** kwargs ) return wrapped
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : def gen_groups ( items ) : j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) if len ( groups ) == 1 : return False normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian_distance < normal_distance else : return False
8760	def get_subnets_count ( context , filters = None ) : LOG . info ( "get_subnets_count for tenant %s with filters %s" % ( context . tenant_id , filters ) ) return db_api . subnet_count_all ( context , ** filters )
4838	def get_course_and_course_run ( self , course_run_id ) : course_id = parse_course_key ( course_run_id ) course = self . get_course_details ( course_id ) course_run = None if course : course_run = None course_runs = [ course_run for course_run in course [ 'course_runs' ] if course_run [ 'key' ] == course_run_id ] if course_runs : course_run = course_runs [ 0 ] return course , course_run
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
5426	def _group_tasks_by_jobid ( tasks ) : ret = collections . defaultdict ( list ) for t in tasks : ret [ t . get_field ( 'job-id' ) ] . append ( t ) return ret
10357	def random_by_edges ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 edges = graph . edges ( keys = True ) n = int ( graph . number_of_edges ( ) * percentage ) subedges = random . sample ( edges , n ) rv = graph . fresh_copy ( ) for u , v , k in subedges : safe_add_edge ( rv , u , v , k , graph [ u ] [ v ] [ k ] ) update_node_helper ( graph , rv ) return rv
7505	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : LOGGER . error ( res ) raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp ) as intree : tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . _renamer ( tmp ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) self . _save ( )
10305	def calculate_single_tanimoto_set_distances ( target : Iterable [ X ] , dict_of_sets : Mapping [ Y , Set [ X ] ] ) -> Mapping [ Y , float ] : target_set = set ( target ) return { k : tanimoto_set_similarity ( target_set , s ) for k , s in dict_of_sets . items ( ) }
4092	def addSearchers ( self , * searchers ) : self . _searchers . extend ( searchers ) debug . logger & debug . flagCompiler and debug . logger ( 'current compiled MIBs location(s): %s' % ', ' . join ( [ str ( x ) for x in self . _searchers ] ) ) return self
4397	def adsSyncAddDeviceNotificationReqEx ( port , adr , data_name , pNoteAttrib , callback , user_handle = None ) : global callback_store if NOTEFUNC is None : raise TypeError ( "Callback function type can't be None" ) adsSyncAddDeviceNotificationReqFct = _adsDLL . AdsSyncAddDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) hnl = adsSyncReadWriteReqEx2 ( port , adr , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING ) nIndexGroup = ctypes . c_ulong ( ADSIGRP_SYM_VALBYHND ) nIndexOffset = ctypes . c_ulong ( hnl ) attrib = pNoteAttrib . notificationAttribStruct ( ) pNotification = ctypes . c_ulong ( ) nHUser = ctypes . c_ulong ( hnl ) if user_handle is not None : nHUser = ctypes . c_ulong ( user_handle ) adsSyncAddDeviceNotificationReqFct . argtypes = [ ctypes . c_ulong , ctypes . POINTER ( SAmsAddr ) , ctypes . c_ulong , ctypes . c_ulong , ctypes . POINTER ( SAdsNotificationAttrib ) , NOTEFUNC , ctypes . c_ulong , ctypes . POINTER ( ctypes . c_ulong ) , ] adsSyncAddDeviceNotificationReqFct . restype = ctypes . c_long def wrapper ( addr , notification , user ) : return callback ( notification , data_name ) c_callback = NOTEFUNC ( wrapper ) err_code = adsSyncAddDeviceNotificationReqFct ( port , pAmsAddr , nIndexGroup , nIndexOffset , ctypes . byref ( attrib ) , c_callback , nHUser , ctypes . byref ( pNotification ) , ) if err_code : raise ADSError ( err_code ) callback_store [ pNotification . value ] = c_callback return ( pNotification . value , hnl )
9194	def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
12750	def set_pid_params ( self , * args , ** kwargs ) : for joint in self . joints : joint . target_angles = [ None ] * joint . ADOF joint . controllers = [ pid ( * args , ** kwargs ) for i in range ( joint . ADOF ) ]
13276	def update_desc_rsib_path ( desc , sibs_len ) : if ( desc [ 'sib_seq' ] < ( sibs_len - 1 ) ) : rsib_path = copy . deepcopy ( desc [ 'path' ] ) rsib_path [ - 1 ] = desc [ 'sib_seq' ] + 1 desc [ 'rsib_path' ] = rsib_path else : pass return ( desc )
7469	def concatclusts ( outhandle , alignbits ) : with gzip . open ( outhandle , 'wb' ) as out : for fname in alignbits : with open ( fname ) as infile : out . write ( infile . read ( ) + "//\n//\n" )
9	def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError
1726	def except_token ( source , start , token , throw = True ) : start = pass_white ( source , start ) if start < len ( source ) and source [ start ] == token : return start + 1 if throw : raise SyntaxError ( 'Missing token. Expected %s' % token ) return None
12793	def get ( self , url = None , parse_data = True , key = None , parameters = None ) : return self . _fetch ( "GET" , url , post_data = None , parse_data = parse_data , key = key , parameters = parameters )
12051	def getParent2 ( abfFname , groups ) : if ".abf" in abfFname : abfFname = os . path . basename ( abfFname ) . replace ( ".abf" , "" ) for parentID in groups . keys ( ) : if abfFname in groups [ parentID ] : return parentID return abfFname
5058	def get_notification_subject_line ( course_name , template_configuration = None ) : stock_subject_template = _ ( 'You\'ve been enrolled in {course_name}!' ) default_subject_template = getattr ( settings , 'ENTERPRISE_ENROLLMENT_EMAIL_DEFAULT_SUBJECT_LINE' , stock_subject_template , ) if template_configuration is not None and template_configuration . subject_line : final_subject_template = template_configuration . subject_line else : final_subject_template = default_subject_template try : return final_subject_template . format ( course_name = course_name ) except KeyError : pass try : return default_subject_template . format ( course_name = course_name ) except KeyError : return stock_subject_template . format ( course_name = course_name )
12809	def received ( self , messages ) : if messages : if self . _queue : self . _queue . put_nowait ( messages ) if self . _callback : self . _callback ( messages )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
6720	def init ( self ) : r = self . local_renderer print ( 'Creating new virtual environment...' ) with self . settings ( warn_only = True ) : cmd = '[ ! -d {virtualenv_dir} ] && virtualenv --no-site-packages {virtualenv_dir} || true' if self . is_local : r . run_or_local ( cmd ) else : r . sudo ( cmd )
5785	def _raw_read ( self ) : data = self . _raw_bytes try : data += self . _socket . recv ( 8192 ) except ( socket_ . error ) : pass output = data written = libssl . BIO_write ( self . _rbio , data , len ( data ) ) self . _raw_bytes = data [ written : ] return output
2656	def makedirs ( self , path , mode = 511 , exist_ok = False ) : if exist_ok is False and self . isdir ( path ) : raise OSError ( 'Target directory {} already exists' . format ( path ) ) self . execute_wait ( 'mkdir -p {}' . format ( path ) ) self . sftp_client . chmod ( path , mode )
128	def project ( self , from_shape , to_shape ) : if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return self . copy ( ) ls_proj = self . to_line_string ( closed = False ) . project ( from_shape , to_shape ) return self . copy ( exterior = ls_proj . coords )
6941	def _double_inverted_gaussian ( x , amp1 , loc1 , std1 , amp2 , loc2 , std2 ) : gaussian1 = - _gaussian ( x , amp1 , loc1 , std1 ) gaussian2 = - _gaussian ( x , amp2 , loc2 , std2 ) return gaussian1 + gaussian2
1144	def init ( self ) : "Initialize the message-digest and set all fields to zero." self . length = 0 self . input = [ ] self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
12743	def get_urls ( self ) : urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "2" ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , urls )
5524	def jenks_breaks ( values , nb_class ) : if not isinstance ( values , Iterable ) or isinstance ( values , ( str , bytes ) ) : raise TypeError ( "A sequence of numbers is expected" ) if isinstance ( nb_class , float ) and int ( nb_class ) == nb_class : nb_class = int ( nb_class ) if not isinstance ( nb_class , int ) : raise TypeError ( "Number of class have to be a positive integer: " "expected an instance of 'int' but found {}" . format ( type ( nb_class ) ) ) nb_values = len ( values ) if np and isinstance ( values , np . ndarray ) : values = values [ np . argwhere ( np . isfinite ( values ) ) . reshape ( - 1 ) ] else : values = [ i for i in values if isfinite ( i ) ] if len ( values ) != nb_values : warnings . warn ( 'Invalid values encountered (NaN or Inf) were ignored' ) nb_values = len ( values ) if nb_class >= nb_values or nb_class < 2 : raise ValueError ( "Number of class have to be an integer " "greater than 2 and " "smaller than the number of values to use" ) return jenks . _jenks_breaks ( values , nb_class )
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : return None assert tx . depth == 0 return tx except IndexError : return None
9682	def set_fan_power ( self , power ) : if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
10000	def clear_obj ( self , obj ) : obj_nodes = self . get_nodes_with ( obj ) removed = set ( ) for node in obj_nodes : if self . has_node ( node ) : removed . update ( self . clear_descendants ( node ) ) return removed
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
2499	def create_doc ( self ) : doc_node = URIRef ( 'http://www.spdx.org/tools#SPDXRef-DOCUMENT' ) self . graph . add ( ( doc_node , RDF . type , self . spdx_namespace . SpdxDocument ) ) vers_literal = Literal ( str ( self . document . version ) ) self . graph . add ( ( doc_node , self . spdx_namespace . specVersion , vers_literal ) ) data_lics = URIRef ( self . document . data_license . url ) self . graph . add ( ( doc_node , self . spdx_namespace . dataLicense , data_lics ) ) doc_name = URIRef ( self . document . name ) self . graph . add ( ( doc_node , self . spdx_namespace . name , doc_name ) ) return doc_node
3929	def get_auth ( credentials_prompt , refresh_token_cache , manual_login = False ) : with requests . Session ( ) as session : session . headers = { 'user-agent' : USER_AGENT } try : logger . info ( 'Authenticating with refresh token' ) refresh_token = refresh_token_cache . get ( ) if refresh_token is None : raise GoogleAuthError ( "Refresh token not found" ) access_token = _auth_with_refresh_token ( session , refresh_token ) except GoogleAuthError as e : logger . info ( 'Failed to authenticate using refresh token: %s' , e ) logger . info ( 'Authenticating with credentials' ) if manual_login : authorization_code = ( credentials_prompt . get_authorization_code ( ) ) else : authorization_code = _get_authorization_code ( session , credentials_prompt ) access_token , refresh_token = _auth_with_code ( session , authorization_code ) refresh_token_cache . set ( refresh_token ) logger . info ( 'Authentication successful' ) return _get_session_cookies ( session , access_token )
4803	def raises ( self , ex ) : if not callable ( self . val ) : raise TypeError ( 'val must be callable' ) if not issubclass ( ex , BaseException ) : raise TypeError ( 'given arg must be exception' ) return AssertionBuilder ( self . val , self . description , self . kind , ex )
4023	def _get_localhost_ssh_port ( ) : for line in _get_vm_config ( ) : if line . startswith ( 'Forwarding' ) : spec = line . split ( '=' ) [ 1 ] . strip ( '"' ) name , protocol , host , host_port , target , target_port = spec . split ( ',' ) if name == 'ssh' and protocol == 'tcp' and target_port == '22' : return host_port raise ValueError ( 'Could not determine localhost port for SSH forwarding' )
2827	def convert_hardtanh ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting hardtanh (clip) ...' ) def target_layer ( x , max_val = float ( params [ 'max_val' ] ) , min_val = float ( params [ 'min_val' ] ) ) : return tf . minimum ( max_val , tf . maximum ( min_val , x ) ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
13855	def getTextFromNode ( node ) : t = "" for n in node . childNodes : if n . nodeType == n . TEXT_NODE : t += n . nodeValue else : raise NotTextNodeError return t
9580	def read_struct_array ( fd , endian , header ) : field_name_length = read_elements ( fd , endian , [ 'miINT32' ] ) if field_name_length > 32 : raise ParseError ( 'Unexpected field name length: {}' . format ( field_name_length ) ) fields = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) if isinstance ( fields , basestring ) : fields = [ fields ] empty = lambda : [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] array = { } for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : for field in fields : vheader , next_pos , fd_var = read_var_header ( fd , endian ) data = read_var_array ( fd_var , endian , vheader ) if field not in array : array [ field ] = empty ( ) array [ field ] [ row ] . append ( data ) fd . seek ( next_pos ) for field in fields : rows = array [ field ] for i in range ( header [ 'dims' ] [ 0 ] ) : rows [ i ] = squeeze ( rows [ i ] ) array [ field ] = squeeze ( array [ field ] ) return array
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
222	def is_not_modified ( self , response_headers : Headers , request_headers : Headers ) -> bool : try : if_none_match = request_headers [ "if-none-match" ] etag = response_headers [ "etag" ] if if_none_match == etag : return True except KeyError : pass try : if_modified_since = parsedate ( request_headers [ "if-modified-since" ] ) last_modified = parsedate ( response_headers [ "last-modified" ] ) if ( if_modified_since is not None and last_modified is not None and if_modified_since >= last_modified ) : return True except KeyError : pass return False
7511	def select_samples ( dbsamples , samples , pidx = None ) : samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
9468	def conference_speak ( self , call_params ) : path = '/' + self . api_version + '/ConferenceSpeak/' method = 'POST' return self . request ( path , method , call_params )
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) csp [ 'default-src' ] += flask_talisman_debug_mode
8466	def run ( self ) : options = { } if bool ( self . config [ 'use_proxy' ] ) : options [ 'proxies' ] = { "http" : self . config [ 'proxy' ] , "https" : self . config [ 'proxy' ] } options [ "url" ] = self . config [ 'url' ] options [ "data" ] = { "issues" : json . dumps ( map ( lambda x : x . __todict__ ( ) , self . issues ) ) } if 'get' == self . config [ 'method' ] . lower ( ) : requests . get ( ** options ) else : requests . post ( ** options )
5297	def get_first_of_week ( self ) : if self . first_of_week is None : raise ImproperlyConfigured ( "%s.first_of_week is required." % self . __class__ . __name__ ) if self . first_of_week not in range ( 7 ) : raise ImproperlyConfigured ( "%s.first_of_week must be an integer between 0 and 6." % self . __class__ . __name__ ) return self . first_of_week
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
6614	def receive_one ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive_one ( )
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) if self . ready_event . emit ( my_task . workflow , my_task ) : for assignment in self . post_assign : assignment . assign ( my_task , my_task ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
12700	def get_i_name ( self , num , is_oai = None ) : if num not in ( 1 , 2 ) : raise ValueError ( "`num` parameter have to be 1 or 2!" ) if is_oai is None : is_oai = self . oai_marc i_name = "ind" if not is_oai else "i" return i_name + str ( num )
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
2724	def change_kernel ( self , kernel , return_dict = True ) : if type ( kernel ) != Kernel : raise BadKernelObject ( "Use Kernel object" ) return self . _perform_action ( { 'type' : 'change_kernel' , 'kernel' : kernel . id } , return_dict )
1271	def setup_components_and_tf_funcs ( self , custom_getter = None ) : self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( "Invalid internal initialization value." ) custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . distributions = self . create_distributions ( ) self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter
3954	def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
10841	def delete ( self ) : url = PATHS [ 'DELETE' ] % self . id return self . api . post ( url = url )
7095	def init_map ( self ) : d = self . declaration if d . show_location : self . set_show_location ( d . show_location ) if d . show_traffic : self . set_show_traffic ( d . show_traffic ) if d . show_indoors : self . set_show_indoors ( d . show_indoors ) if d . show_buildings : self . set_show_buildings ( d . show_buildings ) mapview = self . map mid = mapview . getId ( ) mapview . onCameraChange . connect ( self . on_camera_changed ) mapview . onCameraMoveStarted . connect ( self . on_camera_move_started ) mapview . onCameraMoveCanceled . connect ( self . on_camera_move_stopped ) mapview . onCameraIdle . connect ( self . on_camera_move_stopped ) mapview . setOnCameraChangeListener ( mid ) mapview . setOnCameraMoveStartedListener ( mid ) mapview . setOnCameraMoveCanceledListener ( mid ) mapview . setOnCameraIdleListener ( mid ) mapview . onMapClick . connect ( self . on_map_clicked ) mapview . setOnMapClickListener ( mid ) mapview . onMapLongClick . connect ( self . on_map_long_clicked ) mapview . setOnMapLongClickListener ( mid ) mapview . onMarkerClick . connect ( self . on_marker_clicked ) mapview . setOnMarkerClickListener ( self . map . getId ( ) ) mapview . onMarkerDragStart . connect ( self . on_marker_drag_start ) mapview . onMarkerDrag . connect ( self . on_marker_drag ) mapview . onMarkerDragEnd . connect ( self . on_marker_drag_end ) mapview . setOnMarkerDragListener ( mid ) mapview . onInfoWindowClick . connect ( self . on_info_window_clicked ) mapview . onInfoWindowLongClick . connect ( self . on_info_window_long_clicked ) mapview . onInfoWindowClose . connect ( self . on_info_window_closed ) mapview . setOnInfoWindowClickListener ( mid ) mapview . setOnInfoWindowCloseListener ( mid ) mapview . setOnInfoWindowLongClickListener ( mid ) mapview . onPolygonClick . connect ( self . on_poly_clicked ) mapview . onPolylineClick . connect ( self . on_poly_clicked ) mapview . setOnPolygonClickListener ( mid ) mapview . setOnPolylineClickListener ( mid ) mapview . onCircleClick . connect ( self . on_circle_clicked ) mapview . setOnCircleClickListener ( mid )
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters stats = [ ] for field in statsInfo : if statsInfo [ field ] == "number" : statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) r [ "_filename" ] = statsFilename return r
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
8202	def size_or_default ( self ) : if not self . size : self . size = self . DEFAULT_SIZE return self . size
4322	def channels ( self , n_channels ) : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( 'n_channels must be a positive integer.' ) effect_args = [ 'channels' , '{}' . format ( n_channels ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'channels' ) return self
1418	def create_execution_state ( self , topologyName , executionState ) : if not executionState or not executionState . IsInitialized ( ) : raise_ ( StateException ( "Execution State protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_execution_state_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) executionStateString = executionState . SerializeToString ( ) try : self . client . create ( path , value = executionStateString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating execution state" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating execution state" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating execution state" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
2870	def setup ( self , pin , mode ) : self . mraa_gpio . Gpio . dir ( self . mraa_gpio . Gpio ( pin ) , self . _dir_mapping [ mode ] )
12007	def _get_algorithm_info ( self , algorithm_info ) : if algorithm_info [ 'algorithm' ] not in self . ALGORITHMS : raise Exception ( 'Algorithm not supported: %s' % algorithm_info [ 'algorithm' ] ) algorithm = self . ALGORITHMS [ algorithm_info [ 'algorithm' ] ] algorithm_info . update ( algorithm ) return algorithm_info
7291	def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
1838	def JNB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC )
1088	def countOf ( a , b ) : "Return the number of times b occurs in a." count = 0 for i in a : if i == b : count += 1 return count
13698	def wait ( self , timeout = None ) : if timeout is None : timeout = self . _timeout while self . _process . check_readable ( timeout ) : self . _flush ( )
3513	def clicky ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickyNode ( )
4960	def get_earliest_start_date_from_program ( program ) : start_dates = [ ] for course in program . get ( 'courses' , [ ] ) : for run in course . get ( 'course_runs' , [ ] ) : if run . get ( 'start' ) : start_dates . append ( parse_lms_api_datetime ( run [ 'start' ] ) ) if not start_dates : return None return min ( start_dates )
1814	def SETNG ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , 1 , 0 ) )
268	def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) rows = '' for name , value in header_rows . items ( ) : rows += ( '\n <tr style="text-align: right;"><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
2963	def get_source_chains ( self , blockade_id ) : result = { } if not blockade_id : raise ValueError ( "invalid blockade_id" ) lines = self . get_chain_rules ( "FORWARD" ) for line in lines : parts = line . split ( ) if len ( parts ) < 4 : continue try : partition_index = parse_partition_index ( blockade_id , parts [ 0 ] ) except ValueError : continue source = parts [ 3 ] if source : result [ source ] = partition_index return result
3781	def calculate ( self , T , method ) : r if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( lcat , ccat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
6436	def gen_fibonacci ( ) : num_a , num_b = 1 , 2 while True : yield num_a num_a , num_b = num_b , num_a + num_b
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
8635	def get_milestones ( session , project_ids = [ ] , milestone_ids = [ ] , user_details = None , limit = 10 , offset = 0 ) : get_milestones_data = { } if milestone_ids : get_milestones_data [ 'milestones[]' ] = milestone_ids if project_ids : get_milestones_data [ 'projects[]' ] = project_ids get_milestones_data [ 'limit' ] = limit get_milestones_data [ 'offset' ] = offset if user_details : get_milestones_data . update ( user_details ) response = make_get_request ( session , 'milestones' , params_data = get_milestones_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2430	def set_spdx_doc_uri ( self , doc , spdx_doc_uri ) : if validations . validate_doc_namespace ( spdx_doc_uri ) : doc . ext_document_references [ - 1 ] . spdx_document_uri = spdx_doc_uri else : raise SPDXValueError ( 'Document::ExternalDocumentRef' )
8896	def _default_poll_callback ( self , poll_resp ) : if poll_resp . parsed is None : return False success_list = [ 'UpdatesComplete' , True , 'COMPLETE' ] status = None if self . response_format == 'xml' : status = poll_resp . parsed . find ( './Status' ) . text elif self . response_format == 'json' : status = poll_resp . parsed . get ( 'Status' , poll_resp . parsed . get ( 'status' ) ) if status is None : raise RuntimeError ( 'Unable to get poll response status.' ) return status in success_list
12706	def state ( self , state ) : assert self . name == state . name , 'state name "{}" != body name "{}"' . format ( state . name , self . name ) self . position = state . position self . quaternion = state . quaternion self . linear_velocity = state . linear_velocity self . angular_velocity = state . angular_velocity
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
13209	def _prep_snippet_for_pandoc ( self , latex_text ) : replace_cite = CitationLinker ( self . bib_db ) latex_text = replace_cite ( latex_text ) return latex_text
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
9813	def url ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . has_notebook : click . echo ( get_notebook_url ( user , project_name ) ) else : Printer . print_warning ( 'This project `{}` does not have a running notebook.' . format ( project_name ) ) click . echo ( 'You can start a notebook with this command: polyaxon notebook start --help' )
8957	def included ( self , path , is_dir = False ) : inclusive = None for pattern in self . patterns : if pattern . is_dir == is_dir and pattern . matches ( path ) : inclusive = pattern . inclusive return inclusive
3121	def make_signed_jwt ( signer , payload , key_id = None ) : header = { 'typ' : 'JWT' , 'alg' : 'RS256' } if key_id is not None : header [ 'kid' ] = key_id segments = [ _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( header ) ) , _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( payload ) ) , ] signing_input = b'.' . join ( segments ) signature = signer . sign ( signing_input ) segments . append ( _helpers . _urlsafe_b64encode ( signature ) ) logger . debug ( str ( segments ) ) return b'.' . join ( segments )
9154	def bezier ( self , points ) : coordinates = pgmagick . CoordinateList ( ) for point in points : x , y = float ( point [ 0 ] ) , float ( point [ 1 ] ) coordinates . append ( pgmagick . Coordinate ( x , y ) ) self . drawer . append ( pgmagick . DrawableBezier ( coordinates ) )
69	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : image = np . copy ( image ) if copy else image for bb in self . bounding_boxes : image = bb . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image , thickness = thickness ) return image
11944	def add ( self , level , message , extra_tags = '' ) : if not message : return level = int ( level ) if level < self . level : return if level not in stored_messages_settings . STORE_LEVELS or self . user . is_anonymous ( ) : return super ( StorageMixin , self ) . add ( level , message , extra_tags ) self . added_new = True m = self . backend . create_message ( level , message , extra_tags ) self . backend . archive_store ( [ self . user ] , m ) self . _queued_messages . append ( m )
3620	def unregister ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) del self . __registered_models [ model ] post_save . disconnect ( self . __post_save_receiver , model ) pre_delete . disconnect ( self . __pre_delete_receiver , model ) logger . info ( 'UNREGISTER %s' , model )
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s
8689	def _construct_key ( self , values ) : key = { } for column , value in zip ( self . keys . columns , values ) : key . update ( { column . name : value } ) return key
10799	def _eval_firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist_between_points = self . _distance_matrix ( rvecs , self . x ) gaussian_weights = self . _weight ( dist_between_points , sigma = sigma ) return gaussian_weights . dot ( data ) / gaussian_weights . sum ( axis = 1 ) else : ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . _distance_matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . _weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
12453	def config_to_args ( config ) : result = [ ] for key , value in iteritems ( config ) : if value is False : continue key = '--{0}' . format ( key . replace ( '_' , '-' ) ) if isinstance ( value , ( list , set , tuple ) ) : for item in value : result . extend ( ( key , smart_str ( item ) ) ) elif value is not True : result . extend ( ( key , smart_str ( value ) ) ) else : result . append ( key ) return tuple ( result )
3745	def calculate ( self , T , method ) : r if method == DUTT_PRASAD : A , B , C = self . DUTT_PRASAD_coeffs mu = ViswanathNatarajan3 ( T , A , B , C , ) elif method == VISWANATH_NATARAJAN_3 : A , B , C = self . VISWANATH_NATARAJAN_3_coeffs mu = ViswanathNatarajan3 ( T , A , B , C ) elif method == VISWANATH_NATARAJAN_2 : A , B = self . VISWANATH_NATARAJAN_2_coeffs mu = ViswanathNatarajan2 ( T , self . VISWANATH_NATARAJAN_2_coeffs [ 0 ] , self . VISWANATH_NATARAJAN_2_coeffs [ 1 ] ) elif method == VISWANATH_NATARAJAN_2E : C , D = self . VISWANATH_NATARAJAN_2E_coeffs mu = ViswanathNatarajan2Exponential ( T , C , D ) elif method == DIPPR_PERRY_8E : mu = EQ101 ( T , * self . Perrys2_313_coeffs ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'l' ) elif method == LETSOU_STIEL : mu = Letsou_Stiel ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == PRZEDZIECKI_SRIDHAR : Vml = self . Vml ( T ) if hasattr ( self . Vml , '__call__' ) else self . Vml mu = Przedziecki_Sridhar ( T , self . Tm , self . Tc , self . Pc , self . Vc , Vml , self . omega , self . MW ) elif method == VDI_PPDS : A , B , C , D , E = self . VDI_PPDS_coeffs term = ( C - T ) / ( T - D ) if term < 0 : term1 = - ( ( T - C ) / ( T - D ) ) ** ( 1 / 3. ) else : term1 = term ** ( 1 / 3. ) term2 = term * term1 mu = E * exp ( A * term1 + B * term2 ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
4530	def _addLoggingLevel ( levelName , levelNum , methodName = None ) : if not methodName : methodName = levelName . lower ( ) if hasattr ( logging , levelName ) : raise AttributeError ( '{} already defined in logging module' . format ( levelName ) ) if hasattr ( logging , methodName ) : raise AttributeError ( '{} already defined in logging module' . format ( methodName ) ) if hasattr ( logging . getLoggerClass ( ) , methodName ) : raise AttributeError ( '{} already defined in logger class' . format ( methodName ) ) def logForLevel ( self , message , * args , ** kwargs ) : if self . isEnabledFor ( levelNum ) : self . _log ( levelNum , message , args , ** kwargs ) def logToRoot ( message , * args , ** kwargs ) : logging . log ( levelNum , message , * args , ** kwargs ) logging . addLevelName ( levelNum , levelName ) setattr ( logging , levelName , levelNum ) setattr ( logging . getLoggerClass ( ) , methodName , logForLevel ) setattr ( logging , methodName , logToRoot )
1034	def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )
6050	def run ( self , data , results = None , mask = None , positions = None ) : model_image = results . last . unmasked_model_image galaxy_tuples = results . last . constant . name_instance_tuples_for_class ( g . Galaxy ) results_copy = copy . copy ( results . last ) for name , galaxy in galaxy_tuples : optimizer = self . optimizer . copy_with_name_extension ( name ) optimizer . variable . hyper_galaxy = g . HyperGalaxy galaxy_image = results . last . unmasked_image_for_galaxy ( galaxy ) optimizer . fit ( self . __class__ . Analysis ( data , model_image , galaxy_image ) ) getattr ( results_copy . variable , name ) . hyper_galaxy = optimizer . variable . hyper_galaxy getattr ( results_copy . constant , name ) . hyper_galaxy = optimizer . constant . hyper_galaxy return results_copy
4259	def read_markdown ( filename ) : global MD with open ( filename , 'r' , encoding = 'utf-8-sig' ) as f : text = f . read ( ) if MD is None : MD = Markdown ( extensions = [ 'markdown.extensions.meta' , 'markdown.extensions.tables' ] , output_format = 'html5' ) else : MD . reset ( ) MD . Meta = { } output = { 'description' : Markup ( MD . convert ( text ) ) } try : meta = MD . Meta . copy ( ) except AttributeError : pass else : output [ 'meta' ] = meta try : output [ 'title' ] = MD . Meta [ 'title' ] [ 0 ] except KeyError : pass return output
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
6120	def elliptical ( cls , shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_elliptical_from_shape_pixel_scale_and_radius ( shape , pixel_scale , major_axis_radius_arcsec , axis_ratio , phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
11157	def print_big_dir_and_big_file ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table1 = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p1 , size1 in size_table1 [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size1 ) , p1 . abspath ) ) size_table2 = sorted ( [ ( p , p . size ) for p in p1 . select_file ( recursive = True ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p2 , size2 in size_table2 [ : top_n ] : print ( " {:<9} {:<9}" . format ( repr_data_size ( size2 ) , p2 . abspath ) )
7577	def _get_clumpp_table ( self , kpop , max_var_multiple , quiet ) : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : return "no result files found" clumphandle = os . path . join ( self . workdir , "tmp.clumppparams.txt" ) self . clumppparams . kpop = kpop self . clumppparams . c = ninds self . clumppparams . r = nreps with open ( clumphandle , 'w' ) as tmp_c : tmp_c . write ( self . clumppparams . _asfile ( ) ) outfile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) indfile = os . path . join ( self . workdir , "{}-K-{}.indfile" . format ( self . name , kpop ) ) miscfile = os . path . join ( self . workdir , "{}-K-{}.miscfile" . format ( self . name , kpop ) ) cmd = [ "CLUMPP" , clumphandle , "-i" , indfile , "-o" , outfile , "-j" , miscfile , "-r" , str ( nreps ) , "-c" , str ( ninds ) , "-k" , str ( kpop ) ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) _ = proc . communicate ( ) for rfile in [ indfile , miscfile ] : if os . path . exists ( rfile ) : os . remove ( rfile ) ofile = os . path . join ( self . workdir , "{}-K-{}.outfile" . format ( self . name , kpop ) ) if os . path . exists ( ofile ) : csvtable = pd . read_csv ( ofile , delim_whitespace = True , header = None ) table = csvtable . loc [ : , 5 : ] table . columns = range ( table . shape [ 1 ] ) table . index = self . labels if not quiet : sys . stderr . write ( "[K{}] {}/{} results permuted across replicates (max_var={}).\n" . format ( kpop , nreps , nreps + excluded , max_var_multiple ) ) return table else : sys . stderr . write ( "No files ready for {}-K-{} in {}\n" . format ( self . name , kpop , self . workdir ) ) return
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
4187	def window_riesz ( N ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1 - abs ( n / ( N / 2. ) ) ** 2. return w
747	def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )
13309	def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
2722	def _perform_action ( self , params , return_dict = True ) : action = self . get_data ( "droplets/%s/actions/" % self . id , type = POST , params = params ) if return_dict : return action else : action = action [ u'action' ] return_action = Action ( token = self . token ) for attr in action . keys ( ) : setattr ( return_action , attr , action [ attr ] ) return return_action
4830	def get_course_grade ( self , course_id , username ) : results = self . client . courses ( course_id ) . get ( username = username ) for row in results : if row . get ( 'username' ) == username : return row raise HttpNotFoundError ( 'No grade record found for course={}, username={}' . format ( course_id , username ) )
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
8110	def search_news ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_NEWS return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
3524	def intercom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return IntercomNode ( )
9814	def start ( ctx , file , u ) : specification = None job_config = None if file : specification = check_polyaxonfile ( file , log = False ) . specification if u : ctx . invoke ( upload , sync = False ) if specification : check_polyaxonfile_kind ( specification = specification , kind = specification . _NOTEBOOK ) job_config = specification . parsed_data user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . start_notebook ( user , project_name , job_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not start notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 200 : Printer . print_header ( "A notebook for this project is already running on:" ) click . echo ( get_notebook_url ( user , project_name ) ) sys . exit ( 0 ) if response . status_code != 201 : Printer . print_error ( 'Something went wrong, Notebook was not created.' ) sys . exit ( 1 ) Printer . print_success ( 'Notebook is being deployed for project `{}`' . format ( project_name ) ) clint . textui . puts ( "It may take some time before you can access the notebook.\n" ) clint . textui . puts ( "Your notebook will be available on:\n" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( get_notebook_url ( user , project_name ) )
4357	def remove_namespace ( self , namespace ) : if namespace in self . active_ns : del self . active_ns [ namespace ] if len ( self . active_ns ) == 0 and self . connected : self . kill ( detach = True )
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
5196	def configure_stack ( ) : stack_config = asiodnp3 . OutstationStackConfig ( opendnp3 . DatabaseSizes . AllTypes ( 10 ) ) stack_config . outstation . eventBufferConfig = opendnp3 . EventBufferConfig ( ) . AllTypes ( 10 ) stack_config . outstation . params . allowUnsolicited = True stack_config . link . LocalAddr = 10 stack_config . link . RemoteAddr = 1 stack_config . link . KeepAliveTimeout = openpal . TimeDuration ( ) . Max ( ) return stack_config
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
2180	def fetch_access_token ( self , url , verifier = None , ** request_kwargs ) : if verifier : self . _client . client . verifier = verifier if not getattr ( self . _client . client , "verifier" , None ) : raise VerifierMissing ( "No client verifier has been set." ) token = self . _fetch_token ( url , ** request_kwargs ) log . debug ( "Resetting verifier attribute, should not be used anymore." ) self . _client . client . verifier = None return token
8333	def findPrevious ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllPrevious , name , attrs , text , ** kwargs )
5555	def _element_at_zoom ( name , element , zoom ) : if isinstance ( element , dict ) : if "format" in element : return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) if len ( out_elements ) == 0 : return None return out_elements elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) else : return element else : return element
3767	def zs_to_ws ( zs , MWs ) : r Mavg = sum ( zi * MWi for zi , MWi in zip ( zs , MWs ) ) ws = [ zi * MWi / Mavg for zi , MWi in zip ( zs , MWs ) ] return ws
10980	def remove ( group_id , user_id ) : group = Group . query . get_or_404 ( group_id ) user = User . query . get_or_404 ( user_id ) if group . can_edit ( current_user ) : try : group . remove_member ( user ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'User %(user_email)s was removed from %(group_name)s group.' , user_email = user . email , group_name = group . name ) , 'success' ) return redirect ( urlparse ( request . referrer ) . path ) flash ( _ ( 'You cannot delete users of the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
8818	def get_networks ( context , limit = None , sorts = [ 'id' ] , marker = None , page_reverse = False , filters = None , fields = None ) : LOG . info ( "get_networks for tenant %s with filters %s, fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } nets = db_api . network_find ( context , limit , sorts , marker , page_reverse , join_subnets = True , ** filters ) or [ ] nets = [ v . _make_network_dict ( net , fields = fields ) for net in nets ] return nets
4663	def new_tx ( self , * args , ** kwargs ) : builder = self . transactionbuilder_class ( * args , blockchain_instance = self , ** kwargs ) self . _txbuffers . append ( builder ) return builder
11533	def setup ( self , port ) : port = str ( port ) self . _serial = serial . Serial ( port , 115200 , timeout = 2 ) time . sleep ( 2 ) if not self . _serial . is_open : raise RuntimeError ( 'Could not connect to Arduino' ) self . _serial . write ( b'\x01' ) if self . _serial . read ( ) != b'\x06' : raise RuntimeError ( 'Could not connect to Arduino' ) ps = [ p for p in self . available_pins ( ) if p [ 'digital' ] [ 'output' ] ] for pin in ps : self . _set_pin_direction ( pin [ 'id' ] , ahio . Direction . Output )
7810	def from_der_data ( cls , data ) : logger . debug ( "Decoding DER certificate: {0!r}" . format ( data ) ) if cls . _cert_asn1_type is None : cls . _cert_asn1_type = Certificate ( ) cert = der_decoder . decode ( data , asn1Spec = cls . _cert_asn1_type ) [ 0 ] result = cls ( ) tbs_cert = cert . getComponentByName ( 'tbsCertificate' ) subject = tbs_cert . getComponentByName ( 'subject' ) logger . debug ( "Subject: {0!r}" . format ( subject ) ) result . _decode_subject ( subject ) validity = tbs_cert . getComponentByName ( 'validity' ) result . _decode_validity ( validity ) extensions = tbs_cert . getComponentByName ( 'extensions' ) if extensions : for extension in extensions : logger . debug ( "Extension: {0!r}" . format ( extension ) ) oid = extension . getComponentByName ( 'extnID' ) logger . debug ( "OID: {0!r}" . format ( oid ) ) if oid != SUBJECT_ALT_NAME_OID : continue value = extension . getComponentByName ( 'extnValue' ) logger . debug ( "Value: {0!r}" . format ( value ) ) if isinstance ( value , Any ) : value = der_decoder . decode ( value , asn1Spec = OctetString ( ) ) [ 0 ] alt_names = der_decoder . decode ( value , asn1Spec = GeneralNames ( ) ) [ 0 ] logger . debug ( "SubjectAltName: {0!r}" . format ( alt_names ) ) result . _decode_alt_names ( alt_names ) return result
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
9445	def group_call ( self , call_params ) : path = '/' + self . api_version + '/GroupCall/' method = 'POST' return self . request ( path , method , call_params )
3173	def create_or_update ( self , store_id , customer_id , data ) : self . store_id = store_id self . customer_id = customer_id if 'id' not in data : raise KeyError ( 'The store customer must have an id' ) if 'email_address' not in data : raise KeyError ( 'Each store customer must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'opt_in_status' not in data : raise KeyError ( 'The store customer must have an opt_in_status' ) if data [ 'opt_in_status' ] not in [ True , False ] : raise TypeError ( 'The opt_in_status must be True or False' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'customers' , customer_id ) , data = data )
1026	def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) if prevline is not None : write ( prevline ) thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] prevline = thisline if prevline is not None : write ( prevline , lineEnd = stripped )
2049	def create_contract ( self , price = 0 , address = None , caller = None , balance = 0 , init = None , gas = None ) : expected_address = self . create_account ( self . new_address ( sender = caller ) ) if address is None : address = expected_address elif caller is not None and address != expected_address : raise EthereumError ( f"Error: contract created from address {hex(caller)} with nonce {self.get_nonce(caller)} was expected to be at address {hex(expected_address)}, but create_contract was called with address={hex(address)}" ) self . start_transaction ( 'CREATE' , address , price , init , caller , balance , gas = gas ) self . _process_pending_transaction ( ) return address
12906	def objHasUnsavedChanges ( self ) : if not self . obj : return False return self . obj . hasUnsavedChanges ( cascadeObjects = True )
9581	def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
6377	def sim_typo ( src , tar , metric = 'euclidean' , cost = ( 1 , 1 , 0.5 , 0.5 ) , layout = 'QWERTY' ) : return Typo ( ) . sim ( src , tar , metric , cost , layout )
6381	def dist_jaro_winkler ( src , tar , qval = 1 , mode = 'winkler' , long_strings = False , boost_threshold = 0.7 , scaling_factor = 0.1 , ) : return JaroWinkler ( ) . dist ( src , tar , qval , mode , long_strings , boost_threshold , scaling_factor )
722	def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records
13387	def make_upstream_request ( self ) : "Return request object for calling the upstream" url = self . upstream_url ( self . request . uri ) return tornado . httpclient . HTTPRequest ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
2566	def udp_messenger ( domain_name , UDP_IP , UDP_PORT , sock_timeout , message ) : try : if message is None : raise ValueError ( "message was none" ) encoded_message = bytes ( message , "utf-8" ) if encoded_message is None : raise ValueError ( "utf-8 encoding of message failed" ) if domain_name : try : UDP_IP = socket . gethostbyname ( domain_name ) except Exception : pass if UDP_IP is None : raise Exception ( "UDP_IP is None" ) if UDP_PORT is None : raise Exception ( "UDP_PORT is None" ) sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) sock . settimeout ( sock_timeout ) sock . sendto ( bytes ( message , "utf-8" ) , ( UDP_IP , UDP_PORT ) ) sock . close ( ) except socket . timeout : logger . debug ( "Failed to send usage tracking data: socket timeout" ) except OSError as e : logger . debug ( "Failed to send usage tracking data: OSError: {}" . format ( e ) ) except Exception as e : logger . debug ( "Failed to send usage tracking data: Exception: {}" . format ( e ) )
3122	def _verify_signature ( message , signature , certs ) : for pem in certs : verifier = Verifier . from_string ( pem , is_x509_cert = True ) if verifier . verify ( message , signature ) : return raise AppIdentityError ( 'Invalid token signature' )
8240	def tetrad ( clr , angle = 90 ) : clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( angle ) if clr . brightness < 0.5 : c . brightness += 0.2 else : c . brightness -= - 0.2 colors . append ( c ) c = clr . rotate_ryb ( angle * 2 ) if clr . brightness < 0.5 : c . brightness += 0.1 else : c . brightness -= - 0.1 colors . append ( c ) colors . append ( clr . rotate_ryb ( angle * 3 ) . lighten ( 0.1 ) ) return colors
2050	def _swap_mode ( self ) : assert self . mode in ( cs . CS_MODE_ARM , cs . CS_MODE_THUMB ) if self . mode == cs . CS_MODE_ARM : self . mode = cs . CS_MODE_THUMB else : self . mode = cs . CS_MODE_ARM
977	def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( "newIndex must be within one of existing indices" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True
13709	def is_threat ( self , result = None , harmless_age = None , threat_score = None , threat_type = None ) : harmless_age = harmless_age if harmless_age is not None else settings . CACHED_HTTPBL_HARMLESS_AGE threat_score = threat_score if threat_score is not None else settings . CACHED_HTTPBL_THREAT_SCORE threat_type = threat_type if threat_type is not None else - 1 result = result if result is not None else self . _last_result threat = False if result is not None : if result [ 'age' ] < harmless_age and result [ 'threat' ] > threat_score : threat = True if threat_type > - 1 : if result [ 'type' ] & threat_type : threat = True else : threat = False return threat
12955	def _add_id_to_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_key_for_index ( indexedField , val ) , pk )
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
3161	def get ( self , conversation_id , ** queryparams ) : self . conversation_id = conversation_id return self . _mc_client . _get ( url = self . _build_path ( conversation_id ) , ** queryparams )
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
13894	def MatchMasks ( filename , masks ) : import fnmatch if not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] for i_mask in masks : if fnmatch . fnmatch ( filename , i_mask ) : return True return False
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
1073	def getdelimited ( self , beginchar , endchars , allowcomments = 1 ) : if self . field [ self . pos ] != beginchar : return '' slist = [ '' ] quote = 0 self . pos += 1 while self . pos < len ( self . field ) : if quote == 1 : slist . append ( self . field [ self . pos ] ) quote = 0 elif self . field [ self . pos ] in endchars : self . pos += 1 break elif allowcomments and self . field [ self . pos ] == '(' : slist . append ( self . getcomment ( ) ) continue elif self . field [ self . pos ] == '\\' : quote = 1 else : slist . append ( self . field [ self . pos ] ) self . pos += 1 return '' . join ( slist )
10653	def run ( self , clock , generalLedger ) : for c in self . components : c . run ( clock , generalLedger ) for a in self . activities : a . run ( clock , generalLedger )
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
7843	def get_type ( self ) : item_type = self . xmlnode . prop ( "type" ) if not item_type : item_type = "?" return item_type . decode ( "utf-8" )
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
13594	def print_information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest_stable or 'N/A' ) , fg = 'yellow' if handler . latest_stable else 'magenta' ) ) ) if label is not None : latest_revision = handler . latest_revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest_revision or 'N/A' ) , fg = 'yellow' if latest_revision else 'magenta' ) ) )
6700	def apt_key_exists ( keyid ) : gpg_cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn_only = True ) : res = run ( '%(gpg_cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
11451	def get_collection ( self , journal ) : conference = '' for tag in self . document . getElementsByTagName ( 'conference' ) : conference = xml_to_text ( tag ) if conference or journal == "International Journal of Modern Physics: Conference Series" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'ConferencePaper' ) ] elif self . _get_article_type ( ) == "review-article" : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Review' ) ] else : return [ ( 'a' , 'HEP' ) , ( 'a' , 'Published' ) ]
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
9181	def _validate_subjects ( cursor , model ) : subject_vocab = [ term [ 0 ] for term in acquire_subject_vocabulary ( cursor ) ] subjects = model . metadata . get ( 'subjects' , [ ] ) invalid_subjects = [ s for s in subjects if s not in subject_vocab ] if invalid_subjects : raise exceptions . InvalidMetadata ( 'subjects' , invalid_subjects )
13570	def false_exit ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : ret = func ( * args , ** kwargs ) if ret is False : if "TMC_TESTING" in os . environ : raise TMCExit ( ) else : sys . exit ( - 1 ) return ret return inner
1186	def check_charset ( self , ctx , char ) : self . set_dispatcher . reset ( char ) save_position = ctx . code_position result = None while result is None : result = self . set_dispatcher . dispatch ( ctx . peek_code ( ) , ctx ) ctx . code_position = save_position return result
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
8224	def _mouse_pointer_moved ( self , x , y ) : self . _namespace [ 'MOUSEX' ] = x self . _namespace [ 'MOUSEY' ] = y
12085	def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
13012	def pprint ( arr , columns = ( 'temperature' , 'luminosity' ) , names = ( 'Temperature (Kelvin)' , 'Luminosity (solar units)' ) , max_rows = 32 , precision = 2 ) : if max_rows is True : pd . set_option ( 'display.max_rows' , 1000 ) elif type ( max_rows ) is int : pd . set_option ( 'display.max_rows' , max_rows ) pd . set_option ( 'precision' , precision ) df = pd . DataFrame ( arr . flatten ( ) , index = arr [ 'id' ] . flatten ( ) , columns = columns ) df . columns = names return df . style . format ( { names [ 0 ] : '{:.0f}' , names [ 1 ] : '{:.2f}' } )
12662	def load_mask ( image , allow_empty = True ) : img = check_img ( image , make_it_3d = True ) values = np . unique ( img . get_data ( ) ) if len ( values ) == 1 : if values [ 0 ] == 0 and not allow_empty : raise ValueError ( 'Given mask is invalid because it masks all data' ) elif len ( values ) == 2 : if 0 not in values : raise ValueError ( 'Background of the mask must be represented with 0.' ' Given mask contains: {}.' . format ( values ) ) elif len ( values ) != 2 : raise ValueError ( 'Given mask is not made of 2 values: {}. ' 'Cannot interpret as true or false' . format ( values ) ) return nib . Nifti1Image ( as_ndarray ( get_img_data ( img ) , dtype = bool ) , img . get_affine ( ) , img . get_header ( ) )
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
4902	def course_modal ( context , course = None ) : if course : context . update ( { 'course_image_uri' : course . get ( 'course_image_uri' , '' ) , 'course_title' : course . get ( 'course_title' , '' ) , 'course_level_type' : course . get ( 'course_level_type' , '' ) , 'course_short_description' : course . get ( 'course_short_description' , '' ) , 'course_effort' : course . get ( 'course_effort' , '' ) , 'course_full_description' : course . get ( 'course_full_description' , '' ) , 'expected_learning_items' : course . get ( 'expected_learning_items' , [ ] ) , 'staff' : course . get ( 'staff' , [ ] ) , 'premium_modes' : course . get ( 'premium_modes' , [ ] ) , } ) return context
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
12048	def determineProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 5000 ) f . close ( ) protoComment = "unknown" if b"SWHLab4[" in raw : protoComment = raw . split ( b"SWHLab4[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] elif b"SWH[" in raw : protoComment = raw . split ( b"SWH[" ) [ 1 ] . split ( b"]" , 1 ) [ 0 ] else : protoComment = "?" if not type ( protoComment ) is str : protoComment = protoComment . decode ( "utf-8" ) return protoComment
3389	def batch ( self , batch_size , batch_num , fluxes = True ) : for i in range ( batch_num ) : yield self . sample ( batch_size , fluxes = fluxes )
7898	def process_configuration_form_success ( self , stanza ) : if stanza . get_query_ns ( ) != MUC_OWNER_NS : raise ValueError ( "Bad result namespace" ) query = stanza . get_query ( ) form = None for el in xml_element_ns_iter ( query . children , DATAFORM_NS ) : form = Form ( el ) break if not form : raise ValueError ( "No form received" ) self . configuration_form = form self . handler . configuration_form_received ( form )
8394	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] if not argv or argv [ 0 ] == "help" : show_help ( ) return 0 elif argv [ 0 ] == "check" : return check_main ( argv [ 1 : ] ) elif argv [ 0 ] == "list" : return list_main ( argv [ 1 : ] ) elif argv [ 0 ] == "write" : return write_main ( argv [ 1 : ] ) else : print ( u"Don't understand {!r}" . format ( " " . join ( argv ) ) ) show_help ( ) return 1
4955	def get_actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
658	def plotHistogram ( freqCounts , title = 'On-Times Histogram' , xLabel = 'On-Time' ) : import pylab pylab . ion ( ) pylab . figure ( ) pylab . bar ( numpy . arange ( len ( freqCounts ) ) - 0.5 , freqCounts ) pylab . title ( title ) pylab . xlabel ( xLabel )
6343	def raw ( self ) : r doc_list = [ ] for doc in self . corpus : sent_list = [ ] for sent in doc : sent_list . append ( ' ' . join ( sent ) ) doc_list . append ( self . sent_split . join ( sent_list ) ) del sent_list return self . doc_split . join ( doc_list )
10055	def get ( self , pid , record , key , version_id , ** kwargs ) : try : obj = record . files [ str ( key ) ] . get_version ( version_id = version_id ) return self . make_response ( obj = obj or abort ( 404 ) , pid = pid , record = record ) except KeyError : abort ( 404 )
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
13586	def add_object ( cls , attr , title = '' , display = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = attr . capitalize ( ) _display = display def _ref ( self , obj ) : field_obj = admin_obj_attr ( obj , attr ) if not field_obj : return '' return _obj_display ( field_obj , _display ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = attr setattr ( cls , fn_name , _ref )
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
587	def _constructClassificationRecord ( self ) : model = self . htm_prediction_model sp = model . _getSPRegion ( ) tm = model . _getTPRegion ( ) tpImp = tm . getSelf ( ) . _tfdr activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] score = numpy . in1d ( activeColumns , self . _prevPredictedColumns ) . sum ( ) score = ( self . _activeColumnCount - score ) / float ( self . _activeColumnCount ) spSize = sp . getParameter ( 'activeOutputCount' ) tpSize = tm . getParameter ( 'cellsPerColumn' ) * tm . getParameter ( 'columnCount' ) classificationVector = numpy . array ( [ ] ) if self . _vectorType == 'tpc' : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = tpImp . getLearnActiveStateT ( ) . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . _vectorType == 'sp_tpe' : classificationVector = numpy . zeros ( spSize + spSize ) if activeColumns . shape [ 0 ] > 0 : classificationVector [ activeColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . _vectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = tm . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = int ( model . getParameter ( '__numRunCalls' ) - 1 ) , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
9129	def store_populate_failed ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate_failed ( resource ) _store_helper ( action , session = session ) return action
7721	def get_history ( self ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : maxchars = from_utf8 ( child . prop ( "maxchars" ) ) if maxchars is not None : maxchars = int ( maxchars ) maxstanzas = from_utf8 ( child . prop ( "maxstanzas" ) ) if maxstanzas is not None : maxstanzas = int ( maxstanzas ) maxseconds = from_utf8 ( child . prop ( "maxseconds" ) ) if maxseconds is not None : maxseconds = int ( maxseconds ) since = None return HistoryParameters ( maxchars , maxstanzas , maxseconds , since )
1575	def hex_escape ( bin_str ) : printable = string . ascii_letters + string . digits + string . punctuation + ' ' return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str )
8942	def upload ( self , docs_base , release ) : return getattr ( self , '_to_' + self . target ) ( docs_base , release )
12220	def _make_wrapper ( self , func ) : @ wraps ( func ) def executor ( * args , ** kwargs ) : return self . execute ( args , kwargs ) executor . dispatch = self . dispatch executor . dispatch_first = self . dispatch_first executor . func = func executor . lookup = self . lookup return executor
6848	def find_working_password ( self , usernames = None , host_strings = None ) : r = self . local_renderer if host_strings is None : host_strings = [ ] if not host_strings : host_strings . append ( self . genv . host_string ) if usernames is None : usernames = [ ] if not usernames : usernames . append ( self . genv . user ) for host_string in host_strings : for username in usernames : passwords = [ ] passwords . append ( self . genv . user_default_passwords [ username ] ) passwords . append ( self . genv . user_passwords [ username ] ) passwords . append ( self . env . default_password ) for password in passwords : with settings ( warn_only = True ) : r . env . host_string = host_string r . env . password = password r . env . user = username ret = r . _local ( "sshpass -p '{password}' ssh -o StrictHostKeyChecking=no {user}@{host_string} echo hello" , capture = True ) if ret . return_code in ( 1 , 6 ) or 'hello' in ret : return host_string , username , password raise Exception ( 'No working login found.' )
13286	def get_dataframe_from_variable ( nc , data_var ) : time_var = nc . get_variables_by_attributes ( standard_name = 'time' ) [ 0 ] depth_vars = nc . get_variables_by_attributes ( axis = lambda v : v is not None and v . lower ( ) == 'z' ) depth_vars += nc . get_variables_by_attributes ( standard_name = lambda v : v in [ 'height' , 'depth' 'surface_altitude' ] , positive = lambda x : x is not None ) depth_var = None for d in depth_vars : try : if d . _name in data_var . coordinates . split ( " " ) or d . _name in data_var . dimensions : depth_var = d break except AttributeError : continue times = netCDF4 . num2date ( time_var [ : ] , units = time_var . units , calendar = getattr ( time_var , 'calendar' , 'standard' ) ) original_times_size = times . size if depth_var is None and hasattr ( data_var , 'sensor_depth' ) : depth_type = get_type ( data_var . sensor_depth ) depths = np . asarray ( [ data_var . sensor_depth ] * len ( times ) ) . flatten ( ) values = data_var [ : ] . flatten ( ) elif depth_var is None : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) depth_type = get_type ( depths ) values = data_var [ : ] . flatten ( ) else : depths = depth_var [ : ] depth_type = get_type ( depths ) if len ( data_var . shape ) > 1 : times = np . repeat ( times , depths . size ) depths = np . tile ( depths , original_times_size ) values = data_var [ : , : ] . flatten ( ) else : values = data_var [ : ] . flatten ( ) if getattr ( depth_var , 'positive' , 'down' ) . lower ( ) == 'up' : logger . warning ( "Converting depths to positive down before returning the DataFrame" ) depths = depths * - 1 if ( isinstance ( depths , np . ma . core . MaskedConstant ) or ( hasattr ( depths , 'mask' ) and depths . mask . all ( ) ) ) : depths = np . asarray ( [ np . nan ] * len ( times ) ) . flatten ( ) df = pd . DataFrame ( { 'time' : times , 'value' : values . astype ( data_var . dtype ) , 'unit' : data_var . units if hasattr ( data_var , 'units' ) else np . nan , 'depth' : depths . astype ( depth_type ) } ) df . set_index ( [ pd . DatetimeIndex ( df [ 'time' ] ) , pd . Float64Index ( df [ 'depth' ] ) ] , inplace = True ) return df
5712	def retrieve_descriptor ( descriptor ) : the_descriptor = descriptor if the_descriptor is None : the_descriptor = { } if isinstance ( the_descriptor , six . string_types ) : try : if os . path . isfile ( the_descriptor ) : with open ( the_descriptor , 'r' ) as f : the_descriptor = json . load ( f ) else : req = requests . get ( the_descriptor ) req . raise_for_status ( ) req . encoding = 'utf8' the_descriptor = req . json ( ) except ( IOError , requests . exceptions . RequestException ) as error : message = 'Unable to load JSON at "%s"' % descriptor six . raise_from ( exceptions . DataPackageException ( message ) , error ) except ValueError as error : message = 'Unable to parse JSON at "%s". %s' % ( descriptor , error ) six . raise_from ( exceptions . DataPackageException ( message ) , error ) if hasattr ( the_descriptor , 'read' ) : try : the_descriptor = json . load ( the_descriptor ) except ValueError as e : six . raise_from ( exceptions . DataPackageException ( str ( e ) ) , e ) if not isinstance ( the_descriptor , dict ) : msg = 'Data must be a \'dict\', but was a \'{0}\'' raise exceptions . DataPackageException ( msg . format ( type ( the_descriptor ) . __name__ ) ) return the_descriptor
13462	def video_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) return render ( request , 'video/video_list.html' , { 'event' : event , 'video_list' : event . eventvideo_set . all ( ) } )
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
3372	def choose_solver ( model , solver = None , qp = False ) : if solver is None : solver = model . problem else : model . solver = solver if qp and interface_to_str ( solver ) not in qp_solvers : solver = solvers [ get_solver_name ( qp = True ) ] return solver
367	def rotation ( x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : theta = np . pi / 180 * np . random . uniform ( - rg , rg ) else : theta = np . pi / 180 * rg rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x
11346	def handle_endtag ( self , tag ) : if tag in self . mathml_elements : self . fed . append ( "</{0}>" . format ( tag ) )
763	def createRecordSensor ( network , name , dataSource ) : regionType = "py.RecordSensor" regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) sensorRegion = network . regions [ name ] . getSelf ( ) sensorRegion . encoder = createEncoder ( ) network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) sensorRegion . dataSource = dataSource return sensorRegion
4635	def child ( self , offset256 ) : a = bytes ( self . pubkey ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . derive_from_seed ( s )
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
2478	def reset ( self ) : self . reset_creation_info ( ) self . reset_document ( ) self . reset_package ( ) self . reset_file_stat ( ) self . reset_reviews ( ) self . reset_annotations ( ) self . reset_extr_lics ( )
979	def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap
4153	def rst2md ( text ) : top_heading = re . compile ( r'^=+$\s^([\w\s-]+)^=+$' , flags = re . M ) text = re . sub ( top_heading , r'# \1' , text ) math_eq = re . compile ( r'^\.\. math::((?:.+)?(?:\n+^ .+)*)' , flags = re . M ) text = re . sub ( math_eq , lambda match : r'$${0}$$' . format ( match . group ( 1 ) . strip ( ) ) , text ) inline_math = re . compile ( r':math:`(.+)`' ) text = re . sub ( inline_math , r'$\1$' , text ) return text
13366	def apply ( f , obj , * args , ** kwargs ) : return vectorize ( f ) ( obj , * args , ** kwargs )
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
1041	def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]
6394	def sim_levenshtein ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 1 , 1 ) ) : return Levenshtein ( ) . sim ( src , tar , mode , cost )
12464	def print_message ( message = None ) : kwargs = { 'stdout' : sys . stdout , 'stderr' : sys . stderr , 'shell' : True } return subprocess . call ( 'echo "{0}"' . format ( message or '' ) , ** kwargs )
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
1361	def get_argument_endtime ( self ) : try : endtime = self . get_argument ( constants . PARAM_ENDTIME ) return endtime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
9469	def conference_list ( self , call_params ) : path = '/' + self . api_version + '/ConferenceList/' method = 'POST' return self . request ( path , method , call_params )
6541	def parse_python_file ( filepath ) : with _AST_CACHE_LOCK : if filepath not in _AST_CACHE : source = read_file ( filepath ) _AST_CACHE [ filepath ] = ast . parse ( source , filename = filepath ) return _AST_CACHE [ filepath ]
7964	def event ( self , event ) : logger . debug ( u"TCP transport event: {0}" . format ( event ) ) if self . _stream : event . stream = self . _stream self . _event_queue . put ( event )
851	def rewind ( self ) : super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) self . _recordCount = 0
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
11049	def _handle_field_value ( self , field , value ) : if field == 'event' : self . _event = value elif field == 'data' : self . _data_lines . append ( value ) elif field == 'id' : pass elif field == 'retry' : pass
6618	def _expand_tuple ( path_cfg , alias_dict , overriding_kargs ) : new_path_cfg = path_cfg [ 0 ] new_overriding_kargs = path_cfg [ 1 ] . copy ( ) new_overriding_kargs . update ( overriding_kargs ) return expand_path_cfg ( new_path_cfg , overriding_kargs = new_overriding_kargs , alias_dict = alias_dict )
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
6052	def sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels , mask , unmasked_sparse_grid_pixel_centres ) : pix_to_full_pix = np . zeros ( total_sparse_pixels ) pixel_index = 0 for full_pixel_index in range ( unmasked_sparse_grid_pixel_centres . shape [ 0 ] ) : y = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ full_pixel_index , 1 ] if not mask [ y , x ] : pix_to_full_pix [ pixel_index ] = full_pixel_index pixel_index += 1 return pix_to_full_pix
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
2213	def delete ( path , verbose = False ) : if not os . path . exists ( path ) : if os . path . islink ( path ) : if verbose : print ( 'Deleting broken link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : print ( 'Deleting broken directory link="{}"' . format ( path ) ) os . rmdir ( path ) elif os . path . isfile ( path ) : if verbose : print ( 'Deleting broken file link="{}"' . format ( path ) ) os . unlink ( path ) else : if verbose : print ( 'Not deleting non-existant path="{}"' . format ( path ) ) else : if os . path . islink ( path ) : if verbose : print ( 'Deleting symbolic link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isfile ( path ) : if verbose : print ( 'Deleting file="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) if sys . platform . startswith ( 'win32' ) : from ubelt import _win32_links _win32_links . _win32_rmtree ( path , verbose = verbose ) else : import shutil shutil . rmtree ( path )
10627	def Hfr ( self , Hfr ) : self . _Hfr = Hfr self . _T = self . _calculate_T ( Hfr )
8644	def update_track ( session , track_id , latitude , longitude , stop_tracking = False ) : tracking_data = { 'track_point' : { 'latitude' : latitude , 'longitude' : longitude , } , 'stop_tracking' : stop_tracking } response = make_put_request ( session , 'tracks/{}' . format ( track_id ) , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotUpdatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7112	def serve ( self , port = 62000 ) : from http . server import HTTPServer , CGIHTTPRequestHandler os . chdir ( self . log_folder ) httpd = HTTPServer ( ( '' , port ) , CGIHTTPRequestHandler ) print ( "Starting LanguageBoard on port: " + str ( httpd . server_port ) ) webbrowser . open ( 'http://0.0.0.0:{}' . format ( port ) ) httpd . serve_forever ( )
13295	def convert_lsstdoc_tex ( content , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : augmented_content = '\n' . join ( ( LSSTDOC_MACROS , content ) ) return convert_text ( augmented_content , 'latex' , to_fmt , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args )
8840	def missing_some ( data , min_required , args ) : if min_required < 1 : return [ ] found = 0 not_found = object ( ) ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) else : found += 1 if found >= min_required : return [ ] return ret
1017	def _adaptSegment ( self , segUpdate ) : trimSegment = False c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) if segment is not None : if self . verbosity >= 4 : print "Reinforcing segment #%d for cell[%d,%d]" % ( segment . segID , c , i ) print " before:" , segment . debugPrint ( ) segment . lastActiveIteration = self . lrnIterationIdx segment . positiveActivations += 1 segment . dutyCycle ( active = True ) lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print " after:" , segment . debugPrint ( ) else : newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print "New segment #%d for cell[%d,%d]" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
4650	def appendSigner ( self , accounts , permission ) : assert permission in self . permission_types , "Invalid permission" if self . blockchain . wallet . locked ( ) : raise WalletLocked ( ) if not isinstance ( accounts , ( list , tuple , set ) ) : accounts = [ accounts ] for account in accounts : if account not in self . signing_accounts : if isinstance ( account , self . publickey_class ) : self . appendWif ( self . blockchain . wallet . getPrivateKeyForPublicKey ( str ( account ) ) ) else : accountObj = self . account_class ( account , blockchain_instance = self . blockchain ) required_treshold = accountObj [ permission ] [ "weight_threshold" ] keys = self . _fetchkeys ( accountObj , permission , required_treshold = required_treshold ) if not keys and permission != "owner" : keys . extend ( self . _fetchkeys ( accountObj , "owner" , required_treshold = required_treshold ) ) for x in keys : self . appendWif ( x [ 0 ] ) self . signing_accounts . append ( account )
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
7479	def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) os . mkdir ( data . tmpdir ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " building clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp" ) usort = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) async1 = "" if not os . path . exists ( usort ) or force : LOGGER . info ( "building reads file -- loading utemp file into mem" ) async1 = lbview . apply ( sort_seeds , * ( uhandle , usort ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async1 . ready ( ) : break else : time . sleep ( 0.1 ) async2 = lbview . apply ( count_seeds , usort ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 1 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async2 . ready ( ) : break else : time . sleep ( 0.1 ) nseeds = async2 . result ( ) async3 = lbview . apply ( sub_build_clustbits , * ( data , usort , nseeds ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 2 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async3 . ready ( ) : break else : time . sleep ( 0.1 ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 3 , printstr . format ( elapsed ) , spacer = data . _spacer ) print ( "" ) for job in [ async1 , async2 , async3 ] : try : if not job . successful ( ) : raise IPyradWarningExit ( job . result ( ) ) except AttributeError : pass
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , ** ranker_params ) return scores , sim_mat
6374	def pr_lmean ( self ) : r precision = self . precision ( ) recall = self . recall ( ) if not precision or not recall : return 0.0 elif precision == recall : return precision return ( precision - recall ) / ( math . log ( precision ) - math . log ( recall ) )
12385	def parse ( text , encoding = 'utf8' ) : if isinstance ( text , six . binary_type ) : text = text . decode ( encoding ) return Query ( text , split_segments ( text ) )
6724	def exists ( name = None , group = None , release = None , except_release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) return instances
13234	def make_aware ( value , timezone ) : if hasattr ( timezone , 'localize' ) and value not in ( datetime . datetime . min , datetime . datetime . max ) : return timezone . localize ( value , is_dst = None ) else : return value . replace ( tzinfo = timezone )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
3769	def none_and_length_check ( all_inputs , length = None ) : r if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
3499	def assess ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True else : results = dict ( ) results [ 'precursors' ] = assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff ) results [ 'products' ] = assess_component ( model , reaction , 'products' , flux_coefficient_cutoff ) return results
10623	def extract ( self , other ) : if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mass ( other ) elif self . _is_compound_mass_tuple ( other ) : return self . _extract_compound_mass ( other [ 0 ] , other [ 1 ] ) elif type ( other ) is str : return self . _extract_compound ( other ) elif type ( other ) is Material : return self . _extract_material ( other ) else : raise TypeError ( "Invalid extraction argument." )
1223	def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
5266	def sentencecase ( string ) : joiner = ' ' string = re . sub ( r"[\-_\.\s]" , joiner , str ( string ) ) if not string : return string return capitalcase ( trimcase ( re . sub ( r"[A-Z]" , lambda matched : joiner + lowercase ( matched . group ( 0 ) ) , string ) ) )
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : if not bool ( todo ) : break if full_tz in todo : tz_map [ full_tz ] = full_tz todo . discard ( full_tz ) continue if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
9990	def get_object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get_object ( "." . join ( parts ) ) else : return self . _namespace_impl [ child ]
1678	def IsInAlphabeticalOrder ( self , clean_lines , linenum , header_path ) : if ( self . _last_header > header_path and Match ( r'^\s*#\s*include\b' , clean_lines . elided [ linenum - 1 ] ) ) : return False return True
11242	def indent_css ( f , output ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) . rstrip ( ) if len ( string ) > 0 : if string [ - 1 ] == ";" : output . write ( " " + string + "\n" ) else : output . write ( string + "\n" ) output . close ( ) f . close ( )
1347	def clone ( git_uri ) : hash_digest = sha256_hash ( git_uri ) local_path = home_directory_path ( FOLDER , hash_digest ) exists_locally = path_exists ( local_path ) if not exists_locally : _clone_repo ( git_uri , local_path ) else : logging . info ( "Git repository already exists locally." ) return local_path
1713	def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
9938	def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app_location = self . storages [ app ] . location if app_location not in searched_locations : searched_locations . append ( app_location ) match = self . find_in_app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
4473	def __recursive_transform ( self , jam , steps ) : if len ( steps ) > 0 : head_transformer = steps [ 0 ] [ 1 ] for t_jam in head_transformer . transform ( jam ) : for q in self . __recursive_transform ( t_jam , steps [ 1 : ] ) : yield q else : yield jam
8647	def accept_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'accept' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotAcceptedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
5113	def clear ( self ) : self . _t = 0 self . num_events = 0 self . num_agents = np . zeros ( self . nE , int ) self . _fancy_heap = PriorityQueue ( ) self . _prev_edge = None self . _initialized = False self . reset_colors ( ) for q in self . edge2queue : q . clear ( )
8493	def _handle_module ( args ) : module = _get_module_filename ( args . module ) if not module : _error ( "Could not load module or package: %r" , args . module ) elif isinstance ( module , Unparseable ) : _error ( "Could not determine module source: %r" , args . module ) _parse_and_output ( module , args )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
8539	def get_disk_image_by_name ( pbclient , location , image_name ) : all_images = pbclient . list_images ( ) matching = [ i for i in all_images [ 'items' ] if i [ 'properties' ] [ 'name' ] == image_name and i [ 'properties' ] [ 'imageType' ] == "HDD" and i [ 'properties' ] [ 'location' ] == location ] return matching
6702	def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn_only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded
9999	def cells_to_series ( cells , args ) : paramlen = len ( cells . formula . parameters ) is_multidx = paramlen > 1 if len ( cells . data ) == 0 : data = { } indexes = None elif paramlen == 0 : data = list ( cells . data . values ( ) ) indexes = [ np . nan ] else : if len ( args ) > 0 : defaults = tuple ( param . default for param in cells . formula . signature . parameters . values ( ) ) updated_args = [ ] for arg in args : if len ( arg ) > paramlen : arg = arg [ : paramlen ] elif len ( arg ) < paramlen : arg += defaults [ len ( arg ) : ] updated_args . append ( arg ) items = [ ( arg , cells . data [ arg ] ) for arg in updated_args if arg in cells . data ] else : items = [ ( key , value ) for key , value in cells . data . items ( ) ] if not is_multidx : items = [ ( key [ 0 ] , value ) for key , value in items ] if len ( items ) == 0 : indexes , data = None , { } else : indexes , data = zip ( * items ) if is_multidx : indexes = pd . MultiIndex . from_tuples ( indexes ) result = pd . Series ( data = data , name = cells . name , index = indexes ) if indexes is not None and any ( i is not np . nan for i in indexes ) : result . index . names = list ( cells . formula . parameters ) return result
6484	def _process_field_values ( request ) : return { field_key : request . POST [ field_key ] for field_key in request . POST if field_key in course_discovery_filter_fields ( ) }
1603	def to_table ( metrics ) : all_queries = tracker_access . metric_queries ( ) m = tracker_access . queries_map ( ) names = metrics . values ( ) [ 0 ] . keys ( ) stats = [ ] for n in names : info = [ n ] for field in all_queries : try : info . append ( str ( metrics [ field ] [ n ] ) ) except KeyError : pass stats . append ( info ) header = [ 'container id' ] + [ m [ k ] for k in all_queries if k in metrics . keys ( ) ] return stats , header
11221	def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
7899	def request_configuration_form ( self ) : iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "get" ) iq . new_query ( MUC_OWNER_NS , "query" ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_form_success , self . process_configuration_form_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
12882	def main ( world_cls , referee_cls , gui_cls , gui_actor_cls , ai_actor_cls , theater_cls = PygletTheater , default_host = DEFAULT_HOST , default_port = DEFAULT_PORT , argv = None ) : import sys , os , docopt , nonstdlib exe_name = os . path . basename ( sys . argv [ 0 ] ) usage = main . __doc__ . format ( ** locals ( ) ) . strip ( ) args = docopt . docopt ( usage , argv or sys . argv [ 1 : ] ) num_guis = int ( args [ '<num_guis>' ] or 1 ) num_ais = int ( args [ '<num_ais>' ] or 0 ) host , port = args [ '--host' ] , int ( args [ '--port' ] ) logging . basicConfig ( format = '%(levelname)s: %(name)s: %(message)s' , level = nonstdlib . verbosity ( args [ '--verbose' ] ) , ) if args [ 'debug' ] : print ( ) game = MultiplayerDebugger ( world_cls , referee_cls , gui_cls , gui_actor_cls , num_guis , ai_actor_cls , num_ais , theater_cls , host , port ) else : game = theater_cls ( ) ai_actors = [ ai_actor_cls ( ) for i in range ( num_ais ) ] if args [ 'sandbox' ] : game . gui = gui_cls ( ) game . initial_stage = UniplayerGameStage ( world_cls ( ) , referee_cls ( ) , gui_actor_cls ( ) , ai_actors ) game . initial_stage . successor = PostgameSplashStage ( ) if args [ 'client' ] : game . gui = gui_cls ( ) game . initial_stage = ClientConnectionStage ( world_cls ( ) , gui_actor_cls ( ) , host , port ) if args [ 'server' ] : game . initial_stage = ServerConnectionStage ( world_cls ( ) , referee_cls ( ) , num_guis , ai_actors , host , port ) game . play ( )
11427	def record_make_all_subfields_volatile ( rec ) : for tag in rec . keys ( ) : for field_position , field in enumerate ( rec [ tag ] ) : for subfield_position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record_modify_subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield_position , field_position_local = field_position )
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
3071	def _get_flow_for_token ( csrf_token ) : flow_pickle = session . pop ( _FLOW_KEY . format ( csrf_token ) , None ) if flow_pickle is None : return None else : return pickle . loads ( flow_pickle )
6648	def _loadConfig ( self ) : config_dicts = [ self . additional_config , self . app_config ] + [ t . getConfig ( ) for t in self . hierarchy ] config_blame = [ _mirrorStructure ( self . additional_config , 'command-line config' ) , _mirrorStructure ( self . app_config , 'application\'s config.json' ) , ] + [ _mirrorStructure ( t . getConfig ( ) , t . getName ( ) ) for t in self . hierarchy ] self . config = _mergeDictionaries ( * config_dicts ) self . config_blame = _mergeDictionaries ( * config_blame )
13054	def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
5599	def open ( self , tile , process , ** kwargs ) : return InputTile ( tile , process , kwargs . get ( "resampling" , None ) )
9275	def apply_exclude_tags_regex ( self , all_tags ) : filtered = [ ] for tag in all_tags : if not re . match ( self . options . exclude_tags_regex , tag [ "name" ] ) : filtered . append ( tag ) if len ( all_tags ) == len ( filtered ) : self . warn_if_nonmatching_regex ( ) return filtered
5393	def _make_environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers_util . get_file_environment_variables ( inputs ) ) env . update ( providers_util . get_file_environment_variables ( outputs ) ) env . update ( providers_util . get_file_environment_variables ( mounts ) ) return env
1021	def buildSequencePool ( numSequences = 10 , seqLen = [ 2 , 3 , 4 ] , numPatterns = 5 , numOnBitsPerPattern = 3 , patternOverlap = 0 , ** kwargs ) : patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] for _ in xrange ( numSequences ) : sequence = [ ] length = random . choice ( seqLen ) for _ in xrange ( length ) : patIdx = random . choice ( xrange ( numPatterns ) ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print "\nTraining sequences" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )
12292	def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
8725	def at_time ( cls , at , target ) : at = cls . _from_timestamp ( at ) cmd = cls . from_datetime ( at ) cmd . delay = at - now ( ) cmd . target = target return cmd
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
10994	def randomize_parameters ( self , ptp = 0.2 , fourier = False , vmin = None , vmax = None ) : if vmin is not None and vmax is not None : ptp = vmax - vmin elif vmax is not None and vmin is None : vmin = vmax - ptp elif vmin is not None and vmax is None : vmax = vmin + ptp else : vmax = 1.0 vmin = vmax - ptp self . set_values ( self . category + '-scale' , 1.0 ) self . set_values ( self . category + '-off' , 0.0 ) for k , v in iteritems ( self . poly_params ) : norm = ( self . zorder + 1.0 ) * 2 self . set_values ( k , ptp * ( np . random . rand ( ) - 0.5 ) / norm ) for i , p in enumerate ( self . barnes_params ) : N = len ( p ) if fourier : t = ( ( np . random . rand ( N ) - 0.5 ) + 1.j * ( np . random . rand ( N ) - 0.5 ) ) / ( np . arange ( N ) + 1 ) q = np . real ( np . fft . ifftn ( t ) ) / ( i + 1 ) else : t = ptp * np . sqrt ( N ) * ( np . random . rand ( N ) - 0.5 ) q = np . cumsum ( t ) / ( i + 1 ) q = ptp * q / q . ptp ( ) / len ( self . barnes_params ) q -= q . mean ( ) self . set_values ( p , q ) self . _norm_stat = [ ptp , vmin ] if self . shape : self . initialize ( ) if self . _parent : param = self . category + '-scale' self . trigger_update ( param , self . get_values ( param ) )
7844	def set_type ( self , item_type ) : if not item_type : raise ValueError ( "Type is required in DiscoIdentity" ) item_type = unicode ( item_type ) self . xmlnode . setProp ( "type" , item_type . encode ( "utf-8" ) )
11238	def imap_send ( func , gen ) : gen = iter ( gen ) assert _is_just_started ( gen ) yielder = yield_from ( gen ) for item in yielder : with yielder : yielder . send ( func ( ( yield item ) ) ) return_ ( yielder . result )
10750	def download ( self , bands , download_dir = None , metadata = False ) : super ( GoogleDownloader , self ) . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) image_list = [ ] band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download_dir is None : download_dir = DOWNLOAD_DIR check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) filename = "%s%s" % ( self . sceneInfo . name , self . __remote_file_ext ) downloaded = self . fetch ( self . remote_file_url , download_dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) logger . debug ( 'Starting data extraction in directory ' , folder_path ) tar . extractall ( folder_path ) remove ( downloaded [ 0 ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image_list
2707	def top_sentences ( kernel , path ) : key_sent = { } i = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : graf = meta [ "graf" ] tagged_sent = [ WordNode . _make ( x ) for x in graf ] text = " " . join ( [ w . raw for w in tagged_sent ] ) m_sent = mh_digest ( [ str ( w . word_id ) for w in tagged_sent ] ) dist = sum ( [ m_sent . jaccard ( m ) * rl . rank for rl , m in kernel ] ) key_sent [ text ] = ( dist , i ) i += 1 for text , ( dist , i ) in sorted ( key_sent . items ( ) , key = lambda x : x [ 1 ] [ 0 ] , reverse = True ) : yield SummarySent ( dist = dist , idx = i , text = text )
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
10213	def calculate_subgraph_edge_overlap ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Tuple [ Mapping [ str , EdgeSet ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , EdgeSet ] ] , Mapping [ str , Mapping [ str , float ] ] , ] : sg2edge = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : if not edge_has_annotation ( d , annotation ) : continue sg2edge [ d [ ANNOTATIONS ] [ annotation ] ] . add ( ( u , v ) ) subgraph_intersection = defaultdict ( dict ) subgraph_union = defaultdict ( dict ) result = defaultdict ( dict ) for sg1 , sg2 in itt . product ( sg2edge , repeat = 2 ) : subgraph_intersection [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] & sg2edge [ sg2 ] subgraph_union [ sg1 ] [ sg2 ] = sg2edge [ sg1 ] | sg2edge [ sg2 ] result [ sg1 ] [ sg2 ] = len ( subgraph_intersection [ sg1 ] [ sg2 ] ) / len ( subgraph_union [ sg1 ] [ sg2 ] ) return sg2edge , subgraph_intersection , subgraph_union , result
9500	def _get_instructions_bytes ( code , varnames = None , names = None , constants = None , cells = None , linestarts = None , line_offset = 0 ) : labels = dis . findlabels ( code ) extended_arg = 0 starts_line = None free = None n = len ( code ) i = 0 while i < n : op = code [ i ] offset = i if linestarts is not None : starts_line = linestarts . get ( i , None ) if starts_line is not None : starts_line += line_offset is_jump_target = i in labels i = i + 1 arg = None argval = None argrepr = '' if op >= dis . HAVE_ARGUMENT : arg = code [ i ] + code [ i + 1 ] * 256 + extended_arg extended_arg = 0 i = i + 2 if op == dis . EXTENDED_ARG : extended_arg = arg * 65536 argval = arg if op in dis . hasconst : argval , argrepr = dis . _get_const_info ( arg , constants ) elif op in dis . hasname : argval , argrepr = dis . _get_name_info ( arg , names ) elif op in dis . hasjrel : argval = i + arg argrepr = "to " + repr ( argval ) elif op in dis . haslocal : argval , argrepr = dis . _get_name_info ( arg , varnames ) elif op in dis . hascompare : argval = dis . cmp_op [ arg ] argrepr = argval elif op in dis . hasfree : argval , argrepr = dis . _get_name_info ( arg , cells ) elif op in dis . hasnargs : argrepr = "%d positional, %d keyword pair" % ( code [ i - 2 ] , code [ i - 1 ] ) yield dis . Instruction ( dis . opname [ op ] , op , arg , argval , argrepr , offset , starts_line , is_jump_target )
3805	def calculate_P ( self , T , P , method ) : r if method == ELI_HANLEY_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley_dense ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm , Vmg ) elif method == CHUNG_DENSE : Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T , P ) if hasattr ( self . mug , '__call__' ) else self . mug kg = chung_dense ( T , self . MW , self . Tc , self . Vc , self . omega , Cvgm , Vmg , mug , self . dipole ) elif method == STIEL_THODOS_DENSE : kg = self . T_dependent_property ( T ) Vmg = self . Vmg ( T , P ) if hasattr ( self . Vmg , '__call__' ) else self . Vmg kg = stiel_thodos_dense ( T , self . MW , self . Tc , self . Pc , self . Vc , self . Zc , Vmg , kg ) elif method == COOLPROP : kg = PropsSI ( 'L' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : kg = self . interpolate_P ( T , P , method ) return kg
7393	def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
11264	def stdout ( prev , endl = '\n' , thru = False ) : for i in prev : sys . stdout . write ( str ( i ) + endl ) if thru : yield i
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
11973	def convert ( ip , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( ip , notation , inotation , _check = check , _isnm = False )
4659	def as_quote ( self , quote ) : if quote == self [ "quote" ] [ "symbol" ] : return self . copy ( ) elif quote == self [ "base" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
10915	def _check_groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj_get_positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj_get_radii ( ) . size ) == np . sort ( ans ) ) . all ( )
5428	def _validate_job_and_task_arguments ( job_params , task_descriptors ) : if not task_descriptors : return task_params = task_descriptors [ 0 ] . task_params from_jobs = { label . name for label in job_params [ 'labels' ] } from_tasks = { label . name for label in task_params [ 'labels' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for labels on the command-line and in the --tasks file must not ' 'be repeated: {}' . format ( ',' . join ( intersect ) ) ) from_jobs = { item . name for item in job_params [ 'envs' ] | job_params [ 'inputs' ] | job_params [ 'outputs' ] } from_tasks = { item . name for item in task_params [ 'envs' ] | task_params [ 'inputs' ] | task_params [ 'outputs' ] } intersect = from_jobs & from_tasks if intersect : raise ValueError ( 'Names for envs, inputs, and outputs on the command-line and in the ' '--tasks file must not be repeated: {}' . format ( ',' . join ( intersect ) ) )
132	def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
5136	def get_class_traits ( klass ) : source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
519	def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
3582	def _get_objects_by_path ( self , paths ) : return map ( lambda x : self . _bus . get_object ( 'org.bluez' , x ) , paths )
13427	def delete_messages ( self , messages ) : url = "/2/messages/?%s" % urlencode ( [ ( 'ids' , "," . join ( messages ) ) ] ) data = self . _delete_resource ( url ) return data
11806	def viterbi_segment ( text , P ) : n = len ( text ) words = [ '' ] + list ( text ) best = [ 1.0 ] + [ 0.0 ] * n for i in range ( n + 1 ) : for j in range ( 0 , i ) : w = text [ j : i ] if P [ w ] * best [ i - len ( w ) ] >= best [ i ] : best [ i ] = P [ w ] * best [ i - len ( w ) ] words [ i ] = w sequence = [ ] i = len ( words ) - 1 while i > 0 : sequence [ 0 : 0 ] = [ words [ i ] ] i = i - len ( words [ i ] ) return sequence , best [ - 1 ]
5535	def read ( self , output_tile ) : if self . config . mode not in [ "readonly" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be readonly, continue or overwrite" ) if isinstance ( output_tile , tuple ) : output_tile = self . config . output_pyramid . tile ( * output_tile ) elif isinstance ( output_tile , BufferedTile ) : pass else : raise TypeError ( "output_tile must be tuple or BufferedTile" ) return self . config . output . read ( output_tile )
5456	def _validate_label ( cls , name , value ) : cls . _check_label_name ( name ) cls . _check_label_value ( value ) if not cls . _allow_reserved_keys and name in RESERVED_LABELS : raise ValueError ( 'Label flag (%s=...) must not use reserved keys: %r' % ( name , list ( RESERVED_LABELS ) ) )
5617	def clean_geometry_type ( geometry , target_type , allow_multipart = True ) : multipart_geoms = { "Point" : MultiPoint , "LineString" : MultiLineString , "Polygon" : MultiPolygon , "MultiPoint" : MultiPoint , "MultiLineString" : MultiLineString , "MultiPolygon" : MultiPolygon } if target_type not in multipart_geoms . keys ( ) : raise TypeError ( "target type is not supported: %s" % target_type ) if geometry . geom_type == target_type : return geometry elif allow_multipart : target_multipart_type = multipart_geoms [ target_type ] if geometry . geom_type == "GeometryCollection" : return target_multipart_type ( [ clean_geometry_type ( g , target_type , allow_multipart ) for g in geometry ] ) elif any ( [ isinstance ( geometry , target_multipart_type ) , multipart_geoms [ geometry . geom_type ] == target_multipart_type ] ) : return geometry raise GeometryTypeError ( "geometry type does not match: %s, %s" % ( geometry . geom_type , target_type ) )
12036	def dictFlat ( l ) : if type ( l ) is dict : return [ l ] if "numpy" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts
13106	def add_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
9117	def reset_jails ( confirm = True , keep_cleanser_master = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) reset_cleansers ( confirm = False ) jails = [ 'appserver' , 'webserver' , 'worker' ] if not value_asbool ( keep_cleanser_master ) : jails . append ( 'cleanser' ) with fab . warn_only ( ) : for jail in jails : fab . run ( 'ezjail-admin delete -fw {jail}' . format ( jail = jail ) ) fab . run ( 'rm /usr/jails/cleanser/usr/home/cleanser/.ssh/authorized_keys' )
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , ** kwargs )
7661	def trim ( self , start_time , end_time , strict = False ) : if end_time <= start_time : raise ParameterError ( 'end_time must be greater than start_time.' ) if self . duration is None : orig_time = start_time orig_duration = end_time - start_time warnings . warn ( "Annotation.duration is not defined, cannot check " "for temporal intersection, assuming the annotation " "is valid between start_time and end_time." ) else : orig_time = self . time orig_duration = self . duration if start_time > ( orig_time + orig_duration ) or ( end_time < orig_time ) : warnings . warn ( 'Time range defined by [start_time,end_time] does not ' 'intersect with the time range spanned by this annotation, ' 'the trimmed annotation will be empty.' ) trim_start = self . time trim_end = trim_start else : trim_start = max ( orig_time , start_time ) trim_end = min ( orig_time + orig_duration , end_time ) ann_trimmed = Annotation ( self . namespace , data = None , annotation_metadata = self . annotation_metadata , sandbox = self . sandbox , time = trim_start , duration = trim_end - trim_start ) for obs in self . data : obs_start = obs . time obs_end = obs_start + obs . duration if obs_start < trim_end and obs_end > trim_start : new_start = max ( obs_start , trim_start ) new_end = min ( obs_end , trim_end ) new_duration = new_end - new_start if ( ( not strict ) or ( new_start == obs_start and new_end == obs_end ) ) : ann_trimmed . append ( time = new_start , duration = new_duration , value = obs . value , confidence = obs . confidence ) if 'trim' not in ann_trimmed . sandbox . keys ( ) : ann_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ] ) else : ann_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time , 'trim_start' : trim_start , 'trim_end' : trim_end } ) return ann_trimmed
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
8656	def get_threads ( session , query ) : response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1611	def ParseNolintSuppressions ( filename , raw_line , linenum , error ) : matched = Search ( r'\bNOLINT(NEXTLINE)?\b(\([^)]+\))?' , raw_line ) if matched : if matched . group ( 1 ) : suppressed_line = linenum + 1 else : suppressed_line = linenum category = matched . group ( 2 ) if category in ( None , '(*)' ) : _error_suppressions . setdefault ( None , set ( ) ) . add ( suppressed_line ) else : if category . startswith ( '(' ) and category . endswith ( ')' ) : category = category [ 1 : - 1 ] if category in _ERROR_CATEGORIES : _error_suppressions . setdefault ( category , set ( ) ) . add ( suppressed_line ) elif category not in _LEGACY_ERROR_CATEGORIES : error ( filename , linenum , 'readability/nolint' , 5 , 'Unknown NOLINT error category: %s' % category )
4229	def make_formatter ( format_name ) : if "json" in format_name : from json import dumps import datetime def jsonhandler ( obj ) : obj . isoformat ( ) if isinstance ( obj , ( datetime . datetime , datetime . date ) ) else obj if format_name == "prettyjson" : def jsondumps ( data ) : return dumps ( data , default = jsonhandler , indent = 2 , separators = ( ',' , ': ' ) ) else : def jsondumps ( data ) : return dumps ( data , default = jsonhandler ) def jsonify ( data ) : if isinstance ( data , dict ) : print ( jsondumps ( data ) ) elif isinstance ( data , list ) : print ( jsondumps ( [ device . _asdict ( ) for device in data ] ) ) else : print ( dumps ( { 'result' : data } ) ) return jsonify else : def printer ( data ) : if isinstance ( data , dict ) : print ( data ) else : for row in data : print ( row ) return printer
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( ** jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
3755	def Carcinogen ( CASRN , AvailableMethods = False , Method = None ) : r methods = [ COMBINED , IARC , NTP ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == IARC : if CASRN in IARC_data . index : status = IARC_codes [ IARC_data . at [ CASRN , 'group' ] ] else : status = UNLISTED elif Method == NTP : if CASRN in NTP_data . index : status = NTP_codes [ NTP_data . at [ CASRN , 'Listing' ] ] else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = Carcinogen ( CASRN , Method = method ) else : raise Exception ( 'Failure in in function' ) return status
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
4919	def course_run_detail ( self , request , pk , course_id ) : enterprise_customer_catalog = self . get_object ( ) course_run = enterprise_customer_catalog . get_course_run ( course_id ) if not course_run : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseRunDetailSerializer ( course_run , context = context ) return Response ( serializer . data )
9775	def logs ( ctx , past , follow , hide_time ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if past : try : response = PolyaxonClient ( ) . job . logs ( user , project_name , _job , stream = False ) get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . job . logs ( user , project_name , _job , message_handler = get_logs_handler ( handle_job_info = False , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
12400	def add ( self , requirements , required = None ) : if isinstance ( requirements , RequirementsManager ) : requirements = list ( requirements ) elif not isinstance ( requirements , list ) : requirements = [ requirements ] for req in requirements : name = req . project_name if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req , required = required ) elif required is not None : req . required = required add = True if name in self . requirements : for existing_req in self . requirements [ name ] : if req == existing_req : add = False break replace = False if ( req . specs and req . specs [ 0 ] [ 0 ] == '==' and existing_req . specs and existing_req . specs [ 0 ] [ 0 ] == '==' ) : if pkg_resources . parse_version ( req . specs [ 0 ] [ 1 ] ) < pkg_resources . parse_version ( existing_req . specs [ 0 ] [ 1 ] ) : req . requirement = existing_req . requirement replace = True if not ( req . specs and existing_req . specs ) : if existing_req . specs : req . requirement = existing_req . requirement replace = True if replace : req . required |= existing_req . required if existing_req . required_by and not req . required_by : req . required_by = existing_req . required_by self . requirements [ name ] . remove ( existing_req ) break if add : self . requirements [ name ] . append ( req )
11246	def future_value ( present_value , annual_rate , periods_per_year , years ) : rate_per_period = annual_rate / float ( periods_per_year ) periods = periods_per_year * years return present_value * ( 1 + rate_per_period ) ** periods
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
11210	def _set_tzdata ( self , tzobj ) : for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
72	def deepcopy ( self ) : bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )
6449	def pylint_color ( score ) : score_cutoffs = ( 10 , 9.5 , 8.5 , 7.5 , 5 ) for i in range ( len ( score_cutoffs ) ) : if score >= score_cutoffs [ i ] : return BADGE_COLORS [ i ] return BADGE_COLORS [ - 1 ]
1594	def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
10884	def slicer ( self ) : return tuple ( np . s_ [ l : r ] for l , r in zip ( * self . bounds ) )
2801	def convert_reduce_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reduce_sum ...' ) keepdims = params [ 'keepdims' ] > 0 axis = params [ 'axes' ] def target_layer ( x , keepdims = keepdims , axis = axis ) : import keras . backend as K return K . sum ( x , keepdims = keepdims , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
3171	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The store customer must have an id' ) if 'email_address' not in data : raise KeyError ( 'The store customer must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'opt_in_status' not in data : raise KeyError ( 'The store customer must have an opt_in_status' ) if data [ 'opt_in_status' ] not in [ True , False ] : raise TypeError ( 'The opt_in_status must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'customers' ) , data = data ) if response is not None : self . customer_id = response [ 'id' ] else : self . customer_id = None return response
8703	def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) self . __write ( 'C' ) sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
5201	def Operate ( self , command , index , op_type ) : OutstationApplication . process_point_value ( 'Operate' , command , index , op_type ) return opendnp3 . CommandStatus . SUCCESS
13893	def _HandleContentsEol ( contents , eol_style ) : if eol_style == EOL_STYLE_NONE : return contents if eol_style == EOL_STYLE_UNIX : return contents . replace ( '\r\n' , eol_style ) . replace ( '\r' , eol_style ) if eol_style == EOL_STYLE_MAC : return contents . replace ( '\r\n' , eol_style ) . replace ( '\n' , eol_style ) if eol_style == EOL_STYLE_WINDOWS : return contents . replace ( '\r\n' , '\n' ) . replace ( '\r' , '\n' ) . replace ( '\n' , EOL_STYLE_WINDOWS ) raise ValueError ( 'Unexpected eol style: %r' % ( eol_style , ) )
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
690	def encodeValue ( self , value , toBeAdded = True ) : encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) if toBeAdded : self . encodings . append ( encodedValue ) self . numEncodings += 1 return encodedValue
10355	def get_largest_component ( graph : BELGraph ) -> BELGraph : biggest_component_nodes = max ( nx . weakly_connected_components ( graph ) , key = len ) return subgraph ( graph , biggest_component_nodes )
11434	def _tag_matches_pattern ( tag , pattern ) : for char1 , char2 in zip ( tag , pattern ) : if char2 not in ( '%' , char1 ) : return False return True
8559	def update_lan ( self , datacenter_id , lan_id , name = None , public = None , ip_failover = None ) : data = { } if name : data [ 'name' ] = name if public is not None : data [ 'public' ] = public if ip_failover : data [ 'ipFailover' ] = ip_failover response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
9930	def post ( self , request ) : serializer = self . get_serializer ( data = request . data ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data ) return Response ( serializer . errors , status = status . HTTP_400_BAD_REQUEST )
1108	def get_grouped_opcodes ( self , n = 3 ) : codes = self . get_opcodes ( ) if not codes : codes = [ ( "equal" , 0 , 1 , 0 , 1 ) ] if codes [ 0 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ 0 ] codes [ 0 ] = tag , max ( i1 , i2 - n ) , i2 , max ( j1 , j2 - n ) , j2 if codes [ - 1 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ - 1 ] codes [ - 1 ] = tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) nn = n + n group = [ ] for tag , i1 , i2 , j1 , j2 in codes : if tag == 'equal' and i2 - i1 > nn : group . append ( ( tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) ) ) yield group group = [ ] i1 , j1 = max ( i1 , i2 - n ) , max ( j1 , j2 - n ) group . append ( ( tag , i1 , i2 , j1 , j2 ) ) if group and not ( len ( group ) == 1 and group [ 0 ] [ 0 ] == 'equal' ) : yield group
389	def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break return mask
1372	def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
10517	def setmax ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 1 return 1
3707	def Amgat ( xs , Vms ) : r if not none_and_length_check ( [ xs , Vms ] ) : raise Exception ( 'Function inputs are incorrect format' ) return mixing_simple ( xs , Vms )
9592	def set_window_position ( self , x , y , window_handle = 'current' ) : self . _execute ( Command . SET_WINDOW_POSITION , { 'x' : int ( x ) , 'y' : int ( y ) , 'window_handle' : window_handle } )
8490	def start_watching ( self ) : if self . watcher and self . watcher . is_alive ( ) : return self . watcher = Watcher ( ) self . watcher . start ( )
13687	def assert_equal_files ( self , obtained_fn , expected_fn , fix_callback = lambda x : x , binary = False , encoding = None ) : import os from zerotk . easyfs import GetFileContents , GetFileLines __tracebackhide__ = True import io def FindFile ( filename ) : data_filename = self . get_filename ( filename ) if os . path . isfile ( data_filename ) : return data_filename if os . path . isfile ( filename ) : return filename from . _exceptions import MultipleFilesNotFound raise MultipleFilesNotFound ( [ filename , data_filename ] ) obtained_fn = FindFile ( obtained_fn ) expected_fn = FindFile ( expected_fn ) if binary : obtained_lines = GetFileContents ( obtained_fn , binary = True ) expected_lines = GetFileContents ( expected_fn , binary = True ) assert obtained_lines == expected_lines else : obtained_lines = fix_callback ( GetFileLines ( obtained_fn , encoding = encoding ) ) expected_lines = GetFileLines ( expected_fn , encoding = encoding ) if obtained_lines != expected_lines : html_fn = os . path . splitext ( obtained_fn ) [ 0 ] + '.diff.html' html_diff = self . _generate_html_diff ( expected_fn , expected_lines , obtained_fn , obtained_lines ) with io . open ( html_fn , 'w' ) as f : f . write ( html_diff ) import difflib diff = [ 'FILES DIFFER:' , obtained_fn , expected_fn ] diff += [ 'HTML DIFF: %s' % html_fn ] diff += difflib . context_diff ( obtained_lines , expected_lines ) raise AssertionError ( '\n' . join ( diff ) + '\n' )
12277	def add_file_normal ( f , targetdir , generator , script , source ) : basename = os . path . basename ( f ) if targetdir != "." : relativepath = os . path . join ( targetdir , basename ) else : relativepath = basename relpath = os . path . relpath ( f , os . getcwd ( ) ) filetype = 'data' if script : filetype = 'script' if generator : filetype = 'generator' update = OrderedDict ( [ ( 'type' , filetype ) , ( 'generator' , generator ) , ( 'relativepath' , relativepath ) , ( 'content' , "" ) , ( 'source' , source ) , ( 'localfullpath' , f ) , ( 'localrelativepath' , relpath ) ] ) update = annotate_record ( update ) return ( basename , update )
4427	async def _seek ( self , ctx , * , time : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . is_playing : return await ctx . send ( 'Not playing.' ) seconds = time_rx . search ( time ) if not seconds : return await ctx . send ( 'You need to specify the amount of seconds to skip!' ) seconds = int ( seconds . group ( ) ) * 1000 if time . startswith ( '-' ) : seconds *= - 1 track_time = player . position + seconds await player . seek ( track_time ) await ctx . send ( f'Moved track to **{lavalink.Utils.format_time(track_time)}**' )
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
5527	def backend_version ( backend , childprocess = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) if not childprocess : return _backend_version ( backend ) else : return run_in_childprocess ( _backend_version , None , backend )
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( ** params )
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
863	def Enum ( * args , ** kwargs ) : def getLabel ( cls , val ) : return cls . __labels [ val ] def validate ( cls , val ) : return val in cls . __values def getValues ( cls ) : return list ( cls . __values ) def getLabels ( cls ) : return list ( cls . __labels . values ( ) ) def getValue ( cls , label ) : return cls . __labels [ label ] for arg in list ( args ) + kwargs . keys ( ) : if type ( arg ) is not str : raise TypeError ( "Enum arg {0} must be a string" . format ( arg ) ) if not __isidentifier ( arg ) : raise ValueError ( "Invalid enum value '{0}'. " "'{0}' is not a valid identifier" . format ( arg ) ) kwargs . update ( zip ( args , args ) ) newType = type ( "Enum" , ( object , ) , kwargs ) newType . __labels = dict ( ( v , k ) for k , v in kwargs . iteritems ( ) ) newType . __values = set ( newType . __labels . keys ( ) ) newType . getLabel = functools . partial ( getLabel , newType ) newType . validate = functools . partial ( validate , newType ) newType . getValues = functools . partial ( getValues , newType ) newType . getLabels = functools . partial ( getLabels , newType ) newType . getValue = functools . partial ( getValue , newType ) return newType
9441	def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
12864	def days_in_month ( year , month ) : eom = _days_per_month [ month - 1 ] if is_leap_year ( year ) and month == 2 : eom += 1 return eom
11176	def parse ( self , argv ) : if len ( argv ) < self . nargs : raise BadNumberOfArguments ( self . nargs , len ( argv ) ) if self . nargs == 1 : return self . parse_argument ( argv . pop ( 0 ) ) return [ self . parse_argument ( argv . pop ( 0 ) ) for tmp in range ( self . nargs ) ]
9264	def filter_merged_pull_requests ( self , pull_requests ) : if self . options . verbose : print ( "Fetching merge date for pull requests..." ) closed_pull_requests = self . fetcher . fetch_closed_pull_requests ( ) if not pull_requests : return [ ] pulls = copy . deepcopy ( pull_requests ) for pr in pulls : fetched_pr = None for fpr in closed_pull_requests : if fpr [ 'number' ] == pr [ 'number' ] : fetched_pr = fpr if fetched_pr : pr [ 'merged_at' ] = fetched_pr [ 'merged_at' ] closed_pull_requests . remove ( fetched_pr ) for pr in pulls : if not pr . get ( 'merged_at' ) : pulls . remove ( pr ) return pulls
12685	def pods ( self ) : if not self . xml_tree : return [ ] return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
2681	def create_function ( cfg , path_to_zip_file , use_s3 = False , s3_file = None ) : print ( 'Creating your new Lambda function' ) byte_stream = read ( path_to_zip_file , binary_file = True ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) account_id = get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' , ) , ) role = get_role_name ( cfg . get ( 'region' ) , account_id , cfg . get ( 'role' , 'lambda_basic_execution' ) , ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) buck_name = ( os . environ . get ( 'S3_BUCKET_NAME' ) or cfg . get ( 'bucket_name' ) ) func_name = ( os . environ . get ( 'LAMBDA_FUNCTION_NAME' ) or cfg . get ( 'function_name' ) ) print ( 'Creating lambda function with name: {}' . format ( func_name ) ) if use_s3 : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'S3Bucket' : '{}' . format ( buck_name ) , 'S3Key' : '{}' . format ( s3_file ) , } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } else : kwargs = { 'FunctionName' : func_name , 'Runtime' : cfg . get ( 'runtime' , 'python2.7' ) , 'Role' : role , 'Handler' : cfg . get ( 'handler' ) , 'Code' : { 'ZipFile' : byte_stream } , 'Description' : cfg . get ( 'description' , '' ) , 'Timeout' : cfg . get ( 'timeout' , 15 ) , 'MemorySize' : cfg . get ( 'memory_size' , 512 ) , 'VpcConfig' : { 'SubnetIds' : cfg . get ( 'subnet_ids' , [ ] ) , 'SecurityGroupIds' : cfg . get ( 'security_group_ids' , [ ] ) , } , 'Publish' : True , } if 'tags' in cfg : kwargs . update ( Tags = { key : str ( value ) for key , value in cfg . get ( 'tags' ) . items ( ) } ) if 'environment_variables' in cfg : kwargs . update ( Environment = { 'Variables' : { key : get_environment_variable_value ( value ) for key , value in cfg . get ( 'environment_variables' ) . items ( ) } , } , ) client . create_function ( ** kwargs ) concurrency = get_concurrency ( cfg ) if concurrency > 0 : client . put_function_concurrency ( FunctionName = func_name , ReservedConcurrentExecutions = concurrency )
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
8345	def find ( self , name = None , attrs = { } , recursive = True , text = None , ** kwargs ) : r = None l = self . findAll ( name , attrs , recursive , text , 1 , ** kwargs ) if l : r = l [ 0 ] return r
11850	def move_to ( self , thing , destination ) : "Move a thing to a new location." thing . bump = self . some_things_at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing_moved ( thing )
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
11129	def stats ( cls , traces ) : data = { } stats = { } for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
1316	def GetAllPixelColors ( self ) -> ctypes . Array : return self . GetPixelColorsOfRect ( 0 , 0 , self . Width , self . Height )
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
11762	def alphabeta_search ( state , game , d = 4 , cutoff_test = None , eval_fn = None ) : player = game . to_move ( state ) def max_value ( state , alpha , beta , depth ) : if cutoff_test ( state , depth ) : return eval_fn ( state ) v = - infinity for a in game . actions ( state ) : v = max ( v , min_value ( game . result ( state , a ) , alpha , beta , depth + 1 ) ) if v >= beta : return v alpha = max ( alpha , v ) return v def min_value ( state , alpha , beta , depth ) : if cutoff_test ( state , depth ) : return eval_fn ( state ) v = infinity for a in game . actions ( state ) : v = min ( v , max_value ( game . result ( state , a ) , alpha , beta , depth + 1 ) ) if v <= alpha : return v beta = min ( beta , v ) return v cutoff_test = ( cutoff_test or ( lambda state , depth : depth > d or game . terminal_test ( state ) ) ) eval_fn = eval_fn or ( lambda state : game . utility ( state , player ) ) return argmax ( game . actions ( state ) , lambda a : min_value ( game . result ( state , a ) , - infinity , infinity , 0 ) )
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
9815	def stop ( ctx , commit , yes ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not yes and not click . confirm ( "Are sure you want to stop notebook " "for project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without stopping notebook.' ) sys . exit ( 1 ) if commit is None : commit = True try : PolyaxonClient ( ) . project . stop_notebook ( user , project_name , commit ) Printer . print_success ( 'Notebook is being deleted' ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
7039	def object_info ( lcc_server , objectid , db_collection_id ) : urlparams = { 'objectid' : objectid , 'collection' : db_collection_id } urlqs = urlencode ( urlparams ) url = '%s/api/object?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting info for %s in collection %s from %s' % ( objectid , db_collection_id , lcc_server ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) objectinfo = json . loads ( resp . read ( ) ) [ 'result' ] return objectinfo except HTTPError as e : if e . code == 404 : LOGERROR ( 'additional info for object %s not ' 'found in collection: %s' % ( objectid , db_collection_id ) ) else : LOGERROR ( 'could not retrieve object info, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
11494	def get_default_api_key ( self , email , password ) : parameters = dict ( ) parameters [ 'email' ] = email parameters [ 'password' ] = password response = self . request ( 'midas.user.apikey.default' , parameters ) return response [ 'apikey' ]
4644	def get ( self , key , default = None ) : if key in self : return self . __getitem__ ( key ) else : return default
5650	def _remove_I_columns ( df ) : all_columns = list ( filter ( lambda el : el [ - 2 : ] == "_I" , df . columns ) ) for column in all_columns : del df [ column ]
13647	def get_fuel_prices ( self ) -> GetFuelPricesResponse : response = requests . get ( '{}/prices' . format ( API_URL_BASE ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetFuelPricesResponse . deserialize ( response . json ( ) )
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) return state
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
6883	def read_csvlc ( lcfile ) : if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped HATLC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading HATLC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) lcformat_check = infd . read ( 12 ) . decode ( ) if 'LCC-CSVLC' in lcformat_check : infd . close ( ) return read_lcc_csvlc ( lcfile ) else : infd . seek ( 0 ) lctext = infd . read ( ) . decode ( ) infd . close ( ) lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x for x in lccolumns if len ( x ) > 0 ] lcdict = _parse_csv_header ( lcheader ) lccolumns = [ x . split ( ',' ) for x in lccolumns ] lccolumns = list ( zip ( * lccolumns ) ) for colind , col in enumerate ( lcdict [ 'columns' ] ) : if ( col . split ( '_' ) [ 0 ] in LC_MAG_COLUMNS or col . split ( '_' ) [ 0 ] in LC_ERR_COLUMNS or col . split ( '_' ) [ 0 ] in LC_FLAG_COLUMNS ) : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col . split ( '_' ) [ 0 ] ] [ 2 ] ) for x in lccolumns [ colind ] ] ) elif col in COLUMNDEFS : lcdict [ col ] = np . array ( [ _smartcast ( x , COLUMNDEFS [ col ] [ 2 ] ) for x in lccolumns [ colind ] ] ) else : LOGWARNING ( 'lcdict col %s has no formatter available' % col ) continue return lcdict
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
11670	def _get_Ks ( self ) : "Ks as an array and type-checked." Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
2048	def new_address ( self , sender = None , nonce = None ) : if sender is not None and nonce is None : nonce = self . get_nonce ( sender ) new_address = self . calculate_new_address ( sender , nonce ) if sender is None and new_address in self : return self . new_address ( sender , nonce ) return new_address
4735	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.pci.env: invalid SSH environment" ) return 1 pci = cij . env_to_dict ( PREFIX , REQUIRED ) pci [ "BUS_PATH" ] = "/sys/bus/pci" pci [ "DEV_PATH" ] = os . sep . join ( [ pci [ "BUS_PATH" ] , "devices" , pci [ "DEV_NAME" ] ] ) cij . env_export ( PREFIX , EXPORTED , pci ) return 0
6535	def merge_dict ( dict1 , dict2 , merge_lists = False ) : merged = dict ( dict1 ) for key , value in iteritems ( dict2 ) : if isinstance ( merged . get ( key ) , dict ) : merged [ key ] = merge_dict ( merged [ key ] , value ) elif merge_lists and isinstance ( merged . get ( key ) , list ) : merged [ key ] = merge_list ( merged [ key ] , value ) else : merged [ key ] = value return merged
7074	def variable_index_gridsearch_magbin ( simbasedir , stetson_stdev_range = ( 1.0 , 20.0 ) , inveta_stdev_range = ( 1.0 , 20.0 ) , iqr_stdev_range = ( 1.0 , 20.0 ) , ngridpoints = 32 , ngridworkers = None ) : outdir = os . path . join ( simbasedir , 'recvar-threshold-pkls' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] magbinmedians = siminfo [ 'magrms' ] [ magcols [ 0 ] ] [ 'binned_sdssr_median' ] stetson_grid = np . linspace ( stetson_stdev_range [ 0 ] , stetson_stdev_range [ 1 ] , num = ngridpoints ) inveta_grid = np . linspace ( inveta_stdev_range [ 0 ] , inveta_stdev_range [ 1 ] , num = ngridpoints ) iqr_grid = np . linspace ( iqr_stdev_range [ 0 ] , iqr_stdev_range [ 1 ] , num = ngridpoints ) stet_inveta_iqr_grid = [ ] for stet in stetson_grid : for inveta in inveta_grid : for iqr in iqr_grid : grid_point = [ stet , inveta , iqr ] stet_inveta_iqr_grid . append ( grid_point ) grid_results = { 'stetson_grid' : stetson_grid , 'inveta_grid' : inveta_grid , 'iqr_grid' : iqr_grid , 'stet_inveta_iqr_grid' : stet_inveta_iqr_grid , 'magbinmedians' : magbinmedians , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'simbasedir' : os . path . abspath ( simbasedir ) , 'recovery' : [ ] } pool = mp . Pool ( ngridworkers ) for magbinmedian in magbinmedians : LOGINFO ( 'running stetson J-inveta grid-search ' 'for magbinmedian = %.3f...' % magbinmedian ) tasks = [ ( simbasedir , gp , magbinmedian ) for gp in stet_inveta_iqr_grid ] thisbin_results = pool . map ( magbin_varind_gridsearch_worker , tasks ) grid_results [ 'recovery' ] . append ( thisbin_results ) pool . close ( ) pool . join ( ) LOGINFO ( 'done.' ) with open ( os . path . join ( simbasedir , 'fakevar-recovery-per-magbin.pkl' ) , 'wb' ) as outfd : pickle . dump ( grid_results , outfd , pickle . HIGHEST_PROTOCOL ) return grid_results
8691	def put ( self , key ) : self . client . write ( self . _key_path ( key [ 'name' ] ) , ** key ) return self . _key_path ( key [ 'name' ] )
2792	def load ( self ) : data = self . get_data ( "certificates/%s" % self . id ) certificate = data [ "certificate" ] for attr in certificate . keys ( ) : setattr ( self , attr , certificate [ attr ] ) return self
11456	def load_config ( from_key , to_key ) : from . mappings import mappings kbs = { } for key , values in mappings [ 'config' ] . iteritems ( ) : parse_dict = { } for mapping in values : parse_dict [ mapping [ from_key ] ] = mapping [ to_key ] kbs [ key ] = parse_dict return kbs
8473	def _addConfig ( instance , config , parent_section ) : try : section_name = "{p}/{n}" . format ( p = parent_section , n = instance . NAME . lower ( ) ) config . add_section ( section_name ) for k in instance . CONFIG . keys ( ) : config . set ( section_name , k , instance . CONFIG [ k ] ) except Exception as e : print "[!] %s" % e
5168	def __intermediate_htmode ( self , radio ) : protocol = radio . pop ( 'protocol' ) channel_width = radio . pop ( 'channel_width' ) if 'htmode' in radio : return radio [ 'htmode' ] if protocol == '802.11n' : return 'HT{0}' . format ( channel_width ) elif protocol == '802.11ac' : return 'VHT{0}' . format ( channel_width ) return 'NONE'
1520	def get_remote_home ( host , cl_args ) : cmd = "echo ~" if not is_self ( host ) : cmd = ssh_remote_execute ( cmd , host , cl_args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
13702	def _after ( self , response ) : if getattr ( request , '_tracy_exclude' , False ) : return response duration = None if getattr ( request , '_tracy_start_time' , None ) : duration = monotonic ( ) - request . _tracy_start_time trace_id = None if getattr ( request , '_tracy_id' , None ) : trace_id = request . _tracy_id response . headers [ trace_header_id ] = trace_id trace_client = None if getattr ( request , '_tracy_client' , None ) : trace_client = request . _tracy_client d = { 'status_code' : response . status_code , 'url' : request . base_url , 'client_ip' : request . remote_addr , 'trace_name' : trace_client , 'trace_id' : trace_id , 'trace_duration' : duration } logger . info ( None , extra = d ) return response
3886	def _add_user_from_conv_part ( self , conv_part ) : user_ = User . from_conv_part_data ( conv_part , self . _self_user . id_ ) existing = self . _user_dict . get ( user_ . id_ ) if existing is None : logger . warning ( 'Adding fallback User with %s name "%s"' , user_ . name_type . name . lower ( ) , user_ . full_name ) self . _user_dict [ user_ . id_ ] = user_ return user_ else : existing . upgrade_name ( user_ ) return existing
10302	def set_percentage ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a : return 0.0 return len ( a & b ) / len ( a )
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
8879	def fit ( self , X , y = None ) : X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
10707	def create_vacation ( body ) : arequest = requests . post ( VACATIONS_URL , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '200' : _LOGGER . error ( "Failed to create vacation. " + status_code ) _LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
7666	def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
4666	def sign ( self , wifkeys , chain = None ) : if not chain : chain = self . get_default_prefix ( ) self . deriveDigest ( chain ) self . privkeys = [ ] for item in wifkeys : if item not in self . privkeys : self . privkeys . append ( item ) sigs = [ ] for wif in self . privkeys : signature = sign_message ( self . message , wif ) sigs . append ( Signature ( signature ) ) self . data [ "signatures" ] = Array ( sigs ) return self
6936	def cp_objectinfo_worker ( task ) : cpf , cpkwargs = task try : newcpf = update_checkplot_objectinfo ( cpf , ** cpkwargs ) return newcpf except Exception as e : LOGEXCEPTION ( 'failed to update objectinfo for %s' % cpf ) return None
745	def anomalyRemoveLabels ( self , start , end , labelFilter ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter )
11297	def _check_for_exceptions ( self , resp , multiple_rates ) : if resp [ 'rCode' ] != 100 : raise exceptions . get_exception_for_code ( resp [ 'rCode' ] ) ( resp ) results = resp [ 'results' ] if len ( results ) == 0 : raise exceptions . ZipTaxNoResults ( 'No results found' ) if len ( results ) > 1 and not multiple_rates : rates = [ result [ 'taxSales' ] for result in results ] if len ( set ( rates ) ) != 1 : raise exceptions . ZipTaxMultipleResults ( 'Multiple results found but requested only one' )
1614	def Match ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . match ( s )
10291	def enrich_unqualified ( graph : BELGraph ) : enrich_complexes ( graph ) enrich_composites ( graph ) enrich_reactions ( graph ) enrich_variants ( graph )
7914	def get_int_range_validator ( start , stop ) : def validate_int_range ( value ) : value = int ( value ) if value >= start and value < stop : return value raise ValueError ( "Not in <{0},{1}) range" . format ( start , stop ) ) return validate_int_range
2615	def cancel ( self , job_ids ) : for job in job_ids : logger . debug ( "Terminating job/proc_id: {0}" . format ( job ) ) if self . resources [ job ] [ 'proc' ] : proc = self . resources [ job ] [ 'proc' ] os . killpg ( os . getpgid ( proc . pid ) , signal . SIGTERM ) self . resources [ job ] [ 'status' ] = 'CANCELLED' elif self . resources [ job ] [ 'remote_pid' ] : cmd = "kill -- -$(ps -o pgid={} | grep -o '[0-9]*')" . format ( self . resources [ job ] [ 'remote_pid' ] ) retcode , stdout , stderr = self . channel . execute_wait ( cmd , self . cmd_timeout ) if retcode != 0 : logger . warning ( "Failed to kill PID: {} and child processes on {}" . format ( self . resources [ job ] [ 'remote_pid' ] , self . label ) ) rets = [ True for i in job_ids ] return rets
12916	def prune ( self , regex = r".*" ) : return filetree ( self . root , ignore = self . ignore , regex = regex )
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
2617	def read_state_file ( self , state_file ) : try : fh = open ( state_file , 'r' ) state = json . load ( fh ) self . vpc_id = state [ 'vpcID' ] self . sg_id = state [ 'sgID' ] self . sn_ids = state [ 'snIDs' ] self . instances = state [ 'instances' ] except Exception as e : logger . debug ( "Caught exception while reading state file: {0}" . format ( e ) ) raise e logger . debug ( "Done reading state from the local state file." )
5229	def _load_yaml_ ( file_name ) : if not os . path . exists ( file_name ) : return dict ( ) with open ( file_name , 'r' , encoding = 'utf-8' ) as fp : return YAML ( ) . load ( stream = fp )
9266	def sort_tags_by_date ( self , tags ) : if self . options . verbose : print ( "Sorting tags..." ) tags . sort ( key = lambda x : self . get_time_of_tag ( x ) ) tags . reverse ( ) return tags
11686	def get_user_details ( user_id ) : reasons = [ ] try : url = OSM_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : user_data = user_request . content xml_data = ET . fromstring ( user_data ) . getchildren ( ) [ 0 ] . getchildren ( ) changesets = [ i for i in xml_data if i . tag == 'changesets' ] [ 0 ] blocks = [ i for i in xml_data if i . tag == 'blocks' ] [ 0 ] if int ( changesets . get ( 'count' ) ) <= 5 : reasons . append ( 'New mapper' ) elif int ( changesets . get ( 'count' ) ) <= 30 : url = MAPBOX_USERS_API . format ( user_id = requests . compat . quote ( user_id ) ) user_request = requests . get ( url ) if user_request . status_code == 200 : mapping_days = int ( user_request . json ( ) . get ( 'extra' ) . get ( 'mapping_days' ) ) if mapping_days <= 5 : reasons . append ( 'New mapper' ) if int ( blocks . getchildren ( ) [ 0 ] . get ( 'count' ) ) > 1 : reasons . append ( 'User has multiple blocks' ) except Exception as e : message = 'Could not verify user of the changeset: {}, {}' print ( message . format ( user_id , str ( e ) ) ) return reasons
6122	def zoom_region ( self ) : where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
3821	async def delete_conversation ( self , delete_conversation_request ) : response = hangouts_pb2 . DeleteConversationResponse ( ) await self . _pb_request ( 'conversations/deleteconversation' , delete_conversation_request , response ) return response
3089	def locked_get ( self ) : credentials = None if self . _cache : json = self . _cache . get ( self . _key_name ) if json : credentials = client . Credentials . new_from_json ( json ) if credentials is None : entity = self . _get_entity ( ) if entity is not None : credentials = getattr ( entity , self . _property_name ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) ) if credentials and hasattr ( credentials , 'set_store' ) : credentials . set_store ( self ) return credentials
2227	def _digest_hasher ( hasher , hashlen , base ) : hex_text = hasher . hexdigest ( ) base_text = _convert_hexstr_base ( hex_text , base ) text = base_text [ : hashlen ] return text
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) matched_new_or_template = match . group ( 1 ) if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
2962	def expand_partitions ( containers , partitions ) : all_names = frozenset ( c . name for c in containers if not c . holy ) holy_names = frozenset ( c . name for c in containers if c . holy ) neutral_names = frozenset ( c . name for c in containers if c . neutral ) partitions = [ frozenset ( p ) for p in partitions ] unknown = set ( ) holy = set ( ) union = set ( ) for partition in partitions : unknown . update ( partition - all_names - holy_names ) holy . update ( partition - all_names ) union . update ( partition ) if unknown : raise BlockadeError ( 'Partitions contain unknown containers: %s' % list ( unknown ) ) if holy : raise BlockadeError ( 'Partitions contain holy containers: %s' % list ( holy ) ) leftover = all_names . difference ( union ) if leftover : partitions . append ( leftover ) if not neutral_names . issubset ( leftover ) : partitions . append ( neutral_names ) return partitions
4459	def between ( a , b , inclusive_min = True , inclusive_max = True ) : return RangeValue ( a , b , inclusive_min = inclusive_min , inclusive_max = inclusive_max )
13315	def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
5764	def _unarmor_pem ( data , password = None ) : object_type , headers , der_bytes = pem . unarmor ( data ) type_regex = '^((DSA|EC|RSA) PRIVATE KEY|ENCRYPTED PRIVATE KEY|PRIVATE KEY|PUBLIC KEY|RSA PUBLIC KEY|CERTIFICATE)' armor_type = re . match ( type_regex , object_type ) if not armor_type : raise ValueError ( pretty_message ( ) ) pem_header = armor_type . group ( 1 ) data = data . strip ( ) if pem_header in set ( [ 'RSA PRIVATE KEY' , 'DSA PRIVATE KEY' , 'EC PRIVATE KEY' ] ) : algo = armor_type . group ( 2 ) . lower ( ) return ( 'private key' , algo , _unarmor_pem_openssl_private ( headers , der_bytes , password ) ) key_type = pem_header . lower ( ) algo = None if key_type == 'encrypted private key' : key_type = 'private key' elif key_type == 'rsa public key' : key_type = 'public key' algo = 'rsa' return ( key_type , algo , der_bytes )
8811	def filter_factory ( global_conf , ** local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
10209	def check_write_permissions ( file ) : try : open ( file , 'a' ) except IOError : print ( "Can't open file {}. " "Please grant write permissions or change the path in your config" . format ( file ) ) sys . exit ( 1 )
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
5798	def _find_sections ( md_ast , sections , last , last_class , total_lines = None ) : def child_walker ( node ) : for child , entering in node . walker ( ) : if child == node : continue yield child , entering for child , entering in child_walker ( md_ast ) : if child . t == 'heading' : start_line = child . sourcepos [ 0 ] [ 0 ] if child . level == 2 : if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , start_line - 1 ) last . clear ( ) if child . level in set ( [ 3 , 5 ] ) : heading_elements = [ ] for heading_child , _ in child_walker ( child ) : heading_elements . append ( heading_child ) if len ( heading_elements ) != 2 : continue first = heading_elements [ 0 ] second = heading_elements [ 1 ] if first . t != 'code' : continue if second . t != 'text' : continue type_name = second . literal . strip ( ) identifier = first . literal . strip ( ) . replace ( '()' , '' ) . lstrip ( '.' ) if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , start_line - 1 ) last . clear ( ) if type_name == 'function' : if child . level != 3 : continue if type_name == 'class' : if child . level != 3 : continue last_class . append ( identifier ) if type_name in set ( [ 'method' , 'attribute' ] ) : if child . level != 5 : continue identifier = last_class [ - 1 ] + '.' + identifier last . update ( { 'type_name' : type_name , 'identifier' : identifier , 'start_line' : start_line , } ) elif child . t == 'block_quote' : find_sections ( child , sections , last , last_class ) if last : sections [ ( last [ 'type_name' ] , last [ 'identifier' ] ) ] = ( last [ 'start_line' ] , total_lines )
8555	def get_lan ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
3128	def delete ( self , template_id ) : self . template_id = template_id return self . _mc_client . _delete ( url = self . _build_path ( template_id ) )
11765	def k_in_row ( self , board , move , player , ( delta_x , delta_y ) ) : "Return true if there is a line through move on board for player." x , y = move n = 0 while board . get ( ( x , y ) ) == player : n += 1 x , y = x + delta_x , y + delta_y x , y = move while board . get ( ( x , y ) ) == player : n += 1 x , y = x - delta_x , y - delta_y n -= 1 return n >= self . k
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
12629	def recursive_glob ( base_directory , regex = '' ) : files = glob ( op . join ( base_directory , regex ) ) for path , dirlist , filelist in os . walk ( base_directory ) : for dir_name in dirlist : files . extend ( glob ( op . join ( path , dir_name , regex ) ) ) return files
11245	def clean_strings ( iterable ) : retval = [ ] for val in iterable : try : retval . append ( val . strip ( ) ) except ( AttributeError ) : retval . append ( val ) return retval
831	def decode ( self , encoded , parentFieldName = '' ) : fieldsDict = dict ( ) fieldsOrder = [ ] if parentFieldName == '' : parentName = self . name else : parentName = "%s.%s" % ( parentFieldName , self . name ) if self . encoders is not None : for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] ( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , parentFieldName = parentName ) fieldsDict . update ( subFieldsDict ) fieldsOrder . extend ( subFieldsOrder ) return ( fieldsDict , fieldsOrder )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = read_hatpi_textlc ( lclist [ 0 ] ) lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval else : lcdict [ col ] = lcdict [ col ] - thismedval for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) for col in lcdict [ 'columns' ] : if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval else : thislcd [ col ] = thislcd [ col ] - thismedval lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] lcdict [ 'nconcatenated' ] = lccounter + 1 if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
5205	def proc_elms ( ** kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
1617	def _ShouldPrintError ( category , confidence , linenum ) : if IsErrorSuppressedByNolint ( category , linenum ) : return False if confidence < _cpplint_state . verbose_level : return False is_filtered = False for one_filter in _Filters ( ) : if one_filter . startswith ( '-' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = True elif one_filter . startswith ( '+' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = False else : assert False if is_filtered : return False return True
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
8067	def loadGrammar ( self , grammar , searchpaths = None ) : self . grammar = self . _load ( grammar , searchpaths = searchpaths ) self . refs = { } for ref in self . grammar . getElementsByTagName ( "ref" ) : self . refs [ ref . attributes [ "id" ] . value ] = ref
13527	def clean ( options , info ) : info ( "Cleaning patterns %s" , options . paved . clean . patterns ) for wd in options . paved . clean . dirs : info ( "Cleaning in %s" , wd ) for p in options . paved . clean . patterns : for f in wd . walkfiles ( p ) : f . remove ( )
648	def generateHubSequences ( nCoinc = 10 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) for hub in hubs : coincList . remove ( hub ) seqList = [ ] for i in xrange ( nSeq ) : length = random . choice ( seqLength ) - 1 seq = random . sample ( coincList , length ) seq . insert ( length // 2 , random . choice ( hubs ) ) seqList . append ( seq ) return seqList
6927	def newcursor ( self , dictcursor = False ) : handle = hashlib . sha256 ( os . urandom ( 12 ) ) . hexdigest ( ) if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return ( self . cursors [ handle ] , handle )
4033	def parse ( s ) : if IS_PY3 : r = sre_parse . parse ( s , flags = U ) else : r = sre_parse . parse ( s . decode ( 'utf-8' ) , flags = U ) return list ( r )
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , ** kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , ** kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
12789	def get ( self , q = None , page = None ) : etag = generate_etag ( current_ext . content_version . encode ( 'utf8' ) ) self . check_etag ( etag , weak = True ) res = jsonify ( current_ext . styles ) res . set_etag ( etag ) return res
7966	def end ( self , tag ) : self . _level -= 1 if self . _level < 0 : self . _handler . stream_parse_error ( u"Unexpected end tag for: {0!r}" . format ( tag ) ) return if self . _level == 0 : if tag != self . _root . tag : self . _handler . stream_parse_error ( u"Unexpected end tag for:" " {0!r} (stream end tag expected)" . format ( tag ) ) return self . _handler . stream_end ( ) return element = self . _builder . end ( tag ) if self . _level == 1 : self . _handler . stream_element ( element )
10840	def publish ( self ) : url = PATHS [ 'PUBLISH' ] % self . id return self . api . post ( url = url )
9055	def fast_scan ( self , M , verbose = True ) : from tqdm import tqdm if M . ndim != 2 : raise ValueError ( "`M` array must be bidimensional." ) p = M . shape [ 1 ] lmls = empty ( p ) effsizes0 = empty ( ( p , self . _XTQ [ 0 ] . shape [ 0 ] ) ) effsizes0_se = empty ( ( p , self . _XTQ [ 0 ] . shape [ 0 ] ) ) effsizes1 = empty ( p ) effsizes1_se = empty ( p ) scales = empty ( p ) if verbose : nchunks = min ( p , 30 ) else : nchunks = min ( p , 1 ) chunk_size = ( p + nchunks - 1 ) // nchunks for i in tqdm ( range ( nchunks ) , desc = "Scanning" , disable = not verbose ) : start = i * chunk_size stop = min ( start + chunk_size , M . shape [ 1 ] ) r = self . _fast_scan_chunk ( M [ : , start : stop ] ) lmls [ start : stop ] = r [ "lml" ] effsizes0 [ start : stop , : ] = r [ "effsizes0" ] effsizes0_se [ start : stop , : ] = r [ "effsizes0_se" ] effsizes1 [ start : stop ] = r [ "effsizes1" ] effsizes1_se [ start : stop ] = r [ "effsizes1_se" ] scales [ start : stop ] = r [ "scale" ] return { "lml" : lmls , "effsizes0" : effsizes0 , "effsizes0_se" : effsizes0_se , "effsizes1" : effsizes1 , "effsizes1_se" : effsizes1_se , "scale" : scales , }
9324	def refresh_collections ( self , accept = MEDIA_TYPE_TAXII_V20 ) : url = self . url + "collections/" response = self . _conn . get ( url , headers = { "Accept" : accept } ) self . _collections = [ ] for item in response . get ( "collections" , [ ] ) : collection_url = url + item [ "id" ] + "/" collection = Collection ( collection_url , conn = self . _conn , collection_info = item ) self . _collections . append ( collection ) self . _loaded_collections = True
6558	def add_constraint ( self , constraint , variables = tuple ( ) ) : if isinstance ( constraint , Constraint ) : if variables and ( tuple ( variables ) != constraint . variables ) : raise ValueError ( "mismatched variables and Constraint" ) elif isinstance ( constraint , Callable ) : constraint = Constraint . from_func ( constraint , variables , self . vartype ) elif isinstance ( constraint , Iterable ) : constraint = Constraint . from_configurations ( constraint , variables , self . vartype ) else : raise TypeError ( "Unknown constraint type given" ) self . constraints . append ( constraint ) for v in constraint . variables : self . variables [ v ] . append ( constraint )
5178	def resource ( self , type_ , title , ** kwargs ) : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return next ( resource for resource in resources )
11766	def update ( x , ** entries ) : if isinstance ( x , dict ) : x . update ( entries ) else : x . __dict__ . update ( entries ) return x
10787	def add_subtract_misfeatured_tile ( st , tile , rad = 'calc' , max_iter = 3 , invert = 'guess' , max_allowed_remove = 20 , minmass = None , use_tp = False , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) if invert == 'guess' : invert = guess_invert ( st ) initial_error = np . copy ( st . error ) rinds = np . nonzero ( tile . contains ( st . obj_get_positions ( ) ) ) [ 0 ] if rinds . size >= max_allowed_remove : CLOG . fatal ( 'Misfeatured region too large!' ) raise RuntimeError elif rinds . size >= max_allowed_remove / 2 : CLOG . warn ( 'Large misfeatured regions.' ) elif rinds . size > 0 : rpos , rrad = st . obj_remove_particle ( rinds ) n_added = - rinds . size added_poses = [ ] for _ in range ( max_iter ) : if invert : im = 1 - st . residuals [ tile . slicer ] else : im = st . residuals [ tile . slicer ] guess , _ = _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp ) accepts , poses = check_add_particles ( st , guess + tile . l , rad = rad , do_opt = True , ** kwargs ) added_poses . extend ( poses ) n_added += accepts if accepts == 0 : break else : CLOG . warn ( 'Runaway adds or insufficient max_iter' ) ainds = [ ] for p in added_poses : ainds . append ( st . obj_closest_particle ( p ) ) if len ( ainds ) > max_allowed_remove : for i in range ( 0 , len ( ainds ) , max_allowed_remove ) : opt . do_levmarq_particles ( st , np . array ( ainds [ i : i + max_allowed_remove ] ) , include_rad = True , max_iter = 3 ) elif len ( ainds ) > 0 : opt . do_levmarq_particles ( st , ainds , include_rad = True , max_iter = 3 ) did_something = ( rinds . size > 0 ) or ( len ( ainds ) > 0 ) if did_something & ( st . error > initial_error ) : CLOG . info ( 'Failed addsub, Tile {} -> {}' . format ( tile . l . tolist ( ) , tile . r . tolist ( ) ) ) if len ( ainds ) > 0 : _ = st . obj_remove_particle ( ainds ) if rinds . size > 0 : for p , r in zip ( rpos . reshape ( - 1 , 3 ) , rrad . reshape ( - 1 ) ) : _ = st . obj_add_particle ( p , r ) n_added = 0 ainds = [ ] return n_added , ainds
4014	def register_consumer ( ) : global _consumers hostname , port = request . form [ 'hostname' ] , request . form [ 'port' ] app_name = _app_name_from_forwarding_info ( hostname , port ) containers = get_dusty_containers ( [ app_name ] , include_exited = True ) if not containers : raise ValueError ( 'No container exists for app {}' . format ( app_name ) ) container = containers [ 0 ] new_id = uuid1 ( ) new_consumer = Consumer ( container [ 'Id' ] , datetime . utcnow ( ) ) _consumers [ str ( new_id ) ] = new_consumer response = jsonify ( { 'app_name' : app_name , 'consumer_id' : new_id } ) response . headers [ 'Access-Control-Allow-Origin' ] = '*' response . headers [ 'Access-Control-Allow-Methods' ] = 'GET, POST' return response
4876	def validate_user_email ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : user = User . objects . get ( email = value ) return models . EnterpriseCustomerUser . objects . get ( user_id = user . id , enterprise_customer = enterprise_customer ) except ( models . EnterpriseCustomerUser . DoesNotExist , User . DoesNotExist ) : pass return value
5908	def make_ndx_captured ( ** kwargs ) : kwargs [ 'stdout' ] = False user_input = kwargs . pop ( 'input' , [ ] ) user_input = [ cmd for cmd in user_input if cmd != 'q' ] kwargs [ 'input' ] = user_input + [ '' , 'q' ] return gromacs . make_ndx ( ** kwargs )
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
11159	def execute_pyfile ( self , py_exe = None ) : import subprocess self . assert_is_dir_and_exists ( ) if py_exe is None : if six . PY2 : py_exe = "python2" elif six . PY3 : py_exe = "python3" for p in self . select_by_ext ( ".py" ) : subprocess . Popen ( '%s "%s"' % ( py_exe , p . abspath ) )
9120	def dropbox_fileupload ( dropbox , request ) : attachment = request . POST [ 'attachment' ] attached = dropbox . add_attachment ( attachment ) return dict ( files = [ dict ( name = attached , type = attachment . type , ) ] )
4943	def get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = None , program_uuid = None ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : if course_id : return get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) return get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) except EnterpriseCustomer . DoesNotExist : return None
6357	def encode ( self , word ) : def _to_regex ( pattern , left_match = True ) : new_pattern = '' replacements = { '#' : '[AEIOU]+' , ':' : '[BCDFGHJKLMNPQRSTVWXYZ]*' , '^' : '[BCDFGHJKLMNPQRSTVWXYZ]' , '.' : '[BDVGJLMNTWZ]' , '%' : '(ER|E|ES|ED|ING|ELY)' , '+' : '[EIY]' , ' ' : '^' , } for char in pattern : new_pattern += ( replacements [ char ] if char in replacements else char ) if left_match : new_pattern += '$' if '^' not in pattern : new_pattern = '^.*' + new_pattern else : new_pattern = '^' + new_pattern . replace ( '^' , '$' ) if '$' not in new_pattern : new_pattern += '.*$' return new_pattern word = word . upper ( ) pron = '' pos = 0 while pos < len ( word ) : left_orig = word [ : pos ] right_orig = word [ pos : ] first = word [ pos ] if word [ pos ] in self . _rules else ' ' for rule in self . _rules [ first ] : left , match , right , out = rule if right_orig . startswith ( match ) : if left : l_pattern = _to_regex ( left , left_match = True ) if right : r_pattern = _to_regex ( right , left_match = False ) if ( not left or re_match ( l_pattern , left_orig ) ) and ( not right or re_match ( r_pattern , right_orig [ len ( match ) : ] ) ) : pron += out pos += len ( match ) break else : pron += word [ pos ] pos += 1 return pron
2941	def deserialize_workflow_spec ( self , s_state , filename = None ) : dom = minidom . parseString ( s_state ) node = dom . getElementsByTagName ( 'process-definition' ) [ 0 ] name = node . getAttribute ( 'name' ) if name == '' : _exc ( '%s without a name attribute' % node . nodeName ) workflow_spec = specs . WorkflowSpec ( name , filename ) del workflow_spec . task_specs [ 'Start' ] end = specs . Simple ( workflow_spec , 'End' ) , [ ] read_specs = dict ( end = end ) for child_node in node . childNodes : if child_node . nodeType != minidom . Node . ELEMENT_NODE : continue if child_node . nodeName == 'name' : workflow_spec . name = child_node . firstChild . nodeValue elif child_node . nodeName == 'description' : workflow_spec . description = child_node . firstChild . nodeValue elif child_node . nodeName . lower ( ) in _spec_map : self . deserialize_task_spec ( workflow_spec , child_node , read_specs ) else : _exc ( 'Unknown node: %s' % child_node . nodeName ) workflow_spec . start = read_specs [ 'start' ] [ 0 ] for name in read_specs : spec , successors = read_specs [ name ] for condition , successor_name in successors : if successor_name not in read_specs : _exc ( 'Unknown successor: "%s"' % successor_name ) successor , foo = read_specs [ successor_name ] if condition is None : spec . connect ( successor ) else : spec . connect_if ( condition , successor ) return workflow_spec
7600	def get_popular_tournaments ( self , ** params : keys ) : url = self . api . POPULAR + '/tournament' return self . _get_model ( url , PartialTournament , ** params )
9665	def clean_all ( G , settings ) : quiet = settings [ "quiet" ] recon = settings [ "recon" ] sprint = settings [ "sprint" ] error = settings [ "error" ] all_outputs = [ ] for node in G . nodes ( data = True ) : if "output" in node [ 1 ] : for item in get_all_outputs ( node [ 1 ] ) : all_outputs . append ( item ) all_outputs . append ( ".shastore" ) retcode = 0 for item in sorted ( all_outputs ) : if os . path . isfile ( item ) : if recon : sprint ( "Would remove file: {}" . format ( item ) ) continue sprint ( "Attempting to remove file '{}'" , level = "verbose" ) try : os . remove ( item ) sprint ( "Removed file" , level = "verbose" ) except : errmes = "Error: file '{}' failed to be removed" error ( errmes . format ( item ) ) retcode = 1 if not retcode and not recon : sprint ( "All clean" , color = True ) return retcode
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
1865	def PSHUFW ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) arg3 = Operators . ZEXTEND ( op3 . read ( ) , size ) assert size == 64 arg0 |= ( ( arg1 >> ( ( arg3 >> 0 ) & 3 * 16 ) ) & 0xffff ) arg0 |= ( ( arg1 >> ( ( arg3 >> 2 ) & 3 * 16 ) ) & 0xffff ) << 16 arg0 |= ( ( arg1 >> ( ( arg3 >> 4 ) & 3 * 16 ) ) & 0xffff ) << 32 arg0 |= ( ( arg1 >> ( ( arg3 >> 6 ) & 3 * 16 ) ) & 0xffff ) << 48 op0 . write ( arg0 )
8676	def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase , dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )
11488	def _find_resource_id_from_path ( path ) : session . token = verify_credentials ( ) parsed_path = path . split ( '/' ) if parsed_path [ - 1 ] == '' : parsed_path . pop ( ) if path . startswith ( '/users/' ) : parsed_path . pop ( 0 ) parsed_path . pop ( 0 ) name = parsed_path . pop ( 0 ) firstname , lastname = name . split ( '_' ) end = parsed_path . pop ( ) user = session . communicator . get_user_by_name ( firstname , lastname ) leaf_folder_id = _descend_folder_for_id ( parsed_path , user [ 'folder_id' ] ) return _search_folder_for_item_or_folder ( end , leaf_folder_id ) elif path . startswith ( '/communities/' ) : print ( parsed_path ) parsed_path . pop ( 0 ) parsed_path . pop ( 0 ) community_name = parsed_path . pop ( 0 ) end = parsed_path . pop ( ) community = session . communicator . get_community_by_name ( community_name ) leaf_folder_id = _descend_folder_for_id ( parsed_path , community [ 'folder_id' ] ) return _search_folder_for_item_or_folder ( end , leaf_folder_id ) else : return False , - 1
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
7407	def populate ( publications ) : customlinks = CustomLink . objects . filter ( publication__in = publications ) customfiles = CustomFile . objects . filter ( publication__in = publications ) publications_ = { } for publication in publications : publication . links = [ ] publication . files = [ ] publications_ [ publication . id ] = publication for link in customlinks : publications_ [ link . publication_id ] . links . append ( link ) for file in customfiles : publications_ [ file . publication_id ] . files . append ( file )
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
7866	def set_item ( self , key , value , timeout = None , timeout_callback = None ) : with self . _lock : logger . debug ( "expdict.__setitem__({0!r}, {1!r}, {2!r}, {3!r})" . format ( key , value , timeout , timeout_callback ) ) if not timeout : timeout = self . _default_timeout self . _timeouts [ key ] = ( time . time ( ) + timeout , timeout_callback ) return dict . __setitem__ ( self , key , value )
2907	def _is_descendant_of ( self , parent ) : if self . parent is None : return False if self . parent == parent : return True return self . parent . _is_descendant_of ( parent )
1851	def RCL ( cpu , dest , src ) : OperandSize = dest . size count = src . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) value = dest . read ( ) if isinstance ( tempCount , int ) and tempCount == 0 : new_val = value dest . write ( new_val ) else : carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) right = value >> ( OperandSize - tempCount ) new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) dest . write ( new_val ) def sf ( v , size ) : return ( v & ( 1 << ( size - 1 ) ) ) != 0 cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) cpu . OF = Operators . ITE ( tempCount == 1 , sf ( new_val , OperandSize ) != cpu . CF , cpu . OF )
13389	def manifest ( ) : prune = options . paved . dist . manifest . prune graft = set ( ) if options . paved . dist . manifest . include_sphinx_docroot : docroot = options . get ( 'docroot' , 'docs' ) graft . update ( [ docroot ] ) if options . paved . dist . manifest . exclude_sphinx_builddir : builddir = docroot + '/' + options . get ( "builddir" , ".build" ) prune . update ( [ builddir ] ) with open ( options . paved . cwd / 'MANIFEST.in' , 'w' ) as fo : for item in graft : fo . write ( 'graft %s\n' % item ) for item in options . paved . dist . manifest . include : fo . write ( 'include %s\n' % item ) for item in options . paved . dist . manifest . recursive_include : fo . write ( 'recursive-include %s\n' % item ) for item in prune : fo . write ( 'prune %s\n' % item )
10766	def get_poll ( self , arg , * , request_policy = None ) : if isinstance ( arg , str ) : match = self . _url_re . match ( arg ) if match : arg = match . group ( 'id' ) return self . _http_client . get ( '{}/{}' . format ( self . _POLLS , arg ) , request_policy = request_policy , cls = strawpoll . Poll )
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
9724	async def load_project ( self , project_path ) : cmd = "loadproject %s" % project_path return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
4064	def add_tags ( self , item , * tags ) : try : assert item [ "data" ] [ "tags" ] except AssertionError : item [ "data" ] [ "tags" ] = list ( ) for tag in tags : item [ "data" ] [ "tags" ] . append ( { "tag" : "%s" % tag } ) assert self . check_items ( [ item ] ) return self . update_item ( item )
1038	def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )
8485	def load ( self , clear = False ) : if clear : self . settings = { } defer = [ ] for conf in pkg_resources . iter_entry_points ( 'pyconfig' ) : if conf . attrs : raise RuntimeError ( "config must be a module" ) mod_name = conf . module_name base_name = conf . name if conf . name != 'any' else None log . info ( "Loading module '%s'" , mod_name ) mod_dict = runpy . run_module ( mod_name ) if mod_dict . get ( 'deferred' , None ) is deferred : log . info ( "Deferring module '%s'" , mod_name ) mod_dict . pop ( 'deferred' ) defer . append ( ( mod_name , base_name , mod_dict ) ) continue self . _update ( mod_dict , base_name ) for mod_name , base_name , mod_dict in defer : log . info ( "Loading deferred module '%s'" , mod_name ) self . _update ( mod_dict , base_name ) if etcd ( ) . configured : mod_dict = etcd ( ) . load ( ) if mod_dict : self . _update ( mod_dict ) mod_dict = None try : mod_dict = runpy . run_module ( 'localconfig' ) except ImportError : pass except ValueError as err : if getattr ( err , 'message' ) != '__package__ set to non-string' : raise mod_name = 'localconfig' if sys . version_info < ( 2 , 7 ) : loader , code , fname = runpy . _get_module_details ( mod_name ) else : _ , loader , code , fname = runpy . _get_module_details ( mod_name ) mod_dict = runpy . _run_code ( code , { } , { } , mod_name , fname , loader , pkg_name = None ) if mod_dict : log . info ( "Loading module 'localconfig'" ) self . _update ( mod_dict ) self . call_reload_hooks ( )
3810	async def connect ( self ) : proxy = os . environ . get ( 'HTTP_PROXY' ) self . _session = http_utils . Session ( self . _cookies , proxy = proxy ) try : self . _channel = channel . Channel ( self . _session , self . _max_retries , self . _retry_backoff_base ) self . _channel . on_connect . add_observer ( self . on_connect . fire ) self . _channel . on_reconnect . add_observer ( self . on_reconnect . fire ) self . _channel . on_disconnect . add_observer ( self . on_disconnect . fire ) self . _channel . on_receive_array . add_observer ( self . _on_receive_array ) self . _listen_future = asyncio . ensure_future ( self . _channel . listen ( ) ) try : await self . _listen_future except asyncio . CancelledError : self . _listen_future . cancel ( ) logger . info ( 'Client.connect returning because Channel.listen returned' ) finally : await self . _session . close ( )
3683	def calculate ( self , T , method ) : r if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
9386	def parse ( self ) : for infile in self . infile_list : logger . info ( 'Processing : %s' , infile ) status = True file_status = naarad . utils . is_valid_file ( infile ) if not file_status : return False with open ( infile ) as fh : for line in fh : words = line . split ( ) if not words : continue if re . match ( '^\d\d\d\d-\d\d-\d\d$' , line ) : self . ts_date = words [ 0 ] continue prefix_word = words [ 0 ] . strip ( ) if prefix_word == 'top' : self . process_top_line ( words ) self . saw_pid = False elif self . ts_valid_lines : if prefix_word == 'Tasks:' : self . process_tasks_line ( words ) elif prefix_word == 'Cpu(s):' : self . process_cpu_line ( words ) elif prefix_word == 'Mem:' : self . process_mem_line ( words ) elif prefix_word == 'Swap:' : self . process_swap_line ( words ) elif prefix_word == 'PID' : self . saw_pid = True self . process_headers = words else : if self . saw_pid and len ( words ) >= len ( self . process_headers ) : self . process_individual_command ( words ) for out_csv in self . data . keys ( ) : self . csv_files . append ( out_csv ) with open ( out_csv , 'w' ) as fh : fh . write ( '\n' . join ( self . data [ out_csv ] ) ) gc . collect ( ) return status
8127	def search_images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_IMAGES return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
12016	def model_uncert ( self ) : Y = self . photometry_array . T Y /= np . median ( Y , axis = 1 ) [ : , None ] C = np . median ( Y , axis = 0 ) nstars , nobs = np . shape ( Y ) Z = np . empty ( ( nstars , 4 ) ) qs = self . qs . astype ( int ) for s in range ( 4 ) : Z [ : , s ] = np . median ( ( Y / C ) [ : , qs == s ] , axis = 1 ) resid2 = ( Y - Z [ : , qs ] * C ) ** 2 z = Z [ : , qs ] trend = z * C [ None , : ] lnS = np . log ( np . nanmedian ( resid2 , axis = 0 ) ) jitter = np . log ( 0.1 * np . nanmedian ( np . abs ( np . diff ( Y , axis = 1 ) ) ) ) cal_ferr = np . sqrt ( np . exp ( 2 * ( jitter / trend ) ) + z ** 2 * np . exp ( lnS ) [ None , : ] ) self . modeled_uncert = cal_ferr self . target_uncert = cal_ferr [ 0 ]
4391	def adsSyncReadDeviceInfoReqEx ( port , address ) : sync_read_device_info_request = _adsDLL . AdsSyncReadDeviceInfoReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) device_name_buffer = ctypes . create_string_buffer ( 20 ) device_name_pointer = ctypes . pointer ( device_name_buffer ) ads_version = SAdsVersion ( ) ads_version_pointer = ctypes . pointer ( ads_version ) error_code = sync_read_device_info_request ( port , ams_address_pointer , device_name_pointer , ads_version_pointer ) if error_code : raise ADSError ( error_code ) return ( device_name_buffer . value . decode ( ) , AdsVersion ( ads_version ) )
1219	def restore ( self , sess , save_path ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before restore" ) self . _saver . restore ( sess = sess , save_path = save_path )
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
11729	def flag_inner_classes ( obj ) : for tup in class_members ( obj ) : tup [ 1 ] . _parent = obj tup [ 1 ] . _parent_inst = None tup [ 1 ] . __getattr__ = my_getattr flag_inner_classes ( tup [ 1 ] )
10079	def _publish_new ( self , id_ = None ) : minter = current_pidstore . minters [ current_app . config [ 'DEPOSIT_PID_MINTER' ] ] id_ = id_ or uuid . uuid4 ( ) record_pid = minter ( id_ , self ) self [ '_deposit' ] [ 'pid' ] = { 'type' : record_pid . pid_type , 'value' : record_pid . pid_value , 'revision_id' : 0 , } data = dict ( self . dumps ( ) ) data [ '$schema' ] = self . record_schema with self . _process_files ( id_ , data ) : record = self . published_record_class . create ( data , id_ = id_ ) return record
13052	def nmap ( nmap_args , ips ) : config = Config ( ) arguments = [ 'nmap' , '-Pn' ] arguments . extend ( ips ) arguments . extend ( nmap_args ) output_file = '' now = datetime . datetime . now ( ) if not '-oA' in nmap_args : output_name = 'nmap_jackal_{}' . format ( now . strftime ( "%Y-%m-%d %H:%M" ) ) path_name = os . path . join ( config . get ( 'nmap' , 'directory' ) , output_name ) print_notification ( "Writing output of nmap to {}" . format ( path_name ) ) if not os . path . exists ( config . get ( 'nmap' , 'directory' ) ) : os . makedirs ( config . get ( 'nmap' , 'directory' ) ) output_file = path_name + '.xml' arguments . extend ( [ '-oA' , path_name ] ) else : output_file = nmap_args [ nmap_args . index ( '-oA' ) + 1 ] + '.xml' print_notification ( "Starting nmap" ) subprocess . call ( arguments ) with open ( output_file , 'r' ) as f : return f . read ( )
5896	def render_toolbar ( context , config ) : quill_config = getattr ( quill_app , config ) t = template . loader . get_template ( quill_config [ 'toolbar_template' ] ) return t . render ( context )
13363	def echo_via_pager ( text , color = None ) : color = resolve_color_default ( color ) if not isinstance ( text , string_types ) : text = text_type ( text ) from . _termui_impl import pager return pager ( text + '\n' , color )
10434	def getcellvalue ( self , window_name , object_name , row_index , column = 0 ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] count = len ( cell . AXChildren ) if column < 0 or column > count : raise LdtpServerException ( 'Column index out of range: %d' % column ) obj = cell . AXChildren [ column ] if not re . search ( "AXColumn" , obj . AXRole ) : obj = cell . AXChildren [ column ] return obj . AXValue
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
4123	def _twosided_zerolag ( data , zerolag ) : res = twosided ( np . insert ( data , 0 , zerolag ) ) return res
13557	def get_all_images ( self ) : self_imgs = self . image_set . all ( ) update_ids = self . update_set . values_list ( 'id' , flat = True ) u_images = UpdateImage . objects . filter ( update__id__in = update_ids ) return list ( chain ( self_imgs , u_images ) )
9881	def _reliability_data_to_value_counts ( reliability_data , value_domain ) : return np . array ( [ [ sum ( 1 for rate in unit if rate == v ) for v in value_domain ] for unit in reliability_data . T ] )
12977	def deleteOne ( self , obj , conn = None ) : if not getattr ( obj , '_id' , None ) : return 0 if conn is None : conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) executeAfter = True else : pipeline = conn executeAfter = False pipeline . delete ( self . _get_key_for_id ( obj . _id ) ) self . _rem_id_from_keys ( obj . _id , pipeline ) for indexedFieldName in self . indexedFields : self . _rem_id_from_index ( indexedFieldName , obj . _id , obj . _origData [ indexedFieldName ] , pipeline ) obj . _id = None if executeAfter is True : pipeline . execute ( ) return 1
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
1047	def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = ' File "%s", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + ' %s\n' % line . strip ( ) list . append ( item ) return list
8685	def _encrypt ( self , value ) : value = json . dumps ( value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) encrypted_value = self . cipher . encrypt ( value . encode ( 'utf8' ) ) hexified_value = binascii . hexlify ( encrypted_value ) . decode ( 'ascii' ) return hexified_value
6853	def partitions ( device = "" ) : partitions_list = { } with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'sfdisk -d %(device)s' % locals ( ) ) spart = re . compile ( r'(?P<pname>^/.*) : .* Id=(?P<ptypeid>[0-9a-z]+)' ) for line in res . splitlines ( ) : m = spart . search ( line ) if m : partitions_list [ m . group ( 'pname' ) ] = int ( m . group ( 'ptypeid' ) , 16 ) return partitions_list
13555	def delete_shifts ( self , shifts ) : url = "/2/shifts/?%s" % urlencode ( { 'ids' : "," . join ( str ( s ) for s in shifts ) } ) data = self . _delete_resource ( url ) return data
8699	def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush ( )
1693	def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
7934	def _compute_handshake ( self ) : return hashlib . sha1 ( to_utf8 ( self . stream_id ) + to_utf8 ( self . secret ) ) . hexdigest ( )
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
9749	def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
6522	def add_issues ( self , issues ) : if not isinstance ( issues , ( list , tuple ) ) : issues = [ issues ] with self . _lock : self . _all_issues . extend ( issues ) self . _cleaned_issues = None
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
12480	def get_rcfile_section ( app_name , section_name ) : try : settings = rcfile ( app_name , section_name ) except IOError : raise except : raise KeyError ( 'Error looking for section {} in {} ' ' rcfiles.' . format ( section_name , app_name ) ) else : return settings
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
8453	def clean ( ) : temple . check . in_git_repo ( ) current_branch = _get_current_branch ( ) update_branch = temple . constants . UPDATE_BRANCH_NAME temp_update_branch = temple . constants . TEMP_UPDATE_BRANCH_NAME if current_branch in ( update_branch , temp_update_branch ) : err_msg = ( 'You must change from the "{}" branch since it will be deleted during cleanup' ) . format ( current_branch ) raise temple . exceptions . InvalidCurrentBranchError ( err_msg ) if temple . check . _has_branch ( update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( update_branch ) ) if temple . check . _has_branch ( temp_update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( temp_update_branch ) )
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
1999	def _method ( self , expression , * args ) : assert expression . __class__ . __mro__ [ - 1 ] is object for cls in expression . __class__ . __mro__ : sort = cls . __name__ methodname = 'visit_%s' % sort method = getattr ( self , methodname , None ) if method is not None : method ( expression , * args ) return return
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
10536	def get_category ( category_id ) : try : res = _pybossa_req ( 'get' , 'category' , category_id ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
1128	def SeqN ( n , * inner_rules , ** kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( "loc" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule
13484	def ghpages ( ) : opts = options docroot = path ( opts . get ( 'docroot' , 'docs' ) ) if not docroot . exists ( ) : raise BuildFailure ( "Sphinx documentation root (%s) does not exist." % docroot ) builddir = docroot / opts . get ( "builddir" , ".build" ) builddir = builddir / 'html' if not builddir . exists ( ) : raise BuildFailure ( "Sphinx build directory (%s) does not exist." % builddir ) nojekyll = path ( builddir ) / '.nojekyll' nojekyll . touch ( ) sh ( 'ghp-import -p %s' % ( builddir ) )
11003	def psffunc ( self , * args , ** kwargs ) : if self . polychromatic : func = psfcalc . calculate_polychrome_linescan_psf else : func = psfcalc . calculate_linescan_psf return func ( * args , ** kwargs )
3101	def _SendRecv ( ) : port = int ( os . getenv ( DEVSHELL_ENV , 0 ) ) if port == 0 : raise NoDevshellServer ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL_INFO_REQUEST_JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall ( _helpers . _to_bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\n' not in header : raise CommunicationError ( 'saw no newline in the first 6 bytes' ) len_str , json_str = header . split ( '\n' , 1 ) to_read = int ( len_str ) - len ( json_str ) if to_read > 0 : json_str += sock . recv ( to_read , socket . MSG_WAITALL ) . decode ( ) return CredentialInfoResponse ( json_str )
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
254	def add_closing_transactions ( positions , transactions ) : closed_txns = transactions [ [ 'symbol' , 'amount' , 'price' ] ] pos_at_end = positions . drop ( 'cash' , axis = 1 ) . iloc [ - 1 ] open_pos = pos_at_end . replace ( 0 , np . nan ) . dropna ( ) end_dt = open_pos . name + pd . Timedelta ( seconds = 1 ) for sym , ending_val in open_pos . iteritems ( ) : txn_sym = transactions [ transactions . symbol == sym ] ending_amount = txn_sym . amount . sum ( ) ending_price = ending_val / ending_amount closing_txn = { 'symbol' : sym , 'amount' : - ending_amount , 'price' : ending_price } closing_txn = pd . DataFrame ( closing_txn , index = [ end_dt ] ) closed_txns = closed_txns . append ( closing_txn ) closed_txns = closed_txns [ closed_txns . amount != 0 ] return closed_txns
12544	def nifti_out ( f ) : @ wraps ( f ) def wrapped ( * args , ** kwargs ) : r = f ( * args , ** kwargs ) img = read_img ( args [ 0 ] ) return nib . Nifti1Image ( r , affine = img . get_affine ( ) , header = img . header ) return wrapped
3209	def get_load_balancer ( load_balancer , flags = FLAGS . ALL ^ FLAGS . POLICY_TYPES , ** conn ) : try : basestring except NameError as _ : basestring = str if isinstance ( load_balancer , basestring ) : load_balancer = dict ( LoadBalancerName = load_balancer ) return registry . build_out ( flags , start_with = load_balancer , pass_datastructure = True , ** conn )
13177	def get_cache_key ( prefix , * args , ** kwargs ) : hash_args_kwargs = hash ( tuple ( kwargs . iteritems ( ) ) + args ) return '{}_{}' . format ( prefix , hash_args_kwargs )
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return while True : if self . __writer is None : if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : self . __readers [ me ] += 1 return else : self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
9495	def _parse_document_id ( elm_tree ) : xpath = '//md:content-id/text()' return [ x for x in elm_tree . xpath ( xpath , namespaces = COLLECTION_NSMAP ) ] [ 0 ]
8739	def _allocate_from_v6_subnet ( self , context , net_id , subnet , port_id , reuse_after , ip_address = None , ** kwargs ) : LOG . info ( "Attempting to allocate a v6 address - [{0}]" . format ( utils . pretty_kwargs ( network_id = net_id , subnet = subnet , port_id = port_id , ip_address = ip_address ) ) ) if ip_address : LOG . info ( "IP %s explicitly requested, deferring to standard " "allocation" % ip_address ) return self . _allocate_from_subnet ( context , net_id = net_id , subnet = subnet , port_id = port_id , reuse_after = reuse_after , ip_address = ip_address , ** kwargs ) else : mac = kwargs . get ( "mac_address" ) if mac : mac = kwargs [ "mac_address" ] . get ( "address" ) if subnet and subnet [ "ip_policy" ] : ip_policy_cidrs = subnet [ "ip_policy" ] . get_cidrs_ip_set ( ) else : ip_policy_cidrs = netaddr . IPSet ( [ ] ) for tries , ip_address in enumerate ( generate_v6 ( mac , port_id , subnet [ "cidr" ] ) ) : LOG . info ( "Attempt {0} of {1}" . format ( tries + 1 , CONF . QUARK . v6_allocation_attempts ) ) if tries > CONF . QUARK . v6_allocation_attempts - 1 : LOG . info ( "Exceeded v6 allocation attempts, bailing" ) raise ip_address_failure ( net_id ) ip_address = netaddr . IPAddress ( ip_address ) . ipv6 ( ) LOG . info ( "Generated a new v6 address {0}" . format ( str ( ip_address ) ) ) if ( ip_policy_cidrs is not None and ip_address in ip_policy_cidrs ) : LOG . info ( "Address {0} excluded by policy" . format ( str ( ip_address ) ) ) continue try : with context . session . begin ( ) : address = db_api . ip_address_create ( context , address = ip_address , subnet_id = subnet [ "id" ] , version = subnet [ "ip_version" ] , network_id = net_id , address_type = kwargs . get ( 'address_type' , ip_types . FIXED ) ) return address except db_exception . DBDuplicateEntry : LOG . info ( "{0} exists but was already " "allocated" . format ( str ( ip_address ) ) ) LOG . debug ( "Duplicate entry found when inserting subnet_id" " %s ip_address %s" , subnet [ "id" ] , ip_address )
166	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( self . coords , from_shape , to_shape ) return self . copy ( coords = coords_proj )
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
13419	def schema ( args ) : try : import south cmd = args and 'schemamigration %s' % ' ' . join ( options . args ) or 'schemamigration' call_manage ( cmd ) except ImportError : error ( 'Could not import south.' )
5304	def sanitize_color_palette ( colorpalette ) : new_palette = { } def __make_valid_color_name ( name ) : if len ( name ) == 1 : name = name [ 0 ] return name [ : 1 ] . lower ( ) + name [ 1 : ] return name [ 0 ] . lower ( ) + '' . join ( word . capitalize ( ) for word in name [ 1 : ] ) for key , value in colorpalette . items ( ) : if isinstance ( value , str ) : value = utils . hex_to_rgb ( value ) new_palette [ __make_valid_color_name ( key . split ( ) ) ] = value return new_palette
9801	def config ( list ) : if list : _config = GlobalConfigManager . get_config_or_default ( ) Printer . print_header ( 'Current config:' ) dict_tabulate ( _config . to_dict ( ) )
7223	def save ( self , recipe ) : if 'id' in recipe and recipe [ 'id' ] is not None : self . logger . debug ( "Updating existing recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe [ 'id' ] } r = self . gbdx_connection . put ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise return recipe [ 'id' ] else : self . logger . debug ( "Creating new recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json' % { 'base_url' : self . base_url } r = self . gbdx_connection . post ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise recipe_json = r . json ( ) return recipe_json [ 'id' ]
2446	def create_package ( self , doc , name ) : if not self . package_set : self . package_set = True doc . package = package . Package ( name = name ) return True else : raise CardinalityError ( 'Package::Name' )
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
9409	def _extract ( data , session = None ) : if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) if data . dtype . names : if data . size == 1 : return _create_struct ( data , session ) return StructArray ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
13907	def create_commands ( self , commands , parser ) : self . apply_defaults ( commands ) def create_single_command ( command ) : keys = command [ 'keys' ] del command [ 'keys' ] kwargs = { } for item in command : kwargs [ item ] = command [ item ] parser . add_argument ( * keys , ** kwargs ) if len ( commands ) > 1 : for command in commands : create_single_command ( command ) else : create_single_command ( commands [ 0 ] )
11087	def sleep ( self , channel ) : self . log . info ( 'Sleeping in %s' , channel ) self . _bot . dispatcher . ignore ( channel ) self . send_message ( channel , 'Good night' )
13860	def is_date_type ( cls ) : if not isinstance ( cls , type ) : return False return issubclass ( cls , date ) and not issubclass ( cls , datetime )
8806	def calc_periods ( hour = 0 , minute = 0 ) : period_end = datetime . datetime . utcnow ( ) . replace ( hour = hour , minute = minute , second = 0 , microsecond = 0 ) period_start = period_end - datetime . timedelta ( days = 1 ) period_end -= datetime . timedelta ( seconds = 1 ) return ( period_start , period_end )
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
2728	def get_action ( self , action_id ) : return Action . get_object ( api_token = self . token , action_id = action_id )
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
6337	def dist_baystat ( src , tar , min_ss_len = None , left_ext = None , right_ext = None ) : return Baystat ( ) . dist ( src , tar , min_ss_len , left_ext , right_ext )
3244	def get_security_group ( security_group , flags = FLAGS . ALL , ** kwargs ) : result = registry . build_out ( flags , start_with = security_group , pass_datastructure = True , ** kwargs ) result . pop ( 'security_group_rules' , [ ] ) return result
12237	def doublewell ( theta ) : k0 , k1 , depth = 0.01 , 100 , 0.5 shallow = 0.5 * k0 * theta ** 2 + depth deep = 0.5 * k1 * theta ** 2 obj = float ( np . minimum ( shallow , deep ) ) grad = np . where ( deep < shallow , k1 * theta , k0 * theta ) return obj , grad
3578	def initialize ( self ) : GObject . threads_init ( ) dbus . mainloop . glib . threads_init ( ) self . _mainloop = dbus . mainloop . glib . DBusGMainLoop ( set_as_default = True ) self . _bus = dbus . SystemBus ( ) self . _bluez = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , '/' ) , 'org.freedesktop.DBus.ObjectManager' )
2207	def truepath ( path , real = False ) : path = expanduser ( path ) path = expandvars ( path ) if real : path = realpath ( path ) else : path = abspath ( path ) path = normpath ( path ) return path
12578	def apply_smoothing ( self , smooth_fwhm ) : if smooth_fwhm <= 0 : return old_smooth_fwhm = self . _smooth_fwhm self . _smooth_fwhm = smooth_fwhm try : data = self . get_data ( smoothed = True , masked = True , safe_copy = True ) except ValueError as ve : self . _smooth_fwhm = old_smooth_fwhm raise else : self . _smooth_fwhm = smooth_fwhm return data
8950	def error ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;37;41mERROR: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
7604	def get_player_chests ( self , tag : crtag , timeout : int = None ) : url = self . api . PLAYER + '/' + tag + '/upcomingchests' return self . _get_model ( url , timeout = timeout )
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) return results return [ ]
227	def get_max_median_position_concentration ( positions ) : expos = get_percent_alloc ( positions ) expos = expos . drop ( 'cash' , axis = 1 ) longs = expos . where ( expos . applymap ( lambda x : x > 0 ) ) shorts = expos . where ( expos . applymap ( lambda x : x < 0 ) ) alloc_summary = pd . DataFrame ( ) alloc_summary [ 'max_long' ] = longs . max ( axis = 1 ) alloc_summary [ 'median_long' ] = longs . median ( axis = 1 ) alloc_summary [ 'median_short' ] = shorts . median ( axis = 1 ) alloc_summary [ 'max_short' ] = shorts . min ( axis = 1 ) return alloc_summary
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
8865	def complete ( code , line , column , path , encoding , prefix ) : ret_val = [ ] try : script = jedi . Script ( code , line + 1 , column , path , encoding ) completions = script . completions ( ) print ( 'completions: %r' % completions ) except jedi . NotFoundError : completions = [ ] for completion in completions : ret_val . append ( { 'name' : completion . name , 'icon' : icon_from_typename ( completion . name , completion . type ) , 'tooltip' : completion . description } ) return ret_val
11348	def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) return ret
11549	def main ( ) : usage = "Usage: %prog PATH_TO_PACKAGE" parser = optparse . OptionParser ( usage = usage ) parser . add_option ( "-v" , "--verbose" , action = "store_true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add_option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output_dir" , default = '' , help = "" ) parser . add_option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test_args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse_args ( ) if options . verbose : log_level = logging . DEBUG else : log_level = logging . INFO logging . basicConfig ( level = log_level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise RuntimeError ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test_args : parts . append ( options . test_args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output_dir : coverage_dir = options . output_dir open_in_browser = False else : coverage_dir = 'htmlcov' open_in_browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage_dir ) ) logger . info ( "Wrote coverage files to %s" , coverage_dir ) if open_in_browser : index_file = os . path . abspath ( os . path . join ( coverage_dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index_file ) webbrowser . open ( 'file://' + index_file ) logger . info ( "Opened reports in your browser." )
10028	def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
10158	def get_viewset_transition_action_mixin ( model , ** kwargs ) : instance = model ( ) class Mixin ( object ) : save_after_transition = True transitions = instance . get_all_status_transitions ( ) transition_names = set ( x . name for x in transitions ) for transition_name in transition_names : setattr ( Mixin , transition_name , get_transition_viewset_method ( transition_name , ** kwargs ) ) return Mixin
8130	def layer ( self , img , x = 0 , y = 0 , name = "" ) : from types import StringType if isinstance ( img , Image . Image ) : img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1 if isinstance ( img , Layer ) : img . canvas = self self . layers . append ( img ) return len ( self . layers ) - 1 if type ( img ) == StringType : img = Image . open ( img ) img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , ** kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
6856	def query ( query , use_sudo = True , ** kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
9507	def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
2239	def _syspath_modname_to_modpath ( modname , sys_path = None , exclude = None ) : def _isvalid ( modpath , base ) : subdir = dirname ( modpath ) while subdir and subdir != base : if not exists ( join ( subdir , '__init__.py' ) ) : return False subdir = dirname ( subdir ) return True _fname_we = modname . replace ( '.' , os . path . sep ) candidate_fnames = [ _fname_we + '.py' , ] candidate_fnames += [ _fname_we + ext for ext in _platform_pylib_exts ( ) ] if sys_path is None : sys_path = sys . path candidate_dpaths = [ '.' if p == '' else p for p in sys_path ] if exclude : def normalize ( p ) : if sys . platform . startswith ( 'win32' ) : return realpath ( p ) . lower ( ) else : return realpath ( p ) real_exclude = { normalize ( p ) for p in exclude } candidate_dpaths = [ p for p in candidate_dpaths if normalize ( p ) not in real_exclude ] for dpath in candidate_dpaths : modpath = join ( dpath , _fname_we ) if exists ( modpath ) : if isfile ( join ( modpath , '__init__.py' ) ) : if _isvalid ( modpath , dpath ) : return modpath for fname in candidate_fnames : modpath = join ( dpath , fname ) if isfile ( modpath ) : if _isvalid ( modpath , dpath ) : return modpath
8307	def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
9756	def update ( ctx , name , description , tags ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment . update_experiment ( user , project_name , _experiment , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment updated." ) get_experiment_details ( response )
12691	def write_table_pair_potential ( func , dfunc = None , bounds = ( 1.0 , 10.0 ) , samples = 1000 , tollerance = 1e-6 , keyword = 'PAIR' ) : r_min , r_max = bounds if dfunc is None : dfunc = lambda r : ( func ( r + tollerance ) - func ( r - tollerance ) ) / ( 2 * tollerance ) i = np . arange ( 1 , samples + 1 ) r = np . linspace ( r_min , r_max , samples ) forces = func ( r ) energies = dfunc ( r ) lines = [ '%d %f %f %f\n' % ( index , radius , force , energy ) for index , radius , force , energy in zip ( i , r , forces , energies ) ] return "%s\nN %d\n\n" % ( keyword , samples ) + '' . join ( lines )
12869	async def manage ( self ) : cm = _ContextManager ( self . database ) if isinstance ( self . database . obj , AIODatabase ) : cm . connection = await self . database . async_connect ( ) else : cm . connection = self . database . connect ( ) return cm
7418	def make ( assembly , samples ) : longname = max ( [ len ( i ) for i in assembly . samples . keys ( ) ] ) names = [ i . name for i in samples ] partitions = makephy ( assembly , samples , longname ) makenex ( assembly , names , longname , partitions )
9240	def fetch_events_for_issues_and_pr ( self ) : self . fetcher . fetch_events_async ( self . issues , "issues" ) self . fetcher . fetch_events_async ( self . pull_requests , "pull requests" )
6329	def gng_importer ( self , corpus_file ) : with c_open ( corpus_file , 'r' , encoding = 'utf-8' ) as gng : for line in gng : line = line . rstrip ( ) . split ( '\t' ) words = line [ 0 ] . split ( ) self . _add_to_ngcorpus ( self . ngcorpus , words , int ( line [ 2 ] ) )
7008	def _fourier_chisq ( fourierparams , phase , mags , errs ) : f = _fourier_func ( fourierparams , phase , mags ) chisq = npsum ( ( ( mags - f ) * ( mags - f ) ) / ( errs * errs ) ) return chisq
9428	def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date_time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file_size ) )
12528	def upload ( ctx , repo ) : artifacts = ' ' . join ( shlex . quote ( str ( n ) ) for n in ROOT . joinpath ( 'dist' ) . glob ( 'pipfile[-_]cli-*' ) ) ctx . run ( f'twine upload --repository="{repo}" {artifacts}' )
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
3238	def get_role_managed_policy_documents ( role , client = None , ** kwargs ) : policies = get_role_managed_policies ( role , force_client = client ) policy_names = ( policy [ 'name' ] for policy in policies ) delayed_gmpd_calls = ( delayed ( get_managed_policy_document ) ( policy [ 'arn' ] , force_client = client ) for policy in policies ) policy_documents = Parallel ( n_jobs = 20 , backend = "threading" ) ( delayed_gmpd_calls ) return dict ( zip ( policy_names , policy_documents ) )
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : "Gets within-bag distances for each bag." logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
4083	def check ( self , text : str , srctext = None ) -> [ Match ] : root = self . _get_root ( self . _url , self . _encode ( text , srctext ) ) return [ Match ( e . attrib ) for e in root if e . tag == 'error' ]
3534	def olark ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OlarkNode ( )
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
358	def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( "[!] Fail to load %s" % file_path )
12429	def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
5016	def transmit ( self , payload , ** kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
6656	def calibrateEB ( variances , sigma2 ) : if ( sigma2 <= 0 or min ( variances ) == max ( variances ) ) : return ( np . maximum ( variances , 0 ) ) sigma = np . sqrt ( sigma2 ) eb_prior = gfit ( variances , sigma ) part = functools . partial ( gbayes , g_est = eb_prior , sigma = sigma ) if len ( variances ) >= 200 : calib_x = np . percentile ( variances , np . arange ( 0 , 102 , 2 ) ) calib_y = list ( map ( part , calib_x ) ) calib_all = np . interp ( variances , calib_x , calib_y ) else : calib_all = list ( map ( part , variances ) ) return np . asarray ( calib_all )
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
2481	def build ( self , ** kwargs ) : self . yacc = yacc . yacc ( module = self , ** kwargs )
8397	def gettrans ( t ) : obj = t if isinstance ( obj , str ) : name = '{}_trans' . format ( obj ) obj = globals ( ) [ name ] ( ) if callable ( obj ) : obj = obj ( ) if isinstance ( obj , type ) : obj = obj ( ) if not isinstance ( obj , trans ) : raise ValueError ( "Could not get transform object." ) return obj
3250	def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
2810	def convert_transpose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transpose ...' ) if params [ 'perm' ] [ 0 ] != 0 : if inputs [ 0 ] in layers : print ( '!!! Cannot permute batch dimension. Result may be wrong !!!' ) layers [ scope_name ] = layers [ inputs [ 0 ] ] else : print ( 'Skip weight matrix transpose, result may be wrong.' ) else : if names : tf_name = 'PERM' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) permute = keras . layers . Permute ( params [ 'perm' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = permute ( layers [ inputs [ 0 ] ] )
8272	def _load ( self , top = 5 , blue = "blue" , archive = None , member = None ) : if archive is None : path = os . path . join ( self . cache , self . name + ".xml" ) xml = open ( path ) . read ( ) else : assert member is not None xml = archive . read ( member ) dom = parseString ( xml ) . documentElement attr = lambda e , a : e . attributes [ a ] . value for e in dom . getElementsByTagName ( "color" ) [ : top ] : w = float ( attr ( e , "weight" ) ) try : rgb = e . getElementsByTagName ( "rgb" ) [ 0 ] clr = color ( float ( attr ( rgb , "r" ) ) , float ( attr ( rgb , "g" ) ) , float ( attr ( rgb , "b" ) ) , float ( attr ( rgb , "a" ) ) , mode = "rgb" ) try : clr . name = attr ( e , "name" ) if clr . name == "blue" : clr = color ( blue ) except : pass except : name = attr ( e , "name" ) if name == "blue" : name = blue clr = color ( name ) for s in e . getElementsByTagName ( "shade" ) : self . ranges . append ( ( clr , shade ( attr ( s , "name" ) ) , w * float ( attr ( s , "weight" ) ) ) )
8795	def serialize_rules ( self , rules ) : serialized = [ ] for rule in rules : direction = rule [ "direction" ] source = '' destination = '' if rule . get ( "remote_ip_prefix" ) : prefix = rule [ "remote_ip_prefix" ] if direction == "ingress" : source = self . _convert_remote_network ( prefix ) else : if ( Capabilities . EGRESS not in CONF . QUARK . environment_capabilities ) : raise q_exc . EgressSecurityGroupRulesNotEnabled ( ) else : destination = self . _convert_remote_network ( prefix ) optional_fields = { } protocol_map = protocols . PROTOCOL_MAP [ rule [ "ethertype" ] ] if rule [ "protocol" ] == protocol_map [ "icmp" ] : optional_fields [ "icmp type" ] = rule [ "port_range_min" ] optional_fields [ "icmp code" ] = rule [ "port_range_max" ] else : optional_fields [ "port start" ] = rule [ "port_range_min" ] optional_fields [ "port end" ] = rule [ "port_range_max" ] payload = { "ethertype" : rule [ "ethertype" ] , "protocol" : rule [ "protocol" ] , "source network" : source , "destination network" : destination , "action" : "allow" , "direction" : direction } payload . update ( optional_fields ) serialized . append ( payload ) return serialized
1260	def restore_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) component . restore ( sess = self . session , save_path = save_path )
10973	def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_invitations ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , page = page , per_page = per_page , )
3182	def create ( self , data ) : if 'id' not in data : raise KeyError ( 'The store must have an id' ) if 'list_id' not in data : raise KeyError ( 'The store must have a list_id' ) if 'name' not in data : raise KeyError ( 'The store must have a name' ) if 'currency_code' not in data : raise KeyError ( 'The store must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . store_id = response [ 'id' ] else : self . store_id = None return response
12826	def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( ** space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
2968	def _sm_relieve_pain ( self , * args , ** kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
171	def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
2559	def create_reg_message ( self ) : msg = { 'parsl_v' : PARSL_VERSION , 'python_v' : "{}.{}.{}" . format ( sys . version_info . major , sys . version_info . minor , sys . version_info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b_msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b_msg
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
139	def to_bounding_box ( self ) : from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )
2974	def cmd_up ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . create ( verbose = opts . verbose , force = opts . force ) print_containers ( containers , opts . json )
4011	def get_dusty_containers ( services , include_exited = False ) : client = get_docker_client ( ) if services : containers = [ get_container_for_app_or_service ( service , include_exited = include_exited ) for service in services ] return [ container for container in containers if container ] else : return [ container for container in client . containers ( all = include_exited ) if any ( name . startswith ( '/dusty' ) for name in container . get ( 'Names' , [ ] ) ) ]
2703	def collect_keyword ( sent , ranks , stopwords ) : for w in sent : if ( w . word_id > 0 ) and ( w . root in ranks ) and ( w . pos [ 0 ] in "NV" ) and ( w . root not in stopwords ) : rl = RankedLexeme ( text = w . raw . lower ( ) , rank = ranks [ w . root ] / 2.0 , ids = [ w . word_id ] , pos = w . pos . lower ( ) , count = 1 ) if DEBUG : print ( rl ) yield rl
4994	def unlink_inactive_learners ( channel_code , channel_pk ) : start = time . time ( ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Processing learners to unlink inactive users using configuration: [%s]' , integrated_channel ) integrated_channel . unlink_inactive_learners ( ) duration = time . time ( ) - start LOGGER . info ( 'Unlink inactive learners task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
2845	def enumerate_device_serials ( vid = FT232H_VID , pid = FT232H_PID ) : try : ctx = None ctx = ftdi . new ( ) device_list = None count , device_list = ftdi . usb_find_all ( ctx , vid , pid ) if count < 0 : raise RuntimeError ( 'ftdi_usb_find_all returned error {0}: {1}' . format ( count , ftdi . get_error_string ( self . _ctx ) ) ) devices = [ ] while device_list is not None : ret , manufacturer , description , serial = ftdi . usb_get_strings ( ctx , device_list . dev , 256 , 256 , 256 ) if serial is not None : devices . append ( serial ) device_list = device_list . next return devices finally : if device_list is not None : ftdi . list_free ( device_list ) if ctx is not None : ftdi . free ( ctx )
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
1161	def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( "%s.acquire(%s): blocked waiting, value=%s" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( "%s.acquire: success, value=%s" , self , self . __value ) rc = True return rc
8176	def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
4521	def get ( self , ring , angle ) : pixel = self . angleToPixel ( angle , ring ) return self . _get_base ( pixel )
11274	def check_pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
5091	def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
11572	def output_entire_buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display_buffer [ row ] [ col ] == self . LED_GREEN : green |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_RED : red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_YELLOW : green |= 1 << col red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c_write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c_write ( 0x70 , row * 2 + 1 , 0 , red )
8209	def coordinates ( self , x0 , y0 , distance , angle ) : x = x0 + cos ( radians ( angle ) ) * distance y = y0 + sin ( radians ( angle ) ) * distance return Point ( x , y )
46	def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )
6942	def invgauss_eclipses_func ( ebparams , times , mags , errs ) : ( period , epoch , pdepth , pduration , depthratio , secondaryphase ) = ebparams iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) primaryecl_amp = - pdepth secondaryecl_amp = - pdepth * depthratio primaryecl_std = pduration / 5.0 secondaryecl_std = pduration / 5.0 halfduration = pduration / 2.0 primary_eclipse_ingress = ( ( phase >= ( 1.0 - halfduration ) ) & ( phase <= 1.0 ) ) primary_eclipse_egress = ( ( phase >= 0.0 ) & ( phase <= halfduration ) ) secondary_eclipse_phase = ( ( phase >= ( secondaryphase - halfduration ) ) & ( phase <= ( secondaryphase + halfduration ) ) ) modelmags [ primary_eclipse_ingress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_ingress ] , primaryecl_amp , 1.0 , primaryecl_std ) ) modelmags [ primary_eclipse_egress ] = ( zerolevel + _gaussian ( phase [ primary_eclipse_egress ] , primaryecl_amp , 0.0 , primaryecl_std ) ) modelmags [ secondary_eclipse_phase ] = ( zerolevel + _gaussian ( phase [ secondary_eclipse_phase ] , secondaryecl_amp , secondaryphase , secondaryecl_std ) ) return modelmags , phase , ptimes , pmags , perrs
11631	def _readNamelist ( currentlyIncluding , cache , namFilename , unique_glyphs ) : filename = os . path . abspath ( os . path . normcase ( namFilename ) ) if filename in currentlyIncluding : raise NamelistRecursionError ( filename ) currentlyIncluding . add ( filename ) try : result = __readNamelist ( cache , filename , unique_glyphs ) finally : currentlyIncluding . remove ( filename ) return result
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
5501	def remove_tweets ( self , url ) : try : del self . cache [ url ] self . mark_updated ( ) return True except KeyError : return False
3333	def dynamic_instantiate_middleware ( name , args , expand = None ) : def _expand ( v ) : if expand and compat . is_basestring ( v ) and v . lower ( ) in expand : return expand [ v ] return v try : the_class = dynamic_import_class ( name ) inst = None if type ( args ) in ( tuple , list ) : args = tuple ( map ( _expand , args ) ) inst = the_class ( * args ) else : assert type ( args ) is dict args = { k : _expand ( v ) for k , v in args . items ( ) } inst = the_class ( ** args ) _logger . debug ( "Instantiate {}({}) => {}" . format ( name , args , inst ) ) except Exception : _logger . exception ( "ERROR: Instantiate {}({}) => {}" . format ( name , args , inst ) ) return inst
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
1900	def get_all_values ( self , constraints , expression , maxcnt = None , silent = False ) : if not isinstance ( expression , Expression ) : return [ expression ] assert isinstance ( constraints , ConstraintSet ) assert isinstance ( expression , Expression ) expression = simplify ( expression ) if maxcnt is None : maxcnt = consts . maxsolutions with constraints as temp_cs : if isinstance ( expression , Bool ) : var = temp_cs . new_bool ( ) elif isinstance ( expression , BitVec ) : var = temp_cs . new_bitvec ( expression . size ) elif isinstance ( expression , Array ) : var = temp_cs . new_array ( index_max = expression . index_max , value_bits = expression . value_bits , taint = expression . taint ) . array else : raise NotImplementedError ( f"get_all_values only implemented for {type(expression)} expression type." ) temp_cs . add ( var == expression ) self . _reset ( temp_cs . to_string ( related_to = var ) ) result = [ ] while self . _is_sat ( ) : value = self . _getvalue ( var ) result . append ( value ) self . _assert ( var != value ) if len ( result ) >= maxcnt : if silent : break else : raise TooManySolutions ( result ) return result
3104	def oauth_enabled ( decorated_function = None , scopes = None , ** decorator_kwargs ) : def curry_wrapper ( wrapped_function ) : @ wraps ( wrapped_function ) def enabled_wrapper ( request , * args , ** kwargs ) : return_url = decorator_kwargs . pop ( 'return_url' , request . get_full_path ( ) ) user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) setattr ( request , django_util . oauth2_settings . request_prefix , user_oauth ) return wrapped_function ( request , * args , ** kwargs ) return enabled_wrapper if decorated_function : return curry_wrapper ( decorated_function ) else : return curry_wrapper
2319	def autoset_settings ( set_var ) : try : devices = ast . literal_eval ( os . environ [ "CUDA_VISIBLE_DEVICES" ] ) if type ( devices ) != list and type ( devices ) != tuple : devices = [ devices ] if len ( devices ) != 0 : set_var . GPU = len ( devices ) set_var . NB_JOBS = len ( devices ) warnings . warn ( "Detecting CUDA devices : {}" . format ( devices ) ) except KeyError : set_var . GPU = check_cuda_devices ( ) set_var . NB_JOBS = set_var . GPU warnings . warn ( "Detecting {} CUDA devices." . format ( set_var . GPU ) ) if not set_var . GPU : warnings . warn ( "No GPU automatically detected. Setting SETTINGS.GPU to 0, " + "and SETTINGS.NB_JOBS to cpu_count." ) set_var . GPU = 0 set_var . NB_JOBS = multiprocessing . cpu_count ( ) return set_var
6683	def require ( self , path = None , contents = None , source = None , url = None , md5 = None , use_sudo = False , owner = None , group = '' , mode = None , verify_remote = True , temp_dir = '/tmp' ) : func = use_sudo and run_as_root or self . run if path and not ( contents or source or url ) : assert path if not self . is_file ( path ) : func ( 'touch "%(path)s"' % locals ( ) ) elif url : if not path : path = os . path . basename ( urlparse ( url ) . path ) if not self . is_file ( path ) or md5 and self . md5sum ( path ) != md5 : func ( 'wget --progress=dot:mega "%(url)s" -O "%(path)s"' % locals ( ) ) else : if source : assert not contents t = None else : fd , source = mkstemp ( ) t = os . fdopen ( fd , 'w' ) t . write ( contents ) t . close ( ) if verify_remote : digest = hashlib . md5 ( ) f = open ( source , 'rb' ) try : while True : d = f . read ( BLOCKSIZE ) if not d : break digest . update ( d ) finally : f . close ( ) else : digest = None if ( not self . is_file ( path , use_sudo = use_sudo ) or ( verify_remote and self . md5sum ( path , use_sudo = use_sudo ) != digest . hexdigest ( ) ) ) : with self . settings ( hide ( 'running' ) ) : self . put ( local_path = source , remote_path = path , use_sudo = use_sudo , temp_dir = temp_dir ) if t is not None : os . unlink ( source ) if use_sudo and owner is None : owner = 'root' if ( owner and self . get_owner ( path , use_sudo ) != owner ) or ( group and self . get_group ( path , use_sudo ) != group ) : func ( 'chown %(owner)s:%(group)s "%(path)s"' % locals ( ) ) if use_sudo and mode is None : mode = oct ( 0o666 & ~ int ( self . umask ( use_sudo = True ) , base = 8 ) ) if mode and self . get_mode ( path , use_sudo ) != mode : func ( 'chmod %(mode)s "%(path)s"' % locals ( ) )
5088	def ecommerce_coupon_url ( self , instance ) : if not instance . entitlement_id : return "N/A" return format_html ( '<a href="{base_url}/coupons/{id}" target="_blank">View coupon "{id}" details</a>' , base_url = settings . ECOMMERCE_PUBLIC_URL_ROOT , id = instance . entitlement_id )
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
4084	def get_newest_possible_languagetool_version ( ) : java_path = find_executable ( 'java' ) if not java_path : return JAVA_6_COMPATIBLE_VERSION output = subprocess . check_output ( [ java_path , '-version' ] , stderr = subprocess . STDOUT , universal_newlines = True ) java_version = parse_java_version ( output ) if java_version >= ( 1 , 8 ) : return LATEST_VERSION elif java_version >= ( 1 , 7 ) : return JAVA_7_COMPATIBLE_VERSION elif java_version >= ( 1 , 6 ) : warn ( 'language-check would be able to use a newer version of ' 'LanguageTool if you had Java 7 or newer installed' ) return JAVA_6_COMPATIBLE_VERSION else : raise SystemExit ( 'You need at least Java 6 to use language-check' )
7188	def get_offset_and_prefix ( body , skip_assignments = False ) : assert body . type in ( syms . file_input , syms . suite ) _offset = 0 prefix = '' for _offset , child in enumerate ( body . children ) : if child . type == syms . simple_stmt : stmt = child . children [ 0 ] if stmt . type == syms . expr_stmt : expr = stmt . children if not skip_assignments : break if ( len ( expr ) != 2 or expr [ 0 ] . type != token . NAME or expr [ 1 ] . type != syms . annassign or _eq in expr [ 1 ] . children ) : break elif stmt . type not in ( syms . import_name , syms . import_from , token . STRING ) : break elif child . type == token . INDENT : assert isinstance ( child , Leaf ) prefix = child . value elif child . type != token . NEWLINE : break prefix , child . prefix = child . prefix , prefix return _offset , prefix
1555	def _add_out_streams ( self , spbl ) : if self . outputs is None : return output_map = self . _sanitize_outputs ( ) for stream_id , out_fields in output_map . items ( ) : out_stream = spbl . outputs . add ( ) out_stream . stream . CopyFrom ( self . _get_stream_id ( self . name , stream_id ) ) out_stream . schema . CopyFrom ( self . _get_stream_schema ( out_fields ) )
4282	def video_size ( source , converter = 'ffmpeg' ) : res = subprocess . run ( [ converter , '-i' , source ] , stderr = subprocess . PIPE ) stderr = res . stderr . decode ( 'utf8' ) pattern = re . compile ( r'Stream.*Video.* ([0-9]+)x([0-9]+)' ) match = pattern . search ( stderr ) rot_pattern = re . compile ( r'rotate\s*:\s*-?(90|270)' ) rot_match = rot_pattern . search ( stderr ) if match : x , y = int ( match . groups ( ) [ 0 ] ) , int ( match . groups ( ) [ 1 ] ) else : x = y = 0 if rot_match : x , y = y , x return x , y
18	def learn ( network , env , seed = None , nsteps = 5 , total_timesteps = int ( 80e6 ) , vf_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 0.5 , lr = 7e-4 , lrschedule = 'linear' , epsilon = 1e-5 , alpha = 0.99 , gamma = 0.99 , log_interval = 100 , load_path = None , ** network_kwargs ) : set_global_seeds ( seed ) nenvs = env . num_envs policy = build_policy ( env , network , ** network_kwargs ) model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) if load_path is not None : model . load ( load_path ) runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) nbatch = nenvs * nsteps tstart = time . time ( ) for update in range ( 1 , total_timesteps // nbatch + 1 ) : obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) nseconds = time . time ( ) - tstart fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : ev = explained_variance ( values , rewards ) logger . record_tabular ( "nupdates" , update ) logger . record_tabular ( "total_timesteps" , update * nbatch ) logger . record_tabular ( "fps" , fps ) logger . record_tabular ( "policy_entropy" , float ( policy_entropy ) ) logger . record_tabular ( "value_loss" , float ( value_loss ) ) logger . record_tabular ( "explained_variance" , float ( ev ) ) logger . record_tabular ( "eprewmean" , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( "eplenmean" , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) return model
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
6350	def _pnums_with_leading_space ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return ' ' + self . _phonetic_number ( phonetic ) prefix = phonetic [ : alt_start ] alt_start += 1 alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for alt in alt_array : result += self . _pnums_with_leading_space ( prefix + alt + suffix ) return result
2159	def _format_yaml ( self , payload ) : return parser . ordered_dump ( payload , Dumper = yaml . SafeDumper , default_flow_style = False )
8214	def gtk_mouse_button_down ( self , widget , event ) : if self . menu_enabled and event . button == 3 : menu = self . uimanager . get_widget ( '/Save as' ) menu . popup ( None , None , None , None , event . button , event . time ) else : super ( ShoebotWindow , self ) . gtk_mouse_button_down ( widget , event )
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
13881	def MoveDirectory ( source_dir , target_dir ) : if not IsDir ( source_dir ) : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( source_dir ) if Exists ( target_dir ) : from . _exceptions import DirectoryAlreadyExistsError raise DirectoryAlreadyExistsError ( target_dir ) from six . moves . urllib . parse import urlparse source_url = urlparse ( source_dir ) target_url = urlparse ( target_dir ) if _UrlIsLocal ( source_url ) and _UrlIsLocal ( target_url ) : import shutil shutil . move ( source_dir , target_dir ) elif source_url . scheme == 'ftp' and target_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( target_url . scheme ) else : raise NotImplementedError ( 'Can only move directories local->local or ftp->ftp' )
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
2114	def update ( self , pk = None , create_on_missing = False , monitor = False , wait = False , timeout = None , name = None , organization = None ) : project = self . get ( pk , name = name , organization = organization ) pk = project [ 'id' ] debug . log ( 'Asking whether the project can be updated.' , header = 'details' ) result = client . get ( '/projects/%d/update/' % pk ) if not result . json ( ) [ 'can_update' ] : raise exc . CannotStartJob ( 'Cannot update project.' ) debug . log ( 'Updating the project.' , header = 'details' ) result = client . post ( '/projects/%d/update/' % pk ) project_update_id = result . json ( ) [ 'project_update' ] if monitor : return self . monitor ( project_update_id , parent_pk = pk , timeout = timeout ) elif wait : return self . wait ( project_update_id , parent_pk = pk , timeout = timeout ) return { 'id' : project_update_id , 'changed' : True , }
813	def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm
13293	def ensure_pandoc ( func ) : logger = logging . getLogger ( __name__ ) @ functools . wraps ( func ) def _install_and_run ( * args , ** kwargs ) : try : result = func ( * args , ** kwargs ) except OSError : message = "pandoc needed but not found. Now installing it for you." logger . warning ( message ) pypandoc . download_pandoc ( version = '1.19.1' ) logger . debug ( "pandoc download complete" ) result = func ( * args , ** kwargs ) return result return _install_and_run
4916	def entitlements ( self , request , pk = None ) : enterprise_customer_user = self . get_object ( ) instance = { "entitlements" : enterprise_customer_user . entitlements } serializer = serializers . EnterpriseCustomerUserEntitlementSerializer ( instance , context = { 'request' : request } ) return Response ( serializer . data )
1094	def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) def finditer ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . finditer ( string )
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
2906	def _assign_new_thread_id ( self , recursive = True ) : self . __class__ . thread_id_pool += 1 self . thread_id = self . __class__ . thread_id_pool if not recursive : return self . thread_id for child in self : child . thread_id = self . thread_id return self . thread_id
7663	def pop_data ( self ) : data = self . data self . data = SortedKeyList ( key = self . _key ) return data
10178	def list_bookmarks ( self , start_date = None , end_date = None , limit = None ) : query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : query = query . filter ( 'range' , date = range_args ) return query [ 0 : limit ] . execute ( ) if limit else query . scan ( )
12311	def record ( self , localStreamName , pathToFile , ** kwargs ) : return self . protocol . execute ( 'record' , localStreamName = localStreamName , pathToFile = pathToFile , ** kwargs )
226	def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs
6409	def seiffert_mean ( nums ) : r if len ( nums ) == 1 : return nums [ 0 ] if len ( nums ) > 2 : raise AttributeError ( 'seiffert_mean supports no more than two values' ) if nums [ 0 ] + nums [ 1 ] == 0 or nums [ 0 ] - nums [ 1 ] == 0 : return float ( 'NaN' ) return ( nums [ 0 ] - nums [ 1 ] ) / ( 2 * math . asin ( ( nums [ 0 ] - nums [ 1 ] ) / ( nums [ 0 ] + nums [ 1 ] ) ) )
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
6981	def get_centroid_offsets ( lcd , t_ing_egr , oot_buffer_time = 0.1 , sample_factor = 3 ) : qnum = int ( np . unique ( lcd [ 'quarter' ] ) ) LOGINFO ( 'Getting centroid offsets (qnum: {:d})...' . format ( qnum ) ) arcsec_per_px = 3.98 times = lcd [ 'ctd_dtr' ] [ 'times' ] ctd_resid_x = lcd [ 'ctd_dtr' ] [ 'ctd_x' ] - lcd [ 'ctd_dtr' ] [ 'fit_ctd_x' ] ctd_resid_y = lcd [ 'ctd_dtr' ] [ 'ctd_y' ] - lcd [ 'ctd_dtr' ] [ 'fit_ctd_y' ] cd = { } for ix , ( t_ing , t_egr ) in enumerate ( t_ing_egr ) : in_tra_times = times [ ( times > t_ing ) & ( times < t_egr ) ] transit_dur = t_egr - t_ing oot_window_len = sample_factor * transit_dur oot_before = times [ ( times < ( t_ing - oot_buffer_time ) ) & ( times > ( t_ing - oot_buffer_time - oot_window_len ) ) ] oot_after = times [ ( times > ( t_egr + oot_buffer_time ) ) & ( times < ( t_egr + oot_buffer_time + oot_window_len ) ) ] oot_times = npconcatenate ( [ oot_before , oot_after ] ) mask_tra = npin1d ( times , in_tra_times ) mask_oot = npin1d ( times , oot_times ) ctd_x_in_tra = ctd_resid_x [ mask_tra ] * arcsec_per_px ctd_y_in_tra = ctd_resid_y [ mask_tra ] * arcsec_per_px ctd_x_oot = ctd_resid_x [ mask_oot ] * arcsec_per_px ctd_y_oot = ctd_resid_y [ mask_oot ] * arcsec_per_px cd [ ix ] = { 'ctd_x_in_tra' : ctd_x_in_tra , 'ctd_y_in_tra' : ctd_y_in_tra , 'ctd_x_oot' : ctd_x_oot , 'ctd_y_oot' : ctd_y_oot , 'npts_in_tra' : len ( ctd_x_in_tra ) , 'npts_oot' : len ( ctd_x_oot ) , 'in_tra_times' : in_tra_times , 'oot_times' : oot_times } LOGINFO ( 'Got centroid offsets (qnum: {:d}).' . format ( qnum ) ) return cd
11393	def relative_to_full ( url , example_url ) : if re . match ( 'https?:\/\/' , url ) : return url domain = get_domain ( example_url ) if domain : return '%s%s' % ( domain , url ) return url
9281	def parse_header ( head ) : try : ( fromcall , path ) = head . split ( '>' , 1 ) except : raise ParseError ( "invalid packet header" ) if ( not 1 <= len ( fromcall ) <= 9 or not re . findall ( r"^[a-z0-9]{0,9}(\-[a-z0-9]{1,8})?$" , fromcall , re . I ) ) : raise ParseError ( "fromcallsign is invalid" ) path = path . split ( ',' ) if len ( path [ 0 ] ) == 0 : raise ParseError ( "no tocallsign in header" ) tocall = path [ 0 ] path = path [ 1 : ] validate_callsign ( tocall , "tocallsign" ) for digi in path : if not re . findall ( r"^[A-Z0-9\-]{1,9}\*?$" , digi , re . I ) : raise ParseError ( "invalid callsign in path" ) parsed = { 'from' : fromcall , 'to' : tocall , 'path' : path , } viacall = "" if len ( path ) >= 2 and re . match ( r"^q..$" , path [ - 2 ] ) : viacall = path [ - 1 ] parsed . update ( { 'via' : viacall } ) return parsed
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
8740	def _create_flip ( context , flip , port_fixed_ips ) : if port_fixed_ips : context . session . begin ( ) try : ports = [ val [ 'port' ] for val in port_fixed_ips . values ( ) ] flip = db_api . port_associate_ip ( context , ports , flip , port_fixed_ips . keys ( ) ) for port_id in port_fixed_ips : fixed_ip = port_fixed_ips [ port_id ] [ 'fixed_ip' ] flip = db_api . floating_ip_associate_fixed_ip ( context , flip , fixed_ip ) flip_driver = registry . DRIVER_REGISTRY . get_driver ( ) flip_driver . register_floating_ip ( flip , port_fixed_ips ) context . session . commit ( ) except Exception : context . session . rollback ( ) raise billing . notify ( context , billing . IP_ASSOC , flip )
5035	def get_pending_users_queryset ( self , search_keyword , customer_uuid ) : queryset = PendingEnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) if search_keyword is not None : queryset = queryset . filter ( user_email__icontains = search_keyword ) return queryset
1374	def parse_override_config_and_write_file ( namespace ) : overrides = parse_override_config ( namespace ) try : tmp_dir = tempfile . mkdtemp ( ) override_config_file = os . path . join ( tmp_dir , OVERRIDE_YAML ) with open ( override_config_file , 'w' ) as f : f . write ( yaml . dump ( overrides ) ) return override_config_file except Exception as e : raise Exception ( "Failed to parse override config: %s" % str ( e ) )
11066	def add_user_to_allow ( self , name , user ) : if not self . remove_user_from_acl ( name , user ) : return False if name not in self . _acl : return False self . _acl [ name ] [ 'allow' ] . append ( user ) return True
10822	def query_invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query_by_user ( user , state = MembershipState . PENDING_USER , eager = eager )
1697	def reduce_by_window ( self , window_config , reduce_function ) : from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) self . _add_child ( reduce_streamlet ) return reduce_streamlet
8841	def jsonLogic ( tests , data = None ) : if tests is None or not isinstance ( tests , dict ) : return tests data = data or { } operator = list ( tests . keys ( ) ) [ 0 ] values = tests [ operator ] if not isinstance ( values , list ) and not isinstance ( values , tuple ) : values = [ values ] values = [ jsonLogic ( val , data ) for val in values ] if operator == 'var' : return get_var ( data , * values ) if operator == 'missing' : return missing ( data , * values ) if operator == 'missing_some' : return missing_some ( data , * values ) if operator not in operations : raise ValueError ( "Unrecognized operation %s" % operator ) return operations [ operator ] ( * values )
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
5980	def bin_up_mask_2d ( mask_2d , bin_up_factor ) : padded_array_2d = array_util . pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = mask_2d , bin_up_factor = bin_up_factor , pad_value = True ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = True for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 if padded_array_2d [ padded_y , padded_x ] == False : value = False binned_array_2d [ y , x ] = value return binned_array_2d
7626	def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , ** kwargs )
73	def Emboss ( alpha = 0 , strength = 1 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) strength_param = iap . handle_continuous_param ( strength , "strength" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) strength_sample = strength_param . draw_sample ( random_state = random_state_func ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ - 1 - strength_sample , 0 - strength_sample , 0 ] , [ 0 - strength_sample , 1 , 0 + strength_sample ] , [ 0 , 0 + strength_sample , 1 + strength_sample ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
7558	def resolve_ambigs ( tmpseq ) : for aidx in xrange ( 6 ) : ambig , res1 , res2 = GETCONS [ aidx ] idx , idy = np . where ( tmpseq == ambig ) halfmask = np . random . choice ( np . array ( [ True , False ] ) , idx . shape [ 0 ] ) for col in xrange ( idx . shape [ 0 ] ) : if halfmask [ col ] : tmpseq [ idx [ col ] , idy [ col ] ] = res1 else : tmpseq [ idx [ col ] , idy [ col ] ] = res2 return tmpseq
13010	def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
3315	def _find ( self , url ) : vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
13412	def changeLogType ( self ) : logType = self . selectedType ( ) programs = self . logList . get ( logType ) [ 0 ] default = self . logList . get ( logType ) [ 1 ] if logType in self . logList : self . programName . clear ( ) self . programName . addItems ( programs ) self . programName . setCurrentIndex ( programs . index ( default ) )
13377	def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
19	def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
852	def getNextRecord ( self , useCache = True ) : assert self . _file is not None assert self . _mode == self . _FILE_READ_MODE try : line = self . _reader . next ( ) except StopIteration : if self . rewindAtEOF : if self . _recordCount == 0 : raise Exception ( "The source configured to reset at EOF but " "'%s' appears to be empty" % self . _filename ) self . rewind ( ) line = self . _reader . next ( ) else : return None self . _recordCount += 1 record = [ ] for i , f in enumerate ( line ) : if f in self . _missingValues : record . append ( SENTINEL_VALUE_FOR_MISSING_DATA ) else : record . append ( self . _adapters [ i ] ( f ) ) return record
8082	def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
6635	def ignores ( self , path ) : test_path = PurePath ( '/' , path ) test_paths = tuple ( [ test_path ] + list ( test_path . parents ) ) for exp in self . ignore_patterns : for tp in test_paths : if tp . match ( exp ) : logger . debug ( '"%s" ignored ("%s" matched "%s")' , path , tp , exp ) return True return False
6060	def numpy_array_2d_from_fits ( file_path , hdu ) : hdu_list = fits . open ( file_path ) return np . flipud ( np . array ( hdu_list [ hdu ] . data ) )
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
5059	def send_email_notification_message ( user , enrolled_in , enterprise_customer , email_connection = None ) : if hasattr ( user , 'first_name' ) and hasattr ( user , 'username' ) : user_name = user . first_name if not user_name : user_name = user . username else : user_name = None if hasattr ( user , 'email' ) : user_email = user . email elif hasattr ( user , 'user_email' ) : user_email = user . user_email else : raise TypeError ( _ ( '`user` must have one of either `email` or `user_email`.' ) ) msg_context = { 'user_name' : user_name , 'enrolled_in' : enrolled_in , 'organization_name' : enterprise_customer . name , } try : enterprise_template_config = enterprise_customer . enterprise_enrollment_template except ( ObjectDoesNotExist , AttributeError ) : enterprise_template_config = None plain_msg , html_msg = build_notification_message ( msg_context , enterprise_template_config ) subject_line = get_notification_subject_line ( enrolled_in [ 'name' ] , enterprise_template_config ) from_email_address = get_configuration_value_for_site ( enterprise_customer . site , 'DEFAULT_FROM_EMAIL' , default = settings . DEFAULT_FROM_EMAIL ) return mail . send_mail ( subject_line , plain_msg , from_email_address , [ user_email ] , html_message = html_msg , connection = email_connection )
12796	def parse ( self , text , key = None ) : try : data = json . loads ( text ) except ValueError as e : raise ValueError ( "%s: Value: [%s]" % ( e , text ) ) if data and key : if key not in data : raise ValueError ( "Invalid response (key %s not found): %s" % ( key , data ) ) data = data [ key ] return data
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
10222	def get_nift_values ( ) -> Mapping [ str , str ] : r = get_bel_resource ( NIFT ) return { name . lower ( ) : name for name in r [ 'Values' ] }
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
5688	def get_transit_events ( self , start_time_ut = None , end_time_ut = None , route_type = None ) : table_name = self . _get_day_trips_table_name ( ) event_query = "SELECT stop_I, seq, trip_I, route_I, routes.route_id AS route_id, routes.type AS route_type, " "shape_id, day_start_ut+dep_time_ds AS dep_time_ut, day_start_ut+arr_time_ds AS arr_time_ut " "FROM " + table_name + " " "JOIN trips USING(trip_I) " "JOIN routes USING(route_I) " "JOIN stop_times USING(trip_I)" where_clauses = [ ] if end_time_ut : where_clauses . append ( table_name + ".start_time_ut< {end_time_ut}" . format ( end_time_ut = end_time_ut ) ) where_clauses . append ( "dep_time_ut <={end_time_ut}" . format ( end_time_ut = end_time_ut ) ) if start_time_ut : where_clauses . append ( table_name + ".end_time_ut > {start_time_ut}" . format ( start_time_ut = start_time_ut ) ) where_clauses . append ( "arr_time_ut >={start_time_ut}" . format ( start_time_ut = start_time_ut ) ) if route_type is not None : assert route_type in ALL_ROUTE_TYPES where_clauses . append ( "routes.type={route_type}" . format ( route_type = route_type ) ) if len ( where_clauses ) > 0 : event_query += " WHERE " for i , where_clause in enumerate ( where_clauses ) : if i is not 0 : event_query += " AND " event_query += where_clause event_query += " ORDER BY trip_I, day_start_ut+dep_time_ds;" events_result = pd . read_sql_query ( event_query , self . conn ) from_indices = numpy . nonzero ( ( events_result [ 'trip_I' ] [ : - 1 ] . values == events_result [ 'trip_I' ] [ 1 : ] . values ) * ( events_result [ 'seq' ] [ : - 1 ] . values < events_result [ 'seq' ] [ 1 : ] . values ) ) [ 0 ] to_indices = from_indices + 1 assert ( events_result [ 'trip_I' ] [ from_indices ] . values == events_result [ 'trip_I' ] [ to_indices ] . values ) . all ( ) trip_Is = events_result [ 'trip_I' ] [ from_indices ] from_stops = events_result [ 'stop_I' ] [ from_indices ] to_stops = events_result [ 'stop_I' ] [ to_indices ] shape_ids = events_result [ 'shape_id' ] [ from_indices ] dep_times = events_result [ 'dep_time_ut' ] [ from_indices ] arr_times = events_result [ 'arr_time_ut' ] [ to_indices ] route_types = events_result [ 'route_type' ] [ from_indices ] route_ids = events_result [ 'route_id' ] [ from_indices ] route_Is = events_result [ 'route_I' ] [ from_indices ] durations = arr_times . values - dep_times . values assert ( durations >= 0 ) . all ( ) from_seqs = events_result [ 'seq' ] [ from_indices ] to_seqs = events_result [ 'seq' ] [ to_indices ] data_tuples = zip ( from_stops , to_stops , dep_times , arr_times , shape_ids , route_types , route_ids , trip_Is , durations , from_seqs , to_seqs , route_Is ) columns = [ "from_stop_I" , "to_stop_I" , "dep_time_ut" , "arr_time_ut" , "shape_id" , "route_type" , "route_id" , "trip_I" , "duration" , "from_seq" , "to_seq" , "route_I" ] df = pd . DataFrame . from_records ( data_tuples , columns = columns ) return df
7654	def summary ( obj , indent = 0 ) : if hasattr ( obj , '__summary__' ) : rep = obj . __summary__ ( ) elif isinstance ( obj , SortedKeyList ) : rep = '<{:d} observations>' . format ( len ( obj ) ) else : rep = repr ( obj ) return rep . replace ( '\n' , '\n' + ' ' * indent )
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
2230	def register ( self , hash_types ) : if not isinstance ( hash_types , ( list , tuple ) ) : hash_types = [ hash_types ] def _decor_closure ( hash_func ) : for hash_type in hash_types : key = ( hash_type . __module__ , hash_type . __name__ ) self . keyed_extensions [ key ] = ( hash_type , hash_func ) return hash_func return _decor_closure
193	def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
9374	def download_file ( url ) : try : ( local_file , headers ) = urllib . urlretrieve ( url ) except : sys . exit ( "ERROR: Problem downloading config file. Please check the URL (" + url + "). Exiting..." ) return local_file
624	def neighborhood ( centerIndex , radius , dimensions ) : centerPosition = coordinatesFromIndex ( centerIndex , dimensions ) intervals = [ ] for i , dimension in enumerate ( dimensions ) : left = max ( 0 , centerPosition [ i ] - radius ) right = min ( dimension - 1 , centerPosition [ i ] + radius ) intervals . append ( xrange ( left , right + 1 ) ) coords = numpy . array ( list ( itertools . product ( * intervals ) ) ) return numpy . ravel_multi_index ( coords . T , dimensions )
4348	def trim ( self , start_time , end_time = None ) : if not is_number ( start_time ) or start_time < 0 : raise ValueError ( "start_time must be a positive number." ) effect_args = [ 'trim' , '{:f}' . format ( start_time ) ] if end_time is not None : if not is_number ( end_time ) or end_time < 0 : raise ValueError ( "end_time must be a positive number." ) if start_time >= end_time : raise ValueError ( "start_time must be smaller than end_time." ) effect_args . append ( '{:f}' . format ( end_time - start_time ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'trim' ) return self
6880	def _parse_csv_header ( header ) : headerlines = header . split ( '\n' ) headerlines = [ x . lstrip ( '# ' ) for x in headerlines ] objectstart = headerlines . index ( 'OBJECT' ) metadatastart = headerlines . index ( 'METADATA' ) camfilterstart = headerlines . index ( 'CAMFILTERS' ) photaperturestart = headerlines . index ( 'PHOTAPERTURES' ) columnstart = headerlines . index ( 'COLUMNS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) objectinfo = headerlines [ objectstart + 1 : metadatastart - 1 ] metadatainfo = headerlines [ metadatastart + 1 : camfilterstart - 1 ] camfilterinfo = headerlines [ camfilterstart + 1 : photaperturestart - 1 ] photapertureinfo = headerlines [ photaperturestart + 1 : columnstart - 1 ] columninfo = headerlines [ columnstart + 1 : lcstart - 1 ] metadict = { 'objectinfo' : { } } objectinfo = [ x . split ( ';' ) for x in objectinfo ] for elem in objectinfo : for kvelem in elem : key , val = kvelem . split ( ' = ' , 1 ) metadict [ 'objectinfo' ] [ key . strip ( ) ] = ( _smartcast ( val , METAKEYS [ key . strip ( ) ] ) ) metadict [ 'objectid' ] = metadict [ 'objectinfo' ] [ 'objectid' ] [ : ] del metadict [ 'objectinfo' ] [ 'objectid' ] metadatainfo = [ x . split ( ';' ) for x in metadatainfo ] for elem in metadatainfo : for kvelem in elem : try : key , val = kvelem . split ( ' = ' , 1 ) if key . strip ( ) == 'lcbestaperture' : val = json . loads ( val ) if key . strip ( ) in ( 'datarelease' , 'lcversion' ) : val = int ( val ) if key . strip ( ) == 'lastupdated' : val = float ( val ) metadict [ key . strip ( ) ] = val except Exception as e : LOGWARNING ( 'could not understand header element "%s",' ' skipped.' % kvelem ) metadict [ 'filters' ] = [ ] for row in camfilterinfo : filterid , filtername , filterdesc = row . split ( ' - ' ) metadict [ 'filters' ] . append ( ( int ( filterid ) , filtername , filterdesc ) ) metadict [ 'lcapertures' ] = { } for row in photapertureinfo : apnum , appix = row . split ( ' - ' ) appix = float ( appix . rstrip ( ' px' ) ) metadict [ 'lcapertures' ] [ apnum . strip ( ) ] = appix metadict [ 'columns' ] = [ ] for row in columninfo : colnum , colname , coldesc = row . split ( ' - ' ) metadict [ 'columns' ] . append ( colname ) return metadict
12286	def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
3746	def calculate_P ( self , T , P , method ) : r if method == LUCAS : mu = self . T_dependent_property ( T ) Psat = self . Psat ( T ) if hasattr ( self . Psat , '__call__' ) else self . Psat mu = Lucas ( T , P , self . Tc , self . Pc , self . omega , Psat , mu ) elif method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
4988	def eligible_for_direct_audit_enrollment ( self , request , enterprise_customer , resource_id , course_key = None ) : course_identifier = course_key if course_key else resource_id return request . GET . get ( 'audit' ) and request . path == self . COURSE_ENROLLMENT_VIEW_URL . format ( enterprise_customer . uuid , course_identifier ) and enterprise_customer . catalog_contains_course ( resource_id ) and EnrollmentApiClient ( ) . has_course_mode ( resource_id , 'audit' )
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
12854	def parse ( filename ) : for event , elt in et . iterparse ( filename , events = ( 'start' , 'end' , 'comment' , 'pi' ) , huge_tree = True ) : if event == 'start' : obj = _elt2obj ( elt ) obj [ 'type' ] = ENTER yield obj if elt . text : yield { 'type' : TEXT , 'text' : elt . text } elif event == 'end' : yield { 'type' : EXIT } if elt . tail : yield { 'type' : TEXT , 'text' : elt . tail } elt . clear ( ) elif event == 'comment' : yield { 'type' : COMMENT , 'text' : elt . text } elif event == 'pi' : yield { 'type' : PI , 'text' : elt . text } else : assert False , ( event , elt )
3801	def calculate ( self , T , method ) : r if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
3852	def _get_lookup_spec ( identifier ) : if identifier . startswith ( '+' ) : return hangups . hangouts_pb2 . EntityLookupSpec ( phone = identifier , create_offnetwork_gaia = True ) elif '@' in identifier : return hangups . hangouts_pb2 . EntityLookupSpec ( email = identifier , create_offnetwork_gaia = True ) else : return hangups . hangouts_pb2 . EntityLookupSpec ( gaia_id = identifier )
4323	def contrast ( self , amount = 75 ) : if not is_number ( amount ) or amount < 0 or amount > 100 : raise ValueError ( 'amount must be a number between 0 and 100.' ) effect_args = [ 'contrast' , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'contrast' ) return self
7267	def run ( self , * args , ** kw ) : log . debug ( '[operator] run "{}" with arguments: {}' . format ( self . __class__ . __name__ , args ) ) if self . kind == OperatorTypes . ATTRIBUTE : return self . match ( self . ctx ) else : return self . run_matcher ( * args , ** kw )
2171	def unified_job_template_options ( method ) : jt_dec = click . option ( '--job-template' , type = types . Related ( 'job_template' ) , help = 'Use this job template as unified_job_template field' ) prj_dec = click . option ( '--project' , type = types . Related ( 'project' ) , help = 'Use this project as unified_job_template field' ) inv_src_dec = click . option ( '--inventory-source' , type = types . Related ( 'inventory_source' ) , help = 'Use this inventory source as unified_job_template field' ) def ujt_translation ( _method ) : def _ujt_translation ( * args , ** kwargs ) : for fd in [ 'job_template' , 'project' , 'inventory_source' ] : if fd in kwargs and kwargs [ fd ] is not None : kwargs [ 'unified_job_template' ] = kwargs . pop ( fd ) return _method ( * args , ** kwargs ) return functools . wraps ( _method ) ( _ujt_translation ) return ujt_translation ( inv_src_dec ( prj_dec ( jt_dec ( method ) ) ) )
13543	def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( ** task )
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_topologies_watch ( state_manager , topologies ) : Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
7245	def _tile_coords ( self , bounds ) : tfm = partial ( pyproj . transform , pyproj . Proj ( init = "epsg:3857" ) , pyproj . Proj ( init = "epsg:4326" ) ) bounds = ops . transform ( tfm , box ( * bounds ) ) . bounds west , south , east , north = bounds epsilon = 1.0e-10 if east != west and north != south : west += epsilon south += epsilon east -= epsilon north -= epsilon params = [ west , south , east , north , [ self . zoom_level ] ] tile_coords = [ ( tile . x , tile . y ) for tile in mercantile . tiles ( * params ) ] xtiles , ytiles = zip ( * tile_coords ) minx = min ( xtiles ) miny = min ( ytiles ) maxx = max ( xtiles ) maxy = max ( ytiles ) return minx , miny , maxx , maxy
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
9731	def get_force_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . plate_count ) : component_position , plate = QRTPacket . _get_exact ( RTForcePlateSingle , data , component_position ) component_position , force = QRTPacket . _get_exact ( RTForce , data , component_position ) append_components ( ( plate , force ) ) return components
12813	def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
7774	def _compute_response ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) : logger . debug ( "_compute_response{0!r}" . format ( ( urp_hash , nonce , cnonce , nonce_count , authzid , digest_uri ) ) ) if authzid : a1 = b":" . join ( ( urp_hash , nonce , cnonce , authzid ) ) else : a1 = b":" . join ( ( urp_hash , nonce , cnonce ) ) a2 = b"AUTHENTICATE:" + digest_uri return b2a_hex ( _kd_value ( b2a_hex ( _h_value ( a1 ) ) , b":" . join ( ( nonce , nonce_count , cnonce , b"auth" , b2a_hex ( _h_value ( a2 ) ) ) ) ) )
5607	def resample_from_array ( in_raster = None , in_affine = None , out_tile = None , in_crs = None , resampling = "nearest" , nodataval = 0 ) : if isinstance ( in_raster , ma . MaskedArray ) : pass if isinstance ( in_raster , np . ndarray ) : in_raster = ma . MaskedArray ( in_raster , mask = in_raster == nodataval ) elif isinstance ( in_raster , ReferencedRaster ) : in_affine = in_raster . affine in_crs = in_raster . crs in_raster = in_raster . data elif isinstance ( in_raster , tuple ) : in_raster = ma . MaskedArray ( data = np . stack ( in_raster ) , mask = np . stack ( [ band . mask if isinstance ( band , ma . masked_array ) else np . where ( band == nodataval , True , False ) for band in in_raster ] ) , fill_value = nodataval ) else : raise TypeError ( "wrong input data type: %s" % type ( in_raster ) ) if in_raster . ndim == 2 : in_raster = ma . expand_dims ( in_raster , axis = 0 ) elif in_raster . ndim == 3 : pass else : raise TypeError ( "input array must have 2 or 3 dimensions" ) if in_raster . fill_value != nodataval : ma . set_fill_value ( in_raster , nodataval ) out_shape = ( in_raster . shape [ 0 ] , ) + out_tile . shape dst_data = np . empty ( out_shape , in_raster . dtype ) in_raster = ma . masked_array ( data = in_raster . filled ( ) , mask = in_raster . mask , fill_value = nodataval ) reproject ( in_raster , dst_data , src_transform = in_affine , src_crs = in_crs if in_crs else out_tile . crs , dst_transform = out_tile . affine , dst_crs = out_tile . crs , resampling = Resampling [ resampling ] ) return ma . MaskedArray ( dst_data , mask = dst_data == nodataval )
8395	def show_help ( ) : print ( ) for cmd in [ write_main , check_main , list_main ] : print ( cmd . __doc__ . lstrip ( "\n" ) )
13723	def log_file ( self , url = None ) : if url is None : url = self . url f = re . sub ( "file://" , "" , url ) try : with open ( f , "a" ) as of : of . write ( str ( self . store . get_json_tuples ( True ) ) ) except IOError as e : print ( e ) print ( "Could not write the content to the file.." )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
3890	def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
3803	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return mixing_simple ( zs , ks ) elif method == DIPPR_9H : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return DIPPR9H ( ws , ks ) elif method == FILIPPOV : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return Filippov ( ws , ks ) elif method == MAGOMEDOV : k_w = self . ThermalConductivityLiquids [ self . index_w ] ( T , P ) ws = list ( ws ) ws . pop ( self . index_w ) return thermal_conductivity_Magomedov ( T , P , ws , self . wCASs , k_w ) else : raise Exception ( 'Method not valid' )
11697	def count ( self ) : xml = get_changeset ( self . id ) actions = [ action . tag for action in xml . getchildren ( ) ] self . create = actions . count ( 'create' ) self . modify = actions . count ( 'modify' ) self . delete = actions . count ( 'delete' ) self . verify_editor ( ) try : if ( self . create / len ( actions ) > self . percentage and self . create > self . create_threshold and ( self . powerfull_editor or self . create > self . top_threshold ) ) : self . label_suspicious ( 'possible import' ) elif ( self . modify / len ( actions ) > self . percentage and self . modify > self . modify_threshold ) : self . label_suspicious ( 'mass modification' ) elif ( ( self . delete / len ( actions ) > self . percentage and self . delete > self . delete_threshold ) or self . delete > self . top_threshold ) : self . label_suspicious ( 'mass deletion' ) except ZeroDivisionError : print ( 'It seems this changeset was redacted' )
4082	def set_directory ( path = None ) : old_path = get_directory ( ) terminate_server ( ) cache . clear ( ) if path : cache [ 'language_check_dir' ] = path try : get_jar_info ( ) except Error : cache [ 'language_check_dir' ] = old_path raise
12383	def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
7776	def __from_xml ( self , value ) : n = value . children vns = get_node_ns ( value ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and vns and ns . getContent ( ) != vns . getContent ( ) ) : n = n . next continue if n . name == 'POBOX' : self . pobox = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( 'EXTADR' , 'EXTADD' ) : self . extadr = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'STREET' : self . street = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'LOCALITY' : self . locality = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'REGION' : self . region = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'PCODE' : self . pcode = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name == 'CTRY' : self . ctry = unicode ( n . getContent ( ) , "utf-8" , "replace" ) elif n . name in ( "HOME" , "WORK" , "POSTAL" , "PARCEL" , "DOM" , "INTL" , "PREF" ) : self . type . append ( n . name . lower ( ) ) n = n . next if self . type == [ ] : self . type = [ "intl" , "postal" , "parcel" , "work" ] elif "dom" in self . type and "intl" in self . type : raise ValueError ( "Both 'dom' and 'intl' specified in vcard ADR" )
11828	def exact_sqrt ( n2 ) : "If n2 is a perfect square, return its square root, else raise error." n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
11575	def encoder_data ( self , data ) : prev_val = self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) if val > 8192 : val -= 16384 pin = data [ 0 ] with self . pymata . data_lock : self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if prev_val != val : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback is not None : callback ( [ self . pymata . ENCODER , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] )
6165	def sqrt_rc_imp ( Ns , alpha , M = 6 ) : n = np . arange ( - M * Ns , M * Ns + 1 ) b = np . zeros ( len ( n ) ) Ns *= 1.0 a = alpha for i in range ( len ( n ) ) : if abs ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) <= np . finfo ( np . float ) . eps / 2 : b [ i ] = 1 / 2. * ( ( 1 + a ) * np . sin ( ( 1 + a ) * np . pi / ( 4. * a ) ) - ( 1 - a ) * np . cos ( ( 1 - a ) * np . pi / ( 4. * a ) ) + ( 4 * a ) / np . pi * np . sin ( ( 1 - a ) * np . pi / ( 4. * a ) ) ) else : b [ i ] = 4 * a / ( np . pi * ( 1 - 16 * a ** 2 * ( n [ i ] / Ns ) ** 2 ) ) b [ i ] = b [ i ] * ( np . cos ( ( 1 + a ) * np . pi * n [ i ] / Ns ) + np . sinc ( ( 1 - a ) * n [ i ] / Ns ) * ( 1 - a ) * np . pi / ( 4. * a ) ) return b
5490	def discover ( cls ) : file = os . path . join ( Config . config_dir , Config . config_name ) return cls . from_file ( file )
10983	def get_initial_featuring ( statemaker , feature_rad , actual_rad = None , im_name = None , tile = None , invert = True , desc = '' , use_full_path = False , featuring_params = { } , statemaker_kwargs = { } , ** kwargs ) : if actual_rad is None : actual_rad = feature_rad _ , im_name = _pick_state_im_name ( '' , im_name , use_full_path = use_full_path ) im = util . RawImage ( im_name , tile = tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) if np . size ( pos ) == 0 : msg = 'No particles found. Try using a smaller `feature_rad`.' raise ValueError ( msg ) rad = np . ones ( pos . shape [ 0 ] , dtype = 'float' ) * actual_rad s = statemaker ( im , pos , rad , ** statemaker_kwargs ) RLOG . info ( 'State Created.' ) if desc is not None : states . save ( s , desc = desc + 'initial' ) optimize_from_initial ( s , invert = invert , desc = desc , ** kwargs ) return s
13879	def AppendToFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , encoding = None , binary = False ) : _AssertIsLocal ( filename ) assert isinstance ( contents , six . text_type ) ^ binary , 'Must always receive unicode contents, unless binary=True' if not binary : contents = _HandleContentsEol ( contents , eol_style ) contents = contents . encode ( encoding or sys . getfilesystemencoding ( ) ) oss = open ( filename , 'ab' ) try : oss . write ( contents ) finally : oss . close ( )
11058	def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
324	def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
6788	def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
12941	def pprint ( self , stream = None ) : pprint . pprint ( self . asDict ( includeMeta = True , forStorage = False , strKeys = True ) , stream = stream )
8074	def image ( self , path , x , y , width = None , height = None , alpha = 1.0 , data = None , draw = True , ** kwargs ) : return self . Image ( path , x , y , width , height , alpha , data , ** kwargs )
6036	def array_2d_from_array_1d ( self , array_1d ) : return mapping_util . map_masked_1d_array_to_2d_array_from_array_1d_shape_and_one_to_two ( array_1d = array_1d , shape = self . mask . shape , one_to_two = self . mask . masked_grid_index_to_pixel )
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
3018	def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
11550	def setup ( self , configuration = "ModbusSerialClient(method='rtu',port='/dev/cu.usbmodem14101',baudrate=9600)" ) : from pymodbus3 . client . sync import ModbusSerialClient , ModbusUdpClient , ModbusTcpClient self . _client = eval ( configuration ) self . _client . connect ( )
8311	def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
225	async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ "type" ] assert message_type in { "websocket.accept" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ "type" ] assert message_type in { "websocket.send" , "websocket.close" } if message_type == "websocket.close" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call "send" once a close message has been sent.' )
10474	def _isSingleCharacter ( keychr ) : if not keychr : return False if len ( keychr ) == 1 : return True return keychr . count ( '<' ) == 1 and keychr . count ( '>' ) == 1 and keychr [ 0 ] == '<' and keychr [ - 1 ] == '>'
9077	def make_downloader ( url : str , path : str ) -> Callable [ [ bool ] , str ] : def download_data ( force_download : bool = False ) -> str : if os . path . exists ( path ) and not force_download : log . info ( 'using cached data at %s' , path ) else : log . info ( 'downloading %s to %s' , url , path ) urlretrieve ( url , path ) return path return download_data
10423	def pair_is_consistent ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> Optional [ str ] : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } if 1 != len ( relations ) : return return list ( relations ) [ 0 ]
7727	def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != self . ns : pass elif n . name == "item" : ret . append ( MucItem ( n ) ) elif n . name == "status" : ret . append ( MucStatus ( n ) ) n = n . next return ret
6025	def geometry_from_grid ( self , grid , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer pixel_scales = ( float ( ( y_max - y_min ) / self . shape [ 0 ] ) , float ( ( x_max - x_min ) / self . shape [ 1 ] ) ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) pixel_neighbors , pixel_neighbors_size = self . neighbors_from_pixelization ( ) return self . Geometry ( shape = self . shape , pixel_scales = pixel_scales , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
13582	def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) writer . writerow ( dict ( ( x , x ) for x in header ) ) for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
8275	def recombine ( self , other , d = 0.7 ) : a , b = self , other d1 = max ( 0 , min ( d , 1 ) ) d2 = d1 c = ColorTheme ( name = a . name [ : int ( len ( a . name ) * d1 ) ] + b . name [ int ( len ( b . name ) * d2 ) : ] , ranges = a . ranges [ : int ( len ( a . ranges ) * d1 ) ] + b . ranges [ int ( len ( b . ranges ) * d2 ) : ] , top = a . top , cache = os . path . join ( DEFAULT_CACHE , "recombined" ) , blue = a . blue , length = a . length * d1 + b . length * d2 ) c . tags = a . tags [ : int ( len ( a . tags ) * d1 ) ] c . tags += b . tags [ int ( len ( b . tags ) * d2 ) : ] return c
5941	def transform_args ( self , * args , ** kwargs ) : newargs = self . _combineargs ( * args , ** kwargs ) return self . _build_arg_list ( ** newargs )
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
6182	def hash ( self ) : hash_list = [ ] for key , value in sorted ( self . __dict__ . items ( ) ) : if not callable ( value ) : if isinstance ( value , np . ndarray ) : hash_list . append ( value . tostring ( ) ) else : hash_list . append ( str ( value ) ) return hashlib . md5 ( repr ( hash_list ) . encode ( ) ) . hexdigest ( )
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) Quartz . CGEventSetFlags ( keyDown , modFlags ) Quartz . CGEventSetFlags ( keyUp , modFlags ) if not globally : macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
8109	def search_images ( q , start = 0 , size = "" , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_IMAGES return GoogleSearch ( q , start , service , size , wait , asynchronous , cached )
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
1364	def get_argument_length ( self ) : try : length = self . get_argument ( constants . PARAM_LENGTH ) return length except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
8861	def defined_names ( request_data ) : global _old_definitions ret_val = [ ] path = request_data [ 'path' ] toplvl_definitions = jedi . names ( request_data [ 'code' ] , path , 'utf-8' ) for d in toplvl_definitions : definition = _extract_def ( d , path ) if d . type != 'import' : ret_val . append ( definition ) ret_val = [ d . to_dict ( ) for d in ret_val ] return ret_val
10327	def statistics ( graph , ps , spanning_cluster = True , model = 'bond' , alpha = alpha_1sigma , runs = 40 ) : my_microcanonical_averages = microcanonical_averages ( graph = graph , runs = runs , spanning_cluster = spanning_cluster , model = model , alpha = alpha ) my_microcanonical_averages_arrays = microcanonical_averages_arrays ( my_microcanonical_averages ) return canonical_averages ( ps , my_microcanonical_averages_arrays )
10484	def _matchOther ( self , obj , ** kwargs ) : if obj is not None : if self . _findFirstR ( ** kwargs ) : return obj . _match ( ** kwargs ) return False
6191	def volume ( self ) : return ( self . x2 - self . x1 ) * ( self . y2 - self . y1 ) * ( self . z2 - self . z1 )
527	def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )
2035	def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
23	def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( "data" ) as f : return pickle . load ( f ) else : with open ( path , "rb" ) as f : return pickle . load ( f )
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name return list ( solvers ) [ 0 ] elif qp : for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
6957	def _log_prior_transit ( theta , priorbounds ) : allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
11020	def photos ( context , path ) : config = context . obj header ( 'Looking for the latest article...' ) article_filename = find_last_article ( config [ 'CONTENT_DIR' ] ) if not article_filename : return click . secho ( 'No articles.' , fg = 'red' ) click . echo ( os . path . basename ( article_filename ) ) header ( 'Looking for images...' ) images = list ( sorted ( find_images ( path ) ) ) if not images : return click . secho ( 'Found no images.' , fg = 'red' ) for filename in images : click . secho ( filename , fg = 'green' ) if not click . confirm ( '\nAdd these images to the latest article' ) : abort ( config ) url_prefix = os . path . join ( '{filename}' , IMAGES_PATH ) images_dir = os . path . join ( config [ 'CONTENT_DIR' ] , IMAGES_PATH ) os . makedirs ( images_dir , exist_ok = True ) header ( 'Processing images...' ) urls = [ ] for filename in images : image_basename = os . path . basename ( filename ) . replace ( ' ' , '-' ) . lower ( ) urls . append ( os . path . join ( url_prefix , image_basename ) ) image_filename = os . path . join ( images_dir , image_basename ) print ( filename , image_filename ) import_image ( filename , image_filename ) content = '\n' for url in urls : url = url . replace ( '\\' , '/' ) content += '\n![image description]({})\n' . format ( url ) header ( 'Adding to article: {}' . format ( article_filename ) ) with click . open_file ( article_filename , 'a' ) as f : f . write ( content ) click . launch ( article_filename )
3489	def _sbase_notes_dict ( sbase , notes ) : if notes and len ( notes ) > 0 : tokens = [ '<html xmlns = "http://www.w3.org/1999/xhtml" >' ] + [ "<p>{}: {}</p>" . format ( k , v ) for ( k , v ) in notes . items ( ) ] + [ "</html>" ] _check ( sbase . setNotes ( "\n" . join ( tokens ) ) , "Setting notes on sbase: {}" . format ( sbase ) )
8171	def alignment ( self , d = 5 ) : vx = vy = vz = 0 for b in self . boids : if b != self : vx , vy , vz = vx + b . vx , vy + b . vy , vz + b . vz n = len ( self . boids ) - 1 vx , vy , vz = vx / n , vy / n , vz / n return ( vx - self . vx ) / d , ( vy - self . vy ) / d , ( vz - self . vz ) / d
5066	def get_enterprise_customer_or_404 ( enterprise_uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : enterprise_uuid = UUID ( enterprise_uuid ) return EnterpriseCustomer . objects . get ( uuid = enterprise_uuid ) except ( TypeError , ValueError , EnterpriseCustomer . DoesNotExist ) : LOGGER . error ( 'Unable to find enterprise customer for UUID: [%s]' , enterprise_uuid ) raise Http404
12303	def url_is_valid ( self , url ) : if url . startswith ( "file://" ) : url = url . replace ( "file://" , "" ) return os . path . exists ( url )
12188	def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : self . _assert_version ( VER_3_0 )
12939	def clearRedisPools ( ) : global RedisPools global _redisManagedConnectionParams for pool in RedisPools . values ( ) : try : pool . disconnect ( ) except : pass for paramsList in _redisManagedConnectionParams . values ( ) : for params in paramsList : if 'connection_pool' in params : del params [ 'connection_pool' ] RedisPools . clear ( ) _redisManagedConnectionParams . clear ( )
3309	def _run_wsgiref ( app , config , mode ) : from wsgiref . simple_server import make_server , software_version version = "WsgiDAV/{} {}" . format ( __version__ , software_version ) _logger . info ( "Running {}..." . format ( version ) ) _logger . warning ( "WARNING: This single threaded server (wsgiref) is not meant for production." ) httpd = make_server ( config [ "host" ] , config [ "port" ] , app ) try : httpd . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( ** experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
2551	def unescape ( data ) : cc = re . compile ( r'&(?:(?:#(\d+))|([^;]+));' ) result = [ ] m = cc . search ( data ) while m : result . append ( data [ 0 : m . start ( ) ] ) d = m . group ( 1 ) if d : d = int ( d ) result . append ( unichr ( d ) ) else : d = _unescape . get ( m . group ( 2 ) , ord ( '?' ) ) result . append ( unichr ( d ) ) data = data [ m . end ( ) : ] m = cc . search ( data ) result . append ( data ) return '' . join ( result )
8523	def add_float ( self , name , min , max , warp = None ) : min , max = map ( float , ( min , max ) ) if not min < max : raise ValueError ( 'variable %s: min >= max error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = FloatVariable ( name , min , max , warp )
484	def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return
2456	def set_pkg_license_from_file ( self , doc , lic ) : self . assert_package_exists ( ) if validations . validate_lics_from_file ( lic ) : doc . package . licenses_from_files . append ( lic ) return True else : raise SPDXValueError ( 'Package::LicensesFromFile' )
4351	def join ( self , room ) : self . socket . rooms . add ( self . _get_room_name ( room ) )
13215	def dump ( self , name , filename ) : if not self . exists ( name ) : raise DatabaseError ( 'database %s does not exist!' ) log . info ( 'dumping %s to %s' % ( name , filename ) ) self . _run_cmd ( 'pg_dump' , '--verbose' , '--blobs' , '--format=custom' , '--file=%s' % filename , name )
7641	def parse_arguments ( args ) : parser = argparse . ArgumentParser ( description = 'Convert JAMS to .lab files' ) parser . add_argument ( '-c' , '--comma-separated' , dest = 'csv' , action = 'store_true' , default = False , help = 'Output in .csv instead of .lab' ) parser . add_argument ( '--comment' , dest = 'comment_char' , type = str , default = '#' , help = 'Comment character' ) parser . add_argument ( '-n' , '--namespace' , dest = 'namespaces' , nargs = '+' , default = [ '.*' ] , help = 'One or more namespaces to output. Default is all.' ) parser . add_argument ( 'jams_file' , help = 'Path to the input jams file' ) parser . add_argument ( 'output_prefix' , help = 'Prefix for output files' ) return vars ( parser . parse_args ( args ) )
4493	def list_ ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) for store in project . storages : prefix = store . name for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] print ( os . path . join ( prefix , path ) )
1840	def JNG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC )
2361	def t_tabbedheredoc ( self , t ) : r'<<-\S+\r?\n' t . lexer . is_tabbed = True self . _init_heredoc ( t ) t . lexer . begin ( 'tabbedheredoc' )
9691	def stop ( self ) : if self . receiver != None : self . receiver . join ( ) for s in self . senders . values ( ) : s . join ( )
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
2072	def get_obj_cols ( df ) : obj_cols = [ ] for idx , dt in enumerate ( df . dtypes ) : if dt == 'object' or is_category ( dt ) : obj_cols . append ( df . columns . values [ idx ] ) return obj_cols
3959	def ensure_local_repo ( self ) : if os . path . exists ( self . managed_path ) : logging . debug ( 'Repo {} already exists' . format ( self . remote_path ) ) return logging . info ( 'Initiating clone of local repo {}' . format ( self . remote_path ) ) repo_path_parent = parent_dir ( self . managed_path ) if not os . path . exists ( repo_path_parent ) : os . makedirs ( repo_path_parent ) with git_error_handling ( ) : git . Repo . clone_from ( self . assemble_remote_path ( ) , self . managed_path )
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
4353	def socketio_manage ( environ , namespaces , request = None , error_handler = None , json_loads = None , json_dumps = None ) : socket = environ [ 'socketio' ] socket . _set_environ ( environ ) socket . _set_namespaces ( namespaces ) if request : socket . _set_request ( request ) if error_handler : socket . _set_error_handler ( error_handler ) if json_loads : socket . _set_json_loads ( json_loads ) if json_dumps : socket . _set_json_dumps ( json_dumps ) receiver_loop = socket . _spawn_receiver_loop ( ) gevent . joinall ( [ receiver_loop ] ) return
13163	def serialize_text ( out , text ) : padding = len ( out ) add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
2412	def gen_feats ( self , p_set ) : if self . _initialized != True : error_message = "Dictionaries have not been initialized." log . exception ( error_message ) raise util_functions . InputError ( p_set , error_message ) textual_features = [ ] for i in xrange ( 0 , len ( p_set . _essay_sets ) ) : textual_features . append ( self . _extractors [ i ] . gen_feats ( p_set . _essay_sets [ i ] ) ) textual_matrix = numpy . concatenate ( textual_features , axis = 1 ) predictor_matrix = numpy . array ( p_set . _numeric_features ) print textual_matrix . shape print predictor_matrix . shape overall_matrix = numpy . concatenate ( ( textual_matrix , predictor_matrix ) , axis = 1 ) return overall_matrix . copy ( )
3523	def intercom_user_hash ( data ) : if getattr ( settings , 'INTERCOM_HMAC_SECRET_KEY' , None ) : return hmac . new ( key = _hashable_bytes ( settings . INTERCOM_HMAC_SECRET_KEY ) , msg = _hashable_bytes ( data ) , digestmod = hashlib . sha256 , ) . hexdigest ( ) else : return None
13348	def prompt ( prefix = None , colored = True ) : if platform == 'win' : return '[{0}] $P$G' . format ( prefix ) else : if colored : return ( '[{0}] ' '\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\] ' '\\[\\033[01;34m\\]\\w $ \\[\\033[00m\\]' ) . format ( prefix ) return '[{0}] \\u@\\h \\w $ ' . format ( prefix )
7960	def handle_err ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( ) raise PyXMPPIOError ( "Unhandled error on socket" )
3554	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . stopScan ( ) self . _is_scanning = False
305	def plot_monthly_returns_timeseries ( returns , ax = None , ** kwargs ) : def cumulate_returns ( x ) : return ep . cum_returns ( x ) [ - 1 ] if ax is None : ax = plt . gca ( ) monthly_rets = returns . resample ( 'M' ) . apply ( lambda x : cumulate_returns ( x ) ) monthly_rets = monthly_rets . to_period ( ) sns . barplot ( x = monthly_rets . index , y = monthly_rets . values , color = 'steelblue' ) locs , labels = plt . xticks ( ) plt . setp ( labels , rotation = 90 ) xticks_coord = [ ] xticks_label = [ ] count = 0 for i in monthly_rets . index : if i . month == 1 : xticks_label . append ( i ) xticks_coord . append ( count ) ax . axvline ( count , color = 'gray' , ls = '--' , alpha = 0.3 ) count += 1 ax . axhline ( 0.0 , color = 'darkgray' , ls = '-' ) ax . set_xticks ( xticks_coord ) ax . set_xticklabels ( xticks_label ) return ax
9631	def send ( self , extra_context = None , ** kwargs ) : message = self . render_to_message ( extra_context = extra_context , ** kwargs ) return message . send ( )
4862	def save ( self ) : course_id = self . validated_data [ 'course_id' ] __ , created = models . EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = self . enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'rest-api-enrollment' , self . enterprise_customer_user . user_id , course_id )
10212	def count_subgraph_sizes ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Counter [ int ] : return count_dict_values ( group_nodes_by_annotation ( graph , annotation ) )
9519	def interleave ( infile_1 , infile_2 , outfile , suffix1 = None , suffix2 = None ) : seq_reader_1 = sequences . file_reader ( infile_1 ) seq_reader_2 = sequences . file_reader ( infile_2 ) f_out = utils . open_file_write ( outfile ) for seq_1 in seq_reader_1 : try : seq_2 = next ( seq_reader_2 ) except : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_1 . id , ' ... cannot continue' ) if suffix1 is not None and not seq_1 . id . endswith ( suffix1 ) : seq_1 . id += suffix1 if suffix2 is not None and not seq_2 . id . endswith ( suffix2 ) : seq_2 . id += suffix2 print ( seq_1 , file = f_out ) print ( seq_2 , file = f_out ) try : seq_2 = next ( seq_reader_2 ) except : seq_2 = None if seq_2 is not None : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_2 . id , ' ... cannot continue' ) utils . close ( f_out )
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
6571	def register_chooser ( self , chooser , ** kwargs ) : if not issubclass ( chooser , Chooser ) : return self . register_simple_chooser ( chooser , ** kwargs ) self . choosers [ chooser . model ] = chooser ( ** kwargs ) return chooser
2522	def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
6763	def dumpload ( self , site = None , role = None ) : r = self . database_renderer ( site = site , role = role ) r . run ( 'pg_dump -c --host={host_string} --username={db_user} ' '--blobs --format=c {db_name} -n public | ' 'pg_restore -U {db_postgresql_postgres_user} --create ' '--dbname={db_name}' )
3870	async def get_events ( self , event_id = None , max_events = 50 ) : if event_id is None : conv_events = self . _events [ - 1 * max_events : ] else : conv_event = self . get_event ( event_id ) if self . _events [ 0 ] . id_ != event_id : conv_events = self . _events [ self . _events . index ( conv_event ) + 1 : ] else : logger . info ( 'Loading events for conversation {} before {}' . format ( self . id_ , conv_event . timestamp ) ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) ) , include_event = True , max_events_per_conversation = max_events , event_continuation_token = self . _event_cont_token ) ) if res . conversation_state . HasField ( 'conversation' ) : self . update_conversation ( res . conversation_state . conversation ) self . _event_cont_token = ( res . conversation_state . event_continuation_token ) conv_events = [ self . _wrap_event ( event ) for event in res . conversation_state . event ] logger . info ( 'Loaded {} events for conversation {}' . format ( len ( conv_events ) , self . id_ ) ) for conv_event in reversed ( conv_events ) : if conv_event . id_ not in self . _events_dict : self . _events . insert ( 0 , conv_event ) self . _events_dict [ conv_event . id_ ] = conv_event else : logger . info ( 'Conversation %s ignoring duplicate event %s' , self . id_ , conv_event . id_ ) return conv_events
5207	def format_output ( data : pd . DataFrame , source , col_maps = None ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) if source == 'bdp' : req_cols = [ 'ticker' , 'field' , 'value' ] else : req_cols = [ 'ticker' , 'field' , 'name' , 'value' , 'position' ] if any ( col not in data for col in req_cols ) : return pd . DataFrame ( ) if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) if source == 'bdp' : res = pd . DataFrame ( pd . concat ( [ pd . Series ( { ** { 'ticker' : t } , ** grp . set_index ( 'field' ) . value . to_dict ( ) } ) for t , grp in data . groupby ( 'ticker' ) ] , axis = 1 , sort = False ) ) . transpose ( ) . set_index ( 'ticker' ) else : res = pd . DataFrame ( pd . concat ( [ grp . loc [ : , [ 'name' , 'value' ] ] . set_index ( 'name' ) . transpose ( ) . reset_index ( drop = True ) . assign ( ticker = t ) for ( t , _ ) , grp in data . groupby ( [ 'ticker' , 'position' ] ) ] , sort = False ) ) . reset_index ( drop = True ) . set_index ( 'ticker' ) res . columns . name = None if col_maps is None : col_maps = dict ( ) return res . rename ( columns = lambda vv : col_maps . get ( vv , vv . lower ( ) . replace ( ' ' , '_' ) . replace ( '-' , '_' ) ) ) . apply ( pd . to_numeric , errors = 'ignore' , downcast = 'float' )
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
2389	def f7 ( seq ) : seen = set ( ) seen_add = seen . add return [ x for x in seq if x not in seen and not seen_add ( x ) ]
11787	def add ( self , o ) : "Add an observation o to the distribution." self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
7873	def get_all_payload ( self , specialize = False ) : if self . _payload is None : self . decode_payload ( specialize ) elif specialize : for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ i ] = payload return list ( self . _payload )
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , ** info )
2670	def deploy ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , preserve_vpc = False ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) existing_config = get_function_config ( cfg ) if existing_config : update_function ( cfg , path_to_zip_file , existing_config , preserve_vpc = preserve_vpc ) else : create_function ( cfg , path_to_zip_file )
11291	def json ( request , * args , ** kwargs ) : params = dict ( request . GET . items ( ) ) callback = params . pop ( 'callback' , None ) url = params . pop ( 'url' , None ) if not url : return HttpResponseBadRequest ( 'Required parameter missing: URL' ) try : provider = oembed . site . provider_for_url ( url ) if not provider . provides : raise OEmbedMissingEndpoint ( ) except OEmbedMissingEndpoint : raise Http404 ( 'No provider found for %s' % url ) query = dict ( [ ( smart_str ( k ) , smart_str ( v ) ) for k , v in params . items ( ) if v ] ) try : resource = oembed . site . embed ( url , ** query ) except OEmbedException , e : raise Http404 ( 'Error embedding %s: %s' % ( url , str ( e ) ) ) response = HttpResponse ( mimetype = 'application/json' ) json = resource . json if callback : response . write ( '%s(%s)' % ( defaultfilters . force_escape ( callback ) , json ) ) else : response . write ( json ) return response
11816	def score ( self , code ) : text = permutation_decode ( self . ciphertext , code ) logP = ( sum ( [ log ( self . Pwords [ word ] ) for word in words ( text ) ] ) + sum ( [ log ( self . P1 [ c ] ) for c in text ] ) + sum ( [ log ( self . P2 [ b ] ) for b in bigrams ( text ) ] ) ) return exp ( logP )
1168	def b2a_qp ( data , quotetabs = False , istext = True , header = False ) : MAXLINESIZE = 76 lf = data . find ( '\n' ) crlf = lf > 0 and data [ lf - 1 ] == '\r' inp = 0 linelen = 0 odata = [ ] while inp < len ( data ) : c = data [ inp ] if ( c > '~' or c == '=' or ( header and c == '_' ) or ( c == '.' and linelen == 0 and ( inp + 1 == len ( data ) or data [ inp + 1 ] == '\n' or data [ inp + 1 ] == '\r' ) ) or ( not istext and ( c == '\r' or c == '\n' ) ) or ( ( c == '\t' or c == ' ' ) and ( inp + 1 == len ( data ) ) ) or ( c <= ' ' and c != '\r' and c != '\n' and ( quotetabs or ( not quotetabs and ( c != '\t' and c != ' ' ) ) ) ) ) : linelen += 3 if linelen >= MAXLINESIZE : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 3 odata . append ( '=' + two_hex_digits ( ord ( c ) ) ) inp += 1 else : if ( istext and ( c == '\n' or ( inp + 1 < len ( data ) and c == '\r' and data [ inp + 1 ] == '\n' ) ) ) : linelen = 0 if ( len ( odata ) > 0 and ( odata [ - 1 ] == ' ' or odata [ - 1 ] == '\t' ) ) : ch = ord ( odata [ - 1 ] ) odata [ - 1 ] = '=' odata . append ( two_hex_digits ( ch ) ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) if c == '\r' : inp += 2 else : inp += 1 else : if ( inp + 1 < len ( data ) and data [ inp + 1 ] != '\n' and ( linelen + 1 ) >= MAXLINESIZE ) : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 0 linelen += 1 if header and c == ' ' : c = '_' odata . append ( c ) inp += 1 return '' . join ( odata )
2976	def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
2222	def _rectify_base ( base ) : if base is NoParam or base == 'default' : return DEFAULT_ALPHABET elif base in [ 26 , 'abc' , 'alpha' ] : return _ALPHABET_26 elif base in [ 16 , 'hex' ] : return _ALPHABET_16 elif base in [ 10 , 'dec' ] : return _ALPHABET_10 else : if not isinstance ( base , ( list , tuple ) ) : raise TypeError ( 'Argument `base` must be a key, list, or tuple; not {}' . format ( type ( base ) ) ) return base
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
11582	def retrieve_url ( self , url ) : try : r = requests . get ( url ) except requests . ConnectionError : raise exceptions . RetrieveError ( 'Connection fail' ) if r . status_code >= 400 : raise exceptions . RetrieveError ( 'Connected, but status code is %s' % ( r . status_code ) ) real_url = r . url content = r . content try : content_type = r . headers [ 'Content-Type' ] except KeyError : content_type , encoding = mimetypes . guess_type ( real_url , strict = False ) self . response = r return content_type . lower ( ) , content
2534	def validate_str_fields ( self , fields , optional , messages ) : for field_str in fields : field = getattr ( self , field_str ) if field is not None : attr = getattr ( field , '__str__' , None ) if not callable ( attr ) : messages = messages + [ '{0} must provide __str__ method.' . format ( field ) ] elif not optional : messages = messages + [ 'Package {0} can not be None.' . format ( field_str ) ] return messages
13230	def get_macros ( tex_source ) : r macros = { } macros . update ( get_def_macros ( tex_source ) ) macros . update ( get_newcommand_macros ( tex_source ) ) return macros
2202	def ensure_app_cache_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_cache_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
9821	def create ( ctx , name , description , tags , private , init ) : try : tags = tags . split ( ',' ) if tags else None project_dict = dict ( name = name , description = description , is_public = not private , tags = tags ) project_config = ProjectConfig . from_dict ( project_dict ) except ValidationError : Printer . print_error ( 'Project name should contain only alpha numerical, "-", and "_".' ) sys . exit ( 1 ) try : _project = PolyaxonClient ( ) . project . create_project ( project_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not create project `{}`.' . format ( name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project `{}` was created successfully." . format ( _project . name ) ) if init : ctx . obj = { } ctx . invoke ( init_project , project = name )
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
3231	def get_cache_access_details ( key = None ) : from cloudaux . gcp . decorators import _GCP_CACHE return _GCP_CACHE . get_access_details ( key = key )
10002	def add_path ( self , nodes , ** attr ) : if nx . __version__ [ 0 ] == "1" : return super ( ) . add_path ( nodes , ** attr ) else : return nx . add_path ( self , nodes , ** attr )
8278	def objs ( self ) : for obj in self . objects . itervalues ( ) : if obj . sessionid in self . sessions : yield obj
4079	def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
10976	def delete ( group_id ) : group = Group . query . get_or_404 ( group_id ) if group . can_edit ( current_user ) : try : group . delete ( ) except Exception as e : flash ( str ( e ) , "error" ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'Successfully removed group "%(group_name)s"' , group_name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) flash ( _ ( 'You cannot delete the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( ".index" ) )
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : depth } return modeldict
6279	def clear ( self ) : self . ctx . fbo . clear ( red = self . clear_color [ 0 ] , green = self . clear_color [ 1 ] , blue = self . clear_color [ 2 ] , alpha = self . clear_color [ 3 ] , depth = self . clear_depth , )
9997	def del_cells ( self , name ) : if name in self . cells : cells = self . cells [ name ] self . cells . del_item ( name ) self . inherit ( ) self . model . spacegraph . update_subspaces ( self ) elif name in self . dynamic_spaces : cells = self . dynamic_spaces . pop ( name ) self . dynamic_spaces . set_update ( ) else : raise KeyError ( "Cells '%s' does not exist" % name ) NullImpl ( cells )
1227	def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : raise NotImplementedError
13307	def gmb ( a , b ) : return np . exp ( np . log ( a ) . mean ( ) - np . log ( b ) . mean ( ) )
13343	def _broadcast_shape ( * args ) : shapes = [ a . shape if hasattr ( type ( a ) , '__array_interface__' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )
7662	def slice ( self , start_time , end_time , strict = False ) : sliced_ann = self . trim ( start_time , end_time , strict = strict ) raw_data = sliced_ann . pop_data ( ) for obs in raw_data : new_time = max ( 0 , obs . time - start_time ) sliced_ann . append ( time = new_time , duration = obs . duration , value = obs . value , confidence = obs . confidence ) ref_time = sliced_ann . time slice_start = ref_time slice_end = ref_time + sliced_ann . duration if 'slice' not in sliced_ann . sandbox . keys ( ) : sliced_ann . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ] ) else : sliced_ann . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ) sliced_ann . time = max ( 0 , ref_time - start_time ) return sliced_ann
889	def _leastUsedCell ( cls , random , cells , connections ) : leastUsedCells = [ ] minNumSegments = float ( "inf" ) for cell in cells : numSegments = connections . numSegments ( cell ) if numSegments < minNumSegments : minNumSegments = numSegments leastUsedCells = [ ] if numSegments == minNumSegments : leastUsedCells . append ( cell ) i = random . getUInt32 ( len ( leastUsedCells ) ) return leastUsedCells [ i ]
8369	def create_canvas ( src , format = None , outputfile = None , multifile = False , buff = None , window = False , title = None , fullscreen = None , show_vars = False ) : from core import CairoCanvas , CairoImageSink if outputfile : sink = CairoImageSink ( outputfile , format , multifile , buff ) elif window or show_vars : from gui import ShoebotWindow if not title : if src and os . path . isfile ( src ) : title = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + ' - Shoebot' else : title = 'Untitled - Shoebot' sink = ShoebotWindow ( title , show_vars , fullscreen = fullscreen ) else : if src and isinstance ( src , cairo . Surface ) : outputfile = src format = 'surface' elif src and os . path . isfile ( src ) : outputfile = os . path . splitext ( os . path . basename ( src ) ) [ 0 ] + '.' + ( format or 'svg' ) else : outputfile = 'output.svg' sink = CairoImageSink ( outputfile , format , multifile , buff ) canvas = CairoCanvas ( sink ) return canvas
944	def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , "savedmodels" ) baseDir = os . path . abspath ( baseDir ) return baseDir
11104	def acquire_lock ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : with self . locker as r : acquired , code , _ = r if acquired : try : r = func ( self , * args , ** kwargs ) except Exception as err : e = str ( err ) else : e = None else : warnings . warn ( "code %s. Unable to aquire the lock when calling '%s'. You may try again!" % ( code , func . __name__ ) ) e = None r = None if e is not None : traceback . print_stack ( ) raise Exception ( e ) return r return wrapper
2901	def complete_task_from_id ( self , task_id ) : if task_id is None : raise WorkflowException ( self . spec , 'task_id is None' ) for task in self . task_tree : if task . id == task_id : return task . complete ( ) msg = 'A task with the given task_id (%s) was not found' % task_id raise WorkflowException ( self . spec , msg )
10642	def Ra ( L : float , Ts : float , Tf : float , alpha : float , beta : float , nu : float ) -> float : return g * beta * ( Ts - Tinf ) * L ** 3.0 / ( nu * alpha )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
4283	def generate_video ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) converter = settings [ 'video_converter' ] w_src , h_src = video_size ( source , converter = converter ) w_dst , h_dst = settings [ 'video_size' ] logger . debug ( 'Video size: %i, %i -> %i, %i' , w_src , h_src , w_dst , h_dst ) base , src_ext = splitext ( source ) base , dst_ext = splitext ( outname ) if dst_ext == src_ext and w_src <= w_dst and h_src <= h_dst : logger . debug ( 'Video is smaller than the max size, copying it instead' ) shutil . copy ( source , outname ) return if h_dst * w_src < h_src * w_dst : resize_opt = [ '-vf' , "scale=trunc(oh*a/2)*2:%i" % h_dst ] else : resize_opt = [ '-vf' , "scale=%i:trunc(ow/a/2)*2" % w_dst ] if w_src <= w_dst and h_src <= h_dst : resize_opt = [ ] cmd = [ converter , '-i' , source , '-y' ] if options is not None : cmd += options cmd += resize_opt + [ outname ] logger . debug ( 'Processing video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname )
6540	def read_file ( filepath ) : with _FILE_CACHE_LOCK : if filepath not in _FILE_CACHE : _FILE_CACHE [ filepath ] = _read_file ( filepath ) return _FILE_CACHE [ filepath ]
13750	def one_to_many ( clsname , ** kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , ** kw ) return o2m
9202	def count_cycles ( series , ndigits = None , left = False , right = False ) : counts = defaultdict ( float ) round_ = _get_round_function ( ndigits ) for low , high , mult in extract_cycles ( series , left = left , right = right ) : delta = round_ ( abs ( high - low ) ) counts [ delta ] += mult return sorted ( counts . items ( ) )
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
5855	def update_dataset ( self , dataset_id , name = None , description = None , public = None ) : data = { "public" : _convert_bool_to_public_value ( public ) } if name : data [ "name" ] = name if description : data [ "description" ] = description dataset = { "dataset" : data } failure_message = "Failed to update dataset {}" . format ( dataset_id ) response = self . _get_success_json ( self . _post_json ( routes . update_dataset ( dataset_id ) , data = dataset , failure_message = failure_message ) ) return _dataset_from_response_dict ( response )
1027	def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits
6564	def and_gate ( variables , vartype = dimod . BINARY , name = 'AND' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configurations = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 0 ) , ( 1 , 0 , 0 ) , ( 1 , 1 , 1 ) ] ) def func ( in1 , in2 , out ) : return ( in1 and in2 ) == out else : configurations = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , - 1 ) , ( + 1 , - 1 , - 1 ) , ( + 1 , + 1 , + 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) and ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configurations , variables , vartype = vartype , name = name )
11310	def get_object ( self , url , month_format = '%b' , day_format = '%d' ) : params = self . get_params ( url ) try : year = params [ self . _meta . year_part ] month = params [ self . _meta . month_part ] day = params [ self . _meta . day_part ] except KeyError : try : year , month , day = params [ '_0' ] , params [ '_1' ] , params [ '_2' ] except KeyError : raise OEmbedException ( 'Error extracting date from url parameters' ) try : tt = time . strptime ( '%s-%s-%s' % ( year , month , day ) , '%s-%s-%s' % ( '%Y' , month_format , day_format ) ) date = datetime . date ( * tt [ : 3 ] ) except ValueError : raise OEmbedException ( 'Error parsing date from: %s' % url ) if isinstance ( self . _meta . model . _meta . get_field ( self . _meta . date_field ) , DateTimeField ) : min_date = datetime . datetime . combine ( date , datetime . time . min ) max_date = datetime . datetime . combine ( date , datetime . time . max ) query = { '%s__range' % self . _meta . date_field : ( min_date , max_date ) } else : query = { self . _meta . date_field : date } for key , value in self . _meta . fields_to_match . iteritems ( ) : try : query [ value ] = params [ key ] except KeyError : raise OEmbedException ( '%s was not found in the urlpattern parameters. Valid names are: %s' % ( key , ', ' . join ( params . keys ( ) ) ) ) try : obj = self . get_queryset ( ) . get ( ** query ) except self . _meta . model . DoesNotExist : raise OEmbedException ( 'Requested object not found' ) return obj
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
4896	def get_enterprise_sso_uid ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . get_remote_id ( )
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
8705	def write_file ( self , path , destination = '' , verify = 'none' ) : filename = os . path . basename ( path ) if not destination : destination = filename log . info ( 'Transferring %s as %s' , path , destination ) self . __writeln ( "recv()" ) res = self . __expect ( 'C> ' ) if not res . endswith ( 'C> ' ) : log . error ( 'Error waiting for esp "%s"' , res ) raise CommunicationTimeout ( 'Error waiting for device to start receiving' , res ) log . debug ( 'sending destination filename "%s"' , destination ) self . __write ( destination + '\x00' , True ) if not self . __got_ack ( ) : log . error ( 'did not ack destination filename' ) raise NoAckException ( 'Device did not ACK destination filename' ) content = from_file ( path ) log . debug ( 'sending %d bytes in %s' , len ( content ) , filename ) pos = 0 chunk_size = 128 while pos < len ( content ) : rest = len ( content ) - pos if rest > chunk_size : rest = chunk_size data = content [ pos : pos + rest ] if not self . __write_chunk ( data ) : resp = self . __expect ( ) log . error ( 'Bad chunk response "%s" %s' , resp , hexify ( resp ) ) raise BadResponseException ( 'Bad chunk response' , ACK , resp ) pos += chunk_size log . debug ( 'sending zero block' ) self . __write_chunk ( '' ) if verify != 'none' : self . verify_file ( path , destination , verify )
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
4512	def resize ( image , x , y , stretch = False , top = None , left = None , mode = 'RGB' , resample = None ) : if x <= 0 : raise ValueError ( 'x must be greater than zero' ) if y <= 0 : raise ValueError ( 'y must be greater than zero' ) from PIL import Image resample = Image . ANTIALIAS if resample is None else resample if not isinstance ( resample , numbers . Number ) : try : resample = getattr ( Image , resample . upper ( ) ) except : raise ValueError ( "(1) Didn't understand resample=%s" % resample ) if not isinstance ( resample , numbers . Number ) : raise ValueError ( "(2) Didn't understand resample=%s" % resample ) size = x , y if stretch : return image . resize ( size , resample = resample ) result = Image . new ( mode , size ) ratios = [ d1 / d2 for d1 , d2 in zip ( size , image . size ) ] if ratios [ 0 ] < ratios [ 1 ] : new_size = ( size [ 0 ] , int ( image . size [ 1 ] * ratios [ 0 ] ) ) else : new_size = ( int ( image . size [ 0 ] * ratios [ 1 ] ) , size [ 1 ] ) image = image . resize ( new_size , resample = resample ) if left is None : box_x = int ( ( x - new_size [ 0 ] ) / 2 ) elif left : box_x = 0 else : box_x = x - new_size [ 0 ] if top is None : box_y = int ( ( y - new_size [ 1 ] ) / 2 ) elif top : box_y = 0 else : box_y = y - new_size [ 1 ] result . paste ( image , box = ( box_x , box_y ) ) return result
8768	def add_job_to_context ( context , job_id ) : db_job = db_api . async_transaction_find ( context , id = job_id , scope = db_api . ONE ) if not db_job : return context . async_job = { "job" : v . _make_job_dict ( db_job ) }
8886	def finalize ( self ) : if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. finalize unusable' ) elif not self . __head_generate : warn ( f'{self.__class__.__name__} already finalized or fitted' ) elif not self . __head_dict : raise NotFittedError ( f'{self.__class__.__name__} instance is not fitted yet' ) else : if self . remove_rare_ratio : self . __clean_head ( * self . __head_rare ) self . __prepare_header ( ) self . __head_rare = None self . __head_generate = False
2568	def construct_start_message ( self ) : uname = getpass . getuser ( ) . encode ( 'latin1' ) hashed_username = hashlib . sha256 ( uname ) . hexdigest ( ) [ 0 : 10 ] hname = socket . gethostname ( ) . encode ( 'latin1' ) hashed_hostname = hashlib . sha256 ( hname ) . hexdigest ( ) [ 0 : 10 ] message = { 'uuid' : self . uuid , 'uname' : hashed_username , 'hname' : hashed_hostname , 'test' : self . test_mode , 'parsl_v' : self . parsl_version , 'python_v' : self . python_version , 'os' : platform . system ( ) , 'os_v' : platform . release ( ) , 'start' : time . time ( ) } return json . dumps ( message )
4432	def unregister_hook ( self , func ) : if func in self . hooks : self . hooks . remove ( func )
8560	def get_lan_members ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s/nics?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
4072	def split_elements ( value ) : items = [ v . strip ( ) for v in value . split ( ',' ) ] if len ( items ) == 1 : items = value . split ( ) return items
7232	def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( url + ID ) r . raise_for_status ( ) return r . json ( )
13160	def delete ( cls , cur , table : str , where_keys : list ) : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _delete_query . format ( table , where_clause ) yield from cur . execute ( query , values ) return cur . rowcount
12669	def niftilist_mask_to_array ( img_filelist , mask_file = None , outdtype = None ) : img = check_img ( img_filelist [ 0 ] ) if not outdtype : outdtype = img . dtype mask_data , _ = load_mask_data ( mask_file ) indices = np . where ( mask_data ) mask = check_img ( mask_file ) outmat = np . zeros ( ( len ( img_filelist ) , np . count_nonzero ( mask_data ) ) , dtype = outdtype ) for i , img_item in enumerate ( img_filelist ) : img = check_img ( img_item ) if not are_compatible_imgs ( img , mask ) : raise NiftiFilesNotCompatible ( repr_imgs ( img ) , repr_imgs ( mask_file ) ) vol = get_img_data ( img ) outmat [ i , : ] = vol [ indices ] return outmat , mask_data
1328	def channel_axis ( self , batch ) : axis = self . __model . channel_axis ( ) if not batch : axis = axis - 1 return axis
4307	def _validate_file_formats ( input_filepath_list , combine_type ) : _validate_sample_rates ( input_filepath_list , combine_type ) if combine_type == 'concatenate' : _validate_num_channels ( input_filepath_list , combine_type )
11774	def NeuralNetLearner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict
8853	def on_open ( self ) : filename , filter = QtWidgets . QFileDialog . getOpenFileName ( self , 'Open' ) if filename : self . open_file ( filename ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True )
3763	def Pt ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in Staveley_data . index and not np . isnan ( Staveley_data . at [ CASRN , 'Pt' ] ) : methods . append ( STAVELEY ) if Tt ( CASRN ) and VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == STAVELEY : Pt = Staveley_data . at [ CASRN , 'Pt' ] elif Method == DEFINITION : Pt = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( T = Tt ( CASRN ) ) elif Method == NONE : Pt = None else : raise Exception ( 'Failure in in function' ) return Pt
9957	def setup_ipython ( self ) : if self . is_ipysetup : return from ipykernel . kernelapp import IPKernelApp self . shell = IPKernelApp . instance ( ) . shell if not self . shell and is_ipython ( ) : self . shell = get_ipython ( ) if self . shell : shell_class = type ( self . shell ) shell_class . default_showtraceback = shell_class . showtraceback shell_class . showtraceback = custom_showtraceback self . is_ipysetup = True else : raise RuntimeError ( "IPython shell not found." )
7978	def _post_auth ( self ) : ClientStream . _post_auth ( self ) if not self . initiator : self . unset_iq_get_handler ( "query" , "jabber:iq:auth" ) self . unset_iq_set_handler ( "query" , "jabber:iq:auth" )
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
12235	def generate_versionwarning_data_json ( app , config = None , ** kwargs ) : config = config or kwargs . pop ( 'config' , None ) if config is None : config = app . config if config . versionwarning_project_version in config . versionwarning_messages : custom = True message = config . versionwarning_messages . get ( config . versionwarning_project_version ) else : custom = False message = config . versionwarning_default_message banner_html = config . versionwarning_banner_html . format ( id_div = config . versionwarning_banner_id_div , banner_title = config . versionwarning_banner_title , message = message . format ( ** { config . versionwarning_message_placeholder : '<a href="#"></a>' } , ) , admonition_type = config . versionwarning_admonition_type , ) data = json . dumps ( { 'meta' : { 'api_url' : config . versionwarning_api_url , } , 'banner' : { 'html' : banner_html , 'id_div' : config . versionwarning_banner_id_div , 'body_selector' : config . versionwarning_body_selector , 'custom' : custom , } , 'project' : { 'slug' : config . versionwarning_project_slug , } , 'version' : { 'slug' : config . versionwarning_project_version , } , } , indent = 4 ) data_path = os . path . join ( STATIC_PATH , 'data' ) if not os . path . exists ( data_path ) : os . mkdir ( data_path ) with open ( os . path . join ( data_path , JSON_DATA_FILENAME ) , 'w' ) as f : f . write ( data ) config . html_static_path . append ( STATIC_PATH )
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : pass
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
4598	def read_from ( self , data , pad = 0 ) : for i in range ( self . BEGIN , self . END + 1 ) : index = self . index ( i , len ( data ) ) yield pad if index is None else data [ index ]
2010	def _get_memfee ( self , address , size = 1 ) : if not issymbolic ( size ) and size == 0 : return 0 address = self . safe_add ( address , size ) allocated = self . allocated GMEMORY = 3 GQUADRATICMEMDENOM = 512 old_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( allocated , 31 ) , 32 ) , 512 ) new_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( address , 31 ) , 32 ) , 512 ) old_totalfee = self . safe_mul ( old_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( old_size , old_size ) , GQUADRATICMEMDENOM ) new_totalfee = self . safe_mul ( new_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( new_size , new_size ) , GQUADRATICMEMDENOM ) memfee = new_totalfee - old_totalfee flag = Operators . UGT ( new_totalfee , old_totalfee ) return Operators . ITEBV ( 512 , size == 0 , 0 , Operators . ITEBV ( 512 , flag , memfee , 0 ) )
10287	def enrich_complexes ( graph : BELGraph ) -> None : nodes = list ( get_nodes_by_function ( graph , COMPLEX ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
6462	def filter_symlog ( y , base = 10.0 ) : log_base = np . log ( base ) sign = np . sign ( y ) logs = np . log ( np . abs ( y ) / log_base ) return sign * logs
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
3961	def update_managed_repos ( force = False ) : log_to_client ( 'Pulling latest updates for all active managed repos:' ) update_specs_repo_and_known_hosts ( ) repos_to_update = get_all_repos ( active_only = True , include_specs_repo = False ) with parallel_task_queue ( ) as queue : log_to_client ( 'Updating managed repos' ) for repo in repos_to_update : if not repo . is_overridden : repo . update_local_repo_async ( queue , force = force )
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
8438	def _patched_run_hook ( hook_name , project_dir , context ) : if hook_name == 'post_gen_project' : with temple . utils . cd ( project_dir ) : temple . utils . write_temple_config ( context [ 'cookiecutter' ] , context [ 'template' ] , context [ 'version' ] ) return cc_hooks . run_hook ( hook_name , project_dir , context )
7548	def cluster_info ( ipyclient , spacer = "" ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
1152	def warn ( message , category = None , stacklevel = 1 ) : if isinstance ( message , Warning ) : category = message . __class__ if category is None : category = UserWarning assert issubclass ( category , Warning ) try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
11553	def disable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_DISABLE ] self . _command_handler . send_command ( command )
7360	def _check_hla_alleles ( alleles , valid_alleles = None ) : require_iterable_of ( alleles , string_types , "HLA alleles" ) alleles = { normalize_allele_name ( allele . strip ( ) . upper ( ) ) for allele in alleles } if valid_alleles : missing_alleles = [ allele for allele in alleles if allele not in valid_alleles ] if len ( missing_alleles ) > 0 : raise UnsupportedAllele ( "Unsupported HLA alleles: %s" % missing_alleles ) return list ( alleles )
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
1417	def _get_execution_state_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_execution_state_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_execution_state ( data , stats ) : if data : executionState = ExecutionState ( ) executionState . ParseFromString ( data ) callback ( executionState ) else : callback ( None ) return isWatching
5999	def mapping_matrix ( self ) : return mapper_util . mapping_matrix_from_sub_to_pix ( sub_to_pix = self . sub_to_pix , pixels = self . pixels , regular_pixels = self . grid_stack . regular . shape [ 0 ] , sub_to_regular = self . grid_stack . sub . sub_to_regular , sub_grid_fraction = self . grid_stack . sub . sub_grid_fraction )
10087	def update ( self , * args , ** kwargs ) : super ( Deposit , self ) . update ( * args , ** kwargs )
2653	def push_file ( self , local_source , remote_dir ) : remote_dest = remote_dir + '/' + os . path . basename ( local_source ) try : self . makedirs ( remote_dir , exist_ok = True ) except IOError as e : logger . exception ( "Pushing {0} to {1} failed" . format ( local_source , remote_dir ) ) if e . errno == 2 : raise BadScriptPath ( e , self . hostname ) elif e . errno == 13 : raise BadPermsScriptPath ( e , self . hostname ) else : logger . exception ( "File push failed due to SFTP client failure" ) raise FileCopyException ( e , self . hostname ) try : self . sftp_client . put ( local_source , remote_dest , confirm = True ) self . sftp_client . chmod ( remote_dest , 0o777 ) except Exception as e : logger . exception ( "File push from local source {} to remote destination {} failed" . format ( local_source , remote_dest ) ) raise FileCopyException ( e , self . hostname ) return remote_dest
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
13143	def _transform_triple_numpy ( x ) : return np . array ( [ x . head , x . relation , x . tail ] , dtype = np . int64 )
2516	def p_file_notice ( self , f_term , predicate ) : try : for _ , _ , notice in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_notice ( self . doc , six . text_type ( notice ) ) except CardinalityError : self . more_than_one_error ( 'file notice' )
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
950	def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = "uint32" ) bitsToSwap = int ( noiseLevel * numActiveCols ) for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2
13624	def Integer ( value , base = 10 , encoding = None ) : try : return int ( Text ( value , encoding ) , base ) except ( TypeError , ValueError ) : return None
12493	def check_array ( array , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False ) : if isinstance ( accept_sparse , str ) : accept_sparse = [ accept_sparse ] if sp . issparse ( array ) : array = _ensure_sparse_format ( array , accept_sparse , dtype , order , copy , force_all_finite ) else : if ensure_2d : array = np . atleast_2d ( array ) array = np . array ( array , dtype = dtype , order = order , copy = copy ) if not allow_nd and array . ndim >= 3 : raise ValueError ( "Found array with dim %d. Expected <= 2" % array . ndim ) if force_all_finite : _assert_all_finite ( array ) return array
1502	def template_scheduler_yaml ( cl_args , masters ) : single_master = masters [ 0 ] scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] template_file ( scheduler_config_template , scheduler_config_actual , { "<scheduler_uri>" : "http://%s:4646" % single_master } )
11421	def field_xml_output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( ' <controlfield tag="%s">%s</controlfield>' % ( tag , MathMLParser . html_to_text ( field [ 3 ] ) ) ) else : marcxml . append ( ' <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ _subfield_xml_output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( ' </datafield>' ) return '\n' . join ( marcxml )
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
10322	def spanning_1d_chain ( length ) : ret = nx . grid_graph ( dim = [ int ( length + 2 ) ] ) ret . node [ 0 ] [ 'span' ] = 0 ret [ 0 ] [ 1 ] [ 'span' ] = 0 ret . node [ length + 1 ] [ 'span' ] = 1 ret [ length ] [ length + 1 ] [ 'span' ] = 1 return ret
12052	def getNotesForABF ( abfFile ) : parent = getParent ( abfFile ) parent = os . path . basename ( parent ) . replace ( ".abf" , "" ) expFile = os . path . dirname ( abfFile ) + "/experiment.txt" if not os . path . exists ( expFile ) : return "no experiment file" with open ( expFile ) as f : raw = f . readlines ( ) for line in raw : if line [ 0 ] == '~' : line = line [ 1 : ] . strip ( ) if line . startswith ( parent ) : while "\t\t" in line : line = line . replace ( "\t\t" , "\t" ) line = line . replace ( "\t" , "\n" ) return line return "experiment.txt found, but didn't contain %s" % parent
13183	def dict_to_row ( cls , observation_data ) : row = [ ] row . append ( observation_data [ 'name' ] ) row . append ( observation_data [ 'date' ] ) row . append ( observation_data [ 'magnitude' ] ) comment_code = observation_data . get ( 'comment_code' , 'na' ) if not comment_code : comment_code = 'na' row . append ( comment_code ) comp1 = observation_data . get ( 'comp1' , 'na' ) if not comp1 : comp1 = 'na' row . append ( comp1 ) comp2 = observation_data . get ( 'comp2' , 'na' ) if not comp2 : comp2 = 'na' row . append ( comp2 ) chart = observation_data . get ( 'chart' , 'na' ) if not chart : chart = 'na' row . append ( chart ) notes = observation_data . get ( 'notes' , 'na' ) if not notes : notes = 'na' row . append ( notes ) return row
9612	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } data . setdefault ( 'element_id' , self . element_id ) return self . _driver . _execute ( command , data , unpack )
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
6051	def map_2d_array_to_masked_1d_array_from_array_2d_and_mask ( mask , array_2d ) : total_image_pixels = mask_util . total_regular_pixels_from_mask ( mask ) array_1d = np . zeros ( shape = total_image_pixels ) index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : array_1d [ index ] = array_2d [ y , x ] index += 1 return array_1d
364	def threading_data ( data = None , fn = None , thread_count = None , ** kwargs ) : def apply_fn ( results , i , data , kwargs ) : results [ i ] = fn ( data , ** kwargs ) if thread_count is None : results = [ None ] * len ( data ) threads = [ ] for i , d in enumerate ( data ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , d , kwargs ) ) t . start ( ) threads . append ( t ) else : divs = np . linspace ( 0 , len ( data ) , thread_count + 1 ) divs = np . round ( divs ) . astype ( int ) results = [ None ] * thread_count threads = [ ] for i in range ( thread_count ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , data [ divs [ i ] : divs [ i + 1 ] ] , kwargs ) ) t . start ( ) threads . append ( t ) for t in threads : t . join ( ) if thread_count is None : try : return np . asarray ( results ) except Exception : return results else : return np . concatenate ( results )
11971	def _detect ( ip , _isnm ) : ip = str ( ip ) if len ( ip ) > 1 : if ip [ 0 : 2 ] == '0x' : if _CHECK_FUNCT [ IP_HEX ] [ _isnm ] ( ip ) : return IP_HEX elif ip [ 0 ] == '0' : if _CHECK_FUNCT [ IP_OCT ] [ _isnm ] ( ip ) : return IP_OCT if _CHECK_FUNCT [ IP_DOT ] [ _isnm ] ( ip ) : return IP_DOT elif _isnm and _CHECK_FUNCT [ NM_BITS ] [ _isnm ] ( ip ) : return NM_BITS elif _CHECK_FUNCT [ IP_DEC ] [ _isnm ] ( ip ) : return IP_DEC elif _isnm and _CHECK_FUNCT [ NM_WILDCARD ] [ _isnm ] ( ip ) : return NM_WILDCARD elif _CHECK_FUNCT [ IP_BIN ] [ _isnm ] ( ip ) : return IP_BIN return IP_UNKNOWN
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
6430	def sim ( self , src , tar ) : def _lcsstr_stl ( src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , src_longest , tar_longest = 0 , 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] src_longest = i tar_longest = j else : lengths [ i , j ] = 0 return src_longest - longest , tar_longest - longest , longest def _sstr_matches ( src , tar ) : src_start , tar_start , length = _lcsstr_stl ( src , tar ) if length == 0 : return 0 return ( _sstr_matches ( src [ : src_start ] , tar [ : tar_start ] ) + length + _sstr_matches ( src [ src_start + length : ] , tar [ tar_start + length : ] ) ) if src == tar : return 1.0 elif not src or not tar : return 0.0 return 2 * _sstr_matches ( src , tar ) / ( len ( src ) + len ( tar ) )
5936	def col ( self , c ) : m = self . COLOUR . search ( c ) if not m : self . logger . fatal ( "Cannot parse colour specification %r." , c ) raise ParseError ( "XPM reader: Cannot parse colour specification {0!r}." . format ( c ) ) value = m . group ( 'value' ) color = m . group ( 'symbol' ) self . logger . debug ( "%s: %s %s\n" , c . strip ( ) , color , value ) return color , value
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( "--" ) and len ( arg ) > 2 : break if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )
12603	def duplicated_rows ( df , col_name ) : _check_cols ( df , [ col_name ] ) dups = df [ pd . notnull ( df [ col_name ] ) & df . duplicated ( subset = [ col_name ] ) ] return dups
3737	def molecular_diameter ( Tc = None , Pc = None , Vc = None , Zc = None , omega = None , Vm = None , Vb = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and Pc and omega : methods . append ( TEEGOTOSTEWARD4 ) if Tc and Pc : methods . append ( SILVALIUMACEDO ) methods . append ( BSLC2 ) methods . append ( TEEGOTOSTEWARD3 ) if Vc and Zc : methods . append ( STIELTHODOSMD ) if Vc : methods . append ( FLYNN ) methods . append ( BSLC1 ) if Vb : methods . append ( BSLB ) if Vm : methods . append ( BSLM ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : sigma = sigma_Flynn ( Vc ) elif Method == BSLC1 : sigma = sigma_Bird_Stewart_Lightfoot_critical_1 ( Vc ) elif Method == BSLC2 : sigma = sigma_Bird_Stewart_Lightfoot_critical_2 ( Tc , Pc ) elif Method == TEEGOTOSTEWARD3 : sigma = sigma_Tee_Gotoh_Steward_1 ( Tc , Pc ) elif Method == SILVALIUMACEDO : sigma = sigma_Silva_Liu_Macedo ( Tc , Pc ) elif Method == BSLB : sigma = sigma_Bird_Stewart_Lightfoot_boiling ( Vb ) elif Method == BSLM : sigma = sigma_Bird_Stewart_Lightfoot_melting ( Vm ) elif Method == STIELTHODOSMD : sigma = sigma_Stiel_Thodos ( Vc , Zc ) elif Method == TEEGOTOSTEWARD4 : sigma = sigma_Tee_Gotoh_Steward_2 ( Tc , Pc , omega ) elif Method == MAGALHAES : sigma = float ( MagalhaesLJ_data . at [ CASRN , "sigma" ] ) elif Method == NONE : sigma = None else : raise Exception ( 'Failure in in function' ) return sigma
319	def calc_distribution_stats ( x ) : return pd . Series ( { 'mean' : np . mean ( x ) , 'median' : np . median ( x ) , 'std' : np . std ( x ) , '5%' : np . percentile ( x , 5 ) , '25%' : np . percentile ( x , 25 ) , '75%' : np . percentile ( x , 75 ) , '95%' : np . percentile ( x , 95 ) , 'IQR' : np . subtract . reduce ( np . percentile ( x , [ 75 , 25 ] ) ) , } )
1827	def CALL ( cpu , op0 ) : proc = op0 . read ( ) cpu . push ( cpu . PC , cpu . address_bit_size ) cpu . PC = proc
7052	def parallel_tfa_lcdir ( lcdir , templateinfo , lcfileglob = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if lcfileglob is None : lcfileglob = dfileglob lclist = sorted ( glob . glob ( os . path . join ( lcdir , lcfileglob ) ) ) return parallel_tfa_lclist ( lclist , templateinfo , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = None , interp = interp , sigclip = sigclip , mintemplatedist_arcmin = mintemplatedist_arcmin , nworkers = nworkers , maxworkertasks = maxworkertasks )
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , ** kwargs )
3512	def optimizely ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return OptimizelyNode ( )
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
13242	def period ( self ) : start_time = self . root . findtext ( 'daily_start_time' ) if start_time : return Period ( text_to_time ( start_time ) , text_to_time ( self . root . findtext ( 'daily_end_time' ) ) ) return Period ( datetime . time ( 0 , 0 ) , datetime . time ( 23 , 59 ) )
11364	def run_shell_command ( commands , ** kwargs ) : p = subprocess . Popen ( commands , stdout = subprocess . PIPE , stderr = subprocess . PIPE , ** kwargs ) output , error = p . communicate ( ) return p . returncode , output , error
2067	def reverse_dummies ( self , X , mapping ) : out_cols = X . columns . values . tolist ( ) mapped_columns = [ ] for switch in mapping : col = switch . get ( 'col' ) mod = switch . get ( 'mapping' ) insert_at = out_cols . index ( mod . columns [ 0 ] ) X . insert ( insert_at , col , 0 ) positive_indexes = mod . index [ mod . index > 0 ] for i in range ( positive_indexes . shape [ 0 ] ) : existing_col = mod . columns [ i ] val = positive_indexes [ i ] X . loc [ X [ existing_col ] == 1 , col ] = val mapped_columns . append ( existing_col ) X . drop ( mod . columns , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
8690	def put ( self , key ) : self . _consul_request ( 'PUT' , self . _key_url ( key [ 'name' ] ) , json = key ) return key [ 'name' ]
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
12386	def split_segments ( text , closing_paren = False ) : buf = StringIO ( ) segments = [ ] combinators = [ ] last_group = False iterator = iter ( text ) last_negation = False for character in iterator : if character in COMBINATORS : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) val = buf . getvalue ( ) reset_stringio ( buf ) if not last_group and not len ( val ) : raise ValueError ( 'Unexpected %s.' % character ) if len ( val ) : segments . append ( parse_segment ( val ) ) combinators . append ( COMBINATORS [ character ] ) elif character == constants . GROUP_BEGIN : if buf . tell ( ) : raise ValueError ( 'Unexpected %s' % character ) seg = split_segments ( iterator , True ) if last_negation : seg = UnarySegmentCombinator ( seg ) segments . append ( seg ) last_group = True continue elif character == constants . GROUP_END : val = buf . getvalue ( ) if not buf . tell ( ) or not closing_paren : raise ValueError ( 'Unexpected %s' % character ) segments . append ( parse_segment ( val ) ) return combine ( segments , combinators ) elif character == constants . OPERATOR_NEGATION and not buf . tell ( ) : last_negation = True continue else : if last_negation : buf . write ( constants . OPERATOR_NEGATION ) if last_group : raise ValueError ( 'Unexpected %s' % character ) buf . write ( character ) last_negation = False last_group = False else : if closing_paren : raise ValueError ( 'Expected %s.' % constants . GROUP_END ) if not last_group : segments . append ( parse_segment ( buf . getvalue ( ) ) ) return combine ( segments , combinators )
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : msg += '::' elif msg in [ '3' , '4' , '5' ] : if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] elif msg == '8' : msg += '::' return msg
7557	def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
4121	def twosided_2_centerdc ( data ) : N = len ( data ) newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
1148	def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : memo [ id ( memo ) ] = [ x ]
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
1169	def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )
4936	def strfdelta ( tdelta , fmt = '{D:02}d {H:02}h {M:02}m {S:02}s' , input_type = 'timedelta' ) : if input_type == 'timedelta' : remainder = int ( tdelta . total_seconds ( ) ) elif input_type in [ 's' , 'seconds' ] : remainder = int ( tdelta ) elif input_type in [ 'm' , 'minutes' ] : remainder = int ( tdelta ) * 60 elif input_type in [ 'h' , 'hours' ] : remainder = int ( tdelta ) * 3600 elif input_type in [ 'd' , 'days' ] : remainder = int ( tdelta ) * 86400 elif input_type in [ 'w' , 'weeks' ] : remainder = int ( tdelta ) * 604800 else : raise ValueError ( 'input_type is not valid. Valid input_type strings are: "timedelta", "s", "m", "h", "d", "w"' ) f = Formatter ( ) desired_fields = [ field_tuple [ 1 ] for field_tuple in f . parse ( fmt ) ] possible_fields = ( 'W' , 'D' , 'H' , 'M' , 'S' ) constants = { 'W' : 604800 , 'D' : 86400 , 'H' : 3600 , 'M' : 60 , 'S' : 1 } values = { } for field in possible_fields : if field in desired_fields and field in constants : values [ field ] , remainder = divmod ( remainder , constants [ field ] ) return f . format ( fmt , ** values )
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
13093	def write_targets ( self ) : if len ( self . ldap_strings ) == 0 and len ( self . ips ) == 0 : print_notification ( "No targets left" ) if self . auto_exit : if self . notifier : self . notifier . stop ( ) self . terminate_processes ( ) with open ( self . targets_file , 'w' ) as f : f . write ( '\n' . join ( self . ldap_strings + self . ips ) )
11071	def with_proxies ( proxy_map , get_key ) : def wrapper ( cls ) : for label , ProxiedClass in six . iteritems ( proxy_map ) : proxy = proxy_factory ( cls , label , ProxiedClass , get_key ) setattr ( cls , label , proxy ) return cls return wrapper
6559	def stitch ( csp , min_classical_gap = 2.0 , max_graph_size = 8 ) : try : dwavebinarycsp . assert_penaltymodel_factory_available ( ) except AssertionError as e : raise RuntimeError ( e ) def aux_factory ( ) : for i in count ( ) : yield 'aux{}' . format ( i ) aux = aux_factory ( ) bqm = dimod . BinaryQuadraticModel . empty ( csp . vartype ) for const in csp . constraints : configurations = const . configurations if len ( const . variables ) > max_graph_size : msg = ( "The given csp contains a constraint {const} with {num_var} variables. " "This cannot be mapped to a graph with {max_graph_size} nodes. " "Consider checking whether your constraint is irreducible." "" ) . format ( const = const , num_var = len ( const . variables ) , max_graph_size = max_graph_size ) raise ImpossibleBQM ( msg ) pmodel = None if len ( const ) == 0 : continue if min_classical_gap <= 2.0 : if len ( const ) == 1 and max_graph_size >= 1 : bqm . update ( _bqm_from_1sat ( const ) ) continue elif len ( const ) == 2 and max_graph_size >= 2 : bqm . update ( _bqm_from_2sat ( const ) ) continue for G in iter_complete_graphs ( const . variables , max_graph_size + 1 , aux ) : spec = pm . Specification ( graph = G , decision_variables = const . variables , feasible_configurations = configurations , min_classical_gap = min_classical_gap , vartype = csp . vartype ) try : pmodel = pm . get_penalty_model ( spec ) except pm . ImpossiblePenaltyModel : continue if pmodel . classical_gap >= min_classical_gap : break else : msg = ( "No penalty model can be build for constraint {}" . format ( const ) ) raise ImpossibleBQM ( msg ) bqm . update ( pmodel . model ) return bqm
3085	def service_account_email ( self ) : if self . _service_account_email is None : self . _service_account_email = ( app_identity . get_service_account_name ( ) ) return self . _service_account_email
5915	def _process_range ( self , selection , name = None ) : try : first , last , gmx_atomname = selection except ValueError : try : first , last = selection gmx_atomname = '*' except : logger . error ( "%r is not a valid range selection" , selection ) raise if name is None : name = "{first!s}-{last!s}_{gmx_atomname!s}" . format ( ** vars ( ) ) _first = self . _translate_residue ( first , default_atomname = gmx_atomname ) _last = self . _translate_residue ( last , default_atomname = gmx_atomname ) _selection = 'r {0:d} - {1:d} & & a {2!s}' . format ( _first [ 'resid' ] , _last [ 'resid' ] , gmx_atomname ) cmd = [ 'keep 0' , 'del 0' , _selection , 'name 0 {name!s}' . format ( ** vars ( ) ) , 'q' ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) rc , out , err = self . make_ndx ( n = self . ndx , o = ndx , input = cmd ) self . check_output ( out , "No atoms found for " "%(selection)r % vars ( ) ) return name , ndx
6035	def map_function ( self , func , * arg_lists ) : return GridStack ( * [ func ( * args ) for args in zip ( self , * arg_lists ) ] )
3868	async def set_typing ( self , typing = hangouts_pb2 . TYPING_TYPE_STARTED ) : try : await self . _client . set_typing ( hangouts_pb2 . SetTypingRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , type = typing , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to set typing status: {}' . format ( e ) ) raise
13630	def _renderResource ( resource , request ) : meth = getattr ( resource , 'render_' + nativeString ( request . method ) , None ) if meth is None : try : allowedMethods = resource . allowedMethods except AttributeError : allowedMethods = _computeAllowedMethods ( resource ) raise UnsupportedMethod ( allowedMethods ) return meth ( request )
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
1301	def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
8678	def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id
2676	def get_callable_handler_function ( src , handler ) : os . chdir ( src ) module_name , function_name = handler . split ( '.' ) filename = get_handler_filename ( handler ) path_to_module_file = os . path . join ( src , filename ) module = load_source ( module_name , path_to_module_file ) return getattr ( module , function_name )
653	def spDiff ( SP1 , SP2 ) : if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : print "Connected synapse matrices are different sizes" return False if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : print "Potential synapse matrices are different sizes" return False if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : print "Permanence matrices are different sizes" return False for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : connected1 = SP1 . _masterConnectedM [ i ] connected2 = SP2 . _masterConnectedM [ i ] if ( connected1 != connected2 ) : print "Connected Matrices for cell %d different" % ( i ) return False permanences1 = SP1 . _masterPermanenceM [ i ] permanences2 = SP2 . _masterPermanenceM [ i ] if ( permanences1 != permanences2 ) : print "Permanence Matrices for cell %d different" % ( i ) return False potential1 = SP1 . _masterPotentialM [ i ] potential2 = SP2 . _masterPotentialM [ i ] if ( potential1 != potential2 ) : print "Potential Matrices for cell %d different" % ( i ) return False if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : print "Firing boost factors are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : print "Duty cycles after inhibition are different between spatial poolers" return False if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : print "Duty cycles before inhibition are different between spatial poolers" return False print ( "Spatial Poolers are equivalent" ) return True
6794	def shell ( self ) : r = self . local_renderer if '@' in self . genv . host_string : r . env . shell_host_string = self . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' r . env . shell_default_dir = self . genv . shell_default_dir_template r . env . shell_interactive_djshell_str = self . genv . interactive_shell_template r . run_or_local ( 'ssh -t -i {key_filename} {shell_host_string} "{shell_interactive_djshell_str}"' )
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
7509	def _save ( self ) : fulldict = copy . deepcopy ( self . __dict__ ) for i , j in fulldict . items ( ) : if isinstance ( j , Params ) : fulldict [ i ] = j . __dict__ fulldumps = json . dumps ( fulldict , sort_keys = False , indent = 4 , separators = ( "," , ":" ) , ) assemblypath = os . path . join ( self . dirs , self . name + ".tet.json" ) if not os . path . exists ( self . dirs ) : os . mkdir ( self . dirs ) done = 0 while not done : try : with open ( assemblypath , 'w' ) as jout : jout . write ( fulldumps ) done = 1 except ( KeyboardInterrupt , SystemExit ) : print ( '.' ) continue
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = ' ' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
7078	def tic_conesearch ( ra , decl , radius_arcmin = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 10.0 , refresh = 5.0 , maxtimeout = 90.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'ra' : ra , 'dec' : decl , 'radius' : radius_arcmin / 60.0 } service = 'Mast.Catalogs.Tic.Cone' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
9023	def write ( self , bytes_ ) : string = bytes_ . decode ( self . _encoding ) self . _file . write ( string )
13486	def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) for rule in _REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
9103	def get_long_description ( ) : with codecs . open ( os . path . join ( HERE , 'README.rst' ) , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
13132	def parse_user ( entry , domain_groups ) : result = { } distinguished_name = get_field ( entry , 'distinguishedName' ) result [ 'domain' ] = "." . join ( distinguished_name . split ( ',DC=' ) [ 1 : ] ) result [ 'name' ] = get_field ( entry , 'name' ) result [ 'username' ] = get_field ( entry , 'sAMAccountName' ) result [ 'description' ] = get_field ( entry , 'description' ) result [ 'sid' ] = get_field ( entry , 'objectSid' ) . split ( '-' ) [ - 1 ] primary_group = get_field ( entry , 'primaryGroupID' ) member_of = entry [ 'attributes' ] . get ( 'memberOf' , [ ] ) groups = [ ] for member in member_of : for e in member . split ( ',' ) : if e . startswith ( 'CN=' ) : groups . append ( e [ 3 : ] ) groups . append ( domain_groups . get ( primary_group , '' ) ) result [ 'groups' ] = groups flags = [ ] try : uac = int ( get_field ( entry , 'userAccountControl' ) ) for flag , value in uac_flags . items ( ) : if uac & value : flags . append ( flag ) except ValueError : pass result [ 'flags' ] = flags return result
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
5450	def validate_bucket_name ( bucket ) : if not bucket . startswith ( 'gs://' ) : raise ValueError ( 'Invalid bucket path "%s". Must start with "gs://".' % bucket ) bucket_name = bucket [ len ( 'gs://' ) : ] if not re . search ( r'^\w[\w_\.-]{1,61}\w$' , bucket_name ) : raise ValueError ( 'Invalid bucket name: %s' % bucket )
2125	def disassociate_always_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'always' ) , parent , child )
11815	def decode ( self , ciphertext ) : "Search for a decoding of the ciphertext." self . ciphertext = ciphertext problem = PermutationDecoderProblem ( decoder = self ) return search . best_first_tree_search ( problem , lambda node : self . score ( node . state ) )
6634	def displayOutdated ( modules , dependency_specs , use_colours ) : if use_colours : DIM = colorama . Style . DIM NORMAL = colorama . Style . NORMAL BRIGHT = colorama . Style . BRIGHT YELLOW = colorama . Fore . YELLOW RED = colorama . Fore . RED GREEN = colorama . Fore . GREEN RESET = colorama . Style . RESET_ALL else : DIM = BRIGHT = YELLOW = RED = GREEN = RESET = u'' status = 0 from yotta . lib import access from yotta . lib import access_common from yotta . lib import sourceparse for name , m in modules . items ( ) : if m . isTestDependency ( ) : continue try : latest_v = access . latestSuitableVersion ( name , '*' , registry = 'modules' , quiet = True ) except access_common . Unavailable as e : latest_v = None if not m : m_version = u' ' + RESET + BRIGHT + RED + u"missing" + RESET else : m_version = DIM + u'@%s' % ( m . version ) if not latest_v : print ( u'%s%s%s%s not available from the registry%s' % ( RED , name , m_version , NORMAL , RESET ) ) status = 2 continue elif not m or m . version < latest_v : update_prevented_by = '' if m : specs_preventing_update = [ x for x in dependency_specs if x . name == name and not sourceparse . parseSourceURL ( x . nonShrinkwrappedVersionReq ( ) ) . semanticSpecMatches ( latest_v ) ] shrinkwrap_prevents_update = [ x for x in dependency_specs if x . name == name and x . isShrinkwrapped ( ) and not sourceparse . parseSourceURL ( x . versionReq ( ) ) . semanticSpecMatches ( latest_v ) ] if len ( specs_preventing_update ) : update_prevented_by = ' (update prevented by specifications: %s)' % ( ', ' . join ( [ '%s from %s' % ( x . version_req , x . specifying_module ) for x in specs_preventing_update ] ) ) if len ( shrinkwrap_prevents_update ) : update_prevented_by += ' yotta-shrinkwrap.json prevents update' if m . version . major ( ) < latest_v . major ( ) : colour = GREEN elif m . version . minor ( ) < latest_v . minor ( ) : colour = YELLOW else : colour = RED else : colour = RED print ( u'%s%s%s latest: %s%s%s%s' % ( name , m_version , RESET , colour , latest_v . version , update_prevented_by , RESET ) ) if not status : status = 1 return status
10388	def calculate_average_scores_on_graph ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) : subgraphs = generate_bioprocess_mechanisms ( graph , key = key ) scores = calculate_average_scores_on_subgraphs ( subgraphs , key = key , tag = tag , default_score = default_score , runs = runs , use_tqdm = use_tqdm ) return scores
10777	def make_clean_figure ( figsize , remove_tooltips = False , remove_keybindings = False ) : tooltip = mpl . rcParams [ 'toolbar' ] if remove_tooltips : mpl . rcParams [ 'toolbar' ] = 'None' fig = pl . figure ( figsize = figsize ) mpl . rcParams [ 'toolbar' ] = tooltip if remove_keybindings : fig . canvas . mpl_disconnect ( fig . canvas . manager . key_press_handler_id ) return fig
7001	def parallel_pf ( lclist , outdir , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , pfmethods = ( 'gls' , 'pdm' , 'mav' , 'win' ) , pfkwargs = ( { } , { } , { } , { } ) , sigclip = 10.0 , getblssnr = False , nperiodworkers = NCPUS , ncontrolworkers = 1 , liststartindex = None , listmaxobjects = None , minobservations = 500 , excludeprocessed = True ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if ( liststartindex is not None ) and ( listmaxobjects is None ) : lclist = lclist [ liststartindex : ] elif ( liststartindex is None ) and ( listmaxobjects is not None ) : lclist = lclist [ : listmaxobjects ] elif ( liststartindex is not None ) and ( listmaxobjects is not None ) : lclist = lclist [ liststartindex : liststartindex + listmaxobjects ] tasklist = [ ( x , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nperiodworkers , minobservations , excludeprocessed ) for x in lclist ] with ProcessPoolExecutor ( max_workers = ncontrolworkers ) as executor : resultfutures = executor . map ( _runpf_worker , tasklist ) results = [ x for x in resultfutures ] return results
588	def compute ( self ) : result = self . _constructClassificationRecord ( ) if result . ROWID >= self . _autoDetectWaitRecords : self . _updateState ( result ) self . saved_states . append ( result ) if len ( self . saved_states ) > self . _history_length : self . saved_states . pop ( 0 ) return result
9219	def _blocks ( self , name ) : i = len ( self ) while i >= 0 : i -= 1 if name in self [ i ] [ '__names__' ] : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and r == name : return b else : for b in self [ i ] [ '__blocks__' ] : r = b . raw ( ) if r and name . startswith ( r ) : b = utility . blocksearch ( b , name ) if b : return b return False
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid return logList
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
5958	def break_array ( a , threshold = numpy . pi , other = None ) : assert len ( a . shape ) == 1 , "Only 1D arrays supported" if other is not None and a . shape != other . shape : raise ValueError ( "arrays must be of identical shape" ) breaks = numpy . where ( numpy . abs ( numpy . diff ( a ) ) >= threshold ) [ 0 ] breaks += 1 m = len ( breaks ) b = numpy . empty ( ( len ( a ) + m ) ) b_breaks = breaks + numpy . arange ( m ) mask = numpy . zeros_like ( b , dtype = numpy . bool ) mask [ b_breaks ] = True b [ ~ mask ] = a b [ mask ] = numpy . NAN if other is not None : c = numpy . empty_like ( b ) c [ ~ mask ] = other c [ mask ] = numpy . NAN ma_c = numpy . ma . array ( c , mask = mask ) else : ma_c = None return numpy . ma . array ( b , mask = mask ) , ma_c
1104	def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
12239	def beale ( theta ) : x , y = theta A = 1.5 - x + x * y B = 2.25 - x + x * y ** 2 C = 2.625 - x + x * y ** 3 obj = A ** 2 + B ** 2 + C ** 2 grad = np . array ( [ 2 * A * ( y - 1 ) + 2 * B * ( y ** 2 - 1 ) + 2 * C * ( y ** 3 - 1 ) , 2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2 ] ) return obj , grad
11799	def prune ( self , var , value , removals ) : "Rule out var=value." self . curr_domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
3353	def _replace_on_id ( self , new_object ) : the_id = new_object . id the_index = self . _dict [ the_id ] list . __setitem__ ( self , the_index , new_object )
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
1156	def go_str ( value ) : io = StringIO . StringIO ( ) io . write ( '"' ) for c in value : if c in _ESCAPES : io . write ( _ESCAPES [ c ] ) elif c in _SIMPLE_CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '"' ) return io . getvalue ( )
11207	def gettz_db_metadata ( ) : warnings . warn ( "zoneinfo.gettz_db_metadata() will be removed in future " "versions, to use the dateutil-provided zoneinfo files, " "ZoneInfoFile object and query the 'metadata' attribute " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . metadata
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
521	def _initPermanence ( self , potential , connectedPct ) : perm = numpy . zeros ( self . _numInputs , dtype = realDType ) for i in xrange ( self . _numInputs ) : if ( potential [ i ] < 1 ) : continue if ( self . _random . getReal64 ( ) <= connectedPct ) : perm [ i ] = self . _initPermConnected ( ) else : perm [ i ] = self . _initPermNonConnected ( ) perm [ perm < self . _synPermTrimThreshold ] = 0 return perm
8695	def __set_baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . __writeln ( UART_SETUP . format ( baud = baud ) ) time . sleep ( 0.1 ) try : self . _port . setBaudrate ( baud ) except AttributeError : self . _port . baudrate = baud
1031	def b32decode ( s , casefold = False , map01 = None ) : quanta , leftover = divmod ( len ( s ) , 8 ) if leftover : raise TypeError ( 'Incorrect padding' ) if map01 : s = s . translate ( string . maketrans ( b'01' , b'O' + map01 ) ) if casefold : s = s . upper ( ) padchars = 0 mo = re . search ( '(?P<pad>[=]*)$' , s ) if mo : padchars = len ( mo . group ( 'pad' ) ) if padchars > 0 : s = s [ : - padchars ] parts = [ ] acc = 0 shift = 35 for c in s : val = _b32rev . get ( c ) if val is None : raise TypeError ( 'Non-base32 digit found' ) acc += _b32rev [ c ] << shift shift -= 5 if shift < 0 : parts . append ( binascii . unhexlify ( '%010x' % acc ) ) acc = 0 shift = 35 last = binascii . unhexlify ( '%010x' % acc ) if padchars == 0 : last = '' elif padchars == 1 : last = last [ : - 1 ] elif padchars == 3 : last = last [ : - 2 ] elif padchars == 4 : last = last [ : - 3 ] elif padchars == 6 : last = last [ : - 4 ] else : raise TypeError ( 'Incorrect padding' ) parts . append ( last ) return EMPTYSTRING . join ( parts )
2128	def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : return func ( * args , ** kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
12523	def check_call ( cmd_args ) : p = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) ( output , err ) = p . communicate ( ) return output
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : load_overrides ( args . config ) defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) config_cli_args = get_group ( 'cli' ) for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
9171	def declare_browsable_routes ( config ) : config . add_notfound_view ( default_exceptionresponse_view , append_slash = True ) add_route = config . add_route add_route ( 'admin-index' , '/a/' ) add_route ( 'admin-moderation' , '/a/moderation/' ) add_route ( 'admin-api-keys' , '/a/api-keys/' ) add_route ( 'admin-add-site-messages' , '/a/site-messages/' , request_method = 'GET' ) add_route ( 'admin-add-site-messages-POST' , '/a/site-messages/' , request_method = 'POST' ) add_route ( 'admin-delete-site-messages' , '/a/site-messages/' , request_method = 'DELETE' ) add_route ( 'admin-edit-site-message' , '/a/site-messages/{id}/' , request_method = 'GET' ) add_route ( 'admin-edit-site-message-POST' , '/a/site-messages/{id}/' , request_method = 'POST' ) add_route ( 'admin-content-status' , '/a/content-status/' ) add_route ( 'admin-content-status-single' , '/a/content-status/{uuid}' ) add_route ( 'admin-print-style' , '/a/print-style/' ) add_route ( 'admin-print-style-single' , '/a/print-style/{style}' )
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
1706	def expand_args ( command ) : if isinstance ( command , ( str , unicode ) ) : splitter = shlex . shlex ( command . encode ( 'utf-8' ) ) splitter . whitespace = '|' splitter . whitespace_split = True command = [ ] while True : token = splitter . get_token ( ) if token : command . append ( token ) else : break command = list ( map ( shlex . split , command ) ) return command
8773	def get_lswitch_ids_for_network ( self , context , network_id ) : lswitches = self . _lswitches_for_network ( context , network_id ) . results ( ) return [ s [ 'uuid' ] for s in lswitches [ "results" ] ]
3798	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . Tc , self . S1 , self . S2 = self . ais [ i ] , self . Tcs [ i ] , self . S1s [ i ] , self . S2s [ i ]
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
2232	def _register_numpy_extensions ( self ) : import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
12674	def subset ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . subset ( * args [ 1 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . SUBSET , * args )
426	def check_unfinished_task ( self , task_name = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( "[Database] No unfinished task - task_name: {}" . format ( task_name ) ) return False else : logging . info ( "[Database] Find {} unfinished task - task_name: {}" . format ( n_task , task_name ) ) return True
8876	def compute_dosage ( expec , alt = None ) : r if alt is None : return expec [ ... , - 1 ] try : return expec [ : , alt ] except NotImplementedError : alt = asarray ( alt , int ) return asarray ( expec , float ) [ : , alt ]
5106	def _current_color ( self , which = 0 ) : if which == 1 : color = self . colors [ 'edge_loop_color' ] elif which == 2 : color = self . colors [ 'vertex_color' ] else : div = self . coloring_sensitivity * self . num_servers + 1. tmp = 1. - min ( self . num_system / div , 1 ) if self . edge [ 0 ] == self . edge [ 1 ] : color = [ i * tmp for i in self . colors [ 'vertex_fill_color' ] ] color [ 3 ] = 1.0 else : color = [ i * tmp for i in self . colors [ 'edge_color' ] ] color [ 3 ] = 1 / 2. return color
6819	def sync_media ( self , sync_set = None , clean = 0 , iter_local_paths = 0 ) : self . genv . SITE = self . genv . SITE or self . genv . default_site r = self . local_renderer clean = int ( clean ) self . vprint ( 'Getting site data for %s...' % self . genv . SITE ) self . set_site_specifics ( self . genv . SITE ) sync_sets = r . env . sync_sets if sync_set : sync_sets = [ sync_set ] ret_paths = [ ] for _sync_set in sync_sets : for paths in r . env . sync_sets [ _sync_set ] : r . env . sync_local_path = os . path . abspath ( paths [ 'local_path' ] % self . genv ) if paths [ 'local_path' ] . endswith ( '/' ) and not r . env . sync_local_path . endswith ( '/' ) : r . env . sync_local_path += '/' if iter_local_paths : ret_paths . append ( r . env . sync_local_path ) continue r . env . sync_remote_path = paths [ 'remote_path' ] % self . genv if clean : r . sudo ( 'rm -Rf {apache_sync_remote_path}' ) print ( 'Syncing %s to %s...' % ( r . env . sync_local_path , r . env . sync_remote_path ) ) r . env . tmp_chmod = paths . get ( 'chmod' , r . env . chmod ) r . sudo ( 'mkdir -p {apache_sync_remote_path}' ) r . sudo ( 'chmod -R {apache_tmp_chmod} {apache_sync_remote_path}' ) r . local ( 'rsync -rvz --progress --recursive --no-p --no-g ' '--rsh "ssh -o StrictHostKeyChecking=no -i {key_filename}" {apache_sync_local_path} {user}@{host_string}:{apache_sync_remote_path}' ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {apache_sync_remote_path}' ) if iter_local_paths : return ret_paths
6290	def add_program_dir ( self , directory ) : dirs = list ( self . PROGRAM_DIRS ) dirs . append ( directory ) self . PROGRAM_DIRS = dirs
3629	def horiz_div ( col_widths , horiz , vert , padding ) : horizs = [ horiz * w for w in col_widths ] div = '' . join ( [ padding * horiz , vert , padding * horiz ] ) return div . join ( horizs )
1055	def shuffle ( self , x , random = None ) : if random is None : random = self . random _int = int for i in reversed ( xrange ( 1 , len ( x ) ) ) : j = _int ( random ( ) * ( i + 1 ) ) x [ i ] , x [ j ] = x [ j ] , x [ i ]
10546	def get_taskruns ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'taskrun' , params = params ) if type ( res ) . __name__ == 'list' : return [ TaskRun ( taskrun ) for taskrun in res ] else : raise TypeError except : raise
3647	def sendToSbs ( self , challenge_id , item_id ) : method = 'PUT' url = 'sbs/challenge/%s/squad' % challenge_id squad = self . sbsSquad ( challenge_id ) players = [ ] moved = False n = 0 for i in squad [ 'squad' ] [ 'players' ] : if i [ 'itemData' ] [ 'id' ] == item_id : return False if i [ 'itemData' ] [ 'id' ] == 0 and not moved : i [ 'itemData' ] [ 'id' ] = item_id moved = True players . append ( { "index" : n , "itemData" : { "id" : i [ 'itemData' ] [ 'id' ] , "dream" : False } } ) n += 1 data = { 'players' : players } if not moved : return False else : self . __request__ ( method , url , data = json . dumps ( data ) ) return True
5249	def bopen ( ** kwargs ) : con = BCon ( ** kwargs ) con . start ( ) try : yield con finally : con . stop ( )
6365	def to_dict ( self ) : return { 'tp' : self . _tp , 'tn' : self . _tn , 'fp' : self . _fp , 'fn' : self . _fn }
7699	def verify_roster_push ( self , fix = False ) : self . _verify ( ( None , u"from" , u"to" , u"both" , u"remove" ) , fix )
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
4384	def allow ( self , roles , methods , with_children = True ) : def decorator ( view_func ) : _methods = [ m . upper ( ) for m in methods ] for r , m , v in itertools . product ( roles , _methods , [ view_func . __name__ ] ) : self . before_acl [ 'allow' ] . append ( ( r , m , v , with_children ) ) return view_func return decorator
4977	def course_or_program_exist ( self , course_id , program_uuid ) : course_exists = course_id and CourseApiClient ( ) . get_course_details ( course_id ) program_exists = program_uuid and CourseCatalogApiServiceClient ( ) . program_exists ( program_uuid ) return course_exists or program_exists
8393	def parse_pylint_output ( pylint_output ) : for line in pylint_output : if not line . strip ( ) : continue if line [ 0 : 5 ] in ( "-" * 5 , "*" * 5 ) : continue parsed = PYLINT_PARSEABLE_REGEX . search ( line ) if parsed is None : LOG . warning ( u"Unable to parse %r. If this is a lint failure, please re-run pylint with the " u"--output-format=parseable option, otherwise, you can ignore this message." , line ) continue parsed_dict = parsed . groupdict ( ) parsed_dict [ 'linenum' ] = int ( parsed_dict [ 'linenum' ] ) yield PylintError ( ** parsed_dict )
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
12058	def TK_askPassword ( title = "input" , msg = "type here:" ) : root = tkinter . Tk ( ) root . withdraw ( ) root . attributes ( "-topmost" , True ) root . lift ( ) value = tkinter . simpledialog . askstring ( title , msg ) root . destroy ( ) return value
2980	def cmd_logs ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) puts ( b . logs ( opts . container ) . decode ( encoding = 'UTF-8' ) )
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
4718	def tsuite_enter ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { name: %r }" % tsuite [ "name" ] ) rcode = 0 for hook in tsuite [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:enter { rcode: %r } " % rcode , rcode ) return rcode
3358	def index ( self , id , * args ) : if isinstance ( id , string_types ) : try : return self . _dict [ id ] except KeyError : raise ValueError ( "%s not found" % id ) try : i = self . _dict [ id . id ] if self [ i ] is not id : raise ValueError ( "Another object with the identical id (%s) found" % id . id ) return i except KeyError : raise ValueError ( "%s not found" % str ( id ) )
11924	def render_to ( path , template , ** data ) : try : renderer . render_to ( path , template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
2857	def read ( self , length ) : if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
3349	def geometric_fba ( model , epsilon = 1E-06 , max_tries = 200 , processes = None ) : with model : consts = [ ] obj_vars = [ ] updating_vars_cons = [ ] prob = model . problem add_pfba ( model ) model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 for rxn in model . reactions : var = prob . Variable ( "geometric_fba_" + rxn . id , lb = 0 , ub = mean_flux [ rxn . id ] ) upper_const = prob . Constraint ( rxn . flux_expression - var , ub = mean_flux [ rxn . id ] , name = "geometric_fba_upper_const_" + rxn . id ) lower_const = prob . Constraint ( rxn . flux_expression + var , lb = fva_sol . at [ rxn . id , "minimum" ] , name = "geometric_fba_lower_const_" + rxn . id ) updating_vars_cons . append ( ( rxn . id , var , upper_const , lower_const ) ) consts . extend ( [ var , upper_const , lower_const ] ) obj_vars . append ( var ) model . add_cons_vars ( consts ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count = 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) while delta > epsilon and count < max_tries : for rxn_id , var , u_c , l_c in updating_vars_cons : var . ub = mean_flux [ rxn_id ] u_c . ub = mean_flux [ rxn_id ] l_c . lb = fva_sol . at [ rxn_id , "minimum" ] sol = model . optimize ( ) fva_sol = flux_variability_analysis ( model , processes = processes ) mean_flux = ( fva_sol [ "maximum" ] + fva_sol [ "minimum" ] ) . abs ( ) / 2 delta = ( fva_sol [ "maximum" ] - fva_sol [ "minimum" ] ) . max ( ) count += 1 LOGGER . debug ( "Iteration: %d; delta: %.3g; status: %s." , count , delta , sol . status ) if count == max_tries : raise RuntimeError ( "The iterations have exceeded the maximum value of {}. " "This is probably due to the increased complexity of the " "model and can lead to inaccurate results. Please set a " "different convergence tolerance and/or increase the " "maximum iterations" . format ( max_tries ) ) return sol
5376	def outputs_are_present ( outputs ) : for o in outputs : if not o . value : continue if o . recursive : if not folder_exists ( o . value ) : return False else : if not simple_pattern_exists_in_gcs ( o . value ) : return False return True
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
5025	def get_channel_classes ( channel_code ) : if channel_code : channel_code = channel_code . upper ( ) if channel_code not in INTEGRATED_CHANNEL_CHOICES : raise CommandError ( _ ( 'Invalid integrated channel: {channel}' ) . format ( channel = channel_code ) ) channel_classes = [ INTEGRATED_CHANNEL_CHOICES [ channel_code ] ] else : channel_classes = INTEGRATED_CHANNEL_CHOICES . values ( ) return channel_classes
8662	def migrate ( src_path , src_passphrase , src_backend , dst_path , dst_passphrase , dst_backend ) : src_storage = STORAGE_MAPPING [ src_backend ] ( ** _parse_path_string ( src_path ) ) dst_storage = STORAGE_MAPPING [ dst_backend ] ( ** _parse_path_string ( dst_path ) ) src_stash = Stash ( src_storage , src_passphrase ) dst_stash = Stash ( dst_storage , dst_passphrase ) keys = src_stash . export ( ) dst_stash . load ( src_passphrase , keys = keys )
6939	def checkplot_infokey_worker ( task ) : cpf , keys = task cpd = _read_checkplot_picklefile ( cpf ) resultkeys = [ ] for k in keys : try : resultkeys . append ( _dict_get ( cpd , k ) ) except Exception as e : resultkeys . append ( np . nan ) return resultkeys
1411	def filter_spouts ( table , header ) : spouts_info = [ ] for row in table : if row [ 0 ] == 'spout' : spouts_info . append ( row ) return spouts_info , header
9834	def parse ( self , DXfield ) : self . DXfield = DXfield self . currentobject = None self . objects = [ ] self . tokens = [ ] with open ( self . filename , 'r' ) as self . dxfile : self . use_parser ( 'general' ) for o in self . objects : if o . type == 'field' : DXfield . id = o . id continue c = o . initialize ( ) self . DXfield . add ( c . component , c ) del self . currentobject , self . objects
1786	def CMPXCHG8B ( cpu , dest ) : size = dest . size cmp_reg_name_l = { 64 : 'EAX' , 128 : 'RAX' } [ size ] cmp_reg_name_h = { 64 : 'EDX' , 128 : 'RDX' } [ size ] src_reg_name_l = { 64 : 'EBX' , 128 : 'RBX' } [ size ] src_reg_name_h = { 64 : 'ECX' , 128 : 'RCX' } [ size ] cmph = cpu . read_register ( cmp_reg_name_h ) cmpl = cpu . read_register ( cmp_reg_name_l ) srch = cpu . read_register ( src_reg_name_h ) srcl = cpu . read_register ( src_reg_name_l ) cmp0 = Operators . CONCAT ( size , cmph , cmpl ) src0 = Operators . CONCAT ( size , srch , srcl ) arg_dest = dest . read ( ) cpu . ZF = arg_dest == cmp0 dest . write ( Operators . ITEBV ( size , cpu . ZF , Operators . CONCAT ( size , srch , srcl ) , arg_dest ) ) cpu . write_register ( cmp_reg_name_l , Operators . ITEBV ( size // 2 , cpu . ZF , cmpl , Operators . EXTRACT ( arg_dest , 0 , size // 2 ) ) ) cpu . write_register ( cmp_reg_name_h , Operators . ITEBV ( size // 2 , cpu . ZF , cmph , Operators . EXTRACT ( arg_dest , size // 2 , size // 2 ) ) )
3130	def check_url ( url ) : URL_REGEX = re . compile ( u"^" u"(?:(?:https?|ftp)://)" u"(?:\S+(?::\S*)?@)?" u"(?:" u"(?!(?:10|127)(?:\.\d{1,3}){3})" u"(?!(?:169\.254|192\.168)(?:\.\d{1,3}){2})" u"(?!172\.(?:1[6-9]|2\d|3[0-1])(?:\.\d{1,3}){2})" u"(?:[1-9]\d?|1\d\d|2[01]\d|22[0-3])" u"(?:\.(?:1?\d{1,2}|2[0-4]\d|25[0-5])){2}" u"(?:\.(?:[1-9]\d?|1\d\d|2[0-4]\d|25[0-4]))" u"|" u"(?:(?:[a-z\u00a1-\uffff0-9]-?)*[a-z\u00a1-\uffff0-9]+)" u"(?:\.(?:[a-z\u00a1-\uffff0-9]-?)*[a-z\u00a1-\uffff0-9]+)*" u"(?:\.(?:[a-z\u00a1-\uffff]{2,}))" u")" u"(?::\d{2,5})?" u"(?:/\S*)?" u"$" , re . UNICODE ) if not re . match ( URL_REGEX , url ) : raise ValueError ( 'String passed is not a valid url' ) return
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
10217	def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
1563	def add_task_hook ( self , task_hook ) : if not isinstance ( task_hook , ITaskHook ) : raise TypeError ( "In add_task_hook(): attempt to add non ITaskHook instance, given: %s" % str ( type ( task_hook ) ) ) self . task_hooks . append ( task_hook )
5365	def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
4605	def upgrade ( self ) : assert callable ( self . blockchain . upgrade_account ) return self . blockchain . upgrade_account ( account = self )
5353	def retain_identities ( self , retention_time ) : enrich_es = self . conf [ 'es_enrichment' ] [ 'url' ] sortinghat_db = self . db current_data_source = self . get_backend ( self . backend_section ) active_data_sources = self . config . get_active_data_sources ( ) if retention_time is None : logger . debug ( "[identities retention] Retention policy disabled, no identities will be deleted." ) return if retention_time <= 0 : logger . debug ( "[identities retention] Retention time must be greater than 0." ) return retain_identities ( retention_time , enrich_es , sortinghat_db , current_data_source , active_data_sources )
12318	def drop ( self , repo , args = [ ] ) : rootdir = repo . rootdir if os . path . exists ( rootdir ) : print ( "Cleaning repo directory: {}" . format ( rootdir ) ) shutil . rmtree ( rootdir ) server_repodir = self . server_rootdir_from_repo ( repo , create = False ) if os . path . exists ( server_repodir ) : print ( "Cleaning data from local git 'server': {}" . format ( server_repodir ) ) shutil . rmtree ( server_repodir ) super ( GitRepoManager , self ) . drop ( repo ) return { 'status' : 'success' , 'message' : "successful cleanup" }
11982	def is_valid_ip ( self , ip ) : if not isinstance ( ip , ( IPv4Address , CIDR ) ) : if str ( ip ) . find ( '/' ) == - 1 : ip = IPv4Address ( ip ) else : ip = CIDR ( ip ) if isinstance ( ip , IPv4Address ) : if ip < self . _first_ip or ip > self . _last_ip : return False elif isinstance ( ip , CIDR ) : if ip . _nm . _ip_dec == 0xFFFFFFFE and self . _nm . _ip_dec != 0xFFFFFFFE : compare_to_first = self . _net_ip . _ip_dec compare_to_last = self . _bc_ip . _ip_dec else : compare_to_first = self . _first_ip . _ip_dec compare_to_last = self . _last_ip . _ip_dec if ip . _first_ip . _ip_dec < compare_to_first or ip . _last_ip . _ip_dec > compare_to_last : return False return True
11414	def record_get_subfields ( rec , tag , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) return field [ 0 ]
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
2604	def close ( self ) : if self . reuse : logger . debug ( "Ipcontroller not shutting down: reuse enabled" ) return if self . mode == "manual" : logger . debug ( "Ipcontroller not shutting down: Manual mode" ) return try : pgid = os . getpgid ( self . proc . pid ) os . killpg ( pgid , signal . SIGTERM ) time . sleep ( 0.2 ) os . killpg ( pgid , signal . SIGKILL ) try : self . proc . wait ( timeout = 1 ) x = self . proc . returncode if x == 0 : logger . debug ( "Controller exited with {0}" . format ( x ) ) else : logger . error ( "Controller exited with {0}. May require manual cleanup" . format ( x ) ) except subprocess . TimeoutExpired : logger . warn ( "Ipcontroller process:{0} cleanup failed. May require manual cleanup" . format ( self . proc . pid ) ) except Exception as e : logger . warn ( "Failed to kill the ipcontroller process[{0}]: {1}" . format ( self . proc . pid , e ) )
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
1911	def GetNBits ( value , nbits ) : if isinstance ( value , int ) : return Operators . EXTRACT ( value , 0 , nbits ) elif isinstance ( value , BitVec ) : if value . size < nbits : return Operators . ZEXTEND ( value , nbits ) else : return Operators . EXTRACT ( value , 0 , nbits )
12617	def get_data ( img ) : if hasattr ( img , '_data_cache' ) and img . _data_cache is None : img = copy . deepcopy ( img ) gc . collect ( ) return img . get_data ( )
5718	def pull_datapackage ( descriptor , name , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) datapackage_name = name plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) resources = [ ] for table in storage . buckets : schema = storage . describe ( table ) base = os . path . dirname ( descriptor ) path , name = _restore_path ( table ) fullpath = os . path . join ( base , path ) helpers . ensure_dir ( fullpath ) with io . open ( fullpath , 'wb' ) as file : model = Schema ( deepcopy ( schema ) ) data = storage . iter ( table ) writer = csv . writer ( file , encoding = 'utf-8' ) writer . writerow ( model . headers ) for row in data : writer . writerow ( row ) resource = { 'schema' : schema , 'path' : path } if name is not None : resource [ 'name' ] = name resources . append ( resource ) mode = 'w' encoding = 'utf-8' if six . PY2 : mode = 'wb' encoding = None resources = _restore_resources ( resources ) helpers . ensure_dir ( descriptor ) with io . open ( descriptor , mode = mode , encoding = encoding ) as file : descriptor = { 'name' : datapackage_name , 'resources' : resources , } json . dump ( descriptor , file , indent = 4 ) return storage
8423	def hls_palette ( n_colors = 6 , h = .01 , l = .6 , s = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues -= hues . astype ( int ) palette = [ colorsys . hls_to_rgb ( h_i , l , s ) for h_i in hues ] return palette
13788	def open ( name = None , fileobj = None , closefd = True ) : return Guesser ( ) . open ( name = name , fileobj = fileobj , closefd = closefd )
13430	def create_site ( self , params = { } ) : url = "/2/sites/" body = params data = self . _post_resource ( url , body ) return self . site_from_json ( data [ "site" ] )
5966	def get_lipid_vdwradii ( outdir = os . path . curdir , libdir = None ) : vdwradii_dat = os . path . join ( outdir , "vdwradii.dat" ) if libdir is not None : filename = os . path . join ( libdir , 'vdwradii.dat' ) if not os . path . exists ( filename ) : msg = 'No VDW database file found in {filename!r}.' . format ( ** vars ( ) ) logger . exception ( msg ) raise OSError ( msg , errno . ENOENT ) else : try : filename = os . path . join ( os . environ [ 'GMXLIB' ] , 'vdwradii.dat' ) except KeyError : try : filename = os . path . join ( os . environ [ 'GMXDATA' ] , 'top' , 'vdwradii.dat' ) except KeyError : msg = "Cannot find vdwradii.dat. Set GMXLIB (point to 'top') or GMXDATA ('share/gromacs')." logger . exception ( msg ) raise OSError ( msg , errno . ENOENT ) if not os . path . exists ( filename ) : msg = "Cannot find {filename!r}; something is wrong with the Gromacs installation." . format ( ** vars ( ) ) logger . exception ( msg , errno . ENOENT ) raise OSError ( msg ) patterns = vdw_lipid_resnames + list ( { x [ : 3 ] for x in vdw_lipid_resnames } ) with open ( vdwradii_dat , 'w' ) as outfile : outfile . write ( '; Special larger vdw radii for solvating lipid membranes\n' ) for resname in patterns : for atom , radius in vdw_lipid_atom_radii . items ( ) : outfile . write ( '{resname:4!s} {atom:<5!s} {radius:5.3f}\n' . format ( ** vars ( ) ) ) with open ( filename , 'r' ) as infile : for line in infile : outfile . write ( line ) logger . debug ( 'Created lipid vdW radii file {vdwradii_dat!r}.' . format ( ** vars ( ) ) ) return realpath ( vdwradii_dat )
1902	def summarized_name ( self , name ) : components = name . split ( '.' ) prefix = '.' . join ( c [ 0 ] for c in components [ : - 1 ] ) return f'{prefix}.{components[-1]}'
6347	def _redo_language ( self , term , name_mode , rules , final_rules1 , final_rules2 , concat ) : language_arg = self . _language ( term , name_mode ) return self . _phonetic ( term , name_mode , rules , final_rules1 , final_rules2 , language_arg , concat , )
904	def read ( cls , proto ) : anomalyLikelihood = object . __new__ ( cls ) anomalyLikelihood . _iteration = proto . iteration anomalyLikelihood . _historicalScores = collections . deque ( maxlen = proto . historicWindowSize ) for i , score in enumerate ( proto . historicalScores ) : anomalyLikelihood . _historicalScores . append ( ( i , score . value , score . anomalyScore ) ) if proto . distribution . name : anomalyLikelihood . _distribution = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] [ "name" ] = proto . distribution . name anomalyLikelihood . _distribution [ 'distribution' ] [ "mean" ] = proto . distribution . mean anomalyLikelihood . _distribution [ 'distribution' ] [ "variance" ] = proto . distribution . variance anomalyLikelihood . _distribution [ 'distribution' ] [ "stdev" ] = proto . distribution . stdev anomalyLikelihood . _distribution [ "movingAverage" ] = { } anomalyLikelihood . _distribution [ "movingAverage" ] [ "windowSize" ] = proto . distribution . movingAverage . windowSize anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] = [ ] for value in proto . distribution . movingAverage . historicalValues : anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] . append ( value ) anomalyLikelihood . _distribution [ "movingAverage" ] [ "total" ] = proto . distribution . movingAverage . total anomalyLikelihood . _distribution [ "historicalLikelihoods" ] = [ ] for likelihood in proto . distribution . historicalLikelihoods : anomalyLikelihood . _distribution [ "historicalLikelihoods" ] . append ( likelihood ) else : anomalyLikelihood . _distribution = None anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod anomalyLikelihood . _learningPeriod = proto . learningPeriod anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod return anomalyLikelihood
7876	def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type = "set" ) payload = ResourceBindingPayload ( resource = resource ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _bind_success , self . _bind_error ) stream . send ( stanza ) stream . event ( BindingResourceEvent ( resource ) )
5948	def remove_legend ( ax = None ) : from pylab import gca , draw if ax is None : ax = gca ( ) ax . legend_ = None draw ( )
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
13020	def _execute ( self , query , commit = False , working_columns = None ) : log . debug ( "RawlBase._execute()" ) result = [ ] if working_columns is None : working_columns = self . columns with RawlConnection ( self . dsn ) as conn : query_id = random . randrange ( 9999 ) curs = conn . cursor ( ) try : log . debug ( "Executing(%s): %s" % ( query_id , query . as_string ( curs ) ) ) except : log . exception ( "LOGGING EXCEPTION LOL" ) curs . execute ( query ) log . debug ( "Executed" ) if commit == True : log . debug ( "COMMIT(%s)" % query_id ) conn . commit ( ) log . debug ( "curs.rowcount: %s" % curs . rowcount ) if curs . rowcount > 0 : result_rows = curs . fetchall ( ) for row in result_rows : i = 0 row_dict = { } for col in working_columns : try : col = col . replace ( '.' , '_' ) row_dict [ col ] = row [ i ] except IndexError : pass i += 1 log . debug ( "Appending dict to result: %s" % row_dict ) rr = RawlResult ( working_columns , row_dict ) result . append ( rr ) curs . close ( ) return result
12379	def post ( self , request , response ) : if self . slug is not None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'create' ) data = self . _clean ( None , self . request . read ( deserialize = True ) ) item = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( item )
214	def change_normalization ( cls , arr , source , target ) : ia . do_assert ( ia . is_np_array ( arr ) ) if isinstance ( source , HeatmapsOnImage ) : source = ( source . min_value , source . max_value ) else : ia . do_assert ( isinstance ( source , tuple ) ) ia . do_assert ( len ( source ) == 2 ) ia . do_assert ( source [ 0 ] < source [ 1 ] ) if isinstance ( target , HeatmapsOnImage ) : target = ( target . min_value , target . max_value ) else : ia . do_assert ( isinstance ( target , tuple ) ) ia . do_assert ( len ( target ) == 2 ) ia . do_assert ( target [ 0 ] < target [ 1 ] ) eps = np . finfo ( arr . dtype ) . eps mins_same = source [ 0 ] - 10 * eps < target [ 0 ] < source [ 0 ] + 10 * eps maxs_same = source [ 1 ] - 10 * eps < target [ 1 ] < source [ 1 ] + 10 * eps if mins_same and maxs_same : return np . copy ( arr ) min_source , max_source = source min_target , max_target = target diff_source = max_source - min_source diff_target = max_target - min_target arr_0to1 = ( arr - min_source ) / diff_source arr_target = min_target + arr_0to1 * diff_target return arr_target
12099	def get_root_directory ( self , timestamp = None ) : if timestamp is None : timestamp = self . timestamp if self . timestamp_format is not None : root_name = ( time . strftime ( self . timestamp_format , timestamp ) + '-' + self . batch_name ) else : root_name = self . batch_name path = os . path . join ( self . output_directory , * ( self . subdir + [ root_name ] ) ) return os . path . abspath ( path )
1917	def fork ( self , state , expression , policy = 'ALL' , setstate = None ) : assert isinstance ( expression , Expression ) if setstate is None : setstate = lambda x , y : None solutions = state . concretize ( expression , policy ) if not solutions : raise ExecutorError ( "Forking on unfeasible constraint set" ) if len ( solutions ) == 1 : setstate ( state , solutions [ 0 ] ) return state logger . info ( "Forking. Policy: %s. Values: %s" , policy , ', ' . join ( f'0x{sol:x}' for sol in solutions ) ) self . _publish ( 'will_fork_state' , state , expression , solutions , policy ) children = [ ] for new_value in solutions : with state as new_state : new_state . constrain ( expression == new_value ) setstate ( new_state , new_value ) self . _publish ( 'did_fork_state' , new_state , expression , new_value , policy ) state_id = self . enqueue ( new_state ) children . append ( state_id ) logger . info ( "Forking current state into states %r" , children ) return None
10809	def update ( self , name = None , description = None , privacy_policy = None , subscription_policy = None , is_managed = None ) : with db . session . begin_nested ( ) : if name is not None : self . name = name if description is not None : self . description = description if ( privacy_policy is not None and PrivacyPolicy . validate ( privacy_policy ) ) : self . privacy_policy = privacy_policy if ( subscription_policy is not None and SubscriptionPolicy . validate ( subscription_policy ) ) : self . subscription_policy = subscription_policy if is_managed is not None : self . is_managed = is_managed db . session . merge ( self ) return self
4277	def build ( self , force = False ) : "Create the image gallery" if not self . albums : self . logger . warning ( "No albums found." ) return def log_func ( x ) : available_length = get_terminal_size ( ) [ 0 ] - 64 if x and available_length > 10 : return x . name [ : available_length ] else : return "" try : with progressbar ( self . albums . values ( ) , label = "Collecting files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : media_list = [ f for album in albums for f in self . process_dir ( album , force = force ) ] except KeyboardInterrupt : sys . exit ( 'Interrupted' ) bar_opt = { 'label' : "Processing files" , 'show_pos' : True , 'file' : self . progressbar_target } failed_files = [ ] if self . pool : try : with progressbar ( length = len ( media_list ) , ** bar_opt ) as bar : for res in self . pool . imap_unordered ( worker , media_list ) : if res : failed_files . append ( res ) bar . update ( 1 ) self . pool . close ( ) self . pool . join ( ) except KeyboardInterrupt : self . pool . terminate ( ) sys . exit ( 'Interrupted' ) except pickle . PicklingError : self . logger . critical ( "Failed to process files with the multiprocessing feature." " This can be caused by some module import or object " "defined in the settings file, which can't be serialized." , exc_info = True ) sys . exit ( 'Abort' ) else : with progressbar ( media_list , ** bar_opt ) as medias : for media_item in medias : res = process_file ( media_item ) if res : failed_files . append ( res ) if failed_files : self . remove_files ( failed_files ) if self . settings [ 'write_html' ] : album_writer = AlbumPageWriter ( self . settings , index_title = self . title ) album_list_writer = AlbumListPageWriter ( self . settings , index_title = self . title ) with progressbar ( self . albums . values ( ) , label = "%16s" % "Writing files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : for album in albums : if album . albums : if album . medias : self . logger . warning ( "Album %s contains sub-albums and images. " "Please move images to their own sub-album. " "Images in album %s will not be visible." , album . title , album . title ) album_list_writer . write ( album ) else : album_writer . write ( album ) print ( '' ) signals . gallery_build . send ( self )
9564	def create_validator ( ) : field_names = ( 'study_id' , 'patient_id' , 'gender' , 'age_years' , 'age_months' , 'date_inclusion' ) validator = CSVValidator ( field_names ) validator . add_header_check ( 'EX1' , 'bad header' ) validator . add_record_length_check ( 'EX2' , 'unexpected record length' ) validator . add_value_check ( 'study_id' , int , 'EX3' , 'study id must be an integer' ) validator . add_value_check ( 'patient_id' , int , 'EX4' , 'patient id must be an integer' ) validator . add_value_check ( 'gender' , enumeration ( 'M' , 'F' ) , 'EX5' , 'invalid gender' ) validator . add_value_check ( 'age_years' , number_range_inclusive ( 0 , 120 , int ) , 'EX6' , 'invalid age in years' ) validator . add_value_check ( 'date_inclusion' , datetime_string ( '%Y-%m-%d' ) , 'EX7' , 'invalid date' ) def check_age_variables ( r ) : age_years = int ( r [ 'age_years' ] ) age_months = int ( r [ 'age_months' ] ) valid = ( age_months >= age_years * 12 and age_months % age_years < 12 ) if not valid : raise RecordError ( 'EX8' , 'invalid age variables' ) validator . add_record_check ( check_age_variables ) return validator
7335	async def upload_media ( self , file_ , media_type = None , media_category = None , chunked = None , size_limit = None , ** params ) : if isinstance ( file_ , str ) : url = urlparse ( file_ ) if url . scheme . startswith ( 'http' ) : media = await self . _session . get ( file_ ) else : path = urlparse ( file_ ) . path . strip ( " \"'" ) media = await utils . execute ( open ( path , 'rb' ) ) elif hasattr ( file_ , 'read' ) or isinstance ( file_ , bytes ) : media = file_ else : raise TypeError ( "upload_media input must be a file object or a " "filename or binary data or an aiohttp request" ) media_size = await utils . get_size ( media ) if chunked is not None : size_test = False else : size_test = await self . _size_test ( media_size , size_limit ) if isinstance ( media , aiohttp . ClientResponse ) : media = media . content if chunked or ( size_test and chunked is None ) : args = media , media_size , file_ , media_type , media_category response = await self . _chunked_upload ( * args , ** params ) else : response = await self . upload . media . upload . post ( media = media , ** params ) if not hasattr ( file_ , 'read' ) and not getattr ( media , 'closed' , True ) : media . close ( ) return response
1955	def empty_platform ( cls , arch ) : platform = cls ( None ) platform . _init_cpu ( arch ) platform . _init_std_fds ( ) return platform
12270	def read_file ( self , filename ) : try : fh = open ( filename , 'rb' ) table_set = any_tableset ( fh ) except : table_set = None return table_set
10511	def onwindowcreate ( self , window_name , fn_name , * args ) : self . _pollEvents . _callback [ window_name ] = [ "onwindowcreate" , fn_name , args ] return self . _remote_onwindowcreate ( window_name )
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
10418	def get_variants_to_controllers ( graph : BELGraph , node : Protein , modifications : Optional [ Set [ str ] ] = None , ) -> Mapping [ Protein , Set [ Protein ] ] : rv = defaultdict ( set ) variants = variants_of ( graph , node , modifications ) for controller , variant , data in graph . in_edges ( variants , data = True ) : if data [ RELATION ] in CAUSAL_RELATIONS : rv [ variant ] . add ( controller ) return rv
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
11676	def fit ( self , X , y = None ) : self . features_ = as_features ( X , stack = True , bare = True ) return self
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . generation = 1 self . divinity = god
13082	def register_plugins ( self ) : if len ( [ plugin for plugin in self . __plugins__ . values ( ) if plugin . clear_routes ] ) > 0 : self . _urls = list ( ) self . cached = list ( ) clear_assets = [ plugin for plugin in self . __plugins__ . values ( ) if plugin . clear_assets ] if len ( clear_assets ) > 0 and not self . prevent_plugin_clearing_assets : self . __assets__ = copy ( type ( self ) . ASSETS ) static_path = [ plugin . static_folder for plugin in clear_assets if plugin . static_folder ] if len ( static_path ) > 0 : self . static_folder = static_path [ - 1 ] for plugin in self . __plugins__ . values ( ) : self . _urls . extend ( [ ( url , function , methods , plugin ) for url , function , methods in plugin . routes ] ) self . _filters . extend ( [ ( filt , plugin ) for filt in plugin . filters ] ) self . __templates_namespaces__ . extend ( [ ( namespace , directory ) for namespace , directory in plugin . templates . items ( ) ] ) for asset_type in self . __assets__ : for key , value in plugin . assets [ asset_type ] . items ( ) : self . __assets__ [ asset_type ] [ key ] = value if plugin . augment : self . __plugins_render_views__ . append ( plugin ) if hasattr ( plugin , "CACHED" ) : for func in plugin . CACHED : self . cached . append ( ( getattr ( plugin , func ) , plugin ) ) plugin . register_nemo ( self )
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
6718	def virtualenv_exists ( self , virtualenv_dir = None ) : r = self . local_renderer ret = True with self . settings ( warn_only = True ) : ret = r . run_or_local ( 'ls {virtualenv_dir}' ) or '' ret = 'cannot access' not in ret . strip ( ) . lower ( ) if self . verbose : if ret : print ( 'Yes' ) else : print ( 'No' ) return ret
3583	def _print_tree ( self ) : objects = self . _bluez . GetManagedObjects ( ) for path in objects . keys ( ) : print ( "[ %s ]" % ( path ) ) interfaces = objects [ path ] for interface in interfaces . keys ( ) : if interface in [ "org.freedesktop.DBus.Introspectable" , "org.freedesktop.DBus.Properties" ] : continue print ( " %s" % ( interface ) ) properties = interfaces [ interface ] for key in properties . keys ( ) : print ( " %s = %s" % ( key , properties [ key ] ) )
728	def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap
8593	def remove_snapshot ( self , snapshot_id ) : response = self . _perform_request ( url = '/snapshots/' + snapshot_id , method = 'DELETE' ) return response
5717	def push_datapackage ( descriptor , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) tables = [ ] schemas = [ ] datamap = { } mapping = { } model = Package ( descriptor ) plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) for resource in model . resources : if not resource . tabular : continue name = resource . descriptor . get ( 'name' , None ) table = _convert_path ( resource . descriptor [ 'path' ] , name ) schema = resource . descriptor [ 'schema' ] data = resource . table . iter ( keyed = True ) def values ( schema , data ) : for item in data : row = [ ] for field in schema [ 'fields' ] : row . append ( item . get ( field [ 'name' ] , None ) ) yield tuple ( row ) tables . append ( table ) schemas . append ( schema ) datamap [ table ] = values ( schema , data ) if name is not None : mapping [ name ] = table schemas = _convert_schemas ( mapping , schemas ) for table in tables : if table in storage . buckets : storage . delete ( table ) storage . create ( tables , schemas ) for table in storage . buckets : if table in datamap : storage . write ( table , datamap [ table ] ) return storage
6366	def population ( self ) : return self . _tp + self . _tn + self . _fp + self . _fn
5221	def exch_info ( ticker : str ) -> pd . Series : logger = logs . get_logger ( exch_info , level = 'debug' ) if ' ' not in ticker . strip ( ) : ticker = f'XYZ {ticker.strip()} Equity' info = param . load_info ( cat = 'exch' ) . get ( market_info ( ticker = ticker ) . get ( 'exch' , '' ) , dict ( ) ) if ( 'allday' in info ) and ( 'day' not in info ) : info [ 'day' ] = info [ 'allday' ] if any ( req not in info for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return pd . Series ( ) for ss in ValidSessions : if ss not in info : continue info [ ss ] = [ param . to_hour ( num = s ) for s in info [ ss ] ] return pd . Series ( info )
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
953	def closenessScores ( self , expValues , actValues , ** kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )
6201	def simulate_timestamps_mix ( self , max_rates , populations , bg_rate , rs = None , seed = 1 , chunksize = 2 ** 16 , comp_filter = None , overwrite = False , skip_existing = False , scale = 10 , path = None , t_chunksize = None , timeslice = None ) : self . open_store_timestamp ( chunksize = chunksize , path = path ) rs = self . _get_group_randomstate ( rs , seed , self . ts_group ) if t_chunksize is None : t_chunksize = self . emission . chunkshape [ 1 ] timeslice_size = self . n_samples if timeslice is not None : timeslice_size = timeslice // self . t_step name = self . _get_ts_name_mix ( max_rates , populations , bg_rate , rs = rs ) kw = dict ( name = name , clk_p = self . t_step / scale , max_rates = max_rates , bg_rate = bg_rate , populations = populations , num_particles = self . num_particles , bg_particle = self . num_particles , overwrite = overwrite , chunksize = chunksize ) if comp_filter is not None : kw . update ( comp_filter = comp_filter ) try : self . _timestamps , self . _tparticles = ( self . ts_store . add_timestamps ( ** kw ) ) except ExistingArrayError as e : if skip_existing : print ( ' - Skipping already present timestamps array.' ) return else : raise e self . ts_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'PyBroMo' ] = __version__ ts_list , part_list = [ ] , [ ] bg_rates = [ None ] * ( len ( max_rates ) - 1 ) + [ bg_rate ] prev_time = 0 for i_start , i_end in iter_chunk_index ( timeslice_size , t_chunksize ) : curr_time = np . around ( i_start * self . t_step , decimals = 0 ) if curr_time > prev_time : print ( ' %.1fs' % curr_time , end = '' , flush = True ) prev_time = curr_time em_chunk = self . emission [ : , i_start : i_end ] times_chunk_s , par_index_chunk_s = self . _sim_timestamps_populations ( em_chunk , max_rates , populations , bg_rates , i_start , rs , scale ) ts_list . append ( times_chunk_s ) part_list . append ( par_index_chunk_s ) for ts , part in zip ( ts_list , part_list ) : self . _timestamps . append ( ts ) self . _tparticles . append ( part ) self . ts_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'last_random_state' ] = rs . get_state ( ) self . ts_store . h5file . flush ( )
13265	def get_plugin_instance ( plugin_class , * args , ** kwargs ) : assert issubclass ( plugin_class , BasePlugin ) , type ( plugin_class ) global _yaz_plugin_instance_cache qualname = plugin_class . __qualname__ if not qualname in _yaz_plugin_instance_cache : plugin_class = get_plugin_list ( ) [ qualname ] _yaz_plugin_instance_cache [ qualname ] = plugin = plugin_class ( * args , ** kwargs ) funcs = [ func for _ , func in inspect . getmembers ( plugin ) if inspect . ismethod ( func ) and hasattr ( func , "yaz_dependency_config" ) ] for func in funcs : signature = inspect . signature ( func ) assert all ( parameter . kind is parameter . POSITIONAL_OR_KEYWORD and issubclass ( parameter . annotation , BasePlugin ) for parameter in signature . parameters . values ( ) ) , "All parameters for {} must type hint to a BasePlugin" . format ( func ) func ( * [ get_plugin_instance ( parameter . annotation ) for parameter in signature . parameters . values ( ) ] ) return _yaz_plugin_instance_cache [ qualname ]
2421	def checksum_from_sha1 ( value ) : CHECKSUM_RE = re . compile ( 'SHA1:\s*([\S]+)' , re . UNICODE ) match = CHECKSUM_RE . match ( value ) if match : return checksum . Algorithm ( identifier = 'SHA1' , value = match . group ( 1 ) ) else : return None
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , ** kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
2718	def remove_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __remove_resources ( resources ) return False
10720	def get_parser ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( "package" , choices = arg_map . keys ( ) , help = "designates the package to test" ) parser . add_argument ( "--ignore" , help = "ignore these files" ) return parser
9722	async def load ( self , filename ) : cmd = "load %s" % filename return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
7431	def _count_PIS ( seqsamp , N ) : counts = [ Counter ( col ) for col in seqsamp . T if not ( "-" in col or "N" in col ) ] pis = [ i . most_common ( 2 ) [ 1 ] [ 1 ] > 1 for i in counts if len ( i . most_common ( 2 ) ) > 1 ] if sum ( pis ) >= N : return sum ( pis ) else : return 0
1935	def get_source_for ( self , asm_offset , runtime = True ) : srcmap = self . get_srcmap ( runtime ) try : beg , size , _ , _ = srcmap [ asm_offset ] except KeyError : return '' output = '' nl = self . source_code [ : beg ] . count ( '\n' ) + 1 snippet = self . source_code [ beg : beg + size ] for l in snippet . split ( '\n' ) : output += ' %s %s\n' % ( nl , l ) nl += 1 return output
8359	def scale_context_and_center ( self , cr ) : bot_width , bot_height = self . bot_size if self . width != bot_width or self . height != bot_height : if self . width < self . height : scale_x = float ( self . width ) / float ( bot_width ) scale_y = scale_x cr . translate ( 0 , ( self . height - ( bot_height * scale_y ) ) / 2.0 ) elif self . width > self . height : scale_y = float ( self . height ) / float ( bot_height ) scale_x = scale_y cr . translate ( ( self . width - ( bot_width * scale_x ) ) / 2.0 , 0 ) else : scale_x = 1.0 scale_y = 1.0 cr . scale ( scale_x , scale_y ) self . input_device . scale_x = scale_y self . input_device . scale_y = scale_y
10110	def write ( self , _force = False , _exists_ok = False , ** items ) : if self . fname and self . fname . exists ( ) : raise ValueError ( 'db file already exists, use force=True to overwrite' ) with self . connection ( ) as db : for table in self . tables : db . execute ( table . sql ( translate = self . translate ) ) db . execute ( 'PRAGMA foreign_keys = ON;' ) db . commit ( ) refs = defaultdict ( list ) for t in self . tables : if t . name not in items : continue rows , keys = [ ] , [ ] cols = { c . name : c for c in t . columns } for i , row in enumerate ( items [ t . name ] ) : pk = row [ t . primary_key [ 0 ] ] if t . primary_key and len ( t . primary_key ) == 1 else None values = [ ] for k , v in row . items ( ) : if k in t . many_to_many : assert pk at = t . many_to_many [ k ] atkey = tuple ( [ at . name ] + [ c . name for c in at . columns ] ) for vv in v : fkey , context = self . association_table_context ( t , k , vv ) refs [ atkey ] . append ( ( pk , fkey , context ) ) else : col = cols [ k ] if isinstance ( v , list ) : v = ( col . separator or ';' ) . join ( col . convert ( vv ) for vv in v ) else : v = col . convert ( v ) if v is not None else None if i == 0 : keys . append ( col . name ) values . append ( v ) rows . append ( tuple ( values ) ) insert ( db , self . translate , t . name , keys , * rows ) for atkey , rows in refs . items ( ) : insert ( db , self . translate , atkey [ 0 ] , atkey [ 1 : ] , * rows ) db . commit ( )
4927	def transform_description ( self , content_metadata_item ) : description_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : description_with_locales . append ( { 'locale' : locale , 'value' : ( content_metadata_item . get ( 'full_description' ) or content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' , '' ) ) } ) return description_with_locales
12337	def pip_r ( self , requirements , raise_on_error = True ) : cmd = "pip install -r %s" % requirements return self . wait ( cmd , raise_on_error = raise_on_error )
9025	def kivy_svg ( self ) : from kivy . graphics . svg import Svg path = self . temporary_path ( ".svg" ) try : return Svg ( path ) finally : remove_file ( path )
4810	def train_model ( best_processed_path , weight_path = '../weight/model_weight.h5' , verbose = 2 ) : x_train_char , x_train_type , y_train = prepare_feature ( best_processed_path , option = 'train' ) x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) validation_set = False if os . path . isdir ( os . path . join ( best_processed_path , 'val' ) ) : validation_set = True x_val_char , x_val_type , y_val = prepare_feature ( best_processed_path , option = 'val' ) if not os . path . isdir ( os . path . dirname ( weight_path ) ) : os . makedirs ( os . path . dirname ( weight_path ) ) callbacks_list = [ ReduceLROnPlateau ( ) , ModelCheckpoint ( weight_path , save_best_only = True , save_weights_only = True , monitor = 'val_loss' , mode = 'min' , verbose = 1 ) ] model = get_convo_nn2 ( ) train_params = [ ( 10 , 256 ) , ( 3 , 512 ) , ( 3 , 2048 ) , ( 3 , 4096 ) , ( 3 , 8192 ) ] for ( epochs , batch_size ) in train_params : print ( "train with {} epochs and {} batch size" . format ( epochs , batch_size ) ) if validation_set : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list , validation_data = ( [ x_val_char , x_val_type ] , y_val ) ) else : model . fit ( [ x_train_char , x_train_type ] , y_train , epochs = epochs , batch_size = batch_size , verbose = verbose , callbacks = callbacks_list ) return model
8608	def remove_group_user ( self , group_id , user_id ) : response = self . _perform_request ( url = '/um/groups/%s/users/%s' % ( group_id , user_id ) , method = 'DELETE' ) return response
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
7238	def iterwindows ( self , count = 64 , window_shape = ( 256 , 256 ) ) : if count is None : while True : yield self . randwindow ( window_shape ) else : for i in xrange ( count ) : yield self . randwindow ( window_shape )
10104	def execute ( self , timeout = None ) : logger . debug ( ' > Batch API request (length %s)' % len ( self . _commands ) ) auth = self . _build_http_auth ( ) headers = self . _build_request_headers ( ) logger . debug ( '\tbatch headers: %s' % headers ) logger . debug ( '\tbatch command length: %s' % len ( self . _commands ) ) path = self . _build_request_path ( self . BATCH_ENDPOINT ) data = json . dumps ( self . _commands , cls = self . _json_encoder ) r = requests . post ( path , auth = auth , headers = headers , data = data , timeout = ( self . DEFAULT_TIMEOUT if timeout is None else timeout ) ) self . _commands = [ ] logger . debug ( '\tresponse code:%s' % r . status_code ) try : logger . debug ( '\tresponse: %s' % r . json ( ) ) except : logger . debug ( '\tresponse: %s' % r . content ) return r
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
10347	def run_rcr ( graph , tag = 'dgxp' ) : hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) correct = defaultdict ( int ) contra = defaultdict ( int ) ambiguous = defaultdict ( int ) missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
3458	def main ( argv ) : source , target , tag = argv if "a" in tag : bump = "alpha" if "b" in tag : bump = "beta" else : bump = find_bump ( target , tag ) filename = "{}.md" . format ( tag ) destination = copy ( join ( source , filename ) , target ) build_hugo_md ( destination , tag , bump )
7961	def disconnect ( self ) : logger . debug ( "TCPTransport.disconnect()" ) with self . lock : if self . _socket is None : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" ) return if self . _hup or not self . _serializer : self . _close ( ) else : self . send_stream_tail ( )
12366	def update ( self , id , name ) : return super ( Keys , self ) . update ( id , name = name )
8710	def __write_chunk ( self , chunk ) : log . debug ( 'writing %d bytes chunk' , len ( chunk ) ) data = BLOCK_START + chr ( len ( chunk ) ) + chunk if len ( chunk ) < 128 : padding = 128 - len ( chunk ) log . debug ( 'pad with %d characters' , padding ) data = data + ( ' ' * padding ) log . debug ( "packet size %d" , len ( data ) ) self . __write ( data ) self . _port . flush ( ) return self . __got_ack ( )
8541	def _save_config ( self , filename = None ) : if filename is None : filename = self . _config_filename parent_path = os . path . dirname ( filename ) if not os . path . isdir ( parent_path ) : os . makedirs ( parent_path ) with open ( filename , "w" ) as configfile : self . _config . write ( configfile )
4332	def noiseprof ( self , input_filepath , profile_path ) : if os . path . isdir ( profile_path ) : raise ValueError ( "profile_path {} is a directory." . format ( profile_path ) ) if os . path . dirname ( profile_path ) == '' and profile_path != '' : _abs_profile_path = os . path . join ( os . getcwd ( ) , profile_path ) else : _abs_profile_path = profile_path if not os . access ( os . path . dirname ( _abs_profile_path ) , os . W_OK ) : raise IOError ( "profile_path {} is not writeable." . format ( _abs_profile_path ) ) effect_args = [ 'noiseprof' , profile_path ] self . build ( input_filepath , None , extra_args = effect_args ) return None
8600	def add_share ( self , group_id , resource_id , ** kwargs ) : properties = { } for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
7953	def handle_write ( self ) : with self . lock : logger . debug ( "handle_write: queue: {0!r}" . format ( self . _write_queue ) ) try : job = self . _write_queue . popleft ( ) except IndexError : return if isinstance ( job , WriteData ) : self . _do_write ( job . data ) elif isinstance ( job , ContinueConnect ) : self . _continue_connect ( ) elif isinstance ( job , StartTLS ) : self . _initiate_starttls ( ** job . kwargs ) elif isinstance ( job , TLSHandshake ) : self . _continue_tls_handshake ( ) else : raise ValueError ( "Unrecognized job in the write queue: " "{0!r}" . format ( job ) )
2951	def _on_trigger ( self , my_task ) : for task in my_task . workflow . task_tree . _find_any ( self ) : if task . thread_id != my_task . thread_id : continue self . _do_join ( task )
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
3862	def _get_default_delivery_medium ( self ) : medium_options = ( self . _conversation . self_conversation_state . delivery_medium_option ) try : default_medium = medium_options [ 0 ] . delivery_medium except IndexError : logger . warning ( 'Conversation %r has no delivery medium' , self . id_ ) default_medium = hangouts_pb2 . DeliveryMedium ( medium_type = hangouts_pb2 . DELIVERY_MEDIUM_BABEL ) for medium_option in medium_options : if medium_option . current_default : default_medium = medium_option . delivery_medium return default_medium
1012	def _cleanUpdatesList ( self , col , cellIdx , seg ) : for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )
10382	def multi_run_epicom ( graphs : Iterable [ BELGraph ] , path : Union [ None , str , TextIO ] ) -> None : if isinstance ( path , str ) : with open ( path , 'w' ) as file : _multi_run_helper_file_wrapper ( graphs , file ) else : _multi_run_helper_file_wrapper ( graphs , path )
3577	def disconnect_devices ( self , service_uuids ) : cbuuids = map ( uuid_to_cbuuid , service_uuids ) for device in self . _central_manager . retrieveConnectedPeripheralsWithServices_ ( cbuuids ) : self . _central_manager . cancelPeripheralConnection_ ( device )
2716	def __extract_resources_from_droplets ( self , data ) : resources = [ ] if not isinstance ( data , list ) : return data for a_droplet in data : res = { } try : if isinstance ( a_droplet , unicode ) : res = { "resource_id" : a_droplet , "resource_type" : "droplet" } except NameError : pass if isinstance ( a_droplet , str ) or isinstance ( a_droplet , int ) : res = { "resource_id" : str ( a_droplet ) , "resource_type" : "droplet" } elif isinstance ( a_droplet , Droplet ) : res = { "resource_id" : str ( a_droplet . id ) , "resource_type" : "droplet" } if len ( res ) > 0 : resources . append ( res ) return resources
3438	def merge ( self , right , prefix_existing = None , inplace = True , objective = 'left' ) : if inplace : new_model = self else : new_model = self . copy ( ) new_model . id = '{}_{}' . format ( self . id , right . id ) new_reactions = deepcopy ( right . reactions ) if prefix_existing is not None : existing = new_reactions . query ( lambda rxn : rxn . id in self . reactions ) for reaction in existing : reaction . id = '{}{}' . format ( prefix_existing , reaction . id ) new_model . add_reactions ( new_reactions ) interface = new_model . problem new_vars = [ interface . Variable . clone ( v ) for v in right . variables if v . name not in new_model . variables ] new_model . add_cons_vars ( new_vars ) new_cons = [ interface . Constraint . clone ( c , model = new_model . solver ) for c in right . constraints if c . name not in new_model . constraints ] new_model . add_cons_vars ( new_cons , sloppy = True ) new_model . objective = dict ( left = self . objective , right = right . objective , sum = self . objective . expression + right . objective . expression ) [ objective ] return new_model
5933	def besttype ( x ) : x = to_unicode ( x ) try : x = x . strip ( ) except AttributeError : pass m = re . match ( r , x ) if m is None : for converter in int , float , to_unicode : try : return converter ( x ) except ValueError : pass else : x = to_unicode ( m . group ( 'value' ) ) return x
12156	def list_move_to_back ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . append ( value ) return l
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
4983	def get ( self , request , enterprise_uuid , course_id ) : embargo_url = EmbargoApiClient . redirect_if_blocked ( [ course_id ] , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) enterprise_customer , course , course_run , modes = self . get_base_details ( request , enterprise_uuid , course_id ) enterprise_customer_user = get_enterprise_customer_user ( request . user . id , enterprise_uuid ) data_sharing_consent = DataSharingConsent . objects . proxied_get ( username = enterprise_customer_user . username , course_id = course_id , enterprise_customer = enterprise_customer ) enrollment_client = EnrollmentApiClient ( ) enrolled_course = enrollment_client . get_course_enrollment ( request . user . username , course_id ) try : enterprise_course_enrollment = EnterpriseCourseEnrollment . objects . get ( enterprise_customer_user__enterprise_customer = enterprise_customer , enterprise_customer_user__user_id = request . user . id , course_id = course_id ) except EnterpriseCourseEnrollment . DoesNotExist : enterprise_course_enrollment = None if enrolled_course and enterprise_course_enrollment : return redirect ( LMS_COURSEWARE_URL . format ( course_id = course_id ) ) return self . get_enterprise_course_enrollment_page ( request , enterprise_customer , course , course_run , modes , enterprise_course_enrollment , data_sharing_consent , )
13546	def get_users ( self , params = { } ) : param_list = [ ( k , params [ k ] ) for k in sorted ( params ) ] url = "/2/users/?%s" % urlencode ( param_list ) data = self . _get_resource ( url ) users = [ ] for entry in data [ "users" ] : users . append ( self . user_from_json ( entry ) ) return users
3184	def create ( self , store_id , product_id , data ) : self . store_id = store_id self . product_id = product_id if 'id' not in data : raise KeyError ( 'The product image must have an id' ) if 'title' not in data : raise KeyError ( 'The product image must have a url' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'products' , product_id , 'images' ) , data = data ) if response is not None : self . image_id = response [ 'id' ] else : self . image_id = None return response
2304	def _run_gies ( self , data , fixedGaps = None , verbose = True ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_gies' + id + '/' ) self . arguments [ '{FOLDER}' ] = '/tmp/cdt_gies' + id + '/' def retrieve_result ( ) : return read_csv ( '/tmp/cdt_gies' + id + '/result.csv' , delimiter = ',' ) . values try : data . to_csv ( '/tmp/cdt_gies' + id + '/data.csv' , header = False , index = False ) if fixedGaps is not None : fixedGaps . to_csv ( '/tmp/cdt_gies' + id + '/fixedgaps.csv' , index = False , header = False ) self . arguments [ '{SKELETON}' ] = 'TRUE' else : self . arguments [ '{SKELETON}' ] = 'FALSE' gies_result = launch_R_script ( "{}/R_templates/gies.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , self . arguments , output_function = retrieve_result , verbose = verbose ) except Exception as e : rmtree ( '/tmp/cdt_gies' + id + '' ) raise e except KeyboardInterrupt : rmtree ( '/tmp/cdt_gies' + id + '/' ) raise KeyboardInterrupt rmtree ( '/tmp/cdt_gies' + id + '' ) return gies_result
11265	def readline ( prev , filename = None , mode = 'r' , trim = str . rstrip , start = 1 , end = sys . maxsize ) : if prev is None : if filename is None : raise Exception ( 'No input available for readline.' ) elif is_str_type ( filename ) : file_list = [ filename , ] else : file_list = filename else : file_list = prev for fn in file_list : if isinstance ( fn , file_type ) : fd = fn else : fd = open ( fn , mode ) try : if start <= 1 and end == sys . maxsize : for line in fd : yield trim ( line ) else : for line_no , line in enumerate ( fd , 1 ) : if line_no < start : continue yield trim ( line ) if line_no >= end : break finally : if fd != fn : fd . close ( )
6515	def output ( self , msg , newline = True ) : click . echo ( text_type ( msg ) , nl = newline , file = self . output_file )
9857	def get_devices ( self ) : retn = [ ] api_devices = self . api_call ( 'devices' ) self . log ( 'DEVICES:' ) self . log ( api_devices ) for device in api_devices : retn . append ( AmbientWeatherStation ( self , device ) ) self . log ( 'DEVICE INSTANCE LIST:' ) self . log ( retn ) return retn
11730	def pvpc_calc_tcu_cp_feu_d ( df , verbose = True , convert_kwh = True ) : if 'TCU' + TARIFAS [ 0 ] not in df . columns : if convert_kwh : cols_mwh = [ c + t for c in COLS_PVPC for t in TARIFAS if c != 'COF' ] df [ cols_mwh ] = df [ cols_mwh ] . applymap ( lambda x : x / 1000. ) gb_t = df . groupby ( lambda x : TARIFAS [ np . argmax ( [ t in x for t in TARIFAS ] ) ] , axis = 1 ) for k , g in gb_t : if verbose : print ( 'TARIFA {}' . format ( k ) ) print ( g . head ( ) ) df [ 'TCU{}' . format ( k ) ] = g [ k ] - g [ 'TEU{}' . format ( k ) ] cols_cp = [ c + k for c in COLS_PVPC if c not in [ '' , 'COF' , 'TEU' ] ] df [ 'CP{}' . format ( k ) ] = g [ cols_cp ] . sum ( axis = 1 ) cols_k = [ 'TEU' + k , 'TCU' + k , 'COF' + k ] g = df [ cols_k ] . groupby ( 'TEU' + k ) pr = g . apply ( lambda x : x [ 'TCU' + k ] . dot ( x [ 'COF' + k ] ) / x [ 'COF' + k ] . sum ( ) ) pr . name = 'PD_' + k df = df . join ( pr , on = 'TEU' + k , rsuffix = '_r' ) df [ 'PD_' + k ] += df [ 'TEU' + k ] return df
4956	def get_object ( self , name , description ) : return Activity ( id = X_API_ACTIVITY_COURSE , definition = ActivityDefinition ( name = LanguageMap ( { 'en-US' : ( name or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , description = LanguageMap ( { 'en-US' : ( description or '' ) . encode ( "ascii" , "ignore" ) . decode ( 'ascii' ) } ) , ) , )
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
1003	def _inferPhase1 ( self , activeColumns , useStartCells ) : self . infActiveState [ 't' ] . fill ( 0 ) numPredictedColumns = 0 if useStartCells : for c in activeColumns : self . infActiveState [ 't' ] [ c , 0 ] = 1 else : for c in activeColumns : predictingCells = numpy . where ( self . infPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictingCells = len ( predictingCells ) if numPredictingCells > 0 : self . infActiveState [ 't' ] [ c , predictingCells ] = 1 numPredictedColumns += 1 else : self . infActiveState [ 't' ] [ c , : ] = 1 if useStartCells or numPredictedColumns >= 0.50 * len ( activeColumns ) : return True else : return False
7549	def _set_debug_dict ( __loglevel__ ) : _lconfig . dictConfig ( { 'version' : 1 , 'disable_existing_loggers' : False , 'formatters' : { 'standard' : { 'format' : "%(asctime)s \t" + "pid=%(process)d \t" + "[%(filename)s]\t" + "%(levelname)s \t" + "%(message)s" } , } , 'handlers' : { __name__ : { 'level' : __loglevel__ , 'class' : 'logging.FileHandler' , 'filename' : __debugfile__ , 'formatter' : "standard" , 'mode' : 'a+' } } , 'loggers' : { __name__ : { 'handlers' : [ __name__ ] , 'level' : __loglevel__ , 'propogate' : True } } } )
1620	def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
1444	def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
12395	def register ( self , method , args , kwargs ) : invoc = self . dump_invoc ( * args , ** kwargs ) self . registry . append ( ( invoc , method . __name__ ) )
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
11253	def attr ( prev , attr_name ) : for obj in prev : if hasattr ( obj , attr_name ) : yield getattr ( obj , attr_name )
6037	def scaled_array_2d_from_array_1d ( self , array_1d ) : return scaled_array . ScaledSquarePixelArray ( array = self . array_2d_from_array_1d ( array_1d ) , pixel_scale = self . mask . pixel_scale , origin = self . mask . origin )
7529	def setup_dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ "project_dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{}_clust_{}" . format ( data . name , data . paramsdict [ "clust_threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) if not data . paramsdict [ "assembly_method" ] == "denovo" : data . dirs . refmapping = os . path . join ( pdir , "{}_refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
13385	def store_env ( path = None ) : path = path or get_store_env_tmp ( ) env_dict = yaml . safe_dump ( os . environ . data , default_flow_style = False ) with open ( path , 'w' ) as f : f . write ( env_dict ) return path
10839	def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post_data = "text=%s&" % text if now : post_data += "now=%s&" % now if utc : post_data += "utc=%s&" % utc if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) return Update ( api = self . api , raw_response = response [ 'update' ] )
805	def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( ** self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
11691	def filter ( self ) : self . content = [ ch for ch in self . xml . getchildren ( ) if get_bounds ( ch ) . intersects ( self . area ) ]
5799	def walk_ast ( node , code_lines , sections , md_chunks ) : if isinstance ( node , _ast . FunctionDef ) : key = ( 'function' , node . name ) if key not in sections : return docstring = ast . get_docstring ( node ) def_lineno = node . lineno + len ( node . decorator_list ) definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> ' ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , definition , description_md ) + "\n" md_chunks [ key ] = md_chunk . replace ( '>\n\n' , '' ) elif isinstance ( node , _ast . ClassDef ) : if ( 'class' , node . name ) not in sections : return for subnode in node . body : if isinstance ( subnode , _ast . FunctionDef ) : node_id = node . name + '.' + subnode . name method_key = ( 'method' , node_id ) is_method = method_key in sections attribute_key = ( 'attribute' , node_id ) is_attribute = attribute_key in sections is_constructor = subnode . name == '__init__' if not is_constructor and not is_attribute and not is_method : continue docstring = ast . get_docstring ( subnode ) def_lineno = subnode . lineno + len ( subnode . decorator_list ) if not docstring : continue if is_method or is_constructor : definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> > ' ) if is_constructor : key = ( 'class' , node . name ) class_docstring = ast . get_docstring ( node ) or '' class_description = textwrap . dedent ( class_docstring ) . strip ( ) if class_description : class_description_md = "> %s\n>" % ( class_description . replace ( "\n" , "\n> " ) ) else : class_description_md = '' md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , class_description_md , definition , description_md ) md_chunk = md_chunk . replace ( '\n\n\n' , '\n\n' ) else : key = method_key md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , definition , description_md ) if md_chunk [ - 5 : ] == '\n> >\n' : md_chunk = md_chunk [ 0 : - 5 ] else : key = attribute_key description = textwrap . dedent ( docstring ) . strip ( ) description_md = "> > %s" % ( description . replace ( "\n" , "\n> > " ) ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , description_md ) md_chunks [ key ] = re . sub ( '[ \\t]+\n' , '\n' , md_chunk . rstrip ( ) ) elif isinstance ( node , _ast . If ) : for subast in node . body : walk_ast ( subast , code_lines , sections , md_chunks ) for subast in node . orelse : walk_ast ( subast , code_lines , sections , md_chunks )
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
9405	def _get_user_class ( self , name ) : self . _user_classes . setdefault ( name , _make_user_class ( self , name ) ) return self . _user_classes [ name ]
8834	def less ( a , b , * args ) : types = set ( [ type ( a ) , type ( b ) ] ) if float in types or int in types : try : a , b = float ( a ) , float ( b ) except TypeError : return False return a < b and ( not args or less ( b , * args ) )
7801	def handle_authorized ( self , event ) : stream = event . stream if not stream : return if not stream . initiator : return if stream . features is None : return element = stream . features . find ( SESSION_TAG ) if element is None : return logger . debug ( "Establishing IM session" ) stanza = Iq ( stanza_type = "set" ) payload = XMLPayload ( ElementTree . Element ( SESSION_TAG ) ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _session_success , self . _session_error ) stream . send ( stanza )
7177	def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as src_buffer : src_encoding = src_buffer . encoding src_node = lib2to3_parse ( src_buffer . read ( ) ) try : with open ( ( pyi_dir / src . name ) . with_suffix ( '.pyi' ) ) as pyi_file : pyi_txt = pyi_file . read ( ) except FileNotFoundError : if not quiet : print ( f'warning: .pyi file for source {src} not found in {pyi_dir}' , file = sys . stderr , ) else : pyi_ast = ast3 . parse ( pyi_txt ) assert isinstance ( pyi_ast , ast3 . Module ) reapply_all ( pyi_ast . body , src_node ) fix_remaining_type_comments ( src_node ) targets . mkdir ( parents = True , exist_ok = True ) with open ( targets / src . name , 'w' , encoding = src_encoding ) as target_file : target_file . write ( lib2to3_unparse ( src_node , hg = hg ) ) return targets / src . name
11555	def enable_analog_reporting ( self , pin ) : command = [ self . _command_handler . REPORT_ANALOG + pin , self . REPORTING_ENABLE ] self . _command_handler . send_command ( command )
13724	def log_post ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do_verify_certificate is None : do_verify_certificate = self . do_verify_certificate if credentials and "base64" in credentials : headers = { "Content-Type" : "application/json" , 'Authorization' : 'Basic %s' % credentials [ "base64" ] } else : headers = { "Content-Type" : "application/json" } try : request = requests . post ( url , headers = headers , data = self . store . get_json ( ) , verify = do_verify_certificate ) except httplib . IncompleteRead as e : request = e . partial
6740	def get_packager ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
11480	def _upload_as_item ( local_file , parent_folder_id , file_path , reuse_existing = False ) : current_item_id = _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing ) _create_bitstream ( file_path , local_file , current_item_id ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , current_item_id )
9543	def add_record_length_check ( self , code = RECORD_LENGTH_CHECK_FAILED , message = MESSAGES [ RECORD_LENGTH_CHECK_FAILED ] , modulus = 1 ) : t = code , message , modulus self . _record_length_checks . append ( t )
9852	def _export_dx ( self , filename , type = None , typequote = '"' , ** kwargs ) : root , ext = os . path . splitext ( filename ) filename = root + '.dx' comments = [ 'OpenDX density file written by gridDataFormats.Grid.export()' , 'File format: http://opendx.sdsc.edu/docs/html/pages/usrgu068.htm#HDREDF' , 'Data are embedded in the header and tied to the grid positions.' , 'Data is written in C array order: In grid[x,y,z] the axis z is fastest' , 'varying, then y, then finally x, i.e. z is the innermost loop.' ] if self . metadata : comments . append ( 'Meta data stored with the python Grid object:' ) for k in self . metadata : comments . append ( ' ' + str ( k ) + ' = ' + str ( self . metadata [ k ] ) ) comments . append ( '(Note: the VMD dx-reader chokes on comments below this line)' ) components = dict ( positions = OpenDX . gridpositions ( 1 , self . grid . shape , self . origin , self . delta ) , connections = OpenDX . gridconnections ( 2 , self . grid . shape ) , data = OpenDX . array ( 3 , self . grid , type = type , typequote = typequote ) , ) dx = OpenDX . field ( 'density' , components = components , comments = comments ) dx . write ( filename )
5323	def _interrupt_read ( self ) : data = self . _device . read ( ENDPOINT , REQ_INT_LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
2942	def _add_notify ( self , task_spec ) : if task_spec . name in self . task_specs : raise KeyError ( 'Duplicate task spec name: ' + task_spec . name ) self . task_specs [ task_spec . name ] = task_spec task_spec . id = len ( self . task_specs )
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
13840	def ConsumeBool ( self ) : try : result = ParseBool ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
4302	def create_user ( config_data ) : with chdir ( os . path . abspath ( config_data . project_directory ) ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) subprocess . check_call ( [ sys . executable , 'create_user.py' ] , env = env , stderr = subprocess . STDOUT ) for ext in [ 'py' , 'pyc' ] : try : os . remove ( 'create_user.{0}' . format ( ext ) ) except OSError : pass
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
6174	def reset_lock ( self ) : redis_key = self . CELERY_LOCK . format ( task_id = self . task_identifier ) self . celery_self . backend . client . delete ( redis_key )
7708	def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( "Checking roster-related features" ) if event . features . find ( FEATURE_ROSTERVER ) is not None : logger . debug ( " Roster versioning available" ) server_features . add ( "versioning" ) if event . features . find ( FEATURE_APPROVALS ) is not None : logger . debug ( " Subscription pre-approvals available" ) server_features . add ( "pre-approvals" ) self . server_features = server_features
6789	def get_settings ( self , site = None , role = None ) : r = self . local_renderer _stdout = sys . stdout _stderr = sys . stderr if not self . verbose : sys . stdout = StringIO ( ) sys . stderr = StringIO ( ) try : sys . path . insert ( 0 , r . env . src_dir ) tmp_site = self . genv . SITE if site and site . endswith ( '_secure' ) : site = site [ : - 7 ] site = site or self . genv . SITE or self . genv . default_site self . set_site ( site ) tmp_role = self . genv . ROLE if role : self . set_role ( role ) try : if r . env . delete_module_with_prefixes : for name in sorted ( sys . modules ) : for prefix in r . env . delete_module_with_prefixes : if name . startswith ( prefix ) : if self . verbose : print ( 'Deleting module %s prior to re-import.' % name ) del sys . modules [ name ] break for name in list ( sys . modules ) : for s in r . env . delete_module_containing : if s in name : del sys . modules [ name ] break if r . env . settings_module in sys . modules : del sys . modules [ r . env . settings_module ] if 'django_settings_module' in r . genv : r . env . settings_module = r . genv . django_settings_module else : r . env . settings_module = r . env . settings_module or r . genv . dj_settings_module if self . verbose : print ( 'r.env.settings_module:' , r . env . settings_module , r . format ( r . env . settings_module ) ) module = import_module ( r . format ( r . env . settings_module ) ) if site : assert site == module . SITE , 'Unable to set SITE to "%s" Instead it is set to "%s".' % ( site , module . SITE ) import imp imp . reload ( module ) except ImportError as e : print ( 'Warning: Could not import settings for site "%s": %s' % ( site , e ) , file = _stdout ) traceback . print_exc ( file = _stdout ) return finally : if tmp_site : self . set_site ( tmp_site ) if tmp_role : self . set_role ( tmp_role ) finally : sys . stdout = _stdout sys . stderr = _stderr sys . path . remove ( r . env . src_dir ) return module
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
3540	def status_printer ( ) : last_len = [ 0 ] def p ( s ) : s = next ( spinner ) + ' ' + s len_s = len ( s ) output = '\r' + s + ( ' ' * max ( last_len [ 0 ] - len_s , 0 ) ) sys . stdout . write ( output ) sys . stdout . flush ( ) last_len [ 0 ] = len_s return p
6344	def idf ( self , term , transform = None ) : r docs_with_term = 0 docs = self . docs_of_words ( ) for doc in docs : doc_set = set ( doc ) if transform : transformed_doc = [ ] for word in doc_set : transformed_doc . append ( transform ( word ) ) doc_set = set ( transformed_doc ) if term in doc_set : docs_with_term += 1 if docs_with_term == 0 : return float ( 'inf' ) return log10 ( len ( docs ) / docs_with_term )
5965	def make_main_index ( struct , selection = '"Protein"' , ndx = 'main.ndx' , oldndx = None ) : logger . info ( "Building the main index file {ndx!r}..." . format ( ** vars ( ) ) ) _ , out , _ = gromacs . make_ndx ( f = struct , n = oldndx , o = ndx , stdout = False , input = ( "" , "q" ) ) groups = cbook . parse_ndxlist ( out ) selection = selection . strip ( "\"" ) selected_groups = [ g for g in groups if g [ 'name' ] . lower ( ) == selection . lower ( ) ] if len ( selected_groups ) > 1 : logging . warn ( "make_ndx created duplicated groups, performing work around" ) if len ( selected_groups ) <= 0 : msg = "no groups found for selection {0}, available groups are {1}" . format ( selection , groups ) logging . error ( msg ) raise ValueError ( msg ) last = len ( groups ) - 1 assert last == groups [ - 1 ] [ 'nr' ] group = selected_groups [ 0 ] _ , out , _ = gromacs . make_ndx ( f = struct , n = ndx , o = ndx , stdout = False , input = ( "{0}" . format ( group [ 'nr' ] ) , "name {0} __main__" . format ( last + 1 ) , "! \"__main__\"" , "name {0} __environment__" . format ( last + 2 ) , "" , "q" ) ) return cbook . parse_ndxlist ( out )
4658	def as_base ( self , base ) : if base == self [ "base" ] [ "symbol" ] : return self . copy ( ) elif base == self [ "quote" ] [ "symbol" ] : return self . copy ( ) . invert ( ) else : raise InvalidAssetException
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) metadata = re . sub ( r'^\d+\.+' , '' , metadata ) metadata = re . sub ( r'[^\w\s]' , '' , metadata ) metadata = re . sub ( r'\s+' , ' ' , metadata ) metadata = re . sub ( r'^\s+' , '' , metadata ) metadata = re . sub ( r'\s+$' , '' , metadata ) metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) return metadata
966	def percentOverlap ( x1 , x2 , size ) : nonZeroX1 = np . count_nonzero ( x1 ) nonZeroX2 = np . count_nonzero ( x2 ) minX1X2 = min ( nonZeroX1 , nonZeroX2 ) percentOverlap = 0 if minX1X2 > 0 : percentOverlap = float ( np . dot ( x1 , x2 ) ) / float ( minX1X2 ) return percentOverlap
2526	def get_annotation_type ( self , r_term ) : for _ , _ , typ in self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationType' ] , None ) ) : if typ is not None : return typ else : self . error = True msg = 'Annotation must have exactly one annotation type.' self . logger . log ( msg ) return
12042	def XMLtoPython ( xmlStr = r"C:\Apps\pythonModules\GSTemp.xml" ) : if os . path . exists ( xmlStr ) : with open ( xmlStr ) as f : xmlStr = f . read ( ) print ( xmlStr ) print ( "DONE" ) return
13018	def configure ( self , argv = None ) : self . _setupOptions ( ) self . _parseOptions ( argv ) self . _setupLogging ( ) self . _setupModel ( ) self . dbsession . commit ( ) return self
12478	def get_sys_path ( rcpath , app_name , section_name = None ) : if op . exists ( rcpath ) : return op . realpath ( op . expanduser ( rcpath ) ) try : settings = rcfile ( app_name , section_name ) except : raise try : sys_path = op . expanduser ( settings [ rcpath ] ) except KeyError : raise IOError ( 'Could not find an existing variable with name {0} in' ' section {1} of {2}rc config setup. Maybe it is a ' ' folder that could not be found.' . format ( rcpath , section_name , app_name ) ) else : if not op . exists ( sys_path ) : raise IOError ( 'Could not find the path {3} indicated by the ' 'variable {0} in section {1} of {2}rc config ' 'setup.' . format ( rcpath , section_name , app_name , sys_path ) ) return op . realpath ( op . expanduser ( sys_path ) )
1680	def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
8646	def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
13499	def parse ( s ) : try : m = _regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with_revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except AttributeError : return None
11613	def report_depths ( self , filename , tpm = True , grp_wise = False , reorder = 'as-is' , notes = None ) : if grp_wise : lname = self . probability . gname depths = self . allelic_expression * self . grp_conv_mat else : lname = self . probability . lname depths = self . allelic_expression if tpm : depths *= ( 1000000.0 / depths . sum ( ) ) total_depths = depths . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_depths . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_depths . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( depths , total_depths ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
10453	def waittillguinotexist ( self , window_name , object_name = '' , guiTimeOut = 30 ) : timeout = 0 while timeout < guiTimeOut : if not self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
1189	def _randbelow ( self , n ) : k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
7858	def make_error_response ( self , cond ) : if self . stanza_type in ( "result" , "error" ) : raise ValueError ( "Errors may not be generated for" " 'result' and 'error' iq" ) stanza = Iq ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : Stanza . add_payload ( stanza , payload ) return stanza
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
780	def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID
2113	def modify ( self , pk = None , create_on_missing = False , ** kwargs ) : if 'job_timeout' in kwargs and 'timeout' not in kwargs : kwargs [ 'timeout' ] = kwargs . pop ( 'job_timeout' ) return super ( Resource , self ) . write ( pk , create_on_missing = create_on_missing , force_on_exists = True , ** kwargs )
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
13706	def iter_space_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word codelen = sum ( len ( s ) for s in get_codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : yield fmtfunc ( curline ) curline = word else : curline = possibleline if curline : yield fmtfunc ( curline )
1492	def register_timer_task_in_sec ( self , task , second ) : second_in_float = float ( second ) expiration = time . time ( ) + second_in_float heappush ( self . timer_tasks , ( expiration , task ) )
3034	def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
2523	def p_file_lic_conc ( self , f_term , predicate ) : try : for _ , _ , licenses in self . graph . triples ( ( f_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) self . builder . set_concluded_license ( self . doc , lics ) except SPDXValueError : self . value_error ( 'FILE_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'file {0}' . format ( predicate ) )
12763	def load_attachments ( self , source , skeleton ) : self . targets = { } self . offsets = { } filename = source if isinstance ( source , str ) : source = open ( source ) else : filename = '(file-{})' . format ( id ( source ) ) for i , line in enumerate ( source ) : tokens = line . split ( '#' ) [ 0 ] . strip ( ) . split ( ) if not tokens : continue label = tokens . pop ( 0 ) if label not in self . channels : logging . info ( '%s:%d: unknown marker %s' , filename , i , label ) continue if not tokens : continue name = tokens . pop ( 0 ) bodies = [ b for b in skeleton . bodies if b . name == name ] if len ( bodies ) != 1 : logging . info ( '%s:%d: %d skeleton bodies match %s' , filename , i , len ( bodies ) , name ) continue b = self . targets [ label ] = bodies [ 0 ] o = self . offsets [ label ] = np . array ( list ( map ( float , tokens ) ) ) * b . dimensions / 2 logging . info ( '%s < , label , b . name , o )
3411	def add_moma ( model , solution = None , linear = True ) : r if 'moma_old_objective' in model . solver . variables : raise ValueError ( 'model is already adjusted for MOMA' ) if not linear : model . solver = sutil . choose_solver ( model , qp = True ) if solution is None : solution = pfba ( model ) prob = model . problem v = prob . Variable ( "moma_old_objective" ) c = prob . Constraint ( model . solver . objective . expression - v , lb = 0.0 , ub = 0.0 , name = "moma_old_objective_constraint" ) to_add = [ v , c ] model . objective = prob . Objective ( Zero , direction = "min" , sloppy = True ) obj_vars = [ ] for r in model . reactions : flux = solution . fluxes [ r . id ] if linear : components = sutil . add_absolute_expression ( model , r . flux_expression , name = "moma_dist_" + r . id , difference = flux , add = False ) to_add . extend ( components ) obj_vars . append ( components . variable ) else : dist = prob . Variable ( "moma_dist_" + r . id ) const = prob . Constraint ( r . flux_expression - dist , lb = flux , ub = flux , name = "moma_constraint_" + r . id ) to_add . extend ( [ dist , const ] ) obj_vars . append ( dist ** 2 ) model . add_cons_vars ( to_add ) if linear : model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } ) else : model . objective = prob . Objective ( add ( obj_vars ) , direction = "min" , sloppy = True )
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
13124	def object_to_id ( self , obj ) : search = Service . search ( ) search = search . filter ( "term" , address = obj . address ) search = search . filter ( "term" , protocol = obj . protocol ) search = search . filter ( "term" , port = obj . port ) search = search . filter ( "term" , state = obj . state ) if search . count ( ) : result = search [ 0 ] . execute ( ) [ 0 ] return result . meta . id else : return None
10405	def bond_canonical_statistics ( microcanonical_statistics , convolution_factors , ** kwargs ) : spanning_cluster = ( 'has_spanning_cluster' in microcanonical_statistics . dtype . names ) ret = np . empty ( 1 , dtype = canonical_statistics_dtype ( spanning_cluster ) ) if spanning_cluster : ret [ 'percolation_probability' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'has_spanning_cluster' ] ) ret [ 'max_cluster_size' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'max_cluster_size' ] ) ret [ 'moments' ] = np . sum ( convolution_factors [ : , np . newaxis ] * microcanonical_statistics [ 'moments' ] , axis = 0 , ) return ret
3901	def _exception_handler ( self , _loop , context ) : self . _coroutine_queue . put ( self . _client . disconnect ( ) ) default_exception = Exception ( context . get ( 'message' ) ) self . _exception = context . get ( 'exception' , default_exception )
71	def clip_out_of_image ( self ) : bbs_cut = [ bb . clip_out_of_image ( self . shape ) for bb in self . bounding_boxes if bb . is_partly_within_image ( self . shape ) ] return BoundingBoxesOnImage ( bbs_cut , shape = self . shape )
9016	def instruction_in_row ( self , row , specification ) : whole_instruction_ = self . _as_instruction ( specification ) return self . _spec . new_instruction_in_row ( row , whole_instruction_ )
1580	def send ( self , dispatcher ) : if self . sent_complete : return sent = dispatcher . send ( self . to_send ) self . to_send = self . to_send [ sent : ]
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )
10156	def merge_dicts ( base , changes ) : for k , v in changes . items ( ) : if isinstance ( v , dict ) : merge_dicts ( base . setdefault ( k , { } ) , v ) else : base . setdefault ( k , v )
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
4109	def mexican ( lb , ub , n ) : r if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = ( 1. - x ** 2. ) * ( 2. / ( numpy . sqrt ( 3. ) * pi ** 0.25 ) ) * numpy . exp ( - x ** 2 / 2. ) return psi
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
866	def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( "Setting custom configuration properties=%r; caller=%r" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )
3413	def _update_optional ( cobra_object , new_dict , optional_attribute_dict , ordered_keys ) : for key in ordered_keys : default = optional_attribute_dict [ key ] value = getattr ( cobra_object , key ) if value is None or value == default : continue new_dict [ key ] = _fix_type ( value )
12433	def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
12150	def htmlFor ( self , fname ) : if os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.jpg' , '.png' ] : html = '<a href="%s"><img src="%s"></a>' % ( fname , fname ) if "_tif_" in fname : html = html . replace ( '<img ' , '<img class="datapic micrograph"' ) if "_plot_" in fname : html = html . replace ( '<img ' , '<img class="datapic intrinsic" ' ) if "_experiment_" in fname : html = html . replace ( '<img ' , '<img class="datapic experiment" ' ) elif os . path . splitext ( fname ) [ 1 ] . lower ( ) in [ '.html' , '.htm' ] : html = 'LINK: %s' % fname else : html = '<br>Not sure how to show: [%s]</br>' % fname return html
10359	def shuffle_relations ( graph : BELGraph , percentage : Optional [ str ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_edges ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) edges = result . edges ( keys = True ) for _ in range ( swaps ) : ( s1 , t1 , k1 ) , ( s2 , t2 , k2 ) = random . sample ( edges , 2 ) result [ s1 ] [ t1 ] [ k1 ] , result [ s2 ] [ t2 ] [ k2 ] = result [ s2 ] [ t2 ] [ k2 ] , result [ s1 ] [ t1 ] [ k1 ] return result
5947	def unlink_f ( path ) : try : os . unlink ( path ) except OSError as err : if err . errno != errno . ENOENT : raise
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
12959	def _peekNextID ( self , conn = None ) : if conn is None : conn = self . _get_connection ( ) return to_unicode ( conn . get ( self . _get_next_id_key ( ) ) or 0 )
1905	def strcmp ( state , s1 , s2 ) : cpu = state . cpu if issymbolic ( s1 ) : raise ConcretizeArgument ( state . cpu , 1 ) if issymbolic ( s2 ) : raise ConcretizeArgument ( state . cpu , 2 ) s1_zero_idx = _find_zero ( cpu , state . constraints , s1 ) s2_zero_idx = _find_zero ( cpu , state . constraints , s2 ) min_zero_idx = min ( s1_zero_idx , s2_zero_idx ) ret = None for offset in range ( min_zero_idx , - 1 , - 1 ) : s1char = ZEXTEND ( cpu . read_int ( s1 + offset , 8 ) , cpu . address_bit_size ) s2char = ZEXTEND ( cpu . read_int ( s2 + offset , 8 ) , cpu . address_bit_size ) if issymbolic ( s1char ) or issymbolic ( s2char ) : if ret is None or ( not issymbolic ( ret ) and ret == 0 ) : ret = s1char - s2char else : ret = ITEBV ( cpu . address_bit_size , s1char != s2char , s1char - s2char , ret ) else : if s1char != s2char : ret = s1char - s2char elif ret is None : ret = 0 return ret
13051	def import_nmap ( result , tag , check_function = all_hosts , import_services = False ) : host_search = HostSearch ( arguments = False ) service_search = ServiceSearch ( ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) imported_hosts = 0 imported_services = 0 for nmap_host in report . hosts : if check_function ( nmap_host ) : imported_hosts += 1 host = host_search . id_to_object ( nmap_host . address ) host . status = nmap_host . status host . add_tag ( tag ) if nmap_host . os_fingerprinted : host . os = nmap_host . os_fingerprint if nmap_host . hostnames : host . hostname . extend ( nmap_host . hostnames ) if import_services : for service in nmap_host . services : imported_services += 1 serv = Service ( ** service . get_dict ( ) ) serv . address = nmap_host . address service_id = service_search . object_to_id ( serv ) if service_id : serv_old = Service . get ( service_id ) if service . banner : serv_old . banner = service . banner serv_old . save ( ) else : serv . address = nmap_host . address serv . save ( ) if service . state == 'open' : host . open_ports . append ( service . port ) if service . state == 'closed' : host . closed_ports . append ( service . port ) if service . state == 'filtered' : host . filtered_ports . append ( service . port ) host . save ( ) if imported_hosts : print_success ( "Imported {} hosts, with tag {}" . format ( imported_hosts , tag ) ) else : print_error ( "No hosts found" ) return { 'hosts' : imported_hosts , 'services' : imported_services }
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) cpu . _calculate_logic_flags ( dest . size , res )
689	def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
1237	def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network
2016	def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
210	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_0to1_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr_0to1 , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) heatmaps = HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) if return_pad_amounts : return heatmaps , pad_amounts else : return heatmaps
1703	def outer_left_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
10807	def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
10762	def _wait_for_connection ( self , port ) : connected = False max_tries = 10 num_tries = 0 wait_time = 0.5 while not connected or num_tries >= max_tries : time . sleep ( wait_time ) try : af = socket . AF_INET addr = ( '127.0.0.1' , port ) sock = socket . socket ( af , socket . SOCK_STREAM ) sock . connect ( addr ) except socket . error : if sock : sock . close ( ) num_tries += 1 continue connected = True if not connected : print ( "Error connecting to sphinx searchd" , file = sys . stderr )
5445	def _parse_image_uri ( self , raw_uri ) : docker_uri = os . path . join ( self . _relative_path , raw_uri . replace ( 'https://' , 'https/' , 1 ) ) return docker_uri
9473	def DFS_prefix ( self , root = None ) : if not root : root = self . _root return self . _DFS_prefix ( root )
10284	def count_targets ( edge_iter : EdgeIterator ) -> Counter : return Counter ( v for _ , v , _ in edge_iter )
4398	def adsSyncDelDeviceNotificationReqEx ( port , adr , notification_handle , user_handle ) : adsSyncDelDeviceNotificationReqFct = _adsDLL . AdsSyncDelDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) nHNotification = ctypes . c_ulong ( notification_handle ) err_code = adsSyncDelDeviceNotificationReqFct ( port , pAmsAddr , nHNotification ) callback_store . pop ( notification_handle , None ) if err_code : raise ADSError ( err_code ) adsSyncWriteReqEx ( port , adr , ADSIGRP_SYM_RELEASEHND , 0 , user_handle , PLCTYPE_UDINT )
3404	def normalize_cutoff ( model , zero_cutoff = None ) : if zero_cutoff is None : return model . tolerance else : if zero_cutoff < model . tolerance : raise ValueError ( "The chosen zero cutoff cannot be less than the model's " "tolerance value." ) else : return zero_cutoff
10080	def _publish_edited ( self ) : record_pid , record = self . fetch_published ( ) if record . revision_id == self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] : data = dict ( self . dumps ( ) ) else : data = self . merge_with_published ( ) data [ '$schema' ] = self . record_schema data [ '_deposit' ] = self [ '_deposit' ] record = record . __class__ ( data , model = record . model ) return record
11007	def get_bets ( self , type = None , order_by = None , state = None , project_id = None , page = None , page_size = None ) : if page is None : page = 1 if page_size is None : page_size = 100 if state == 'all' : _states = [ ] elif state == 'closed' : _states = self . CLOSED_STATES else : _states = self . ACTIVE_STATES url = urljoin ( self . settings [ 'bets_url' ] , 'bets?page={}&page_size={}' . format ( page , page_size ) ) url += '&state={}' . format ( ',' . join ( _states ) ) if type is not None : url += '&type={}' . format ( type ) if order_by in [ '-last_stake' , 'last_stake' ] : url += '&order_by={}' . format ( order_by ) if project_id is not None : url += '&kava_project_id={}' . format ( project_id ) res = self . _req ( url ) return res [ 'bets' ] [ 'results' ]
2240	def modname_to_modpath ( modname , hide_init = True , hide_main = False , sys_path = None ) : modpath = _syspath_modname_to_modpath ( modname , sys_path ) if modpath is None : return None modpath = normalize_modpath ( modpath , hide_init = hide_init , hide_main = hide_main ) return modpath
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
8749	def delete_scalingip ( context , id ) : LOG . info ( 'delete_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . SCALING )
9593	def switch_to_frame ( self , frame_reference = None ) : if frame_reference is not None and type ( frame_reference ) not in [ int , WebElement ] : raise TypeError ( 'Type of frame_reference must be None or int or WebElement' ) self . _execute ( Command . SWITCH_TO_FRAME , { 'id' : frame_reference } )
9000	def unique ( iterables ) : included_elements = set ( ) def included ( element ) : result = element in included_elements included_elements . add ( element ) return result return [ element for elements in iterables for element in elements if not included ( element ) ]
3885	def get_user ( self , user_id ) : try : return self . _user_dict [ user_id ] except KeyError : logger . warning ( 'UserList returning unknown User for UserID %s' , user_id ) return User ( user_id , None , None , None , [ ] , False )
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
7679	def event ( annotation , ** kwargs ) : times , values = annotation . to_interval_values ( ) if any ( values ) : labels = values else : labels = None return mir_eval . display . events ( times , labels = labels , ** kwargs )
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
616	def expGenerator ( args ) : parser = OptionParser ( ) parser . set_usage ( "%prog [options] --description='{json object with args}'\n" + "%prog [options] --descriptionFromFile='{filename}'\n" + "%prog [options] --showSchema" ) parser . add_option ( "--description" , dest = "description" , help = "Tells ExpGenerator to generate an experiment description.py and " "permutations.py file using the given JSON formatted experiment " "description string." ) parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , help = "Tells ExpGenerator to open the given filename and use it's " "contents as the JSON formatted experiment description." ) parser . add_option ( "--claDescriptionTemplateFile" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = "The file containing the template description file for " " ExpGenerator [default: %default]" ) parser . add_option ( "--showSchema" , action = "store_true" , dest = "showSchema" , help = "Prints the JSON schemas for the --description arg." ) parser . add_option ( "--version" , dest = 'version' , default = 'v2' , help = "Generate the permutations file for this version of hypersearch." " Possible choices are 'v1' and 'v2' [default: %default]." ) parser . add_option ( "--outDir" , dest = "outDir" , default = None , help = "Where to generate experiment. If not specified, " "then a temp directory will be created" ) ( options , remainingArgs ) = parser . parse_args ( args ) if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Unexpected command-line args: <%s>" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( "The specified command options are " + "mutually-exclusive: %s" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( "Error in validating command options. No option " "provided:\n" , parser . get_usage ( ) ) )
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
8632	def search_projects ( session , query , search_filter = None , project_details = None , user_details = None , limit = 10 , offset = 0 , active_only = None ) : search_data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search_filter : search_data . update ( search_filter ) if project_details : search_data . update ( project_details ) if user_details : search_data . update ( user_details ) endpoint = 'projects/{}' . format ( 'active' if active_only else 'all' ) response = make_get_request ( session , endpoint , params_data = search_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6688	def groupinstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupinstall "%(group)s"' % locals ( ) , pty = False )
9690	def start ( self ) : self . receiver = self . Receiver ( self . read , self . write , self . send_lock , self . senders , self . frames_received , callback = self . receive_callback , fcs_nack = self . fcs_nack , ) self . receiver . start ( )
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) distribution = self . get_distribution ( ) if distribution : self . project_name = distribution . get_name ( ) else : self . project_name = self . project_dir . name if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : package_name = self . project_name . replace ( "-" , "_" ) def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
8683	def export ( self , output_path = None , decrypt = False ) : self . _assert_valid_stash ( ) all_keys = [ ] for key in self . list ( ) : all_keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all_keys : if output_path : with open ( output_path , 'w' ) as output_file : output_file . write ( json . dumps ( all_keys , indent = 4 ) ) return all_keys else : raise GhostError ( 'There are no keys to export' )
4465	def save ( filename_audio , filename_jam , jam , strict = True , fmt = 'auto' , ** kwargs ) : y = jam . sandbox . muda . _audio [ 'y' ] sr = jam . sandbox . muda . _audio [ 'sr' ] psf . write ( filename_audio , y , sr , ** kwargs ) jam . save ( filename_jam , strict = strict , fmt = fmt )
4958	def validate_email_to_link ( email , raw_email = None , message_template = None , ignore_existing = False ) : raw_email = raw_email if raw_email is not None else email message_template = message_template if message_template is not None else ValidationMessages . INVALID_EMAIL try : validate_email ( email ) except ValidationError : raise ValidationError ( message_template . format ( argument = raw_email ) ) existing_record = EnterpriseCustomerUser . objects . get_link_by_email ( email ) if existing_record and not ignore_existing : raise ValidationError ( ValidationMessages . USER_ALREADY_REGISTERED . format ( email = email , ec_name = existing_record . enterprise_customer . name ) ) return existing_record or False
278	def axes_style ( style = 'darkgrid' , rc = None ) : if rc is None : rc = { } rc_default = { } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . axes_style ( style = style , rc = rc )
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
2672	def upload ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) upload_s3 ( cfg , path_to_zip_file )
8063	def do_set ( self , line ) : try : name , value = [ part . strip ( ) for part in line . split ( '=' ) ] if name not in self . bot . _vars : self . print_response ( 'No such variable %s enter vars to see available vars' % name ) return variable = self . bot . _vars [ name ] variable . value = variable . sanitize ( value . strip ( ';' ) ) success , msg = self . bot . canvas . sink . var_changed ( name , variable . value ) if success : print ( '{}={}' . format ( name , variable . value ) , file = self . stdout ) else : print ( '{}\n' . format ( msg ) , file = self . stdout ) except Exception as e : print ( 'Invalid Syntax.' , e ) return
10538	def create_category ( name , description ) : try : category = dict ( name = name , short_name = name . lower ( ) . replace ( " " , "" ) , description = description ) res = _pybossa_req ( 'post' , 'category' , payload = category ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
7381	def simplified_edges ( self ) : for group , edgelist in self . edges . items ( ) : for u , v , d in edgelist : yield ( u , v )
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : raise e
2031	def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get_code ( account ) self . _allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . _store ( address + i , extbytecode [ offset + i ] ) else : self . _store ( address + i , 0 )
9927	def get_user ( self , user_id ) : try : return get_user_model ( ) . objects . get ( id = user_id ) except get_user_model ( ) . DoesNotExist : return None
2210	def parse_requirements ( fname = 'requirements.txt' ) : from os . path import dirname , join , exists import re require_fpath = join ( dirname ( __file__ ) , fname ) def parse_line ( line ) : info = { } if line . startswith ( '-e ' ) : info [ 'package' ] = line . split ( '#egg=' ) [ 1 ] else : pat = '(' + '|' . join ( [ '>=' , '==' , '>' ] ) + ')' parts = re . split ( pat , line , maxsplit = 1 ) parts = [ p . strip ( ) for p in parts ] info [ 'package' ] = parts [ 0 ] if len ( parts ) > 1 : op , rest = parts [ 1 : ] if ';' in rest : version , platform_deps = map ( str . strip , rest . split ( ';' ) ) info [ 'platform_deps' ] = platform_deps else : version = rest info [ 'version' ] = ( op , version ) return info if exists ( require_fpath ) : with open ( require_fpath , 'r' ) as f : packages = [ ] for line in f . readlines ( ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : info = parse_line ( line ) package = info [ 'package' ] if not sys . version . startswith ( '3.4' ) : platform_deps = info . get ( 'platform_deps' ) if platform_deps is not None : package += ';' + platform_deps packages . append ( package ) return packages return [ ]
1075	def _days_in_month ( year , month ) : "year, month -> number of days in that month in that year." assert 1 <= month <= 12 , month if month == 2 and _is_leap ( year ) : return 29 return _DAYS_IN_MONTH [ month ]
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
2196	def flush ( self ) : if self . redirect is not None : self . redirect . flush ( ) super ( TeeStringIO , self ) . flush ( )
9741	async def await_event ( self , event = None , timeout = None ) : if self . event_future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait_for ( self . _wait_loop ( event ) , timeout ) return result
10478	def _waitFor ( self , timeout , notification , ** kwargs ) : callback = self . _matchOther retelem = None callbackArgs = None callbackKwargs = None if 'callback' in kwargs : callback = kwargs [ 'callback' ] del kwargs [ 'callback' ] if 'args' in kwargs : if not isinstance ( kwargs [ 'args' ] , tuple ) : errStr = 'Notification callback args not given as a tuple' raise TypeError ( errStr ) callbackArgs = kwargs [ 'args' ] del kwargs [ 'args' ] if 'kwargs' in kwargs : if not isinstance ( kwargs [ 'kwargs' ] , dict ) : errStr = 'Notification callback kwargs not given as a dict' raise TypeError ( errStr ) callbackKwargs = kwargs [ 'kwargs' ] del kwargs [ 'kwargs' ] if kwargs : if callbackKwargs : callbackKwargs . update ( kwargs ) else : callbackKwargs = kwargs else : callbackArgs = ( retelem , ) callbackKwargs = kwargs return self . _setNotification ( timeout , notification , callback , callbackArgs , callbackKwargs )
2677	def _install_packages ( path , packages ) : def _filter_blacklist ( package ) : blacklist = [ '-i' , '#' , 'Python==' , 'python-lambda==' ] return all ( package . startswith ( entry ) is False for entry in blacklist ) filtered_packages = filter ( _filter_blacklist , packages ) for package in filtered_packages : if package . startswith ( '-e ' ) : package = package . replace ( '-e ' , '' ) print ( 'Installing {package}' . format ( package = package ) ) subprocess . check_call ( [ sys . executable , '-m' , 'pip' , 'install' , package , '-t' , path , '--ignore-installed' ] ) print ( 'Install directory contents are now: {directory}' . format ( directory = os . listdir ( path ) ) )
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
13913	def _InternalUnpackAny ( msg ) : type_url = msg . type_url db = symbol_database . Default ( ) if not type_url : return None type_name = type_url . split ( "/" ) [ - 1 ] descriptor = db . pool . FindMessageTypeByName ( type_name ) if descriptor is None : return None message_class = db . GetPrototype ( descriptor ) message = message_class ( ) message . ParseFromString ( msg . value ) return message
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
7481	def cleanup_tempfiles ( data ) : tmps1 = glob . glob ( os . path . join ( data . tmpdir , "*.fa" ) ) tmps2 = glob . glob ( os . path . join ( data . tmpdir , "*.npy" ) ) for tmp in tmps1 + tmps2 : if os . path . exists ( tmp ) : os . remove ( tmp ) removal = [ os . path . join ( data . dirs . across , data . name + ".utemp" ) , os . path . join ( data . dirs . across , data . name + ".htemp" ) , os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) , os . path . join ( data . dirs . across , data . name + "_cathaps.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catshuf.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catsort.tmp" ) , os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) , os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) , ] for rfile in removal : if os . path . exists ( rfile ) : os . remove ( rfile ) smpios = glob . glob ( os . path . join ( data . dirs . across , '*.tmp.h5' ) ) for smpio in smpios : if os . path . exists ( smpio ) : os . remove ( smpio )
12885	def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
3008	def _credentials_from_request ( request ) : if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
8185	def offset ( self , node ) : x = self . x + node . x - _ctx . WIDTH / 2 y = self . y + node . y - _ctx . HEIGHT / 2 return x , y
4553	def set_colors ( self , colors , pos ) : self . _colors = colors self . _pos = pos end = self . _pos + self . numLEDs if end > len ( self . _colors ) : raise ValueError ( 'Needed %d colors but found %d' % ( end , len ( self . _colors ) ) )
12256	def lbfgs ( x , rho , f_df , maxiter = 20 ) : def f_df_augmented ( theta ) : f , df = f_df ( theta ) obj = f + ( rho / 2. ) * np . linalg . norm ( theta - x ) ** 2 grad = df + rho * ( theta - x ) return obj , grad res = scipy_minimize ( f_df_augmented , x , jac = True , method = 'L-BFGS-B' , options = { 'maxiter' : maxiter , 'disp' : False } ) return res . x
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
12510	def get_img_data ( image , copy = True ) : try : img = check_img ( image ) if copy : return get_data ( img ) else : return img . get_data ( ) except Exception as exc : raise Exception ( 'Error when reading file {0}.' . format ( repr_imgs ( image ) ) ) from exc
2404	def gen_prompt_feats ( self , e_set ) : prompt_toks = nltk . word_tokenize ( e_set . _prompt ) expand_syns = [ ] for word in prompt_toks : synonyms = util_functions . get_wordnet_syns ( word ) expand_syns . append ( synonyms ) expand_syns = list ( chain . from_iterable ( expand_syns ) ) prompt_overlap = [ ] prompt_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 prompt_overlap . append ( len ( [ i for i in j if i in prompt_toks ] ) ) prompt_overlap_prop . append ( prompt_overlap [ len ( prompt_overlap ) - 1 ] / float ( tok_length ) ) expand_overlap = [ ] expand_overlap_prop = [ ] for j in e_set . _tokens : tok_length = len ( j ) if ( tok_length == 0 ) : tok_length = 1 expand_overlap . append ( len ( [ i for i in j if i in expand_syns ] ) ) expand_overlap_prop . append ( expand_overlap [ len ( expand_overlap ) - 1 ] / float ( tok_length ) ) prompt_arr = numpy . array ( ( prompt_overlap , prompt_overlap_prop , expand_overlap , expand_overlap_prop ) ) . transpose ( ) return prompt_arr . copy ( )
12754	def joint_distances ( self ) : return [ ( ( np . array ( j . anchor ) - j . anchor2 ) ** 2 ) . sum ( ) for j in self . joints ]
7356	def create_input_peptides_files ( peptides , max_peptides_per_file = None , group_by_length = False ) : if group_by_length : peptide_lengths = { len ( p ) for p in peptides } peptide_groups = { l : [ ] for l in peptide_lengths } for p in peptides : peptide_groups [ len ( p ) ] . append ( p ) else : peptide_groups = { "" : peptides } file_names = [ ] for key , group in peptide_groups . items ( ) : n_peptides = len ( group ) if not max_peptides_per_file : max_peptides_per_file = n_peptides input_file = None for i , p in enumerate ( group ) : if i % max_peptides_per_file == 0 : if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) input_file = make_writable_tempfile ( prefix_number = i // max_peptides_per_file , prefix_name = key , suffix = ".txt" ) input_file . write ( "%s\n" % p ) if input_file is not None : file_names . append ( input_file . name ) input_file . close ( ) return file_names
2855	def setup ( self , pin , mode ) : self . _setup_pin ( pin , mode ) self . mpsse_write_gpio ( )
7252	def order ( self , image_catalog_ids , batch_size = 100 , callback = None ) : def _order_single_batch ( url_ , ids , results_list ) : data = json . dumps ( ids ) if callback is None else json . dumps ( { "acquisitionIds" : ids , "callback" : callback } ) r = self . gbdx_connection . post ( url_ , data = data ) r . raise_for_status ( ) order_id = r . json ( ) . get ( "order_id" ) if order_id : results_list . append ( order_id ) self . logger . debug ( 'Place order' ) url = ( '%s/order' if callback is None else '%s/ordercb' ) % self . base_url batch_size = min ( 100 , batch_size ) if not isinstance ( image_catalog_ids , list ) : image_catalog_ids = [ image_catalog_ids ] sanitized_ids = list ( set ( ( id for id in ( _id . strip ( ) for _id in image_catalog_ids ) if id ) ) ) res = [ ] acq_ids_by_batch = zip ( * ( [ iter ( sanitized_ids ) ] * batch_size ) ) for ids_batch in acq_ids_by_batch : _order_single_batch ( url , ids_batch , res ) remain_count = len ( sanitized_ids ) % batch_size if remain_count > 0 : _order_single_batch ( url , sanitized_ids [ - remain_count : ] , res ) if len ( res ) == 1 : return res [ 0 ] elif len ( res ) > 1 : return res
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
13511	def http_exception_error_handler ( exception ) : assert issubclass ( type ( exception ) , HTTPException ) , type ( exception ) assert hasattr ( exception , "code" ) assert hasattr ( exception , "description" ) return response ( exception . code , exception . description )
13306	def correlation ( a , b ) : diff1 = a - a . mean ( ) diff2 = b - b . mean ( ) return ( diff1 * diff2 ) . mean ( ) / ( np . sqrt ( np . square ( diff1 ) . mean ( ) * np . square ( diff2 ) . mean ( ) ) )
6270	def swap_buffers ( self ) : if not self . window . context : return self . frames += 1 self . window . flip ( ) self . window . dispatch_events ( )
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
2540	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True doc . package . summary = text else : raise CardinalityError ( 'Package::Summary' )
11559	def i2c_write ( self , address , * args ) : data = [ address , self . I2C_WRITE ] for item in args : data . append ( item & 0x7f ) data . append ( ( item >> 7 ) & 0x7f ) self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
229	def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure
11740	def first ( self , symbols ) : ret = set ( ) if EPSILON in symbols : return set ( [ EPSILON ] ) for symbol in symbols : ret |= self . _first [ symbol ] - set ( [ EPSILON ] ) if EPSILON not in self . _first [ symbol ] : break else : ret . add ( EPSILON ) return ret
4314	def validate_input_file ( input_filepath ) : if not os . path . exists ( input_filepath ) : raise IOError ( "input_filepath {} does not exist." . format ( input_filepath ) ) ext = file_extension ( input_filepath ) if ext not in VALID_FORMATS : logger . info ( "Valid formats: %s" , " " . join ( VALID_FORMATS ) ) logger . warning ( "This install of SoX cannot process .{} files." . format ( ext ) )
3602	def get_user ( self ) : token = self . authenticator . create_token ( self . extra ) user_id = self . extra . get ( 'id' ) return FirebaseUser ( self . email , token , self . provider , user_id )
8124	def draw_cornu_flat ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd ) : for j in range ( 0 , 100 ) : t = j * .01 s , c = eval_cornu ( t0 + t * ( t1 - t0 ) ) s *= flip s -= s0 c -= c0 x = c * cs - s * ss y = s * cs + c * ss print_pt ( x0 + x , y0 + y , cmd ) cmd = 'lineto' return cmd
918	def warning ( self , msg , * args , ** kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
3346	def guess_mime_type ( url ) : ( mimetype , _mimeencoding ) = mimetypes . guess_type ( url ) if not mimetype : ext = os . path . splitext ( url ) [ 1 ] mimetype = _MIME_TYPES . get ( ext ) _logger . debug ( "mimetype({}): {}" . format ( url , mimetype ) ) if not mimetype : mimetype = "application/octet-stream" return mimetype
6358	def lcsstr ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , i_longest = 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] i_longest = i else : lengths [ i , j ] = 0 return src [ i_longest - longest : i_longest ]
8994	def relative_folder ( self , module , folder ) : folder = self . _relative_to_absolute ( module , folder ) return self . folder ( folder )
13707	def squeeze_words ( line , width = 60 ) : while ( ' ' in line ) and ( len ( line ) > width ) : head , _ , tail = line . rpartition ( ' ' ) line = ' ' . join ( ( head , tail ) ) return line
6963	def initialize ( self , currentdir , assetpath , cplist , cplistfile , executor , readonly , baseurl ) : self . currentdir = currentdir self . assetpath = assetpath self . currentproject = cplist self . cplistfile = cplistfile self . executor = executor self . readonly = readonly self . baseurl = baseurl
4850	def _transmit_create ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . create_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _create_transmissions ( chunk )
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
5341	def __get_menu_entries ( self , kibiter_major ) : menu_entries = [ ] for entry in self . panels_menu : if entry [ 'source' ] not in self . data_sources : continue parent_menu_item = { 'name' : entry [ 'name' ] , 'title' : entry [ 'name' ] , 'description' : "" , 'type' : "menu" , 'dashboards' : [ ] } for subentry in entry [ 'menu' ] : try : dash_name = get_dashboard_name ( subentry [ 'panel' ] ) except FileNotFoundError : logging . error ( "Can't open dashboard file %s" , subentry [ 'panel' ] ) continue child_item = { "name" : subentry [ 'name' ] , "title" : subentry [ 'name' ] , "description" : "" , "type" : "entry" , "panel_id" : dash_name } parent_menu_item [ 'dashboards' ] . append ( child_item ) menu_entries . append ( parent_menu_item ) return menu_entries
702	def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results
9853	def centers ( self ) : for idx in numpy . ndindex ( self . grid . shape ) : yield self . delta * numpy . array ( idx ) + self . origin
3060	def locked_get ( self ) : credential = self . _backend . locked_get ( self . _key ) if credential is not None : credential . set_store ( self ) return credential
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
1622	def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : for i in range ( begin , end ) : lines [ i ] = '/**/'
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
12986	def toBytes ( self , value ) : if type ( value ) == bytes : return value return value . encode ( self . getEncoding ( ) )
4678	def getActiveKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) for authority in account [ "active" ] [ "key_auths" ] : try : return self . getPrivateKeyForPublicKey ( authority [ 0 ] ) except Exception : pass return False
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
12023	def check_phase ( self ) : plus_minus = set ( [ '+' , '-' ] ) for k , g in groupby ( sorted ( [ line for line in self . lines if line [ 'line_type' ] == 'feature' and line [ 'type' ] == 'CDS' and 'Parent' in line [ 'attributes' ] ] , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) , key = lambda x : x [ 'attributes' ] [ 'Parent' ] ) : cds_list = list ( g ) strand_set = list ( set ( [ line [ 'strand' ] for line in cds_list ] ) ) if len ( strand_set ) != 1 : for line in cds_list : self . add_line_error ( line , { 'message' : 'Inconsistent CDS strand with parent: {0:s}' . format ( k ) , 'error_type' : 'STRAND' } ) continue if len ( cds_list ) == 1 : if cds_list [ 0 ] [ 'phase' ] != 0 : self . add_line_error ( cds_list [ 0 ] , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( cds_list [ 0 ] [ 'phase' ] , 0 ) , 'error_type' : 'PHASE' } ) continue strand = strand_set [ 0 ] if strand not in plus_minus : continue if strand == '-' : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'end' ] , reverse = True ) else : sorted_cds_list = sorted ( cds_list , key = lambda x : x [ 'start' ] ) phase = 0 for line in sorted_cds_list : if line [ 'phase' ] != phase : self . add_line_error ( line , { 'message' : 'Wrong phase {0:d}, should be {1:d}' . format ( line [ 'phase' ] , phase ) , 'error_type' : 'PHASE' } ) phase = ( 3 - ( ( line [ 'end' ] - line [ 'start' ] + 1 - phase ) % 3 ) ) % 3
475	def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\d" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ]
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) return ( pmags - modelmags ) / perrs
1244	def update_batch ( self , loss_per_instance ) : if self . batch_indices is None : raise TensorForceError ( "Need to call get_batch before each update_batch call." ) for index , loss in zip ( self . batch_indices , loss_per_instance ) : new_priority = ( np . abs ( loss ) + self . prioritization_constant ) ** self . prioritization_weight self . observations . _move ( index , new_priority ) self . none_priority_index += 1
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
12086	def html_index ( self , launch = False , showChildren = False ) : self . makePics ( ) html = '<a href="index_splash.html" target="content">./%s/</a><br>' % os . path . basename ( self . abfFolder ) for ID in smartSort ( self . fnamesByCell . keys ( ) ) : link = '' if ID + ".html" in self . fnames2 : link = 'href="%s.html" target="content"' % ID html += ( '<a %s>%s</a><br>' % ( link , ID ) ) if showChildren : for fname in self . fnamesByCell [ ID ] : thisID = os . path . splitext ( fname ) [ 0 ] files2 = [ x for x in self . fnames2 if x . startswith ( thisID ) and not x . endswith ( ".html" ) ] html += '<i>%s</i>' % thisID if len ( files2 ) : html += ' (%s)' % len ( files2 ) html += '<br>' html += "<br>" style . save ( html , self . abfFolder2 + "/index_menu.html" ) self . html_index_splash ( ) style . frames ( self . abfFolder2 + "/index.html" , launch = launch )
7163	def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
8998	def string ( self , string ) : object_ = json . loads ( string ) return self . object ( object_ )
2001	def visit_BitVecOr ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left elif right . value == left . mask : return right elif isinstance ( left , BitVecOr ) : left_left = left . operands [ 0 ] left_right = left . operands [ 1 ] if isinstance ( right , Constant ) : return BitVecOr ( left_left , ( left_right | right ) , taint = expression . taint ) elif isinstance ( left , BitVecConstant ) : return BitVecOr ( right , left , taint = expression . taint )
13407	def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
9051	def poisson_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) link = LogLink ( ) lik = PoissonProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
5756	def _strip_version_suffix ( version ) : global version_regex if not version : return version match = version_regex . search ( version ) return match . group ( 0 ) if match else version
13652	def pre ( self , command , output_dir , vars ) : vars [ 'license_name' ] = 'Apache' vars [ 'year' ] = time . strftime ( '%Y' , time . localtime ( ) )
3288	def _get_repo_info ( self , environ , rev , reload = False ) : caches = environ . setdefault ( "wsgidav.hg.cache" , { } ) if caches . get ( compat . to_native ( rev ) ) is not None : _logger . debug ( "_get_repo_info(%s): cache hit." % rev ) return caches [ compat . to_native ( rev ) ] start_time = time . time ( ) self . ui . pushbuffer ( ) commands . manifest ( self . ui , self . repo , rev ) res = self . ui . popbuffer ( ) files = [ ] dirinfos = { } filedict = { } for file in res . split ( "\n" ) : if file . strip ( ) == "" : continue file = file . replace ( "\\" , "/" ) parents = file . split ( "/" ) if len ( parents ) >= 1 : p1 = "" for i in range ( 0 , len ( parents ) - 1 ) : p2 = parents [ i ] dir = dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) if p2 not in dir [ 0 ] : dir [ 0 ] . append ( p2 ) if p1 == "" : p1 = p2 else : p1 = "%s/%s" % ( p1 , p2 ) dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) [ 1 ] . append ( parents [ - 1 ] ) filedict [ file ] = True files . sort ( ) cache = { "files" : files , "dirinfos" : dirinfos , "filedict" : filedict } caches [ compat . to_native ( rev ) ] = cache _logger . info ( "_getRepoInfo(%s) took %.3f" % ( rev , time . time ( ) - start_time ) ) return cache
13688	def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
5567	def area_at_zoom ( self , zoom = None ) : if zoom is None : if not self . _cache_full_process_area : logger . debug ( "calculate process area ..." ) self . _cache_full_process_area = cascaded_union ( [ self . _area_at_zoom ( z ) for z in self . init_zoom_levels ] ) . buffer ( 0 ) return self . _cache_full_process_area else : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) return self . _area_at_zoom ( zoom )
5313	def translate_style ( style , colormode , colorpalette ) : style_parts = iter ( style . split ( '_' ) ) ansi_start_sequence = [ ] ansi_end_sequence = [ ] try : part = None for mod_part in style_parts : part = mod_part if part not in ansi . MODIFIERS : break mod_start_code , mod_end_code = resolve_modifier_to_ansi_code ( part , colormode ) ansi_start_sequence . append ( mod_start_code ) ansi_end_sequence . append ( mod_end_code ) else : raise StopIteration ( ) if part != 'on' : ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . FOREGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) next ( style_parts ) part = next ( style_parts ) ansi_start_code , ansi_end_code = translate_colorname_to_ansi_code ( part , ansi . BACKGROUND_COLOR_OFFSET , colormode , colorpalette ) ansi_start_sequence . append ( ansi_start_code ) ansi_end_sequence . append ( ansi_end_code ) except StopIteration : pass return '' . join ( ansi_start_sequence ) , '' . join ( ansi_end_sequence )
11398	def update_reportnumbers ( self ) : rep_088_fields = record_get_field_instances ( self . record , '088' ) for field in rep_088_fields : subs = field_get_subfields ( field ) if '9' in subs : for val in subs [ '9' ] : if val . startswith ( 'P0' ) or val . startswith ( 'CM-P0' ) : sf = [ ( '9' , 'CERN' ) , ( 'b' , val ) ] record_add_field ( self . record , '595' , subfields = sf ) for key , val in field [ 0 ] : if key in [ 'a' , '9' ] and not val . startswith ( 'SIS-' ) : record_add_field ( self . record , '037' , subfields = [ ( 'a' , val ) ] ) record_delete_fields ( self . record , "088" ) rep_037_fields = record_get_field_instances ( self . record , '037' ) for field in rep_037_fields : subs = field_get_subfields ( field ) if 'a' in subs : for value in subs [ 'a' ] : if 'arXiv' in value : new_subs = [ ( 'a' , value ) , ( '9' , 'arXiv' ) ] for fld in record_get_field_instances ( self . record , '695' ) : for key , val in field_get_subfield_instances ( fld ) : if key == 'a' : new_subs . append ( ( 'c' , val ) ) break nf = create_field ( subfields = new_subs ) record_replace_field ( self . record , '037' , nf , field [ 4 ] ) for key , val in field [ 0 ] : if key in [ 'a' , '9' ] and val . startswith ( 'SIS-' ) : record_delete_field ( self . record , '037' , field_position_global = field [ 4 ] )
1718	def fix_js_args ( func ) : fcode = six . get_function_code ( func ) fargs = fcode . co_varnames [ fcode . co_argcount - 2 : fcode . co_argcount ] if fargs == ( 'this' , 'arguments' ) or fargs == ( 'arguments' , 'var' ) : return func code = append_arguments ( six . get_function_code ( func ) , ( 'this' , 'arguments' ) ) return types . FunctionType ( code , six . get_function_globals ( func ) , func . __name__ , closure = six . get_function_closure ( func ) )
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
5238	def shift_time ( start_time , mins ) -> str : s_time = pd . Timestamp ( start_time ) e_time = s_time + np . sign ( mins ) * pd . Timedelta ( f'00:{abs(mins)}:00' ) return e_time . strftime ( '%H:%M' )
9915	def update ( self , instance , validated_data ) : is_primary = validated_data . pop ( "is_primary" , False ) instance = super ( EmailSerializer , self ) . update ( instance , validated_data ) if is_primary : instance . set_primary ( ) return instance
13396	def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
13041	def pipe_worker ( pipename , filename , object_type , query , format_string , unique = False ) : print_notification ( "[{}] Starting pipe" . format ( pipename ) ) object_type = object_type ( ) try : while True : uniq = set ( ) if os . path . exists ( filename ) : os . remove ( filename ) os . mkfifo ( filename ) with open ( filename , 'w' ) as pipe : print_success ( "[{}] Providing data" . format ( pipename ) ) objects = object_type . search ( ** query ) for obj in objects : data = fmt . format ( format_string , ** obj . to_dict ( ) ) if unique : if not data in uniq : uniq . add ( data ) pipe . write ( data + '\n' ) else : pipe . write ( data + '\n' ) os . unlink ( filename ) except KeyboardInterrupt : print_notification ( "[{}] Shutting down named pipe" . format ( pipename ) ) except Exception as e : print_error ( "[{}] Error: {}, stopping named pipe" . format ( e , pipename ) ) finally : os . remove ( filename )
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
10612	def _calculate_T ( self , H ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_H ( x [ 0 ] ) - H ) y . append ( self . _calculate_H ( x [ 1 ] ) - H ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_H ( x [ i ] ) - H ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
2257	def allsame ( iterable , eq = operator . eq ) : iter_ = iter ( iterable ) try : first = next ( iter_ ) except StopIteration : return True return all ( eq ( first , item ) for item in iter_ )
5231	def create_folder ( path_name : str , is_file = False ) : path_sep = path_name . replace ( '\\' , '/' ) . split ( '/' ) for i in range ( 1 , len ( path_sep ) + ( 0 if is_file else 1 ) ) : cur_path = '/' . join ( path_sep [ : i ] ) if not os . path . exists ( cur_path ) : os . mkdir ( cur_path )
7828	def _new_from_xml ( cls , xmlnode ) : label = from_utf8 ( xmlnode . prop ( "label" ) ) child = xmlnode . children value = None for child in xml_element_ns_iter ( xmlnode . children , DATAFORM_NS ) : if child . name == "value" : value = from_utf8 ( child . getContent ( ) ) break if value is None : raise BadRequestProtocolError ( "No value in <option/> element" ) return cls ( value , label )
6560	def _bqm_from_1sat ( constraint ) : configurations = constraint . configurations num_configurations = len ( configurations ) bqm = dimod . BinaryQuadraticModel . empty ( constraint . vartype ) if num_configurations == 1 : val , = next ( iter ( configurations ) ) v , = constraint . variables bqm . add_variable ( v , - 1 if val > 0 else + 1 , vartype = dimod . SPIN ) else : bqm . add_variables_from ( ( v , 0.0 ) for v in constraint . variables ) return bqm
9851	def _export_python ( self , filename , ** kwargs ) : data = dict ( grid = self . grid , edges = self . edges , metadata = self . metadata ) with open ( filename , 'wb' ) as f : cPickle . dump ( data , f , cPickle . HIGHEST_PROTOCOL )
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
5670	def temporal_network ( gtfs , start_time_ut = None , end_time_ut = None , route_type = None ) : events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) events_df . drop ( 'to_seq' , 1 , inplace = True ) events_df . drop ( 'shape_id' , 1 , inplace = True ) events_df . drop ( 'duration' , 1 , inplace = True ) events_df . drop ( 'route_id' , 1 , inplace = True ) events_df . rename ( columns = { 'from_seq' : "seq" } , inplace = True ) return events_df
7819	def dispatch ( self , block = False , timeout = None ) : logger . debug ( " dispatching..." ) try : event = self . queue . get ( block , timeout ) except Queue . Empty : logger . debug ( " queue empty" ) return None try : logger . debug ( " event: {0!r}" . format ( event ) ) if event is QUIT : return QUIT handlers = list ( self . _handler_map [ None ] ) klass = event . __class__ if klass in self . _handler_map : handlers += self . _handler_map [ klass ] logger . debug ( " handlers: {0!r}" . format ( handlers ) ) handlers . sort ( key = lambda x : x [ 0 ] ) for dummy , handler in handlers : logger . debug ( u" passing the event to: {0!r}" . format ( handler ) ) result = handler ( event ) if isinstance ( result , Event ) : self . queue . put ( result ) elif result and event is not QUIT : return event return event finally : self . queue . task_done ( )
11818	def expected_utility ( a , s , U , mdp ) : "The expected utility of doing a in state s, according to the MDP and U." return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
11308	def get_image ( self , obj ) : if self . _meta . image_field : return getattr ( obj , self . _meta . image_field )
11696	def verify_editor ( self ) : powerful_editors = [ 'josm' , 'level0' , 'merkaartor' , 'qgis' , 'arcgis' , 'upload.py' , 'osmapi' , 'Services_OpenStreetMap' ] if self . editor is not None : for editor in powerful_editors : if editor in self . editor . lower ( ) : self . powerfull_editor = True break if 'iD' in self . editor : trusted_hosts = [ 'www.openstreetmap.org/id' , 'www.openstreetmap.org/edit' , 'improveosm.org' , 'strava.github.io/iD' , 'preview.ideditor.com/release' , 'preview.ideditor.com/master' , 'hey.mapbox.com/iD-internal' , 'projets.pavie.info/id-indoor' , 'maps.mapcat.com/edit' , 'id.softek.ir' ] if self . host . split ( '://' ) [ - 1 ] . strip ( '/' ) not in trusted_hosts : self . label_suspicious ( 'Unknown iD instance' ) else : self . powerfull_editor = True self . label_suspicious ( 'Software editor was not declared' )
9856	def get_data ( self , ** kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , ** data )
6889	def _parallel_bls_worker ( task ) : try : times , mags , errs = task [ : 3 ] magsarefluxes = task [ 3 ] minfreq , nfreq , stepsize = task [ 4 : 7 ] ndurations , mintransitduration , maxtransitduration = task [ 7 : 10 ] blsobjective , blsmethod , blsoversample = task [ 10 : ] frequencies = minfreq + nparange ( nfreq ) * stepsize periods = 1.0 / frequencies durations = nplinspace ( mintransitduration * periods . min ( ) , maxtransitduration * periods . min ( ) , ndurations ) if magsarefluxes : blsmodel = BoxLeastSquares ( times * u . day , mags * u . dimensionless_unscaled , dy = errs * u . dimensionless_unscaled ) else : blsmodel = BoxLeastSquares ( times * u . day , mags * u . mag , dy = errs * u . mag ) blsresult = blsmodel . power ( periods * u . day , durations * u . day , objective = blsobjective , method = blsmethod , oversample = blsoversample ) return { 'blsresult' : blsresult , 'blsmodel' : blsmodel , 'durations' : durations , 'power' : nparray ( blsresult . power ) } except Exception as e : LOGEXCEPTION ( 'BLS for frequency chunk: (%.6f, %.6f) failed.' % ( frequencies [ 0 ] , frequencies [ - 1 ] ) ) return { 'blsresult' : None , 'blsmodel' : None , 'durations' : durations , 'power' : nparray ( [ npnan for x in range ( nfreq ) ] ) , }
1036	def chain ( self , expanded_from ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = expanded_from )
3638	def clubConsumables ( self , fast = False ) : method = 'GET' url = 'club/consumables/development' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Club' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables' ) ] self . pin . send ( events , fast = fast ) events = [ self . pin . event ( 'page_view' , 'Club - Consumables - List View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'itemData' , ( ) ) ]
1802	def LEA ( cpu , dest , src ) : dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) )
12107	def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
10462	def doesmenuitemexist ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) return 1 except LdtpServerException : return 0
3431	def remove_reactions ( self , reactions , remove_orphans = False ) : if isinstance ( reactions , string_types ) or hasattr ( reactions , "id" ) : warn ( "need to pass in a list" ) reactions = [ reactions ] context = get_context ( self ) for reaction in reactions : try : reaction = self . reactions [ self . reactions . index ( reaction ) ] except ValueError : warn ( '%s not in %s' % ( reaction , self ) ) else : forward = reaction . forward_variable reverse = reaction . reverse_variable if context : obj_coef = reaction . objective_coefficient if obj_coef != 0 : context ( partial ( self . solver . objective . set_linear_coefficients , { forward : obj_coef , reverse : - obj_coef } ) ) context ( partial ( self . _populate_solver , [ reaction ] ) ) context ( partial ( setattr , reaction , '_model' , self ) ) context ( partial ( self . reactions . add , reaction ) ) self . remove_cons_vars ( [ forward , reverse ] ) self . reactions . remove ( reaction ) reaction . _model = None for met in reaction . _metabolites : if reaction in met . _reaction : met . _reaction . remove ( reaction ) if context : context ( partial ( met . _reaction . add , reaction ) ) if remove_orphans and len ( met . _reaction ) == 0 : self . remove_metabolites ( met ) for gene in reaction . _genes : if reaction in gene . _reaction : gene . _reaction . remove ( reaction ) if context : context ( partial ( gene . _reaction . add , reaction ) ) if remove_orphans and len ( gene . _reaction ) == 0 : self . genes . remove ( gene ) if context : context ( partial ( self . genes . add , gene ) ) associated_groups = self . get_associated_groups ( reaction ) for group in associated_groups : group . remove_members ( reaction )
454	def initialize_rnn_state ( state , feed_dict = None ) : if isinstance ( state , LSTMStateTuple ) : c = state . c . eval ( feed_dict = feed_dict ) h = state . h . eval ( feed_dict = feed_dict ) return c , h else : new_state = state . eval ( feed_dict = feed_dict ) return new_state
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
12247	def create_bucket ( self , * args , ** kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , ** kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
3864	async def send_message ( self , segments , image_file = None , image_id = None , image_user_id = None ) : async with self . _send_message_lock : if image_file : try : uploaded_image = await self . _client . upload_image ( image_file , return_uploaded_image = True ) except exceptions . NetworkError as e : logger . warning ( 'Failed to upload image: {}' . format ( e ) ) raise image_id = uploaded_image . image_id try : request = hangouts_pb2 . SendChatMessageRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , message_content = hangouts_pb2 . MessageContent ( segment = [ seg . serialize ( ) for seg in segments ] , ) , ) if image_id is not None : request . existing_media . photo . photo_id = image_id if image_user_id is not None : request . existing_media . photo . user_id = image_user_id request . existing_media . photo . is_custom_user_id = True await self . _client . send_chat_message ( request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to send message: {}' . format ( e ) ) raise
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
7463	def depthplot ( data , samples = None , dims = ( None , None ) , canvas = ( None , None ) , xmax = 50 , log = False , outprefix = None , use_maxdepth = False ) : if not samples : samples = data . samples . keys ( ) samples . sort ( ) subsamples = OrderedDict ( [ ( i , data . samples [ i ] ) for i in samples ] ) if any ( dims ) : print ( "userdims" ) else : if len ( subsamples ) <= 4 : dims = ( 1 , len ( subsamples ) ) else : dims = ( len ( subsamples ) / 4 , 4 ) if any ( canvas ) : print ( "usercanvas" ) canvas = toyplot . Canvas ( width = canvas [ 0 ] , height = canvas [ 1 ] ) else : canvas = toyplot . Canvas ( width = 200 * dims [ 1 ] , height = 150 * dims [ 0 ] ) for panel , sample in enumerate ( subsamples ) : statdat = subsamples [ sample ] . depths statdat = statdat [ statdat >= data . paramsdict [ "mindepth_statistical" ] ] if use_maxdepth : statdat = { i : j for ( i , j ) in statdat if i < data . paramsdict [ "maxdepth" ] } sdat = np . histogram ( statdat , range ( 50 ) ) statdat = subsamples [ sample ] . depths statdat = statdat [ statdat < data . paramsdict [ "mindepth_statistical" ] ] statdat = statdat [ statdat >= data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : statdat = statdat [ statdat < data . paramsdict [ "maxdepth" ] ] mdat = np . histogram ( statdat , range ( 50 ) ) tots = data . samples [ sample ] . depths tots = tots [ tots < data . paramsdict [ "mindepth_majrule" ] ] if use_maxdepth : tots = tots [ tots < data . paramsdict [ "maxdepth" ] ] edat = np . histogram ( tots , range ( 50 ) ) axes = canvas . cartesian ( grid = ( dims [ 0 ] , dims [ 1 ] , panel ) , gutter = 25 ) axes . x . domain . xmax = xmax axes . label . text = sample if log : axes . y . scale = "log" axes . bars ( sdat ) axes . bars ( edat ) axes . bars ( mdat ) if outprefix : toyplot . html . render ( canvas , fobj = outprefix + ".html" ) toyplot . svg . render ( canvas , fobj = outprefix + ".svg" )
11043	def listen ( self , reactor , endpoint_description ) : endpoint = serverFromString ( reactor , endpoint_description ) return endpoint . listen ( Site ( self . app . resource ( ) ) )
5534	def execute ( self , process_tile , raise_nodata = False ) : if self . config . mode not in [ "memory" , "continue" , "overwrite" ] : raise ValueError ( "process mode must be memory, continue or overwrite" ) if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif isinstance ( process_tile , BufferedTile ) : pass else : raise TypeError ( "process_tile must be tuple or BufferedTile" ) if process_tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( process_tile ) return self . _execute ( process_tile , raise_nodata = raise_nodata )
9816	def check ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) check = False if self . is_kubernetes : check = self . check_for_kubernetes ( ) elif self . is_docker_compose : check = self . check_for_docker_compose ( ) elif self . is_docker : check = self . check_for_docker ( ) elif self . is_heroku : check = self . check_for_heroku ( ) if not check : raise PolyaxonDeploymentConfigError ( 'Deployment `{}` is not valid' . format ( self . deployment_type ) )
13262	def task ( func , ** config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
6725	def get_or_create ( name = None , group = None , config = None , extra = 0 , verbose = 0 , backend_opts = None ) : require ( 'vm_type' , 'vm_group' ) backend_opts = backend_opts or { } verbose = int ( verbose ) extra = int ( extra ) if config : config_fn = common . find_template ( config ) config = yaml . load ( open ( config_fn ) ) env . update ( config ) env . vm_type = ( env . vm_type or '' ) . lower ( ) assert env . vm_type , 'No VM type specified.' group = group or env . vm_group assert group , 'No VM group specified.' ret = exists ( name = name , group = group ) if not extra and ret : if verbose : print ( 'VM %s:%s exists.' % ( name , group ) ) return ret today = datetime . date . today ( ) release = int ( '%i%02i%02i' % ( today . year , today . month , today . day ) ) if not name : existing_instances = list_instances ( group = group , release = release , verbose = verbose ) name = env . vm_name_template . format ( index = len ( existing_instances ) + 1 ) if env . vm_type == EC2 : return get_or_create_ec2_instance ( name = name , group = group , release = release , verbose = verbose , backend_opts = backend_opts ) else : raise NotImplementedError
9308	def get_canonical_headers ( cls , req , include = None ) : if include is None : include = cls . default_include_headers include = [ x . lower ( ) for x in include ] headers = req . headers . copy ( ) if 'host' not in headers : headers [ 'host' ] = urlparse ( req . url ) . netloc . split ( ':' ) [ 0 ] cano_headers_dict = { } for hdr , val in headers . items ( ) : hdr = hdr . strip ( ) . lower ( ) val = cls . amz_norm_whitespace ( val ) . strip ( ) if ( hdr in include or '*' in include or ( 'x-amz-*' in include and hdr . startswith ( 'x-amz-' ) and not hdr == 'x-amz-client-context' ) ) : vals = cano_headers_dict . setdefault ( hdr , [ ] ) vals . append ( val ) cano_headers = '' signed_headers_list = [ ] for hdr in sorted ( cano_headers_dict ) : vals = cano_headers_dict [ hdr ] val = ',' . join ( sorted ( vals ) ) cano_headers += '{}:{}\n' . format ( hdr , val ) signed_headers_list . append ( hdr ) signed_headers = ';' . join ( signed_headers_list ) return ( cano_headers , signed_headers )
13075	def create_blueprint ( self ) : self . register_plugins ( ) self . blueprint = Blueprint ( self . name , "nemo" , url_prefix = self . prefix , template_folder = self . template_folder , static_folder = self . static_folder , static_url_path = self . static_url_path ) for url , name , methods , instance in self . _urls : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) , methods = methods ) for url , name , methods , instance in self . _semantic_url : self . blueprint . add_url_rule ( url , view_func = self . view_maker ( name , instance ) , endpoint = _plugin_endpoint_rename ( name , instance ) + "_semantic" , methods = methods ) self . register_assets ( ) self . register_filters ( ) self . __templates_namespaces__ . extend ( self . __instance_templates__ ) for namespace , directory in self . __templates_namespaces__ [ : : - 1 ] : if namespace not in self . __template_loader__ : self . __template_loader__ [ namespace ] = [ ] self . __template_loader__ [ namespace ] . append ( jinja2 . FileSystemLoader ( op . abspath ( directory ) ) ) self . blueprint . jinja_loader = jinja2 . PrefixLoader ( { namespace : jinja2 . ChoiceLoader ( paths ) for namespace , paths in self . __template_loader__ . items ( ) } , "::" ) if self . cache is not None : for func , instance in self . cached : setattr ( instance , func . __name__ , self . cache . memoize ( ) ( func ) ) return self . blueprint
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
4533	def set_color_list ( self , color_list , offset = 0 ) : if not len ( color_list ) : return color_list = make . colors ( color_list ) size = len ( self . _colors ) - offset if len ( color_list ) > size : color_list = color_list [ : size ] self . _colors [ offset : offset + len ( color_list ) ] = color_list
1887	def emulate ( self , instruction ) : while True : self . reset ( ) for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
7355	def seq_to_str ( obj , sep = "," ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list , tuple ) ) : return sep . join ( [ str ( x ) for x in obj ] ) else : return str ( obj )
11681	def send ( self , command , timeout = 5 ) : logger . info ( u'Sending %s' % command ) _ , writable , __ = select . select ( [ ] , [ self . sock ] , [ ] , timeout ) if not writable : raise SendTimeoutError ( ) writable [ 0 ] . sendall ( command + '\n' )
13830	def _url ( self ) : if self . _api_arg : mypart = str ( self . _api_arg ) else : mypart = self . _name if self . _parent : return '/' . join ( filter ( None , [ self . _parent . _url , mypart ] ) ) else : return mypart
5253	def assemble_one ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : try : instruction_table = instruction_tables [ fork ] asmcode = asmcode . strip ( ) . split ( ' ' ) instr = instruction_table [ asmcode [ 0 ] . upper ( ) ] if pc : instr . pc = pc if instr . operand_size > 0 : assert len ( asmcode ) == 2 instr . operand = int ( asmcode [ 1 ] , 0 ) return instr except : raise AssembleError ( "Something wrong at pc %d" % pc )
11682	def _readline ( self ) : line = '' while 1 : readable , _ , __ = select . select ( [ self . sock ] , [ ] , [ ] , 0.5 ) if self . _stop : break if not readable : continue data = readable [ 0 ] . recv ( 1 ) if data == '\n' : break line += unicode ( data , self . encoding ) return line
12736	def are_connected ( self , body_a , body_b ) : return bool ( ode . areConnected ( self . get_body ( body_a ) . ode_body , self . get_body ( body_b ) . ode_body ) )
5075	def is_course_run_enrollable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) end = parse_datetime_handle_invalid ( course_run . get ( 'end' ) ) enrollment_start = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_start' ) ) enrollment_end = parse_datetime_handle_invalid ( course_run . get ( 'enrollment_end' ) ) return ( not end or end > now ) and ( not enrollment_start or enrollment_start < now ) and ( not enrollment_end or enrollment_end > now )
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
7269	def attribute ( * args , ** kw ) : return operator ( kind = Operator . Type . ATTRIBUTE , * args , ** kw )
6231	def apply_mesh_programs ( self , mesh_programs = None ) : if not mesh_programs : mesh_programs = [ ColorProgram ( ) , TextureProgram ( ) , FallbackProgram ( ) ] for mesh in self . meshes : for mp in mesh_programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , MeshProgram ) : mesh . mesh_program = mp break else : raise ValueError ( "apply() must return a MeshProgram instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh_program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
8829	def security_group_rule_update ( context , rule , ** kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
8239	def triad ( clr , angle = 120 ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . rotate_ryb ( angle ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( - angle ) . lighten ( 0.1 ) ) return colors
1087	def concat ( a , b ) : "Same as a + b, for a and b sequences." if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
11202	def strip_comments ( string , comment_symbols = frozenset ( ( '#' , '//' ) ) ) : lines = string . splitlines ( ) for k in range ( len ( lines ) ) : for symbol in comment_symbols : lines [ k ] = strip_comment_line_with_symbol ( lines [ k ] , start = symbol ) return '\n' . join ( lines )
5942	def _get_gmx_docs ( self ) : if self . _doc_cache is not None : return self . _doc_cache try : logging . disable ( logging . CRITICAL ) rc , header , docs = self . run ( 'h' , stdout = PIPE , stderr = PIPE , use_input = False ) except : logging . critical ( "Invoking command {0} failed when determining its doc string. Proceed with caution" . format ( self . command_name ) ) self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache finally : logging . disable ( logging . NOTSET ) m = re . match ( self . doc_pattern , docs , re . DOTALL ) if m is None : m = re . match ( self . doc_pattern , header , re . DOTALL ) if m is None : self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache self . _doc_cache = m . group ( 'DOCS' ) return self . _doc_cache
3193	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
2445	def reset_package ( self ) : self . package_set = False self . package_vers_set = False self . package_file_name_set = False self . package_supplier_set = False self . package_originator_set = False self . package_down_location_set = False self . package_home_set = False self . package_verif_set = False self . package_chk_sum_set = False self . package_source_info_set = False self . package_conc_lics_set = False self . package_license_declared_set = False self . package_license_comment_set = False self . package_cr_text_set = False self . package_summary_set = False self . package_desc_set = False
4848	def _partition_items ( self , channel_metadata_item_map ) : items_to_create = { } items_to_update = { } items_to_delete = { } transmission_map = { } export_content_ids = channel_metadata_item_map . keys ( ) for transmission in self . _get_transmissions ( ) : transmission_map [ transmission . content_id ] = transmission if transmission . content_id not in export_content_ids : items_to_delete [ transmission . content_id ] = transmission . channel_metadata for item in channel_metadata_item_map . values ( ) : content_id = item . content_id channel_metadata = item . channel_metadata transmitted_item = transmission_map . get ( content_id , None ) if transmitted_item is not None : if diff ( channel_metadata , transmitted_item . channel_metadata ) : items_to_update [ content_id ] = channel_metadata else : items_to_create [ content_id ] = channel_metadata LOGGER . info ( 'Preparing to transmit creation of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_create ) , self . enterprise_configuration , items_to_create . keys ( ) , ) LOGGER . info ( 'Preparing to transmit update of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_update ) , self . enterprise_configuration , items_to_update . keys ( ) , ) LOGGER . info ( 'Preparing to transmit deletion of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_delete ) , self . enterprise_configuration , items_to_delete . keys ( ) , ) return items_to_create , items_to_update , items_to_delete , transmission_map
11723	def init_app ( self , app , ** kwargs ) : self . init_config ( app ) self . limiter = Limiter ( app , key_func = get_ipaddr ) if app . config [ 'APP_ENABLE_SECURE_HEADERS' ] : self . talisman = Talisman ( app , ** app . config . get ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) ) if app . config [ 'APP_HEALTH_BLUEPRINT_ENABLED' ] : blueprint = Blueprint ( 'invenio_app_ping' , __name__ ) @ blueprint . route ( '/ping' ) def ping ( ) : return 'OK' ping . talisman_view_options = { 'force_https' : False } app . register_blueprint ( blueprint ) requestid_header = app . config . get ( 'APP_REQUESTID_HEADER' ) if requestid_header : @ app . before_request def set_request_id ( ) : request_id = request . headers . get ( requestid_header ) if request_id : g . request_id = request_id [ : 200 ] try : from flask_debugtoolbar import DebugToolbarExtension app . extensions [ 'flask-debugtoolbar' ] = DebugToolbarExtension ( app ) except ImportError : app . logger . debug ( 'Flask-DebugToolbar extension not installed.' ) app . extensions [ 'invenio-app' ] = self
10185	def _events_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
4122	def centerdc_2_twosided ( data ) : N = len ( data ) newpsd = np . concatenate ( ( data [ N // 2 : ] , ( cshift ( data [ 0 : N // 2 ] , - 1 ) ) ) ) return newpsd
11229	def after ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : for i in gen : if i >= dt : return i else : for i in gen : if i > dt : return i return None
13887	def CreateDirectory ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) if _UrlIsLocal ( directory_url ) : if not os . path . exists ( directory ) : os . makedirs ( directory ) return directory elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
6046	def array_2d_from_array_1d ( self , padded_array_1d ) : padded_array_2d = self . map_to_2d_keep_padded ( padded_array_1d ) pad_size_0 = self . mask . shape [ 0 ] - self . image_shape [ 0 ] pad_size_1 = self . mask . shape [ 1 ] - self . image_shape [ 1 ] return ( padded_array_2d [ pad_size_0 // 2 : self . mask . shape [ 0 ] - pad_size_0 // 2 , pad_size_1 // 2 : self . mask . shape [ 1 ] - pad_size_1 // 2 ] )
3826	async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
3691	def solve_T ( self , P , V , quick = True ) : r if self . S2 == 0 : self . m = self . S1 return SRK . solve_T ( self , P , V , quick = quick ) else : Tc , a , b , S1 , S2 = self . Tc , self . a , self . b , self . S1 , self . S2 if quick : x2 = R / ( V - b ) x3 = ( V * ( V + b ) ) def to_solve ( T ) : x0 = ( T / Tc ) ** 0.5 x1 = x0 - 1. return ( x2 * T - a * ( S1 * x1 + S2 * x1 / x0 - 1. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( S1 * ( - sqrt ( T / Tc ) + 1 ) + S2 * ( - sqrt ( T / Tc ) + 1 ) / sqrt ( T / Tc ) + 1 ) ** 2 / ( V * ( V + b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
2174	def token_from_fragment ( self , authorization_response ) : self . _client . parse_request_uri_response ( authorization_response , state = self . _state ) self . token = self . _client . token return self . token
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
250	def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns
11520	def search ( self , search , token = None ) : parameters = dict ( ) parameters [ 'search' ] = search if token : parameters [ 'token' ] = token response = self . request ( 'midas.resource.search' , parameters ) return response
216	def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( "latin-1" ) set_value = value . encode ( "latin-1" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( "latin-1" ) self . _list . append ( ( set_key , set_value ) ) return value
10043	def create_blueprint ( endpoints ) : from invenio_records_ui . views import create_url_rule blueprint = Blueprint ( 'invenio_deposit_ui' , __name__ , static_folder = '../static' , template_folder = '../templates' , url_prefix = '' , ) @ blueprint . errorhandler ( PIDDeletedError ) def tombstone_errorhandler ( error ) : return render_template ( current_app . config [ 'DEPOSIT_UI_TOMBSTONE_TEMPLATE' ] , pid = error . pid , record = error . record or { } , ) , 410 for endpoint , options in ( endpoints or { } ) . items ( ) : options = deepcopy ( options ) options . pop ( 'jsonschema' , None ) options . pop ( 'schemaform' , None ) blueprint . add_url_rule ( ** create_url_rule ( endpoint , ** options ) ) @ blueprint . route ( '/deposit' ) @ login_required def index ( ) : return render_template ( current_app . config [ 'DEPOSIT_UI_INDEX_TEMPLATE' ] ) @ blueprint . route ( '/deposit/new' ) @ login_required def new ( ) : deposit_type = request . values . get ( 'type' ) return render_template ( current_app . config [ 'DEPOSIT_UI_NEW_TEMPLATE' ] , record = { '_deposit' : { 'id' : None } } , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , ) return blueprint
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
1758	def _raw_read ( self , where : int , size = 1 ) -> bytes : map = self . memory . map_containing ( where ) start = map . _get_offset ( where ) mapType = type ( map ) if mapType is FileMap : end = map . _get_offset ( where + size ) if end > map . _mapped_size : logger . warning ( f"Missing {end - map._mapped_size} bytes at the end of {map._filename}" ) raw_data = map . _data [ map . _get_offset ( where ) : min ( end , map . _mapped_size ) ] if len ( raw_data ) < end : raw_data += b'\x00' * ( end - len ( raw_data ) ) data = b'' for offset in sorted ( map . _overlay . keys ( ) ) : data += raw_data [ len ( data ) : offset ] data += map . _overlay [ offset ] data += raw_data [ len ( data ) : ] elif mapType is AnonMap : data = bytes ( map . _data [ start : start + size ] ) else : data = b'' . join ( self . memory [ where : where + size ] ) assert len ( data ) == size , 'Raw read resulted in wrong data read which should never happen' return data
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
6032	def grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_grid = RegularGrid . from_mask ( mask ) sub_grid = SubGrid . from_mask_and_sub_grid_size ( mask , sub_grid_size ) blurring_grid = RegularGrid . blurring_grid_from_mask_and_psf_shape ( mask , psf_shape ) return GridStack ( regular_grid , sub_grid , blurring_grid )
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
5822	def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { 'service_id' : self . attrs [ 'service_id' ] , 'version' : self . attrs [ 'number' ] , 'name' : name , 'content' : content , } vcl . save ( ) return vcl
1961	def sys_fsync ( self , fd ) : ret = 0 try : self . files [ fd ] . sync ( ) except IndexError : ret = - errno . EBADF except FdError : ret = - errno . EINVAL return ret
8243	def guess_name ( clr ) : clr = Color ( clr ) if clr . is_transparent : return "transparent" if clr . is_black : return "black" if clr . is_white : return "white" if clr . is_black : return "black" for name in named_colors : try : r , g , b = named_colors [ name ] except : continue if r == clr . r and g == clr . g and b == clr . b : return name for shade in shades : if clr in shade : return shade . name + " " + clr . nearest_hue ( ) break return clr . nearest_hue ( )
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice print ( self . _chunksize ) rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break fillsets [ : ] = tmpr del tmpr
2082	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : uj_res = get_resource ( 'unified_job' ) query_params = ( ( 'unified_job_node__workflow_job' , pk ) , ( 'order_by' , 'finished' ) , ( 'status__in' , 'successful,failed,error' ) ) jobs_list = uj_res . list ( all_pages = True , query = query_params ) if jobs_list [ 'count' ] == 0 : return '' return_content = ResSubcommand ( uj_res ) . _format_human ( jobs_list ) lines = return_content . split ( '\n' ) if not full : lines = lines [ : - 1 ] N = len ( lines ) start_range = start_line if start_line is None : start_range = 0 elif start_line > N : start_range = N end_range = end_line if end_line is None or end_line > N : end_range = N lines = lines [ start_range : end_range ] return_content = '\n' . join ( lines ) if len ( lines ) > 0 : return_content += '\n' return return_content
1063	def readheaders ( self ) : self . dict = { } self . unixfrom = '' self . headers = lst = [ ] self . status = '' headerseen = "" firstline = 1 startofline = unread = tell = None if hasattr ( self . fp , 'unread' ) : unread = self . fp . unread elif self . seekable : tell = self . fp . tell while 1 : if tell : try : startofline = tell ( ) except IOError : startofline = tell = None self . seekable = 0 line = self . fp . readline ( ) if not line : self . status = 'EOF in headers' break if firstline and line . startswith ( 'From ' ) : self . unixfrom = self . unixfrom + line continue firstline = 0 if headerseen and line [ 0 ] in ' \t' : lst . append ( line ) x = ( self . dict [ headerseen ] + "\n " + line . strip ( ) ) self . dict [ headerseen ] = x . strip ( ) continue elif self . iscomment ( line ) : continue elif self . islast ( line ) : break headerseen = self . isheader ( line ) if headerseen : lst . append ( line ) self . dict [ headerseen ] = line [ len ( headerseen ) + 1 : ] . strip ( ) continue elif headerseen is not None : continue else : if not self . dict : self . status = 'No headers' else : self . status = 'Non-header line where header expected' if unread : unread ( line ) elif tell : self . fp . seek ( startofline ) else : self . status = self . status + '; bad seek' break
12459	def main ( * args ) : r with disable_error_handler ( ) : args = parse_args ( args or sys . argv [ 1 : ] ) config = read_config ( args . config , args ) if config is None : return True bootstrap = config [ __script__ ] if not check_pre_requirements ( bootstrap [ 'pre_requirements' ] ) : return True env_args = prepare_args ( config [ 'virtualenv' ] , bootstrap ) if not create_env ( bootstrap [ 'env' ] , env_args , bootstrap [ 'recreate' ] , bootstrap [ 'ignore_activated' ] , bootstrap [ 'quiet' ] ) : return True pip_args = prepare_args ( config [ 'pip' ] , bootstrap ) if not install ( bootstrap [ 'env' ] , bootstrap [ 'requirements' ] , pip_args , bootstrap [ 'ignore_activated' ] , bootstrap [ 'install_dev_requirements' ] , bootstrap [ 'quiet' ] ) : return True run_hook ( bootstrap [ 'hook' ] , bootstrap , bootstrap [ 'quiet' ] ) if not bootstrap [ 'quiet' ] : print_message ( 'All OK!' ) return False
4047	def _totals ( self , query ) : self . add_parameters ( limit = 1 ) query = self . _build_query ( query ) self . _retrieve_data ( query ) self . url_params = None return int ( self . request . headers [ "Total-Results" ] )
4306	def play ( args ) : if args [ 0 ] . lower ( ) != "play" : args . insert ( 0 , "play" ) else : args [ 0 ] = "play" try : logger . info ( "Executing: %s" , " " . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) status = process_handle . wait ( ) if process_handle . stderr is not None : logger . info ( process_handle . stderr ) if status == 0 : return True else : logger . info ( "Play returned with error code %s" , status ) return False except OSError as error_msg : logger . error ( "OSError: Play failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return False
5632	def unindent ( lines ) : try : indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
10088	def patch ( self , * args , ** kwargs ) : return super ( Deposit , self ) . patch ( * args , ** kwargs )
7517	def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : catg [ 0 ] += 1 elif ncol [ idx ] == 65 : catg [ 1 ] += 1 elif ncol [ idx ] == 84 : catg [ 2 ] += 1 elif ncol [ idx ] == 71 : catg [ 3 ] += 1 elif ncol [ idx ] == 82 : catg [ 1 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 75 : catg [ 2 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 83 : catg [ 0 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 89 : catg [ 0 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 87 : catg [ 1 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 77 : catg [ 0 ] += 1 catg [ 1 ] += 1 catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
9570	async def _get_response ( self , message ) : view = self . discovery_view ( message ) if not view : return if inspect . iscoroutinefunction ( view ) : response = await view ( message ) else : response = view ( message ) return self . prepare_response ( response , message )
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
1772	def push ( cpu , value , size ) : assert size in ( 8 , 16 , cpu . address_bit_size ) cpu . STACK = cpu . STACK - size // 8 base , _ , _ = cpu . get_descriptor ( cpu . read_register ( 'SS' ) ) address = cpu . STACK + base cpu . write_int ( address , value , size )
11623	def _equivalent ( self , char , prev , next , implicitA ) : result = [ ] if char . isVowel == False : result . append ( char . chr ) if char . isConsonant and ( ( next is not None and next . isConsonant ) or next is None ) : result . append ( DevanagariCharacter . _VIRAMA ) else : if prev is None or prev . isConsonant == False : result . append ( char . chr ) else : if char . _dependentVowel is not None : result . append ( char . _dependentVowel ) return result
1402	def setTopologyInfo ( self , topology ) : if not topology . execution_state : Log . info ( "No execution state found for: " + topology . name ) return Log . info ( "Setting topology info for topology: " + topology . name ) has_physical_plan = True if not topology . physical_plan : has_physical_plan = False Log . info ( "Setting topology info for topology: " + topology . name ) has_packing_plan = True if not topology . packing_plan : has_packing_plan = False has_tmaster_location = True if not topology . tmaster : has_tmaster_location = False has_scheduler_location = True if not topology . scheduler_location : has_scheduler_location = False topologyInfo = { "name" : topology . name , "id" : topology . id , "logical_plan" : None , "physical_plan" : None , "packing_plan" : None , "execution_state" : None , "tmaster_location" : None , "scheduler_location" : None , } executionState = self . extract_execution_state ( topology ) executionState [ "has_physical_plan" ] = has_physical_plan executionState [ "has_packing_plan" ] = has_packing_plan executionState [ "has_tmaster_location" ] = has_tmaster_location executionState [ "has_scheduler_location" ] = has_scheduler_location executionState [ "status" ] = topology . get_status ( ) topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) topologyInfo [ "execution_state" ] = executionState topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
4359	def spawn ( self , fn , * args , ** kwargs ) : log . debug ( "Spawning sub-Socket Greenlet: %s" % fn . __name__ ) job = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( job ) return job
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
8747	def create_scalingip ( context , content ) : LOG . info ( 'create_scalingip for tenant %s and body %s' , context . tenant_id , content ) network_id = content . get ( 'scaling_network_id' ) ip_address = content . get ( 'scaling_ip_address' ) requested_ports = content . get ( 'ports' , [ ] ) network = _get_network ( context , network_id ) port_fixed_ips = { } for req_port in requested_ports : port = _get_port ( context , req_port [ 'port_id' ] ) fixed_ip = _get_fixed_ip ( context , req_port . get ( 'fixed_ip_address' ) , port ) port_fixed_ips [ port . id ] = { "port" : port , "fixed_ip" : fixed_ip } scip = _allocate_ip ( context , network , None , ip_address , ip_types . SCALING ) _create_flip ( context , scip , port_fixed_ips ) return v . _make_scaling_ip_dict ( scip )
1419	def get_scheduler_location ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_scheduler_location_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
415	def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
2076	def main ( loader , name ) : scores = [ ] raw_scores_ds = { } X , y , mapping = loader ( ) clf = linear_model . LogisticRegression ( solver = 'lbfgs' , multi_class = 'auto' , max_iter = 200 , random_state = 0 ) encoders = ( set ( category_encoders . __all__ ) - { 'WOEEncoder' } ) for encoder_name in encoders : encoder = getattr ( category_encoders , encoder_name ) start_time = time . time ( ) score , stds , raw_scores , dim = score_models ( clf , X , y , encoder ) scores . append ( [ encoder_name , name , dim , score , stds , time . time ( ) - start_time ] ) raw_scores_ds [ encoder_name ] = raw_scores gc . collect ( ) results = pd . DataFrame ( scores , columns = [ 'Encoding' , 'Dataset' , 'Dimensionality' , 'Avg. Score' , 'Score StDev' , 'Elapsed Time' ] ) raw = pd . DataFrame . from_dict ( raw_scores_ds ) ax = raw . plot ( kind = 'box' , return_type = 'axes' ) plt . title ( 'Scores for Encodings on %s Dataset' % ( name , ) ) plt . ylabel ( 'Score (higher is better)' ) for tick in ax . get_xticklabels ( ) : tick . set_rotation ( 90 ) plt . grid ( ) plt . tight_layout ( ) plt . show ( ) return results , raw
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
6744	def install_config ( local_path = None , remote_path = None , render = True , extra = None , formatter = None ) : local_path = find_template ( local_path ) if render : extra = extra or { } local_path = render_to_file ( template = local_path , extra = extra , formatter = formatter ) put_or_dryrun ( local_path = local_path , remote_path = remote_path , use_sudo = True )
8794	def set_all ( self , model , ** tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except TagValidationError as e : raise n_exc . BadRequest ( resource = "tags" , msg = "%s" % ( e . message ) )
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
9012	def _start ( self ) : self . _instruction_library = self . _spec . new_default_instructions ( ) self . _as_instruction = self . _instruction_library . as_instruction self . _id_cache = { } self . _pattern_set = None self . _inheritance_todos = [ ] self . _instruction_todos = [ ]
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
8584	def attach_volume ( self , datacenter_id , server_id , volume_id ) : data = '{ "id": "' + volume_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/volumes' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
6993	def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( flareparams , times , mags , errs ) return ( mags - modelmags ) / errs
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
96	def quokka_keypoints ( size = None , extract = None ) : from imgaug . augmentables . kps import Keypoint , KeypointsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) keypoints = [ ] for kp_dict in json_dict [ "keypoints" ] : keypoints . append ( Keypoint ( x = kp_dict [ "x" ] - left , y = kp_dict [ "y" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) kpsoi = KeypointsOnImage ( keypoints , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) kpsoi = kpsoi . on ( shape_resized ) return kpsoi
3382	def step ( sampler , x , delta , fraction = None , tries = 0 ) : prob = sampler . problem valid = ( ( np . abs ( delta ) > sampler . feasibility_tol ) & np . logical_not ( prob . variable_fixed ) ) valphas = ( ( 1.0 - sampler . bounds_tol ) * prob . variable_bounds - x ) [ : , valid ] valphas = ( valphas / delta [ valid ] ) . flatten ( ) if prob . bounds . shape [ 0 ] > 0 : ineqs = prob . inequalities . dot ( delta ) valid = np . abs ( ineqs ) > sampler . feasibility_tol balphas = ( ( 1.0 - sampler . bounds_tol ) * prob . bounds - prob . inequalities . dot ( x ) ) [ : , valid ] balphas = ( balphas / ineqs [ valid ] ) . flatten ( ) alphas = np . hstack ( [ valphas , balphas ] ) else : alphas = valphas pos_alphas = alphas [ alphas > 0.0 ] neg_alphas = alphas [ alphas <= 0.0 ] alpha_range = np . array ( [ neg_alphas . max ( ) if len ( neg_alphas ) > 0 else 0 , pos_alphas . min ( ) if len ( pos_alphas ) > 0 else 0 ] ) if fraction : alpha = alpha_range [ 0 ] + fraction * ( alpha_range [ 1 ] - alpha_range [ 0 ] ) else : alpha = np . random . uniform ( alpha_range [ 0 ] , alpha_range [ 1 ] ) p = x + alpha * delta if ( np . any ( sampler . _bounds_dist ( p ) < - sampler . bounds_tol ) or np . abs ( np . abs ( alpha_range ) . max ( ) * delta ) . max ( ) < sampler . bounds_tol ) : if tries > MAX_TRIES : raise RuntimeError ( "Can not escape sampling region, model seems" " numerically unstable :( Reporting the " "model to " "https://github.com/opencobra/cobrapy/issues " "will help us to fix this :)" ) LOGGER . info ( "found bounds infeasibility in sample, " "resetting to center" ) newdir = sampler . warmup [ np . random . randint ( sampler . n_warmup ) ] sampler . retries += 1 return step ( sampler , sampler . center , newdir - sampler . center , None , tries + 1 ) return p
1832	def JC ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF , target . read ( ) , cpu . PC )
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : stringelems = _squeeze ( filterstring ) . lower ( ) stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] wordset = set ( stringwords2 ) allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) if len ( validatecheck ) > 0 : LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( ** self . get_formset_kwargs ( ) )
7950	def send_element ( self , element ) : with self . lock : if self . _eof or self . _socket is None or not self . _serializer : logger . debug ( "Dropping element: {0}" . format ( element_to_unicode ( element ) ) ) return data = self . _serializer . emit_stanza ( element ) self . _write ( data . encode ( "utf-8" ) )
4571	def adapt_animation_layout ( animation ) : layout = animation . layout required = getattr ( animation , 'LAYOUT_CLASS' , None ) if not required or isinstance ( layout , required ) : return msg = LAYOUT_WARNING % ( type ( animation ) . __name__ , required . __name__ , type ( layout ) . __name__ ) setter = layout . set adaptor = None if required is strip . Strip : if isinstance ( layout , matrix . Matrix ) : width = layout . width def adaptor ( pixel , color = None ) : y , x = divmod ( pixel , width ) setter ( x , y , color or BLACK ) elif isinstance ( layout , cube . Cube ) : lx , ly = layout . x , layout . y def adaptor ( pixel , color = None ) : yz , x = divmod ( pixel , lx ) z , y = divmod ( yz , ly ) setter ( x , y , z , color or BLACK ) elif isinstance ( layout , circle . Circle ) : def adaptor ( pixel , color = None ) : layout . _set_base ( pixel , color or BLACK ) elif required is matrix . Matrix : if isinstance ( layout , strip . Strip ) : width = animation . width def adaptor ( x , y , color = None ) : setter ( x + y * width , color or BLACK ) if not adaptor : raise ValueError ( msg ) log . warning ( msg ) animation . layout . set = adaptor
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" del context del frames return modname , funcname , lineno
6768	def update ( self ) : packager = self . packager if packager == APT : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update' ) elif packager == YUM : self . sudo ( 'yum update' ) else : raise Exception ( 'Unknown packager: %s' % ( packager , ) )
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
10796	def translate_fourier ( image , dx ) : N = image . shape [ 0 ] f = 2 * np . pi * np . fft . fftfreq ( N ) kx , ky , kz = np . meshgrid ( * ( f , ) * 3 , indexing = 'ij' ) kv = np . array ( [ kx , ky , kz ] ) . T q = np . fft . fftn ( image ) * np . exp ( - 1.j * ( kv * dx ) . sum ( axis = - 1 ) ) . T return np . real ( np . fft . ifftn ( q ) )
1097	def decode ( in_file , out_file = None , mode = None , quiet = 0 ) : opened_files = [ ] if in_file == '-' : in_file = sys . stdin elif isinstance ( in_file , basestring ) : in_file = open ( in_file ) opened_files . append ( in_file ) try : while True : hdr = in_file . readline ( ) if not hdr : raise Error ( 'No valid begin line found in input file' ) if not hdr . startswith ( 'begin' ) : continue hdrfields = hdr . split ( ' ' , 2 ) if len ( hdrfields ) == 3 and hdrfields [ 0 ] == 'begin' : try : int ( hdrfields [ 1 ] , 8 ) break except ValueError : pass if out_file is None : out_file = hdrfields [ 2 ] . rstrip ( ) if os . path . exists ( out_file ) : raise Error ( 'Cannot overwrite existing file: %s' % out_file ) if mode is None : mode = int ( hdrfields [ 1 ] , 8 ) if out_file == '-' : out_file = sys . stdout elif isinstance ( out_file , basestring ) : fp = open ( out_file , 'wb' ) try : os . path . chmod ( out_file , mode ) except AttributeError : pass out_file = fp opened_files . append ( out_file ) s = in_file . readline ( ) while s and s . strip ( ) != 'end' : try : data = binascii . a2b_uu ( s ) except binascii . Error , v : nbytes = ( ( ( ord ( s [ 0 ] ) - 32 ) & 63 ) * 4 + 5 ) // 3 data = binascii . a2b_uu ( s [ : nbytes ] ) if not quiet : sys . stderr . write ( "Warning: %s\n" % v ) out_file . write ( data ) s = in_file . readline ( ) if not s : raise Error ( 'Truncated input file' ) finally : for f in opened_files : f . close ( )
8831	def send ( self , s ) : self . _socket . send ( s . encode ( ) ) return self . read ( )
4611	def block_time ( self , block_num ) : return self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( )
3918	def _handle_event ( self , conv_event ) : if not self . _is_scrolling : self . set_focus ( conv_event . id_ ) else : self . _modified ( )
9095	def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
5322	def get_humidity ( self , sensors = None ) : _sensors = sensors if _sensors is None : _sensors = list ( range ( 0 , self . _sensor_count ) ) if not set ( _sensors ) . issubset ( list ( range ( 0 , self . _sensor_count ) ) ) : raise ValueError ( 'Some or all of the sensors in the list %s are out of range ' 'given a sensor_count of %d. Valid range: %s' % ( _sensors , self . _sensor_count , list ( range ( 0 , self . _sensor_count ) ) , ) ) data = self . get_data ( ) data = data [ 'humidity_data' ] results = { } for sensor in _sensors : offset = self . lookup_humidity_offset ( sensor ) if offset is None : continue humidity = ( struct . unpack_from ( '>H' , data , offset ) [ 0 ] * 32 ) / 1000.0 results [ sensor ] = { 'ports' : self . get_ports ( ) , 'bus' : self . get_bus ( ) , 'sensor' : sensor , 'humidity_pc' : humidity , } return results
4752	def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
1852	def SAR ( cpu , dest , src ) : OperandSize = dest . size countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] count = src . read ( ) & countMask value = dest . read ( ) res = Operators . SAR ( OperandSize , value , Operators . ZEXTEND ( count , OperandSize ) ) dest . write ( res ) SIGN_MASK = ( 1 << ( OperandSize - 1 ) ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( Operators . AND ( count != 0 , count <= OperandSize ) , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : if count > OperandSize : count = OperandSize cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count == 1 , False , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
912	def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps
11860	def all_events ( vars , bn , e ) : "Yield every way of extending e with values for all vars." if not vars : yield e else : X , rest = vars [ 0 ] , vars [ 1 : ] for e1 in all_events ( rest , bn , e ) : for x in bn . variable_values ( X ) : yield extend ( e1 , X , x )
5372	def _file_exists_in_gcs ( gcs_file_path , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , object_name = gcs_file_path [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . get ( bucket = bucket_name , object = object_name , projection = 'noAcl' ) try : request . execute ( ) return True except errors . HttpError : return False
12783	def speak ( self , message ) : campfire = self . get_campfire ( ) if not isinstance ( message , Message ) : message = Message ( campfire , message ) result = self . _connection . post ( "room/%s/speak" % self . id , { "message" : message . get_data ( ) } , parse_data = True , key = "message" ) if result [ "success" ] : return Message ( campfire , result [ "data" ] ) return result [ "success" ]
5866	def organization_data_is_valid ( organization_data ) : if organization_data is None : return False if 'id' in organization_data and not organization_data . get ( 'id' ) : return False if 'name' in organization_data and not organization_data . get ( 'name' ) : return False return True
4844	def is_course_in_catalog ( self , catalog_id , course_id ) : try : course_run_id = str ( CourseKey . from_string ( course_id ) ) except InvalidKeyError : course_run_id = None endpoint = self . client . catalogs ( catalog_id ) . contains if course_run_id : resp = endpoint . get ( course_run_id = course_run_id ) else : resp = endpoint . get ( course_id = course_id ) return resp . get ( 'courses' , { } ) . get ( course_id , False )
12209	def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
3563	def find_descriptor ( self , uuid ) : for desc in self . list_descriptors ( ) : if desc . uuid == uuid : return desc return None
9138	def drop_all ( self , check_first : bool = True ) : self . _metadata . drop_all ( self . engine , checkfirst = check_first ) self . _store_drop ( )
8044	def leapfrog ( self , kind , value = None ) : while self . current is not None : if self . current . kind == kind and ( value is None or self . current . value == value ) : self . consume ( kind ) return self . stream . move ( )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : return L_ANY if ( code & ( code - 1 ) ) != 0 : return L_ANY return code
3596	def bulkDetails ( self , packageNames ) : params = { 'au' : '1' } req = googleplay_pb2 . BulkDetailsRequest ( ) req . docid . extend ( packageNames ) data = req . SerializeToString ( ) message = self . executeRequestApi2 ( BULK_URL , post_data = data . decode ( "utf-8" ) , content_type = CONTENT_TYPE_PROTO , params = params ) response = message . payload . bulkDetailsResponse return [ None if not utils . hasDoc ( entry ) else utils . parseProtobufObj ( entry . doc ) for entry in response . entry ]
9278	def to_decimal ( text ) : if not isinstance ( text , string_type ) : raise TypeError ( "expected str or unicode, %s given" % type ( text ) ) if findall ( r"[\x00-\x20\x7c-\xff]" , text ) : raise ValueError ( "invalid character in sequence" ) text = text . lstrip ( '!' ) decimal = 0 length = len ( text ) - 1 for i , char in enumerate ( text ) : decimal += ( ord ( char ) - 33 ) * ( 91 ** ( length - i ) ) return decimal if text != '' else 0
13235	def make_naive ( value , timezone ) : value = value . astimezone ( timezone ) if hasattr ( timezone , 'normalize' ) : value = timezone . normalize ( value ) return value . replace ( tzinfo = None )
9235	def parse ( data ) : sections = re . compile ( "^## .+$" , re . MULTILINE ) . split ( data ) headings = re . findall ( "^## .+?$" , data , re . MULTILINE ) sections . pop ( 0 ) parsed = [ ] def func ( h , s ) : p = parse_heading ( h ) p [ "content" ] = s parsed . append ( p ) list ( map ( func , headings , sections ) ) return parsed
2301	def predict ( self , df_data , threshold = 0.05 , ** kwargs ) : nb_jobs = kwargs . get ( "nb_jobs" , SETTINGS . NB_JOBS ) list_nodes = list ( df_data . columns . values ) if nb_jobs != 1 : result_feature_selection = Parallel ( n_jobs = nb_jobs ) ( delayed ( self . run_feature_selection ) ( df_data , node , idx , ** kwargs ) for idx , node in enumerate ( list_nodes ) ) else : result_feature_selection = [ self . run_feature_selection ( df_data , node , idx , ** kwargs ) for idx , node in enumerate ( list_nodes ) ] for idx , i in enumerate ( result_feature_selection ) : try : i . insert ( idx , 0 ) except AttributeError : result_feature_selection [ idx ] = np . insert ( i , idx , 0 ) matrix_results = np . array ( result_feature_selection ) matrix_results *= matrix_results . transpose ( ) np . fill_diagonal ( matrix_results , 0 ) matrix_results /= 2 graph = nx . Graph ( ) for ( i , j ) , x in np . ndenumerate ( matrix_results ) : if matrix_results [ i , j ] > threshold : graph . add_edge ( list_nodes [ i ] , list_nodes [ j ] , weight = matrix_results [ i , j ] ) for node in list_nodes : if node not in graph . nodes ( ) : graph . add_node ( node ) return graph
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
12342	def images ( self ) : "List of paths to images." tifs = _pattern ( self . _image_path , extension = 'tif' ) pngs = _pattern ( self . _image_path , extension = 'png' ) imgs = [ ] imgs . extend ( glob ( tifs ) ) imgs . extend ( glob ( pngs ) ) return imgs
788	def jobInfoWithModels ( self , jobID ) : combinedResults = None with ConnectionFactory . get ( ) as conn : query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( "jobID=%s not found within the jobs table" % ( jobID ) )
12971	def getMultiple ( self , pks , cascadeFetch = False ) : if type ( pks ) == set : pks = list ( pks ) if len ( pks ) == 1 : return IRQueryableList ( [ self . get ( pks [ 0 ] , cascadeFetch = cascadeFetch ) ] , mdl = self . mdl ) conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) for pk in pks : key = self . _get_key_for_id ( pk ) pipeline . hgetall ( key ) res = pipeline . execute ( ) ret = IRQueryableList ( mdl = self . mdl ) i = 0 pksLen = len ( pks ) while i < pksLen : if res [ i ] is None : ret . append ( None ) i += 1 continue res [ i ] [ '_id' ] = pks [ i ] obj = self . _redisResultToObj ( res [ i ] ) ret . append ( obj ) i += 1 if cascadeFetch is True : for obj in ret : if not obj : continue self . _doCascadeFetch ( obj ) return ret
971	def _copyAllocatedStates ( self ) : if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
6657	def calc_inbag ( n_samples , forest ) : if not forest . bootstrap : e_s = "Cannot calculate the inbag from a forest that has " e_s = " bootstrap=False" raise ValueError ( e_s ) n_trees = forest . n_estimators inbag = np . zeros ( ( n_samples , n_trees ) ) sample_idx = [ ] for t_idx in range ( n_trees ) : sample_idx . append ( _generate_sample_indices ( forest . estimators_ [ t_idx ] . random_state , n_samples ) ) inbag [ : , t_idx ] = np . bincount ( sample_idx [ - 1 ] , minlength = n_samples ) return inbag
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
7642	def _conversion ( target , source ) : def register ( func ) : __CONVERSION__ [ target ] [ source ] = func return func return register
6166	def my_psd ( x , NFFT = 2 ** 10 , Fs = 1 ) : Px , f = pylab . mlab . psd ( x , NFFT , Fs ) return Px . flatten ( ) , f
8713	def file_do ( self , filename ) : log . info ( 'Executing ' + filename ) res = self . __exchange ( 'dofile("' + filename + '")' ) log . info ( res ) return res
5913	def cat ( self , out_ndx = None ) : if out_ndx is None : out_ndx = self . output self . make_ndx ( o = out_ndx , input = [ 'q' ] ) return out_ndx
11012	def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT_AUTHOR' ) ) slug = slugify ( title ) creation_date = datetime . now ( ) basename = '{:%Y-%m-%d}_{}.md' . format ( creation_date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Author' , author ) , ) file_content = '' for key , value in meta : file_content += '{}: {}\n' . format ( key , value ) file_content += '\n\n' file_content += 'Text...\n\n' file_content += '![image description]({filename}/images/my-photo.jpg)\n\n' file_content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT_DIR' ] , exist_ok = True ) path = os . path . join ( config [ 'CONTENT_DIR' ] , basename ) with click . open_file ( path , 'w' ) as f : f . write ( file_content ) click . echo ( path ) click . launch ( path )
8391	def check_visitors ( cls ) : for name in dir ( cls ) : if name . startswith ( "visit_" ) : if name [ 6 : ] not in CLASS_NAMES : raise Exception ( u"Method {} doesn't correspond to a node class" . format ( name ) ) return cls
6474	def apply_function ( self , points ) : if not self . option . function : return points if np is None : raise ImportError ( 'numpy is not available' ) if ':' in self . option . function : function , arguments = self . option . function . split ( ':' , 1 ) arguments = arguments . split ( ',' ) else : function = self . option . function arguments = [ ] arguments = list ( map ( self . _function_argument , arguments ) ) filter_function = FUNCTION . get ( function ) if filter_function is None : raise TypeError ( 'Invalid function "%s"' % ( function , ) ) else : return filter_function ( np . array ( list ( points ) ) , * arguments )
3023	def _in_gae_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name in ( 'GAE_PRODUCTION' , 'GAE_LOCAL' ) try : import google . appengine except ImportError : pass else : server_software = os . environ . get ( _SERVER_SOFTWARE , '' ) if server_software . startswith ( 'Google App Engine/' ) : SETTINGS . env_name = 'GAE_PRODUCTION' return True elif server_software . startswith ( 'Development/' ) : SETTINGS . env_name = 'GAE_LOCAL' return True return False
5558	def _flatten_tree ( tree , old_path = None ) : flat_tree = [ ] for key , value in tree . items ( ) : new_path = "/" . join ( [ old_path , key ] ) if old_path else key if isinstance ( value , dict ) and "format" not in value : flat_tree . extend ( _flatten_tree ( value , old_path = new_path ) ) else : flat_tree . append ( ( new_path , value ) ) return flat_tree
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
313	def sharpe_ratio ( returns , risk_free = 0 , period = DAILY ) : return ep . sharpe_ratio ( returns , risk_free = risk_free , period = period )
5189	def main ( ) : app = MyMaster ( log_handler = MyLogger ( ) , listener = AppChannelListener ( ) , soe_handler = SOEHandler ( ) , master_application = MasterApplication ( ) ) _log . debug ( 'Initialization complete. In command loop.' ) app . shutdown ( ) _log . debug ( 'Exiting.' ) exit ( )
2502	def to_special_value ( self , value ) : if value == self . spdx_namespace . none : return utils . SPDXNone ( ) elif value == self . spdx_namespace . noassertion : return utils . NoAssert ( ) elif value == self . spdx_namespace . unknown : return utils . UnKnown ( ) else : return value
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
7080	def tic_objectsearch ( objectid , idcol_to_use = "ID" , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'columns' : '*' , 'filters' : [ { "paramName" : idcol_to_use , "values" : [ str ( objectid ) ] } ] } service = 'Mast.Catalogs.Filtered.Tic' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
1825	def LEAVE ( cpu ) : cpu . STACK = cpu . FRAME cpu . FRAME = cpu . pop ( cpu . address_bit_size )
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** kwargs ) : try : return method ( self , * args , ** kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
10446	def launchapp ( self , cmd , args = [ ] , delay = 0 , env = 1 , lang = "C" ) : try : atomac . NativeUIElement . launchAppByBundleId ( cmd ) return 1 except RuntimeError : if atomac . NativeUIElement . launchAppByBundlePath ( cmd , args ) : try : time . sleep ( int ( delay ) ) except ValueError : time . sleep ( 5 ) return 1 else : raise LdtpServerException ( u"Unable to find app '%s'" % cmd )
4	def parse_unknown_args ( args ) : retval = { } preceded_by_key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded_by_key = True elif preceded_by_key : retval [ key ] = arg preceded_by_key = False return retval
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
13098	def wait ( self ) : try : self . relay . wait ( ) self . responder . wait ( ) except KeyboardInterrupt : print_notification ( "Stopping" ) finally : self . terminate_processes ( )
3126	def get ( self , template_id , ** queryparams ) : self . template_id = template_id return self . _mc_client . _get ( url = self . _build_path ( template_id ) , ** queryparams )
6622	def configure ( self , component , all_dependencies ) : r = { } builddir = self . buildroot available_dependencies = OrderedDict ( ( k , v ) for k , v in all_dependencies . items ( ) if v ) self . set_toplevel_definitions = '' if self . build_info_include_file is None : self . build_info_include_file , build_info_definitions = self . getBuildInfo ( component . path , builddir ) self . set_toplevel_definitions += build_info_definitions if self . config_include_file is None : self . config_include_file , config_definitions , self . config_json_file = self . _getConfigData ( available_dependencies , component , builddir , self . build_info_include_file ) self . set_toplevel_definitions += config_definitions self . configured = True return { 'merged_config_include' : self . config_include_file , 'merged_config_json' : self . config_json_file , 'build_info_include' : self . build_info_include_file }
2038	def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
10024	def deploy_version ( self , environment_name , version_label ) : out ( "Deploying " + str ( version_label ) + " to " + str ( environment_name ) ) self . ebs . update_environment ( environment_name = environment_name , version_label = version_label )
5199	def process_point_value ( cls , command_type , command , index , op_type ) : _log . debug ( 'Processing received point value for index {}: {}' . format ( index , command ) )
8407	def expand_range ( range , mul = 0 , add = 0 , zero_width = 1 ) : x = range try : x [ 0 ] except TypeError : x = ( x , x ) if zero_range ( x ) : new = x [ 0 ] - zero_width / 2 , x [ 0 ] + zero_width / 2 else : dx = ( x [ 1 ] - x [ 0 ] ) * mul + add new = x [ 0 ] - dx , x [ 1 ] + dx return new
6642	def satisfyDependenciesRecursive ( self , available_components = None , search_dirs = None , update_installed = False , traverse_links = False , target = None , test = False ) : def provider ( dspec , available_components , search_dirs , working_directory , update_installed , dep_of = None ) : r = access . satisfyFromAvailable ( dspec . name , available_components ) if r : if r . isTestDependency ( ) and not dspec . is_test_dependency : logger . debug ( 'test dependency subsequently occurred as real dependency: %s' , r . getName ( ) ) r . setTestDependency ( False ) return r update_if_installed = False if update_installed is True : update_if_installed = True elif update_installed : update_if_installed = dspec . name in update_installed r = access . satisfyVersionFromSearchPaths ( dspec . name , dspec . versionReq ( ) , search_dirs , update_if_installed , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : r . setTestDependency ( dspec . is_test_dependency ) return r default_path = os . path . join ( self . modulesPath ( ) , dspec . name ) if fsutils . isLink ( default_path ) : r = Component ( default_path , test_dependency = dspec . is_test_dependency , installed_linked = fsutils . isLink ( default_path ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if r : assert ( r . installedLinked ( ) ) return r else : logger . error ( 'linked module %s is invalid: %s' , dspec . name , r . getError ( ) ) return r r = access . satisfyVersionByInstalling ( dspec . name , dspec . versionReq ( ) , self . modulesPath ( ) , inherit_shrinkwrap = dep_of . getShrinkwrap ( ) ) if not r : logger . error ( 'could not install %s' % dspec . name ) if r is not None : r . setTestDependency ( dspec . is_test_dependency ) return r return self . __getDependenciesRecursiveWithProvider ( available_components = available_components , search_dirs = search_dirs , target = target , traverse_links = traverse_links , update_installed = update_installed , provider = provider , test = test )
6179	def merge_ph_times ( times_list , times_par_list , time_block ) : offsets = np . arange ( len ( times_list ) ) * time_block cum_sizes = np . cumsum ( [ ts . size for ts in times_list ] ) times = np . zeros ( cum_sizes [ - 1 ] ) times_par = np . zeros ( cum_sizes [ - 1 ] , dtype = 'uint8' ) i1 = 0 for i2 , ts , ts_par , offset in zip ( cum_sizes , times_list , times_par_list , offsets ) : times [ i1 : i2 ] = ts + offset times_par [ i1 : i2 ] = ts_par i1 = i2 return times , times_par
8207	def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
12717	def angles ( self ) : return [ self . ode_obj . getAngle ( i ) for i in range ( self . ADOF ) ]
13266	def xml_to_json ( root ) : j = { } if len ( root ) == 0 : return _maybe_intify ( root . text ) if len ( root ) == 1 and root [ 0 ] . tag . startswith ( '{' + NS_GML ) : return gml_to_geojson ( root [ 0 ] ) if root . tag == 'open511' : j [ 'meta' ] = { 'version' : root . get ( 'version' ) } for elem in root : name = elem . tag if name == 'link' and elem . get ( 'rel' ) : name = elem . get ( 'rel' ) + '_url' if name == 'self_url' : name = 'url' if root . tag == 'open511' : j [ 'meta' ] [ name ] = elem . get ( 'href' ) continue elif name . startswith ( '{' + NS_PROTECTED ) : name = '!' + name [ name . index ( '}' ) + 1 : ] elif name [ 0 ] == '{' : name = '+' + name [ name . index ( '}' ) + 1 : ] if name in j : continue elif elem . tag == 'link' and not elem . text : j [ name ] = elem . get ( 'href' ) elif len ( elem ) : if name == 'grouped_events' : j [ name ] = [ xml_link_to_json ( child , to_dict = False ) for child in elem ] elif name in ( 'attachments' , 'media_files' ) : j [ name ] = [ xml_link_to_json ( child , to_dict = True ) for child in elem ] elif all ( ( name == pluralize ( child . tag ) for child in elem ) ) : j [ name ] = [ xml_to_json ( child ) for child in elem ] else : j [ name ] = xml_to_json ( elem ) else : if root . tag == 'open511' and name . endswith ( 's' ) and not elem . text : j [ name ] = [ ] else : j [ name ] = _maybe_intify ( elem . text ) return j
8197	def bezier_arc ( x1 , y1 , x2 , y2 , start_angle = 0 , extent = 90 ) : x1 , y1 , x2 , y2 = min ( x1 , x2 ) , max ( y1 , y2 ) , max ( x1 , x2 ) , min ( y1 , y2 ) if abs ( extent ) <= 90 : frag_angle = float ( extent ) nfrag = 1 else : nfrag = int ( ceil ( abs ( extent ) / 90. ) ) if nfrag == 0 : warnings . warn ( 'Invalid value for extent: %r' % extent ) return [ ] frag_angle = float ( extent ) / nfrag x_cen = ( x1 + x2 ) / 2. y_cen = ( y1 + y2 ) / 2. rx = ( x2 - x1 ) / 2. ry = ( y2 - y1 ) / 2. half_angle = radians ( frag_angle ) / 2 kappa = abs ( 4. / 3. * ( 1. - cos ( half_angle ) ) / sin ( half_angle ) ) if frag_angle < 0 : sign = - 1 else : sign = 1 point_list = [ ] for i in range ( nfrag ) : theta0 = radians ( start_angle + i * frag_angle ) theta1 = radians ( start_angle + ( i + 1 ) * frag_angle ) c0 = cos ( theta0 ) c1 = cos ( theta1 ) s0 = sin ( theta0 ) s1 = sin ( theta1 ) if frag_angle > 0 : signed_kappa = - kappa else : signed_kappa = kappa point_list . append ( ( x_cen + rx * c0 , y_cen - ry * s0 , x_cen + rx * ( c0 + signed_kappa * s0 ) , y_cen - ry * ( s0 - signed_kappa * c0 ) , x_cen + rx * ( c1 - signed_kappa * s1 ) , y_cen - ry * ( s1 + signed_kappa * c1 ) , x_cen + rx * c1 , y_cen - ry * s1 ) ) return point_list
8803	def notify ( context , event_type , ipaddress , send_usage = False , * args , ** kwargs ) : if ( event_type == IP_ADD and not CONF . QUARK . notify_ip_add ) or ( event_type == IP_DEL and not CONF . QUARK . notify_ip_delete ) or ( event_type == IP_ASSOC and not CONF . QUARK . notify_flip_associate ) or ( event_type == IP_DISASSOC and not CONF . QUARK . notify_flip_disassociate ) or ( event_type == IP_EXISTS and not CONF . QUARK . notify_ip_exists ) : LOG . debug ( 'IP_BILL: notification {} is disabled by config' . format ( event_type ) ) return if 'rollback' in kwargs and kwargs [ 'rollback' ] : LOG . debug ( 'IP_BILL: not sending notification because we are in undo' ) return ts = ipaddress . allocated_at if event_type == IP_ADD else _now ( ) payload = build_payload ( ipaddress , event_type , event_time = ts ) do_notify ( context , event_type , payload ) if send_usage : if ipaddress . allocated_at is not None and ipaddress . allocated_at >= _midnight_today ( ) : start_time = ipaddress . allocated_at else : start_time = _midnight_today ( ) payload = build_payload ( ipaddress , IP_EXISTS , start_time = start_time , end_time = ts ) do_notify ( context , IP_EXISTS , payload )
5250	def start ( self ) : logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
9426	def namelist ( self ) : names = [ ] for member in self . filelist : names . append ( member . filename ) return names
163	def compute_neighbour_distances ( self ) : if len ( self . coords ) <= 1 : return np . zeros ( ( 0 , ) , dtype = np . float32 ) return np . sqrt ( np . sum ( ( self . coords [ : - 1 , : ] - self . coords [ 1 : , : ] ) ** 2 , axis = 1 ) )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
2989	def serialize_options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT_OPTIONS : LOG . warning ( "Unknown option passed to Flask-CORS: %s" , key ) options [ 'origins' ] = sanitize_regex_param ( options . get ( 'origins' ) ) options [ 'allow_headers' ] = sanitize_regex_param ( options . get ( 'allow_headers' ) ) if r'.*' in options [ 'origins' ] and options [ 'supports_credentials' ] and options [ 'send_wildcard' ] : raise ValueError ( "Cannot use supports_credentials in conjunction with" "an origin string of '*'. See: " "http://www.w3.org/TR/cors/#resource-requests" ) serialize_option ( options , 'expose_headers' ) serialize_option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max_age' ) , timedelta ) : options [ 'max_age' ] = str ( int ( options [ 'max_age' ] . total_seconds ( ) ) ) return options
7155	def raw ( prompt , * args , ** kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
8293	def unique ( list ) : unique = [ ] [ unique . append ( x ) for x in list if x not in unique ] return unique
164	def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( "Expected Keypoint or LineString or tuple (x,y), " + "got type %s." ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
264	def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : positions = get_percent_alloc ( positions ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions
6163	def QPSK_rx ( fc , N_symb , Rs , EsN0 = 100 , fs = 125 , lfsr_len = 10 , phase = 0 , pulse = 'src' ) : Ns = int ( np . round ( fs / Rs ) ) print ( 'Ns = ' , Ns ) print ( 'Rs = ' , fs / float ( Ns ) ) print ( 'EsN0 = ' , EsN0 , 'dB' ) print ( 'phase = ' , phase , 'degrees' ) print ( 'pulse = ' , pulse ) x , b , data = QPSK_bb ( N_symb , Ns , lfsr_len , pulse ) x = cpx_AWGN ( x , EsN0 , Ns ) n = np . arange ( len ( x ) ) xc = x * np . exp ( 1j * 2 * np . pi * fc / float ( fs ) * n ) * np . exp ( 1j * phase ) return xc , b , data
6192	def _generate ( num_particles , D , box , rs ) : X0 = rs . rand ( num_particles ) * ( box . x2 - box . x1 ) + box . x1 Y0 = rs . rand ( num_particles ) * ( box . y2 - box . y1 ) + box . y1 Z0 = rs . rand ( num_particles ) * ( box . z2 - box . z1 ) + box . z1 return [ Particle ( D = D , x0 = x0 , y0 = y0 , z0 = z0 ) for x0 , y0 , z0 in zip ( X0 , Y0 , Z0 ) ]
597	def _compute ( self , inputs , outputs ) : if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) self . _conditionalBreak ( ) self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
13565	def register ( app ) : error_handler = json . http_exception_error_handler @ app . errorhandler ( 400 ) def handle_bad_request ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 404 ) def handle_not_found ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 405 ) def handle_method_not_allowed ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 422 ) def handle_unprocessable_entity ( exception ) : return error_handler ( exception ) @ app . errorhandler ( 500 ) def handle_internal_server_error ( exception ) : return error_handler ( exception )
3022	def _detect_gce_environment ( ) : http = transport . get_http_object ( timeout = GCE_METADATA_TIMEOUT ) try : response , _ = transport . request ( http , _GCE_METADATA_URI , headers = _GCE_HEADERS ) return ( response . status == http_client . OK and response . get ( _METADATA_FLAVOR_HEADER ) == _DESIRED_METADATA_FLAVOR ) except socket . error : logger . info ( 'Timeout attempting to reach GCE metadata service.' ) return False
836	def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
2949	def execute ( self , task , script , ** kwargs ) : locals ( ) . update ( kwargs ) exec ( script )
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
13398	def check_docstring ( cls ) : docstring = inspect . getdoc ( cls ) if not docstring : breadcrumbs = " -> " . join ( t . __name__ for t in inspect . getmro ( cls ) [ : - 1 ] [ : : - 1 ] ) msg = "docstring required for plugin '%s' (%s, defined in %s)" args = ( cls . __name__ , breadcrumbs , cls . __module__ ) raise InternalCashewException ( msg % args ) max_line_length = cls . _class_settings . get ( 'max-docstring-length' ) if max_line_length : for i , line in enumerate ( docstring . splitlines ( ) ) : if len ( line ) > max_line_length : msg = "docstring line %s of %s is %s chars too long" args = ( i , cls . __name__ , len ( line ) - max_line_length ) raise Exception ( msg % args ) return docstring
34	def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
7698	def as_xml ( self , parent = None ) : if parent is not None : element = ElementTree . SubElement ( parent , ITEM_TAG ) else : element = ElementTree . Element ( ITEM_TAG ) element . set ( "jid" , unicode ( self . jid ) ) if self . name is not None : element . set ( "name" , self . name ) if self . subscription is not None : element . set ( "subscription" , self . subscription ) if self . ask : element . set ( "ask" , self . ask ) if self . approved : element . set ( "approved" , "true" ) for group in self . groups : ElementTree . SubElement ( element , GROUP_TAG ) . text = group return element
9319	def _validate_status ( self ) : if not self . id : msg = "No 'id' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . status : msg = "No 'status' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . total_count is None : msg = "No 'total_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . success_count is None : msg = "No 'success_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . failure_count is None : msg = "No 'failure_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . pending_count is None : msg = "No 'pending_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if len ( self . successes ) != self . success_count : msg = "Found successes={}, but success_count={} in status '{}'" raise ValidationError ( msg . format ( self . successes , self . success_count , self . id ) ) if len ( self . pendings ) != self . pending_count : msg = "Found pendings={}, but pending_count={} in status '{}'" raise ValidationError ( msg . format ( self . pendings , self . pending_count , self . id ) ) if len ( self . failures ) != self . failure_count : msg = "Found failures={}, but failure_count={} in status '{}'" raise ValidationError ( msg . format ( self . failures , self . failure_count , self . id ) ) if ( self . success_count + self . pending_count + self . failure_count != self . total_count ) : msg = ( "(success_count={} + pending_count={} + " "failure_count={}) != total_count={} in status '{}'" ) raise ValidationError ( msg . format ( self . success_count , self . pending_count , self . failure_count , self . total_count , self . id ) )
968	def runCPU ( ) : model = ModelFactory . create ( model_params . MODEL_PARAMS ) model . enableInference ( { 'predictedField' : 'cpu' } ) shifter = InferenceShifter ( ) actHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) predHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) actline , = plt . plot ( range ( WINDOW ) , actHistory ) predline , = plt . plot ( range ( WINDOW ) , predHistory ) actline . axes . set_ylim ( 0 , 100 ) predline . axes . set_ylim ( 0 , 100 ) while True : s = time . time ( ) cpu = psutil . cpu_percent ( ) modelInput = { 'cpu' : cpu } result = shifter . shift ( model . run ( modelInput ) ) inference = result . inferences [ 'multiStepBestPredictions' ] [ 5 ] if inference is not None : actHistory . append ( result . rawInput [ 'cpu' ] ) predHistory . append ( inference ) actline . set_ydata ( actHistory ) predline . set_ydata ( predHistory ) plt . draw ( ) plt . legend ( ( 'actual' , 'predicted' ) ) try : plt . pause ( SECONDS_PER_STEP ) except : pass
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
939	def runExperiment ( args , model = None ) : opt = _parseCommandLineOptions ( args ) model = _runExperimentImpl ( opt , model ) return model
11669	def topological_sort ( deps ) : order = [ ] available = set ( ) def _move_available ( ) : to_delete = [ ] for n , parents in iteritems ( deps ) : if not parents : available . add ( n ) to_delete . append ( n ) for n in to_delete : del deps [ n ] _move_available ( ) while available : n = available . pop ( ) order . append ( n ) for parents in itervalues ( deps ) : parents . discard ( n ) _move_available ( ) if available : raise ValueError ( "dependency cycle found" ) return order
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
3956	def update_nginx_from_config ( nginx_config ) : logging . info ( 'Updating nginx with new Dusty config' ) temp_dir = tempfile . mkdtemp ( ) os . mkdir ( os . path . join ( temp_dir , 'html' ) ) _write_nginx_config ( constants . NGINX_BASE_CONFIG , os . path . join ( temp_dir , constants . NGINX_PRIMARY_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'http' ] , os . path . join ( temp_dir , constants . NGINX_HTTP_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'stream' ] , os . path . join ( temp_dir , constants . NGINX_STREAM_CONFIG_NAME ) ) _write_nginx_config ( constants . NGINX_502_PAGE_HTML , os . path . join ( temp_dir , 'html' , constants . NGINX_502_PAGE_NAME ) ) sync_local_path_to_vm ( temp_dir , constants . NGINX_CONFIG_DIR_IN_VM )
11277	def run_program ( prog_list , debug , shell ) : try : if not shell : process = Popen ( prog_list , stdout = PIPE , stderr = PIPE ) stdout , stderr = process . communicate ( ) retcode = process . returncode if debug >= 1 : print ( "Program : " , " " . join ( prog_list ) ) print ( "Return Code: " , retcode ) print ( "Stdout: " , stdout ) print ( "Stderr: " , stderr ) return bool ( retcode ) else : command = " " . join ( prog_list ) os . system ( command ) return True except : return False
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
11380	def do_oembed ( parser , token ) : args = token . split_contents ( ) template_dir = None var_name = None if len ( args ) > 2 : if len ( args ) == 3 and args [ 1 ] == 'in' : template_dir = args [ 2 ] elif len ( args ) == 3 and args [ 1 ] == 'as' : var_name = args [ 2 ] elif len ( args ) == 4 and args [ 2 ] == 'in' : template_dir = args [ 3 ] elif len ( args ) == 4 and args [ 2 ] == 'as' : var_name = args [ 3 ] elif len ( args ) == 6 and args [ 4 ] == 'as' : template_dir = args [ 3 ] var_name = args [ 5 ] else : raise template . TemplateSyntaxError ( "OEmbed either takes a single " "(optional) argument: WIDTHxHEIGHT, where WIDTH and HEIGHT " "are positive integers, and or an optional 'in " " \"template_dir\"' argument set." ) if template_dir : if not ( template_dir [ 0 ] == template_dir [ - 1 ] and template_dir [ 0 ] in ( '"' , "'" ) ) : raise template . TemplateSyntaxError ( "template_dir must be quoted" ) template_dir = template_dir [ 1 : - 1 ] if len ( args ) >= 2 and 'x' in args [ 1 ] : width , height = args [ 1 ] . lower ( ) . split ( 'x' ) if not width and height : raise template . TemplateSyntaxError ( "OEmbed's optional WIDTHxHEIGH" "T argument requires WIDTH and HEIGHT to be positive integers." ) else : width , height = None , None nodelist = parser . parse ( ( 'endoembed' , ) ) parser . delete_first_token ( ) return OEmbedNode ( nodelist , width , height , template_dir , var_name )
8444	def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running "temple update" and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )
1180	def _create_regs ( self , state ) : regs = [ ( state . start , state . string_position ) ] for group in range ( self . re . groups ) : mark_index = 2 * group if mark_index + 1 < len ( state . marks ) and state . marks [ mark_index ] is not None and state . marks [ mark_index + 1 ] is not None : regs . append ( ( state . marks [ mark_index ] , state . marks [ mark_index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
2640	def runner ( incoming_q , outgoing_q ) : logger . debug ( "[RUNNER] Starting" ) def execute_task ( bufs ) : user_ns = locals ( ) user_ns . update ( { '__builtins__' : __builtins__ } ) f , args , kwargs = unpack_apply_message ( bufs , user_ns , copy = False ) fname = getattr ( f , '__name__' , 'f' ) prefix = "parsl_" fname = prefix + "f" argname = prefix + "args" kwargname = prefix + "kwargs" resultname = prefix + "result" user_ns . update ( { fname : f , argname : args , kwargname : kwargs , resultname : resultname } ) code = "{0} = {1}(*{2}, **{3})" . format ( resultname , fname , argname , kwargname ) try : logger . debug ( "[RUNNER] Executing: {0}" . format ( code ) ) exec ( code , user_ns , user_ns ) except Exception as e : logger . warning ( "Caught exception; will raise it: {}" . format ( e ) ) raise e else : logger . debug ( "[RUNNER] Result: {0}" . format ( user_ns . get ( resultname ) ) ) return user_ns . get ( resultname ) while True : try : msg = incoming_q . get ( block = True , timeout = 10 ) except queue . Empty : logger . debug ( "[RUNNER] Queue is empty" ) except IOError as e : logger . debug ( "[RUNNER] Broken pipe: {}" . format ( e ) ) try : outgoing_q . put ( None ) except Exception : pass break except Exception as e : logger . debug ( "[RUNNER] Caught unknown exception: {}" . format ( e ) ) else : if not msg : logger . debug ( "[RUNNER] Received exit request" ) outgoing_q . put ( None ) break else : logger . debug ( "[RUNNER] Got a valid task with ID {}" . format ( msg [ "task_id" ] ) ) try : response_obj = execute_task ( msg [ 'buffer' ] ) response = { "task_id" : msg [ "task_id" ] , "result" : serialize_object ( response_obj ) } logger . debug ( "[RUNNER] Returing result: {}" . format ( deserialize_object ( response [ "result" ] ) ) ) except Exception as e : logger . debug ( "[RUNNER] Caught task exception: {}" . format ( e ) ) response = { "task_id" : msg [ "task_id" ] , "exception" : serialize_object ( e ) } outgoing_q . put ( response ) logger . debug ( "[RUNNER] Terminating" )
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
4719	def tsuite_setup ( trun , declr , enum ) : suite = copy . deepcopy ( TESTSUITE ) suite [ "name" ] = declr . get ( "name" ) if suite [ "name" ] is None : cij . err ( "rnr:tsuite_setup: no testsuite is given" ) return None suite [ "alias" ] = declr . get ( "alias" ) suite [ "ident" ] = "%s_%d" % ( suite [ "name" ] , enum ) suite [ "res_root" ] = os . sep . join ( [ trun [ "conf" ] [ "OUTPUT" ] , suite [ "ident" ] ] ) suite [ "aux_root" ] = os . sep . join ( [ suite [ "res_root" ] , "_aux" ] ) suite [ "evars" ] . update ( copy . deepcopy ( trun [ "evars" ] ) ) suite [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( suite [ "res_root" ] ) os . makedirs ( suite [ "aux_root" ] ) suite [ "hooks" ] = hooks_setup ( trun , suite , declr . get ( "hooks" ) ) suite [ "hooks_pr_tcase" ] = declr . get ( "hooks_pr_tcase" , [ ] ) suite [ "fname" ] = "%s.suite" % suite [ "name" ] suite [ "fpath" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTSUITES" ] , suite [ "fname" ] ] ) tcase_fpaths = [ ] if os . path . exists ( suite [ "fpath" ] ) : suite_lines = ( l . strip ( ) for l in open ( suite [ "fpath" ] ) . read ( ) . splitlines ( ) ) tcase_fpaths . extend ( ( l for l in suite_lines if len ( l ) > 1 and l [ 0 ] != "#" ) ) else : tcase_fpaths . extend ( declr . get ( "testcases" , [ ] ) ) if len ( set ( tcase_fpaths ) ) != len ( tcase_fpaths ) : cij . err ( "rnr:suite: failed: duplicate tcase in suite not supported" ) return None for tcase_fname in tcase_fpaths : tcase = tcase_setup ( trun , suite , tcase_fname ) if not tcase : cij . err ( "rnr:suite: failed: tcase_setup" ) return None suite [ "testcases" ] . append ( tcase ) return suite
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
6749	def capture_bash ( self ) : class Capture ( object ) : def __init__ ( self , satchel ) : self . satchel = satchel self . _dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . _stdout = sys . stdout self . _stderr = sys . stderr self . stdout = sys . stdout = StringIO ( ) self . stderr = sys . stderr = StringIO ( ) def __enter__ ( self ) : return self def __exit__ ( self , type , value , traceback ) : endcap ( ) self . satchel . dryrun = self . _dryrun sys . stdout = self . _stdout sys . stderr = self . _stderr return Capture ( self )
10687	def H ( self , phase , T ) : try : return self . _phases [ phase ] . H ( T ) except KeyError : raise Exception ( "The phase '{}' was not found in compound '{}'." . format ( phase , self . formula ) )
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
1625	def FindStartOfExpressionInLine ( line , endpos , stack ) : i = endpos while i >= 0 : char = line [ i ] if char in ')]}' : stack . append ( char ) elif char == '>' : if ( i > 0 and ( line [ i - 1 ] == '-' or Match ( r'\s>=\s' , line [ i - 1 : ] ) or Search ( r'\boperator\s*$' , line [ 0 : i ] ) ) ) : i -= 1 else : stack . append ( '>' ) elif char == '<' : if i > 0 and line [ i - 1 ] == '<' : i -= 1 else : if stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( i , None ) elif char in '([{' : while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( char == '(' and stack [ - 1 ] == ')' ) or ( char == '[' and stack [ - 1 ] == ']' ) or ( char == '{' and stack [ - 1 ] == '}' ) ) : stack . pop ( ) if not stack : return ( i , None ) else : return ( - 1 , None ) elif char == ';' : while stack and stack [ - 1 ] == '>' : stack . pop ( ) if not stack : return ( - 1 , None ) i -= 1 return ( - 1 , stack )
1430	def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
11054	def rm_fwd_refs ( obj ) : for stack , key in obj . _backrefs_flat : backref_key , parent_schema_name , parent_field_name = stack parent_schema = obj . _collections [ parent_schema_name ] parent_key_store = parent_schema . _pk_to_storage ( key ) parent_object = parent_schema . load ( parent_key_store ) if parent_object is None : continue if parent_object . _fields [ parent_field_name ] . _list : getattr ( parent_object , parent_field_name ) . remove ( obj ) else : parent_field_object = parent_object . _fields [ parent_field_name ] setattr ( parent_object , parent_field_name , parent_field_object . _gen_default ( ) ) parent_object . save ( )
9303	def get_request_date ( cls , req ) : date = None for header in [ 'x-amz-date' , 'date' ] : if header not in req . headers : continue try : date_str = cls . parse_date ( req . headers [ header ] ) except DateFormatError : continue try : date = datetime . datetime . strptime ( date_str , '%Y-%m-%d' ) . date ( ) except ValueError : continue else : break return date
12655	def remove_dcm2nii_underprocessed ( filepaths ) : cln_flist = [ ] len_sorted = sorted ( filepaths , key = len ) for idx , fpath in enumerate ( len_sorted ) : remove = False fname = op . basename ( fpath ) rest = len_sorted [ idx + 1 : ] for rest_fpath in rest : rest_file = op . basename ( rest_fpath ) if rest_file . endswith ( fname ) : remove = True break if not remove : cln_flist . append ( fpath ) return cln_flist
4822	def get_course_details ( self , course_id ) : try : return self . client . course ( course_id ) . get ( ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to retrieve course enrollment details for course [%s] due to: [%s]' , course_id , str ( exc ) ) return { }
4328	def echo ( self , gain_in = 0.8 , gain_out = 0.9 , n_echos = 1 , delays = [ 60 ] , decays = [ 0.4 ] ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not isinstance ( n_echos , int ) or n_echos <= 0 : raise ValueError ( "n_echos must be a positive integer." ) if not isinstance ( delays , list ) : raise ValueError ( "delays must be a list" ) if len ( delays ) != n_echos : raise ValueError ( "the length of delays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 ) for p in delays ) : raise ValueError ( "the elements of delays must be numbers > 0" ) if not isinstance ( decays , list ) : raise ValueError ( "decays must be a list" ) if len ( decays ) != n_echos : raise ValueError ( "the length of decays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 or p > 1 ) for p in decays ) : raise ValueError ( "the elements of decays must be between 0 and 1" ) effect_args = [ 'echo' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) ] for i in range ( n_echos ) : effect_args . extend ( [ '{}' . format ( delays [ i ] ) , '{}' . format ( decays [ i ] ) ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'echo' ) return self
3334	def string_repr ( s ) : if compat . is_bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
2440	def add_annotator ( self , doc , annotator ) : self . reset_annotations ( ) if validations . validate_annotator ( annotator ) : doc . add_annotation ( annotation . Annotation ( annotator = annotator ) ) return True else : raise SPDXValueError ( 'Annotation::Annotator' )
4528	def _receive ( self , msg ) : msg = self . _convert ( msg ) if msg is None : return str_msg = self . verbose and self . _msg_to_str ( msg ) if self . verbose and log . is_debug ( ) : log . debug ( 'Message %s' , str_msg ) if self . pre_routing : self . pre_routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str_msg [ : 128 ] , msg , repr ( receiver ) )
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
12180	def get_author_and_version ( package ) : init_py = open ( os . path . join ( package , '__init__.py' ) ) . read ( ) author = re . search ( "__author__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) version = re . search ( "__version__ = ['\"]([^'\"]+)['\"]" , init_py ) . group ( 1 ) return author , version
3878	async def _handle_conversation_delta ( self , conversation ) : conv_id = conversation . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is None : await self . _get_or_fetch_conversation ( conv_id ) else : conv . update_conversation ( conversation )
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
13528	def printoptions ( ) : x = json . dumps ( environment . options , indent = 4 , sort_keys = True , skipkeys = True , cls = MyEncoder ) print ( x )
12388	def set ( self , target , value ) : if not self . _set : return if self . path is None : self . set = lambda * a : None return None if self . _segments [ target . __class__ ] : self . get ( target ) if self . _segments [ target . __class__ ] : return parent_getter = compose ( * self . _getters [ target . __class__ ] [ : - 1 ] ) target = parent_getter ( target ) func = self . _make_setter ( self . path . split ( '.' ) [ - 1 ] , target . __class__ ) func ( target , value ) def setter ( target , value ) : func ( parent_getter ( target ) , value ) self . set = setter
12250	def _get_key_internal ( self , * args , ** kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , ** kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
7409	def get_order ( tre ) : anode = tre . tree & ">A" sister = anode . get_sisters ( ) [ 0 ] sisters = ( anode . name [ 1 : ] , sister . name [ 1 : ] ) others = [ i for i in list ( "ABCD" ) if i not in sisters ] return sorted ( sisters ) + sorted ( others )
12934	def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
9046	def rsolve ( A , y ) : from numpy_sugar . linalg import rsolve as _rsolve try : beta = _rsolve ( A , y ) except LinAlgError : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , RuntimeWarning ) beta = zeros ( A . shape [ 0 ] ) return beta
6306	def load_package ( self ) : try : self . package = importlib . import_module ( self . name ) except ModuleNotFoundError : raise ModuleNotFoundError ( "Effect package '{}' not found." . format ( self . name ) )
9215	def t_istringapostrophe_css_string ( self , t ) : r'[^\'@]+' t . lexer . lineno += t . value . count ( '\n' ) return t
115	def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )
3888	def remove_observer ( self , callback ) : if callback not in self . _observers : raise ValueError ( '{} is not an observer of {}' . format ( callback , self ) ) self . _observers . remove ( callback )
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
2978	def cmd_partition ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) if opts . random : if opts . partitions : raise BlockadeError ( "Either specify individual partitions " "or --random, but not both" ) b . random_partition ( ) else : partitions = [ ] for partition in opts . partitions : names = [ ] for name in partition . split ( "," ) : name = name . strip ( ) if name : names . append ( name ) partitions . append ( names ) if not partitions : raise BlockadeError ( "Either specify individual partitions " "or random" ) b . partition ( partitions )
8232	def set_callbacks ( self , ** kwargs ) : for name in self . SUPPORTED_CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )
2225	def _update_hasher ( hasher , data , types = True ) : if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : _update_hasher ( hasher , item , types ) for item in iter_ : _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
4061	def _attachment ( self , payload , parentid = None ) : attachment = Zupload ( self , payload , parentid ) res = attachment . upload ( ) return res
11856	def extender ( self , edge ) : "See what edges can be extended by this edge." ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
3495	def reaction_elements ( reaction ) : c_elements = [ coeff * met . elements . get ( 'C' , 0 ) for met , coeff in iteritems ( reaction . metabolites ) ] return [ elem for elem in c_elements if elem != 0 ]
6378	def manhattan ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Manhattan ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
1988	def load_state ( self , key , delete = True ) : with self . load_stream ( key , binary = True ) as f : state = self . _serializer . deserialize ( f ) if delete : self . rm ( key ) return state
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
727	def numbersForBit ( self , bit ) : if bit >= self . _n : raise IndexError ( "Invalid bit" ) numbers = set ( ) for index , pattern in self . _patterns . iteritems ( ) : if bit in pattern : numbers . add ( index ) return numbers
11996	def _set_options ( self , options ) : if not options : return self . options . copy ( ) options = options . copy ( ) if 'magic' in options : self . set_magic ( options [ 'magic' ] ) del ( options [ 'magic' ] ) if 'flags' in options : flags = options [ 'flags' ] del ( options [ 'flags' ] ) for key , value in flags . iteritems ( ) : if not isinstance ( value , bool ) : raise TypeError ( 'Invalid flag type for: %s' % key ) else : flags = self . options [ 'flags' ] if 'info' in options : del ( options [ 'info' ] ) for key , value in options . iteritems ( ) : if not isinstance ( value , int ) : raise TypeError ( 'Invalid option type for: %s' % key ) if value < 0 or value > 255 : raise ValueError ( 'Option value out of range for: %s' % key ) new_options = self . options . copy ( ) new_options . update ( options ) new_options [ 'flags' ] . update ( flags ) return new_options
1151	def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = "%s: %s: %s\n" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += " %s\n" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = "%s:%s" % ( filename , s ) return s
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
9282	def set_filter ( self , filter_text ) : self . filter = filter_text self . logger . info ( "Setting filter to: %s" , self . filter ) if self . _connected : self . _sendall ( "#filter %s\r\n" % self . filter )
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
4918	def course_detail ( self , request , pk , course_key ) : enterprise_customer_catalog = self . get_object ( ) course = enterprise_customer_catalog . get_course ( course_key ) if not course : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . CourseDetailSerializer ( course , context = context ) return Response ( serializer . data )
6785	def fake ( self , components = None ) : self . init ( ) if components : current_tp = self . get_previous_thumbprint ( ) or { } current_tp . update ( self . get_current_thumbprint ( components = components ) or { } ) else : current_tp = self . get_current_thumbprint ( components = components ) or { } tp_text = yaml . dump ( current_tp ) r = self . local_renderer r . upload_content ( content = tp_text , fn = self . manifest_filename ) self . reset_all_satchels ( )
2620	def create_vpc ( self ) : try : vpc = self . ec2 . create_vpc ( CidrBlock = '10.0.0.0/16' , AmazonProvidedIpv6CidrBlock = False , ) except Exception as e : logger . error ( "{}\n" . format ( e ) ) raise e internet_gateway = self . ec2 . create_internet_gateway ( ) internet_gateway . attach_to_vpc ( VpcId = vpc . vpc_id ) self . internet_gateway = internet_gateway . id route_table = self . config_route_table ( vpc , internet_gateway ) self . route_table = route_table . id availability_zones = self . client . describe_availability_zones ( ) for num , zone in enumerate ( availability_zones [ 'AvailabilityZones' ] ) : if zone [ 'State' ] == "available" : subnet = vpc . create_subnet ( CidrBlock = '10.0.{}.0/20' . format ( 16 * num ) , AvailabilityZone = zone [ 'ZoneName' ] ) subnet . meta . client . modify_subnet_attribute ( SubnetId = subnet . id , MapPublicIpOnLaunch = { "Value" : True } ) route_table . associate_with_subnet ( SubnetId = subnet . id ) self . sn_ids . append ( subnet . id ) else : logger . info ( "{} unavailable" . format ( zone [ 'ZoneName' ] ) ) self . security_group ( vpc ) self . vpc_id = vpc . id return vpc
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
11269	def substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . substitute ( data )
10663	def elements ( compounds ) : elementlist = [ parse_compound ( compound ) . count ( ) . keys ( ) for compound in compounds ] return set ( ) . union ( * elementlist )
5098	def graph2dict ( g , return_dict_of_dict = True ) : if not isinstance ( g , nx . DiGraph ) : g = QueueNetworkDiGraph ( g ) dict_of_dicts = nx . to_dict_of_dicts ( g ) if return_dict_of_dict : return dict_of_dicts else : return { k : list ( val . keys ( ) ) for k , val in dict_of_dicts . items ( ) }
3368	def _valid_atoms ( model , expression ) : atoms = expression . atoms ( optlang . interface . Variable ) return all ( a . problem is model . solver for a in atoms )
13657	def _forObject ( self , obj ) : router = type ( self ) ( ) router . _routes = list ( self . _routes ) router . _self = obj return router
9121	def dropbox_submission ( dropbox , request ) : try : data = dropbox_schema . deserialize ( request . POST ) except Exception : return HTTPFound ( location = request . route_url ( 'dropbox_form' ) ) dropbox . message = data . get ( 'message' ) if 'testing_secret' in dropbox . settings : dropbox . from_watchdog = is_equal ( unicode ( dropbox . settings [ 'test_submission_secret' ] ) , data . pop ( 'testing_secret' , u'' ) ) if data . get ( 'upload' ) is not None : dropbox . add_attachment ( data [ 'upload' ] ) dropbox . submit ( ) drop_url = request . route_url ( 'dropbox_view' , drop_id = dropbox . drop_id ) print ( "Created dropbox %s" % drop_url ) return HTTPFound ( location = drop_url )
670	def createSensorToClassifierLinks ( network , sensorRegionName , classifierRegionName ) : network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "actValueOut" , destInput = "actValueIn" ) network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , srcOutput = "categoryOut" , destInput = "categoryIn" )
3484	def _create_bound ( model , reaction , bound_type , f_replace , units = None , flux_udef = None ) : value = getattr ( reaction , bound_type ) if value == config . lower_bound : return LOWER_BOUND_ID elif value == 0 : return ZERO_BOUND_ID elif value == config . upper_bound : return UPPER_BOUND_ID elif value == - float ( "Inf" ) : return BOUND_MINUS_INF elif value == float ( "Inf" ) : return BOUND_PLUS_INF else : rid = reaction . id if f_replace and F_REACTION_REV in f_replace : rid = f_replace [ F_REACTION_REV ] ( rid ) pid = rid + "_" + bound_type _create_parameter ( model , pid = pid , value = value , sbo = SBO_FLUX_BOUND , units = units , flux_udef = flux_udef ) return pid
10802	def _c2x ( self , c ) : return 0.5 * ( self . window [ 0 ] + self . window [ 1 ] + c * ( self . window [ 1 ] - self . window [ 0 ] ) )
13073	def r_assets ( self , filetype , asset ) : if filetype in self . assets and asset in self . assets [ filetype ] and self . assets [ filetype ] [ asset ] : return send_from_directory ( directory = self . assets [ filetype ] [ asset ] , filename = asset ) abort ( 404 )
2594	def interactive ( f ) : if isinstance ( f , FunctionType ) : mainmod = __import__ ( '__main__' ) f = FunctionType ( f . __code__ , mainmod . __dict__ , f . __name__ , f . __defaults__ , ) f . __module__ = '__main__' return f
8356	def _toUnicode ( self , data , encoding ) : if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
12871	def chain ( * args ) : def chain_block ( * args , ** kwargs ) : v = args [ 0 ] ( * args , ** kwargs ) for p in args [ 1 : ] : v = p ( v ) return v return chain_block
5787	def _advapi32_encrypt ( cipher , key , data , iv , padding ) : context_handle = None key_handle = None try : context_handle , key_handle = _advapi32_create_handles ( cipher , key , iv ) out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( key_handle , null ( ) , True , 0 , buffer , out_len , buffer_len ) handle_error ( res ) output = bytes_from_buffer ( buffer , deref ( out_len ) ) if cipher == 'aes' and not padding : if output [ - 16 : ] != ( b'\x10' * 16 ) : raise ValueError ( 'Invalid padding generated by OS crypto library' ) output = output [ : - 16 ] return output finally : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle )
6545	def is_connected ( self ) : try : self . exec_command ( b"Query(ConnectionState)" ) return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
5539	def read ( self , ** kwargs ) : if self . tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( self . tile . bounds , self . tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( self . tile ) return self . config . output . extract_subset ( input_data_tiles = [ ( output_tile , self . config . output . read ( output_tile ) ) for output_tile in output_tiles ] , out_tile = self . tile , )
2695	def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
4519	def set_project ( self , project ) : def visit ( x ) : set_project = getattr ( x , 'set_project' , None ) if set_project : set_project ( project ) values = getattr ( x , 'values' , lambda : ( ) ) for v in values ( ) : visit ( v ) visit ( self . routing )
2767	def get_all_volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get_data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( ** jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
12767	def forces ( self , dx_tm1 = None ) : cfm = self . cfms [ self . _frame_no ] [ : , None ] kp = self . erp / ( cfm * self . world . dt ) kd = ( 1 - self . erp ) / cfm dx = self . distances ( ) F = kp * dx if dx_tm1 is not None : bad = np . isnan ( dx ) | np . isnan ( dx_tm1 ) F [ ~ bad ] += ( kd * ( dx - dx_tm1 ) / self . world . dt ) [ ~ bad ] return F
12445	def options ( self , request , response ) : response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) response . status = http . client . OK
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
13623	def Text ( value , encoding = None ) : if encoding is None : encoding = 'utf-8' if isinstance ( value , bytes ) : return value . decode ( encoding ) elif isinstance ( value , unicode ) : return value return None
9189	def get_api_keys ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) api_keys = [ x [ 0 ] for x in cursor . fetchall ( ) ] return api_keys
192	def SimplexNoiseAlpha ( first = None , second = None , per_channel = False , size_px_max = ( 2 , 16 ) , upscale_method = None , iterations = ( 1 , 3 ) , aggregation_method = "max" , sigmoid = True , sigmoid_thresh = None , name = None , deterministic = False , random_state = None ) : upscale_method_default = iap . Choice ( [ "nearest" , "linear" , "cubic" ] , p = [ 0.05 , 0.6 , 0.35 ] ) sigmoid_thresh_default = iap . Normal ( 0.0 , 5.0 ) noise = iap . SimplexNoise ( size_px_max = size_px_max , upscale_method = upscale_method if upscale_method is not None else upscale_method_default ) if iterations != 1 : noise = iap . IterativeNoiseAggregator ( noise , iterations = iterations , aggregation_method = aggregation_method ) if sigmoid is False or ( ia . is_single_number ( sigmoid ) and sigmoid <= 0.01 ) : noise = iap . Sigmoid . create_for_noise ( noise , threshold = sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default , activated = sigmoid ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AlphaElementwise ( factor = noise , first = first , second = second , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
13050	def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : response . status = http . client . MOVED_PERMANENTLY else : response . status = http . client . TEMPORARY_REDIRECT else : response . status = http . client . PERMANENT_REDIRECT response . close ( )
6466	def color ( self , index ) : if self . colors == 16 : if index >= 8 : return self . csi ( 'bold' ) + self . csi ( 'setaf' , index - 8 ) else : return self . csi ( 'sgr0' ) + self . csi ( 'setaf' , index ) else : return self . csi ( 'setaf' , index )
12300	def search ( self , what , name = None , version = None ) : filtered = { } if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k_name , k_version ) = key if name is not None and k_name != name : continue if version is not None and k_version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) return filtered
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
10699	def set ( self , key , value ) : if self . in_memory : self . _memory_db [ key ] = value else : db = self . _read_file ( ) db [ key ] = value with open ( self . db_path , 'w' ) as f : f . write ( json . dumps ( db , ensure_ascii = False , indent = 2 ) )
3867	async def set_notification_level ( self , level ) : await self . _client . set_conversation_notification_level ( hangouts_pb2 . SetConversationNotificationLevelRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , level = level , ) )
7860	def _request_tls ( self ) : self . requested = True element = ElementTree . Element ( STARTTLS_TAG ) self . stream . write_element ( element )
1669	def FlagCxx14Features ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] include = Match ( r'\s*#\s*include\s+[<"]([^<"]+)[">]' , line ) if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : error ( filename , linenum , 'build/c++14' , 5 , ( '<%s> is an unapproved C++14 header.' ) % include . group ( 1 ) )
2751	def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( ** jsoned ) image . token = self . token images . append ( image ) return images
1700	def consume ( self , consume_function ) : from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet consume_streamlet = ConsumeStreamlet ( consume_function , self ) self . _add_child ( consume_streamlet ) return
4969	def get_catalog_options ( self ) : if hasattr ( self . instance , 'site' ) : catalog_api = CourseCatalogApiClient ( self . user , self . instance . site ) else : catalog_api = CourseCatalogApiClient ( self . user ) catalogs = catalog_api . get_all_catalogs ( ) catalogs = sorted ( catalogs , key = lambda catalog : catalog . get ( 'name' , '' ) . lower ( ) ) return BLANK_CHOICE_DASH + [ ( catalog [ 'id' ] , catalog [ 'name' ] , ) for catalog in catalogs ]
4621	def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
1849	def LOOP ( cpu , dest ) : counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter == 0 , ( cpu . PC + dest . read ( ) ) & ( ( 1 << dest . size ) - 1 ) , cpu . PC + cpu . instruction . size )
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
5330	def get_identities ( config ) : TaskProjects ( config ) . execute ( ) task = TaskIdentitiesMerge ( config ) task . execute ( ) logging . info ( "Merging identities finished!" )
12079	def figure_sweeps ( self , offsetX = 0 , offsetY = 0 ) : self . log . debug ( "creating overlayed sweeps plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) self . setColorBySweep ( ) plt . plot ( self . abf . sweepX2 + sweep * offsetX , self . abf . sweepY + sweep * offsetY , ** self . kwargs ) if offsetX : self . marginX = .05 self . decorate ( )
11561	def play_tone ( self , pin , tone_command , frequency , duration ) : if tone_command == self . TONE_TONE : if duration : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , duration & 0x7f , ( duration >> 7 ) & 0x7f ] else : data = [ tone_command , pin , frequency & 0x7f , ( frequency >> 7 ) & 0x7f , 0 , 0 ] self . _command_handler . digital_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_MODE ] = self . TONE else : data = [ tone_command , pin ] self . _command_handler . send_sysex ( self . _command_handler . TONE_PLAY , data )
9524	def sort_by_size ( infile , outfile , smallest_first = False ) : seqs = { } file_to_dict ( infile , seqs ) seqs = list ( seqs . values ( ) ) seqs . sort ( key = lambda x : len ( x ) , reverse = not smallest_first ) fout = utils . open_file_write ( outfile ) for seq in seqs : print ( seq , file = fout ) utils . close ( fout )
5140	def new_noncomment ( self , start_lineno , end_lineno ) : block = NonComment ( start_lineno , end_lineno ) self . blocks . append ( block ) self . current_block = block
383	def pt2map ( list_points = None , size = ( 100 , 100 ) , val = 1 ) : if list_points is None : raise Exception ( "list_points : list of 2 int" ) i_m = np . zeros ( size ) if len ( list_points ) == 0 : return i_m for xx in list_points : for x in xx : i_m [ int ( np . round ( x [ 0 ] ) ) ] [ int ( np . round ( x [ 1 ] ) ) ] = val return i_m
7733	def make_kick_request ( self , nick , reason ) : self . clear_muc_child ( ) self . muc_child = MucAdminQuery ( parent = self . xmlnode ) item = MucItem ( "none" , "none" , nick = nick , reason = reason ) self . muc_child . add_item ( item ) return self . muc_child
4056	def _json_processor ( self , retrieved ) : json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict try : items = [ json . loads ( e [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) for e in retrieved . entries ] except KeyError : return self . _tags_data ( retrieved ) return items
13566	def plot ( * args , ax = None , ** kwargs ) : if ax is None : fig , ax = _setup_axes ( ) pl = ax . plot ( * args , ** kwargs ) if _np . shape ( args ) [ 0 ] > 1 : if type ( args [ 1 ] ) is not str : min_x = min ( args [ 0 ] ) max_x = max ( args [ 0 ] ) ax . set_xlim ( ( min_x , max_x ) ) return pl
6508	def generate_field_filters ( cls , ** kwargs ) : generator = _load_class ( getattr ( settings , "SEARCH_FILTER_GENERATOR" , None ) , cls ) ( ) return ( generator . field_dictionary ( ** kwargs ) , generator . filter_dictionary ( ** kwargs ) , generator . exclude_dictionary ( ** kwargs ) , )
4203	def LEVINSON ( r , order = None , allow_singularity = False ) : r T0 = numpy . real ( r [ 0 ] ) T = r [ 1 : ] M = len ( T ) if order is None : M = len ( T ) else : assert order <= M , 'order must be less than size of the input data' M = order realdata = numpy . isrealobj ( r ) if realdata is True : A = numpy . zeros ( M , dtype = float ) ref = numpy . zeros ( M , dtype = float ) else : A = numpy . zeros ( M , dtype = complex ) ref = numpy . zeros ( M , dtype = complex ) P = T0 for k in range ( 0 , M ) : save = T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] temp = - save / P if realdata : P = P * ( 1. - temp ** 2. ) else : P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 and allow_singularity == False : raise ValueError ( "singular matrix" ) A [ k ] = temp ref [ k ] = temp if k == 0 : continue khalf = ( k + 1 ) // 2 if realdata is True : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] if j != kj : A [ kj ] += temp * save else : for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) return A , P , ref
13090	def main ( branch ) : try : output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : return ensure_remote_branch_is_tracked ( branch ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
8891	def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( ** kwargs )
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
10293	def expand_internal_causal ( universe : BELGraph , graph : BELGraph ) -> None : expand_internal ( universe , graph , edge_predicates = is_causal_relation )
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
9958	def restore_ipython ( self ) : if not self . is_ipysetup : return shell_class = type ( self . shell ) shell_class . showtraceback = shell_class . default_showtraceback del shell_class . default_showtraceback self . is_ipysetup = False
7240	def window_cover ( self , window_shape , pad = True ) : size_y , size_x = window_shape [ 0 ] , window_shape [ 1 ] _ndepth , _nheight , _nwidth = self . shape nheight , _m = divmod ( _nheight , size_y ) nwidth , _n = divmod ( _nwidth , size_x ) img = self if pad is True : new_height , new_width = _nheight , _nwidth if _m != 0 : new_height = ( nheight + 1 ) * size_y if _n != 0 : new_width = ( nwidth + 1 ) * size_x if ( new_height , new_width ) != ( _nheight , _nwidth ) : bounds = box ( 0 , 0 , new_width , new_height ) geom = ops . transform ( self . __geo_transform__ . fwd , bounds ) img = self [ geom ] row_lims = range ( 0 , img . shape [ 1 ] , size_y ) col_lims = range ( 0 , img . shape [ 2 ] , size_x ) for maxy , maxx in product ( row_lims , col_lims ) : reg = img [ : , maxy : ( maxy + size_y ) , maxx : ( maxx + size_x ) ] if pad is False : if reg . shape [ 1 : ] == window_shape : yield reg else : yield reg
12090	def proto_01_13_steps025dual ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) f1 = swhlab . ap . getAvgBySweep ( abf , 'freq' , None , 1 ) f2 = swhlab . ap . getAvgBySweep ( abf , 'freq' , 1 , None ) f1 = np . nan_to_num ( f1 ) f2 = np . nan_to_num ( f2 ) Xs = abf . clampValues ( abf . dataX [ int ( abf . protoSeqX [ 1 ] + .01 ) ] ) swhlab . plot . new ( abf , title = "gain function" , xlabel = "command current (pA)" , ylabel = "average inst. freq. (Hz)" ) pylab . plot ( Xs , f1 , '.-' , ms = 20 , alpha = .5 , label = "step 1" , color = 'b' ) pylab . plot ( Xs , f2 , '.-' , ms = 20 , alpha = .5 , label = "step 2" , color = 'r' ) pylab . legend ( loc = 'upper left' ) pylab . axis ( [ Xs [ 0 ] , Xs [ - 1 ] , None , None ] ) swhlab . plot . save ( abf , tag = 'gain' )
7762	def make_error_response ( self , cond ) : if self . stanza_type == "error" : raise ValueError ( "Errors may not be generated in response" " to errors" ) msg = Message ( stanza_type = "error" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id , error_cond = cond , subject = self . _subject , body = self . _body , thread = self . _thread ) if self . _payload is None : self . decode_payload ( ) for payload in self . _payload : msg . add_payload ( payload . copy ( ) ) return msg
11837	def actions ( self , state ) : "In the leftmost empty column, try all non-conflicting rows." if state [ - 1 ] is not None : return [ ] else : col = state . index ( None ) return [ row for row in range ( self . N ) if not self . conflicted ( state , row , col ) ]
5589	def hillshade ( elevation , tile , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : azimuth = float ( azimuth ) altitude = float ( altitude ) z = float ( z ) scale = float ( scale ) xres = tile . tile . pixel_x_size yres = - tile . tile . pixel_y_size slope , aspect = calculate_slope_aspect ( elevation , xres , yres , z = z , scale = scale ) deg2rad = math . pi / 180.0 shaded = np . sin ( altitude * deg2rad ) * np . sin ( slope ) + np . cos ( altitude * deg2rad ) * np . cos ( slope ) * np . cos ( ( azimuth - 90.0 ) * deg2rad - aspect ) shaded = ( ( ( shaded + 1.0 ) / 2 ) * - 255.0 ) . astype ( "uint8" ) return ma . masked_array ( data = np . pad ( shaded , 1 , mode = 'edge' ) , mask = elevation . mask )
2771	def get_object ( cls , api_token , id ) : load_balancer = cls ( token = api_token , id = id ) load_balancer . load ( ) return load_balancer
10108	def normalize_name ( s ) : s = s . replace ( '-' , '_' ) . replace ( '.' , '_' ) . replace ( ' ' , '_' ) if s in keyword . kwlist : return s + '_' s = '_' . join ( slug ( ss , lowercase = False ) for ss in s . split ( '_' ) ) if not s : s = '_' if s [ 0 ] not in string . ascii_letters + '_' : s = '_' + s return s
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , ** saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
8222	def do_toggle_variables ( self , action ) : self . show_vars = action . get_active ( ) if self . show_vars : self . show_variables_window ( ) else : self . hide_variables_window ( )
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
3201	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id ) , data = data )
11648	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) self . train_ = X memory = get_memory ( self . memory ) lo , = memory . cache ( scipy . linalg . eigvalsh ) ( X , eigvals = ( 0 , 0 ) ) self . shift_ = max ( self . min_eig - lo , 0 ) return self
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
6371	def fallout ( self ) : r if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : raise ValueError return index
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
2922	def _clear_celery_task_data ( self , my_task ) : if 'task_id' in my_task . internal_data : history = my_task . _get_internal_data ( 'task_history' , [ ] ) history . append ( my_task . _get_internal_data ( 'task_id' ) ) del my_task . internal_data [ 'task_id' ] my_task . _set_internal_data ( task_history = history ) if 'task_state' in my_task . internal_data : del my_task . internal_data [ 'task_state' ] if 'error' in my_task . internal_data : del my_task . internal_data [ 'error' ] if hasattr ( my_task , 'async_call' ) : delattr ( my_task , 'async_call' ) if hasattr ( my_task , 'deserialized' ) : delattr ( my_task , 'deserialized' )
11345	def handle_starttag ( self , tag , attrs ) : if tag in self . mathml_elements : final_attr = "" for key , value in attrs : final_attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final_attr ) )
5001	def _get_enterprise_customer_users_batch ( self , start , end ) : LOGGER . info ( 'Fetching new batch of enterprise customer users from indexes: %s to %s' , start , end ) return User . objects . filter ( pk__in = self . _get_enterprise_customer_user_ids ( ) ) [ start : end ]
6808	def configure_hdmi ( self ) : r = self . local_renderer r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_force_hotplug' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_drive' , value = 2 , use_sudo = True , )
562	def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )
13350	def add_file ( self , file , ** kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
3822	async def easter_egg ( self , easter_egg_request ) : response = hangouts_pb2 . EasterEggResponse ( ) await self . _pb_request ( 'conversations/easteregg' , easter_egg_request , response ) return response
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
6592	def poll ( self ) : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_all_finished_pkgidx_result_pairs ( ) return ret
1162	def set ( self ) : with self . __cond : self . __flag = True self . __cond . notify_all ( )
931	def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )
2812	def convert_squeeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting squeeze ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert squeeze by multiple dimensions' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) ) : import tensorflow as tf return tf . squeeze ( x , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
867	def clear ( cls ) : super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = False )
5928	def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
5871	def fetch_course_organizations ( course_key ) : queryset = internal . OrganizationCourse . objects . filter ( course_id = text_type ( course_key ) , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
11350	def merge_kwargs ( self , kwargs ) : if kwargs : self . parser_kwargs . update ( kwargs ) self . parser_kwargs . setdefault ( 'dest' , self . name ) if 'default' in kwargs : self . parser_kwargs [ "default" ] = kwargs [ "default" ] self . parser_kwargs [ "required" ] = False elif 'action' in kwargs : if kwargs [ 'action' ] in set ( [ 'store_false' , 'store_true' ] ) : self . parser_kwargs [ 'required' ] = False elif kwargs [ 'action' ] in set ( [ 'version' ] ) : self . parser_kwargs . pop ( 'required' , False ) else : self . parser_kwargs . setdefault ( "required" , True )
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url self . _add_annotation_to_graph ( graph ) return namespace
7185	def copy_type_comments_to_annotations ( args ) : for arg in args . args : copy_type_comment_to_annotation ( arg ) if args . vararg : copy_type_comment_to_annotation ( args . vararg ) for arg in args . kwonlyargs : copy_type_comment_to_annotation ( arg ) if args . kwarg : copy_type_comment_to_annotation ( args . kwarg )
11247	def triangle_area ( point1 , point2 , point3 ) : a = point_distance ( point1 , point2 ) b = point_distance ( point1 , point3 ) c = point_distance ( point2 , point3 ) s = ( a + b + c ) / 2.0 return math . sqrt ( s * ( s - a ) * ( s - b ) * ( s - c ) )
4275	def zip ( self ) : zip_gallery = self . settings [ 'zip_gallery' ] if zip_gallery and len ( self ) > 0 : zip_gallery = zip_gallery . format ( album = self ) archive_path = join ( self . dst_path , zip_gallery ) if ( self . settings . get ( 'zip_skip_if_exists' , False ) and isfile ( archive_path ) ) : self . logger . debug ( "Archive %s already created, passing" , archive_path ) return zip_gallery archive = zipfile . ZipFile ( archive_path , 'w' , allowZip64 = True ) attr = ( 'src_path' if self . settings [ 'zip_media_format' ] == 'orig' else 'dst_path' ) for p in self : path = getattr ( p , attr ) try : archive . write ( path , os . path . split ( path ) [ 1 ] ) except OSError as e : self . logger . warn ( 'Failed to add %s to the ZIP: %s' , p , e ) archive . close ( ) self . logger . debug ( 'Created ZIP archive %s' , archive_path ) return zip_gallery
9334	def full ( shape , value , dtype = 'f8' ) : shared = empty ( shape , dtype ) shared [ : ] = value return shared
13259	def combine ( self , members , output_file , dimension = None , start_index = None , stop_index = None , stride = None ) : nco = None try : nco = Nco ( ) except BaseException : raise ImportError ( "NCO not found. The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : members = [ m . path for m in members ] options = [ '-4' ] options += [ '-L' , '3' ] options += [ '-h' ] if dimension is not None : if start_index is None : start_index = 0 if stop_index is None : stop_index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start_index , stop_index , stride ) ] nco . ncrcat ( input = members , output = output_file , options = options )
9357	def sentences ( quantity = 2 , as_list = False ) : result = [ sntc . strip ( ) for sntc in random . sample ( get_dictionary ( 'lorem_ipsum' ) , quantity ) ] if as_list : return result else : return ' ' . join ( result )
3685	def set_from_PT ( self , Vs ) : good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
6030	def grid_interpolate ( func ) : @ wraps ( func ) def wrapper ( profile , grid , grid_radial_minimum = None , * args , ** kwargs ) : if hasattr ( grid , "interpolator" ) : interpolator = grid . interpolator if grid . interpolator is not None : values = func ( profile , interpolator . interp_grid , grid_radial_minimum , * args , ** kwargs ) if values . ndim == 1 : return interpolator . interpolated_values_from_values ( values = values ) elif values . ndim == 2 : y_values = interpolator . interpolated_values_from_values ( values = values [ : , 0 ] ) x_values = interpolator . interpolated_values_from_values ( values = values [ : , 1 ] ) return np . asarray ( [ y_values , x_values ] ) . T return func ( profile , grid , grid_radial_minimum , * args , ** kwargs ) return wrapper
896	def read ( cls , proto ) : tm = object . __new__ ( cls ) tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) tm . _random = Random ( ) tm . _random . read ( proto . random ) tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm
8219	def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
1147	def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , "__deepcopy__" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(deep)copyable object of type %s" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) return y
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size dest . write ( src . read ( ) ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
12041	def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
2228	def hash_data ( data , hasher = NoParam , base = NoParam , types = False , hashlen = NoParam , convert = False ) : if convert and isinstance ( data , six . string_types ) : try : data = json . dumps ( data ) except TypeError as ex : pass base = _rectify_base ( base ) hashlen = _rectify_hashlen ( hashlen ) hasher = _rectify_hasher ( hasher ) ( ) _update_hasher ( hasher , data , types = types ) text = _digest_hasher ( hasher , hashlen , base ) return text
13187	def image_path ( instance , filename ) : filename , ext = os . path . splitext ( filename . lower ( ) ) instance_id_hash = hashlib . md5 ( str ( instance . id ) ) . hexdigest ( ) filename_hash = '' . join ( random . sample ( hashlib . md5 ( filename . encode ( 'utf-8' ) ) . hexdigest ( ) , 8 ) ) return '{}/{}{}' . format ( instance_id_hash , filename_hash , ext )
8921	def _get_version ( self ) : if "version" in self . document . attrib : value = self . document . attrib [ "version" ] . lower ( ) if value in allowed_versions [ self . params [ 'service' ] ] : self . params [ "version" ] = value else : raise OWSInvalidParameterValue ( "Version %s is not supported" % value , value = "version" ) elif self . _get_request_type ( ) == "getcapabilities" : self . params [ "version" ] = None else : raise OWSMissingParameterValue ( 'Parameter "version" is missing' , value = "version" ) return self . params [ "version" ]
1354	def get_argument_cluster ( self ) : try : return self . get_argument ( constants . PARAM_CLUSTER ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
12425	def reverse ( self ) : if not self . test_drive and self . bumps : map ( lambda b : b . reverse ( ) , self . bumpers )
2032	def MLOAD ( self , address ) : self . _allocate ( address , 32 ) value = self . _load ( address , 32 ) return value
1	def nature_cnn ( unscaled_images , ** conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )
281	def plot_monthly_returns_dist ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) ax . hist ( 100 * monthly_ret_table , color = 'orangered' , alpha = 0.80 , bins = 20 , ** kwargs ) ax . axvline ( 100 * monthly_ret_table . mean ( ) , color = 'gold' , linestyle = '--' , lw = 4 , alpha = 1.0 ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 , alpha = 0.75 ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Number of months' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Distribution of monthly returns" ) return ax
11700	def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] egg_donors += gen_xx sperm_donors += gen_xy
4736	def info ( txt ) : print ( "%s# %s%s%s" % ( PR_EMPH_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
3256	def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
9747	def send_discovery_packet ( self ) : if self . port is None : return self . transport . sendto ( QRTDiscoveryP1 . pack ( QRTDiscoveryPacketSize , QRTPacketType . PacketDiscover . value ) + QRTDiscoveryP2 . pack ( self . port ) , ( "<broadcast>" , 22226 ) , )
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
8143	def rotate ( self , angle ) : from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
9546	def add_record_check ( self , record_check , modulus = 1 ) : assert callable ( record_check ) , 'record check must be a callable function' t = record_check , modulus self . _record_checks . append ( t )
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
10991	def makestate ( im , pos , rad , slab = None , mem_level = 'hi' ) : if slab is not None : o = comp . ComponentCollection ( [ objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) , slab ] , category = 'obj' ) else : o = objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) p = exactpsf . FixedSSChebLinePSF ( ) npts , iorder = _calc_ilm_order ( im . get_image ( ) . shape ) i = ilms . BarnesStreakLegPoly2P1D ( npts = npts , zorder = iorder ) b = ilms . LegendrePoly2P1D ( order = ( 9 , 3 , 5 ) , category = 'bkg' ) c = comp . GlobalScalar ( 'offset' , 0.0 ) s = states . ImageState ( im , [ o , i , b , c , p ] ) runner . link_zscale ( s ) if mem_level != 'hi' : s . set_mem_level ( mem_level ) opt . do_levmarq ( s , [ 'ilm-scale' ] , max_iter = 1 , run_length = 6 , max_mem = 1e4 ) return s
6081	def potential_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . potential_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
4042	def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment parsed = list ( urlparse ( self . self_link ) ) stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : return None
10521	def oneright ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 maxValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue >= 1 : raise LdtpServerException ( 'Maximum limit reached' ) object_handle . AXValue += maxValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to increase scrollbar' )
13461	def event_update_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) updates = Update . objects . filter ( event__slug = slug ) if event . recently_ended ( ) : updates = updates . order_by ( 'id' ) else : updates = updates . order_by ( '-id' ) return render ( request , 'happenings/updates/update_list.html' , { 'event' : event , 'object_list' : updates , } )
13651	def get_reference_data ( self , modified_since : Optional [ datetime . datetime ] = None ) -> GetReferenceDataResponse : if modified_since is None : modified_since = datetime . datetime ( year = 2010 , month = 1 , day = 1 ) response = requests . get ( '{}/lovs' . format ( API_URL_BASE ) , headers = { 'if-modified-since' : self . _format_dt ( modified_since ) , ** self . _get_headers ( ) , } , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetReferenceDataResponse . deserialize ( response . json ( ) )
505	def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector
7852	def get_identities ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:identity" ) if l is not None : for i in l : ret . append ( DiscoIdentity ( self , i ) ) return ret
5166	def __intermediate_dns_servers ( self , uci , address ) : if 'dns' in uci : return uci [ 'dns' ] if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
12696	def to_XML ( self ) : marcxml_template = oai_template = leader = self . leader if self . leader is not None else "" if leader : leader = "<leader>" + leader + "</leader>" if self . oai_marc : leader = "" xml_template = oai_template if self . oai_marc else marcxml_template xml_output = Template ( xml_template ) . substitute ( LEADER = leader . strip ( ) , CONTROL_FIELDS = self . _serialize_ctl_fields ( ) . strip ( ) , DATA_FIELDS = self . _serialize_data_fields ( ) . strip ( ) ) return xml_output
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : if element . next is not None : return element elif element . prev is not None : return element return Plain ( * element . content )
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
13832	def _SkipFieldValue ( tokenizer ) : if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
1896	def _is_sat ( self ) -> bool : logger . debug ( "Solver.check() " ) start = time . time ( ) self . _send ( '(check-sat)' ) status = self . _recv ( ) logger . debug ( "Check took %s seconds (%s)" , time . time ( ) - start , status ) if status not in ( 'sat' , 'unsat' , 'unknown' ) : raise SolverError ( status ) if consider_unknown_as_unsat : if status == 'unknown' : logger . info ( 'Found an unknown core, probably a solver timeout' ) status = 'unsat' if status == 'unknown' : raise SolverUnknown ( status ) return status == 'sat'
12227	def bind_proxy ( values , category = None , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : addrs = OrderedDict ( ) depth = 3 for local_name , locals_dict in traverse_local_prefs ( depth ) : addrs [ id ( locals_dict [ local_name ] ) ] = local_name proxies = [ ] locals_dict = get_frame_locals ( depth ) for value in values : id_val = id ( value ) if id_val in addrs : local_name = addrs [ id_val ] local_val = locals_dict [ local_name ] if isinstance ( local_val , PatchedLocal ) and not isinstance ( local_val , PrefProxy ) : proxy = PrefProxy ( local_name , value . val , category = category , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) app_name = locals_dict [ '__name__' ] . split ( '.' ) [ - 2 ] prefs = get_prefs ( ) if app_name not in prefs : prefs [ app_name ] = OrderedDict ( ) prefs [ app_name ] [ local_name . lower ( ) ] = proxy locals_dict [ local_name ] = proxy proxies . append ( proxy ) return proxies
8623	def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
6026	def geometry_from_grid ( self , grid , pixel_centres , pixel_neighbors , pixel_neighbors_size , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer shape_arcsec = ( y_max - y_min , x_max - x_min ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) return self . Geometry ( shape_arcsec = shape_arcsec , pixel_centres = pixel_centres , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
4942	def enterprise_customer_uuid ( self ) : try : enterprise_user = EnterpriseCustomerUser . objects . get ( user_id = self . user . id ) except ObjectDoesNotExist : LOGGER . warning ( 'User {} has a {} assignment but is not linked to an enterprise!' . format ( self . __class__ , self . user . id ) ) return None except MultipleObjectsReturned : LOGGER . warning ( 'User {} is linked to multiple enterprises, which is not yet supported!' . format ( self . user . id ) ) return None return str ( enterprise_user . enterprise_customer . uuid )
11195	def cp ( resume , quiet , dataset_uri , dest_base_uri ) : _copy ( resume , quiet , dataset_uri , dest_base_uri )
12610	def _concat_queries ( queries , operators = '__and__' ) : if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
11266	def sh ( prev , * args , ** kw ) : endl = '\n' if 'endl' not in kw else kw . pop ( 'endl' ) trim = None if 'trim' not in kw else kw . pop ( 'trim' ) if trim is None : trim = bytes . rstrip if is_py3 else str . rstrip cmdline = ' ' . join ( args ) if not cmdline : if prev is not None : for i in prev : yield i else : while True : yield None process = subprocess . Popen ( cmdline , shell = True , stdin = subprocess . PIPE , stdout = subprocess . PIPE , ** kw ) if prev is not None : stdin_buffer = StringIO ( ) for i in prev : stdin_buffer . write ( i ) if endl : stdin_buffer . write ( endl ) if is_py3 : process . stdin . write ( stdin_buffer . getvalue ( ) . encode ( 'utf-8' ) ) else : process . stdin . write ( stdin_buffer . getvalue ( ) ) process . stdin . flush ( ) process . stdin . close ( ) stdin_buffer . close ( ) for line in process . stdout : yield trim ( line ) process . wait ( )
12892	def handle_long ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u32 . text ) or None
11379	def extract_oembeds ( text , args = None ) : resource_type = width = height = None if args : dimensions = args . lower ( ) . split ( 'x' ) if len ( dimensions ) in ( 3 , 1 ) : resource_type = dimensions . pop ( ) if len ( dimensions ) == 2 : width , height = map ( lambda x : int ( x ) , dimensions ) client = OEmbedConsumer ( ) return client . extract ( text , width , height , resource_type )
6933	def colormagdiagram_cpdir ( cpdir , outpkl , cpfileglob = 'checkplot*.pkl*' , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return colormagdiagram_cplist ( cplist , outpkl , color_mag1 = color_mag1 , color_mag2 = color_mag2 , yaxis_mag = yaxis_mag )
8403	def squish_infinite ( x , range = ( 0 , 1 ) ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) x [ x == - np . inf ] = range [ 0 ] x [ x == np . inf ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
11658	def transform ( self , X ) : X = check_array ( X , copy = self . copy ) X *= self . scale_ X += self . min_ if self . truncate : np . maximum ( self . feature_range [ 0 ] , X , out = X ) np . minimum ( self . feature_range [ 1 ] , X , out = X ) return X
3994	def _load_ssh_auth_pre_yosemite ( ) : for process in psutil . process_iter ( ) : if process . name ( ) == 'ssh-agent' : ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'bsexec' , str ( process . pid ) , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) if ssh_auth_sock : _set_ssh_auth_sock ( ssh_auth_sock ) break else : daemon_warnings . warn ( 'ssh' , 'No running ssh-agent found linked to SSH_AUTH_SOCK' )
13145	def remove_near_duplicate_relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) _assert_threshold ( threshold ) duplicate_rel_counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate_rel_counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num_triples = len ( triples ) removal_relation_set = set ( ) for rel , values in duplicate_rel_counter . items ( ) : duplicate_rel_counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal_relation_set or j in removal_relation_set : continue close_relations = [ i ] if _set_close_to ( duplicate_rel_counter [ i ] , duplicate_rel_counter [ j ] , threshold ) : close_relations . append ( j ) if len ( close_relations ) > 1 : close_relations . pop ( np . random . randint ( len ( close_relations ) ) ) removal_relation_set |= set ( close_relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal_relation_set ) , str ( removal_relation_set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal_relation_set , triples ) )
7260	def get_data_location ( self , catalog_id ) : try : record = self . get ( catalog_id ) except : return None if 'Landsat8' in record [ 'type' ] and 'LandsatAcquisition' in record [ 'type' ] : bucket = record [ 'properties' ] [ 'bucketName' ] prefix = record [ 'properties' ] [ 'bucketPrefix' ] return 's3://' + bucket + '/' + prefix if 'DigitalGlobeAcquisition' in record [ 'type' ] : o = Ordering ( ) res = o . location ( [ catalog_id ] ) return res [ 'acquisitions' ] [ 0 ] [ 'location' ] return None
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
1407	def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , direct_task = None , need_task_ids = False ) : self . pplan_helper . check_output_schema ( stream , tup ) custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) data_tuple = tuple_pb2 . HeronDataTuple ( ) data_tuple . key = 0 if direct_task is not None : if not isinstance ( direct_task , int ) : raise TypeError ( "direct_task argument needs to be an integer, given: %s" % str ( type ( direct_task ) ) ) data_tuple . dest_task_ids . append ( direct_task ) elif custom_target_task_ids is not None : for task_id in custom_target_task_ids : data_tuple . dest_task_ids . append ( task_id ) if tup_id is not None : tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) if self . acking_enabled : root = data_tuple . roots . add ( ) root . taskid = self . pplan_helper . my_task_id root . key = tuple_info . key self . in_flight_tuples [ tuple_info . key ] = tuple_info else : self . immediate_acks . append ( tuple_info ) tuple_size_in_bytes = 0 start_time = time . time ( ) for obj in tup : serialized = self . serializer . serialize ( obj ) data_tuple . values . append ( serialized ) tuple_size_in_bytes += len ( serialized ) serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , tuple_size_in_bytes = tuple_size_in_bytes ) self . total_tuples_emitted += 1 self . spout_metrics . update_emit_count ( stream ) if need_task_ids : sent_task_ids = custom_target_task_ids or [ ] if direct_task is not None : sent_task_ids . append ( direct_task ) return sent_task_ids
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
8902	def _multiple_self_ref_fk_check ( class_model ) : self_fk = [ ] for f in class_model . _meta . concrete_fields : if f . related_model in self_fk : return True if f . related_model == class_model : self_fk . append ( class_model ) return False
9293	def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
1977	def sys_deallocate ( self , cpu , addr , size ) : logger . info ( "DEALLOCATE(0x%08x, %d)" % ( addr , size ) ) if addr & 0xfff != 0 : logger . info ( "DEALLOCATE: addr is not page aligned" ) return Decree . CGC_EINVAL if size == 0 : logger . info ( "DEALLOCATE:length is zero" ) return Decree . CGC_EINVAL cpu . memory . munmap ( addr , size ) self . syscall_trace . append ( ( "_deallocate" , - 1 , size ) ) return 0
5191	def send_direct_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . DirectOperate ( command_set , callback , config )
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
4750	def start ( self ) : self . __thread = Threads ( target = self . run , args = ( True , True , False ) ) self . __thread . setDaemon ( True ) self . __thread . start ( )
173	def draw_heatmap_array ( self , image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : heatmap_lines = self . draw_lines_heatmap_array ( image_shape , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) if size_points <= 0 : return heatmap_lines heatmap_points = self . draw_points_heatmap_array ( image_shape , alpha = alpha_points , size = size_points , raise_if_out_of_image = raise_if_out_of_image ) heatmap = np . dstack ( [ heatmap_lines , heatmap_points ] ) return np . max ( heatmap , axis = 2 )
8433	def cubehelix_pal ( start = 0 , rot = .4 , gamma = 1.0 , hue = 0.8 , light = .85 , dark = .15 , reverse = False ) : cdict = mpl . _cm . cubehelix ( gamma , start , rot , hue ) cubehelix_cmap = mpl . colors . LinearSegmentedColormap ( 'cubehelix' , cdict ) def cubehelix_palette ( n ) : values = np . linspace ( light , dark , n ) return [ mcolors . rgb2hex ( cubehelix_cmap ( x ) ) for x in values ] return cubehelix_palette
8564	def create_loadbalancer ( self , datacenter_id , loadbalancer ) : data = json . dumps ( self . _create_loadbalancer_dict ( loadbalancer ) ) response = self . _perform_request ( url = '/datacenters/%s/loadbalancers' % datacenter_id , method = 'POST' , data = data ) return response
13244	def temp_db ( db , name = None ) : if name is None : name = temp_name ( ) db . create ( name ) if not db . exists ( name ) : raise DatabaseError ( 'failed to create database %s!' ) try : yield name finally : db . drop ( name ) if db . exists ( name ) : raise DatabaseError ( 'failed to drop database %s!' )
3940	async def listen ( self ) : retries = 0 need_new_sid = True while retries <= self . _max_retries : if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) logger . error ( 'Ran out of retries for long-polling request' )
2871	def remove_event_detect ( self , pin ) : self . mraa_gpio . Gpio . isrExit ( self . mraa_gpio . Gpio ( pin ) )
5148	def write ( self , name , path = './' ) : byte_object = self . generate ( ) file_name = '{0}.tar.gz' . format ( name ) if not path . endswith ( '/' ) : path += '/' f = open ( '{0}{1}' . format ( path , file_name ) , 'wb' ) f . write ( byte_object . getvalue ( ) ) f . close ( )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
3667	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpgms = [ i ( T ) for i in self . HeatCapacityGases ] return mixing_simple ( zs , Cpgms ) else : raise Exception ( 'Method not valid' )
730	def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
9480	def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
13661	def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
11079	def stop_timer ( self , func ) : if func in self . _timer_callbacks : t = self . _timer_callbacks [ func ] t . cancel ( ) del self . _timer_callbacks [ func ]
984	def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , "# predicted => active cells per column for each sequence" , numCellsPerColumn )
3081	def xsrf_secret_key ( ) : secret = memcache . get ( XSRF_MEMCACHE_ID , namespace = OAUTH2CLIENT_NAMESPACE ) if not secret : model = SiteXsrfSecretKey . get_or_insert ( key_name = 'site' ) if not model . secret : model . secret = _generate_new_xsrf_secret_key ( ) model . put ( ) secret = model . secret memcache . add ( XSRF_MEMCACHE_ID , secret , namespace = OAUTH2CLIENT_NAMESPACE ) return str ( secret )
4267	def generate_image ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) if settings [ 'use_orig' ] or source . endswith ( '.gif' ) : utils . copy ( source , outname , symlink = settings [ 'orig_link' ] ) return img = _read_image ( source ) original_format = img . format if settings [ 'copy_exif_data' ] and settings [ 'autorotate_images' ] : logger . warning ( "The 'autorotate_images' and 'copy_exif_data' settings " "are not compatible because Sigal can't save the " "modified Orientation tag." ) if settings [ 'copy_exif_data' ] and _has_exif_tags ( img ) : if options is not None : options = deepcopy ( options ) else : options = { } options [ 'exif' ] = img . info [ 'exif' ] if settings [ 'autorotate_images' ] : try : img = Transpose ( ) . process ( img ) except ( IOError , IndexError ) : pass if settings [ 'img_processor' ] : try : logger . debug ( 'Processor: %s' , settings [ 'img_processor' ] ) processor_cls = getattr ( pilkit . processors , settings [ 'img_processor' ] ) except AttributeError : logger . error ( 'Wrong processor name: %s' , settings [ 'img_processor' ] ) sys . exit ( ) width , height = settings [ 'img_size' ] if img . size [ 0 ] < img . size [ 1 ] : height , width = width , height processor = processor_cls ( width , height , upscale = False ) img = processor . process ( img ) for receiver in signals . img_resized . receivers_for ( img ) : img = receiver ( img , settings = settings ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save resized image to %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
4148	def centerdc_gen ( self ) : for a in range ( 0 , self . N ) : yield ( a - self . N / 2 ) * self . df
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
663	def getCentreAndSpreadOffsets ( spaceShape , spreadShape , stepSize = 1 ) : from nupic . math . cross import cross shape = spaceShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : centerOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) centerOffsets = list ( cross ( yPositions , xPositions ) ) numCenterOffsets = len ( centerOffsets ) print "centerOffsets:" , centerOffsets shape = spreadShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : spreadOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) spreadOffsets = list ( cross ( yPositions , xPositions ) ) spreadOffsets . remove ( ( 0 , 0 ) ) spreadOffsets . insert ( 0 , ( 0 , 0 ) ) numSpreadOffsets = len ( spreadOffsets ) print "spreadOffsets:" , spreadOffsets return centerOffsets , spreadOffsets
4873	def create ( self , validated_data ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) lms_user = validated_data . get ( 'lms_user_id' ) tpa_user = validated_data . get ( 'tpa_user_id' ) user_email = validated_data . get ( 'user_email' ) course_run_id = validated_data . get ( 'course_run_id' ) course_mode = validated_data . get ( 'course_mode' ) cohort = validated_data . get ( 'cohort' ) email_students = validated_data . get ( 'email_students' ) is_active = validated_data . get ( 'is_active' ) enterprise_customer_user = lms_user or tpa_user or user_email if isinstance ( enterprise_customer_user , models . EnterpriseCustomerUser ) : validated_data [ 'enterprise_customer_user' ] = enterprise_customer_user try : if is_active : enterprise_customer_user . enroll ( course_run_id , course_mode , cohort = cohort ) else : enterprise_customer_user . unenroll ( course_run_id ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError , HttpClientError ) as exc : validated_data [ 'detail' ] = str ( exc ) return validated_data if is_active : track_enrollment ( 'enterprise-customer-enrollment-api' , enterprise_customer_user . user_id , course_run_id ) else : if is_active : enterprise_customer_user = enterprise_customer . enroll_user_pending_registration ( user_email , course_mode , course_run_id , cohort = cohort ) else : enterprise_customer . clear_pending_registration ( user_email , course_run_id ) if email_students : enterprise_customer . notify_enrolled_learners ( self . context . get ( 'request_user' ) , course_run_id , [ enterprise_customer_user ] ) validated_data [ 'detail' ] = 'success' return validated_data
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
3601	def create_token ( self , data , options = None ) : if not options : options = { } options . update ( { 'admin' : self . admin , 'debug' : self . debug } ) claims = self . _create_options_claims ( options ) claims [ 'v' ] = self . TOKEN_VERSION claims [ 'iat' ] = int ( time . mktime ( time . gmtime ( ) ) ) claims [ 'd' ] = data return self . _encode_token ( self . secret , claims )
9879	def _random_coincidences ( value_domain , n , n_v ) : n_v_column = n_v . reshape ( - 1 , 1 ) return ( n_v_column . dot ( n_v_column . T ) - np . eye ( len ( value_domain ) ) * n_v_column ) / ( n - 1 )
10471	def _queueEvent ( self , event , args ) : if not hasattr ( self , 'eventList' ) : self . eventList = deque ( [ ( event , args ) ] ) return self . eventList . append ( ( event , args ) )
3653	def represent_pixel_location ( self ) : if self . data is None : return None return self . _data . reshape ( self . height + self . y_padding , int ( self . width * self . _num_components_per_pixel + self . x_padding ) )
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
10091	def setup ( app ) : if 'http' not in app . domains : httpdomain . setup ( app ) app . add_directive ( 'autopyramid' , RouteDirective )
2337	def remove_indirect_links ( g , alg = "aracne" , ** kwargs ) : alg = { "aracne" : aracne , "nd" : network_deconvolution , "clr" : clr } [ alg ] mat = np . array ( nx . adjacency_matrix ( g ) . todense ( ) ) return nx . relabel_nodes ( nx . DiGraph ( alg ( mat , ** kwargs ) ) , { idx : i for idx , i in enumerate ( list ( g . nodes ( ) ) ) } )
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
9439	def heartbeat ( ) : print "We got a call heartbeat notification\n" if request . method == 'POST' : print request . form else : print request . args return "OK"
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
3263	def get_workspace ( self , name ) : workspaces = self . get_workspaces ( names = name ) return self . _return_first_item ( workspaces )
3751	def TWA ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] : _TWA = ( _OntarioExposureLimits [ CASRN ] [ "TWA (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _TWA = None else : raise Exception ( 'Failure in in function' ) return _TWA
10267	def collapse_nodes_with_same_names ( graph : BELGraph ) -> None : survivor_mapping = defaultdict ( set ) victims = set ( ) it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number_of_nodes ( ) * ( graph . number_of_nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a_name , b_name = a . get ( NAME ) , b . get ( NAME ) if not a_name or not b_name or a_name . lower ( ) != b_name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : continue for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : continue survivor_mapping [ a ] . add ( b ) victims . add ( b ) collapse_nodes ( graph , survivor_mapping )
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
3979	def _get_referenced_services ( specs ) : active_services = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for service in app_spec [ 'depends' ] [ 'services' ] : active_services . add ( service ) for bundle_spec in specs [ 'bundles' ] . values ( ) : for service in bundle_spec [ 'services' ] : active_services . add ( service ) return active_services
12490	def count_node_match ( self , pattern , adict = None ) : mydict = self . _filetree if adict is None else adict k = 0 if isinstance ( mydict , dict ) : names = mydict . keys ( ) k += len ( filter_list ( names , pattern ) ) for nom in names : k += self . count_node_match ( pattern , mydict [ nom ] ) else : k = len ( filter_list ( mydict , pattern ) ) return k
2165	def format_commands ( self , ctx , formatter ) : self . format_command_subsection ( ctx , formatter , self . list_misc_commands ( ) , 'Commands' ) self . format_command_subsection ( ctx , formatter , self . list_resource_commands ( ) , 'Resources' )
10528	def _pybossa_req ( method , domain , id = None , payload = None , params = { } , headers = { 'content-type' : 'application/json' } , files = None ) : url = _opts [ 'endpoint' ] + '/api/' + domain if id is not None : url += '/' + str ( id ) if 'api_key' in _opts : params [ 'api_key' ] = _opts [ 'api_key' ] if method == 'get' : r = requests . get ( url , params = params ) elif method == 'post' : if files is None and headers [ 'content-type' ] == 'application/json' : r = requests . post ( url , params = params , headers = headers , data = json . dumps ( payload ) ) else : r = requests . post ( url , params = params , files = files , data = payload ) elif method == 'put' : r = requests . put ( url , params = params , headers = headers , data = json . dumps ( payload ) ) elif method == 'delete' : r = requests . delete ( url , params = params , headers = headers , data = json . dumps ( payload ) ) if r . status_code // 100 == 2 : if r . text and r . text != '""' : return json . loads ( r . text ) else : return True else : return json . loads ( r . text )
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
359	def load_file_list ( path = None , regx = '\.jpg' , printable = True , keep_prefix = False ) : r if path is None : path = os . getcwd ( ) file_list = os . listdir ( path ) return_list = [ ] for _ , f in enumerate ( file_list ) : if re . search ( regx , f ) : return_list . append ( f ) if keep_prefix : for i , f in enumerate ( return_list ) : return_list [ i ] = os . path . join ( path , f ) if printable : logging . info ( 'Match file list = %s' % return_list ) logging . info ( 'Number of files = %d' % len ( return_list ) ) return return_list
8474	def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : data = dict ( self . config . items ( section ) ) return data
3695	def Clapeyron ( T , Tc , Pc , dZ = 1 , Psat = 101325 ) : r Tr = T / Tc return R * T * dZ * log ( Pc / Psat ) / ( 1. - Tr )
12416	def end ( self , * args , ** kwargs ) : self . send ( * args , ** kwargs ) self . close ( )
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
3383	def __build_problem ( self ) : prob = constraint_matrices ( self . model , zero_tol = self . feasibility_tol ) equalities = prob . equalities b = prob . b bounds = np . atleast_2d ( prob . bounds ) . T var_bounds = np . atleast_2d ( prob . variable_bounds ) . T homogeneous = all ( np . abs ( b ) < self . feasibility_tol ) fixed_non_zero = np . abs ( prob . variable_bounds [ : , 1 ] ) > self . feasibility_tol fixed_non_zero &= prob . variable_fixed if any ( fixed_non_zero ) : n_fixed = fixed_non_zero . sum ( ) rows = np . zeros ( ( n_fixed , prob . equalities . shape [ 1 ] ) ) rows [ range ( n_fixed ) , np . where ( fixed_non_zero ) ] = 1.0 equalities = np . vstack ( [ equalities , rows ] ) var_b = prob . variable_bounds [ : , 1 ] b = np . hstack ( [ b , var_b [ fixed_non_zero ] ] ) homogeneous = False nulls = nullspace ( equalities ) return Problem ( equalities = shared_np_array ( equalities . shape , equalities ) , b = shared_np_array ( b . shape , b ) , inequalities = shared_np_array ( prob . inequalities . shape , prob . inequalities ) , bounds = shared_np_array ( bounds . shape , bounds ) , variable_fixed = shared_np_array ( prob . variable_fixed . shape , prob . variable_fixed , integer = True ) , variable_bounds = shared_np_array ( var_bounds . shape , var_bounds ) , nullspace = shared_np_array ( nulls . shape , nulls ) , homogeneous = homogeneous )
11299	def register ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s is not a subclass of BaseProvider' % provider_class . __name__ ) if provider_class in self . _registered_providers : raise AlreadyRegistered ( '%s is already registered' % provider_class . __name__ ) if issubclass ( provider_class , DjangoProvider ) : signals . post_save . connect ( self . invalidate_stored_oembeds , sender = provider_class . _meta . model ) self . _registered_providers . append ( provider_class ) self . invalidate_providers ( )
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
8981	def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
2893	def get_outgoing_sequence_names ( self ) : return sorted ( [ s . name for s in list ( self . outgoing_sequence_flows_by_id . values ( ) ) ] )
1638	def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : raw = clean_lines . lines_without_raw_strings line = raw [ linenum ] if ( IsBlankLine ( line ) and not nesting_state . InNamespaceBody ( ) and not nesting_state . InExternC ( ) ) : elided = clean_lines . elided prev_line = elided [ linenum - 1 ] prevbrace = prev_line . rfind ( '{' ) if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : exception = False if Match ( r' {6}\w' , prev_line ) : search_position = linenum - 2 while ( search_position >= 0 and Match ( r' {6}\w' , elided [ search_position ] ) ) : search_position -= 1 exception = ( search_position >= 0 and elided [ search_position ] [ : 5 ] == ' :' ) else : exception = ( Match ( r' {4}\w[^\(]*\)\s*(const\s*)?(\{\s*$|:)' , prev_line ) or Match ( r' {4}:' , prev_line ) ) if not exception : error ( filename , linenum , 'whitespace/blank_line' , 2 , 'Redundant blank line at the start of a code block ' 'should be deleted.' ) if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] if ( next_line and Match ( r'\s*}' , next_line ) and next_line . find ( '} else ' ) == - 1 ) : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Redundant blank line at the end of a code block ' 'should be deleted.' ) matched = Match ( r'\s*(public|protected|private):' , prev_line ) if matched : error ( filename , linenum , 'whitespace/blank_line' , 3 , 'Do not leave a blank line after "%s:"' % matched . group ( 1 ) ) next_line_start = 0 if linenum + 1 < clean_lines . NumLines ( ) : next_line = raw [ linenum + 1 ] next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) CheckComment ( line , filename , linenum , next_line_start , error ) line = clean_lines . elided [ linenum ] if Search ( r'\w\s+\[' , line ) and not Search ( r'(?:delete|return)\s+\[' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Extra space before [' ) if ( Search ( r'for *\(.*[^:]:[^: ]' , line ) or Search ( r'for *\(.*[^: ]:[^:]' , line ) ) : error ( filename , linenum , 'whitespace/forcolon' , 2 , 'Missing space around colon in range-based for loop' )
9432	def _c_func ( func , restype , argtypes , errcheck = None ) : func . restype = restype func . argtypes = argtypes if errcheck is not None : func . errcheck = errcheck return func
2336	def aracne ( m , ** kwargs ) : I0 = kwargs . get ( 'I0' , 0.0 ) W0 = kwargs . get ( 'W0' , 0.05 ) m = np . where ( m > I0 , m , 0 ) for i in range ( m . shape [ 0 ] - 2 ) : for j in range ( i + 1 , m . shape [ 0 ] - 1 ) : for k in range ( j + 1 , m . shape [ 0 ] ) : triplet = [ m [ i , j ] , m [ j , k ] , m [ i , k ] ] min_index , min_value = min ( enumerate ( triplet ) , key = operator . itemgetter ( 1 ) ) if 0 < min_value < W0 : if min_index == 0 : m [ i , j ] = m [ j , i ] = 0. elif min_index == 1 : m [ j , k ] = m [ k , j ] = 0. else : m [ i , k ] = m [ k , i ] = 0. return m
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
2306	def plot_gen ( epoch , batch , generated_variables , pairs_to_plot = [ [ 0 , 1 ] ] ) : from matplotlib import pyplot as plt if epoch == 0 : plt . ion ( ) plt . clf ( ) for ( i , j ) in pairs_to_plot : plt . scatter ( generated_variables [ i ] . data . cpu ( ) . numpy ( ) , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "Y -> X" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , generated_variables [ j ] . data . cpu ( ) . numpy ( ) , label = "X -> Y" ) plt . scatter ( batch . data . cpu ( ) . numpy ( ) [ : , i ] , batch . data . cpu ( ) . numpy ( ) [ : , j ] , label = "original data" ) plt . legend ( ) plt . pause ( 0.01 )
2270	def _win32_symlink2 ( path , link , allow_fallback = True , verbose = 0 ) : if _win32_can_symlink ( ) : return _win32_symlink ( path , link , verbose ) else : return _win32_junction ( path , link , verbose )
6435	def sim_eudex ( src , tar , weights = 'exponential' , max_length = 8 ) : return Eudex ( ) . sim ( src , tar , weights , max_length )
4713	def hooks_setup ( trun , parent , hnames = None ) : hooks = { "enter" : [ ] , "exit" : [ ] } if hnames is None : return hooks for hname in hnames : for med in HOOK_PATTERNS : for ptn in HOOK_PATTERNS [ med ] : fpath = os . sep . join ( [ trun [ "conf" ] [ "HOOKS" ] , ptn % hname ] ) if not os . path . exists ( fpath ) : continue hook = hook_setup ( parent , fpath ) if not hook : continue hooks [ med ] . append ( hook ) if not hooks [ "enter" ] + hooks [ "exit" ] : cij . err ( "rnr:hooks_setup:FAIL { hname: %r has no files }" % hname ) return None return hooks
11888	def get_data ( self ) : response = self . send_command ( GET_LIGHTS_COMMAND ) _LOGGER . debug ( "get_data response: %s" , repr ( response ) ) if not response : _LOGGER . debug ( "Empty response: %s" , response ) return { } response = response . strip ( ) if not ( response . startswith ( "GLB" ) and response . endswith ( ";" ) ) : _LOGGER . debug ( "Invalid response: %s" , repr ( response ) ) return { } response = response [ 4 : - 3 ] light_strings = response . split ( ';' ) light_data_by_id = { } for light_string in light_strings : values = light_string . split ( ',' ) try : light_data_by_id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except ValueError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) except IndexError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) return light_data_by_id
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
7472	def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
12564	def _partition_data ( datavol , roivol , roivalue , maskvol = None , zeroe = True ) : if maskvol is not None : indices = ( roivol == roivalue ) * ( maskvol > 0 ) else : indices = roivol == roivalue if datavol . ndim == 4 : ts = datavol [ indices , : ] else : ts = datavol [ indices ] if zeroe : if datavol . ndim == 4 : ts = ts [ ts . sum ( axis = 1 ) != 0 , : ] return ts
13032	def serve_forever ( self , poll_interval = 0.5 ) : logger . info ( 'Starting server on {}:{}...' . format ( self . server_name , self . server_port ) ) while True : try : self . poll_once ( poll_interval ) except ( KeyboardInterrupt , SystemExit ) : break self . handle_close ( ) logger . info ( 'Server stopped.' )
8165	def load_edited_source ( self , source , good_cb = None , bad_cb = None , filename = None ) : with LiveExecution . lock : self . good_cb = good_cb self . bad_cb = bad_cb try : compile ( source + '\n\n' , filename or self . filename , "exec" ) self . edited_source = source except Exception as e : if bad_cb : self . edited_source = None tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) return if filename is not None : self . filename = filename
5992	def plot_figure ( array , as_subplot , units , kpc_per_arcsec , figsize , aspect , cmap , norm , norm_min , norm_max , linthresh , linscale , xticks_manual , yticks_manual ) : fig = plotter_util . setup_figure ( figsize = figsize , as_subplot = as_subplot ) norm_min , norm_max = get_normalization_min_max ( array = array , norm_min = norm_min , norm_max = norm_max ) norm_scale = get_normalization_scale ( norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale ) extent = get_extent ( array = array , units = units , kpc_per_arcsec = kpc_per_arcsec , xticks_manual = xticks_manual , yticks_manual = yticks_manual ) plt . imshow ( array , aspect = aspect , cmap = cmap , norm = norm_scale , extent = extent ) return fig
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
6593	def receive_one ( self ) : if not self . runid_pkgidx_map : return None while True : if not self . runid_to_return : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_next_finished_pkgidx_result_pair ( ) if ret is not None : break if self . runid_pkgidx_map : time . sleep ( self . sleep ) return ret
5108	def next_event_description ( self ) : if self . _departures [ 0 ] . _time < self . _arrivals [ 0 ] . _time : return 2 elif self . _arrivals [ 0 ] . _time < infty : return 1 else : return 0
9286	def sendall ( self , line ) : if isinstance ( line , APRSPacket ) : line = str ( line ) elif not isinstance ( line , string_type ) : raise TypeError ( "Expected line to be str or APRSPacket, got %s" , type ( line ) ) if not self . _connected : raise ConnectionError ( "not connected" ) if line == "" : return line = line . rstrip ( "\r\n" ) + "\r\n" try : self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . _sendall ( line ) except socket . error as exp : self . close ( ) raise ConnectionError ( str ( exp ) )
11809	def index_collection ( self , filenames ) : "Index a whole collection of files." for filename in filenames : self . index_document ( open ( filename ) . read ( ) , filename )
1570	def submit_fatjar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_class ( class_name = main_class , lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) , extra_jars = [ topology_file ] , args = tuple ( unknown_args ) , java_defines = cl_args [ 'topology_main_jvm_property' ] ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res results = launch_topologies ( cl_args , topology_file , tmp_dir ) return results
676	def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID
11929	def watch_files ( self ) : try : while 1 : sleep ( 1 ) try : files_stat = self . get_files_stat ( ) except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) if self . files_stat != files_stat : logger . info ( "Changes detected, start rebuilding.." ) try : generator . re_generate ( ) global _root _root = generator . root except SystemExit : logger . error ( "Error occurred, server shut down" ) self . shutdown_server ( ) self . files_stat = files_stat except KeyboardInterrupt : logger . info ( "^C received, shutting down watcher" ) self . shutdown_watcher ( )
7681	def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , ** kwargs )
792	def jobCancel ( self , jobID ) : self . _logger . info ( 'Canceling jobID=%s' , jobID ) self . jobSetFields ( jobID , { "cancel" : True } , useConnectionID = False )
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
2429	def reset_document ( self ) : self . doc_version_set = False self . doc_comment_set = False self . doc_namespace_set = False self . doc_data_lics_set = False self . doc_name_set = False self . doc_spdx_id_set = False
574	def clippedObj ( obj , maxElementSize = 64 ) : if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut
8816	def update_network ( context , id , network ) : LOG . info ( "update_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) net_dict = network [ "network" ] utils . pop_param ( net_dict , "network_plugin" ) if not context . is_admin and "ipam_strategy" in net_dict : utils . pop_param ( net_dict , "ipam_strategy" ) net = db_api . network_update ( context , net , ** net_dict ) return v . _make_network_dict ( net )
2769	def get_all_firewalls ( self ) : data = self . get_data ( "firewalls" ) firewalls = list ( ) for jsoned in data [ 'firewalls' ] : firewall = Firewall ( ** jsoned ) firewall . token = self . token in_rules = list ( ) for rule in jsoned [ 'inbound_rules' ] : in_rules . append ( InboundRule ( ** rule ) ) firewall . inbound_rules = in_rules out_rules = list ( ) for rule in jsoned [ 'outbound_rules' ] : out_rules . append ( OutboundRule ( ** rule ) ) firewall . outbound_rules = out_rules firewalls . append ( firewall ) return firewalls
3472	def subtract_metabolites ( self , metabolites , combine = True , reversibly = True ) : self . add_metabolites ( { k : - v for k , v in iteritems ( metabolites ) } , combine = combine , reversibly = reversibly )
6414	def aghmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) and round ( m_g , 12 ) != round ( m_h , 12 ) : m_a , m_g , m_h = ( ( m_a + m_g + m_h ) / 3 , ( m_a * m_g * m_h ) ** ( 1 / 3 ) , 3 / ( 1 / m_a + 1 / m_g + 1 / m_h ) , ) return m_a
11034	def get ( self , key : str , default : typing . Any = UNSET , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , mapper : typing . Optional [ typing . Callable [ [ object ] , object ] ] = None , ) -> typing . Any : value = self . environ . get ( key , UNSET ) if value is UNSET and default is UNSET : raise ConfigError ( "Unknown environment variable: {0}" . format ( key ) ) if value is UNSET : value = default else : value = self . parse ( typing . cast ( str , value ) , type_ , subtype ) if mapper : value = mapper ( value ) return value
4492	def fetch ( args ) : storage , remote_path = split_storage ( args . remote ) local_path = args . local if local_path is None : _ , local_path = os . path . split ( remote_path ) local_path_exists = os . path . exists ( local_path ) if local_path_exists and not args . force and not args . update : sys . exit ( "Local file %s already exists, not overwriting." % local_path ) directory , _ = os . path . split ( local_path ) if directory : makedirs ( directory , exist_ok = True ) osf = _setup_osf ( args ) project = osf . project ( args . project ) store = project . storage ( storage ) for file_ in store . files : if norm_remote_path ( file_ . path ) == remote_path : if local_path_exists and not args . force and args . update : if file_ . hashes . get ( 'md5' ) == checksum ( local_path ) : print ( "Local file %s already matches remote." % local_path ) break with open ( local_path , 'wb' ) as fp : file_ . write_to ( fp ) break
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
7263	def use ( cls , name , method : [ str , Set , List ] , url = None ) : if not isinstance ( method , ( str , list , set , tuple ) ) : raise BaseException ( 'Invalid type of method: %s' % type ( method ) . __name__ ) if isinstance ( method , str ) : method = { method } cls . _interface [ name ] = [ { 'method' : method , 'url' : url } ]
694	def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
5479	def _cancel_batch ( batch_fn , cancel_fn , ops ) : canceled = [ ] failed = [ ] def handle_cancel_response ( request_id , response , exception ) : del response if exception : msg = 'error %s: %s' % ( exception . resp . status , exception . resp . reason ) if exception . resp . status == FAILED_PRECONDITION_CODE : detail = json . loads ( exception . content ) status = detail . get ( 'error' , { } ) . get ( 'status' ) if status == FAILED_PRECONDITION_STATUS : msg = 'Not running' failed . append ( { 'name' : request_id , 'msg' : msg } ) else : canceled . append ( { 'name' : request_id } ) return batch = batch_fn ( callback = handle_cancel_response ) ops_by_name = { } for op in ops : op_name = op . get_field ( 'internal-id' ) ops_by_name [ op_name ] = op batch . add ( cancel_fn ( name = op_name , body = { } ) , request_id = op_name ) batch . execute ( ) canceled_ops = [ ops_by_name [ op [ 'name' ] ] for op in canceled ] error_messages = [ ] for fail in failed : op = ops_by_name [ fail [ 'name' ] ] error_messages . append ( "Error canceling '%s': %s" % ( get_operation_full_job_id ( op ) , fail [ 'msg' ] ) ) return canceled_ops , error_messages
6169	def filter ( self , x ) : y = signal . lfilter ( self . b , [ 1 ] , x ) return y
6983	def _legendre_dtr ( x , y , y_err , legendredeg = 10 ) : try : p = Legendre . fit ( x , y , legendredeg ) fit_y = p ( x ) except Exception as e : fit_y = npzeros_like ( y ) fitchisq = npsum ( ( ( fit_y - y ) * ( fit_y - y ) ) / ( y_err * y_err ) ) nparams = legendredeg + 1 fitredchisq = fitchisq / ( len ( y ) - nparams - 1 ) LOGINFO ( 'legendre detrend applied. chisq = %.5f, reduced chisq = %.5f' % ( fitchisq , fitredchisq ) ) return fit_y , fitchisq , fitredchisq
1114	def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] for fromdata , todata , flag in diffs : try : fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist
10830	def create ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls ( group = group , admin = admin , ) db . session . add ( obj ) return obj
5836	def __get_ml_configuration_status ( self , job_id ) : failure_message = "Get status on ml configuration failed" response = self . _get_success_json ( self . _get ( 'v1/descriptors/builders/simple/default/' + job_id + '/status' , None , failure_message = failure_message ) ) [ 'data' ] return response
5898	def check_mdrun_success ( logfile ) : if not os . path . exists ( logfile ) : return None with open ( logfile , 'rb' ) as log : log . seek ( - 1024 , 2 ) for line in log : line = line . decode ( 'ASCII' ) if line . startswith ( "Finished mdrun on" ) : return True return False
11218	def encode ( self ) -> str : payload = { } payload . update ( self . registered_claims ) payload . update ( self . payload ) return encode ( self . secret , payload , self . alg , self . header )
13416	def linkcode_resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = "#L%d-L%d" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = "" fn = relpath ( fn , start = dirname ( scisalt . __file__ ) ) if 'dev' in scisalt . __version__ : return "http://github.com/joelfrederico/SciSalt/blob/master/scisalt/%s%s" % ( fn , linespec ) else : return "http://github.com/joelfrederico/SciSalt/blob/v%s/scisalt/%s%s" % ( scisalt . __version__ , fn , linespec )
239	def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig
2449	def set_pkg_supplier ( self , doc , entity ) : self . assert_package_exists ( ) if not self . package_supplier_set : self . package_supplier_set = True if validations . validate_pkg_supplier ( entity ) : doc . package . supplier = entity return True else : raise SPDXValueError ( 'Package::Supplier' ) else : raise CardinalityError ( 'Package::Supplier' )
13097	def terminate_processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
3252	def get_stores ( self , names = None , workspaces = None ) : if isinstance ( workspaces , Workspace ) : workspaces = [ workspaces ] elif isinstance ( workspaces , list ) and [ w for w in workspaces if isinstance ( w , Workspace ) ] : pass else : workspaces = self . get_workspaces ( names = workspaces ) stores = [ ] for ws in workspaces : ds_list = self . get_xml ( ws . datastore_url ) cs_list = self . get_xml ( ws . coveragestore_url ) wms_list = self . get_xml ( ws . wmsstore_url ) stores . extend ( [ datastore_from_index ( self , ws , n ) for n in ds_list . findall ( "dataStore" ) ] ) stores . extend ( [ coveragestore_from_index ( self , ws , n ) for n in cs_list . findall ( "coverageStore" ) ] ) stores . extend ( [ wmsstore_from_index ( self , ws , n ) for n in wms_list . findall ( "wmsStore" ) ] ) if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if stores and names : return ( [ store for store in stores if store . name in names ] ) return stores
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
12361	def send_request ( self , kind , url_components , ** kwargs ) : return self . api . send_request ( kind , self . resource_path , url_components , ** kwargs )
12657	def merge_dict_of_lists ( adict , indices , pop_later = True , copy = True ) : def check_indices ( idxs , x ) : for i in chain ( * idxs ) : if i < 0 or i >= x : raise IndexError ( "Given indices are out of dict range." ) check_indices ( indices , len ( adict ) ) rdict = adict . copy ( ) if copy else adict dict_keys = list ( rdict . keys ( ) ) for i , j in zip ( * indices ) : rdict [ dict_keys [ i ] ] . extend ( rdict [ dict_keys [ j ] ] ) if pop_later : for i , j in zip ( * indices ) : rdict . pop ( dict_keys [ j ] , '' ) return rdict
3112	def locked_put ( self , credentials ) : serialized = credentials . to_json ( ) self . _dictionary [ self . _key ] = serialized
5573	def read ( self , validity_check = True , no_neighbors = False , ** kwargs ) : if no_neighbors : raise NotImplementedError ( ) return self . _from_cache ( validity_check = validity_check )
9457	def sound_touch ( self , call_params ) : path = '/' + self . api_version + '/SoundTouch/' method = 'POST' return self . request ( path , method , call_params )
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
9434	def load_savefile ( input_file , layers = 0 , verbose = False , lazy = False ) : global VERBOSE old_verbose = VERBOSE VERBOSE = verbose __TRACE__ ( '[+] attempting to load {:s}' , ( input_file . name , ) ) header = _load_savefile_header ( input_file ) if __validate_header__ ( header ) : __TRACE__ ( '[+] found valid header' ) if lazy : packets = _generate_packets ( input_file , header , layers ) __TRACE__ ( '[+] created packet generator' ) else : packets = _load_packets ( input_file , header , layers ) __TRACE__ ( '[+] loaded {:d} packets' , ( len ( packets ) , ) ) sfile = pcap_savefile ( header , packets ) __TRACE__ ( '[+] finished loading savefile.' ) else : __TRACE__ ( '[!] invalid savefile' ) sfile = None VERBOSE = old_verbose return sfile
12101	def _record_info ( self , setup_info = None ) : info_path = os . path . join ( self . root_directory , ( '%s.info' % self . batch_name ) ) if setup_info is None : try : with open ( info_path , 'r' ) as info_file : setup_info = json . load ( info_file ) except : setup_info = { } setup_info . update ( { 'end_time' : tuple ( time . localtime ( ) ) } ) else : setup_info . update ( { 'end_time' : None , 'metadata' : self . metadata } ) with open ( info_path , 'w' ) as info_file : json . dump ( setup_info , info_file , sort_keys = True , indent = 4 )
12471	def get_abspath ( folderpath ) : if not op . exists ( folderpath ) : raise FolderNotFound ( folderpath ) return op . abspath ( folderpath )
11919	def index_row ( self , dataframe ) : return dataframe . loc [ self . kwargs [ self . lookup_url_kwarg ] ] . to_frame ( ) . T
9497	def parse_collection ( path , excludes = None ) : file = path / COLLECTION_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == COLLECTION_FILENAME , lambda filepath : filepath . is_dir ( ) , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Collection ( id , file , resources )
4120	def onesided_2_twosided ( data ) : psd = np . concatenate ( ( data [ 0 : - 1 ] , cshift ( data [ - 1 : 0 : - 1 ] , - 1 ) ) ) / 2. psd [ 0 ] *= 2. psd [ - 1 ] *= 2. return psd
9898	def _initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] if not os . path . exists ( path ) : dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IOError ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : with open ( path , "w" ) as f : json . dump ( data , f ) else : return False
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
11883	def scanProcessForOpenFile ( pid , searchPortion , isExactMatch = True , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e prefixDir = "/proc/%d/fd" % ( pid , ) processFDs = os . listdir ( prefixDir ) matchedFDs = [ ] matchedFilenames = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor == totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) == totalPath . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor in totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) in totalPath . lower ( ) ) for fd in processFDs : fdPath = os . readlink ( prefixDir + '/' + fd ) if isMatch ( searchPortion , fdPath ) : matchedFDs . append ( fd ) matchedFilenames . append ( fdPath ) if len ( matchedFDs ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'fds' : matchedFDs , 'filenames' : matchedFilenames , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
7645	def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
12963	def getPrimaryKeys ( self , sortByAge = False ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : conn = self . _get_connection ( ) matchedKeys = conn . smembers ( self . _get_ids_key ( ) ) elif numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] matchedKeys = conn . smembers ( self . _get_key_for_index ( filterFieldName , filterValue ) ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] matchedKeys = conn . sinter ( indexKeys ) else : notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : matchedKeys = conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) else : indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) matchedKeys = pipeline . execute ( ) [ 1 ] matchedKeys = [ int ( _key ) for _key in matchedKeys ] if sortByAge is False : return list ( matchedKeys ) else : matchedKeys = list ( matchedKeys ) matchedKeys . sort ( ) return matchedKeys
2669	def cleanup_old_versions ( src , keep_last_versions , config_file = 'config.yaml' , profile_name = None , ) : if keep_last_versions <= 0 : print ( "Won't delete all versions. Please do this manually" ) else : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) response = client . list_versions_by_function ( FunctionName = cfg . get ( 'function_name' ) , ) versions = response . get ( 'Versions' ) if len ( response . get ( 'Versions' ) ) < keep_last_versions : print ( 'Nothing to delete. (Too few versions published)' ) else : version_numbers = [ elem . get ( 'Version' ) for elem in versions [ 1 : - keep_last_versions ] ] for version_number in version_numbers : try : client . delete_function ( FunctionName = cfg . get ( 'function_name' ) , Qualifier = version_number , ) except botocore . exceptions . ClientError as e : print ( 'Skipping Version {}: {}' . format ( version_number , e . message ) )
12142	def _info ( self , source , key , filetype , ignore ) : specs , mdata = [ ] , { } mdata_clashes = set ( ) for spec in source . specs : if key not in spec : raise Exception ( "Key %r not available in 'source'." % key ) mdata = dict ( ( k , v ) for ( k , v ) in filetype . metadata ( spec [ key ] ) . items ( ) if k not in ignore ) mdata_spec = { } mdata_spec . update ( spec ) mdata_spec . update ( mdata ) specs . append ( mdata_spec ) mdata_clashes = mdata_clashes | ( set ( spec . keys ( ) ) & set ( mdata . keys ( ) ) ) if mdata_clashes : self . warning ( "Loaded metadata keys overriding source keys." ) return specs
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
981	def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
13245	async def _download_text ( url , session ) : logger = logging . getLogger ( __name__ ) async with session . get ( url ) as response : logger . info ( 'Downloading %r' , url ) return await response . text ( )
8788	def get ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue return None
6997	def runcp_worker ( task ) : pfpickle , outdir , lcbasedir , kwargs = task try : return runcp ( pfpickle , outdir , lcbasedir , ** kwargs ) except Exception as e : LOGEXCEPTION ( ' could not make checkplots for %s: %s' % ( pfpickle , e ) ) return None
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz ox , oz = - sx / 2 , - sz / 2 def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
11649	def transform ( self , X ) : n = self . train_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( n ) ) if self . copy : X = X . copy ( ) if self . shift_ != 0 and X is self . train_ or ( X . shape == self . train_ . shape and np . allclose ( X , self . train_ ) ) : X [ xrange ( n ) , xrange ( n ) ] += self . shift_ return X
8313	def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 row_y += cell_h previous_cell_w = cell_w _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
3019	def from_json ( cls , json_data ) : if not isinstance ( json_data , dict ) : json_data = json . loads ( _helpers . _from_bytes ( json_data ) ) private_key_pkcs8_pem = None pkcs12_val = json_data . get ( _PKCS12_KEY ) password = None if pkcs12_val is None : private_key_pkcs8_pem = json_data [ '_private_key_pkcs8_pem' ] signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) else : pkcs12_val = base64 . b64decode ( pkcs12_val ) password = json_data [ '_private_key_password' ] signer = crypt . Signer . from_string ( pkcs12_val , password ) credentials = cls ( json_data [ '_service_account_email' ] , signer , scopes = json_data [ '_scopes' ] , private_key_id = json_data [ '_private_key_id' ] , client_id = json_data [ 'client_id' ] , user_agent = json_data [ '_user_agent' ] , ** json_data [ '_kwargs' ] ) if private_key_pkcs8_pem is not None : credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem if pkcs12_val is not None : credentials . _private_key_pkcs12 = pkcs12_val if password is not None : credentials . _private_key_password = password credentials . invalid = json_data [ 'invalid' ] credentials . access_token = json_data [ 'access_token' ] credentials . token_uri = json_data [ 'token_uri' ] credentials . revoke_uri = json_data [ 'revoke_uri' ] token_expiry = json_data . get ( 'token_expiry' , None ) if token_expiry is not None : credentials . token_expiry = datetime . datetime . strptime ( token_expiry , client . EXPIRY_FORMAT ) return credentials
11068	def delete_acl ( self , name ) : if name not in self . _acl : return False del self . _acl [ name ] return True
13611	def add_arguments ( cls ) : return [ ( ( '--yes' , ) , dict ( action = 'store_true' , help = 'clean .git repo' ) ) , ( ( '--variable' , '-s' ) , dict ( nargs = '+' , help = 'set extra variable,format is name:value' ) ) , ( ( '--skip-builtin' , ) , dict ( action = 'store_true' , help = 'skip replace builtin variable' ) ) , ]
8247	def str_to_rgb ( self , str ) : str = str . lower ( ) for ch in "_- " : str = str . replace ( ch , "" ) if named_colors . has_key ( str ) : return named_colors [ str ] for suffix in [ "ish" , "ed" , "y" , "like" ] : str = re . sub ( "(.*?)" + suffix + "$" , "\\1" , str ) str = re . sub ( "(.*?)dd$" , "\\1d" , str ) matches = [ ] for name in named_colors : if name in str or str in name : matches . append ( named_colors [ name ] ) if len ( matches ) > 0 : return choice ( matches ) return named_colors [ "transparent" ]
4253	def org_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . org_by_addr ( addr )
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
9230	def fetch_repo_creation_date ( self ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . get ( ) if rc == 200 : return REPO_CREATED_TAG_NAME , data [ "created_at" ] else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) return None , None
1916	def get ( self ) : if self . is_shutdown ( ) : return None while len ( self . _states ) == 0 : if self . running == 0 : return None if self . is_shutdown ( ) : return None logger . debug ( "Waiting for available states" ) self . _lock . wait ( ) state_id = self . _policy . choice ( list ( self . _states ) ) if state_id is None : return None del self . _states [ self . _states . index ( state_id ) ] return state_id
10464	def verifymenucheck ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) try : if menu_handle . AXMenuItemMarkChar : return 1 except atomac . _a11y . Error : pass except LdtpServerException : pass return 0
13224	def main ( ) : parser = argparse . ArgumentParser ( description = 'Discover and ingest metadata from document sources, ' 'including lsstdoc-based LaTeX documents and ' 'reStructuredText-based technotes. Metadata can be ' 'upserted into the LSST Projectmeta MongoDB.' ) parser . add_argument ( '--ltd-product' , dest = 'ltd_product_url' , help = 'URL of an LSST the Docs product ' '(https://keeper.lsst.codes/products/<slug>). If provided, ' 'only this document will be ingested.' ) parser . add_argument ( '--github-token' , help = 'GitHub personal access token.' ) parser . add_argument ( '--mongodb-uri' , help = 'MongoDB connection URI. If provided, metadata will be loaded ' 'into the Projectmeta database. Omit this argument to just ' 'test the ingest pipeline.' ) parser . add_argument ( '--mongodb-db' , default = 'lsstprojectmeta' , help = 'Name of MongoDB database' ) parser . add_argument ( '--mongodb-collection' , default = 'resources' , help = 'Name of the MongoDB collection for projectmeta resources' ) args = parser . parse_args ( ) stream_handler = logging . StreamHandler ( ) stream_formatter = logging . Formatter ( '%(asctime)s %(levelname)8s %(name)s | %(message)s' ) stream_handler . setFormatter ( stream_formatter ) root_logger = logging . getLogger ( ) root_logger . addHandler ( stream_handler ) root_logger . setLevel ( logging . WARNING ) app_logger = logging . getLogger ( 'lsstprojectmeta' ) app_logger . setLevel ( logging . DEBUG ) if args . mongodb_uri is not None : mongo_client = AsyncIOMotorClient ( args . mongodb_uri , ssl = True ) collection = mongo_client [ args . mongodb_db ] [ args . mongodb_collection ] else : collection = None loop = asyncio . get_event_loop ( ) if args . ltd_product_url is not None : loop . run_until_complete ( run_single_ltd_doc ( args . ltd_product_url , args . github_token , collection ) ) else : loop . run_until_complete ( run_bulk_etl ( args . github_token , collection ) )
12585	def spatialimg_to_hdfpath ( file_path , spatial_img , h5path = None , append = True ) : if h5path is None : h5path = '/img' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' with h5py . File ( file_path , mode ) as f : try : h5img = f . create_group ( h5path ) spatialimg_to_hdfgroup ( h5img , spatial_img ) except ValueError as ve : raise Exception ( 'Error creating group ' + h5path ) from ve
1737	def parse_exponent ( source , start ) : if not source [ start ] in { 'e' , 'E' } : if source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start start += 1 if source [ start ] in { '-' , '+' } : start += 1 FOUND = False while source [ start ] in NUMS : FOUND = True start += 1 if not FOUND or source [ start ] in IDENTIFIER_PART : raise SyntaxError ( 'Invalid number literal!' ) return start
1587	def check_output_schema ( self , stream_id , tup ) : size = self . _output_schema . get ( stream_id , None ) if size is None : raise RuntimeError ( "%s emitting to stream %s but was not declared in output fields" % ( self . my_component_name , stream_id ) ) elif size != len ( tup ) : raise RuntimeError ( "Number of fields emitted in stream %s does not match what's expected. " "Expected: %s, Observed: %s" % ( stream_id , size , len ( tup ) ) )
4366	def process_event ( self , packet ) : args = packet [ 'args' ] name = packet [ 'name' ] if not allowed_event_name_regex . match ( name ) : self . error ( "unallowed_event_name" , "name must only contains alpha numerical characters" ) return method_name = 'on_' + name . replace ( ' ' , '_' ) return self . call_method_with_acl ( method_name , packet , * args )
12935	def _parse_allele_data ( self ) : pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , ** cln_data ) if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
388	def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out
4898	def _remove_failed_items ( self , failed_items , items_to_create , items_to_update , items_to_delete ) : for item in failed_items : content_metadata_id = item [ 'courseID' ] items_to_create . pop ( content_metadata_id , None ) items_to_update . pop ( content_metadata_id , None ) items_to_delete . pop ( content_metadata_id , None )
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
8481	def env ( key , default ) : value = os . environ . get ( key , None ) if value is not None : log . info ( ' %s = %r' , key . lower ( ) . replace ( '_' , '.' ) , value ) return value key = key . lower ( ) . replace ( '_' , '.' ) value = get ( key ) if value is not None : return value return default
12536	def get_dcm_reader ( store_metadata = True , header_fields = None ) : if not store_metadata : return lambda fpath : fpath if header_fields is None : build_dcm = lambda fpath : DicomFile ( fpath ) else : dicom_header = namedtuple ( 'DicomHeader' , header_fields ) build_dcm = lambda fpath : dicom_header . _make ( DicomFile ( fpath ) . get_attributes ( header_fields ) ) return build_dcm
10004	def rename ( self , name ) : if is_valid_name ( name ) : if name not in self . system . models : self . name = name return True else : return False else : raise ValueError ( "Invalid name '%s'." % name )
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
1330	def predictions ( self , image , strict = True , return_details = False ) : in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 predictions = self . __model . predictions ( image ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 if return_details : return predictions , is_adversarial , is_best , distance else : return predictions , is_adversarial
12015	def define_spotsignal ( self ) : client = kplr . API ( ) star = client . star ( self . kic ) lcs = star . get_light_curves ( short_cadence = False ) time , flux , ferr , qual = [ ] , [ ] , [ ] , [ ] for lc in lcs : with lc . open ( ) as f : hdu_data = f [ 1 ] . data time . append ( hdu_data [ "time" ] ) flux . append ( hdu_data [ "pdcsap_flux" ] ) ferr . append ( hdu_data [ "pdcsap_flux_err" ] ) qual . append ( hdu_data [ "sap_quality" ] ) tout = np . array ( [ ] ) fout = np . array ( [ ] ) eout = np . array ( [ ] ) for i in range ( len ( flux ) ) : t = time [ i ] [ qual [ i ] == 0 ] f = flux [ i ] [ qual [ i ] == 0 ] e = ferr [ i ] [ qual [ i ] == 0 ] t = t [ np . isfinite ( f ) ] e = e [ np . isfinite ( f ) ] f = f [ np . isfinite ( f ) ] e /= np . median ( f ) f /= np . median ( f ) tout = np . append ( tout , t [ 50 : ] + 54833 ) fout = np . append ( fout , f [ 50 : ] ) eout = np . append ( eout , e [ 50 : ] ) self . spot_signal = np . zeros ( 52 ) for i in range ( len ( self . times ) ) : if self . times [ i ] < 55000 : self . spot_signal [ i ] = 1.0 else : self . spot_signal [ i ] = fout [ np . abs ( self . times [ i ] - tout ) == np . min ( np . abs ( self . times [ i ] - tout ) ) ]
11151	def md5file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . md5 , nbytes = nbytes , chunk_size = chunk_size )
10362	def self_edge_filter ( _ : BELGraph , source : BaseEntity , target : BaseEntity , __ : str ) -> bool : return source == target
4220	def backends ( cls ) : allowed = ( keyring for keyring in filter ( backend . _limit , backend . get_all_keyring ( ) ) if not isinstance ( keyring , ChainerBackend ) and keyring . priority > 0 ) return sorted ( allowed , key = backend . by_priority , reverse = True )
12243	def dixon_price ( theta ) : x , y = theta obj = ( x - 1 ) ** 2 + 2 * ( 2 * y ** 2 - x ) ** 2 grad = np . array ( [ 2 * x - 2 - 4 * ( 2 * y ** 2 - x ) , 16 * ( 2 * y ** 2 - x ) * y , ] ) return obj , grad
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
5078	def get_current_course_run ( course , users_active_course_runs ) : current_course_run = None filtered_course_runs = [ ] all_course_runs = course [ 'course_runs' ] if users_active_course_runs : current_course_run = get_closest_course_run ( users_active_course_runs ) else : for course_run in all_course_runs : if is_course_run_enrollable ( course_run ) and is_course_run_upgradeable ( course_run ) : filtered_course_runs . append ( course_run ) if not filtered_course_runs : filtered_course_runs = all_course_runs if filtered_course_runs : current_course_run = get_closest_course_run ( filtered_course_runs ) return current_course_run
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
11437	def _fields_sort_by_indicators ( fields ) : field_dict = { } field_positions_global = [ ] for field in fields : field_dict . setdefault ( field [ 1 : 3 ] , [ ] ) . append ( field ) field_positions_global . append ( field [ 4 ] ) indicators = field_dict . keys ( ) indicators . sort ( ) field_list = [ ] for indicator in indicators : for field in field_dict [ indicator ] : field_list . append ( field [ : 4 ] + ( field_positions_global . pop ( 0 ) , ) ) return field_list
1109	def compare ( self , a , b ) : r cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
8047	def parse_from_import_statement ( self ) : self . log . debug ( "parsing from/import statement." ) is_future_import = self . _parse_from_import_source ( ) self . _parse_from_import_names ( is_future_import )
5789	def handle_openssl_error ( result , exception_class = None ) : if result > 0 : return if exception_class is None : exception_class = OSError error_num = libcrypto . ERR_get_error ( ) buffer = buffer_from_bytes ( 120 ) libcrypto . ERR_error_string ( error_num , buffer ) error_string = byte_string_from_buffer ( buffer ) raise exception_class ( _try_decode ( error_string ) )
5502	def timeline ( ctx , pager , limit , twtfile , sorting , timeout , porcelain , source , cache , force_update ) : if source : source_obj = ctx . obj [ "conf" ] . get_source_by_nick ( source ) if not source_obj : logger . debug ( "Not following {0}, trying as URL" . format ( source ) ) source_obj = Source ( source , source ) sources = [ source_obj ] else : sources = ctx . obj [ "conf" ] . following tweets = [ ] if cache : try : with Cache . discover ( update_interval = ctx . obj [ "conf" ] . timeline_update_interval ) as cache : force_update = force_update or not cache . is_valid if force_update : tweets = get_remote_tweets ( sources , limit , timeout , cache ) else : logger . debug ( "Multiple calls to 'timeline' within {0} seconds. Skipping update" . format ( cache . update_interval ) ) tweets = list ( chain . from_iterable ( [ cache . get_tweets ( source . url ) for source in sources ] ) ) except OSError as e : logger . debug ( e ) tweets = get_remote_tweets ( sources , limit , timeout ) else : tweets = get_remote_tweets ( sources , limit , timeout ) if twtfile and not source : source = Source ( ctx . obj [ "conf" ] . nick , ctx . obj [ "conf" ] . twturl , file = twtfile ) tweets . extend ( get_local_tweets ( source , limit ) ) if not tweets : return tweets = sort_and_truncate_tweets ( tweets , sorting , limit ) if pager : click . echo_via_pager ( style_timeline ( tweets , porcelain ) ) else : click . echo ( style_timeline ( tweets , porcelain ) )
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
10420	def count_unique_relations ( graph : BELGraph ) -> Counter : return Counter ( itt . chain . from_iterable ( get_edge_relations ( graph ) . values ( ) ) )
3969	def _conditional_links ( assembled_specs , app_name ) : link_to_apps = [ ] potential_links = assembled_specs [ 'apps' ] [ app_name ] [ 'conditional_links' ] for potential_link in potential_links [ 'apps' ] : if potential_link in assembled_specs [ 'apps' ] : link_to_apps . append ( potential_link ) for potential_link in potential_links [ 'services' ] : if potential_link in assembled_specs [ 'services' ] : link_to_apps . append ( potential_link ) return link_to_apps
9903	def is_configured ( self , project , ** kwargs ) : params = self . get_option return bool ( params ( 'server_host' , project ) and params ( 'server_port' , project ) )
2650	def send ( self , message_type , task_id , message ) : x = 0 try : buffer = pickle . dumps ( ( self . source_id , int ( time . time ( ) ) , message_type , message ) ) except Exception as e : print ( "Exception during pickling {}" . format ( e ) ) return try : x = self . sock . sendto ( buffer , ( self . ip , self . port ) ) except socket . timeout : print ( "Could not send message within timeout limit" ) return False return x
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
7833	def add_item ( self , fields = None ) : item = Item ( fields ) self . items . append ( item ) return item
3040	def from_json ( cls , json_data ) : data = json . loads ( _helpers . _from_bytes ( json_data ) ) if ( data . get ( 'token_expiry' ) and not isinstance ( data [ 'token_expiry' ] , datetime . datetime ) ) : try : data [ 'token_expiry' ] = datetime . datetime . strptime ( data [ 'token_expiry' ] , EXPIRY_FORMAT ) except ValueError : data [ 'token_expiry' ] = None retval = cls ( data [ 'access_token' ] , data [ 'client_id' ] , data [ 'client_secret' ] , data [ 'refresh_token' ] , data [ 'token_expiry' ] , data [ 'token_uri' ] , data [ 'user_agent' ] , revoke_uri = data . get ( 'revoke_uri' , None ) , id_token = data . get ( 'id_token' , None ) , id_token_jwt = data . get ( 'id_token_jwt' , None ) , token_response = data . get ( 'token_response' , None ) , scopes = data . get ( 'scopes' , None ) , token_info_uri = data . get ( 'token_info_uri' , None ) ) retval . invalid = data [ 'invalid' ] return retval
6119	def circular_anti_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_anti_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8916	def _retrieve_certificate ( self , access_token , timeout = 3 ) : logger . debug ( "Retrieve certificate with token." ) key_pair = crypto . PKey ( ) key_pair . generate_key ( crypto . TYPE_RSA , 2048 ) private_key = crypto . dump_privatekey ( crypto . FILETYPE_PEM , key_pair ) . decode ( "utf-8" ) cert_request = crypto . X509Req ( ) cert_request . set_pubkey ( key_pair ) cert_request . sign ( key_pair , 'md5' ) der_cert_req = crypto . dump_certificate_request ( crypto . FILETYPE_ASN1 , cert_request ) encoded_cert_req = base64 . b64encode ( der_cert_req ) token = { 'access_token' : access_token , 'token_type' : 'Bearer' } client = OAuth2Session ( token = token ) response = client . post ( self . certificate_url , data = { 'certificate_request' : encoded_cert_req } , verify = False , timeout = timeout , ) if response . ok : content = "{} {}" . format ( response . text , private_key ) with open ( self . esgf_credentials , 'w' ) as fh : fh . write ( content ) logger . debug ( 'Fetched certificate successfully.' ) else : msg = "Could not get certificate: {} {}" . format ( response . status_code , response . reason ) raise Exception ( msg ) return True
2327	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . score self . arguments [ '{BETA}' ] = str ( self . beta ) self . arguments [ '{OPTIM}' ] = str ( self . optim ) . upper ( ) self . arguments [ '{ALPHA}' ] = str ( self . alpha ) whitelist = DataFrame ( list ( nx . edges ( graph ) ) , columns = [ "from" , "to" ] ) blacklist = DataFrame ( list ( nx . edges ( nx . DiGraph ( DataFrame ( - nx . adj_matrix ( graph , weight = None ) . to_dense ( ) + 1 , columns = list ( graph . nodes ( ) ) , index = list ( graph . nodes ( ) ) ) ) ) ) , columns = [ "from" , "to" ] ) results = self . _run_bnlearn ( data , whitelist = whitelist , blacklist = blacklist , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
12901	def set_sleep ( self , value = False ) : return ( yield from self . handle_set ( self . API . get ( 'sleep' ) , int ( value ) ) )
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , ** obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
1804	def SAHF ( cpu ) : eflags_size = 32 val = cpu . AH & 0xD5 | 0x02 cpu . EFLAGS = Operators . ZEXTEND ( val , eflags_size )
1360	def get_argument_starttime ( self ) : try : starttime = self . get_argument ( constants . PARAM_STARTTIME ) return starttime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
9848	def _load_dx ( self , filename ) : dx = OpenDX . field ( 0 ) dx . read ( filename ) grid , edges = dx . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
1378	def check_release_file_exists ( ) : release_file = get_heron_release_file ( ) if not os . path . isfile ( release_file ) : Log . error ( "Required file not found: %s" % release_file ) return False return True
12968	def delete ( self ) : if self . filters or self . notFilters : return self . mdl . deleter . deleteMultiple ( self . allOnlyIndexedFields ( ) ) return self . mdl . deleter . destroyModel ( )
7493	def n_choose_k ( n , k ) : return int ( reduce ( MUL , ( Fraction ( n - i , i + 1 ) for i in range ( k ) ) , 1 ) )
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
4240	def _make_request ( self , service , method , params = None , body = "" , need_auth = True ) : if need_auth and not self . cookie : if not self . login ( ) : return False , None headers = self . _get_headers ( service , method , need_auth ) if not body : if not params : params = "" if isinstance ( params , dict ) : _map = params params = "" for k in _map : params += "<" + k + ">" + _map [ k ] + "</" + k + ">\n" body = CALL_BODY . format ( service = SERVICE_PREFIX + service , method = method , params = params ) message = SOAP_REQUEST . format ( session_id = SESSION_ID , body = body ) try : response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) if need_auth and _is_unauthorized_response ( response ) : self . cookie = None _LOGGER . warning ( "Unauthorized response, let's login and retry..." ) if self . login ( ) : headers = self . _get_headers ( service , method , need_auth ) response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) success = _is_valid_response ( response ) if not success : _LOGGER . error ( "Invalid response" ) _LOGGER . debug ( "%s\n%s\n%s" , response . status_code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . RequestException : _LOGGER . exception ( "Error talking to API" ) return False , None
3124	def _verify_time_range ( payload_dict ) : now = int ( time . time ( ) ) issued_at = payload_dict . get ( 'iat' ) if issued_at is None : raise AppIdentityError ( 'No iat field in token: {0}' . format ( payload_dict ) ) expiration = payload_dict . get ( 'exp' ) if expiration is None : raise AppIdentityError ( 'No exp field in token: {0}' . format ( payload_dict ) ) if expiration >= now + MAX_TOKEN_LIFETIME_SECS : raise AppIdentityError ( 'exp field too far in future: {0}' . format ( payload_dict ) ) earliest = issued_at - CLOCK_SKEW_SECS if now < earliest : raise AppIdentityError ( 'Token used too early, {0} < {1}: {2}' . format ( now , earliest , payload_dict ) ) latest = expiration + CLOCK_SKEW_SECS if now > latest : raise AppIdentityError ( 'Token used too late, {0} > {1}: {2}' . format ( now , latest , payload_dict ) )
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
6128	def _build_backend ( ) : backend_path = os . environ . get ( 'PEP517_BACKEND_PATH' ) if backend_path : extra_pathitems = backend_path . split ( os . pathsep ) sys . path [ : 0 ] = extra_pathitems ep = os . environ [ 'PEP517_BUILD_BACKEND' ] mod_path , _ , obj_path = ep . partition ( ':' ) try : obj = import_module ( mod_path ) except ImportError : raise BackendUnavailable ( traceback . format_exc ( ) ) if backend_path : if not any ( contained_in ( obj . __file__ , path ) for path in extra_pathitems ) : raise BackendInvalid ( "Backend was not loaded from backend-path" ) if obj_path : for path_part in obj_path . split ( '.' ) : obj = getattr ( obj , path_part ) return obj
12829	def parse_conll ( self , texts : List [ str ] , retry_count : int = 0 ) -> List [ str ] : post_data = { 'texts' : texts , 'output_type' : 'conll' } try : response = requests . post ( f'http://{self.hostname}:{self.port}' , json = post_data , headers = { 'Connection' : 'close' } ) response . raise_for_status ( ) except ( requests . exceptions . ConnectionError , requests . exceptions . Timeout ) as server_error : raise ServerError ( server_error , self . hostname , self . port ) except requests . exceptions . HTTPError as http_error : raise http_error else : try : return response . json ( ) except json . JSONDecodeError as json_exception : if retry_count == self . retries : self . log_error ( response . text ) raise Exception ( 'Json Decoding error cannot parse this ' f':\n{response.text}' ) return self . parse_conll ( texts , retry_count + 1 )
6551	def from_configurations ( cls , configurations , variables , vartype , name = None ) : def func ( * args ) : return args in configurations return cls ( func , configurations , variables , vartype , name )
4777	def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
10996	def schedules ( self ) : url = PATHS [ 'GET_SCHEDULES' ] % self . id self . __schedules = self . api . get ( url = url ) return self . __schedules
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
5628	def hook ( self , event_type = 'push' ) : def decorator ( func ) : self . _hooks [ event_type ] . append ( func ) return func return decorator
12837	def init_async ( self , loop = None ) : self . _loop = loop or asyncio . get_event_loop ( ) self . _async_lock = asyncio . Lock ( loop = loop ) if not self . database == ':memory:' : self . _state = ConnectionLocal ( )
11989	def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
5132	def generate_transition_matrix ( g , seed = None ) : g = _test_graph ( g ) if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) nV = g . number_of_nodes ( ) mat = np . zeros ( ( nV , nV ) ) for v in g . nodes ( ) : ind = [ e [ 1 ] for e in sorted ( g . out_edges ( v ) ) ] deg = len ( ind ) if deg == 1 : mat [ v , ind ] = 1 elif deg > 1 : probs = np . ceil ( np . random . rand ( deg ) * 100 ) / 100. if np . isclose ( np . sum ( probs ) , 0 ) : probs [ np . random . randint ( deg ) ] = 1 mat [ v , ind ] = probs / np . sum ( probs ) return mat
9669	def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
10381	def _get_drug_target_interactions ( manager : Optional [ 'bio2bel_drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel_drugbank manager = bio2bel_drugbank . Manager ( ) if not manager . is_populated ( ) : manager . populate ( ) return manager . get_drug_to_hgnc_symbols ( )
7213	def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) else : proj = pyproj . Proj ( init = prj_code ) return proj
10765	def url ( self ) : if self . id is None : return '' return '{}/{}' . format ( strawpoll . API . _BASE_URL , self . id )
3289	def get_resource_inst ( self , path , environ ) : self . _count_get_resource_inst += 1 localHgPath = path . strip ( "/" ) rev = None cmd , rest = util . pop_path ( path ) if cmd == "" : return VirtualCollection ( path , environ , "root" , [ "edit" , "released" , "archive" ] ) elif cmd == "edit" : localHgPath = rest . strip ( "/" ) rev = None elif cmd == "released" : localHgPath = rest . strip ( "/" ) rev = "tip" elif cmd == "archive" : if rest == "/" : loglist = self . _get_log ( limit = 10 ) members = [ compat . to_native ( l [ "local_id" ] ) for l in loglist ] return VirtualCollection ( path , environ , "Revisions" , members ) revid , rest = util . pop_path ( rest ) try : int ( revid ) except Exception : return None rev = revid localHgPath = rest . strip ( "/" ) else : return None cache = self . _get_repo_info ( environ , rev ) if localHgPath in cache [ "filedict" ] : return HgResource ( path , False , environ , rev , localHgPath ) if localHgPath in cache [ "dirinfos" ] or localHgPath == "" : return HgResource ( path , True , environ , rev , localHgPath ) return None
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
2108	def login ( username , password , scope , client_id , client_secret , verbose ) : if not supports_oauth ( ) : raise exc . TowerCLIError ( 'This version of Tower does not support OAuth2.0. Set credentials using tower-cli config.' ) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client_id and client_secret : HTTPBasicAuth ( client_id , client_secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client_id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "client_id" : client_id , "scope" : scope } , headers = req . headers ) else : HTTPBasicAuth ( username , password ) ( req ) r = client . post ( '/users/{}/personal_tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary_fields' , None ) result . pop ( 'related' , None ) if client_id : token = result . pop ( 'access_token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth_token' , token , '--scope=user' ] )
6240	def render_lights_debug ( self , camera_matrix , projection ) : self . ctx . enable ( moderngl . BLEND ) self . ctx . blend_func = moderngl . SRC_ALPHA , moderngl . ONE_MINUS_SRC_ALPHA for light in self . point_lights : m_mv = matrix44 . multiply ( light . matrix , camera_matrix ) light_size = light . radius self . debug_shader [ "m_proj" ] . write ( projection . tobytes ( ) ) self . debug_shader [ "m_mv" ] . write ( m_mv . astype ( 'f4' ) . tobytes ( ) ) self . debug_shader [ "size" ] . value = light_size self . unit_cube . render ( self . debug_shader , mode = moderngl . LINE_STRIP ) self . ctx . disable ( moderngl . BLEND )
9255	def generate_log_for_tag ( self , pull_requests , issues , newer_tag , older_tag_name ) : newer_tag_link , newer_tag_name , newer_tag_time = self . detect_link_tag_time ( newer_tag ) github_site = "https://github.com" or self . options . github_endpoint project_url = "{0}/{1}/{2}" . format ( github_site , self . options . user , self . options . project ) log = self . generate_header ( newer_tag_name , newer_tag_link , newer_tag_time , older_tag_name , project_url ) if self . options . issues : log += self . issues_to_log ( issues , pull_requests ) if self . options . include_pull_request : log += self . generate_sub_section ( pull_requests , self . options . merge_prefix ) return log
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
3398	def update_costs ( self ) : for var in self . indicators : if var not in self . costs : self . costs [ var ] = var . cost else : if var . _get_primal ( ) > self . integer_threshold : self . costs [ var ] += var . cost self . model . objective . set_linear_coefficients ( self . costs )
12647	def set_auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise ValueError ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise ValueError ( 'Must specify both cert and key' ) if pem : set_config_value ( 'security' , 'pem' ) set_config_value ( 'pem_path' , pem ) elif cert or key : set_config_value ( 'security' , 'cert' ) set_config_value ( 'cert_path' , cert ) set_config_value ( 'key_path' , key ) elif aad : set_config_value ( 'security' , 'aad' ) else : set_config_value ( 'security' , 'none' )
11403	def create_record ( marcxml = None , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , sort_fields_by_indicators = False , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : if marcxml is None : return { } try : rec = _create_record_lxml ( marcxml , verbose , correct , keep_singletons = keep_singletons ) except InvenioBibRecordParserError as ex1 : return ( None , 0 , str ( ex1 ) ) if sort_fields_by_indicators : _record_sort_by_indicators ( rec ) errs = [ ] if correct : errs = _correct_record ( rec ) return ( rec , int ( not errs ) , errs )
3723	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : sigmas = [ i ( T ) for i in self . SurfaceTensions ] return mixing_simple ( zs , sigmas ) elif method == DIGUILIOTEJA : return Diguilio_Teja ( T = T , xs = zs , sigmas_Tb = self . sigmas_Tb , Tbs = self . Tbs , Tcs = self . Tcs ) elif method == WINTERFELDSCRIVENDAVIS : sigmas = [ i ( T ) for i in self . SurfaceTensions ] rhoms = [ 1. / i ( T , P ) for i in self . VolumeLiquids ] return Winterfeld_Scriven_Davis ( zs , sigmas , rhoms ) else : raise Exception ( 'Method not valid' )
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
9252	def generate_unreleased_section ( self ) : if not self . filtered_tags : return "" now = datetime . datetime . utcnow ( ) now = now . replace ( tzinfo = dateutil . tz . tzutc ( ) ) head_tag = { "name" : self . options . unreleased_label } self . tag_times_dict [ head_tag [ "name" ] ] = now unreleased_log = self . generate_log_between_tags ( self . filtered_tags [ 0 ] , head_tag ) return unreleased_log
12406	def serialize ( self , data = None ) : if data is not None and self . response is not None : self . response [ 'Content-Type' ] = self . media_types [ 0 ] self . response . write ( data ) return data
13049	def check_service ( service ) : service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
8607	def add_group_user ( self , group_id , user_id ) : data = { "id" : user_id } response = self . _perform_request ( url = '/um/groups/%s/users' % group_id , method = 'POST' , data = json . dumps ( data ) ) return response
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) abf . saveThing ( [ Xs , av1 ] , '01_iv' )
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
13006	def utime ( self , * args , ** kwargs ) : os . utime ( self . extended_path , * args , ** kwargs )
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
4536	def fillHSV ( self , hsv , start = 0 , end = - 1 ) : self . fill ( conversions . hsv2rgb ( hsv ) , start , end )
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
8151	def _addvar ( self , v ) : oldvar = self . _oldvars . get ( v . name ) if oldvar is not None : if isinstance ( oldvar , Variable ) : if oldvar . compliesTo ( v ) : v . value = oldvar . value else : v . value = v . sanitize ( oldvar ) else : for listener in VarListener . listeners : listener . var_added ( v ) self . _vars [ v . name ] = v self . _namespace [ v . name ] = v . value self . _oldvars [ v . name ] = v return v
11565	def stepper_config ( self , steps_per_revolution , stepper_pins ) : data = [ self . STEPPER_CONFIGURE , steps_per_revolution & 0x7f , ( steps_per_revolution >> 7 ) & 0x7f ] for pin in range ( len ( stepper_pins ) ) : data . append ( stepper_pins [ pin ] ) self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
10456	def verifycheck ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name , wait_for_object = False ) if object_handle . AXValue == 1 : return 1 except LdtpServerException : pass return 0
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
10848	def noformat ( self ) : try : formats = { } for h in self . get_handlers ( ) : formats [ h ] = h . formatter self . set_formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v
6654	def pruneCache ( ) : cache_dir = folders . cacheDirectory ( ) def fullpath ( f ) : return os . path . join ( cache_dir , f ) def getMTimeSafe ( f ) : try : return os . stat ( f ) . st_mtime except FileNotFoundError : import time return time . clock ( ) fsutils . mkDirP ( cache_dir ) max_cached_modules = getMaxCachedModules ( ) for f in sorted ( [ f for f in os . listdir ( cache_dir ) if os . path . isfile ( fullpath ( f ) ) and not f . endswith ( '.json' ) and not f . endswith ( '.locked' ) ] , key = lambda f : getMTimeSafe ( fullpath ( f ) ) , reverse = True ) [ max_cached_modules : ] : cache_logger . debug ( 'cleaning up cache file %s' , f ) removeFromCache ( f ) cache_logger . debug ( 'cache pruned to %s items' , max_cached_modules )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
7292	def set_post_data ( self ) : self . form . data = self . post_data_dict for field_key , field in self . form . fields . items ( ) : if has_digit ( field_key ) : base_key = make_key ( field_key , exclude_last_string = True ) for key in self . post_data_dict . keys ( ) : if base_key in key : self . form . fields . update ( { key : field } )
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
7040	def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } urlqs = urlencode ( urlparams ) url = '%s/api/datasets?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting list of recent publicly ' 'visible and owned datasets from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) recent_datasets = json . loads ( resp . read ( ) ) [ 'result' ] return recent_datasets except HTTPError as e : LOGERROR ( 'could not retrieve recent datasets list, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
7901	def configure_room ( self , form ) : if form . type == "cancel" : return None elif form . type != "submit" : raise ValueError ( "A 'submit' form required to configure a room" ) iq = Iq ( to_jid = self . room_jid . bare ( ) , stanza_type = "set" ) query = iq . new_query ( MUC_OWNER_NS , "query" ) form . as_xml ( query ) self . manager . stream . set_response_handlers ( iq , self . process_configuration_success , self . process_configuration_error ) self . manager . stream . send ( iq ) return iq . get_id ( )
8039	def is_public ( self ) : if self . all is not None : return self . name in self . all else : return not self . name . startswith ( "_" )
819	def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
2233	def _register_builtin_class_extensions ( self ) : @ self . register ( uuid . UUID ) def _hash_uuid ( data ) : hashable = data . bytes prefix = b'UUID' return prefix , hashable @ self . register ( OrderedDict ) def _hash_ordered_dict ( data ) : hashable = b'' . join ( _hashable_sequence ( list ( data . items ( ) ) ) ) prefix = b'ODICT' return prefix , hashable
1883	def new_symbolic_value ( self , nbits , label = None , taint = frozenset ( ) ) : assert nbits in ( 1 , 4 , 8 , 16 , 32 , 64 , 128 , 256 ) avoid_collisions = False if label is None : label = 'val' avoid_collisions = True expr = self . _constraints . new_bitvec ( nbits , name = label , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) return expr
9237	def open ( self ) : if self . is_open : return try : os . chdir ( self . working_directory ) if self . chroot_directory : os . chroot ( self . chroot_directory ) os . setgid ( self . gid ) os . setuid ( self . uid ) os . umask ( self . umask ) except OSError as err : raise DaemonError ( 'Setting up Environment failed: {0}' . format ( err ) ) if self . prevent_core : try : resource . setrlimit ( resource . RLIMIT_CORE , ( 0 , 0 ) ) except Exception as err : raise DaemonError ( 'Could not disable core files: {0}' . format ( err ) ) if self . detach_process : try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'First fork failed: {0}' . format ( err ) ) os . setsid ( ) try : if os . fork ( ) > 0 : os . _exit ( 0 ) except OSError as err : raise DaemonError ( 'Second fork failed: {0}' . format ( err ) ) for ( signal_number , handler ) in self . _signal_handler_map . items ( ) : signal . signal ( signal_number , handler ) close_filenos ( self . _files_preserve ) redirect_stream ( sys . stdin , self . stdin ) redirect_stream ( sys . stdout , self . stdout ) redirect_stream ( sys . stderr , self . stderr ) if self . pidfile : self . pidfile . acquire ( ) self . _is_open = True
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
290	def plot_rolling_beta ( returns , factor_returns , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) ax . set_title ( "Rolling portfolio beta to " + str ( factor_returns . name ) ) ax . set_ylabel ( 'Beta' ) rb_1 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) rb_1 . plot ( color = 'steelblue' , lw = 3 , alpha = 0.6 , ax = ax , ** kwargs ) rb_2 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 12 ) rb_2 . plot ( color = 'grey' , lw = 3 , alpha = 0.4 , ax = ax , ** kwargs ) ax . axhline ( rb_1 . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_xlabel ( '' ) ax . legend ( [ '6-mo' , '12-mo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_ylim ( ( - 1.0 , 1.0 ) ) return ax
1889	def min ( self , constraints , X : BitVec , M = 10000 ) : assert isinstance ( X , BitVec ) return self . optimize ( constraints , X , 'minimize' , M )
1692	def CheckCompletedBlocks ( self , filename , error ) : for obj in self . stack : if isinstance ( obj , _ClassInfo ) : error ( filename , obj . starting_linenum , 'build/class' , 5 , 'Failed to find complete declaration of class %s' % obj . name ) elif isinstance ( obj , _NamespaceInfo ) : error ( filename , obj . starting_linenum , 'build/namespaces' , 5 , 'Failed to find complete declaration of namespace %s' % obj . name )
9116	def reset_cleansers ( confirm = True ) : if value_asbool ( confirm ) and not yesno ( ) : exit ( "Glad I asked..." ) get_vars ( ) cleanser_count = AV [ 'ploy_cleanser_count' ] fab . run ( 'ezjail-admin stop worker' ) for cleanser_index in range ( cleanser_count ) : cindex = '{:02d}' . format ( cleanser_index + 1 ) fab . run ( 'ezjail-admin stop cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy tank/jails/cleanser_{cindex}@jdispatch_rollback' . format ( cindex = cindex ) ) fab . run ( 'ezjail-admin delete -fw cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'umount -f /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) fab . run ( 'rm -rf /usr/jails/cleanser_{cindex}' . format ( cindex = cindex ) ) with fab . warn_only ( ) : fab . run ( 'zfs destroy -R tank/jails/cleanser@clonesource' ) fab . run ( 'ezjail-admin start worker' ) fab . run ( 'ezjail-admin stop cleanser' ) fab . run ( 'ezjail-admin start cleanser' )
8687	def get ( self , key_name ) : result = self . db . search ( Query ( ) . name == key_name ) if not result : return { } return result [ 0 ]
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
6991	def cp2png ( checkplotin , extrarows = None ) : if checkplotin . endswith ( '.gz' ) : outfile = checkplotin . replace ( '.pkl.gz' , '.png' ) else : outfile = checkplotin . replace ( '.pkl' , '.png' ) return checkplot_pickle_to_png ( checkplotin , outfile , extrarows = extrarows )
13226	async def process_ltd_doc ( session , github_api_token , ltd_product_url , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) ltd_product_data = await get_ltd_product ( session , url = ltd_product_url ) product_name = ltd_product_data [ 'slug' ] doc_handle_match = DOCUMENT_HANDLE_PATTERN . match ( product_name ) if doc_handle_match is None : logger . debug ( '%s is not a document repo' , product_name ) return try : return await process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotSphinxTechnoteError : logger . debug ( '%s is not a Sphinx-based technote.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return try : return await process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotLanderPageError : logger . debug ( '%s is not a Lander page with a metadata.jsonld file.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) return Cr
12219	def _make_all_matchers ( cls , parameters ) : for name , param in parameters : annotation = param . annotation if annotation is not Parameter . empty : yield name , cls . _make_param_matcher ( annotation , param . kind )
6921	def _autocorr_func3 ( mags , lag , maglen , magmed , magstd ) : result = npcorrelate ( mags , mags , mode = 'full' ) result = result / npmax ( result ) return result [ int ( result . size / 2 ) : ]
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
5394	def _localize_inputs_recursive_command ( self , task_dir , inputs ) : data_dir = os . path . join ( task_dir , _DATA_SUBDIR ) provider_commands = [ providers_util . build_recursive_localize_command ( data_dir , inputs , file_provider ) for file_provider in _SUPPORTED_INPUT_PROVIDERS ] return '\n' . join ( provider_commands )
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
5576	def load_output_writer ( output_params , readonly = False ) : if not isinstance ( output_params , dict ) : raise TypeError ( "output_params must be a dictionary" ) driver_name = output_params [ "format" ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : _driver = v . load ( ) if all ( [ hasattr ( _driver , attr ) for attr in [ "OutputData" , "METADATA" ] ] ) and ( _driver . METADATA [ "driver_name" ] == driver_name ) : return _driver . OutputData ( output_params , readonly = readonly ) raise MapcheteDriverError ( "no loader for driver '%s' could be found." % driver_name )
13316	def create ( name_or_path = None , config = None ) : if utils . is_system_path ( name_or_path ) : path = unipath ( name_or_path ) else : path = unipath ( get_home_path ( ) , name_or_path ) if os . path . exists ( path ) : raise OSError ( '{} already exists' . format ( path ) ) env = VirtualEnvironment ( path ) utils . ensure_path_exists ( env . path ) if config : if utils . is_git_repo ( config ) : Git ( '' ) . clone ( config , env . path ) else : shutil . copy2 ( config , env . config_path ) else : with open ( env . config_path , 'w' ) as f : f . write ( defaults . environment_config ) utils . ensure_path_exists ( env . hook_path ) utils . ensure_path_exists ( env . modules_path ) env . run_hook ( 'precreate' ) virtualenv . create_environment ( env . path ) if not utils . is_home_environment ( env . path ) : EnvironmentCache . add ( env ) EnvironmentCache . save ( ) try : env . update ( ) except : utils . rmtree ( path ) logger . debug ( 'Failed to update, rolling back...' ) raise else : env . run_hook ( 'postcreate' ) return env
12050	def getParent ( abfFname ) : child = os . path . abspath ( abfFname ) files = sorted ( glob . glob ( os . path . dirname ( child ) + "/*.*" ) ) parentID = abfFname for fname in files : if fname . endswith ( ".abf" ) and fname . replace ( ".abf" , ".TIF" ) in files : parentID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if os . path . basename ( child ) in fname : break return parentID
12573	def put_df_as_ndarray ( self , key , df , range_values , loop_multiindex = False , unstack = False , fill_value = 0 , fill_method = None ) : idx_colnames = df . index . names if key is None : key = idx_colnames [ 0 ] if loop_multiindex : idx_values = df . index . get_level_values ( 0 ) . unique ( ) for idx in idx_values : vals , _ = self . _fill_missing_values ( df . xs ( ( idx , ) , level = idx_colnames [ 0 ] ) , range_values , fill_value = fill_value , fill_method = fill_method ) ds_name = str ( idx ) + '_' + '_' . join ( vals . columns ) self . _push_dfblock ( key , vals , ds_name , range_values ) return self . _handle . get_node ( '/' + str ( key ) ) else : if unstack : df = df . unstack ( idx_colnames [ 0 ] ) for idx in df : vals , _ = self . _fill_missing_values ( df [ idx ] , range_values , fill_value = fill_value , fill_method = fill_method ) vals = np . nan_to_num ( vals ) ds_name = '_' . join ( [ str ( x ) for x in vals . name ] ) self . _push_dfblock ( key , vals , ds_name , range_values ) return self . _handle . get_node ( '/' + str ( key ) ) vals , _ = self . _fill_missing_values ( df , range_values , fill_value = fill_value , fill_method = fill_method ) ds_name = self . _array_dsname return self . _push_dfblock ( key , vals , ds_name , range_values )
7230	def create ( self , vectors ) : if type ( vectors ) is dict : vectors = [ vectors ] for vector in vectors : if not 'properties' in list ( vector . keys ( ) ) : raise Exception ( 'Vector does not contain "properties" field.' ) if not 'item_type' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "item_type".' ) if not 'ingest_source' in list ( vector [ 'properties' ] . keys ( ) ) : raise Exception ( 'Vector does not contain "ingest_source".' ) r = self . gbdx_connection . post ( self . create_url , data = json . dumps ( vectors ) ) r . raise_for_status ( ) return r . json ( )
1007	def _learnPhase1 ( self , activeColumns , readOnly = False ) : self . lrnActiveState [ 't' ] . fill ( 0 ) numUnpredictedColumns = 0 for c in activeColumns : predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictedCells = len ( predictingCells ) assert numPredictedCells <= 1 if numPredictedCells == 1 : i = predictingCells [ 0 ] self . lrnActiveState [ 't' ] [ c , i ] = 1 continue numUnpredictedColumns += 1 if readOnly : continue i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) if s is not None and s . isSequenceSegment ( ) : if self . verbosity >= 4 : print "Learn branch 0, found segment match. Learning on col=" , c self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) s . totalActivations += 1 trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , minNumSyns = 0 ) else : i = self . _getCellForNewSegment ( c ) if ( self . verbosity >= 4 ) : print "Learn branch 1, no match. Learning on col=" , c , print ", newCellIdxInCol=" , i self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) segUpdate . sequenceSegment = True self . _adaptSegment ( segUpdate ) numBottomUpColumns = len ( activeColumns ) if numUnpredictedColumns < numBottomUpColumns / 2 : return True else : return False
4595	def make_object ( * args , typename = None , python_path = None , datatype = None , ** kwds ) : datatype = datatype or import_symbol ( typename , python_path ) field_types = getattr ( datatype , 'FIELD_TYPES' , fields . FIELD_TYPES ) return datatype ( * args , ** fields . component ( kwds , field_types ) )
11153	def sha512file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha512 , nbytes = nbytes , chunk_size = chunk_size )
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
2383	def construct_mapping ( self , node , deep = False ) : mapping = super ( ExtendedSafeConstructor , self ) . construct_mapping ( node , deep ) return { ( str ( key ) if isinstance ( key , int ) else key ) : mapping [ key ] for key in mapping }
6865	def get_time_flux_errs_from_Ames_lightcurve ( infile , lctype , cadence_min = 2 ) : warnings . warn ( "Use the astrotess.read_tess_fitslc and " "astrotess.consolidate_tess_fitslc functions instead of this function. " "This function will be removed in astrobase v0.4.2." , FutureWarning ) if lctype not in ( 'PDCSAP' , 'SAP' ) : raise ValueError ( 'unknown light curve type requested: %s' % lctype ) hdulist = pyfits . open ( infile ) main_hdr = hdulist [ 0 ] . header lc_hdr = hdulist [ 1 ] . header lc = hdulist [ 1 ] . data if ( ( 'Ames' not in main_hdr [ 'ORIGIN' ] ) or ( 'LIGHTCURVE' not in lc_hdr [ 'EXTNAME' ] ) ) : raise ValueError ( 'could not understand input LC format. ' 'Is it a TESS TOI LC file?' ) time = lc [ 'TIME' ] flux = lc [ '{:s}_FLUX' . format ( lctype ) ] err_flux = lc [ '{:s}_FLUX_ERR' . format ( lctype ) ] sel = ( lc [ 'QUALITY' ] == 0 ) sel &= np . isfinite ( time ) sel &= np . isfinite ( flux ) sel &= np . isfinite ( err_flux ) sel &= ~ np . isnan ( time ) sel &= ~ np . isnan ( flux ) sel &= ~ np . isnan ( err_flux ) sel &= ( time != 0 ) sel &= ( flux != 0 ) sel &= ( err_flux != 0 ) time = time [ sel ] flux = flux [ sel ] err_flux = err_flux [ sel ] lc_cadence_diff = np . abs ( np . nanmedian ( np . diff ( time ) ) * 24 * 60 - cadence_min ) if lc_cadence_diff > 1.0e-2 : raise ValueError ( 'the light curve is not at the required cadence specified: %.2f' % cadence_min ) fluxmedian = np . nanmedian ( flux ) flux /= fluxmedian err_flux /= fluxmedian return time , flux , err_flux
12441	def require_authentication ( self , request ) : request . user = user = None if request . method == 'OPTIONS' : return for auth in self . meta . authentication : user = auth . authenticate ( request ) if user is False : continue if user is None and not auth . allow_anonymous : auth . unauthenticated ( ) request . user = user return if not user and not auth . allow_anonymous : auth . unauthenticated ( )
4653	def add_required_fees ( self , ops , asset_id = "1.3.0" ) : ws = self . blockchain . rpc fees = ws . get_required_fees ( [ i . json ( ) for i in ops ] , asset_id ) for i , d in enumerate ( ops ) : if isinstance ( fees [ i ] , list ) : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 0 ] [ "amount" ] , asset_id = fees [ i ] [ 0 ] [ "asset_id" ] ) for j , _ in enumerate ( ops [ i ] . op . data [ "proposed_ops" ] . data ) : ops [ i ] . op . data [ "proposed_ops" ] . data [ j ] . data [ "op" ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ 1 ] [ j ] [ "amount" ] , asset_id = fees [ i ] [ 1 ] [ j ] [ "asset_id" ] , ) else : ops [ i ] . op . data [ "fee" ] = Asset ( amount = fees [ i ] [ "amount" ] , asset_id = fees [ i ] [ "asset_id" ] ) return ops
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
13719	def create_tree ( endpoints ) : tree = { } for method , url , doc in endpoints : path = [ p for p in url . strip ( '/' ) . split ( '/' ) ] here = tree version = path [ 0 ] here . setdefault ( version , { } ) here = here [ version ] for p in path [ 1 : ] : part = _camelcase_to_underscore ( p ) here . setdefault ( part , { } ) here = here [ part ] if not 'METHODS' in here : here [ 'METHODS' ] = [ [ method , doc ] ] else : if not method in here [ 'METHODS' ] : here [ 'METHODS' ] . append ( [ method , doc ] ) return tree
4116	def lsf2poly ( lsf ) : lsf = numpy . array ( lsf ) if max ( lsf ) > numpy . pi or min ( lsf ) < 0 : raise ValueError ( 'Line spectral frequencies must be between 0 and pi.' ) p = len ( lsf ) z = numpy . exp ( 1.j * lsf ) rQ = z [ 0 : : 2 ] rP = z [ 1 : : 2 ] rQ = numpy . concatenate ( ( rQ , rQ . conjugate ( ) ) ) rP = numpy . concatenate ( ( rP , rP . conjugate ( ) ) ) Q = numpy . poly ( rQ ) P = numpy . poly ( rP ) if p % 2 : P1 = numpy . convolve ( P , [ 1 , 0 , - 1 ] ) Q1 = Q else : P1 = numpy . convolve ( P , [ 1 , - 1 ] ) Q1 = numpy . convolve ( Q , [ 1 , 1 ] ) a = .5 * ( P1 + Q1 ) return a [ 0 : - 1 : 1 ]
6391	def encode ( self , word , max_length = 8 ) : word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '÷' values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
13013	def strip_labels ( filename ) : labels = [ ] with open ( filename ) as f , open ( 'processed_labels.txt' , 'w' ) as f1 : for l in f : if l . startswith ( '#' ) : next l = l . replace ( " ." , '' ) l = l . replace ( ">\tskos:prefLabel\t" , ' ' ) l = l . replace ( "<" , '' ) l = l . replace ( ">\trdfs:label\t" , ' ' ) f1 . write ( l )
12830	def set_data ( self , data = { } , datetime_fields = [ ] ) : if datetime_fields : for field in datetime_fields : if field in data : data [ field ] = self . _parse_datetime ( data [ field ] ) super ( CampfireEntity , self ) . set_data ( data )
9804	def activate ( username ) : try : PolyaxonClient ( ) . user . activate_user ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not activate user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "User `{}` was activated successfully." . format ( username ) )
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
5963	def plot ( self , ** kwargs ) : columns = kwargs . pop ( 'columns' , Ellipsis ) maxpoints = kwargs . pop ( 'maxpoints' , self . maxpoints_default ) transform = kwargs . pop ( 'transform' , lambda x : x ) method = kwargs . pop ( 'method' , "mean" ) ax = kwargs . pop ( 'ax' , None ) if columns is Ellipsis or columns is None : columns = numpy . arange ( self . array . shape [ 0 ] ) if len ( columns ) == 0 : raise MissingDataError ( "plot() needs at least one column of data" ) if len ( self . array . shape ) == 1 or self . array . shape [ 0 ] == 1 : a = numpy . ravel ( self . array ) X = numpy . arange ( len ( a ) ) a = numpy . vstack ( ( X , a ) ) columns = [ 0 ] + [ c + 1 for c in columns ] else : a = self . array color = kwargs . pop ( 'color' , self . default_color_cycle ) try : cmap = matplotlib . cm . get_cmap ( color ) colors = cmap ( matplotlib . colors . Normalize ( ) ( numpy . arange ( len ( columns [ 1 : ] ) , dtype = float ) ) ) except TypeError : colors = cycle ( utilities . asiterable ( color ) ) if ax is None : ax = plt . gca ( ) a = self . decimate ( method , numpy . asarray ( transform ( a ) ) [ columns ] , maxpoints = maxpoints ) ma = numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) ) for column , color in zip ( range ( 1 , len ( columns ) ) , colors ) : if len ( ma [ column ] ) == 0 : warnings . warn ( "No data to plot for column {column:d}" . format ( ** vars ( ) ) , category = MissingDataWarning ) kwargs [ 'color' ] = color ax . plot ( ma [ 0 ] , ma [ column ] , ** kwargs ) return ax
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
2937	def deserialize_data ( self , workflow , start_node ) : name = start_node . getAttribute ( 'name' ) value = start_node . getAttribute ( 'value' ) return name , value
8667	def put_key ( key_name , value , description , meta , modify , add , lock , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Stashing {0} key...' . format ( key_type ) ) stash . put ( name = key_name , value = _build_dict_from_key_value ( value ) , modify = modify , metadata = _build_dict_from_key_value ( meta ) , description = description , lock = lock , key_type = key_type , add = add ) click . echo ( 'Key stashed successfully' ) except GhostError as ex : sys . exit ( ex )
12418	def capture_stdout ( ) : stdout = sys . stdout try : capture_out = StringIO ( ) sys . stdout = capture_out yield capture_out finally : sys . stdout = stdout
4661	def broadcast ( self , tx = None ) : if tx : return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
1345	def predictions ( self , image ) : return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 )
3658	def _destroy_image_acquirer ( self , ia ) : id_ = None if ia . device : ia . stop_image_acquisition ( ) ia . _release_data_streams ( ) id_ = ia . _device . id_ if ia . device . node_map : if ia . _chunk_adapter : ia . _chunk_adapter . detach_buffer ( ) ia . _chunk_adapter = None self . _logger . info ( 'Detached a buffer from the chunk adapter of {0}.' . format ( id_ ) ) ia . device . node_map . disconnect ( ) self . _logger . info ( 'Disconnected the port from the NodeMap of {0}.' . format ( id_ ) ) if ia . _device . is_open ( ) : ia . _device . close ( ) self . _logger . info ( 'Closed Device module, {0}.' . format ( id_ ) ) ia . _device = None if id_ : self . _logger . info ( 'Destroyed the ImageAcquirer object which {0} ' 'had belonged to.' . format ( id_ ) ) else : self . _logger . info ( 'Destroyed an ImageAcquirer.' ) if self . _profiler : self . _profiler . print_diff ( ) self . _ias . remove ( ia )
1793	def MUL ( cpu , src ) : size = src . size reg_name_low , reg_name_high = { 8 : ( 'AL' , 'AH' ) , 16 : ( 'AX' , 'DX' ) , 32 : ( 'EAX' , 'EDX' ) , 64 : ( 'RAX' , 'RDX' ) } [ size ] res = ( Operators . ZEXTEND ( cpu . read_register ( reg_name_low ) , 256 ) * Operators . ZEXTEND ( src . read ( ) , 256 ) ) cpu . write_register ( reg_name_low , Operators . EXTRACT ( res , 0 , size ) ) cpu . write_register ( reg_name_high , Operators . EXTRACT ( res , size , size ) ) cpu . OF = Operators . EXTRACT ( res , size , size ) != 0 cpu . CF = cpu . OF
11508	def download_item ( self , item_id , token = None , revision = None ) : parameters = dict ( ) parameters [ 'id' ] = item_id if token : parameters [ 'token' ] = token if revision : parameters [ 'revision' ] = revision method_url = self . full_url + 'midas.item.download' request = requests . get ( method_url , params = parameters , stream = True , verify = self . _verify_ssl_certificate ) filename = request . headers [ 'content-disposition' ] [ 21 : ] . strip ( '"' ) return filename , request . iter_content ( chunk_size = 10 * 1024 )
5010	def _call_post_with_user_override ( self , sap_user_id , url , payload ) : SAPSuccessFactorsEnterpriseCustomerConfiguration = apps . get_model ( 'sap_success_factors' , 'SAPSuccessFactorsEnterpriseCustomerConfiguration' ) oauth_access_token , _ = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , sap_user_id , SAPSuccessFactorsEnterpriseCustomerConfiguration . USER_TYPE_USER ) response = requests . post ( url , data = payload , headers = { 'Authorization' : 'Bearer {}' . format ( oauth_access_token ) , 'content-type' : 'application/json' } ) return response . status_code , response . text
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
1264	def sanity_check_states ( states_spec ) : states = copy . deepcopy ( states_spec ) is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) for name , state in states . items ( ) : if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique
11214	def compare_signature ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) return hmac . compare_digest ( expected , actual )
4075	def set_cfg_value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\n' . join ( value ) config [ section ] [ option ] = value
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
7726	def __init ( self , code ) : code = int ( code ) if code < 0 or code > 999 : raise ValueError ( "Bad status code" ) self . code = code
11143	def is_name_allowed ( self , path ) : assert isinstance ( path , basestring ) , "given path must be a string" name = os . path . basename ( path ) if not len ( name ) : return False , "empty name is not allowed" for em in [ self . __repoLock , self . __repoFile , self . __dirInfo , self . __dirLock ] : if name == em : return False , "name '%s' is reserved for pyrep internal usage" % em for pm in [ self . __fileInfo , self . __fileLock ] : if name == pm or ( name . endswith ( pm [ 3 : ] ) and name . startswith ( '.' ) ) : return False , "name pattern '%s' is not allowed as result may be reserved for pyrep internal usage" % pm return True , None
6790	def createsuperuser ( self , username = 'admin' , email = None , password = None , site = None ) : r = self . local_renderer site = site or self . genv . SITE self . set_site_specifics ( site ) options = [ '--username=%s' % username ] if email : options . append ( '--email=%s' % email ) if password : options . append ( '--password=%s' % password ) r . env . options_str = ' ' . join ( options ) if self . is_local : r . env . project_dir = r . env . local_project_dir r . genv . SITE = r . genv . SITE or site r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; {manage_cmd} {createsuperuser_cmd} {options_str}' )
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
9796	def get ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : response = PolyaxonClient ( ) . experiment_group . get_experiment_group ( user , project_name , _group ) cache . cache ( config_manager = GroupManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_group_details ( response )
10403	def microcanonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'n' , 'uint32' ) , ( 'edge' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'has_spanning_cluster' , 'bool' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'uint32' ) , ( 'moments' , '(5,)uint64' ) , ] ) return _ndarray_dtype ( fields )
3319	def create ( self , path , lock ) : self . _lock . acquire_write ( ) try : assert lock . get ( "token" ) is None assert lock . get ( "expire" ) is None , "Use timeout instead of expire" assert path and "/" in path org_path = path path = normalize_lock_root ( path ) lock [ "root" ] = path timeout = float ( lock . get ( "timeout" ) ) if timeout is None : timeout = LockStorageDict . LOCK_TIME_OUT_DEFAULT elif timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout validate_lock ( lock ) token = generate_lock_token ( ) lock [ "token" ] = token self . _dict [ token ] = lock key = "URL2TOKEN:{}" . format ( path ) if key not in self . _dict : self . _dict [ key ] = [ token ] else : tokList = self . _dict [ key ] tokList . append ( token ) self . _dict [ key ] = tokList self . _flush ( ) _logger . debug ( "LockStorageDict.set({!r}): {}" . format ( org_path , lock_string ( lock ) ) ) return lock finally : self . _lock . release ( )
1830	def JB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == True , target . read ( ) , cpu . PC )
11599	def verify ( xml , stream ) : import xmlsec signature_node = xmlsec . tree . find_node ( xml , xmlsec . Node . SIGNATURE ) if signature_node is None : return False ctx = xmlsec . SignatureContext ( ) ctx . register_id ( xml ) for assertion in xml . xpath ( "//*[local-name()='Assertion']" ) : ctx . register_id ( assertion ) key = None for fmt in [ xmlsec . KeyFormat . PEM , xmlsec . KeyFormat . CERT_PEM ] : stream . seek ( 0 ) try : key = xmlsec . Key . from_memory ( stream , fmt ) break except ValueError : pass ctx . key = key try : ctx . verify ( signature_node ) return True except Exception : return False
7194	def histogram_stretch ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , ** kwargs )
1222	def processed_shape ( self , shape ) : for processor in self . preprocessors : shape = processor . processed_shape ( shape = shape ) return shape
5517	def limit ( self , value ) : self . _limit = value self . _start = None self . _sum = 0
11336	def get_records ( self , url ) : page = urllib2 . urlopen ( url ) pages = [ BeautifulSoup ( page ) ] numpag = pages [ 0 ] . body . findAll ( 'span' , attrs = { 'class' : 'number-of-pages' } ) if len ( numpag ) > 0 : if re . search ( '^\d+$' , numpag [ 0 ] . string ) : for i in range ( int ( numpag [ 0 ] . string ) - 1 ) : page = urllib2 . urlopen ( '%s/page/%i' % ( url , i + 2 ) ) pages . append ( BeautifulSoup ( page ) ) else : print ( "number of pages %s not an integer" % ( numpag [ 0 ] . string ) ) impl = getDOMImplementation ( ) doc = impl . createDocument ( None , "collection" , None ) links = [ ] for page in pages : links += page . body . findAll ( 'p' , attrs = { 'class' : 'title' } ) links += page . body . findAll ( 'h3' , attrs = { 'class' : 'title' } ) for link in links : record = self . _get_record ( link ) doc . firstChild . appendChild ( record ) return doc . toprettyxml ( )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
9806	def deploy ( file , manager_path , check , dry_run ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
6034	def padded_grid_stack_from_mask_sub_grid_size_and_psf_shape ( cls , mask , sub_grid_size , psf_shape ) : regular_padded_grid = PaddedRegularGrid . padded_grid_from_shape_psf_shape_and_pixel_scale ( shape = mask . shape , psf_shape = psf_shape , pixel_scale = mask . pixel_scale ) sub_padded_grid = PaddedSubGrid . padded_grid_from_mask_sub_grid_size_and_psf_shape ( mask = mask , sub_grid_size = sub_grid_size , psf_shape = psf_shape ) return GridStack ( regular = regular_padded_grid , sub = sub_padded_grid , blurring = np . array ( [ [ 0.0 , 0.0 ] ] ) )
5976	def total_regular_pixels_from_mask ( mask ) : total_regular_pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total_regular_pixels += 1 return total_regular_pixels
11985	def upload_folder ( self , bucket , folder , key = None , skip = None , content_types = None ) : uploader = FolderUploader ( self , bucket , folder , key , skip , content_types ) return uploader . start ( )
535	def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
8601	def delete_share ( self , group_id , resource_id ) : response = self . _perform_request ( url = '/um/groups/%s/shares/%s' % ( group_id , resource_id ) , method = 'DELETE' ) return response
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
13121	def get_pipe ( self , object_type ) : for line in sys . stdin : try : data = json . loads ( line . strip ( ) ) obj = object_type ( ** data ) yield obj except ValueError : yield self . id_to_object ( line . strip ( ) )
7103	def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
5243	def tz_convert ( dt , to_tz , from_tz = None ) -> str : logger = logs . get_logger ( tz_convert , level = 'info' ) f_tz , t_tz = get_tz ( from_tz ) , get_tz ( to_tz ) from_dt = pd . Timestamp ( str ( dt ) , tz = f_tz ) logger . debug ( f'converting {str(from_dt)} from {f_tz} to {t_tz} ...' ) return str ( pd . Timestamp ( str ( from_dt ) , tz = t_tz ) )
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
11737	def route ( bp , * args , ** kwargs ) : kwargs [ 'strict_slashes' ] = kwargs . pop ( 'strict_slashes' , False ) body = _validate_schema ( kwargs . pop ( '_body' , None ) ) query = _validate_schema ( kwargs . pop ( '_query' , None ) ) output = _validate_schema ( kwargs . pop ( 'marshal_with' , None ) ) validate = kwargs . pop ( 'validate' , True ) def decorator ( f ) : @ bp . route ( * args , ** kwargs ) @ wraps ( f ) def wrapper ( * inner_args , ** inner_kwargs ) : try : if query is not None : query . strict = validate url = furl ( request . url ) inner_kwargs [ '_query' ] = query . load ( data = url . args ) if body is not None : body . strict = validate json_data = request . get_json ( ) if json_data is None : json_data = { } inner_kwargs [ '_body' ] = body . load ( data = json_data ) except ValidationError as err : return jsonify ( err . messages ) , 422 if output : data = output . dump ( f ( * inner_args , ** inner_kwargs ) ) return jsonify ( data [ 0 ] ) return f ( * inner_args , ** inner_kwargs ) return f return decorator
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_out_streams ( spout ) return spout
8863	def run_pep8 ( request_data ) : import pycodestyle from pyqode . python . backend . pep8utils import CustomChecker WARNING = 1 code = request_data [ 'code' ] path = request_data [ 'path' ] max_line_length = request_data [ 'max_line_length' ] ignore_rules = request_data [ 'ignore_rules' ] ignore_rules += [ 'W291' , 'W292' , 'W293' , 'W391' ] pycodestyle . MAX_LINE_LENGTH = max_line_length pep8style = pycodestyle . StyleGuide ( parse_argv = False , config_file = '' , checker_class = CustomChecker ) try : results = pep8style . input_file ( path , lines = code . splitlines ( True ) ) except Exception : _logger ( ) . exception ( 'Failed to run PEP8 analysis with data=%r' % request_data ) return [ ] else : messages = [ ] for line_number , offset , code , text , doc in results : if code in ignore_rules : continue messages . append ( ( '[PEP8] %s: %s' % ( code , text ) , WARNING , line_number - 1 ) ) return messages
5956	def load_v4_tools ( ) : logger . debug ( "Loading v4 tools..." ) names = config . get_tool_names ( ) if len ( names ) == 0 and 'GMXBIN' in os . environ : names = find_executables ( os . environ [ 'GMXBIN' ] ) if len ( names ) == 0 or len ( names ) > len ( V4TOOLS ) * 4 : names = list ( V4TOOLS ) names . extend ( config . get_extra_tool_names ( ) ) tools = { } for name in names : fancy = make_valid_identifier ( name ) tools [ fancy ] = tool_factory ( fancy , name , None ) if not tools : errmsg = "Failed to load v4 tools" logger . debug ( errmsg ) raise GromacsToolLoadingError ( errmsg ) logger . debug ( "Loaded {0} v4 tools successfully!" . format ( len ( tools ) ) ) return tools
13264	def get_configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
10118	def regular_polygon ( cls , center , radius , n_vertices , start_angle = 0 , ** kwargs ) : angles = ( np . arange ( n_vertices ) * 2 * np . pi / n_vertices ) + start_angle return cls ( center + radius * np . array ( [ np . cos ( angles ) , np . sin ( angles ) ] ) . T , ** kwargs )
5185	def catalog ( self , node ) : catalogs = self . catalogs ( path = node ) return next ( x for x in catalogs )
1195	def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps
13451	def imgmax ( self ) : if not hasattr ( self , '_imgmax' ) : imgmax = _np . max ( self . images [ 0 ] ) for img in self . images : imax = _np . max ( img ) if imax > imgmax : imgmax = imax self . _imgmax = imgmax return self . _imgmax
10026	def create_application_version ( self , version_label , key ) : out ( "Creating application version " + str ( version_label ) + " for " + str ( key ) ) self . ebs . create_application_version ( self . app_name , version_label , s3_bucket = self . aws . bucket , s3_key = self . aws . bucket_path + key )
9289	def _send_login ( self ) : login_str = "user {0} pass {1} vers aprslib {3}{2}\r\n" login_str = login_str . format ( self . callsign , self . passwd , ( " filter " + self . filter ) if self . filter != "" else "" , __version__ ) self . logger . info ( "Sending login information" ) try : self . _sendall ( login_str ) self . sock . settimeout ( 5 ) test = self . sock . recv ( len ( login_str ) + 100 ) if is_py3 : test = test . decode ( 'latin-1' ) test = test . rstrip ( ) self . logger . debug ( "Server: %s" , test ) _ , _ , callsign , status , _ = test . split ( ' ' , 4 ) if callsign == "" : raise LoginError ( "Server responded with empty callsign???" ) if callsign != self . callsign : raise LoginError ( "Server: %s" % test ) if status != "verified," and self . passwd != "-1" : raise LoginError ( "Password is incorrect" ) if self . passwd == "-1" : self . logger . info ( "Login successful (receive only)" ) else : self . logger . info ( "Login successful" ) except LoginError as e : self . logger . error ( str ( e ) ) self . close ( ) raise except : self . close ( ) self . logger . error ( "Failed to login" ) raise LoginError ( "Failed to login" )
419	def save_training_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . TrainLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] train log: " + _log )
5320	def find_ports ( device ) : bus_id = device . bus dev_id = device . address for dirent in os . listdir ( USB_SYS_PREFIX ) : matches = re . match ( USB_PORTS_STR + '$' , dirent ) if matches : bus_str = readattr ( dirent , 'busnum' ) if bus_str : busnum = float ( bus_str ) else : busnum = None dev_str = readattr ( dirent , 'devnum' ) if dev_str : devnum = float ( dev_str ) else : devnum = None if busnum == bus_id and devnum == dev_id : return str ( matches . groups ( ) [ 1 ] )
7855	def fetch ( self ) : from . . iq import Iq jid , node = self . address iq = Iq ( to_jid = jid , stanza_type = "get" ) disco = self . disco_class ( node ) iq . add_content ( disco . xmlnode ) self . stream . set_response_handlers ( iq , self . __response , self . __error , self . __timeout ) self . stream . send ( iq )
9442	def reload_cache_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadCacheConfig/' method = 'POST' return self . request ( path , method , call_params )
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
5374	def _prefix_exists_in_gcs ( gcs_prefix , credentials = None ) : gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = gcs_prefix [ len ( 'gs://' ) : ] . split ( '/' , 1 ) request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix , maxResults = 1 ) response = request . execute ( ) return response . get ( 'items' , None )
13109	def r_annotations ( self ) : target = request . args . get ( "target" , None ) wildcard = request . args . get ( "wildcard" , "." , type = str ) include = request . args . get ( "include" ) exclude = request . args . get ( "exclude" ) limit = request . args . get ( "limit" , None , type = int ) start = request . args . get ( "start" , 1 , type = int ) expand = request . args . get ( "expand" , False , type = bool ) if target : try : urn = MyCapytain . common . reference . URN ( target ) except ValueError : return "invalid urn" , 400 count , annotations = self . __queryinterface__ . getAnnotations ( urn , wildcard = wildcard , include = include , exclude = exclude , limit = limit , start = start , expand = expand ) else : count , annotations = self . __queryinterface__ . getAnnotations ( None , limit = limit , start = start , expand = expand ) mapped = [ ] response = { "@context" : type ( self ) . JSONLD_CONTEXT , "id" : url_for ( ".r_annotations" , start = start , limit = limit ) , "type" : "AnnotationCollection" , "startIndex" : start , "items" : [ ] , "total" : count } for a in annotations : mapped . append ( { "id" : url_for ( ".r_annotation" , sha = a . sha ) , "body" : url_for ( ".r_annotation_body" , sha = a . sha ) , "type" : "Annotation" , "target" : a . target . to_json ( ) , "dc:type" : a . type_uri , "owl:sameAs" : [ a . uri ] , "nemo:slug" : a . slug } ) response [ "items" ] = mapped response = jsonify ( response ) return response
11979	def set ( self , ip , netmask = None ) : if isinstance ( ip , str ) and netmask is None : ipnm = ip . split ( '/' ) if len ( ipnm ) != 2 : raise ValueError ( 'set: invalid CIDR: "%s"' % ip ) ip = ipnm [ 0 ] netmask = ipnm [ 1 ] if isinstance ( ip , IPv4Address ) : self . _ip = ip else : self . _ip = IPv4Address ( ip ) if isinstance ( netmask , IPv4NetMask ) : self . _nm = netmask else : self . _nm = IPv4NetMask ( netmask ) ipl = int ( self . _ip ) nml = int ( self . _nm ) base_add = ipl & nml self . _ip_num = 0xFFFFFFFF - 1 - nml if self . _ip_num in ( - 1 , 0 ) : if self . _ip_num == - 1 : self . _ip_num = 1 else : self . _ip_num = 2 self . _net_ip = None self . _bc_ip = None self . _first_ip_dec = base_add self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) if self . _ip_num == 1 : last_ip_dec = self . _first_ip_dec else : last_ip_dec = self . _first_ip_dec + 1 self . _last_ip = IPv4Address ( last_ip_dec , notation = IP_DEC ) return self . _net_ip = IPv4Address ( base_add , notation = IP_DEC ) self . _bc_ip = IPv4Address ( base_add + self . _ip_num + 1 , notation = IP_DEC ) self . _first_ip_dec = base_add + 1 self . _first_ip = IPv4Address ( self . _first_ip_dec , notation = IP_DEC ) self . _last_ip = IPv4Address ( base_add + self . _ip_num , notation = IP_DEC )
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
5734	def _get_result_msg_and_payload ( result , stream ) : groups = _GDB_MI_RESULT_RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance_past_chars ( [ "," ] ) payload = _parse_dict ( stream ) return token , message , payload
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
13780	def FindEnumTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _enum_descriptors : self . FindFileContainingSymbol ( full_name ) return self . _enum_descriptors [ full_name ]
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
11048	def dataReceived ( self , data ) : self . resetTimeout ( ) lines = ( self . _buffer + data ) . splitlines ( ) if data . endswith ( b'\n' ) or data . endswith ( b'\r' ) : self . _buffer = b'' else : self . _buffer = lines . pop ( - 1 ) for line in lines : if self . transport . disconnecting : return if len ( line ) > self . _max_length : self . lineLengthExceeded ( line ) return else : self . lineReceived ( line ) if len ( self . _buffer ) > self . _max_length : self . lineLengthExceeded ( self . _buffer ) return
8083	def transform ( self , mode = None ) : if mode : self . _canvas . mode = mode return self . _canvas . mode
3786	def TP_dependent_property_derivative_T ( self , T , P , order = 1 ) : r sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method in sorted_valid_methods_P : try : return self . calculate_derivative_T ( T , P , method , order ) except : pass return None
13873	def CopyFile ( source_filename , target_filename , override = True , md5_check = False , copy_symlink = True ) : from . _exceptions import FileNotFoundError if not override and Exists ( target_filename ) : from . _exceptions import FileAlreadyExistsError raise FileAlreadyExistsError ( target_filename ) md5_check = md5_check and not target_filename . endswith ( '.md5' ) if md5_check : source_md5_filename = source_filename + '.md5' target_md5_filename = target_filename + '.md5' try : source_md5_contents = GetFileContents ( source_md5_filename ) except FileNotFoundError : source_md5_contents = None try : target_md5_contents = GetFileContents ( target_md5_filename ) except FileNotFoundError : target_md5_contents = None if source_md5_contents is not None and source_md5_contents == target_md5_contents and Exists ( target_filename ) : return MD5_SKIP _DoCopyFile ( source_filename , target_filename , copy_symlink = copy_symlink ) if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents : CreateFile ( target_md5_filename , source_md5_contents )
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
2711	def json_iter ( path ) : with open ( path , 'r' ) as f : for line in f . readlines ( ) : yield json . loads ( line )
2367	def _load ( self , path ) : self . tables = [ ] current_table = DefaultTable ( self ) with Utf8Reader ( path ) as f : self . raw_text = f . read ( ) f . _file . seek ( 0 ) matcher = Matcher ( re . IGNORECASE ) for linenumber , raw_text in enumerate ( f . readlines ( ) ) : linenumber += 1 raw_text = raw_text . replace ( u'\xA0' , ' ' ) raw_text = raw_text . rstrip ( ) cells = TxtReader . split_row ( raw_text ) _heading_regex = r'^\s*\*+\s*(.*?)[ *]*$' if matcher ( _heading_regex , cells [ 0 ] ) : table_name = matcher . group ( 1 ) current_table = tableFactory ( self , linenumber , table_name , raw_text ) self . tables . append ( current_table ) else : current_table . append ( Row ( linenumber , raw_text , cells ) )
5872	def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
2673	def invoke ( src , event_file = 'event.json' , config_file = 'config.yaml' , profile_name = None , verbose = False , ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) if profile_name : os . environ [ 'AWS_PROFILE' ] = profile_name env_vars = cfg . get ( 'environment_variables' ) if env_vars : for key , value in env_vars . items ( ) : os . environ [ key ] = get_environment_variable_value ( value ) path_to_event_file = os . path . join ( src , event_file ) event = read ( path_to_event_file , loader = json . loads ) try : sys . path . index ( src ) except ValueError : sys . path . append ( src ) handler = cfg . get ( 'handler' ) fn = get_callable_handler_function ( src , handler ) timeout = cfg . get ( 'timeout' ) if timeout : context = LambdaContext ( cfg . get ( 'function_name' ) , timeout ) else : context = LambdaContext ( cfg . get ( 'function_name' ) ) start = time . time ( ) results = fn ( event , context ) end = time . time ( ) print ( '{0}' . format ( results ) ) if verbose : print ( '\nexecution time: {:.8f}s\nfunction execution ' 'timeout: {:2}s' . format ( end - start , cfg . get ( 'timeout' , 15 ) ) )
5694	def create_table ( self , conn ) : cur = conn . cursor ( ) if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : cur . execute ( self . tabledef ) conn . commit ( )
6259	def draw ( self , projection_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh : self . mesh . draw ( projection_matrix = projection_matrix , view_matrix = self . matrix_global_bytes , camera_matrix = camera_matrix , time = time ) for child in self . children : child . draw ( projection_matrix = projection_matrix , camera_matrix = camera_matrix , time = time )
2993	def otcSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( otcSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
5407	def _operation_status ( self ) : if not google_v2_operations . is_done ( self . _op ) : return 'RUNNING' if google_v2_operations . is_success ( self . _op ) : return 'SUCCESS' if google_v2_operations . is_canceled ( self . _op ) : return 'CANCELED' if google_v2_operations . is_failed ( self . _op ) : return 'FAILURE' raise ValueError ( 'Status for operation {} could not be determined' . format ( self . _op [ 'name' ] ) )
5087	def has_implicit_access_to_enrollment_api ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE , obj )
8382	def textpath ( self , i ) : if len ( self . _textpaths ) == i : self . _ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : txt += " (" + str ( i + 1 ) + "/" + str ( len ( self . q ) ) + ")" p = self . _ctx . textpath ( txt , 0 , 0 , width = self . _w ) h = self . _ctx . textheight ( txt , width = self . _w ) self . _textpaths . append ( ( p , h ) ) return self . _textpaths [ i ]
5007	def get_user_from_social_auth ( tpa_provider , tpa_username ) : user_social_auth = UserSocialAuth . objects . select_related ( 'user' ) . filter ( user__username = tpa_username , provider = tpa_provider . backend_name ) . first ( ) return user_social_auth . user if user_social_auth else None
9080	def get_providers ( self , ** kwargs ) : if 'ids' in kwargs : ids = [ self . concept_scheme_uri_map . get ( id , id ) for id in kwargs [ 'ids' ] ] providers = [ self . providers [ k ] for k in self . providers . keys ( ) if k in ids ] else : providers = list ( self . providers . values ( ) ) if 'subject' in kwargs : providers = [ p for p in providers if kwargs [ 'subject' ] in p . metadata [ 'subject' ] ] return providers
8035	def summarize ( text , char_limit , sentence_filter = None , debug = False ) : debug_info = { } sents = list ( tools . sent_splitter_ja ( text ) ) words_list = [ w . encode ( 'utf-8' ) for s in sents for w in tools . word_segmenter_ja ( s ) ] tf = collections . Counter ( ) for words in words_list : for w in words : tf [ w ] += 1.0 if sentence_filter is not None : valid_indices = [ i for i , s in enumerate ( sents ) if sentence_filter ( s ) ] sents = [ sents [ i ] for i in valid_indices ] words_list = [ words_list [ i ] for i in valid_indices ] sent_ids = [ str ( i ) for i in range ( len ( sents ) ) ] sent_id2len = dict ( ( id_ , len ( s ) ) for id_ , s in zip ( sent_ids , sents ) ) word_contain = dict ( ) for id_ , words in zip ( sent_ids , words_list ) : word_contain [ id_ ] = collections . defaultdict ( lambda : 0 ) for w in words : word_contain [ id_ ] [ w ] = 1 prob = pulp . LpProblem ( 'summarize' , pulp . LpMaximize ) sent_vars = pulp . LpVariable . dicts ( 'sents' , sent_ids , 0 , 1 , pulp . LpBinary ) word_vars = pulp . LpVariable . dicts ( 'words' , tf . keys ( ) , 0 , 1 , pulp . LpBinary ) prob += pulp . lpSum ( [ tf [ w ] * word_vars [ w ] for w in tf ] ) prob += pulp . lpSum ( [ sent_id2len [ id_ ] * sent_vars [ id_ ] for id_ in sent_ids ] ) <= char_limit , 'lengthRequirement' for w in tf : prob += pulp . lpSum ( [ word_contain [ id_ ] [ w ] * sent_vars [ id_ ] for id_ in sent_ids ] ) >= word_vars [ w ] , 'z:{}' . format ( w ) prob . solve ( ) sent_indices = [ ] for v in prob . variables ( ) : if v . name . startswith ( 'sents' ) and v . varValue == 1 : sent_indices . append ( int ( v . name . split ( '_' ) [ - 1 ] ) ) return [ sents [ i ] for i in sent_indices ] , debug_info
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
3369	def set_objective ( model , value , additive = False ) : interface = model . problem reverse_value = model . solver . objective . expression reverse_value = interface . Objective ( reverse_value , direction = model . solver . objective . direction , sloppy = True ) if isinstance ( value , dict ) : if not model . objective . is_Linear : raise ValueError ( 'can only update non-linear objectives ' 'additively using object of class ' 'model.problem.Objective, not %s' % type ( value ) ) if not additive : model . solver . objective = interface . Objective ( Zero , direction = model . solver . objective . direction ) for reaction , coef in value . items ( ) : model . solver . objective . set_linear_coefficients ( { reaction . forward_variable : coef , reaction . reverse_variable : - coef } ) elif isinstance ( value , ( Basic , optlang . interface . Objective ) ) : if isinstance ( value , Basic ) : value = interface . Objective ( value , direction = model . solver . objective . direction , sloppy = False ) if not _valid_atoms ( model , value . expression ) : value = interface . Objective . clone ( value , model = model . solver ) if not additive : model . solver . objective = value else : model . solver . objective += value . expression else : raise TypeError ( '%r is not a valid objective for %r.' % ( value , model . solver ) ) context = get_context ( model ) if context : def reset ( ) : model . solver . objective = reverse_value model . solver . objective . direction = reverse_value . direction context ( reset )
847	def remapCategories ( self , mapping ) : categoryArray = numpy . array ( self . _categoryList ) newCategoryArray = numpy . zeros ( categoryArray . shape [ 0 ] ) newCategoryArray . fill ( - 1 ) for i in xrange ( len ( mapping ) ) : newCategoryArray [ categoryArray == i ] = mapping [ i ] self . _categoryList = list ( newCategoryArray )
1286	def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops
9100	def write_bel_namespace_mappings ( self , file : TextIO , ** kwargs ) -> None : json . dump ( self . _get_namespace_identifier_to_name ( ** kwargs ) , file , indent = 2 , sort_keys = True )
8401	def rescale_mid ( x , to = ( 0 , 1 ) , _from = None , mid = 0 ) : array_like = True try : len ( x ) except TypeError : array_like = False x = [ x ] if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if _from is None : _from = np . array ( [ np . min ( x ) , np . max ( x ) ] ) else : _from = np . asarray ( _from ) if ( zero_range ( _from ) or zero_range ( to ) ) : out = np . repeat ( np . mean ( to ) , len ( x ) ) else : extent = 2 * np . max ( np . abs ( _from - mid ) ) out = ( x - mid ) / extent * np . diff ( to ) + np . mean ( to ) if not array_like : out = out [ 0 ] return out
5160	def render ( self ) : template_name = '{0}.jinja2' . format ( self . get_name ( ) ) template = self . template_env . get_template ( template_name ) context = getattr ( self . backend , 'intermediate_data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
5085	def has_implicit_access_to_dashboard ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_DASHBOARD_ADMIN_ROLE )
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
12272	def int2fin_reference ( n ) : checksum = 10 - ( sum ( [ int ( c ) * i for c , i in zip ( str ( n ) [ : : - 1 ] , it . cycle ( ( 7 , 3 , 1 ) ) ) ] ) % 10 ) if checksum == 10 : checksum = 0 return "%s%s" % ( n , checksum )
8179	def clear ( self ) : dict . clear ( self ) self . nodes = [ ] self . edges = [ ] self . root = None self . layout . i = 0 self . alpha = 0
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
6871	def estimate_achievable_tmid_precision ( snr , t_ingress_min = 10 , t_duration_hr = 2.14 ) : t_ingress = t_ingress_min * u . minute t_duration = t_duration_hr * u . hour theta = t_ingress / t_duration sigma_tc = ( 1 / snr * t_duration * np . sqrt ( theta / 2 ) ) LOGINFO ( 'assuming t_ingress = {:.1f}' . format ( t_ingress ) ) LOGINFO ( 'assuming t_duration = {:.1f}' . format ( t_duration ) ) LOGINFO ( 'measured SNR={:.2f}\n\t' . format ( snr ) + ' . format ( sigma_tc . to ( u . minute ) , sigma_tc . to ( u . hour ) , sigma_tc . to ( u . day ) ) ) return sigma_tc . to ( u . day ) . value
4158	def ma ( X , Q , M ) : if Q <= 0 or Q >= M : raise ValueError ( 'Q(MA) must be in ]0,lag[' ) a , rho , _c = yulewalker . aryule ( X , M , 'biased' ) a = np . insert ( a , 0 , 1 ) ma_params , _p , _c = yulewalker . aryule ( a , Q , 'biased' ) return ma_params , rho
28	def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return "child" if os . getenv ( "IN_MPI" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = "1" , OMP_NUM_THREADS = "1" , IN_MPI = "1" ) args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return "parent" else : install_mpi_excepthook ( ) return "child"
4692	def env ( ) : ipmi = cij . env_to_dict ( PREFIX , REQUIRED ) if ipmi is None : ipmi [ "USER" ] = "admin" ipmi [ "PASS" ] = "admin" ipmi [ "HOST" ] = "localhost" ipmi [ "PORT" ] = "623" cij . info ( "ipmi.env: USER: %s, PASS: %s, HOST: %s, PORT: %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] ) ) cij . env_export ( PREFIX , EXPORTED , ipmi ) return 0
1798	def CMOVG ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == 0 , cpu . SF == cpu . OF ) , src . read ( ) , dest . read ( ) ) )
1352	def make_success_response ( self , result ) : response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) response [ constants . RESPONSE_KEY_RESULT ] = result return response
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
5894	def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
5002	def _assign_enterprise_role_to_users ( self , _get_batch_method , options , is_feature_role = False ) : role_name = options [ 'role' ] batch_limit = options [ 'batch_limit' ] batch_sleep = options [ 'batch_sleep' ] batch_offset = options [ 'batch_offset' ] current_batch_index = batch_offset users_batch = _get_batch_method ( batch_offset , batch_offset + batch_limit ) role_class = SystemWideEnterpriseRole role_assignment_class = SystemWideEnterpriseUserRoleAssignment if is_feature_role : role_class = EnterpriseFeatureRole role_assignment_class = EnterpriseFeatureUserRoleAssignment enterprise_role = role_class . objects . get ( name = role_name ) while users_batch . count ( ) > 0 : for index , user in enumerate ( users_batch ) : LOGGER . info ( 'Processing user with index %s and id %s' , current_batch_index + index , user . id ) role_assignment_class . objects . get_or_create ( user = user , role = enterprise_role ) sleep ( batch_sleep ) current_batch_index += len ( users_batch ) users_batch = _get_batch_method ( current_batch_index , current_batch_index + batch_limit )
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
10887	def _format_vector ( self , vecs , form = 'broadcast' ) : if form == 'meshed' : return np . meshgrid ( * vecs , indexing = 'ij' ) elif form == 'vector' : vecs = np . meshgrid ( * vecs , indexing = 'ij' ) return np . rollaxis ( np . array ( np . broadcast_arrays ( * vecs ) ) , 0 , self . dim + 1 ) elif form == 'flat' : return vecs else : return [ v [ self . _coord_slicers [ i ] ] for i , v in enumerate ( vecs ) ]
5230	def to_hour ( num ) -> str : to_str = str ( int ( num ) ) return pd . Timestamp ( f'{to_str[:-2]}:{to_str[-2:]}' ) . strftime ( '%H:%M' )
13467	def set_Courant_Snyder ( self , beta , alpha , emit = None , emit_n = None ) : self . _store_emit ( emit = emit , emit_n = emit_n ) self . _sx = _np . sqrt ( beta * self . emit ) self . _sxp = _np . sqrt ( ( 1 + alpha ** 2 ) / beta * self . emit ) self . _sxxp = - alpha * self . emit
11789	def sample ( self ) : "Return a random sample from the distribution." if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
3088	def _delete_entity ( self ) : if self . _is_ndb ( ) : _NDB_KEY ( self . _model , self . _key_name ) . delete ( ) else : entity_key = db . Key . from_path ( self . _model . kind ( ) , self . _key_name ) db . delete ( entity_key )
3324	def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
3016	def from_json_keyfile_name ( cls , filename , scopes = '' , token_uri = None , revoke_uri = None ) : with open ( filename , 'r' ) as file_obj : client_credentials = json . load ( file_obj ) return cls . _from_parsed_json_keyfile ( client_credentials , scopes , token_uri = token_uri , revoke_uri = revoke_uri )
13323	def format_objects ( objects , children = False , columns = None , header = True ) : columns = columns or ( 'NAME' , 'TYPE' , 'PATH' ) objects = sorted ( objects , key = _type_and_name ) data = [ ] for obj in objects : if isinstance ( obj , cpenv . VirtualEnvironment ) : data . append ( get_info ( obj ) ) modules = obj . get_modules ( ) if children and modules : for mod in modules : data . append ( get_info ( mod , indent = 2 , root = obj . path ) ) else : data . append ( get_info ( obj ) ) maxes = [ len ( max ( col , key = len ) ) for col in zip ( * data ) ] tmpl = '{:%d} {:%d} {:%d}' % tuple ( maxes ) lines = [ ] if header : lines . append ( '\n' + bold_blue ( tmpl . format ( * columns ) ) ) for obj_data in data : lines . append ( tmpl . format ( * obj_data ) ) return '\n' . join ( lines )
6847	def purge_keys ( self ) : r = self . local_renderer r . env . default_ip = self . hostname_to_ip ( self . env . default_hostname ) r . env . home_dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {host_string}' ) if self . env . default_hostname : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_hostname}' ) if r . env . default_ip : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_ip}' )
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
9412	def _is_simple_numeric ( data ) : for item in data : if isinstance ( item , set ) : item = list ( item ) if isinstance ( item , list ) : if not _is_simple_numeric ( item ) : return False elif not isinstance ( item , ( int , float , complex ) ) : return False return True
1334	def backward ( self , gradient , image = None , strict = True ) : assert self . has_gradient ( ) assert gradient . ndim == 1 if image is None : image = self . __original_image assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . backward ( gradient , image ) assert gradient . shape == image . shape return gradient
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
13561	def launch ( title , items , selected = None ) : resp = { "code" : - 1 , "done" : False } curses . wrapper ( Menu , title , items , selected , resp ) return resp
7647	def scaper_to_tag ( annotation ) : annotation . namespace = 'tag_open' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = obs . value [ 'label' ] ) return annotation
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
7320	def make_message_multipart ( message ) : if not message . is_multipart ( ) : multipart_message = email . mime . multipart . MIMEMultipart ( 'alternative' ) for header_key in set ( message . keys ( ) ) : values = message . get_all ( header_key , failobj = [ ] ) for value in values : multipart_message [ header_key ] = value original_text = message . get_payload ( ) multipart_message . attach ( email . mime . text . MIMEText ( original_text ) ) message = multipart_message message = _create_boundary ( message ) return message
2295	def predict_proba ( self , a , b , ** kwargs ) : estimators = { 'entropy' : lambda x , y : eval_entropy ( y ) - eval_entropy ( x ) , 'integral' : integral_approx_estimator } ref_measures = { 'gaussian' : lambda x : standard_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'uniform' : lambda x : min_max_scale . fit_transform ( x . reshape ( ( - 1 , 1 ) ) ) , 'None' : lambda x : x } ref_measure = ref_measures [ kwargs . get ( 'refMeasure' , 'gaussian' ) ] estimator = estimators [ kwargs . get ( 'estimator' , 'entropy' ) ] a = ref_measure ( a ) b = ref_measure ( b ) return estimator ( a , b )
12315	def _run_generic_command ( self , repo , cmd ) : result = None with cd ( repo . rootdir ) : output = self . _run ( cmd ) try : result = { 'cmd' : cmd , 'status' : 'success' , 'message' : output , } except Exception as e : result = { 'cmd' : cmd , 'status' : 'error' , 'message' : str ( e ) } return result
11102	def make_zip_archive ( self , dst = None , filters = all_true , compress = True , overwrite = False , makedirs = False , verbose = False ) : self . assert_exists ( ) if dst is None : dst = self . _auto_zip_archive_dst ( ) else : dst = self . change ( new_abspath = dst ) if not dst . basename . lower ( ) . endswith ( ".zip" ) : raise ValueError ( "zip archive name has to be endswith '.zip'!" ) if dst . exists ( ) : if not overwrite : raise IOError ( "'%s' already exists!" % dst ) if compress : compression = ZIP_DEFLATED else : compression = ZIP_STORED if not dst . parent . exists ( ) : if makedirs : os . makedirs ( dst . parent . abspath ) if verbose : msg = "Making zip archive for '%s' ..." % self print ( msg ) current_dir = os . getcwd ( ) if self . is_dir ( ) : total_size = 0 selected = list ( ) for p in self . glob ( "**/*" ) : if filters ( p ) : selected . append ( p ) total_size += p . size if verbose : msg = "Got {} files, total size is {}, compressing ..." . format ( len ( selected ) , repr_data_size ( total_size ) , ) print ( msg ) with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . abspath ) for p in selected : relpath = p . relative_to ( self ) . __str__ ( ) f . write ( relpath ) elif self . is_file ( ) : with ZipFile ( dst . abspath , "w" , compression ) as f : os . chdir ( self . parent . abspath ) f . write ( self . basename ) os . chdir ( current_dir ) if verbose : msg = "Complete! Archive size is {}." . format ( dst . size_in_text ) print ( msg )
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
11993	def set_encryption_passphrases ( self , encryption_passphrases ) : self . encryption_passphrases = self . _update_dict ( encryption_passphrases , { } , replace_data = True )
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
8277	def fseq ( self , client , message ) : client . last_frame = client . current_frame client . current_frame = message [ 3 ]
7853	def identity_is ( self , item_category , item_type = None ) : if not item_category : raise ValueError ( "bad category" ) if not item_type : type_expr = u"" elif '"' not in item_type : type_expr = u' and @type="%s"' % ( item_type , ) elif "'" not in type : type_expr = u" and @type='%s'" % ( item_type , ) else : raise ValueError ( "Invalid type name" ) if '"' not in item_category : expr = u'd:identity[@category="%s"%s]' % ( item_category , type_expr ) elif "'" not in item_category : expr = u"d:identity[@category='%s'%s]" % ( item_category , type_expr ) else : raise ValueError ( "Invalid category name" ) l = self . xpath_ctxt . xpathEval ( to_utf8 ( expr ) ) if l : return True else : return False
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
865	def _readConfigFile ( cls , filename , path = None ) : outputProperties = dict ( ) if path is None : filePath = cls . findConfigFile ( filename ) else : filePath = os . path . join ( path , filename ) try : if filePath is not None : try : _getLoggerBase ( ) . debug ( "Loading config file: %s" , filePath ) with open ( filePath , 'r' ) as inp : contents = inp . read ( ) except Exception : raise RuntimeError ( "Expected configuration file at %s" % filePath ) else : try : contents = resource_string ( "nupic.support" , filename ) except Exception as resourceException : if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : contents = '<configuration/>' else : raise resourceException elements = ElementTree . XML ( contents ) if elements . tag != 'configuration' : raise RuntimeError ( "Expected top-level element to be 'configuration' " "but got '%s'" % ( elements . tag ) ) propertyElements = elements . findall ( './property' ) for propertyItem in propertyElements : propInfo = dict ( ) propertyAttributes = list ( propertyItem ) for propertyAttribute in propertyAttributes : propInfo [ propertyAttribute . tag ] = propertyAttribute . text name = propInfo . get ( 'name' , None ) if 'value' in propInfo and propInfo [ 'value' ] is None : value = '' else : value = propInfo . get ( 'value' , None ) if value is None : if 'novalue' in propInfo : continue else : raise RuntimeError ( "Missing 'value' element within the property " "element: => %s " % ( str ( propInfo ) ) ) restOfValue = value value = '' while True : pos = restOfValue . find ( '${env.' ) if pos == - 1 : value += restOfValue break value += restOfValue [ 0 : pos ] varTailPos = restOfValue . find ( '}' , pos ) if varTailPos == - 1 : raise RuntimeError ( "Trailing environment variable tag delimiter '}'" " not found in %r" % ( restOfValue ) ) varname = restOfValue [ pos + 6 : varTailPos ] if varname not in os . environ : raise RuntimeError ( "Attempting to use the value of the environment" " variable %r, which is not defined" % ( varname ) ) envVarValue = os . environ [ varname ] value += envVarValue restOfValue = restOfValue [ varTailPos + 1 : ] if name is None : raise RuntimeError ( "Missing 'name' element within following property " "element:\n => %s " % ( str ( propInfo ) ) ) propInfo [ 'value' ] = value outputProperties [ name ] = propInfo return outputProperties except Exception : _getLoggerBase ( ) . exception ( "Error while parsing configuration file: %s." , filePath ) raise
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
7969	def _remove_io_handler ( self , handler ) : if handler not in self . io_handlers : return self . io_handlers . remove ( handler ) for thread in self . io_threads : if thread . io_handler is handler : thread . stop ( )
10632	def get_compound_mfr ( self , compound ) : if compound in self . material . compounds : return self . _compound_mfrs [ self . material . get_compound_index ( compound ) ] else : return 0.0
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
4622	def _new_masterpassword ( self , password ) : if self . config_key in self . config and self . config [ self . config_key ] : raise Exception ( "Storage already has a masterpassword!" ) self . decrypted_master = hexlify ( os . urandom ( 32 ) ) . decode ( "ascii" ) self . password = password self . _save_encrypted_masterpassword ( ) return self . masterkey
6641	def hasDependencyRecursively ( self , name , target = None , test_dependencies = False ) : dependencies = self . getDependenciesRecursive ( target = target , test = test_dependencies ) return ( name in dependencies )
4954	def _disconnect_user_post_save_for_migrations ( self , sender , ** kwargs ) : from django . db . models . signals import post_save post_save . disconnect ( sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID )
4637	def claim ( self , account = None , ** kwargs ) : if not account : if "default_account" in self . blockchain . config : account = self . blockchain . config [ "default_account" ] if not account : raise ValueError ( "You need to provide an account" ) account = self . account_class ( account , blockchain_instance = self . blockchain ) pubkeys = self . blockchain . wallet . getPublicKeys ( ) addresses = dict ( ) for p in pubkeys : if p [ : len ( self . blockchain . prefix ) ] != self . blockchain . prefix : continue pubkey = self . publickey_class ( p , prefix = self . blockchain . prefix ) addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey if self [ "owner" ] not in addresses . keys ( ) : raise MissingKeyError ( "Need key for address {}" . format ( self [ "owner" ] ) ) op = self . operations . Balance_claim ( ** { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "deposit_to_account" : account [ "id" ] , "balance_to_claim" : self [ "id" ] , "balance_owner_key" : addresses [ self [ "owner" ] ] , "total_claimed" : self [ "balance" ] , "prefix" : self . blockchain . prefix , } ) signers = [ account [ "name" ] , addresses . get ( self [ "owner" ] ) , ] return self . blockchain . finalizeOp ( op , signers , "active" , ** kwargs )
5790	def peek_openssl_error ( ) : error = libcrypto . ERR_peek_error ( ) lib = int ( ( error >> 24 ) & 0xff ) func = int ( ( error >> 12 ) & 0xfff ) reason = int ( error & 0xfff ) return ( lib , func , reason )
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
9110	def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
7440	def get_params ( self , param = "" ) : fullcurdir = os . path . realpath ( os . path . curdir ) if not param : for index , ( key , value ) in enumerate ( self . paramsdict . items ( ) ) : if isinstance ( value , str ) : value = value . replace ( fullcurdir + "/" , "./" ) sys . stdout . write ( "{}{:<4}{:<28}{:<45}\n" . format ( self . _spacer , index , key , value ) ) else : try : if int ( param ) : return self . paramsdict . values ( ) [ int ( param ) ] except ( ValueError , TypeError , NameError , IndexError ) : try : return self . paramsdict [ param ] except KeyError : return 'key not recognized'
466	def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : if batch_size % num_skips != 0 : raise Exception ( "batch_size should be able to be divided by num_skips." ) if num_skips > 2 * skip_window : raise Exception ( "num_skips <= 2 * skip_window" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index
9797	def delete ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not click . confirm ( "Are sure you want to delete experiment group `{}`" . format ( _group ) ) : click . echo ( 'Existing without deleting experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . delete_experiment_group ( user , project_name , _group ) GroupManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment group `{}` was delete successfully" . format ( _group ) )
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
1983	def save_value ( self , key , value ) : with self . save_stream ( key ) as s : s . write ( value )
11804	def assign ( self , var , val , assignment ) : "Assign var, and keep track of conflicts." oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
8544	def get_datacenter ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
4485	def write_to ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) response = self . _get ( self . _download_url , stream = True ) if response . status_code == 200 : response . raw . decode_content = True copyfileobj ( response . raw , fp , int ( response . headers [ 'Content-Length' ] ) ) else : raise RuntimeError ( "Response has status " "code {}." . format ( response . status_code ) )
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
12591	def get_reliabledictionary_schema ( client , application_name , service_name , dictionary_name , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) result = json . dumps ( dictionary . get_information ( ) , indent = 4 ) if ( output_file == None ) : output_file = "{}-{}-{}-schema-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( 'Printed schema information to: ' + output_file ) print ( result )
7241	def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : return self else : return self [ g ]
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
11628	def clear_sent_messages ( self , offset = None ) : if offset is None : offset = getattr ( settings , 'MAILQUEUE_CLEAR_OFFSET' , defaults . MAILQUEUE_CLEAR_OFFSET ) if type ( offset ) is int : offset = datetime . timedelta ( hours = offset ) delete_before = timezone . now ( ) - offset self . filter ( sent = True , last_attempt__lte = delete_before ) . delete ( )
7831	def add_field ( self , name = None , values = None , field_type = None , label = None , options = None , required = False , desc = None , value = None ) : field = Field ( name , values , field_type , label , options , required , desc , value ) self . fields . append ( field ) return field
10207	def file_download_event_builder ( event , sender_app , obj = None , ** kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , bucket_id = str ( obj . bucket_id ) , file_id = str ( obj . file_id ) , file_key = obj . key , size = obj . file . size , referrer = request . referrer , ** get_user ( ) ) ) return event
11742	def _compute_follow ( self ) : self . _follow [ self . start_symbol ] . add ( END_OF_INPUT ) while True : changed = False for nonterminal , productions in self . nonterminals . items ( ) : for production in productions : for i , symbol in enumerate ( production . rhs ) : if symbol not in self . nonterminals : continue first = self . first ( production . rhs [ i + 1 : ] ) new_follow = first - set ( [ EPSILON ] ) if EPSILON in first or i == ( len ( production . rhs ) - 1 ) : new_follow |= self . _follow [ nonterminal ] if new_follow - self . _follow [ symbol ] : self . _follow [ symbol ] |= new_follow changed = True if not changed : break
2678	def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
12651	def where_is ( strings , pattern , n = 1 , lookup_func = re . match ) : count = 0 for idx , item in enumerate ( strings ) : if lookup_func ( pattern , item ) : count += 1 if count == n : return idx return - 1
13327	def remove ( name_or_path ) : click . echo ( ) try : r = cpenv . resolve ( name_or_path ) except cpenv . ResolveError as e : click . echo ( e ) return obj = r . resolved [ 0 ] if not isinstance ( obj , cpenv . VirtualEnvironment ) : click . echo ( '{} is a module. Use `cpenv module remove` instead.' ) return click . echo ( format_objects ( [ obj ] ) ) click . echo ( ) user_confirmed = click . confirm ( red ( 'Are you sure you want to remove this environment?' ) ) if user_confirmed : click . echo ( 'Attempting to remove...' , nl = False ) try : obj . remove ( ) except Exception as e : click . echo ( bold_red ( 'FAIL' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'OK!' ) )
7552	def _getbins ( ) : if not _sys . maxsize > 2 ** 32 : _sys . exit ( "ipyrad requires 64bit architecture" ) _platform = _sys . platform if 'VIRTUAL_ENV' in _os . environ : ipyrad_path = _os . environ [ 'VIRTUAL_ENV' ] else : path = _os . path . abspath ( _os . path . dirname ( __file__ ) ) ipyrad_path = _os . path . dirname ( path ) ipyrad_path = _os . path . dirname ( path ) bin_path = _os . path . join ( ipyrad_path , "bin" ) if 'linux' in _platform : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-linux-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-linux-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-linux-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-linux-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-linux-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-linux-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-linux-x86_64" ) else : vsearch = _os . path . join ( _os . path . abspath ( bin_path ) , "vsearch-osx-x86_64" ) muscle = _os . path . join ( _os . path . abspath ( bin_path ) , "muscle-osx-x86_64" ) smalt = _os . path . join ( _os . path . abspath ( bin_path ) , "smalt-osx-x86_64" ) bwa = _os . path . join ( _os . path . abspath ( bin_path ) , "bwa-osx-x86_64" ) samtools = _os . path . join ( _os . path . abspath ( bin_path ) , "samtools-osx-x86_64" ) bedtools = _os . path . join ( _os . path . abspath ( bin_path ) , "bedtools-osx-x86_64" ) qmc = _os . path . join ( _os . path . abspath ( bin_path ) , "QMC-osx-x86_64" ) assert _cmd_exists ( muscle ) , "muscle not found here: " + muscle assert _cmd_exists ( vsearch ) , "vsearch not found here: " + vsearch assert _cmd_exists ( smalt ) , "smalt not found here: " + smalt assert _cmd_exists ( bwa ) , "bwa not found here: " + bwa assert _cmd_exists ( samtools ) , "samtools not found here: " + samtools assert _cmd_exists ( bedtools ) , "bedtools not found here: " + bedtools return vsearch , muscle , smalt , bwa , samtools , bedtools , qmc
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
293	def plot_gross_leverage ( returns , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) gl = timeseries . gross_lev ( positions ) gl . plot ( lw = 0.5 , color = 'limegreen' , legend = False , ax = ax , ** kwargs ) ax . axhline ( gl . mean ( ) , color = 'g' , linestyle = '--' , lw = 3 ) ax . set_title ( 'Gross leverage' ) ax . set_ylabel ( 'Gross leverage' ) ax . set_xlabel ( '' ) return ax
6599	def key_vals_dict_to_tuple_list ( key_vals_dict , fill = float ( 'nan' ) ) : tuple_list = [ ] if not key_vals_dict : return tuple_list vlen = max ( [ len ( vs ) for vs in itertools . chain ( * key_vals_dict . values ( ) ) ] ) for k , vs in key_vals_dict . items ( ) : try : tuple_list . extend ( [ k + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) except TypeError : tuple_list . extend ( [ ( k , ) + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) return tuple_list
10441	def getmemorystat ( self , process_name ) : _stat_inst = ProcessStats ( process_name ) _stat_list = [ ] for p in _stat_inst . get_cpu_memory_stat ( ) : try : _stat_list . append ( round ( p . get_memory_percent ( ) , 2 ) ) except psutil . AccessDenied : pass return _stat_list
656	def averageOnTimePerTimestep ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is not None : import pdb pdb . set_trace ( ) countOn = numpy . random . randint ( 0 , numElements , numSamples ) vectors = vectors [ : , countOn ] durations = numpy . zeros ( vectors . shape , dtype = 'int32' ) for col in xrange ( vectors . shape [ 1 ] ) : _fillInOnTimes ( vectors [ : , col ] , durations [ : , col ] ) sums = vectors . sum ( axis = 1 ) sums . clip ( min = 1 , max = numpy . inf , out = sums ) avgDurations = durations . sum ( axis = 1 , dtype = 'float64' ) / sums avgOnTime = avgDurations . sum ( ) / ( avgDurations > 0 ) . sum ( ) freqCounts = _accumulateFrequencyCounts ( avgDurations ) return ( avgOnTime , freqCounts )
10852	def otsu_threshold ( data , bins = 255 ) : h0 , x0 = np . histogram ( data . ravel ( ) , bins = bins ) h = h0 . astype ( 'float' ) / h0 . sum ( ) x = 0.5 * ( x0 [ 1 : ] + x0 [ : - 1 ] ) wk = np . array ( [ h [ : i + 1 ] . sum ( ) for i in range ( h . size ) ] ) mk = np . array ( [ sum ( x [ : i + 1 ] * h [ : i + 1 ] ) for i in range ( h . size ) ] ) mt = mk [ - 1 ] sb = ( mt * wk - mk ) ** 2 / ( wk * ( 1 - wk ) + 1e-15 ) ind = sb . argmax ( ) return 0.5 * ( x0 [ ind ] + x0 [ ind + 1 ] )
10102	def _make_file_dict ( self , f ) : if isinstance ( f , dict ) : file_obj = f [ 'file' ] if 'filename' in f : file_name = f [ 'filename' ] else : file_name = file_obj . name else : file_obj = f file_name = f . name b64_data = base64 . b64encode ( file_obj . read ( ) ) return { 'id' : file_name , 'data' : b64_data . decode ( ) if six . PY3 else b64_data , }
869	def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s while trying to remove dynamic " "configuration file: %s" , e . errno , cls . getPath ( ) ) raise cls . _path = None
12423	def load ( fp , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : converter = None output = cls ( ) arraykeys = set ( ) for line in fp : if converter is None : if isinstance ( line , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator key , value = line . strip ( ) . split ( separator , 1 ) keyparts = key . split ( index_separator ) try : index = int ( keyparts [ - 1 ] ) endwithint = True except ValueError : endwithint = False if len ( keyparts ) > 1 and endwithint : basekey = key . rsplit ( index_separator , 1 ) [ 0 ] if basekey not in arraykeys : arraykeys . add ( basekey ) if basekey in output : if not isinstance ( output [ basekey ] , dict ) : output [ basekey ] = { - 1 : output [ basekey ] } else : output [ basekey ] = { } output [ basekey ] [ index ] = value else : if key in output and isinstance ( output [ key ] , dict ) : output [ key ] [ - 1 ] = value else : output [ key ] = value for key in arraykeys : output [ key ] = list_cls ( pair [ 1 ] for pair in sorted ( six . iteritems ( output [ key ] ) ) ) return output
6262	def resize ( self , width , height ) : self . width = width self . height = height self . buffer_width , self . buffer_height = glfw . get_framebuffer_size ( self . window ) self . set_default_viewport ( )
8045	def parse_docstring ( self ) : self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value ) while self . current . kind in ( tk . COMMENT , tk . NEWLINE , tk . NL ) : self . stream . move ( ) self . log . debug ( "parsing docstring, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . STRING : docstring = self . current . value self . stream . move ( ) return docstring return None
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
4455	def apply ( self , ** kwexpr ) : for alias , expr in kwexpr . items ( ) : self . _projections . append ( [ alias , expr ] ) return self
13658	def _addRoute ( self , f , matcher ) : self . _routes . append ( ( f . func_name , f , matcher ) )
4547	def bresenham_line ( setter , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : steep = abs ( y1 - y0 ) > abs ( x1 - x0 ) if steep : x0 , y0 = y0 , x0 x1 , y1 = y1 , x1 if x0 > x1 : x0 , x1 = x1 , x0 y0 , y1 = y1 , y0 dx = x1 - x0 dy = abs ( y1 - y0 ) err = dx / 2 if y0 < y1 : ystep = 1 else : ystep = - 1 count = 0 for x in range ( x0 , x1 + 1 ) : if colorFunc : color = colorFunc ( count ) count += 1 if steep : setter ( y0 , x , color ) else : setter ( x , y0 , color ) err -= dy if err < 0 : y0 += ystep err += dx
10646	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return np . polyval ( self . _coeffs , state [ 'T' ] )
1188	def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
13066	def make_members ( self , collection , lang = None ) : objects = sorted ( [ self . expose_ancestors_or_children ( member , collection , lang = lang ) for member in collection . members if member . get_label ( ) ] , key = itemgetter ( "label" ) ) return objects
8305	def live_source_load ( self , source ) : source = source . rstrip ( '\n' ) if source != self . source : self . source = source b64_source = base64 . b64encode ( bytes ( bytearray ( source , "ascii" ) ) ) self . send_command ( CMD_LOAD_BASE64 , b64_source )
10836	def filter ( self , ** kwargs ) : if not len ( self ) : self . all ( ) new_list = filter ( lambda item : [ True for arg in kwargs if item [ arg ] == kwargs [ arg ] ] != [ ] , self ) return Profiles ( self . api , new_list )
92	def _quokka_normalize_extract ( extract ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == "square" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( "Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage " + "for parameter 'extract', got %s." % ( type ( extract ) , ) ) return bb
1537	def add_spec ( self , * specs ) : for spec in specs : if not isinstance ( spec , HeronComponentSpec ) : raise TypeError ( "Argument to add_spec needs to be HeronComponentSpec, given: %s" % str ( spec ) ) if spec . name is None : raise ValueError ( "TopologyBuilder cannot take a spec without name" ) if spec . name == "config" : raise ValueError ( "config is a reserved name" ) if spec . name in self . _specs : raise ValueError ( "Attempting to add duplicate spec name: %r %r" % ( spec . name , spec ) ) self . _specs [ spec . name ] = spec
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
4087	def asyncClose ( fn ) : @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : f = asyncio . ensure_future ( fn ( * args , ** kwargs ) ) while not f . done ( ) : QApplication . instance ( ) . processEvents ( ) return wrapper
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
13524	def safe_joinall ( greenlets , timeout = None , raise_error = False ) : greenlets = list ( greenlets ) try : gevent . joinall ( greenlets , timeout = timeout , raise_error = raise_error ) except gevent . GreenletExit : [ greenlet . kill ( ) for greenlet in greenlets if not greenlet . ready ( ) ] raise return greenlets
4618	def parse_time ( block_time ) : return datetime . strptime ( block_time , timeFormat ) . replace ( tzinfo = timezone . utc )
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
9394	def plot_cdf ( self , graphing_library = 'matplotlib' ) : graphed = False for percentile_csv in self . percentiles_files : csv_filename = os . path . basename ( percentile_csv ) column = self . csv_column_map [ percentile_csv . replace ( ".percentiles." , "." ) ] if not self . check_important_sub_metrics ( column ) : continue column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] else : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
2142	def process_extra_vars ( extra_vars_list , force_json = True ) : extra_vars = { } extra_vars_yaml = "" for extra_vars_opt in extra_vars_list : if extra_vars_opt . startswith ( "@" ) : with open ( extra_vars_opt [ 1 : ] , 'r' ) as f : extra_vars_opt = f . read ( ) opt_dict = string_to_dict ( extra_vars_opt , allow_kv = False ) else : opt_dict = string_to_dict ( extra_vars_opt , allow_kv = True ) if any ( line . startswith ( "#" ) for line in extra_vars_opt . split ( '\n' ) ) : extra_vars_yaml += extra_vars_opt + "\n" elif extra_vars_opt != "" : extra_vars_yaml += yaml . dump ( opt_dict , default_flow_style = False ) + "\n" extra_vars . update ( opt_dict ) if not force_json : try : try_dict = yaml . load ( extra_vars_yaml , Loader = yaml . SafeLoader ) assert type ( try_dict ) is dict debug . log ( 'Using unprocessed YAML' , header = 'decision' , nl = 2 ) return extra_vars_yaml . rstrip ( ) except Exception : debug . log ( 'Failed YAML parsing, defaulting to JSON' , header = 'decison' , nl = 2 ) if extra_vars == { } : return "" return json . dumps ( extra_vars , ensure_ascii = False )
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( ** { keyword : value } ) and_q &= or_q except AttributeError : and_q &= Q ( ** { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
5914	def _process_command ( self , command , name = None ) : self . _command_counter += 1 if name is None : name = "CMD{0:03d}" . format ( self . _command_counter ) try : fd , tmp_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'tmp_' + name + '__' ) cmd = [ command , '' , 'q' ] rc , out , err = self . make_ndx ( o = tmp_ndx , input = cmd ) self . check_output ( out , "No atoms found for selection {command!r}." . format ( ** vars ( ) ) , err = err ) groups = parse_ndxlist ( out ) last = groups [ - 1 ] fd , ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = name + '__' ) name_cmd = [ "keep {0:d}" . format ( last [ 'nr' ] ) , "name 0 {0!s}" . format ( name ) , 'q' ] rc , out , err = self . make_ndx ( n = tmp_ndx , o = ndx , input = name_cmd ) finally : utilities . unlink_gmx ( tmp_ndx ) return name , ndx
10922	def do_levmarq_n_directions ( s , directions , max_iter = 2 , run_length = 2 , damping = 1e-3 , collect_stats = False , marquardt_damping = True , ** kwargs ) : normals = np . array ( [ d / np . sqrt ( np . dot ( d , d ) ) for d in directions ] ) if np . isnan ( normals ) . any ( ) : raise ValueError ( '`directions` must not be 0s or contain nan' ) obj = OptState ( s , normals ) lo = LMOptObj ( obj , max_iter = max_iter , run_length = run_length , damping = damping , marquardt_damping = marquardt_damping , ** kwargs ) lo . do_run_1 ( ) if collect_stats : return lo . get_termination_stats ( )
10492	def doubleClickDragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord , clickCount = 2 ) self . _postQueuedEvents ( interval = interval )
7867	def expire ( self ) : with self . _lock : logger . debug ( "expdict.expire. timeouts: {0!r}" . format ( self . _timeouts ) ) next_timeout = None for k in self . _timeouts . keys ( ) : ret = self . _expire_item ( k ) if ret is not None : if next_timeout is None : next_timeout = ret else : next_timeout = min ( next_timeout , ret ) return next_timeout
9491	def _get_name_info ( name_index , name_list ) : argval = name_index if name_list is not None : try : argval = name_list [ name_index ] except IndexError : raise ValidationError ( "Names value out of range: {}" . format ( name_index ) ) from None argrepr = argval else : argrepr = repr ( argval ) return argval , argrepr
2879	def serialize_value_list ( self , list_elem , thelist ) : for value in thelist : value_elem = SubElement ( list_elem , 'value' ) self . serialize_value ( value_elem , value ) return list_elem
13322	def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
11718	def delete ( self ) : headers = self . _default_headers ( ) return self . _request ( self . name , ok_status = None , data = None , headers = headers , method = "DELETE" )
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) return self . _x10_command ( house_code , unit_number , state )
2084	def parse_args ( self , ctx , args ) : if not args and self . no_args_is_help and not ctx . resilient_parsing : click . echo ( ctx . get_help ( ) ) ctx . exit ( ) return super ( ActionSubcommand , self ) . parse_args ( ctx , args )
9723	async def save ( self , filename , overwrite = False ) : cmd = "save %s%s" % ( filename , " overwrite" if overwrite else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
10556	def update_helping_material ( helpingmaterial ) : try : helpingmaterial_id = helpingmaterial . id helpingmaterial = _forbidden_attributes ( helpingmaterial ) res = _pybossa_req ( 'put' , 'helpingmaterial' , helpingmaterial_id , payload = helpingmaterial . data ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
13518	def maximum_deck_area ( self , water_plane_coef = 0.88 ) : AD = self . beam * self . length * water_plane_coef return AD
1396	def removeTopology ( self , topology_name , state_manager_name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology_name and top . state_manager_name == state_manager_name ) : if ( topology_name , state_manager_name ) in self . topologyInfos : self . topologyInfos . pop ( ( topology_name , state_manager_name ) ) else : topologies . append ( top ) self . topologies = topologies
7046	def _bls_runner ( times , mags , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) : workarr_u = npones ( times . size ) workarr_v = npones ( times . size ) blsresult = eebls ( times , mags , workarr_u , workarr_v , nfreq , freqmin , stepsize , nbins , minduration , maxduration ) return { 'power' : blsresult [ 0 ] , 'bestperiod' : blsresult [ 1 ] , 'bestpower' : blsresult [ 2 ] , 'transdepth' : blsresult [ 3 ] , 'transduration' : blsresult [ 4 ] , 'transingressbin' : blsresult [ 5 ] , 'transegressbin' : blsresult [ 6 ] }
7967	def feed ( self , data ) : with self . lock : if self . in_use : raise StreamParseError ( "StreamReader.feed() is not reentrant!" ) self . in_use = True try : if not self . _started : if len ( data ) > 1 : self . parser . feed ( data [ : 1 ] ) data = data [ 1 : ] self . _started = True if data : self . parser . feed ( data ) else : self . parser . close ( ) except ElementTree . ParseError , err : self . handler . stream_parse_error ( unicode ( err ) ) finally : self . in_use = False
458	def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , "alphas" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( "Dtypes do not corresponds: %s and %s" % ( output . dtype . base_dtype , alpha_dtype ) ) return output
4230	def argparser ( ) : parser = ArgumentParser ( prog = 'pynetgear' ) parser . add_argument ( "--format" , choices = [ 'json' , 'prettyjson' , 'py' ] , default = 'prettyjson' ) router_args = parser . add_argument_group ( "router connection config" ) router_args . add_argument ( "--host" , help = "Hostname for the router" ) router_args . add_argument ( "--user" , help = "Account for login" ) router_args . add_argument ( "--port" , help = "Port exposed on the router" ) router_args . add_argument ( "--login-v2" , help = "Force the use of the cookie-based authentication" , dest = "force_login_v2" , default = False , action = "store_true" ) router_args . add_argument ( "--password" , help = "Not required with a wired connection." + "Optionally, set the PYNETGEAR_PASSWORD environment variable" ) router_args . add_argument ( "--url" , help = "Overrides host:port and ssl with url to router" ) router_args . add_argument ( "--no-ssl" , dest = "ssl" , default = True , action = "store_false" , help = "Connect with https" ) subparsers = parser . add_subparsers ( description = "Runs subcommand against the specified router" , dest = "subcommand" ) block_parser = subparsers . add_parser ( "block_device" , help = "Blocks a device from connecting by mac address" ) block_parser . add_argument ( "--mac-addr" ) allow_parser = subparsers . add_parser ( "allow_device" , help = "Allows a device with the mac address to connect" ) allow_parser . add_argument ( "--mac-addr" ) subparsers . add_parser ( "login" , help = "Attempts to login to router." ) attached_devices = subparsers . add_parser ( "attached_devices" , help = "Outputs all attached devices" ) attached_devices . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , help = "Choose between verbose and slower or terse and fast." ) subparsers . add_parser ( "traffic_meter" , help = "Output router's traffic meter data" ) return parser
11323	def check_pkgs_integrity ( filelist , logger , ftp_connector , timeout = 120 , sleep_time = 10 ) : ref_1 = [ ] ref_2 = [ ] i = 1 print >> sys . stdout , "\nChecking packages integrity." for filename in filelist : get_remote_file_size ( ftp_connector , filename , ref_1 ) print >> sys . stdout , "\nGoing to sleep for %i sec." % ( sleep_time , ) time . sleep ( sleep_time ) while sleep_time * i < timeout : for filename in filelist : get_remote_file_size ( ftp_connector , filename , ref_2 ) if ref_1 == ref_2 : print >> sys . stdout , "\nIntegrity OK:)" logger . info ( "Packages integrity OK." ) break else : print >> sys . stdout , "\nWaiting %d time for itegrity..." % ( i , ) logger . info ( "\nWaiting %d time for itegrity..." % ( i , ) ) i += 1 ref_1 , ref_2 = ref_2 , [ ] time . sleep ( sleep_time ) else : not_finished_files = [ ] for count , val1 in enumerate ( ref_1 ) : if val1 != ref_2 [ count ] : not_finished_files . append ( filelist [ count ] ) print >> sys . stdout , "\nOMG, OMG something wrong with integrity." logger . error ( "Integrity check faild for files %s" % ( not_finished_files , ) )
120	def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) while not self . all_finished ( ) : time . sleep ( 0.001 ) if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
10864	def _update_type ( self , params ) : dozscale = False particles = [ ] for p in listify ( params ) : typ , ind = self . _p2i ( p ) particles . append ( ind ) dozscale = dozscale or typ == 'zscale' particles = set ( particles ) return dozscale , particles
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
13062	def get_siblings ( self , objectId , subreference , passage ) : reffs = [ reff for reff , _ in self . get_reffs ( objectId ) ] if subreference in reffs : index = reffs . index ( subreference ) if 0 < index < len ( reffs ) - 1 : return reffs [ index - 1 ] , reffs [ index + 1 ] elif index == 0 and index < len ( reffs ) - 1 : return None , reffs [ 1 ] elif index > 0 and index == len ( reffs ) - 1 : return reffs [ index - 1 ] , None else : return None , None else : return passage . siblingsId
10550	def find_results ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'result' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Result ( result ) for result in res ] else : return res except : raise
3987	def _move_temp_binary_to_path ( tmp_binary_path ) : binary_path = _get_binary_location ( ) if not binary_path . endswith ( constants . DUSTY_BINARY_NAME ) : raise RuntimeError ( 'Refusing to overwrite binary {}' . format ( binary_path ) ) st = os . stat ( binary_path ) permissions = st . st_mode owner = st . st_uid group = st . st_gid shutil . move ( tmp_binary_path , binary_path ) os . chown ( binary_path , owner , group ) os . chmod ( binary_path , permissions ) return binary_path
6322	def resolve_loader ( self , meta : ProgramDescription ) : if not meta . loader : meta . loader = 'single' if meta . path else 'separate' for loader_cls in self . _loaders : if loader_cls . name == meta . loader : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( ( "Program {} has no loader class registered." "Check PROGRAM_LOADERS or PROGRAM_DIRS" ) . format ( meta . path ) )
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
11930	def deploy_blog ( ) : logger . info ( deploy_blog . __doc__ ) call ( 'rsync -aqu ' + join ( dirname ( __file__ ) , 'res' , '*' ) + ' .' , shell = True ) logger . success ( 'Done' ) logger . info ( 'Please edit config.toml to meet your needs' )
9220	def parse_args ( self , args , scope ) : arguments = list ( zip ( args , [ ' ' ] * len ( args ) ) ) if args and args [ 0 ] else None zl = itertools . zip_longest if sys . version_info [ 0 ] == 3 else itertools . izip_longest if self . args : parsed = [ v if hasattr ( v , 'parse' ) else v for v in copy . copy ( self . args ) ] args = args if isinstance ( args , list ) else [ args ] vars = [ self . _parse_arg ( var , arg , scope ) for arg , var in zl ( [ a for a in args ] , parsed ) ] for var in vars : if var : var . parse ( scope ) if not arguments : arguments = [ v . value for v in vars if v ] if not arguments : arguments = '' Variable ( [ '@arguments' , None , arguments ] ) . parse ( scope )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
12244	def styblinski_tang ( theta ) : x , y = theta obj = 0.5 * ( x ** 4 - 16 * x ** 2 + 5 * x + y ** 4 - 16 * y ** 2 + 5 * y ) grad = np . array ( [ 2 * x ** 3 - 16 * x + 2.5 , 2 * y ** 3 - 16 * y + 2.5 , ] ) return obj , grad
6886	def mdwarf_subtype_from_sdsscolor ( ri_color , iz_color ) : if np . isfinite ( ri_color ) and np . isfinite ( iz_color ) : obj_sti = 0.875274 * ri_color + 0.483628 * ( iz_color + 0.00438 ) obj_sts = - 0.483628 * ri_color + 0.875274 * ( iz_color + 0.00438 ) else : obj_sti = np . nan obj_sts = np . nan if ( np . isfinite ( obj_sti ) and np . isfinite ( obj_sts ) and ( obj_sti > 0.666 ) and ( obj_sti < 3.4559 ) ) : if ( ( obj_sti > 0.6660 ) and ( obj_sti < 0.8592 ) ) : m_class = 'M0' if ( ( obj_sti > 0.8592 ) and ( obj_sti < 1.0822 ) ) : m_class = 'M1' if ( ( obj_sti > 1.0822 ) and ( obj_sti < 1.2998 ) ) : m_class = 'M2' if ( ( obj_sti > 1.2998 ) and ( obj_sti < 1.6378 ) ) : m_class = 'M3' if ( ( obj_sti > 1.6378 ) and ( obj_sti < 2.0363 ) ) : m_class = 'M4' if ( ( obj_sti > 2.0363 ) and ( obj_sti < 2.2411 ) ) : m_class = 'M5' if ( ( obj_sti > 2.2411 ) and ( obj_sti < 2.4126 ) ) : m_class = 'M6' if ( ( obj_sti > 2.4126 ) and ( obj_sti < 2.9213 ) ) : m_class = 'M7' if ( ( obj_sti > 2.9213 ) and ( obj_sti < 3.2418 ) ) : m_class = 'M8' if ( ( obj_sti > 3.2418 ) and ( obj_sti < 3.4559 ) ) : m_class = 'M9' else : m_class = None return m_class , obj_sti , obj_sts
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) config_args = _convert_config_to_stdin ( config , parser ) return config_args
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
7812	def _decode_validity ( self , validity ) : not_after = validity . getComponentByName ( 'notAfter' ) not_after = str ( not_after . getComponent ( ) ) if isinstance ( not_after , GeneralizedTime ) : self . not_after = datetime . strptime ( not_after , "%Y%m%d%H%M%SZ" ) else : self . not_after = datetime . strptime ( not_after , "%y%m%d%H%M%SZ" ) self . alt_names = defaultdict ( list )
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
11330	def get_record ( self , record ) : self . document = record rec = create_record ( ) language = self . _get_language ( ) if language and language != 'en' : record_add_field ( rec , '041' , subfields = [ ( 'a' , language ) ] ) publisher = self . _get_publisher ( ) date = self . _get_date ( ) if publisher and date : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) , ( 'c' , date ) ] ) elif publisher : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) ] ) elif date : record_add_field ( rec , '260' , subfields = [ ( 'c' , date ) ] ) title = self . _get_title ( ) if title : record_add_field ( rec , '245' , subfields = [ ( 'a' , title ) ] ) record_copyright = self . _get_copyright ( ) if record_copyright : record_add_field ( rec , '540' , subfields = [ ( 'a' , record_copyright ) ] ) subject = self . _get_subject ( ) if subject : record_add_field ( rec , '650' , ind1 = '1' , ind2 = '7' , subfields = [ ( 'a' , subject ) , ( '2' , 'PoS' ) ] ) authors = self . _get_authors ( ) first_author = True for author in authors : subfields = [ ( 'a' , author [ 0 ] ) ] for affiliation in author [ 1 ] : subfields . append ( ( 'v' , affiliation ) ) if first_author : record_add_field ( rec , '100' , subfields = subfields ) first_author = False else : record_add_field ( rec , '700' , subfields = subfields ) identifier = self . get_identifier ( ) conference = identifier . split ( ':' ) [ 2 ] conference = conference . split ( '/' ) [ 0 ] contribution = identifier . split ( ':' ) [ 2 ] contribution = contribution . split ( '/' ) [ 1 ] record_add_field ( rec , '773' , subfields = [ ( 'p' , 'PoS' ) , ( 'v' , conference . replace ( ' ' , '' ) ) , ( 'c' , contribution ) , ( 'y' , date [ : 4 ] ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'ConferencePaper' ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'HEP' ) ] ) return rec
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
3341	def parse_xml_body ( environ , allow_empty = False ) : clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) if clHeader == "" : requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data env . update ( env_default ) added_keys = set ( env ) . difference ( env_default ) for key in added_keys : if key . startswith ( '_' ) : continue del env [ key ]
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
12867	def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
9418	def document_func_view ( serializer_class = None , response_serializer_class = None , filter_backends = None , permission_classes = None , authentication_classes = None , doc_format_args = list ( ) , doc_format_kwargs = dict ( ) ) : def decorator ( func ) : if serializer_class : func . cls . serializer_class = func . view_class . serializer_class = serializer_class if response_serializer_class : func . cls . response_serializer_class = func . view_class . response_serializer_class = response_serializer_class if filter_backends : func . cls . filter_backends = func . view_class . filter_backends = filter_backends if permission_classes : func . cls . permission_classes = func . view_class . permission_classes = permission_classes if authentication_classes : func . cls . authentication_classes = func . view_class . authentication_classes = authentication_classes if doc_format_args or doc_format_kwargs : func . cls . __doc__ = func . view_class . __doc__ = getdoc ( func ) . format ( * doc_format_args , ** doc_format_kwargs ) return func return decorator
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] pweights = 1.0 / perrs phase = phase * 2.0 * pi_value z = npcos ( phase ) + 1.0j * npsin ( phase ) phase = nharmonics * phase psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) zn = 1.0 + 0.0j phi = pweights + 0.0j theta_aov = 0.0 for _ in range ( two_nharmonics ) : phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) alpha = npsum ( pweights * z * phi ) phi_dot_psi = npvdot ( phi , psi ) phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) alpha = alpha / phi_dot_phi theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) phi = phi * z - alpha * zn * phi . conjugate ( ) zn = zn * z theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
7623	def melody ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_times , ref_p = ref . to_event_values ( ) est_times , est_p = est . to_event_values ( ) ref_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_freq = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . melody . evaluate ( ref_times , ref_freq , est_times , est_freq , ** kwargs )
4343	def silence ( self , location = 0 , silence_threshold = 0.1 , min_silence_duration = 0.1 , buffer_around_silence = False ) : if location not in [ - 1 , 0 , 1 ] : raise ValueError ( "location must be one of -1, 0, 1." ) if not is_number ( silence_threshold ) or silence_threshold < 0 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) elif silence_threshold >= 100 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) if not is_number ( min_silence_duration ) or min_silence_duration <= 0 : raise ValueError ( "min_silence_duration must be a positive number." ) if not isinstance ( buffer_around_silence , bool ) : raise ValueError ( "buffer_around_silence must be a boolean." ) effect_args = [ ] if location == - 1 : effect_args . append ( 'reverse' ) if buffer_around_silence : effect_args . extend ( [ 'silence' , '-l' ] ) else : effect_args . append ( 'silence' ) effect_args . extend ( [ '1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == 0 : effect_args . extend ( [ '-1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'silence' ) return self
9277	def parse ( packet ) : if not isinstance ( packet , string_type_parse ) : raise TypeError ( "Expected packet to be str/unicode/bytes, got %s" , type ( packet ) ) if len ( packet ) == 0 : raise ParseError ( "packet is empty" , packet ) if isinstance ( packet , bytes ) : packet = _unicode_packet ( packet ) packet = packet . rstrip ( "\r\n" ) logger . debug ( "Parsing: %s" , packet ) try : ( head , body ) = packet . split ( ':' , 1 ) except : raise ParseError ( "packet has no body" , packet ) if len ( body ) == 0 : raise ParseError ( "packet body is empty" , packet ) parsed = { 'raw' : packet , } try : parsed . update ( parse_header ( head ) ) except ParseError as msg : raise ParseError ( str ( msg ) , packet ) packet_type = body [ 0 ] body = body [ 1 : ] if len ( body ) == 0 and packet_type != '>' : raise ParseError ( "packet body is empty after packet type character" , packet ) try : _try_toparse_body ( packet_type , body , parsed ) except ( UnknownFormat , ParseError ) as exp : exp . packet = packet raise if 'format' not in parsed : if not re . match ( r"^(AIR.*|ALL.*|AP.*|BEACON|CQ.*|GPS.*|DF.*|DGPS.*|" "DRILL.*|DX.*|ID.*|JAVA.*|MAIL.*|MICE.*|QST.*|QTH.*|" "RTCM.*|SKY.*|SPACE.*|SPC.*|SYM.*|TEL.*|TEST.*|TLM.*|" "WX.*|ZIP.*|UIDIGI)$" , parsed [ 'to' ] ) : raise UnknownFormat ( "format is not supported" , packet ) parsed . update ( { 'format' : 'beacon' , 'text' : packet_type + body , } ) logger . debug ( "Parsed ok." ) return parsed
9460	def conference_unmute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUnmute/' method = 'POST' return self . request ( path , method , call_params )
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , ** savgol_kwargs ) : if errs is None : errs = 0.001 * mags finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , ** savgol_kwargs ) subtracted = fmags - smoothed series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] for ind , extrema_index in enumerate ( extrema_indices ) : pass
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
13004	def modify_input ( ) : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : objects = [ obj for obj in doc_mapper . get_pipe ( ) ] modified = modify_data ( objects ) for line in modified : obj = doc_mapper . line_to_object ( line ) obj . save ( ) print_success ( "Object(s) successfully changed" ) else : print_error ( "Please use this tool with pipes" )
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
3226	def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
5853	def get_pif ( self , dataset_id , uid , dataset_version = None ) : failure_message = "An error occurred retrieving PIF {}" . format ( uid ) if dataset_version == None : response = self . _get ( routes . pif_dataset_uid ( dataset_id , uid ) , failure_message = failure_message ) else : response = self . _get ( routes . pif_dataset_version_uid ( dataset_id , uid , dataset_version ) , failure_message = failure_message ) return pif . loads ( response . content . decode ( "utf-8" ) )
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
8977	def _file ( self , file ) : if not self . __text_is_expected : file = BytesWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
13443	def path ( self , a_hash , b_hash ) : def _path ( a , b ) : if a is b : return [ a ] else : assert len ( a . children ) == 1 return [ a ] + _path ( a . children [ 0 ] , b ) a = self . nodes [ a_hash ] b = self . nodes [ b_hash ] return _path ( a , b ) [ 1 : ]
5316	def setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : if colormode : self . colormode = colormode if colorpalette : if extend_colors : self . update_palette ( colorpalette ) else : self . colorpalette = colorpalette
1954	def symbolic_run_get_cons ( trace ) : m2 = Manticore . linux ( prog , workspace_url = 'mem:' ) f = Follower ( trace ) m2 . verbosity ( VERBOSITY ) m2 . register_plugin ( f ) def on_term_testcase ( mcore , state , stateid , err ) : with m2 . locked_context ( ) as ctx : readdata = [ ] for name , fd , data in state . platform . syscall_trace : if name in ( '_receive' , '_read' ) and fd == 0 : readdata . append ( data ) ctx [ 'readdata' ] = readdata ctx [ 'constraints' ] = list ( state . constraints . constraints ) m2 . subscribe ( 'will_terminate_state' , on_term_testcase ) m2 . run ( ) constraints = m2 . context [ 'constraints' ] datas = m2 . context [ 'readdata' ] return constraints , datas
4408	def connected_channel ( self ) : if not self . channel_id : return None return self . _lavalink . bot . get_channel ( int ( self . channel_id ) )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : r valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
7452	def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
5662	def return_segments ( shape , break_points ) : segs = [ ] bp = 0 bp2 = 0 for i in range ( len ( break_points ) - 1 ) : bp = break_points [ i ] if break_points [ i ] is not None else bp2 bp2 = break_points [ i + 1 ] if break_points [ i + 1 ] is not None else bp segs . append ( shape [ bp : bp2 + 1 ] ) segs . append ( [ ] ) return segs
9269	def version_of_first_item ( self ) : try : sections = read_changelog ( self . options ) return sections [ 0 ] [ "version" ] except ( IOError , TypeError ) : return self . get_temp_tag_for_repo_creation ( )
1056	def _slotnames ( cls ) : names = cls . __dict__ . get ( "__slotnames__" ) if names is not None : return names names = [ ] if not hasattr ( cls , "__slots__" ) : pass else : for c in cls . __mro__ : if "__slots__" in c . __dict__ : slots = c . __dict__ [ '__slots__' ] if isinstance ( slots , basestring ) : slots = ( slots , ) for name in slots : if name in ( "__dict__" , "__weakref__" ) : continue elif name . startswith ( '__' ) and not name . endswith ( '__' ) : names . append ( '_%s%s' % ( c . __name__ , name ) ) else : names . append ( name ) try : cls . __slotnames__ = names except : pass return names
12111	def save ( self , filename , metadata = { } , ** data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
7195	def ndvi ( self , ** kwargs ) : data = self . _read ( self [ self . _ndvi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 0 , : , : ] - data [ 1 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
913	def read ( cls , proto ) : instance = object . __new__ ( cls ) super ( PreviousValueModel , instance ) . __init__ ( proto = proto . modelBase ) instance . _logger = opf_utils . initLogger ( instance ) if len ( proto . predictedField ) : instance . _predictedField = proto . predictedField else : instance . _predictedField = None instance . _fieldNames = list ( proto . fieldNames ) instance . _fieldTypes = list ( proto . fieldTypes ) instance . _predictionSteps = list ( proto . predictionSteps ) return instance
9759	def statuses ( ctx , job , page ) : def get_experiment_statuses ( ) : try : response = PolyaxonClient ( ) . experiment . get_statuses ( user , project_name , _experiment , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could get status for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for experiment `{}`.' . format ( _experiment ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for experiment `{}`.' . format ( _experiment ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'experiment' , None ) dict_tabulate ( objects , is_list_dict = True ) def get_experiment_job_statuses ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_statuses ( user , project_name , _experiment , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True ) page = page or 1 user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_statuses ( ) else : get_experiment_statuses ( )
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
1675	def _ExpandDirectories ( filenames ) : expanded = set ( ) for filename in filenames : if not os . path . isdir ( filename ) : expanded . add ( filename ) continue for root , _ , files in os . walk ( filename ) : for loopfile in files : fullname = os . path . join ( root , loopfile ) if fullname . startswith ( '.' + os . path . sep ) : fullname = fullname [ len ( '.' + os . path . sep ) : ] expanded . add ( fullname ) filtered = [ ] for filename in expanded : if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : filtered . append ( filename ) return filtered
6155	def cruise_control ( wn , zeta , T , vcruise , vmax , tf_mode = 'H' ) : tau = T / 2. * vmax / vcruise g = 9.8 g *= 3 * 60 ** 2 / 5280. Kp = T * ( 2 * zeta * wn - 1 / tau ) / vmax Ki = T * wn ** 2. / vmax K = Kp * vmax / T print ( 'wn = ' , np . sqrt ( K / ( Kp / Ki ) ) ) print ( 'zeta = ' , ( K + 1 / tau ) / ( 2 * wn ) ) a = np . array ( [ 1 , 2 * zeta * wn , wn ** 2 ] ) if tf_mode == 'H' : b = np . array ( [ K , wn ** 2 ] ) elif tf_mode == 'HE' : b = np . array ( [ 1 , 2 * zeta * wn - K , 0. ] ) elif tf_mode == 'HVW' : b = np . array ( [ 1 , wn ** 2 / K + 1 / tau , wn ** 2 / ( K * tau ) ] ) b *= Kp elif tf_mode == 'HED' : b = np . array ( [ g , 0 ] ) else : raise ValueError ( 'tf_mode must be: H, HE, HVU, or HED' ) return b , a
7918	def _validate_ip_address ( family , address ) : try : info = socket . getaddrinfo ( address , 0 , family , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , address ) ) raise ValueError ( "Bad IP address" ) if not info : logger . debug ( "getaddrinfo result empty" ) raise ValueError ( "Bad IP address" ) addr = info [ 0 ] [ 4 ] logger . debug ( " got address: {0!r}" . format ( addr ) ) try : return socket . getnameinfo ( addr , socket . NI_NUMERICHOST ) [ 0 ] except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , addr ) ) raise ValueError ( "Bad IP address" )
7277	def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = "%s %s %s %s" % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , str ( y2 ) ) self . _player_interface . VideoPos ( ObjectPath ( '/not/used' ) , String ( position ) )
1270	def _fire ( self , layers , the_plot ) : if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 self . _teleport ( ( row , col ) )
2585	def migrate_tasks_to_internal ( self , kill_event ) : logger . info ( "[TASK_PULL_THREAD] Starting" ) task_counter = 0 poller = zmq . Poller ( ) poller . register ( self . task_incoming , zmq . POLLIN ) while not kill_event . is_set ( ) : try : msg = self . task_incoming . recv_pyobj ( ) except zmq . Again : logger . debug ( "[TASK_PULL_THREAD] {} tasks in internal queue" . format ( self . pending_task_queue . qsize ( ) ) ) continue if msg == 'STOP' : kill_event . set ( ) break else : self . pending_task_queue . put ( msg ) task_counter += 1 logger . debug ( "[TASK_PULL_THREAD] Fetched task:{}" . format ( task_counter ) )
4745	def dev_get_chunk ( dev_name , state , pugrp = None , punit = None ) : rprt = dev_get_rprt ( dev_name , pugrp , punit ) if not rprt : return None return next ( ( d for d in rprt if d [ "cs" ] == state ) , None )
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
3110	def locked_delete ( self ) : query = { self . key_name : self . key_value } self . model_class . objects . filter ( ** query ) . delete ( )
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
11596	def _rc_keys ( self , pattern = '*' ) : "Returns a list of keys matching ``pattern``" result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
804	def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : query = 'SELECT model_id FROM %s ' ' WHERE status=%%s ' ' AND job_id=%%s ' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , "Unexpected numRows: %r" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE model_id=%%s ' ' AND status=%%s' ' AND TIMESTAMPDIFF(SECOND, ' ' _eng_last_update_time, ' ' UTC_TIMESTAMP()) > %%s ' ' LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
10265	def collapse_entrez_equivalencies ( graph : BELGraph ) : relation_filter = build_relation_predicate ( EQUIVALENT_TO ) source_namespace_filter = build_source_namespace_filter ( [ 'EGID' , 'EG' , 'ENTREZ' ] ) edge_predicates = [ relation_filter , source_namespace_filter , ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
1616	def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
7354	def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
3200	def create ( self , data ) : if 'recipients' not in data : raise KeyError ( 'The campaign must have recipients' ) if 'list_id' not in data [ 'recipients' ] : raise KeyError ( 'The campaign recipients must have a list_id' ) if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) if 'type' not in data : raise KeyError ( 'The campaign must have a type' ) if not data [ 'type' ] in [ 'regular' , 'plaintext' , 'rss' , 'variate' , 'abspilt' ] : raise ValueError ( 'The campaign type must be one of "regular", "plaintext", "rss", or "variate"' ) if data [ 'type' ] == 'variate' : if 'variate_settings' not in data : raise KeyError ( 'The variate campaign must have variate_settings' ) if 'winner_criteria' not in data [ 'variate_settings' ] : raise KeyError ( 'The campaign variate_settings must have a winner_criteria' ) if data [ 'variate_settings' ] [ 'winner_criteria' ] not in [ 'opens' , 'clicks' , 'total_revenue' , 'manual' ] : raise ValueError ( 'The campaign variate_settings ' 'winner_criteria must be one of "opens", "clicks", "total_revenue", or "manual"' ) if data [ 'type' ] == 'rss' : if 'rss_opts' not in data : raise KeyError ( 'The rss campaign must have rss_opts' ) if 'feed_url' not in data [ 'rss_opts' ] : raise KeyError ( 'The campaign rss_opts must have a feed_url' ) if not data [ 'rss_opts' ] [ 'frequency' ] in [ 'daily' , 'weekly' , 'monthly' ] : raise ValueError ( 'The rss_opts frequency must be one of "daily", "weekly", or "monthly"' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . campaign_id = response [ 'id' ] else : self . campaign_id = None return response
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) enrich_backend . studies = all_studies
6891	def _starfeatures_worker ( task ) : try : ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden , custom_bandpasses , lcformat , lcformatdir ) = task return get_starfeatures ( lcfile , outdir , kdtree , objlist , lcflist , neighbor_radius_arcsec , deredden = deredden , custom_bandpasses = custom_bandpasses , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
8645	def get_track_by_id ( session , track_id , track_point_limit = None , track_point_offset = None ) : tracking_data = { } if track_point_limit : tracking_data [ 'track_point_limit' ] = track_point_limit if track_point_offset : tracking_data [ 'track_point_offset' ] = track_point_offset response = make_get_request ( session , 'tracks/{}' . format ( track_id ) , params_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12200	def from_jsonfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . _from_jsonlines ( fp , selector_handler = selector_handler , strict = strict , debug = debug )
10047	def check_oauth2_scope ( can_method , * myscopes ) : def check ( record , * args , ** kwargs ) : @ require_api_auth ( ) @ require_oauth_scopes ( * myscopes ) def can ( self ) : return can_method ( record ) return type ( 'CheckOAuth2Scope' , ( ) , { 'can' : can } ) ( ) return check
337	def plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = 50 , ax = None ) : score = compute_consistency_score ( returns_test , ppc ) ax = _plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = plot_train_len , ax = ax ) ax . text ( 0.40 , 0.90 , 'Consistency score: %.1f' % score , verticalalignment = 'bottom' , horizontalalignment = 'right' , transform = ax . transAxes , ) ax . set_ylabel ( 'Cumulative returns' ) return score
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
10316	def relation_set_has_contradictions ( relations : Set [ str ] ) -> bool : has_increases = any ( relation in CAUSAL_INCREASE_RELATIONS for relation in relations ) has_decreases = any ( relation in CAUSAL_DECREASE_RELATIONS for relation in relations ) has_cnc = any ( relation == CAUSES_NO_CHANGE for relation in relations ) return 1 < sum ( [ has_cnc , has_decreases , has_increases ] )
6243	def load ( self ) : self . _open_image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image_data ( self . image ) texture = self . ctx . texture_array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
12110	def input_options ( self , options , prompt = 'Select option' , default = None ) : check_options = [ x . lower ( ) for x in options ] while True : response = input ( '%s [%s]: ' % ( prompt , ', ' . join ( options ) ) ) . lower ( ) if response in check_options : return response . strip ( ) elif response == '' and default is not None : return default . lower ( ) . strip ( )
12436	def traverse ( cls , request , params = None ) : result = cls . parse ( request . path ) if result is None : return cls , { } elif not result : raise http . exceptions . NotFound ( ) resource , data , rest = result if params : data . update ( params ) if resource is None : return cls , data if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest result = resource . traverse ( request , params = data ) return result
9498	def parse_litezip ( path ) : struct = [ parse_collection ( path ) ] struct . extend ( [ parse_module ( x ) for x in path . iterdir ( ) if x . is_dir ( ) and x . name . startswith ( 'm' ) ] ) return tuple ( sorted ( struct ) )
6429	def stem ( self , word ) : lowered = word . lower ( ) if lowered [ - 3 : ] == 'ies' and lowered [ - 4 : - 3 ] not in { 'e' , 'a' } : return word [ : - 3 ] + ( 'Y' if word [ - 1 : ] . isupper ( ) else 'y' ) if lowered [ - 2 : ] == 'es' and lowered [ - 3 : - 2 ] not in { 'a' , 'e' , 'o' } : return word [ : - 1 ] if lowered [ - 1 : ] == 's' and lowered [ - 2 : - 1 ] not in { 'u' , 's' } : return word [ : - 1 ] return word
13822	def update_config ( new_config ) : flask_app . base_config . update ( new_config ) if new_config . has_key ( 'working_directory' ) : wd = os . path . abspath ( new_config [ 'working_directory' ] ) if nbmanager . notebook_dir != wd : if not os . path . exists ( wd ) : raise IOError ( 'Path not found: %s' % wd ) nbmanager . notebook_dir = wd
13089	def ensure_remote_branch_is_tracked ( branch ) : if branch == MASTER_BRANCH : return output = subprocess . check_output ( [ 'git' , 'branch' , '--list' ] ) for line in output . split ( '\n' ) : if line . strip ( ) == branch : break else : try : sys . stdout . write ( subprocess . check_output ( [ 'git' , 'checkout' , '--track' , 'origin/%s' % branch ] ) ) except subprocess . CalledProcessError : raise SystemExit ( 1 )
3579	def clear_cached_data ( self ) : for device in self . list_devices ( ) : if device . is_connected : continue adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) adapter . RemoveDevice ( device . _device . object_path )
4548	def fill_rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : _draw_fast_vline ( setter , i , y , h , color , aa )
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
8288	def get_child_by_name ( parent , name ) : def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
2799	def convert_convtranspose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transposed convolution ...' ) if names == 'short' : tf_name = 'C' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) if len ( weights [ weights_name ] . numpy ( ) . shape ) == 4 : W = weights [ weights_name ] . numpy ( ) . transpose ( 2 , 3 , 1 , 0 ) height , width , n_filters , channels = W . shape n_groups = params [ 'group' ] if n_groups > 1 : raise AssertionError ( 'Cannot convert conv1d with groups != 1' ) if params [ 'dilations' ] [ 0 ] > 1 : raise AssertionError ( 'Cannot convert conv1d with dilation_rate != 1' ) if bias_name in weights : biases = weights [ bias_name ] . numpy ( ) has_bias = True else : biases = None has_bias = False input_name = inputs [ 0 ] if has_bias : weights = [ W , biases ] else : weights = [ W ] conv = keras . layers . Conv2DTranspose ( filters = n_filters , kernel_size = ( height , width ) , strides = ( params [ 'strides' ] [ 0 ] , params [ 'strides' ] [ 1 ] ) , padding = 'valid' , output_padding = 0 , weights = weights , use_bias = has_bias , activation = None , dilation_rate = params [ 'dilations' ] [ 0 ] , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , name = tf_name ) layers [ scope_name ] = conv ( layers [ input_name ] ) layers [ scope_name ] . set_shape ( layers [ scope_name ] . _keras_shape ) pads = params [ 'pads' ] if pads [ 0 ] > 0 : assert ( len ( pads ) == 2 or ( pads [ 2 ] == pads [ 0 ] and pads [ 3 ] == pads [ 1 ] ) ) crop = keras . layers . Cropping2D ( pads [ : 2 ] , name = tf_name + '_crop' ) layers [ scope_name ] = crop ( layers [ scope_name ] ) else : raise AssertionError ( 'Layer is not supported for now' )
9093	def _update_namespace ( self , namespace : Namespace ) -> None : old_entry_identifiers = self . _get_old_entry_identifiers ( namespace ) new_count = 0 skip_count = 0 for model in self . _iterate_namespace_models ( ) : if self . _get_identifier ( model ) in old_entry_identifiers : continue entry = self . _create_namespace_entry_from_model ( model , namespace = namespace ) if entry is None or entry . name is None : skip_count += 1 continue new_count += 1 self . session . add ( entry ) t = time . time ( ) log . info ( 'got %d new entries. skipped %d entries missing names. committing models' , new_count , skip_count ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t )
12805	def search ( self , terms ) : messages = self . _connection . get ( "search/%s" % urllib . quote_plus ( terms ) , key = "messages" ) if messages : messages = [ Message ( self , message ) for message in messages ] return messages
7556	def random_combination ( nsets , n , k ) : sets = set ( ) while len ( sets ) < nsets : newset = tuple ( sorted ( np . random . choice ( n , k , replace = False ) ) ) sets . add ( newset ) return tuple ( sets )
9476	def add_edge ( self , n1_label , n2_label , directed = False ) : n1 = self . add_node ( n1_label ) n2 = self . add_node ( n2_label ) e = Edge ( n1 , n2 , directed ) self . _edges . append ( e ) return e
9007	def transfer_to_row ( self , new_row ) : if new_row != self . _row : index = self . get_index_in_row ( ) if index is not None : self . _row . instructions . pop ( index ) self . _row = new_row
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
13031	def poll_once ( self , timeout = 0.0 ) : if self . _map : self . _poll_func ( timeout , self . _map )
1013	def _getBestMatchingCell ( self , c , activeState , minThreshold ) : bestActivityInCol = minThreshold bestSegIdxInCol = - 1 bestCellInCol = - 1 for i in xrange ( self . cellsPerColumn ) : maxSegActivity = 0 maxSegIdx = 0 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState ) if activity > maxSegActivity : maxSegActivity = activity maxSegIdx = j if maxSegActivity >= bestActivityInCol : bestActivityInCol = maxSegActivity bestSegIdxInCol = maxSegIdx bestCellInCol = i if bestCellInCol == - 1 : return ( None , None , None ) else : return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , bestActivityInCol )
2533	def validate ( self , messages ) : messages = self . validate_checksum ( messages ) messages = self . validate_optional_str_fields ( messages ) messages = self . validate_mandatory_str_fields ( messages ) messages = self . validate_files ( messages ) messages = self . validate_mandatory_fields ( messages ) messages = self . validate_optional_fields ( messages ) return messages
2055	def ADDW ( cpu , dest , src , add ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc if src . type == 'register' and src . reg in ( 'PC' , 'R15' ) : src = aligned_pc else : src = src . read ( ) dest . write ( src + add . read ( ) )
2552	def attr ( * args , ** kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
5876	def check_link_tag ( self ) : node = self . article . raw_doc meta = self . parser . getElementsByTag ( node , tag = 'link' , attr = 'rel' , value = 'image_src' ) for item in meta : src = self . parser . getAttribute ( item , attr = 'href' ) if src : return self . get_image ( src , extraction_type = 'linktag' ) return None
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
7613	def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . constants . arenas : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/arenas/arena{}.png' . format ( i . arena_id )
1212	def WorkerAgentGenerator ( agent_class ) : if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : def __init__ ( self , model = None , ** kwargs ) : self . model = model if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) super ( WorkerAgent , self ) . __init__ ( ** kwargs ) def initialize_model ( self ) : return self . model return WorkerAgent
8932	def capture ( cmd , ** kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore_failures = kw . pop ( 'ignore_failures' , False ) try : return invoke_run ( cmd , ** kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore_failures : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise
7196	def ndwi ( self ) : data = self . _read ( self [ self . _ndwi_bands , ... ] ) . astype ( np . float32 ) return ( data [ 1 , : , : ] - data [ 0 , : , : ] ) / ( data [ 0 , : , : ] + data [ 1 , : , : ] )
3312	def do_MKCOL ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] provider = self . _davProvider if util . get_content_length ( environ ) != 0 : self . _fail ( HTTP_MEDIATYPE_NOT_SUPPORTED , "The server does not handle any body content." , ) if environ . setdefault ( "HTTP_DEPTH" , "0" ) != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if provider . exists ( path , environ ) : self . _fail ( HTTP_METHOD_NOT_ALLOWED , "MKCOL can only be executed on an unmapped URL." , ) parentRes = provider . get_resource_inst ( util . get_uri_parent ( path ) , environ ) if not parentRes or not parentRes . is_collection : self . _fail ( HTTP_CONFLICT , "Parent must be an existing collection." ) self . _check_write_permission ( parentRes , "0" , environ ) parentRes . create_collection ( util . get_uri_name ( path ) ) return util . send_status_response ( environ , start_response , HTTP_CREATED )
3241	def _get_base ( server_certificate , ** conn ) : server_certificate [ '_version' ] = 1 cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , ** conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
11352	def _fill_text ( self , text , width , indent ) : lines = [ ] for line in text . splitlines ( False ) : if line : lines . extend ( textwrap . wrap ( line . strip ( ) , width , initial_indent = indent , subsequent_indent = indent ) ) else : lines . append ( line ) text = "\n" . join ( lines ) return text
6830	def get_logs_between_commits ( self , a , b ) : print ( 'REAL' ) ret = self . local ( 'git --no-pager log --pretty=oneline %s...%s' % ( a , b ) , capture = True ) if self . verbose : print ( ret ) return str ( ret )
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
10078	def _process_files ( self , record_id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ '_files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( RecordsBuckets ( record_id = record_id , bucket_id = snapshot . id ) ) else : yield data
9624	def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED_APPS : module = import_module ( application ) if module_has_submodule ( module , 'emails' ) : emails = import_module ( '%s.emails' % application ) try : import_module ( '%s.emails.previews' % application ) except ImportError : if module_has_submodule ( emails , 'previews' ) : raise
8629	def create_hireme_project ( session , title , description , currency , budget , jobs , hireme_initial_bid ) : jobs . append ( create_job_object ( id = 417 ) ) project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs , 'hireme' : True , 'hireme_initial_bid' : hireme_initial_bid } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
13637	def maybe ( f , default = None ) : @ wraps ( f ) def _maybe ( x , * a , ** kw ) : if x is None : return default return f ( x , * a , ** kw ) return _maybe
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
13168	def children ( self , name = None , reverse = False ) : elems = self . _children if reverse : elems = reversed ( elems ) for elem in elems : if name is None or elem . tagname == name : yield elem
7878	def serialize ( element ) : if getattr ( _THREAD , "serializer" , None ) is None : _THREAD . serializer = XMPPSerializer ( "jabber:client" ) _THREAD . serializer . emit_head ( None , None ) return _THREAD . serializer . emit_stanza ( element )
9809	def version ( cli , platform ) : version_client = PolyaxonClient ( ) . version cli = cli or not any ( [ cli , platform ] ) if cli : try : server_version = version_client . get_cli_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get cli version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) cli_version = get_version ( PROJECT_CLI_NAME ) Printer . print_header ( 'Current cli version: {}.' . format ( cli_version ) ) Printer . print_header ( 'Supported cli versions:' ) dict_tabulate ( server_version . to_dict ( ) ) if platform : try : platform_version = version_client . get_platform_version ( ) except AuthorizationError : session_expired ( ) sys . exit ( 1 ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get platform version.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) chart_version = version_client . get_chart_version ( ) Printer . print_header ( 'Current platform version: {}.' . format ( chart_version . version ) ) Printer . print_header ( 'Supported platform versions:' ) dict_tabulate ( platform_version . to_dict ( ) )
12993	def level_chunker ( text , getreffs , level = 1 ) : references = getreffs ( level = level ) return [ ( ref . split ( ":" ) [ - 1 ] , ref . split ( ":" ) [ - 1 ] ) for ref in references ]
965	def bitsToString ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s
3876	async def _get_or_fetch_conversation ( self , conv_id ) : conv = self . _conv_dict . get ( conv_id , None ) if conv is None : logger . info ( 'Fetching unknown conversation %s' , conv_id ) res = await self . _client . get_conversation ( hangouts_pb2 . GetConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_spec = hangouts_pb2 . ConversationSpec ( conversation_id = hangouts_pb2 . ConversationId ( id = conv_id ) ) , include_event = False ) ) conv_state = res . conversation_state event_cont_token = None if conv_state . HasField ( 'event_continuation_token' ) : event_cont_token = conv_state . event_continuation_token return self . _add_conversation ( conv_state . conversation , event_cont_token = event_cont_token ) else : return conv
3080	def get_token ( http , service_account = 'default' ) : token_json = get ( http , 'instance/service-accounts/{0}/token' . format ( service_account ) ) token_expiry = client . _UTCNOW ( ) + datetime . timedelta ( seconds = token_json [ 'expires_in' ] ) return token_json [ 'access_token' ] , token_expiry
7696	def delayed_call ( self , delay , function ) : main_loop = self handler = [ ] class DelayedCallHandler ( TimeoutHandler ) : @ timeout_handler ( delay , False ) def callback ( self ) : try : function ( ) finally : main_loop . remove_handler ( handler [ 0 ] ) handler . append ( DelayedCallHandler ( ) ) self . add_handler ( handler [ 0 ] )
7467	def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : return [ _parse_00 ( i ) for i in self . files . outfiles ] else : return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) else : return "summary function not yet ready for this type of result"
6589	def open ( self ) : self . workingArea . open ( ) self . runid_pkgidx_map = { } self . runid_to_return = deque ( )
12153	def convolve ( signal , kernel ) : pad = np . ones ( len ( kernel ) / 2 ) signal = np . concatenate ( ( pad * signal [ 0 ] , signal , pad * signal [ - 1 ] ) ) signal = np . convolve ( signal , kernel , mode = 'same' ) signal = signal [ len ( pad ) : - len ( pad ) ] return signal
13502	def extra_context ( request ) : host = os . environ . get ( 'DJANGO_LIVE_TEST_SERVER_ADDRESS' , None ) or request . get_host ( ) d = { 'request' : request , 'HOST' : host , 'IN_ADMIN' : request . path . startswith ( '/admin/' ) , } return d
7653	def serialize_obj ( obj ) : if isinstance ( obj , np . integer ) : return int ( obj ) elif isinstance ( obj , np . floating ) : return float ( obj ) elif isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , list ) : return [ serialize_obj ( x ) for x in obj ] elif isinstance ( obj , Observation ) : return { k : serialize_obj ( v ) for k , v in six . iteritems ( obj . _asdict ( ) ) } return obj
2288	def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
175	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_lines = color_lines if color_lines is not None else np . float32 ( color ) color_points = color_points if color_points is not None else np . float32 ( color ) * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else np . float32 ( alpha ) alpha_points = alpha_points if alpha_points is not None else np . float32 ( alpha ) size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 image = self . draw_lines_on_image ( image , color = np . array ( color_lines ) . astype ( np . uint8 ) , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) image = self . draw_points_on_image ( image , color = np . array ( color_points ) . astype ( np . uint8 ) , alpha = alpha_points , size = size_points , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
9447	def hangup_all_calls ( self ) : path = '/' + self . api_version + '/HangupAllCalls/' method = 'POST' return self . request ( path , method )
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
3169	def resume ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/resume' ) )
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
9623	def maybe_decode_header ( header ) : value , encoding = decode_header ( header ) [ 0 ] if encoding : return value . decode ( encoding ) else : return value
470	def read_words ( filename = "nietzsche.txt" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , "r" ) as f : try : context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list
3137	def create ( self , data ) : self . app_id = None if 'client_id' not in data : raise KeyError ( 'The authorized app must have a client_id' ) if 'client_secret' not in data : raise KeyError ( 'The authorized app must have a client_secret' ) return self . _mc_client . _post ( url = self . _build_path ( ) , data = data )
9398	def exit ( self ) : if self . _engine : self . _engine . repl . terminate ( ) self . _engine = None
299	def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + " bps" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
335	def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] return 100 - np . abs ( 50 - np . mean ( q ) ) / .5
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , ** filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) links = self . _parse_link_headers ( request [ 'Link' ] ) for link in links : self . relate ( target , read ( self , link [ 'uri' ] ) ) self . response . status = http . client . NO_CONTENT self . make_response ( )
12802	def get_room_by_name ( self , name ) : rooms = self . get_rooms ( ) for room in rooms or [ ] : if room [ "name" ] == name : return self . get_room ( room [ "id" ] ) raise RoomNotFoundException ( "Room %s not found" % name )
8702	def prepare ( self ) : log . info ( 'Preparing esp for transfer.' ) for func in LUA_FUNCTIONS : detected = self . __exchange ( 'print({0})' . format ( func ) ) if detected . find ( 'function:' ) == - 1 : break else : log . info ( 'Preparation already done. Not adding functions again.' ) return True functions = RECV_LUA + '\n' + SEND_LUA data = functions . format ( baud = self . _port . baudrate ) lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : line = line . strip ( ) . replace ( ', ' , ',' ) . replace ( ' = ' , '=' ) if len ( line ) == 0 : continue resp = self . __exchange ( line ) if ( 'unexpected' in resp ) or ( 'stdin' in resp ) or len ( resp ) > len ( functions ) + 10 : log . error ( 'error when preparing "%s"' , resp ) return False return True
5900	def commandline ( self , ** mpiargs ) : cmd = self . MDRUN . commandline ( ) if self . mpiexec : cmd = self . mpicommand ( ** mpiargs ) + cmd return cmd
4642	def _haveKey ( self , key ) : query = ( "SELECT {} FROM {} WHERE {}=?" . format ( self . __value__ , self . __tablename__ , self . __key__ ) , ( key , ) , ) connection = sqlite3 . connect ( self . sqlite_file ) cursor = connection . cursor ( ) cursor . execute ( * query ) return True if cursor . fetchone ( ) else False
7137	def format ( obj , options ) : formatters = { float_types : lambda x : '{:.{}g}' . format ( x , options . digits ) , } for _types , fmtr in formatters . items ( ) : if isinstance ( obj , _types ) : return fmtr ( obj ) try : if six . PY2 and isinstance ( obj , six . string_types ) : return str ( obj . encode ( 'utf-8' ) ) return str ( obj ) except : return 'OBJECT'
3906	async def _on_connect ( self ) : self . _user_list , self . _conv_list = ( await hangups . build_user_conversation_list ( self . _client ) ) self . _conv_list . on_event . add_observer ( self . _on_event ) conv_picker = ConversationPickerWidget ( self . _conv_list , self . on_select_conversation , self . _keys ) self . _tabbed_window = TabbedWindowWidget ( self . _keys ) self . _tabbed_window . set_tab ( conv_picker , switch = True , title = 'Conversations' ) self . _urwid_loop . widget = self . _tabbed_window
1785	def CMPXCHG ( cpu , dest , src ) : size = dest . size reg_name = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] accumulator = cpu . read_register ( reg_name ) sval = src . read ( ) dval = dest . read ( ) cpu . write_register ( reg_name , dval ) dest . write ( Operators . ITEBV ( size , accumulator == dval , sval , dval ) ) cpu . _calculate_CMP_flags ( size , accumulator - dval , accumulator , dval )
8735	def date_range ( start = None , stop = None , step = None ) : if step is None : step = datetime . timedelta ( days = 1 ) if start is None : start = datetime . datetime . now ( ) while start < stop : yield start start += step
2341	def forward ( self , x ) : self . noise . normal_ ( ) return self . layers ( th . cat ( [ x , self . noise ] , 1 ) )
6610	def getArrays ( self , tree , branchName ) : itsArray = self . _getArray ( tree , branchName ) if itsArray is None : return None , None itsCountArray = self . _getCounterArray ( tree , branchName ) return itsArray , itsCountArray
7237	def randwindow ( self , window_shape ) : row = random . randrange ( window_shape [ 0 ] , self . shape [ 1 ] ) col = random . randrange ( window_shape [ 1 ] , self . shape [ 2 ] ) return self [ : , row - window_shape [ 0 ] : row , col - window_shape [ 1 ] : col ]
3477	def _dissociate_gene ( self , cobra_gene ) : self . _genes . discard ( cobra_gene ) cobra_gene . _reaction . discard ( self )
5442	def parse_file_provider ( uri ) : providers = { 'gs' : job_model . P_GCS , 'file' : job_model . P_LOCAL } provider_found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider_found : prefix = provider_found . group ( 1 ) . lower ( ) else : prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise ValueError ( 'File prefix not supported: %s://' % prefix )
4483	def create_file ( self , path , fp , force = False , update = False ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) path = norm_remote_path ( path ) directory , fname = os . path . split ( path ) directories = directory . split ( os . path . sep ) parent = self for directory in directories : if directory : parent = parent . create_folder ( directory , exist_ok = True ) url = parent . _new_file_url connection_error = False if file_empty ( fp ) : response = self . _put ( url , params = { 'name' : fname } , data = b'' ) else : try : response = self . _put ( url , params = { 'name' : fname } , data = fp ) except ConnectionError : connection_error = True if connection_error or response . status_code == 409 : if not force and not update : file_size_bytes = get_local_file_size ( fp ) large_file_cutoff = 2 ** 20 if connection_error and file_size_bytes < large_file_cutoff : msg = ( "There was a connection error which might mean {} " + "already exists. Try again with the `--force` flag " + "specified." ) . format ( path ) raise RuntimeError ( msg ) else : raise FileExistsError ( path ) else : for file_ in self . files : if norm_remote_path ( file_ . path ) == path : if not force : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : break fp . seek ( 0 ) file_ . update ( fp ) break else : raise RuntimeError ( "Could not create a new file at " "({}) nor update it." . format ( path ) )
5363	def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
9081	def find ( self , query , ** kwargs ) : if 'providers' not in kwargs : providers = self . get_providers ( ) else : pargs = kwargs [ 'providers' ] if isinstance ( pargs , list ) : providers = self . get_providers ( ids = pargs ) else : providers = self . get_providers ( ** pargs ) kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . find ( query , ** kwarguments ) } for p in providers ]
6801	def load_db_set ( self , name , r = None ) : r = r or self db_set = r . genv . db_sets . get ( name , { } ) r . genv . update ( db_set )
8260	def cluster_sort ( self , cmp1 = "hue" , cmp2 = "brightness" , reversed = False , n = 12 ) : sorted = self . sort ( cmp1 ) clusters = ColorList ( ) d = 1.0 i = 0 for j in _range ( len ( sorted ) ) : if getattr ( sorted [ j ] , cmp1 ) < d : clusters . extend ( sorted [ i : j ] . sort ( cmp2 ) ) d -= 1.0 / n i = j clusters . extend ( sorted [ i : ] . sort ( cmp2 ) ) if reversed : _list . reverse ( clusters ) return clusters
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
10036	def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
2972	def from_dict ( name , values ) : count = 1 count_value = values . get ( 'count' , 1 ) if isinstance ( count_value , int ) : count = max ( count_value , 1 ) def with_index ( name , idx ) : if name and idx : return '%s_%d' % ( name , idx ) return name def get_instance ( n , idx = None ) : return BlockadeContainerConfig ( with_index ( n , idx ) , values [ 'image' ] , command = values . get ( 'command' ) , links = values . get ( 'links' ) , volumes = values . get ( 'volumes' ) , publish_ports = values . get ( 'ports' ) , expose_ports = values . get ( 'expose' ) , environment = values . get ( 'environment' ) , hostname = values . get ( 'hostname' ) , dns = values . get ( 'dns' ) , start_delay = values . get ( 'start_delay' , 0 ) , neutral = values . get ( 'neutral' , False ) , holy = values . get ( 'holy' , False ) , container_name = with_index ( values . get ( 'container_name' ) , idx ) , cap_add = values . get ( 'cap_add' ) ) if count == 1 : yield get_instance ( name ) else : for idx in range ( 1 , count + 1 ) : yield get_instance ( name , idx )
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
3393	def undelete_model_genes ( cobra_model ) : if cobra_model . _trimmed_genes is not None : for x in cobra_model . _trimmed_genes : x . functional = True if cobra_model . _trimmed_reactions is not None : for the_reaction , ( lower_bound , upper_bound ) in cobra_model . _trimmed_reactions . items ( ) : the_reaction . lower_bound = lower_bound the_reaction . upper_bound = upper_bound cobra_model . _trimmed_genes = [ ] cobra_model . _trimmed_reactions = { } cobra_model . _trimmed = False
8357	def _detectEncoding ( self , xml_data , isHTML = False ) : xml_encoding = sniffed_xml_encoding = None try : if xml_data [ : 4 ] == '\x4c\x6f\xa7\x94' : xml_data = self . _ebcdic_to_ascii ( xml_data ) elif xml_data [ : 4 ] == '\x00\x3c\x00\x3f' : sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data , 'utf-16be' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xfe\xff' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : sniffed_xml_encoding = 'utf-16be' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x3f\x00' : sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data , 'utf-16le' ) . encode ( 'utf-8' ) elif ( len ( xml_data ) >= 4 ) and ( xml_data [ : 2 ] == '\xff\xfe' ) and ( xml_data [ 2 : 4 ] != '\x00\x00' ) : sniffed_xml_encoding = 'utf-16le' xml_data = unicode ( xml_data [ 2 : ] , 'utf-16le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\x00\x3c' : sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x3c\x00\x00\x00' : sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\x00\x00\xfe\xff' : sniffed_xml_encoding = 'utf-32be' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32be' ) . encode ( 'utf-8' ) elif xml_data [ : 4 ] == '\xff\xfe\x00\x00' : sniffed_xml_encoding = 'utf-32le' xml_data = unicode ( xml_data [ 4 : ] , 'utf-32le' ) . encode ( 'utf-8' ) elif xml_data [ : 3 ] == '\xef\xbb\xbf' : sniffed_xml_encoding = 'utf-8' xml_data = unicode ( xml_data [ 3 : ] , 'utf-8' ) . encode ( 'utf-8' ) else : sniffed_xml_encoding = 'ascii' pass except : xml_encoding_match = None xml_encoding_match = re . compile ( '^<\?.*encoding=[\'"](.*?)[\'"].*\?>' ) . match ( xml_data ) if not xml_encoding_match and isHTML : regexp = re . compile ( '<\s*meta[^>]+charset=([^>]*?)[;\'">]' , re . I ) xml_encoding_match = regexp . search ( xml_data ) if xml_encoding_match is not None : xml_encoding = xml_encoding_match . groups ( ) [ 0 ] . lower ( ) if isHTML : self . declaredHTMLEncoding = xml_encoding if sniffed_xml_encoding and ( xml_encoding in ( 'iso-10646-ucs-2' , 'ucs-2' , 'csunicode' , 'iso-10646-ucs-4' , 'ucs-4' , 'csucs4' , 'utf-16' , 'utf-32' , 'utf_16' , 'utf_32' , 'utf16' , 'u16' ) ) : xml_encoding = sniffed_xml_encoding return xml_data , xml_encoding , sniffed_xml_encoding
3841	async def sync_all_new_events ( self , sync_all_new_events_request ) : response = hangouts_pb2 . SyncAllNewEventsResponse ( ) await self . _pb_request ( 'conversations/syncallnewevents' , sync_all_new_events_request , response ) return response
6171	def freq_resp ( self , mode = 'dB' , fs = 8000 , ylim = [ - 100 , 2 ] ) : iir_d . freqz_resp_cas_list ( [ self . sos ] , mode , fs = fs ) pylab . grid ( ) pylab . ylim ( ylim )
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
1739	def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
5873	def deserialize_organization ( organization_dict ) : return models . Organization ( id = organization_dict . get ( 'id' ) , name = organization_dict . get ( 'name' , '' ) , short_name = organization_dict . get ( 'short_name' , '' ) , description = organization_dict . get ( 'description' , '' ) , logo = organization_dict . get ( 'logo' , '' ) )
5385	def _get_operation_input_field_values ( self , metadata , file_input ) : input_args = metadata [ 'request' ] [ 'ephemeralPipeline' ] [ 'inputParameters' ] vals_dict = metadata [ 'request' ] [ 'pipelineArgs' ] [ 'inputs' ] names = [ arg [ 'name' ] for arg in input_args if ( 'localCopy' in arg ) == file_input ] return { name : vals_dict [ name ] for name in names if name in vals_dict }
12515	def new_img_like ( ref_niimg , data , affine = None , copy_header = False ) : if not ( hasattr ( ref_niimg , 'get_data' ) and hasattr ( ref_niimg , 'get_affine' ) ) : if isinstance ( ref_niimg , _basestring ) : ref_niimg = nib . load ( ref_niimg ) elif operator . isSequenceType ( ref_niimg ) : ref_niimg = nib . load ( ref_niimg [ 0 ] ) else : raise TypeError ( ( 'The reference image should be a niimg, %r ' 'was passed' ) % ref_niimg ) if affine is None : affine = ref_niimg . get_affine ( ) if data . dtype == bool : default_dtype = np . int8 if ( LooseVersion ( nib . __version__ ) >= LooseVersion ( '1.2.0' ) and isinstance ( ref_niimg , nib . freesurfer . mghformat . MGHImage ) ) : default_dtype = np . uint8 data = as_ndarray ( data , dtype = default_dtype ) header = None if copy_header : header = copy . copy ( ref_niimg . get_header ( ) ) header [ 'scl_slope' ] = 0. header [ 'scl_inter' ] = 0. header [ 'glmax' ] = 0. header [ 'cal_max' ] = np . max ( data ) if data . size > 0 else 0. header [ 'cal_max' ] = np . min ( data ) if data . size > 0 else 0. return ref_niimg . __class__ ( data , affine , header = header )
6734	def get_hosts_retriever ( s = None ) : s = s or env . hosts_retriever if not s : return env_hosts_retriever return str_to_callable ( s ) or env_hosts_retriever
11776	def WeightedMajority ( predictors , weights ) : "Return a predictor that takes a weighted vote." def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
11755	def tt_check_all ( kb , alpha , symbols , model ) : "Auxiliary routine to implement tt_entails." if not symbols : if pl_true ( kb , model ) : result = pl_true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt_check_all ( kb , alpha , rest , extend ( model , P , True ) ) and tt_check_all ( kb , alpha , rest , extend ( model , P , False ) ) )
12128	def summary ( self ) : print ( "Items: %s" % len ( self ) ) varying_keys = ', ' . join ( '%r' % k for k in self . varying_keys ) print ( "Varying Keys: %s" % varying_keys ) items = ', ' . join ( [ '%s=%r' % ( k , v ) for ( k , v ) in self . constant_items ] ) if self . constant_items : print ( "Constant Items: %s" % items )
344	def train_and_validate_to_end ( self , validate_step_size = 50 ) : while not self . _sess . should_stop ( ) : self . train_on_batch ( ) if self . global_step % validate_step_size == 0 : log_str = 'step: %d, ' % self . global_step for n , m in self . validation_metrics : log_str += '%s: %f, ' % ( n . name , m ) logging . info ( log_str )
9049	def B ( self ) : return unvec ( self . _vecB . value , ( self . X . shape [ 1 ] , self . A . shape [ 0 ] ) )
3344	def read_timeout_value_header ( timeoutvalue ) : timeoutsecs = 0 timeoutvaluelist = timeoutvalue . split ( "," ) for timeoutspec in timeoutvaluelist : timeoutspec = timeoutspec . strip ( ) if timeoutspec . lower ( ) == "infinite" : return - 1 else : listSR = reSecondsReader . findall ( timeoutspec ) for secs in listSR : timeoutsecs = int ( secs ) if timeoutsecs > MAX_FINITE_TIMEOUT_LIMIT : return - 1 if timeoutsecs != 0 : return timeoutsecs return None
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) return payload . encode ( 'utf-8' )
7542	def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) return consens
7316	def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
9465	def conference_record_start ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStart/' method = 'POST' return self . request ( path , method , call_params )
12932	def nav_to_vcf_dir ( ftp , build ) : if build == 'b37' : ftp . cwd ( DIR_CLINVAR_VCF_B37 ) elif build == 'b38' : ftp . cwd ( DIR_CLINVAR_VCF_B38 ) else : raise IOError ( "Genome build not recognized." )
10726	def _handle_variant ( self ) : def the_func ( a_tuple , variant = 0 ) : ( signature , an_obj ) = a_tuple ( func , sig ) = self . COMPLETE . parseString ( signature ) [ 0 ] assert sig == signature ( xformed , _ ) = func ( an_obj , variant = variant + 1 ) return ( xformed , xformed . variant_level ) return ( the_func , 'v' )
4216	def get_credential ( self , service , username ) : if username is not None : password = self . get_password ( service , username ) if password is not None : return credentials . SimpleCredential ( username , password , ) return None
7247	def status ( self , workflow_id ) : self . logger . debug ( 'Get status of workflow: ' + workflow_id ) url = '%(wf_url)s/%(wf_id)s' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( ) [ 'state' ]
12165	def once ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _once [ event ] . append ( listener ) self . _check_limit ( event ) return self
13317	def remove ( name_or_path ) : r = resolve ( name_or_path ) r . resolved [ 0 ] . remove ( ) EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
77	def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj
12486	def get_possible_paths ( base_path , path_regex ) : if not path_regex : return [ ] if len ( path_regex ) < 1 : return [ ] if path_regex [ 0 ] == os . sep : path_regex = path_regex [ 1 : ] rest_files = '' if os . sep in path_regex : node_names = path_regex . partition ( os . sep ) first_node = node_names [ 0 ] rest_nodes = node_names [ 2 ] folder_names = filter_list ( os . listdir ( base_path ) , first_node ) for nom in folder_names : new_base = op . join ( base_path , nom ) if op . isdir ( new_base ) : rest_files = get_possible_paths ( new_base , rest_nodes ) else : rest_files = filter_list ( os . listdir ( base_path ) , path_regex ) files = [ ] if rest_files : files = [ op . join ( base_path , f ) for f in rest_files ] return files
9417	def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
3845	def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
10728	def _handle_struct ( toks ) : subtrees = toks [ 1 : - 1 ] signature = '' . join ( s for ( _ , s ) in subtrees ) funcs = [ f for ( f , _ ) in subtrees ] def the_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "must be a simple sequence, is a dict" ) if len ( a_list ) != len ( funcs ) : raise IntoDPValueError ( a_list , "a_list" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( a_list ) ) ) elements = [ f ( x ) for ( f , x ) in zip ( funcs , a_list ) ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Struct ( ( x for ( x , _ ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_func , '(' + signature + ')' )
11374	def license_is_oa ( license ) : for oal in OA_LICENSES : if re . search ( oal , license ) : return True return False
8262	def repeat ( self , n = 2 , oscillate = False , callback = None ) : colorlist = ColorList ( ) colors = ColorList . copy ( self ) for i in _range ( n ) : colorlist . extend ( colors ) if oscillate : colors = colors . reverse ( ) if callback : colors = callback ( colors ) return colorlist
3183	def update ( self , store_id , data ) : self . store_id = store_id return self . _mc_client . _patch ( url = self . _build_path ( store_id ) , data = data )
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , ** kwargs )
5541	def hillshade ( self , elevation , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : return commons_hillshade . hillshade ( elevation , self , azimuth , altitude , z , scale )
1863	def STOS ( cpu , dest , src ) : size = src . size dest . write ( src . read ( ) ) dest_reg = dest . mem . base increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
5403	def _get_localization_env ( self , inputs , user_project ) : non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
3950	def _read ( self , mux , gain , data_rate , mode ) : config = ADS1x15_CONFIG_OS_SINGLE config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] config |= mode if data_rate is None : data_rate = self . _data_rate_default ( ) config |= self . _data_rate_config ( data_rate ) config |= ADS1x15_CONFIG_COMP_QUE_DISABLE self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) time . sleep ( 1.0 / data_rate + 0.0001 ) result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
13	def copy_obs_dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
5526	def grab ( bbox = None , childprocess = None , backend = None ) : if childprocess is None : childprocess = childprocess_default_value ( ) return _grab ( to_file = False , childprocess = childprocess , backend = backend , bbox = bbox )
10306	def calculate_tanimoto_set_distances ( dict_of_sets : Mapping [ X , Set ] ) -> Mapping [ X , Mapping [ X , float ] ] : result : Dict [ X , Dict [ X , float ] ] = defaultdict ( dict ) for x , y in itt . combinations ( dict_of_sets , 2 ) : result [ x ] [ y ] = result [ y ] [ x ] = tanimoto_set_similarity ( dict_of_sets [ x ] , dict_of_sets [ y ] ) for x in dict_of_sets : result [ x ] [ x ] = 1.0 return dict ( result )
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
1770	def backup_emulate ( self , insn ) : if not hasattr ( self , 'backup_emu' ) : self . backup_emu = UnicornEmulator ( self ) try : self . backup_emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) ) finally : del self . backup_emu
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
7180	def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in reversed ( late_processing ) : lazy_func ( )
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
9686	def pm ( self ) : resp = [ ] data = { } self . cnxn . xfer ( [ 0x32 ] ) sleep ( 10e-3 ) for i in range ( 12 ) : r = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] resp . append ( r ) data [ 'PM1' ] = self . _calculate_float ( resp [ 0 : 4 ] ) data [ 'PM2.5' ] = self . _calculate_float ( resp [ 4 : 8 ] ) data [ 'PM10' ] = self . _calculate_float ( resp [ 8 : ] ) sleep ( 0.1 ) return data
4246	def id_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . id_by_addr ( addr )
9896	def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
2472	def reset_file_stat ( self ) : self . file_spdx_id_set = False self . file_comment_set = False self . file_type_set = False self . file_chksum_set = False self . file_conc_lics_set = False self . file_license_comment_set = False self . file_notice_set = False self . file_copytext_set = False
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
9183	def lookup_document_pointer ( ident_hash , cursor ) : id , version = split_ident_hash ( ident_hash , split_version = True ) stmt = "SELECT name FROM modules WHERE uuid = %s" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += " AND (major_version = %s AND minor_version {} %s)" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except TypeError : raise DocumentLookupError ( ) else : metadata = { 'title' : title } return cnxepub . DocumentPointer ( ident_hash , metadata )
6325	def encode ( self , text ) : text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) minval = Fraction ( 0 ) maxval = Fraction ( 1 ) for char in text + '\x00' : prob_range = self . _probs [ char ] delta = maxval - minval maxval = minval + prob_range [ 1 ] * delta minval = minval + prob_range [ 0 ] * delta delta = ( maxval - minval ) / 2 nbits = long ( 0 ) while delta < 1 : nbits += 1 delta *= 2 if nbits == 0 : return 0 , 0 avg = ( maxval + minval ) * 2 ** ( nbits - 1 ) return avg . numerator // avg . denominator , nbits
2614	def _write_submit_script ( self , template , script_filename , job_name , configs ) : try : submit_script = Template ( template ) . substitute ( jobname = job_name , ** configs ) with open ( script_filename , 'w' ) as f : f . write ( submit_script ) except KeyError as e : logger . error ( "Missing keys for submit script : %s" , e ) raise ( SchedulerMissingArgs ( e . args , self . sitename ) ) except IOError as e : logger . error ( "Failed writing to submit script: %s" , script_filename ) raise ( ScriptPathError ( script_filename , e ) ) except Exception as e : print ( "Template : " , template ) print ( "Args : " , job_name ) print ( "Kwargs : " , configs ) logger . error ( "Uncategorized error: %s" , e ) raise ( e ) return True
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
6002	def regular_to_pix ( self ) : return mapper_util . voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid = self . grid_stack . regular , regular_to_nearest_pix = self . grid_stack . pix . regular_to_nearest_pix , pixel_centres = self . geometry . pixel_centres , pixel_neighbors = self . geometry . pixel_neighbors , pixel_neighbors_size = self . geometry . pixel_neighbors_size ) . astype ( 'int' )
13614	def apply_orientation ( im ) : try : kOrientationEXIFTag = 0x0112 if hasattr ( im , '_getexif' ) : e = im . _getexif ( ) if e is not None : orientation = e [ kOrientationEXIFTag ] f = orientation_funcs [ orientation ] return f ( im ) except : pass return im
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
3911	def _on_event ( self , _ ) : self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
2496	def create_package_node ( self , package ) : package_node = BNode ( ) type_triple = ( package_node , RDF . type , self . spdx_namespace . Package ) self . graph . add ( type_triple ) self . handle_pkg_optional_fields ( package , package_node ) name_triple = ( package_node , self . spdx_namespace . name , Literal ( package . name ) ) self . graph . add ( name_triple ) down_loc_node = ( package_node , self . spdx_namespace . downloadLocation , self . to_special_value ( package . download_location ) ) self . graph . add ( down_loc_node ) verif_node = self . package_verif_node ( package ) verif_triple = ( package_node , self . spdx_namespace . packageVerificationCode , verif_node ) self . graph . add ( verif_triple ) conc_lic_node = self . license_or_special ( package . conc_lics ) conc_lic_triple = ( package_node , self . spdx_namespace . licenseConcluded , conc_lic_node ) self . graph . add ( conc_lic_triple ) decl_lic_node = self . license_or_special ( package . license_declared ) decl_lic_triple = ( package_node , self . spdx_namespace . licenseDeclared , decl_lic_node ) self . graph . add ( decl_lic_triple ) licenses_from_files_nodes = map ( lambda el : self . license_or_special ( el ) , package . licenses_from_files ) lic_from_files_predicate = self . spdx_namespace . licenseInfoFromFiles lic_from_files_triples = [ ( package_node , lic_from_files_predicate , node ) for node in licenses_from_files_nodes ] for triple in lic_from_files_triples : self . graph . add ( triple ) cr_text_node = self . to_special_value ( package . cr_text ) cr_text_triple = ( package_node , self . spdx_namespace . copyrightText , cr_text_node ) self . graph . add ( cr_text_triple ) self . handle_package_has_file ( package , package_node ) return package_node
1474	def _get_tmaster_processes ( self ) : retval = { } tmaster_cmd_lst = [ self . tmaster_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--myhost=%s' % self . master_host , '--master_port=%s' % str ( self . master_port ) , '--controller_port=%s' % str ( self . tmaster_controller_port ) , '--stats_port=%s' % str ( self . tmaster_stats_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : tmaster_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ "heron-tmaster" ] = tmaster_cmd if self . metricscache_manager_mode . lower ( ) != "disabled" : retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) if self . health_manager_mode . lower ( ) != "disabled" : retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ 0 ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) return retval
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
10934	def check_terminate ( self ) : if not self . _has_run : return False else : terminate = self . check_completion ( ) terminate |= ( self . _num_iter >= self . max_iter ) return terminate
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
10093	def templates ( self , timeout = None ) : return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_GET , timeout = timeout )
1082	def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )
11551	def get_exception_from_status_and_error_codes ( status_code , error_code , value ) : if status_code == requests . codes . bad_request : exception = BadRequest ( value ) elif status_code == requests . codes . unauthorized : exception = Unauthorized ( value ) elif status_code == requests . codes . forbidden : exception = Unauthorized ( value ) elif status_code in [ requests . codes . not_found , requests . codes . gone ] : exception = NotFound ( value ) elif status_code == requests . codes . method_not_allowed : exception = MethodNotAllowed ( value ) elif status_code >= requests . codes . bad_request : exception = HTTPError ( value ) else : exception = ResponseError ( value ) if error_code == - 100 : exception = InternalError ( value ) elif error_code == - 101 : exception = InvalidToken ( value ) elif error_code == - 105 : exception = UploadFailed ( value ) elif error_code == - 140 : exception = UploadTokenGenerationFailed ( value ) elif error_code == - 141 : exception = InvalidUploadToken ( value ) elif error_code == - 150 : exception = InvalidParameter ( value ) elif error_code == - 151 : exception = InvalidPolicy ( value ) return exception
647	def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : gene = cobra_model . genes [ gene_index ] cobra_model . genes . _dict . pop ( gene . id ) gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
4836	def get_paginated_catalogs ( self , querystring = None ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , querystring = querystring , traverse_pagination = False , many = False )
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
4271	def get_exif_tags ( data , datetime_format = '%c' ) : logger = logging . getLogger ( __name__ ) simple = { } for tag in ( 'Model' , 'Make' , 'LensModel' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'FNumber' in data : fnumber = data [ 'FNumber' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid FNumber: %r' , fnumber , exc_info = True ) if 'FocalLength' in data : focal = data [ 'FocalLength' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid FocalLength: %r' , focal , exc_info = True ) if 'ExposureTime' in data : exptime = data [ 'ExposureTime' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except ZeroDivisionError : logger . info ( 'Invalid ExposureTime: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for ExposureTime: %r' , exptime ) if data . get ( 'ISOSpeedRatings' ) : simple [ 'iso' ] = data [ 'ISOSpeedRatings' ] if 'DateTimeOriginal' in data : date = data [ 'DateTimeOriginal' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime_format ) except ( ValueError , TypeError ) as e : logger . info ( 'Could not parse DateTimeOriginal: %s' , e ) if 'GPSInfo' in data : info = data [ 'GPSInfo' ] lat_info = info . get ( 'GPSLatitude' ) lon_info = info . get ( 'GPSLongitude' ) lat_ref_info = info . get ( 'GPSLatitudeRef' ) lon_ref_info = info . get ( 'GPSLongitudeRef' ) if lat_info and lon_info and lat_ref_info and lon_ref_info : try : lat = dms_to_degrees ( lat_info ) lon = dms_to_degrees ( lon_info ) except ( ZeroDivisionError , ValueError , TypeError ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat_ref_info != 'N' else lat , 'lon' : - lon if lon_ref_info != 'E' else lon , } return simple
3187	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The order must have an id' ) if 'customer' not in data : raise KeyError ( 'The order must have a customer' ) if 'id' not in data [ 'customer' ] : raise KeyError ( 'The order customer must have an id' ) if 'currency_code' not in data : raise KeyError ( 'The order must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) if 'order_total' not in data : raise KeyError ( 'The order must have an order_total' ) if 'lines' not in data : raise KeyError ( 'The order must have at least one order line' ) for line in data [ 'lines' ] : if 'id' not in line : raise KeyError ( 'Each order line must have an id' ) if 'product_id' not in line : raise KeyError ( 'Each order line must have a product_id' ) if 'product_variant_id' not in line : raise KeyError ( 'Each order line must have a product_variant_id' ) if 'quantity' not in line : raise KeyError ( 'Each order line must have a quantity' ) if 'price' not in line : raise KeyError ( 'Each order line must have a price' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'orders' ) , data = data ) if response is not None : self . order_id = response [ 'id' ] else : self . order_id = None return response
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
12249	def get_key ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , { } ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_key ( * args , ** kwargs )
8121	def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
8949	def warning ( msg ) : _flush ( ) sys . stderr . write ( "\033[1;7;33;40mWARNING: {}\033[0m\n" . format ( msg ) ) sys . stderr . flush ( )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
12125	def pprint_args ( self , pos_args , keyword_args , infix_operator = None , extra_params = { } ) : if infix_operator and not ( len ( pos_args ) == 2 and keyword_args == [ ] ) : raise Exception ( 'Infix format requires exactly two' ' positional arguments and no keywords' ) ( kwargs , _ , _ , _ ) = self . _pprint_args self . _pprint_args = ( keyword_args + kwargs , pos_args , infix_operator , extra_params )
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
9986	def get_description ( ) : with open ( path . join ( here , 'README.rst' ) , 'r' ) as f : data = f . read ( ) return data
361	def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( "[*] creates %s ..." % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( "[!] %s exists ..." % path ) return True
3522	def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise TypeError ( data )
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
4393	def adsSyncWriteReqEx ( port , address , index_group , index_offset , value , plc_data_type ) : sync_write_request = _adsDLL . AdsSyncWriteReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( value . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : if type ( plc_data_type ) . __name__ == "PyCArrayType" : data = plc_data_type ( * value ) else : data = plc_data_type ( value ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
9567	def byteswap ( fmt , data , offset = 0 ) : data = BytesIO ( data ) data . seek ( offset ) data_swapped = BytesIO ( ) for f in fmt : swapped = data . read ( int ( f ) ) [ : : - 1 ] data_swapped . write ( swapped ) return data_swapped . getvalue ( )
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
11832	def mate ( self , other ) : "Return a new individual crossing self and other." c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
9586	def write_numeric_array ( fd , header , array ) : bd = BytesIO ( ) write_var_header ( bd , header ) if not isinstance ( array , basestring ) and header [ 'dims' ] [ 0 ] > 1 : array = list ( chain . from_iterable ( izip ( * array ) ) ) write_elements ( bd , header [ 'mtp' ] , array ) data = bd . getvalue ( ) bd . close ( ) write_var_data ( fd , data )
7041	def list_lc_collections ( lcc_server ) : url = '%s/api/collections' % lcc_server try : LOGINFO ( 'getting list of recent publicly visible ' 'and owned LC collections from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) lcc_list = json . loads ( resp . read ( ) ) [ 'result' ] [ 'collections' ] return lcc_list except HTTPError as e : LOGERROR ( 'could not retrieve list of collections, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
9705	def run ( self ) : while self . isRunning . is_set ( ) : try : try : self . monitorTUN ( ) except timeout_decorator . TimeoutError as error : pass self . checkSerial ( ) except KeyboardInterrupt : break
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
812	def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : encodersDict = ( params [ "modelConfig" ] [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] ) for encoder in encodersDict . itervalues ( ) : if encoder is not None : if encoder [ "type" ] == "RandomDistributedScalarEncoder" : resolution = max ( minResolution , ( maxVal - minVal ) / encoder . pop ( "numBuckets" ) ) encodersDict [ "c1" ] [ "resolution" ] = resolution
2729	def get_kernel_available ( self ) : kernels = list ( ) data = self . get_data ( "droplets/%s/kernels/" % self . id ) while True : for jsond in data [ u'kernels' ] : kernel = Kernel ( ** jsond ) kernel . token = self . token kernels . append ( kernel ) try : url = data [ u'links' ] [ u'pages' ] . get ( u'next' ) if not url : break data = self . get_data ( url ) except KeyError : break return kernels
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
11489	def _download_folder_recursive ( folder_id , path = '.' ) : session . token = verify_credentials ( ) cur_folder = session . communicator . folder_get ( session . token , folder_id ) folder_path = os . path . join ( path , cur_folder [ 'name' ] . replace ( '/' , '_' ) ) print ( 'Creating folder at {0}' . format ( folder_path ) ) try : os . mkdir ( folder_path ) except OSError as e : if e . errno == errno . EEXIST and session . allow_existing_download_paths : pass else : raise cur_children = session . communicator . folder_children ( session . token , folder_id ) for item in cur_children [ 'items' ] : _download_item ( item [ 'item_id' ] , folder_path , item = item ) for folder in cur_children [ 'folders' ] : _download_folder_recursive ( folder [ 'folder_id' ] , folder_path ) for callback in session . folder_download_callbacks : callback ( session . communicator , session . token , cur_folder , folder_path )
12264	def _send_file_internal ( self , * args , ** kwargs ) : super ( Key , self ) . _send_file_internal ( * args , ** kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
4063	def delete_saved_search ( self , keys ) : headers = { "Zotero-Write-Token" : token ( ) } headers . update ( self . default_headers ( ) ) req = requests . delete ( url = self . endpoint + "/{t}/{u}/searches" . format ( t = self . library_type , u = self . library_id ) , headers = headers , params = { "searchKey" : "," . join ( keys ) } , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return req . status_code
617	def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )
3610	def delete ( self , url , name , params = None , headers = None , connection = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) return make_delete_request ( endpoint , params , headers , connection = connection )
10626	def _calculate_T ( self , Hfr ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_Hfr ( x [ 0 ] ) - Hfr ) y . append ( self . _calculate_Hfr ( x [ 1 ] ) - Hfr ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_Hfr ( x [ i ] ) - Hfr ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
13107	def remove_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
5357	def _add_to_conf ( self , new_conf ) : for section in new_conf : if section not in self . conf : self . conf [ section ] = new_conf [ section ] else : for param in new_conf [ section ] : self . conf [ section ] [ param ] = new_conf [ section ] [ param ]
8552	def update_image ( self , image_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/images/' + image_id , method = 'PATCH' , data = json . dumps ( data ) ) return response
12878	def many_until1 ( these , term ) : first = [ these ( ) ] these_results , term_result = many_until ( these , term ) return ( first + these_results , term_result )
2654	def pull_file ( self , remote_source , local_dir ) : local_dest = local_dir + '/' + os . path . basename ( remote_source ) try : os . makedirs ( local_dir ) except OSError as e : if e . errno != errno . EEXIST : logger . exception ( "Failed to create script_dir: {0}" . format ( script_dir ) ) raise BadScriptPath ( e , self . hostname ) if os . path . exists ( local_dest ) : logger . exception ( "Remote file copy will overwrite a local file:{0}" . format ( local_dest ) ) raise FileExists ( None , self . hostname , filename = local_dest ) try : self . sftp_client . get ( remote_source , local_dest ) except Exception as e : logger . exception ( "File pull failed" ) raise FileCopyException ( e , self . hostname ) return local_dest
5489	def from_file ( cls , file ) : if not os . path . exists ( file ) : raise ValueError ( "Config file not found." ) try : config_parser = configparser . ConfigParser ( ) config_parser . read ( file ) configuration = cls ( file , config_parser ) if not configuration . check_config_sanity ( ) : raise ValueError ( "Error in config file." ) else : return configuration except configparser . Error : raise ValueError ( "Config file is invalid." )
3632	def baseId ( resource_id , return_version = False ) : version = 0 resource_id = resource_id + 0xC4000000 while resource_id > 0x01000000 : version += 1 if version == 1 : resource_id -= 0x80000000 elif version == 2 : resource_id -= 0x03000000 else : resource_id -= 0x01000000 if return_version : return resource_id , version - 67 return resource_id
7971	def _run_timeout_threads ( self , handler ) : for dummy , method in inspect . getmembers ( handler , callable ) : if not hasattr ( method , "_pyxmpp_timeout" ) : continue thread = TimeoutThread ( method , daemon = self . daemon , exc_queue = self . exc_queue ) self . timeout_threads . append ( thread ) thread . start ( )
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
483	def getSwarmModelParams ( modelID ) : cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
13517	def resistance ( self ) : self . total_resistance_coef = frictional_resistance_coef ( self . length , self . speed ) + residual_resistance_coef ( self . slenderness_coefficient , self . prismatic_coefficient , froude_number ( self . speed , self . length ) ) RT = 1 / 2 * self . total_resistance_coef * 1025 * self . surface_area * self . speed ** 2 return RT
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
4027	def create_local_copy ( cookie_file ) : if isinstance ( cookie_file , list ) : cookie_file = cookie_file [ 0 ] if os . path . exists ( cookie_file ) : tmp_cookie_file = tempfile . NamedTemporaryFile ( suffix = '.sqlite' ) . name open ( tmp_cookie_file , 'wb' ) . write ( open ( cookie_file , 'rb' ) . read ( ) ) return tmp_cookie_file else : raise BrowserCookieError ( 'Can not find cookie file at: ' + cookie_file )
2824	def convert_sigmoid ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting sigmoid ...' ) if names == 'short' : tf_name = 'SIGM' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sigmoid = keras . layers . Activation ( 'sigmoid' , name = tf_name ) layers [ scope_name ] = sigmoid ( layers [ inputs [ 0 ] ] )
1045	def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( "invalid size value" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( "float too large to pack in this format" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant
9936	def list ( self , ignore_patterns ) : for prefix , root in self . locations : storage = self . storages [ root ] for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
4065	def fields_types ( self , tname , qstring , itemtype ) : template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
5143	def search_for_comment ( self , lineno , default = None ) : if not self . index : self . make_index ( ) block = self . index . get ( lineno , None ) text = getattr ( block , 'text' , default ) return text
12204	def auto_constraints ( self , component = None ) : if not component : for table in self . tables : self . auto_constraints ( table ) return if not component . tableSchema . primaryKey : idcol = component . get_column ( term_uri ( 'id' ) ) if idcol : component . tableSchema . primaryKey = [ idcol . name ] self . _auto_foreign_keys ( component ) try : table_type = self . get_tabletype ( component ) except ValueError : return for table in self . tables : self . _auto_foreign_keys ( table , component = component , table_type = table_type )
1942	def unmap_memory_callback ( self , start , size ) : logger . info ( f"Unmapping memory from {hex(start)} to {hex(start + size)}" ) mask = ( 1 << 12 ) - 1 if ( start & mask ) != 0 : logger . error ( "Memory to be unmapped is not aligned to a page" ) if ( size & mask ) != 0 : size = ( ( size >> 12 ) + 1 ) << 12 logger . warning ( "Forcing unmap size to align to a page" ) self . _emu . mem_unmap ( start , size )
4294	def supported_versions ( django , cms ) : cms_version = None django_version = None try : cms_version = Decimal ( cms ) except ( ValueError , InvalidOperation ) : try : cms_version = CMS_VERSION_MATRIX [ str ( cms ) ] except KeyError : pass try : django_version = Decimal ( django ) except ( ValueError , InvalidOperation ) : try : django_version = DJANGO_VERSION_MATRIX [ str ( django ) ] except KeyError : pass try : if ( cms_version and django_version and not ( LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 0 ] ) <= LooseVersion ( compat . unicode ( django_version ) ) <= LooseVersion ( VERSION_MATRIX [ compat . unicode ( cms_version ) ] [ 1 ] ) ) ) : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) except KeyError : raise RuntimeError ( 'Django and django CMS versions doesn\'t match: ' 'Django {0} is not supported by django CMS {1}' . format ( django_version , cms_version ) ) return ( compat . unicode ( django_version ) if django_version else django_version , compat . unicode ( cms_version ) if cms_version else cms_version )
5982	def output_figure ( array , as_subplot , output_path , output_filename , output_format ) : if not as_subplot : if output_format is 'show' : plt . show ( ) elif output_format is 'png' : plt . savefig ( output_path + output_filename + '.png' , bbox_inches = 'tight' ) elif output_format is 'fits' : array_util . numpy_array_2d_to_fits ( array_2d = array , file_path = output_path + output_filename + '.fits' , overwrite = True )
4504	def put_edit ( self , f , * args , ** kwds ) : self . put_nowait ( functools . partial ( f , * args , ** kwds ) )
7603	def get_player ( self , tag : crtag , timeout = None ) : url = self . api . PLAYER + '/' + tag return self . _get_model ( url , FullPlayer , timeout = timeout )
6842	def force_stop ( self ) : r = self . local_renderer with self . settings ( warn_only = True ) : r . sudo ( 'pkill -9 -f celery' ) r . sudo ( 'rm -f /tmp/celery*.pid' )
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
13628	def parse ( expected , query ) : return dict ( ( key , parser ( query . get ( key , [ ] ) ) ) for key , parser in expected . items ( ) )
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
5273	def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
3783	def select_valid_methods_P ( self , T , P ) : r if self . forced_P : considered_methods = list ( self . user_methods_P ) else : considered_methods = list ( self . all_methods_P ) if self . user_methods_P : [ considered_methods . remove ( i ) for i in self . user_methods_P ] preferences = sorted ( [ self . ranked_methods_P . index ( i ) for i in considered_methods ] ) sorted_methods = [ self . ranked_methods_P [ i ] for i in preferences ] if self . user_methods_P : [ sorted_methods . insert ( 0 , i ) for i in reversed ( self . user_methods_P ) ] sorted_valid_methods_P = [ ] for method in sorted_methods : if self . test_method_validity_P ( T , P , method ) : sorted_valid_methods_P . append ( method ) return sorted_valid_methods_P
9535	def get_complete_version ( version = None ) : if version is None : from django_cryptography import VERSION as version else : assert len ( version ) == 5 assert version [ 3 ] in ( 'alpha' , 'beta' , 'rc' , 'final' ) return version
464	def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( " clear_all ------- %s" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]
3641	def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer List - List View' ) ] if rc . get ( 'auctionInfo' ) : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
6920	def _autocorr_func2 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 0 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) autocovarfunc = npsum ( products ) / lagindex . size varfunc = npsum ( ( mags [ lagindex ] - magmed ) * ( mags [ lagindex ] - magmed ) ) / mags . size acorr = autocovarfunc / varfunc return acorr
5443	def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
8327	def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except ValueError : pass lastChild = self . _lastRecursiveChild ( ) nextElement = lastChild . next if self . previous : self . previous . next = nextElement if nextElement : nextElement . previous = self . previous self . previous = None lastChild . next = None self . parent = None if self . previousSibling : self . previousSibling . nextSibling = self . nextSibling if self . nextSibling : self . nextSibling . previousSibling = self . previousSibling self . previousSibling = self . nextSibling = None return self
4775	def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
10563	def compare_song_collections ( src_songs , dst_songs ) : def gather_field_values ( song ) : return tuple ( ( _normalize_metadata ( song [ field ] ) for field in _filter_comparison_fields ( song ) ) ) dst_songs_criteria = { gather_field_values ( _normalize_song ( dst_song ) ) for dst_song in dst_songs } return [ src_song for src_song in src_songs if gather_field_values ( _normalize_song ( src_song ) ) not in dst_songs_criteria ]
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
10682	def H_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : h = ( - self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 2 + tau ** 9 / 15 + tau ** 15 / 40 ) ) / self . _D_mag else : h = - ( tau ** - 5 / 2 + tau ** - 15 / 21 + tau ** - 25 / 60 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * h
12138	def _expand_pattern ( self , pattern ) : ( globpattern , regexp , fields , types ) = self . _decompose_pattern ( pattern ) filelist = glob . glob ( globpattern ) expansion = [ ] for fname in filelist : if fields == [ ] : expansion . append ( ( fname , { } ) ) continue match = re . match ( regexp , fname ) if match is None : continue match_items = match . groupdict ( ) . items ( ) tags = dict ( ( k , types . get ( k , str ) ( v ) ) for ( k , v ) in match_items ) expansion . append ( ( fname , tags ) ) return expansion
5930	def _canonicalize ( self , filename ) : path , ext = os . path . splitext ( filename ) if not ext : ext = ".collection" return path + ext
9877	def _ratio_metric ( v1 , v2 , ** _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
3883	def from_entity ( entity , self_user_id ) : user_id = UserID ( chat_id = entity . id . chat_id , gaia_id = entity . id . gaia_id ) return User ( user_id , entity . properties . display_name , entity . properties . first_name , entity . properties . photo_url , entity . properties . email , ( self_user_id == user_id ) or ( self_user_id is None ) )
13375	def ensure_path_exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
7528	def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
246	def apply_slippage_penalty ( returns , txn_daily , simulate_starting_capital , backtest_starting_capital , impact = 0.1 ) : mult = simulate_starting_capital / backtest_starting_capital simulate_traded_shares = abs ( mult * txn_daily . amount ) simulate_traded_dollars = txn_daily . price * simulate_traded_shares simulate_pct_volume_used = simulate_traded_shares / txn_daily . volume penalties = simulate_pct_volume_used ** 2 * impact * simulate_traded_dollars daily_penalty = penalties . resample ( 'D' ) . sum ( ) daily_penalty = daily_penalty . reindex ( returns . index ) . fillna ( 0 ) portfolio_value = ep . cum_returns ( returns , starting_value = backtest_starting_capital ) * mult adj_returns = returns - ( daily_penalty / portfolio_value ) return adj_returns
3152	def delete ( self , list_id , webhook_id ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) )
7442	def branch ( self , newname , subsamples = None , infile = None ) : remove = 0 if ( newname == self . name or os . path . exists ( os . path . join ( self . paramsdict [ "project_dir" ] , newname + ".assembly" ) ) ) : print ( "{}Assembly object named {} already exists" . format ( self . _spacer , newname ) ) else : self . _check_name ( newname ) if newname . startswith ( "params-" ) : newname = newname . split ( "params-" ) [ 1 ] newobj = copy . deepcopy ( self ) newobj . name = newname newobj . paramsdict [ "assembly_name" ] = newname if subsamples and infile : print ( BRANCH_NAMES_AND_INPUT ) if infile : if infile [ 0 ] == "-" : remove = 1 infile = infile [ 1 : ] if os . path . exists ( infile ) : subsamples = _read_sample_names ( infile ) if remove : subsamples = list ( set ( self . samples . keys ( ) ) - set ( subsamples ) ) if subsamples : for sname in subsamples : if sname in self . samples : newobj . samples [ sname ] = copy . deepcopy ( self . samples [ sname ] ) else : print ( "Sample name not found: {}" . format ( sname ) ) newobj . samples = { name : sample for name , sample in newobj . samples . items ( ) if name in subsamples } else : for sample in self . samples : newobj . samples [ sample ] = copy . deepcopy ( self . samples [ sample ] ) newobj . save ( ) return newobj
11078	def start_timer ( self , duration , func , * args ) : t = threading . Timer ( duration , self . _timer_callback , ( func , args ) ) self . _timer_callbacks [ func ] = t t . start ( ) self . log . info ( "Scheduled call to %s in %ds" , func . __name__ , duration )
2704	def collect_entities ( sent , ranks , stopwords , spacy_nlp ) : global DEBUG sent_text = " " . join ( [ w . raw for w in sent ] ) if DEBUG : print ( "sent:" , sent_text ) for ent in spacy_nlp ( sent_text ) . ents : if DEBUG : print ( "NER:" , ent . label_ , ent . text ) if ( ent . label_ not in [ "CARDINAL" ] ) and ( ent . text . lower ( ) not in stopwords ) : w_ranks , w_ids = find_entity ( sent , ranks , ent . text . split ( " " ) , 0 ) if w_ranks and w_ids : rl = RankedLexeme ( text = ent . text . lower ( ) , rank = w_ranks , ids = w_ids , pos = "np" , count = 1 ) if DEBUG : print ( rl ) yield rl
2259	def dzip ( items1 , items2 , cls = dict ) : try : len ( items1 ) except TypeError : items1 = list ( items1 ) try : len ( items2 ) except TypeError : items2 = list ( items2 ) if len ( items1 ) == 0 and len ( items2 ) == 1 : items2 = [ ] if len ( items2 ) == 1 and len ( items1 ) > 1 : items2 = items2 * len ( items1 ) if len ( items1 ) != len ( items2 ) : raise ValueError ( 'out of alignment len(items1)=%r, len(items2)=%r' % ( len ( items1 ) , len ( items2 ) ) ) return cls ( zip ( items1 , items2 ) )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
4160	def _get_data ( url ) : if url . startswith ( 'http://' ) : try : resp = urllib . urlopen ( url ) encoding = resp . headers . dict . get ( 'content-encoding' , 'plain' ) except AttributeError : resp = urllib . request . urlopen ( url ) encoding = resp . headers . get ( 'content-encoding' , 'plain' ) data = resp . read ( ) if encoding == 'plain' : pass elif encoding == 'gzip' : data = StringIO ( data ) data = gzip . GzipFile ( fileobj = data ) . read ( ) else : raise RuntimeError ( 'unknown encoding' ) else : with open ( url , 'r' ) as fid : data = fid . read ( ) return data
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
1399	def extract_tmaster ( self , topology ) : tmasterLocation = { "name" : None , "id" : None , "host" : None , "controller_port" : None , "master_port" : None , "stats_port" : None , } if topology . tmaster : tmasterLocation [ "name" ] = topology . tmaster . topology_name tmasterLocation [ "id" ] = topology . tmaster . topology_id tmasterLocation [ "host" ] = topology . tmaster . host tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port tmasterLocation [ "master_port" ] = topology . tmaster . master_port tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port return tmasterLocation
1406	def validated_formatter ( self , url_format ) : valid_parameters = { "${CLUSTER}" : "cluster" , "${ENVIRON}" : "environ" , "${TOPOLOGY}" : "topology" , "${ROLE}" : "role" , "${USER}" : "user" , } dummy_formatted_url = url_format for key , value in valid_parameters . items ( ) : dummy_formatted_url = dummy_formatted_url . replace ( key , value ) if '$' in dummy_formatted_url : raise Exception ( "Invalid viz.url.format: %s" % ( url_format ) ) return url_format
12022	def check_parent_boundary ( self ) : for line in self . lines : for parent_feature in line [ 'parents' ] : ok = False for parent_line in parent_feature : if parent_line [ 'start' ] <= line [ 'start' ] and line [ 'end' ] <= parent_line [ 'end' ] : ok = True break if not ok : self . add_line_error ( line , { 'message' : 'This feature is not contained within the feature boundaries of parent: {0:s}: {1:s}' . format ( parent_feature [ 0 ] [ 'attributes' ] [ 'ID' ] , ',' . join ( [ '({0:s}, {1:d}, {2:d})' . format ( line [ 'seqid' ] , line [ 'start' ] , line [ 'end' ] ) for line in parent_feature ] ) ) , 'error_type' : 'BOUNDS' , 'location' : 'parent_boundary' } )
13403	def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
4929	def transform_launch_points ( self , content_metadata_item ) : return [ { 'providerID' : self . enterprise_configuration . provider_id , 'launchURL' : content_metadata_item [ 'enrollment_url' ] , 'contentTitle' : content_metadata_item [ 'title' ] , 'contentID' : self . get_content_id ( content_metadata_item ) , 'launchType' : 3 , 'mobileEnabled' : True , 'mobileLaunchURL' : content_metadata_item [ 'enrollment_url' ] , } ]
236	def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax
13356	def run_global_hook ( hook_name , * args ) : hook_finder = HookFinder ( get_global_hook_path ( ) ) hook = hook_finder ( hook_name ) if hook : hook . run ( * args )
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
13888	def DeleteDirectory ( directory , skip_on_error = False ) : _AssertIsLocal ( directory ) import shutil def OnError ( fn , path , excinfo ) : if IsLink ( path ) : return if fn is os . remove and os . access ( path , os . W_OK ) : raise import stat os . chmod ( path , stat . S_IWRITE ) fn ( path ) try : if not os . path . isdir ( directory ) : if skip_on_error : return from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( directory ) shutil . rmtree ( directory , onerror = OnError ) except : if not skip_on_error : raise
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
