6834	def vagrant ( self , name = '' ) : r = self . local_renderer config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) r . genv . update ( extra_args )
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
9511	def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
5860	def default ( self , obj ) : if obj is None : return [ ] elif isinstance ( obj , list ) : return [ i . as_dictionary ( ) for i in obj ] elif isinstance ( obj , dict ) : return self . _keys_to_camel_case ( obj ) else : return obj . as_dictionary ( )
2794	def get_object ( cls , api_token , image_id_or_slug ) : if cls . _is_string ( image_id_or_slug ) : image = cls ( token = api_token , slug = image_id_or_slug ) image . load ( use_slug = True ) else : image = cls ( token = api_token , id = image_id_or_slug ) image . load ( ) return image
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
12223	def execute ( self , args , kwargs ) : return self . lookup_explicit ( args , kwargs ) ( * args , ** kwargs )
8667	def put_key ( key_name , value , description , meta , modify , add , lock , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Stashing {0} key...' . format ( key_type ) ) stash . put ( name = key_name , value = _build_dict_from_key_value ( value ) , modify = modify , metadata = _build_dict_from_key_value ( meta ) , description = description , lock = lock , key_type = key_type , add = add ) click . echo ( 'Key stashed successfully' ) except GhostError as ex : sys . exit ( ex )
3993	def _load_ssh_auth_post_yosemite ( mac_username ) : user_id = subprocess . check_output ( [ 'id' , '-u' , mac_username ] ) ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'asuser' , user_id , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) _set_ssh_auth_sock ( ssh_auth_sock )
3010	def _get_scopes ( self ) : if _credentials_from_request ( self . request ) : return ( self . _scopes | _credentials_from_request ( self . request ) . scopes ) else : return self . _scopes
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
9672	def iteration ( self ) : i = 0 conv = np . inf old_conv = - np . inf conv_list = [ ] m = self . original if isinstance ( self . original , pd . DataFrame ) : ipfn_method = self . ipfn_df elif isinstance ( self . original , np . ndarray ) : ipfn_method = self . ipfn_np self . original = self . original . astype ( 'float64' ) else : print ( 'Data input instance not recognized' ) sys . exit ( 0 ) while ( ( i <= self . max_itr and conv > self . conv_rate ) and ( i <= self . max_itr and abs ( conv - old_conv ) > self . rate_tolerance ) ) : old_conv = conv m , conv = ipfn_method ( m , self . aggregates , self . dimensions , self . weight_col ) conv_list . append ( conv ) i += 1 converged = 1 if i <= self . max_itr : if not conv > self . conv_rate : print ( 'ipfn converged: convergence_rate below threshold' ) elif not abs ( conv - old_conv ) > self . rate_tolerance : print ( 'ipfn converged: convergence_rate not updating or below rate_tolerance' ) else : print ( 'Maximum iterations reached' ) converged = 0 if self . verbose == 0 : return m elif self . verbose == 1 : return m , converged elif self . verbose == 2 : return m , converged , pd . DataFrame ( { 'iteration' : range ( i ) , 'conv' : conv_list } ) . set_index ( 'iteration' ) else : print ( 'wrong verbose input, return None' ) sys . exit ( 0 )
1854	def SHLD ( cpu , dest , src , count ) : OperandSize = dest . size tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) MASK = ( ( 1 << OperandSize ) - 1 ) t0 = ( arg0 << tempCount ) t1 = arg1 >> ( OperandSize - tempCount ) res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) res = res & MASK dest . write ( res ) if isinstance ( tempCount , int ) and tempCount == 0 : pass else : SIGN_MASK = 1 << ( OperandSize - 1 ) lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount )
9610	def execute ( self , command , data = { } ) : method , uri = command try : path = self . _formatter . format_map ( uri , data ) body = self . _formatter . get_unused_kwargs ( ) url = "{0}{1}" . format ( self . _url , path ) return self . _request ( method , url , body ) except KeyError as err : LOGGER . debug ( 'Endpoint {0} is missing argument {1}' . format ( uri , err ) ) raise
5453	def _remove_empty_items ( d , required ) : new_dict = { } for k , v in d . items ( ) : if k in required : new_dict [ k ] = v elif isinstance ( v , int ) or v : new_dict [ k ] = v return new_dict
2021	def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
6306	def load_package ( self ) : try : self . package = importlib . import_module ( self . name ) except ModuleNotFoundError : raise ModuleNotFoundError ( "Effect package '{}' not found." . format ( self . name ) )
5594	def intersecting ( self , tile ) : return [ self . tile ( * intersecting_tile . id ) for intersecting_tile in self . tile_pyramid . intersecting ( tile ) ]
1770	def backup_emulate ( self , insn ) : if not hasattr ( self , 'backup_emu' ) : self . backup_emu = UnicornEmulator ( self ) try : self . backup_emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) ) finally : del self . backup_emu
10610	def _calculate_H ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H = H + dH return H
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
1111	def _qformat ( self , aline , bline , atags , btags ) : r common = min ( _count_leading ( aline , "\t" ) , _count_leading ( bline , "\t" ) ) common = min ( common , _count_leading ( atags [ : common ] , " " ) ) common = min ( common , _count_leading ( btags [ : common ] , " " ) ) atags = atags [ common : ] . rstrip ( ) btags = btags [ common : ] . rstrip ( ) yield "- " + aline if atags : yield "? %s%s\n" % ( "\t" * common , atags ) yield "+ " + bline if btags : yield "? %s%s\n" % ( "\t" * common , btags )
2943	def validate ( self ) : results = [ ] from . . specs import Join def recursive_find_loop ( task , history ) : current = history [ : ] current . append ( task ) if isinstance ( task , Join ) : if task in history : msg = "Found loop with '%s': %s then '%s' again" % ( task . name , '->' . join ( [ p . name for p in history ] ) , task . name ) raise Exception ( msg ) for predecessor in task . inputs : recursive_find_loop ( predecessor , current ) for parent in task . inputs : recursive_find_loop ( parent , current ) for task_id , task in list ( self . task_specs . items ( ) ) : try : recursive_find_loop ( task , [ ] ) except Exception as exc : results . append ( exc . __str__ ( ) ) if not task . inputs and task . name not in [ 'Start' , 'Root' ] : if task . outputs : results . append ( "Task '%s' is disconnected (no inputs)" % task . name ) else : LOG . debug ( "Task '%s' is not being used" % task . name ) return results
13231	def get_def_macros ( tex_source ) : r macros = { } for match in DEF_PATTERN . finditer ( tex_source ) : macros [ match . group ( 'name' ) ] = match . group ( 'content' ) return macros
12247	def create_bucket ( self , * args , ** kwargs ) : bucket = super ( S3Connection , self ) . create_bucket ( * args , ** kwargs ) if bucket : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return bucket
4373	def create ( ) : name = request . form . get ( "name" ) if name : room , created = get_or_create ( ChatRoom , name = name ) return redirect ( url_for ( 'room' , slug = room . slug ) ) return redirect ( url_for ( 'rooms' ) )
1261	def get_component ( self , component_name ) : mapping = self . get_components ( ) return mapping [ component_name ] if component_name in mapping else None
8220	def do_window_close ( self , widget , data = None ) : publish_event ( QUIT_EVENT ) if self . has_server : self . sock . close ( ) self . hide_variables_window ( ) self . destroy ( ) self . window_open = False
6364	def to_tuple ( self ) : return self . _tp , self . _tn , self . _fp , self . _fn
10696	def hsv_to_rgb ( hsv ) : h , s , v = hsv c = v * s h /= 60 x = c * ( 1 - abs ( ( h % 2 ) - 1 ) ) m = v - c if h < 1 : res = ( c , x , 0 ) elif h < 2 : res = ( x , c , 0 ) elif h < 3 : res = ( 0 , c , x ) elif h < 4 : res = ( 0 , x , c ) elif h < 5 : res = ( x , 0 , c ) elif h < 6 : res = ( c , 0 , x ) else : raise ColorException ( "Unable to convert from HSV to RGB" ) r , g , b = res return round ( ( r + m ) * 255 , 3 ) , round ( ( g + m ) * 255 , 3 ) , round ( ( b + m ) * 255 , 3 )
13664	def set_item ( filename , item ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : products_data = json . load ( products_file ) uuid_list = [ i for i in filter ( lambda z : z [ "uuid" ] == str ( item [ "uuid" ] ) , products_data ) ] if len ( uuid_list ) == 0 : products_data . append ( item ) json . dump ( products_data , temp_file ) return True return None
10225	def get_correlation_graph ( graph : BELGraph ) -> Graph : result = Graph ( ) for u , v , d in graph . edges ( data = True ) : if d [ RELATION ] not in CORRELATIVE_RELATIONS : continue if not result . has_edge ( u , v ) : result . add_edge ( u , v , ** { d [ RELATION ] : True } ) elif d [ RELATION ] not in result [ u ] [ v ] : log . log ( 5 , 'broken correlation relation for %s, %s' , u , v ) result [ u ] [ v ] [ d [ RELATION ] ] = True result [ v ] [ u ] [ d [ RELATION ] ] = True return result
2024	def GT ( self , a , b ) : return Operators . ITEBV ( 256 , Operators . UGT ( a , b ) , 1 , 0 )
12005	def _remove_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) header_size = version_info [ 'header_size' ] if options [ 'flags' ] [ 'timestamp' ] : header_size += version_info [ 'timestamp_size' ] data = data [ header_size : ] return data
9293	def db_value ( self , value ) : value = self . transform_value ( value ) return self . hhash . encrypt ( value , salt_size = self . salt_size , rounds = self . rounds )
5016	def transmit ( self , payload , ** kwargs ) : IntegratedChannelLearnerDataTransmissionAudit = apps . get_model ( app_label = kwargs . get ( 'app_label' , 'integrated_channel' ) , model_name = kwargs . get ( 'model_name' , 'LearnerDataTransmissionAudit' ) , ) for learner_data in payload . export ( ) : serialized_payload = learner_data . serialize ( enterprise_configuration = self . enterprise_configuration ) LOGGER . debug ( 'Attempting to transmit serialized payload: %s' , serialized_payload ) enterprise_enrollment_id = learner_data . enterprise_course_enrollment_id if learner_data . completed_timestamp is None : LOGGER . info ( 'Skipping in-progress enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue previous_transmissions = IntegratedChannelLearnerDataTransmissionAudit . objects . filter ( enterprise_course_enrollment_id = enterprise_enrollment_id , error_message = '' ) if previous_transmissions . exists ( ) : LOGGER . info ( 'Skipping previously sent enterprise enrollment {}' . format ( enterprise_enrollment_id ) ) continue try : code , body = self . client . create_course_completion ( getattr ( learner_data , kwargs . get ( 'remote_user_id' ) ) , serialized_payload ) LOGGER . info ( 'Successfully sent completion status call for enterprise enrollment {}' . format ( enterprise_enrollment_id , ) ) except RequestException as request_exception : code = 500 body = str ( request_exception ) self . handle_transmission_error ( learner_data , request_exception ) learner_data . status = str ( code ) learner_data . error_message = body if code >= 400 else '' learner_data . save ( )
7517	def snpcount_numba ( superints , snpsarr ) : for iloc in xrange ( superints . shape [ 0 ] ) : for site in xrange ( superints . shape [ 2 ] ) : catg = np . zeros ( 4 , dtype = np . int16 ) ncol = superints [ iloc , : , site ] for idx in range ( ncol . shape [ 0 ] ) : if ncol [ idx ] == 67 : catg [ 0 ] += 1 elif ncol [ idx ] == 65 : catg [ 1 ] += 1 elif ncol [ idx ] == 84 : catg [ 2 ] += 1 elif ncol [ idx ] == 71 : catg [ 3 ] += 1 elif ncol [ idx ] == 82 : catg [ 1 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 75 : catg [ 2 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 83 : catg [ 0 ] += 1 catg [ 3 ] += 1 elif ncol [ idx ] == 89 : catg [ 0 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 87 : catg [ 1 ] += 1 catg [ 2 ] += 1 elif ncol [ idx ] == 77 : catg [ 0 ] += 1 catg [ 1 ] += 1 catg . sort ( ) if not catg [ 2 ] : pass else : if catg [ 2 ] > 1 : snpsarr [ iloc , site , 1 ] = True else : snpsarr [ iloc , site , 0 ] = True return snpsarr
2147	def _configuration ( self , kwargs , config_item ) : if 'notification_configuration' not in config_item : if 'notification_type' not in kwargs : return nc = kwargs [ 'notification_configuration' ] = { } for field in Resource . configuration [ kwargs [ 'notification_type' ] ] : if field not in config_item : raise exc . TowerCLIError ( 'Required config field %s not' ' provided.' % field ) else : nc [ field ] = config_item [ field ] else : kwargs [ 'notification_configuration' ] = config_item [ 'notification_configuration' ]
2408	def extract_features_and_generate_model ( essays , algorithm = util_functions . AlgorithmTypes . regression ) : f = feature_extractor . FeatureExtractor ( ) f . initialize_dictionaries ( essays ) train_feats = f . gen_feats ( essays ) set_score = numpy . asarray ( essays . _score , dtype = numpy . int ) if len ( util_functions . f7 ( list ( set_score ) ) ) > 5 : algorithm = util_functions . AlgorithmTypes . regression else : algorithm = util_functions . AlgorithmTypes . classification clf , clf2 = get_algorithms ( algorithm ) cv_error_results = get_cv_error ( clf2 , train_feats , essays . _score ) try : clf . fit ( train_feats , set_score ) except ValueError : log . exception ( "Not enough classes (0,1,etc) in sample." ) set_score [ 0 ] = 1 set_score [ 1 ] = 0 clf . fit ( train_feats , set_score ) return f , clf , cv_error_results
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
640	def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )
10564	def get_supported_filepaths ( filepaths , supported_extensions , max_depth = float ( 'inf' ) ) : supported_filepaths = [ ] for path in filepaths : if os . name == 'nt' and CYGPATH_RE . match ( path ) : path = convert_cygwin_path ( path ) if os . path . isdir ( path ) : for root , __ , files in walk_depth ( path , max_depth ) : for f in files : if f . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( os . path . join ( root , f ) ) elif os . path . isfile ( path ) and path . lower ( ) . endswith ( supported_extensions ) : supported_filepaths . append ( path ) return supported_filepaths
10116	def append ( self , key , value = MARKER , replace = True ) : return self . add_item ( key , value , replace = replace )
11716	def edit ( self , config , etag ) : data = self . _json_encode ( config ) headers = self . _default_headers ( ) if etag is not None : headers [ "If-Match" ] = etag return self . _request ( self . name , ok_status = None , data = data , headers = headers , method = "PUT" )
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
12187	async def handle_message ( self , message , filters ) : data = self . _unpack_message ( message ) logger . debug ( data ) if data . get ( 'type' ) == 'error' : raise SlackApiError ( data . get ( 'error' , { } ) . get ( 'msg' , str ( data ) ) ) elif self . message_is_to_me ( data ) : text = data [ 'text' ] [ len ( self . address_as ) : ] . strip ( ) if text == 'help' : return self . _respond ( channel = data [ 'channel' ] , text = self . _instruction_list ( filters ) , ) elif text == 'version' : return self . _respond ( channel = data [ 'channel' ] , text = self . VERSION , ) for _filter in filters : if _filter . matches ( data ) : logger . debug ( 'Response triggered' ) async for response in _filter : self . _respond ( channel = data [ 'channel' ] , text = response )
1513	def wait_for_master_to_start ( single_master ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) if r . status_code == 200 : break except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for cluster to come up... %s" % i ) time . sleep ( 1 ) if i > 10 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
948	def run ( self ) : self . __logger . debug ( "run(): Starting task <%s>" , self . __task [ 'taskLabel' ] ) if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) self . __model . resetSequenceStates ( ) self . __taskDriver . setup ( ) while True : try : next ( iterTracker ) except StopIteration : break try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) periodic . tick ( ) self . _getAndEmitExperimentMetrics ( final = True ) self . __taskDriver . finalize ( ) self . __model . resetSequenceStates ( )
6665	def verify_certificate_chain ( self , base = None , crt = None , csr = None , key = None ) : from burlap . common import get_verbose , print_fail , print_success r = self . local_renderer if base : crt = base + '.crt' csr = base + '.csr' key = base + '.key' else : assert crt and csr and key , 'If base not provided, crt and csr and key must be given.' assert os . path . isfile ( crt ) assert os . path . isfile ( csr ) assert os . path . isfile ( key ) csr_md5 = r . local ( 'openssl req -noout -modulus -in %s | openssl md5' % csr , capture = True ) key_md5 = r . local ( 'openssl rsa -noout -modulus -in %s | openssl md5' % key , capture = True ) crt_md5 = r . local ( 'openssl x509 -noout -modulus -in %s | openssl md5' % crt , capture = True ) match = crt_md5 == csr_md5 == key_md5 if self . verbose or not match : print ( 'crt:' , crt_md5 ) print ( 'csr:' , csr_md5 ) print ( 'key:' , key_md5 ) if match : print_success ( 'Files look good!' ) else : print_fail ( 'Files no not match!' ) raise Exception ( 'Files no not match!' )
10044	def default_view_method ( pid , record , template = None ) : record_viewed . send ( current_app . _get_current_object ( ) , pid = pid , record = record , ) deposit_type = request . values . get ( 'type' ) return render_template ( template , pid = pid , record = record , jsonschema = current_deposit . jsonschemas [ deposit_type ] , schemaform = current_deposit . schemaforms [ deposit_type ] , )
8410	def best_units ( self , sequence ) : ts_range = self . value ( max ( sequence ) ) - self . value ( min ( sequence ) ) package = self . determine_package ( sequence [ 0 ] ) if package == 'pandas' : cuts = [ ( 0.9 , 'us' ) , ( 0.9 , 'ms' ) , ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = NANOSECONDS base_units = 'ns' else : cuts = [ ( 0.9 , 's' ) , ( 9 , 'm' ) , ( 6 , 'h' ) , ( 4 , 'd' ) , ( 4 , 'w' ) , ( 4 , 'M' ) , ( 3 , 'y' ) ] denomination = SECONDS base_units = 'ms' for size , units in reversed ( cuts ) : if ts_range >= size * denomination [ units ] : return units return base_units
1728	def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
103	def compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( aspect_ratio > 0 ) height , width = arr . shape [ 0 : 2 ] do_assert ( height > 0 ) aspect_ratio_current = width / height pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 if aspect_ratio_current < aspect_ratio : diff = ( aspect_ratio * height ) - width pad_right = int ( np . ceil ( diff / 2 ) ) pad_left = int ( np . floor ( diff / 2 ) ) elif aspect_ratio_current > aspect_ratio : diff = ( ( 1 / aspect_ratio ) * width ) - height pad_top = int ( np . floor ( diff / 2 ) ) pad_bottom = int ( np . ceil ( diff / 2 ) ) return pad_top , pad_right , pad_bottom , pad_left
8603	def create_user ( self , user ) : data = self . _create_user_dict ( user = user ) response = self . _perform_request ( url = '/um/users' , method = 'POST' , data = json . dumps ( data ) ) return response
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
9773	def statuses ( ctx , page ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) page = page or 1 try : response = PolyaxonClient ( ) . job . get_statuses ( user , project_name , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True )
3118	def _create_file_if_needed ( self ) : if not os . path . exists ( self . _filename ) : old_umask = os . umask ( 0o177 ) try : open ( self . _filename , 'a+b' ) . close ( ) finally : os . umask ( old_umask )
13382	def dict_to_env ( d , pathsep = os . pathsep ) : out_env = { } for k , v in d . iteritems ( ) : if isinstance ( v , list ) : out_env [ k ] = pathsep . join ( v ) elif isinstance ( v , string_types ) : out_env [ k ] = v else : raise TypeError ( '{} not a valid env var type' . format ( type ( v ) ) ) return out_env
11142	def remove_repository ( self , path = None , removeEmptyDirs = True ) : assert isinstance ( removeEmptyDirs , bool ) , "removeEmptyDirs must be boolean" if path is not None : if path != self . __path : repo = Repository ( ) repo . load_repository ( path ) else : repo = self else : repo = self assert repo . path is not None , "path is not given and repository is not initialized" for fdict in reversed ( repo . get_repository_state ( ) ) : relaPath = list ( fdict ) [ 0 ] realPath = os . path . join ( repo . path , relaPath ) path , name = os . path . split ( realPath ) if fdict [ relaPath ] [ 'type' ] == 'file' : if os . path . isfile ( realPath ) : os . remove ( realPath ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileLock % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileLock % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileClass % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileClass % name ) ) elif fdict [ relaPath ] [ 'type' ] == 'dir' : if os . path . isfile ( os . path . join ( realPath , self . __dirInfo ) ) : os . remove ( os . path . join ( realPath , self . __dirInfo ) ) if os . path . isfile ( os . path . join ( realPath , self . __dirLock ) ) : os . remove ( os . path . join ( realPath , self . __dirLock ) ) if not len ( os . listdir ( realPath ) ) and removeEmptyDirs : shutil . rmtree ( realPath ) if os . path . isfile ( os . path . join ( repo . path , self . __repoFile ) ) : os . remove ( os . path . join ( repo . path , self . __repoFile ) ) if os . path . isfile ( os . path . join ( repo . path , self . __repoLock ) ) : os . remove ( os . path . join ( repo . path , self . __repoLock ) )
7073	def magbin_varind_gridsearch_worker ( task ) : simbasedir , gridpoint , magbinmedian = task try : res = get_recovered_variables_for_magbin ( simbasedir , magbinmedian , stetson_stdev_min = gridpoint [ 0 ] , inveta_stdev_min = gridpoint [ 1 ] , iqr_stdev_min = gridpoint [ 2 ] , statsonly = True ) return res except Exception as e : LOGEXCEPTION ( 'failed to get info for %s' % gridpoint ) return None
6101	def intensities_from_grid_radii ( self , grid_radii ) : np . seterr ( all = 'ignore' ) return np . multiply ( self . intensity , np . exp ( np . multiply ( - self . sersic_constant , np . add ( np . power ( np . divide ( grid_radii , self . effective_radius ) , 1. / self . sersic_index ) , - 1 ) ) ) )
4906	def delete_course_completion ( self , user_id , payload ) : return self . _delete ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
12279	def add ( repo , args , targetdir , execute = False , generator = False , includes = [ ] , script = False , source = None ) : if not execute : files = add_files ( args = args , targetdir = targetdir , source = source , script = script , generator = generator ) else : files = run_executable ( repo , args , includes ) if files is None or len ( files ) == 0 : return repo filtered_files = [ ] package = repo . package for h in files : found = False for i , r in enumerate ( package [ 'resources' ] ) : if h [ 'relativepath' ] == r [ 'relativepath' ] : found = True if h [ 'sha256' ] == r [ 'sha256' ] : change = False for attr in [ 'source' ] : if h [ attr ] != r [ attr ] : r [ attr ] = h [ attr ] change = True if change : filtered_files . append ( h ) continue else : filtered_files . append ( h ) package [ 'resources' ] [ i ] = h break if not found : filtered_files . append ( h ) package [ 'resources' ] . append ( h ) if len ( filtered_files ) == 0 : return 0 repo . manager . add_files ( repo , filtered_files ) rootdir = repo . rootdir with cd ( rootdir ) : datapath = "datapackage.json" with open ( datapath , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) return len ( filtered_files )
1851	def RCL ( cpu , dest , src ) : OperandSize = dest . size count = src . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) value = dest . read ( ) if isinstance ( tempCount , int ) and tempCount == 0 : new_val = value dest . write ( new_val ) else : carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) right = value >> ( OperandSize - tempCount ) new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) dest . write ( new_val ) def sf ( v , size ) : return ( v & ( 1 << ( size - 1 ) ) ) != 0 cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) cpu . OF = Operators . ITE ( tempCount == 1 , sf ( new_val , OperandSize ) != cpu . CF , cpu . OF )
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
1993	def load_state ( self , state_id , delete = True ) : return self . _store . load_state ( f'{self._prefix}{state_id:08x}{self._suffix}' , delete = delete )
13771	def collect_links ( self , env = None ) : for asset in self . assets . values ( ) : if asset . has_bundles ( ) : asset . collect_files ( ) if env is None : env = self . config . env if env == static_bundle . ENV_PRODUCTION : self . _minify ( emulate = True ) self . _add_url_prefix ( )
9590	def switch_to_window ( self , window_name ) : data = { 'name' : window_name } self . _execute ( Command . SWITCH_TO_WINDOW , data )
3509	def constraint_matrices ( model , array_type = 'dense' , include_vars = False , zero_tol = 1e-6 ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) array_builder = { 'dense' : np . array , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : pd . DataFrame , } [ array_type ] Problem = namedtuple ( "Problem" , [ "equalities" , "b" , "inequalities" , "bounds" , "variable_fixed" , "variable_bounds" ] ) equality_rows = [ ] inequality_rows = [ ] inequality_bounds = [ ] b = [ ] for const in model . constraints : lb = - np . inf if const . lb is None else const . lb ub = np . inf if const . ub is None else const . ub equality = ( ub - lb ) < zero_tol coefs = const . get_linear_coefficients ( model . variables ) coefs = [ coefs [ v ] for v in model . variables ] if equality : b . append ( lb if abs ( lb ) > zero_tol else 0.0 ) equality_rows . append ( coefs ) else : inequality_rows . append ( coefs ) inequality_bounds . append ( [ lb , ub ] ) var_bounds = np . array ( [ [ v . lb , v . ub ] for v in model . variables ] ) fixed = var_bounds [ : , 1 ] - var_bounds [ : , 0 ] < zero_tol results = Problem ( equalities = array_builder ( equality_rows ) , b = np . array ( b ) , inequalities = array_builder ( inequality_rows ) , bounds = array_builder ( inequality_bounds ) , variable_fixed = np . array ( fixed ) , variable_bounds = array_builder ( var_bounds ) ) return results
5446	def _parse_local_mount_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _local_uri_rewriter ( raw_uri ) local_path = docker_path [ len ( 'file' ) : ] docker_uri = os . path . join ( self . _relative_path , docker_path ) return local_path , docker_uri
12444	def route ( self , request , response ) : self . require_http_allowed_method ( request ) function = getattr ( self , request . method . lower ( ) , None ) if function is None : raise http . exceptions . NotImplemented ( ) return function ( request , response )
5784	def select_write ( self , timeout = None ) : _ , write_ready , _ = select . select ( [ ] , [ self . _socket ] , [ ] , timeout ) return len ( write_ready ) > 0
11511	def set_item_metadata ( self , token , item_id , element , value , qualifier = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id parameters [ 'element' ] = element parameters [ 'value' ] = value if qualifier : parameters [ 'qualifier' ] = qualifier response = self . request ( 'midas.item.setmetadata' , parameters ) return response
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
4262	def save_cache ( gallery ) : if hasattr ( gallery , "exifCache" ) : cache = gallery . exifCache else : cache = gallery . exifCache = { } for album in gallery . albums . values ( ) : for image in album . images : cache [ os . path . join ( image . path , image . filename ) ] = image . exif cachePath = os . path . join ( gallery . settings [ "destination" ] , ".exif_cache" ) if len ( cache ) == 0 : if os . path . exists ( cachePath ) : os . remove ( cachePath ) return try : with open ( cachePath , "wb" ) as cacheFile : pickle . dump ( cache , cacheFile ) logger . debug ( "Stored cache with %d entries" , len ( gallery . exifCache ) ) except Exception as e : logger . warn ( "Could not store cache: %s" , e ) os . remove ( cachePath )
9778	def login ( token , username , password ) : auth_client = PolyaxonClient ( ) . auth if username : if not password : password = click . prompt ( 'Please enter your password' , type = str , hide_input = True ) password = password . strip ( ) if not password : logger . info ( 'You entered an empty string. ' 'Please make sure you enter your password correctly.' ) sys . exit ( 1 ) credentials = CredentialsConfig ( username = username , password = password ) try : access_code = auth_client . login ( credentials = credentials ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not login.' ) Printer . print_error ( 'Error Message `{}`.' . format ( e ) ) sys . exit ( 1 ) if not access_code : Printer . print_error ( "Failed to login" ) return else : if not token : token_url = "{}/app/token" . format ( auth_client . config . http_host ) click . confirm ( 'Authentication token page will now open in your browser. Continue?' , abort = True , default = True ) click . launch ( token_url ) logger . info ( "Please copy and paste the authentication token." ) token = click . prompt ( 'This is an invisible field. Paste token and press ENTER' , type = str , hide_input = True ) if not token : logger . info ( "Empty token received. " "Make sure your shell is handling the token appropriately." ) logger . info ( "See docs for help: http://docs.polyaxon.com/polyaxon_cli/commands/auth" ) return access_code = token . strip ( " " ) try : AuthConfigManager . purge ( ) user = PolyaxonClient ( ) . auth . get_user ( token = access_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load user info.' ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) access_token = AccessTokenConfig ( username = user . username , token = access_code ) AuthConfigManager . set_config ( access_token ) Printer . print_success ( "Login successful" ) server_version = get_server_version ( ) current_version = get_current_version ( ) log_handler = get_log_handler ( ) CliConfigManager . reset ( check_count = 0 , current_version = current_version , min_version = server_version . min_version , log_handler = log_handler )
3180	def update ( self , batch_webhook_id , data ) : self . batch_webhook_id = batch_webhook_id if 'url' not in data : raise KeyError ( 'The batch webhook must have a valid url' ) return self . _mc_client . _patch ( url = self . _build_path ( batch_webhook_id ) , data = data )
12021	def add_line_error ( self , line_data , error_info , log_level = logging . ERROR ) : if not error_info : return try : line_data [ 'line_errors' ] . append ( error_info ) except KeyError : line_data [ 'line_errors' ] = [ error_info ] except TypeError : pass try : self . logger . log ( log_level , Gff3 . error_format . format ( current_line_num = line_data [ 'line_index' ] + 1 , error_type = error_info [ 'error_type' ] , message = error_info [ 'message' ] , line = line_data [ 'line_raw' ] . rstrip ( ) ) ) except AttributeError : pass
3145	def delete ( self , file_id ) : self . file_id = file_id return self . _mc_client . _delete ( url = self . _build_path ( file_id ) )
13854	def append_main_thread ( self ) : thread = MainThread ( main_queue = self . main_queue , main_spider = self . main_spider , branch_spider = self . branch_spider ) thread . daemon = True thread . start ( )
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
5679	def get_all_route_shapes ( self , use_shapes = True ) : cur = self . conn . cursor ( ) query = "SELECT routes.name as name, shape_id, route_I, trip_I, routes.type, " " agency_id, agencies.name as agency_name, max(end_time_ds-start_time_ds) as trip_duration " "FROM trips " "LEFT JOIN routes " "USING(route_I) " "LEFT JOIN agencies " "USING(agency_I) " "GROUP BY routes.route_I" data = pd . read_sql_query ( query , self . conn ) routeShapes = [ ] for i , row in enumerate ( data . itertuples ( ) ) : datum = { "name" : str ( row . name ) , "type" : int ( row . type ) , "route_I" : row . route_I , "agency" : str ( row . agency_id ) , "agency_name" : str ( row . agency_name ) } if use_shapes and row . shape_id : shape = shapes . get_shape_points2 ( cur , row . shape_id ) lats = shape [ 'lats' ] lons = shape [ 'lons' ] else : stop_shape = self . get_trip_stop_coordinates ( row . trip_I ) lats = list ( stop_shape [ 'lat' ] ) lons = list ( stop_shape [ 'lon' ] ) datum [ 'lats' ] = [ float ( lat ) for lat in lats ] datum [ 'lons' ] = [ float ( lon ) for lon in lons ] routeShapes . append ( datum ) return routeShapes
12256	def lbfgs ( x , rho , f_df , maxiter = 20 ) : def f_df_augmented ( theta ) : f , df = f_df ( theta ) obj = f + ( rho / 2. ) * np . linalg . norm ( theta - x ) ** 2 grad = df + rho * ( theta - x ) return obj , grad res = scipy_minimize ( f_df_augmented , x , jac = True , method = 'L-BFGS-B' , options = { 'maxiter' : maxiter , 'disp' : False } ) return res . x
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
11985	def upload_folder ( self , bucket , folder , key = None , skip = None , content_types = None ) : uploader = FolderUploader ( self , bucket , folder , key , skip , content_types ) return uploader . start ( )
10355	def get_largest_component ( graph : BELGraph ) -> BELGraph : biggest_component_nodes = max ( nx . weakly_connected_components ( graph ) , key = len ) return subgraph ( graph , biggest_component_nodes )
7178	def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driver ( grammar , pytree . convert ) if src_txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src_txt [ : 1024 ] else '\n' src_txt += nl try : result = drv . parse_string ( src_txt , True ) except ParseError as pe : lineno , column = pe . context [ 1 ] lines = src_txt . splitlines ( ) try : faulty_line = lines [ lineno - 1 ] except IndexError : faulty_line = "<line number missing in source>" raise ValueError ( f"Cannot parse: {lineno}:{column}: {faulty_line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file_input , [ result ] ) return result
13647	def get_fuel_prices ( self ) -> GetFuelPricesResponse : response = requests . get ( '{}/prices' . format ( API_URL_BASE ) , headers = self . _get_headers ( ) , timeout = self . _timeout , ) if not response . ok : raise FuelCheckError . create ( response ) return GetFuelPricesResponse . deserialize ( response . json ( ) )
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : gene = cobra_model . genes [ gene_index ] cobra_model . genes . _dict . pop ( gene . id ) gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
8380	def drag ( self , node ) : dx = self . mouse . x - self . graph . x dy = self . mouse . y - self . graph . y s = self . graph . styles . default self . _ctx . nofill ( ) self . _ctx . nostroke ( ) if s . stroke : self . _ctx . strokewidth ( s . strokewidth ) self . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . g , 0.75 ) p = self . _ctx . line ( node . x , node . y , dx , dy , draw = False ) try : p . _nsBezierPath . setLineDash_count_phase_ ( [ 2 , 4 ] , 2 , 50 ) except : pass self . _ctx . drawpath ( p ) r = node . __class__ ( None ) . r * 0.75 self . _ctx . oval ( dx - r / 2 , dy - r / 2 , r , r ) node . vx = dx / self . graph . d node . vy = dy / self . graph . d
12588	def insert_volumes_in_one_dataset ( file_path , h5path , file_list , newshape = None , concat_axis = 0 , dtype = None , append = True ) : def isalambda ( v ) : return isinstance ( v , type ( lambda : None ) ) and v . __name__ == '<lambda>' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' imgs = [ nib . load ( vol ) for vol in file_list ] shapes = [ np . array ( img . get_shape ( ) ) for img in imgs ] if newshape is not None : if isalambda ( newshape ) : nushapes = np . array ( [ newshape ( shape ) for shape in shapes ] ) else : nushapes = np . array ( [ shape for shape in shapes ] ) for nushape in nushapes : assert ( len ( nushape ) - 1 < concat_axis ) n_dims = nushapes . shape [ 1 ] ds_shape = np . zeros ( n_dims , dtype = np . int ) for a in list ( range ( n_dims ) ) : if a == concat_axis : ds_shape [ a ] = np . sum ( nushapes [ : , concat_axis ] ) else : ds_shape [ a ] = np . max ( nushapes [ : , a ] ) if dtype is None : dtype = imgs [ 0 ] . get_data_dtype ( ) with h5py . File ( file_path , mode ) as f : try : ic = 0 h5grp = f . create_group ( os . path . dirname ( h5path ) ) h5ds = h5grp . create_dataset ( os . path . basename ( h5path ) , ds_shape , dtype ) for img in imgs : nushape = nushapes [ ic , : ] def append_to_dataset ( h5ds , idx , data , concat_axis ) : shape = data . shape ndims = len ( shape ) if ndims == 1 : if concat_axis == 0 : h5ds [ idx ] = data elif ndims == 2 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif ndims == 3 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif concat_axis == 2 : h5ds [ idx ] = data append_to_dataset ( h5ds , ic , np . reshape ( img . get_data ( ) , tuple ( nushape ) ) , concat_axis ) ic += 1 except ValueError as ve : raise Exception ( 'Error creating group {} in hdf file {}' . format ( h5path , file_path ) ) from ve
3420	def create_mat_dict ( model ) : rxns = model . reactions mets = model . metabolites mat = OrderedDict ( ) mat [ "mets" ] = _cell ( [ met_id for met_id in create_mat_metabolite_id ( model ) ] ) mat [ "metNames" ] = _cell ( mets . list_attr ( "name" ) ) mat [ "metFormulas" ] = _cell ( [ str ( m . formula ) for m in mets ] ) try : mat [ "metCharge" ] = array ( mets . list_attr ( "charge" ) ) * 1. except TypeError : pass mat [ "genes" ] = _cell ( model . genes . list_attr ( "id" ) ) rxn_gene = scipy_sparse . dok_matrix ( ( len ( model . reactions ) , len ( model . genes ) ) ) if min ( rxn_gene . shape ) > 0 : for i , reaction in enumerate ( model . reactions ) : for gene in reaction . genes : rxn_gene [ i , model . genes . index ( gene ) ] = 1 mat [ "rxnGeneMat" ] = rxn_gene mat [ "grRules" ] = _cell ( rxns . list_attr ( "gene_reaction_rule" ) ) mat [ "rxns" ] = _cell ( rxns . list_attr ( "id" ) ) mat [ "rxnNames" ] = _cell ( rxns . list_attr ( "name" ) ) mat [ "subSystems" ] = _cell ( rxns . list_attr ( "subsystem" ) ) stoich_mat = create_stoichiometric_matrix ( model ) mat [ "S" ] = stoich_mat if stoich_mat is not None else [ [ ] ] mat [ "lb" ] = array ( rxns . list_attr ( "lower_bound" ) ) * 1. mat [ "ub" ] = array ( rxns . list_attr ( "upper_bound" ) ) * 1. mat [ "b" ] = array ( mets . list_attr ( "_bound" ) ) * 1. mat [ "c" ] = array ( rxns . list_attr ( "objective_coefficient" ) ) * 1. mat [ "rev" ] = array ( rxns . list_attr ( "reversibility" ) ) * 1 mat [ "description" ] = str ( model . id ) return mat
3012	def locked_put ( self , credentials ) : filters = { self . key_name : self . key_value } query = self . session . query ( self . model_class ) . filter_by ( ** filters ) entity = query . first ( ) if not entity : entity = self . model_class ( ** filters ) setattr ( entity , self . property_name , credentials ) self . session . add ( entity )
8590	def reboot_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/reboot' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
3973	def _get_ports_list ( app_name , port_specs ) : if app_name not in port_specs [ 'docker_compose' ] : return [ ] return [ "{}:{}" . format ( port_spec [ 'mapped_host_port' ] , port_spec [ 'in_container_port' ] ) for port_spec in port_specs [ 'docker_compose' ] [ app_name ] ]
2471	def set_file_atrificat_of_project ( self , doc , symbol , value ) : if self . has_package ( doc ) and self . has_file ( doc ) : self . file ( doc ) . add_artifact ( symbol , value ) else : raise OrderError ( 'File::Artificat' )
12300	def search ( self , what , name = None , version = None ) : filtered = { } if what is None : whats = list ( self . plugins . keys ( ) ) elif what is not None : if what not in self . plugins : raise Exception ( "Unknown class of plugins" ) whats = [ what ] for what in whats : if what not in filtered : filtered [ what ] = [ ] for key in self . plugins [ what ] . keys ( ) : ( k_name , k_version ) = key if name is not None and k_name != name : continue if version is not None and k_version != version : continue if self . plugins [ what ] [ key ] . enable == 'n' : continue filtered [ what ] . append ( key ) return filtered
4741	def paths_from_env ( prefix = None , names = None ) : def expand_path ( path ) : return os . path . abspath ( os . path . expanduser ( os . path . expandvars ( path ) ) ) if prefix is None : prefix = "CIJ" if names is None : names = [ "ROOT" , "ENVS" , "TESTPLANS" , "TESTCASES" , "TESTSUITES" , "MODULES" , "HOOKS" , "TEMPLATES" ] conf = { v : os . environ . get ( "_" . join ( [ prefix , v ] ) ) for v in names } for env in ( e for e in conf . keys ( ) if e [ : len ( prefix ) ] in names and conf [ e ] ) : conf [ env ] = expand_path ( conf [ env ] ) if not os . path . exists ( conf [ env ] ) : err ( "%s_%s: %r, does not exist" % ( prefix , env , conf [ env ] ) ) return conf
162	def get_pointwise_inside_image_mask ( self , image ) : if len ( self . coords ) == 0 : return np . zeros ( ( 0 , ) , dtype = bool ) shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] x_within = np . logical_and ( 0 <= self . xx , self . xx < width ) y_within = np . logical_and ( 0 <= self . yy , self . yy < height ) return np . logical_and ( x_within , y_within )
11975	def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
5764	def _unarmor_pem ( data , password = None ) : object_type , headers , der_bytes = pem . unarmor ( data ) type_regex = '^((DSA|EC|RSA) PRIVATE KEY|ENCRYPTED PRIVATE KEY|PRIVATE KEY|PUBLIC KEY|RSA PUBLIC KEY|CERTIFICATE)' armor_type = re . match ( type_regex , object_type ) if not armor_type : raise ValueError ( pretty_message ( ) ) pem_header = armor_type . group ( 1 ) data = data . strip ( ) if pem_header in set ( [ 'RSA PRIVATE KEY' , 'DSA PRIVATE KEY' , 'EC PRIVATE KEY' ] ) : algo = armor_type . group ( 2 ) . lower ( ) return ( 'private key' , algo , _unarmor_pem_openssl_private ( headers , der_bytes , password ) ) key_type = pem_header . lower ( ) algo = None if key_type == 'encrypted private key' : key_type = 'private key' elif key_type == 'rsa public key' : key_type = 'public key' algo = 'rsa' return ( key_type , algo , der_bytes )
1571	def submit_tar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] java_defines = cl_args [ 'topology_main_jvm_property' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_tar ( main_class , topology_file , tuple ( unknown_args ) , tmp_dir , java_defines ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res return launch_topologies ( cl_args , topology_file , tmp_dir )
7603	def get_player ( self , tag : crtag , timeout = None ) : url = self . api . PLAYER + '/' + tag return self . _get_model ( url , FullPlayer , timeout = timeout )
839	def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats
8860	def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
4540	def advance_permutation ( a , increasing = True , forward = True ) : if not forward : a . reverse ( ) cmp = operator . lt if increasing else operator . gt try : i = next ( i for i in reversed ( range ( len ( a ) - 1 ) ) if cmp ( a [ i ] , a [ i + 1 ] ) ) j = next ( j for j in reversed ( range ( i + 1 , len ( a ) ) ) if cmp ( a [ i ] , a [ j ] ) ) except StopIteration : if forward : a . reverse ( ) return False a [ i ] , a [ j ] = a [ j ] , a [ i ] a [ i + 1 : ] = reversed ( a [ i + 1 : ] ) if not forward : a . reverse ( ) return True
5191	def send_direct_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . DirectOperate ( command_set , callback , config )
3942	async def _longpoll_request ( self ) : params = { 'VER' : 8 , 'gsessionid' : self . _gsessionid_param , 'RID' : 'rpc' , 't' : 1 , 'SID' : self . _sid_param , 'CI' : 0 , 'ctype' : 'hangouts' , 'TYPE' : 'xmlhttp' , } logger . info ( 'Opening new long-polling request' ) try : async with self . _session . fetch_raw ( 'GET' , CHANNEL_URL , params = params ) as res : if res . status != 200 : if res . status == 400 and res . reason == 'Unknown SID' : raise ChannelSessionError ( 'SID became invalid' ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) while True : async with async_timeout . timeout ( PUSH_TIMEOUT ) : chunk = await res . content . read ( MAX_READ_BYTES ) if not chunk : break await self . _on_push_data ( chunk ) except asyncio . TimeoutError : raise exceptions . NetworkError ( 'Request timed out' ) except aiohttp . ServerDisconnectedError as err : raise exceptions . NetworkError ( 'Server disconnected error: %s' % err ) except aiohttp . ClientPayloadError : raise ChannelSessionError ( 'SID is about to expire' ) except aiohttp . ClientError as err : raise exceptions . NetworkError ( 'Request connection error: %s' % err )
13407	def sendToLogbook ( self , fileName , logType , location = None ) : import subprocess success = True if logType == "MCC" : fileString = "" if not self . imagePixmap . isNull ( ) : fileString = fileName + "." + self . imageType logcmd = "xml2elog " + fileName + ".xml " + fileString process = subprocess . Popen ( logcmd , shell = True ) process . wait ( ) if process . returncode != 0 : success = False else : from shutil import copy path = "/u1/" + location . lower ( ) + "/physics/logbook/data/" try : if not self . imagePixmap . isNull ( ) : copy ( fileName + ".png" , path ) if self . imageType == "png" : copy ( fileName + ".ps" , path ) else : copy ( fileName + "." + self . imageType , path ) copy ( fileName + ".xml" , path ) except IOError as error : print ( error ) success = False return success
11568	def open ( self , verbose ) : if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : raise
2415	def write_review ( review , out ) : out . write ( '# Review\n\n' ) write_value ( 'Reviewer' , review . reviewer , out ) write_value ( 'ReviewDate' , review . review_date_iso_format , out ) if review . has_comment : write_text_value ( 'ReviewComment' , review . comment , out )
10737	def path_from_keywords ( keywords , into = 'path' ) : subdirs = [ ] def prepare_string ( s ) : s = str ( s ) s = re . sub ( '[][{},*"' + f"'{os.sep}]" , '_' , s ) if into == 'file' : s = s . replace ( '_' , ' ' ) if ' ' in s : s = s . title ( ) s = s . replace ( ' ' , '' ) return s if isinstance ( keywords , set ) : keywords_list = sorted ( keywords ) for property in keywords_list : subdirs . append ( prepare_string ( property ) ) else : keywords_list = sorted ( keywords . items ( ) ) for property , value in keywords_list : if Bool . valid ( value ) : subdirs . append ( ( '' if value else ( 'not_' if into == 'path' else 'not' ) ) + prepare_string ( property ) ) elif ( Float | Integer ) . valid ( value ) : subdirs . append ( '{}{}' . format ( prepare_string ( property ) , prepare_string ( value ) ) ) else : subdirs . append ( '{}{}{}' . format ( prepare_string ( property ) , '_' if into == 'path' else '' , prepare_string ( value ) ) ) if into == 'path' : out = os . path . join ( * subdirs ) else : out = '_' . join ( subdirs ) return out
11634	def generate_oauth2_headers ( self ) : encoded_credentials = base64 . b64encode ( ( '{0}:{1}' . format ( self . consumer_key , self . consumer_secret ) ) . encode ( 'utf-8' ) ) headers = { 'Authorization' : 'Basic {0}' . format ( encoded_credentials . decode ( 'utf-8' ) ) , 'Content-Type' : 'application/x-www-form-urlencoded' } return headers
6201	def simulate_timestamps_mix ( self , max_rates , populations , bg_rate , rs = None , seed = 1 , chunksize = 2 ** 16 , comp_filter = None , overwrite = False , skip_existing = False , scale = 10 , path = None , t_chunksize = None , timeslice = None ) : self . open_store_timestamp ( chunksize = chunksize , path = path ) rs = self . _get_group_randomstate ( rs , seed , self . ts_group ) if t_chunksize is None : t_chunksize = self . emission . chunkshape [ 1 ] timeslice_size = self . n_samples if timeslice is not None : timeslice_size = timeslice // self . t_step name = self . _get_ts_name_mix ( max_rates , populations , bg_rate , rs = rs ) kw = dict ( name = name , clk_p = self . t_step / scale , max_rates = max_rates , bg_rate = bg_rate , populations = populations , num_particles = self . num_particles , bg_particle = self . num_particles , overwrite = overwrite , chunksize = chunksize ) if comp_filter is not None : kw . update ( comp_filter = comp_filter ) try : self . _timestamps , self . _tparticles = ( self . ts_store . add_timestamps ( ** kw ) ) except ExistingArrayError as e : if skip_existing : print ( ' - Skipping already present timestamps array.' ) return else : raise e self . ts_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'PyBroMo' ] = __version__ ts_list , part_list = [ ] , [ ] bg_rates = [ None ] * ( len ( max_rates ) - 1 ) + [ bg_rate ] prev_time = 0 for i_start , i_end in iter_chunk_index ( timeslice_size , t_chunksize ) : curr_time = np . around ( i_start * self . t_step , decimals = 0 ) if curr_time > prev_time : print ( ' %.1fs' % curr_time , end = '' , flush = True ) prev_time = curr_time em_chunk = self . emission [ : , i_start : i_end ] times_chunk_s , par_index_chunk_s = self . _sim_timestamps_populations ( em_chunk , max_rates , populations , bg_rates , i_start , rs , scale ) ts_list . append ( times_chunk_s ) part_list . append ( par_index_chunk_s ) for ts , part in zip ( ts_list , part_list ) : self . _timestamps . append ( ts ) self . _tparticles . append ( part ) self . ts_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'last_random_state' ] = rs . get_state ( ) self . ts_store . h5file . flush ( )
13322	def rem_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . discard ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
6298	def draw ( self , mesh , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : self . program [ "m_proj" ] . write ( projection_matrix ) self . program [ "m_mv" ] . write ( view_matrix ) mesh . vao . render ( self . program )
6152	def fir_remez_bpf ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandpass_order ( f_stop1 , f_pass1 , f_pass2 , f_stop2 , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) print ( 'Remez filter taps = %d.' % N_taps ) return b
10634	def get_compound_afr ( self , compound ) : index = self . material . get_compound_index ( compound ) return stoich . amount ( compound , self . _compound_mfrs [ index ] )
4679	def getAccountFromPrivateKey ( self , wif ) : pub = self . publickey_from_wif ( wif ) return self . getAccountFromPublicKey ( pub )
13826	def FromJsonString ( self , value ) : self . Clear ( ) for path in value . split ( ',' ) : self . paths . append ( path )
12843	def execute_sync ( self , message ) : info ( "synchronizing message: {message}" ) with self . world . _unlock_temporarily ( ) : message . _sync ( self . world ) self . world . _react_to_sync_response ( message ) for actor in self . actors : actor . _react_to_sync_response ( message )
5062	def get_enterprise_customer_user ( user_id , enterprise_uuid ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( enterprise_customer__uuid = enterprise_uuid , user_id = user_id ) except EnterpriseCustomerUser . DoesNotExist : return None
2096	def status ( self , pk = None , detail = False , ** kwargs ) : self . _pop_none ( kwargs ) if not pk : job = self . get ( include_debug_header = True , ** kwargs ) else : debug . log ( 'Asking for job status.' , header = 'details' ) finished_endpoint = '%s%s/' % ( self . endpoint , pk ) job = client . get ( finished_endpoint ) . json ( ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
4944	def get_course_data_sharing_consent ( username , course_id , enterprise_customer_uuid ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) return DataSharingConsent . objects . proxied_get ( username = username , course_id = course_id , enterprise_customer__uuid = enterprise_customer_uuid )
7121	def filter_config ( config , deploy_config ) : if not os . path . isfile ( deploy_config ) : return DotDict ( ) config_module = get_config_module ( deploy_config ) return config_module . filter ( config )
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
7753	def route_stanza ( self , stanza ) : if stanza . stanza_type not in ( "error" , "result" ) : response = stanza . make_error_response ( u"recipient-unavailable" ) self . send ( response ) return True
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
9016	def instruction_in_row ( self , row , specification ) : whole_instruction_ = self . _as_instruction ( specification ) return self . _spec . new_instruction_in_row ( row , whole_instruction_ )
10241	def count_authors_by_annotation ( graph : BELGraph , annotation : str = 'Subgraph' ) -> Mapping [ str , typing . Counter [ str ] ] : authors = group_as_dict ( _iter_authors_by_annotation ( graph , annotation = annotation ) ) return count_defaultdict ( authors )
6449	def pylint_color ( score ) : score_cutoffs = ( 10 , 9.5 , 8.5 , 7.5 , 5 ) for i in range ( len ( score_cutoffs ) ) : if score >= score_cutoffs [ i ] : return BADGE_COLORS [ i ] return BADGE_COLORS [ - 1 ]
10180	def get ( self , timeout = None ) : result = None try : result = self . _result . get ( True , timeout = timeout ) except Empty : raise Timeout ( ) if isinstance ( result , Failure ) : six . reraise ( * result . exc_info ) else : return result
2753	def get_domain ( self , domain_name ) : return Domain . get_object ( api_token = self . token , domain_name = domain_name )
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
12296	def post ( repo , args = [ ] ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( what = 'metadata' ) keys = keys [ 'metadata' ] if len ( keys ) == 0 : return if 'pipeline' in repo . options : for name , details in repo . options [ 'pipeline' ] . items ( ) : patterns = details [ 'files' ] matching_files = repo . find_matching_files ( patterns ) matching_files . sort ( ) details [ 'files' ] = matching_files for i , f in enumerate ( matching_files ) : r = repo . get_resource ( f ) if 'pipeline' not in r : r [ 'pipeline' ] = [ ] r [ 'pipeline' ] . append ( name + " [Step {}]" . format ( i ) ) if 'metadata-management' in repo . options : print ( "Collecting all the required metadata to post" ) metadata = repo . options [ 'metadata-management' ] if 'include-data-history' in metadata and metadata [ 'include-data-history' ] : repo . package [ 'history' ] = get_history ( repo . rootdir ) if 'include-action-history' in metadata and metadata [ 'include-action-history' ] : annotate_metadata_action ( repo ) if 'include-preview' in metadata : annotate_metadata_data ( repo , task = 'preview' , patterns = metadata [ 'include-preview' ] [ 'files' ] , size = metadata [ 'include-preview' ] [ 'length' ] ) if ( ( 'include-schema' in metadata ) and metadata [ 'include-schema' ] ) : annotate_metadata_data ( repo , task = 'schema' ) if 'include-code-history' in metadata : annotate_metadata_code ( repo , files = metadata [ 'include-code-history' ] ) if 'include-platform' in metadata : annotate_metadata_platform ( repo ) if 'include-validation' in metadata : annotate_metadata_validation ( repo ) if 'include-dependencies' in metadata : annotate_metadata_dependencies ( repo ) history = repo . package . get ( 'history' , None ) if ( ( 'include-tab-diffs' in metadata ) and metadata [ 'include-tab-diffs' ] and history is not None ) : annotate_metadata_diffs ( repo ) repo . package [ 'config' ] = repo . options try : for k in keys : metadatamgr = mgr . get_by_key ( 'metadata' , k ) url = metadatamgr . url o = urlparse ( url ) print ( "Posting to " , o . netloc ) response = metadatamgr . post ( repo ) if isinstance ( response , str ) : print ( "Error while posting:" , response ) elif response . status_code in [ 400 ] : content = response . json ( ) print ( "Error while posting:" ) for k in content : print ( " " , k , "- " , "," . join ( content [ k ] ) ) except NetworkError as e : print ( "Unable to reach metadata server!" ) except NetworkInvalidConfiguration as e : print ( "Invalid network configuration in the INI file" ) print ( e . message ) except Exception as e : print ( "Could not post. Unknown error" ) print ( e )
7104	def transform ( self , transformer ) : self . transformers . append ( transformer ) from languageflow . transformer . tagged import TaggedTransformer if isinstance ( transformer , TaggedTransformer ) : self . X , self . y = transformer . transform ( self . sentences ) if isinstance ( transformer , TfidfVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , CountVectorizer ) : self . X = transformer . fit_transform ( self . X ) if isinstance ( transformer , NumberRemover ) : self . X = transformer . transform ( self . X ) if isinstance ( transformer , MultiLabelBinarizer ) : self . y = transformer . fit_transform ( self . y )
12844	def execute_undo ( self , message ) : info ( "undoing message: {message}" ) with self . world . _unlock_temporarily ( ) : message . _undo ( self . world ) self . world . _react_to_undo_response ( message ) for actor in self . actors : actor . _react_to_undo_response ( message )
5161	def __intermediate_addresses ( self , interface ) : address_list = self . get_copy ( interface , 'addresses' ) if not address_list : return [ { 'proto' : 'none' } ] result = [ ] static = { } dhcp = [ ] for address in address_list : family = address . get ( 'family' ) if address [ 'proto' ] == 'dhcp' : address [ 'proto' ] = 'dhcp' if family == 'ipv4' else 'dhcpv6' dhcp . append ( self . __intermediate_address ( address ) ) continue if 'gateway' in address : uci_key = 'gateway' if family == 'ipv4' else 'ip6gw' interface [ uci_key ] = address [ 'gateway' ] address_key = 'ipaddr' if family == 'ipv4' else 'ip6addr' static . setdefault ( address_key , [ ] ) static [ address_key ] . append ( '{address}/{mask}' . format ( ** address ) ) static . update ( self . __intermediate_address ( address ) ) if static : if len ( static . get ( 'ipaddr' , [ ] ) ) == 1 : network = ip_interface ( six . text_type ( static [ 'ipaddr' ] [ 0 ] ) ) static [ 'ipaddr' ] = str ( network . ip ) static [ 'netmask' ] = str ( network . netmask ) if len ( static . get ( 'ip6addr' , [ ] ) ) == 1 : static [ 'ip6addr' ] = static [ 'ip6addr' ] [ 0 ] result . append ( static ) if dhcp : result += dhcp return result
7828	def _new_from_xml ( cls , xmlnode ) : label = from_utf8 ( xmlnode . prop ( "label" ) ) child = xmlnode . children value = None for child in xml_element_ns_iter ( xmlnode . children , DATAFORM_NS ) : if child . name == "value" : value = from_utf8 ( child . getContent ( ) ) break if value is None : raise BadRequestProtocolError ( "No value in <option/> element" ) return cls ( value , label )
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3600	def http_connection ( timeout ) : def wrapper ( f ) : def wrapped ( * args , ** kwargs ) : if not ( 'connection' in kwargs ) or not kwargs [ 'connection' ] : connection = requests . Session ( ) kwargs [ 'connection' ] = connection else : connection = kwargs [ 'connection' ] if not getattr ( connection , 'timeout' , False ) : connection . timeout = timeout connection . headers . update ( { 'Content-type' : 'application/json' } ) return f ( * args , ** kwargs ) return wraps ( f ) ( wrapped ) return wrapper
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
6448	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) self . _compressor . compress ( src ) src_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar ) tar_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( src + tar ) concat_comp = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) self . _compressor . compress ( tar + src ) concat_comp2 = self . _compressor . flush ( zlib . Z_FULL_FLUSH ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
1274	def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory
6779	def get_deploy_funcs ( components , current_thumbprint , previous_thumbprint , preview = False ) : for component in components : funcs = manifest_deployers . get ( component , [ ] ) for func_name in funcs : if func_name . startswith ( 'burlap.' ) : print ( 'skipping %s' % func_name ) continue takes_diff = manifest_deployers_takes_diff . get ( func_name , False ) func = resolve_deployer ( func_name ) current = current_thumbprint . get ( component ) last = previous_thumbprint . get ( component ) if takes_diff : yield func_name , partial ( func , last = last , current = current ) else : yield func_name , partial ( func )
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , ** kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
11959	def _check_nm ( nm , notation ) : _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
2006	def _serialize_int ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError if not isinstance ( value , ( int , BitVec ) ) : raise ValueError if issymbolic ( value ) : buf = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) value = Operators . SEXTEND ( value , value . size , size * 8 ) buf = ArrayProxy ( buf . write_BE ( padding , value , size ) ) else : value = int ( value ) buf = bytearray ( ) for _ in range ( padding ) : buf . append ( 0 ) for position in reversed ( range ( size ) ) : buf . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) return buf
6729	def deploy_code ( self ) : assert self . genv . SITE , 'Site unspecified.' assert self . genv . ROLE , 'Role unspecified.' r = self . local_renderer if self . env . exclusions : r . env . exclusions_str = ' ' . join ( "--exclude='%s'" % _ for _ in self . env . exclusions ) r . local ( r . env . rsync_command ) r . sudo ( 'chown -R {rsync_chown_user}:{rsync_chown_group} {rsync_dst_dir}' )
11627	def make_present_participles ( verbs ) : res = [ ] for verb in verbs : parts = verb . split ( ) if parts [ 0 ] . endswith ( "e" ) : parts [ 0 ] = parts [ 0 ] [ : - 1 ] + "ing" else : parts [ 0 ] = parts [ 0 ] + "ing" res . append ( " " . join ( parts ) ) return res
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( ** jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
322	def get_top_drawdowns ( returns , top = 10 ) : returns = returns . copy ( ) df_cum = ep . cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 drawdowns = [ ] for t in range ( top ) : peak , valley , recovery = get_max_drawdown_underwater ( underwater ) if not pd . isnull ( recovery ) : underwater . drop ( underwater [ peak : recovery ] . index [ 1 : - 1 ] , inplace = True ) else : underwater = underwater . loc [ : peak ] drawdowns . append ( ( peak , valley , recovery ) ) if ( len ( returns ) == 0 ) or ( len ( underwater ) == 0 ) : break return drawdowns
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
12395	def register ( self , method , args , kwargs ) : invoc = self . dump_invoc ( * args , ** kwargs ) self . registry . append ( ( invoc , method . __name__ ) )
5829	def check_for_rate_limiting ( response , response_lambda , timeout = 1 , attempts = 0 ) : if attempts >= 3 : raise RateLimitingException ( ) if response . status_code == 429 : sleep ( timeout ) new_timeout = timeout + 1 new_attempts = attempts + 1 return check_for_rate_limiting ( response_lambda ( timeout , attempts ) , response_lambda , timeout = new_timeout , attempts = new_attempts ) return response
6998	def parallel_cp ( pfpicklelist , outdir , lcbasedir , fast_mode = False , lcfnamelist = None , cprenorm = False , lclistpkl = None , gaia_max_timeout = 60.0 , gaia_mirror = None , nbrradiusarcsec = 60.0 , maxnumneighbors = 5 , makeneighborlcs = True , xmatchinfo = None , xmatchradiusarcsec = 3.0 , sigclip = 10.0 , minobservations = 99 , lcformat = 'hat-sql' , lcformatdir = None , timecols = None , magcols = None , errcols = None , skipdone = False , done_callback = None , done_callback_args = None , done_callback_kwargs = None , liststartindex = None , maxobjects = None , nworkers = NCPUS , ) : if sys . platform == 'darwin' : import requests requests . get ( 'http://captive.apple.com/hotspot-detect.html' ) if not os . path . exists ( outdir ) : os . mkdir ( outdir ) if ( liststartindex is not None ) and ( maxobjects is None ) : pfpicklelist = pfpicklelist [ liststartindex : ] if lcfnamelist is not None : lcfnamelist = lcfnamelist [ liststartindex : ] elif ( liststartindex is None ) and ( maxobjects is not None ) : pfpicklelist = pfpicklelist [ : maxobjects ] if lcfnamelist is not None : lcfnamelist = lcfnamelist [ : maxobjects ] elif ( liststartindex is not None ) and ( maxobjects is not None ) : pfpicklelist = ( pfpicklelist [ liststartindex : liststartindex + maxobjects ] ) if lcfnamelist is not None : lcfnamelist = lcfnamelist [ liststartindex : liststartindex + maxobjects ] if lcfnamelist is None : lcfnamelist = [ None ] * len ( pfpicklelist ) tasklist = [ ( x , outdir , lcbasedir , { 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'lcfname' : y , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lclistpkl' : lclistpkl , 'gaia_max_timeout' : gaia_max_timeout , 'gaia_mirror' : gaia_mirror , 'nbrradiusarcsec' : nbrradiusarcsec , 'maxnumneighbors' : maxnumneighbors , 'makeneighborlcs' : makeneighborlcs , 'xmatchinfo' : xmatchinfo , 'xmatchradiusarcsec' : xmatchradiusarcsec , 'sigclip' : sigclip , 'minobservations' : minobservations , 'skipdone' : skipdone , 'cprenorm' : cprenorm , 'fast_mode' : fast_mode , 'done_callback' : done_callback , 'done_callback_args' : done_callback_args , 'done_callback_kwargs' : done_callback_kwargs } ) for x , y in zip ( pfpicklelist , lcfnamelist ) ] resultfutures = [ ] results = [ ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( runcp_worker , tasklist ) results = [ x for x in resultfutures ] executor . shutdown ( ) return results
11116	def create_package ( self , path = None , name = None , mode = None ) : assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:bz2' mode = 'w:' if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext self . save ( ) tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) for directory in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : t = tarfile . TarInfo ( directory ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) for file in self . walk_files_relative_path ( ) : tarHandler . add ( os . path . join ( self . __path , file ) , arcname = file ) tarHandler . add ( os . path . join ( self . __path , ".pyrepinfo" ) , arcname = ".pyrepinfo" ) tarHandler . close ( )
247	def map_transaction ( txn ) : if isinstance ( txn [ 'sid' ] , dict ) : sid = txn [ 'sid' ] [ 'sid' ] symbol = txn [ 'sid' ] [ 'symbol' ] else : sid = txn [ 'sid' ] symbol = txn [ 'sid' ] return { 'sid' : sid , 'symbol' : symbol , 'price' : txn [ 'price' ] , 'order_id' : txn [ 'order_id' ] , 'amount' : txn [ 'amount' ] , 'commission' : txn [ 'commission' ] , 'dt' : txn [ 'dt' ] }
13436	def _setup_positions ( self , positions ) : updated_positions = [ ] for i , position in enumerate ( positions ) : ranger = re . search ( r'(?P<start>-?\d*):(?P<end>\d*)' , position ) if ranger : if i > 0 : updated_positions . append ( self . separator ) start = group_val ( ranger . group ( 'start' ) ) end = group_val ( ranger . group ( 'end' ) ) if start and end : updated_positions . extend ( self . _extendrange ( start , end + 1 ) ) elif ranger . group ( 'start' ) : updated_positions . append ( [ start ] ) else : updated_positions . extend ( self . _extendrange ( 1 , end + 1 ) ) else : updated_positions . append ( positions [ i ] ) try : if int ( position ) and int ( positions [ i + 1 ] ) : updated_positions . append ( self . separator ) except ( ValueError , IndexError ) : pass return updated_positions
13128	def tree2commands ( self , adapter , session , lastcmds , xsync ) : assert xsync . tag == constants . NODE_SYNCML assert len ( xsync ) == 2 assert xsync [ 0 ] . tag == constants . CMD_SYNCHDR assert xsync [ 1 ] . tag == constants . NODE_SYNCBODY version = xsync [ 0 ] . findtext ( 'VerProto' ) if version != constants . SYNCML_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML version "%s" (expected "%s")' % ( version , constants . SYNCML_VERSION_1_2 ) ) verdtd = xsync [ 0 ] . findtext ( 'VerDTD' ) if verdtd != constants . SYNCML_DTD_VERSION_1_2 : raise common . FeatureNotSupported ( 'unsupported SyncML DTD version "%s" (expected "%s")' % ( verdtd , constants . SYNCML_DTD_VERSION_1_2 ) ) ret = self . initialize ( adapter , session , xsync ) hdrcmd = ret [ 0 ] if session . isServer : log . debug ( 'received request SyncML message from "%s" (s%s.m%s)' , hdrcmd . target , hdrcmd . sessionID , hdrcmd . msgID ) else : log . debug ( 'received response SyncML message from "%s" (s%s.m%s)' , lastcmds [ 0 ] . target , lastcmds [ 0 ] . sessionID , lastcmds [ 0 ] . msgID ) try : return self . _tree2commands ( adapter , session , lastcmds , xsync , ret ) except Exception , e : if not session . isServer : raise code = '%s.%s' % ( e . __class__ . __module__ , e . __class__ . __name__ ) msg = '' . join ( traceback . format_exception_only ( type ( e ) , e ) ) . strip ( ) log . exception ( 'failed while interpreting command tree: %s' , msg ) return [ hdrcmd , state . Command ( name = constants . CMD_STATUS , cmdID = '1' , msgRef = session . pendingMsgID , cmdRef = 0 , sourceRef = xsync [ 0 ] . findtext ( 'Source/LocURI' ) , targetRef = xsync [ 0 ] . findtext ( 'Target/LocURI' ) , statusOf = constants . CMD_SYNCHDR , statusCode = constants . STATUS_COMMAND_FAILED , errorCode = code , errorMsg = msg , errorTrace = '' . join ( traceback . format_exception ( type ( e ) , e , sys . exc_info ( ) [ 2 ] ) ) , ) , state . Command ( name = constants . CMD_FINAL ) ]
7161	def go_back ( self , n = 1 ) : if not self . can_go_back : return N = max ( len ( self . answers ) - abs ( n ) , 0 ) self . answers = OrderedDict ( islice ( self . answers . items ( ) , N ) )
9294	def python_value ( self , value ) : value = coerce_to_bytes ( value ) obj = HashValue ( value ) obj . field = self return obj
12518	def _get_node_names ( h5file , h5path = '/' , node_type = h5py . Dataset ) : if isinstance ( h5file , str ) : _h5file = get_h5file ( h5file , mode = 'r' ) else : _h5file = h5file if not h5path . startswith ( '/' ) : h5path = '/' + h5path names = [ ] try : h5group = _h5file . require_group ( h5path ) for node in _hdf5_walk ( h5group , node_type = node_type ) : names . append ( node . name ) except : raise RuntimeError ( 'Error getting node names from {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return names
9837	def __object ( self ) : self . __consume ( ) classid = self . __consume ( ) . text word = self . __consume ( ) . text if word != "class" : raise DXParseError ( "reserved word %s should have been 'class'." % word ) if self . currentobject : self . objects . append ( self . currentobject ) classtype = self . __consume ( ) . text self . currentobject = DXInitObject ( classtype = classtype , classid = classid ) self . use_parser ( classtype )
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
2152	def get ( self , pk = None , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , ** kwargs )
4323	def contrast ( self , amount = 75 ) : if not is_number ( amount ) or amount < 0 or amount > 100 : raise ValueError ( 'amount must be a number between 0 and 100.' ) effect_args = [ 'contrast' , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'contrast' ) return self
11581	def run ( self ) : self . command_dispatch . update ( { self . REPORT_VERSION : [ self . report_version , 2 ] } ) self . command_dispatch . update ( { self . REPORT_FIRMWARE : [ self . report_firmware , 1 ] } ) self . command_dispatch . update ( { self . ANALOG_MESSAGE : [ self . analog_message , 2 ] } ) self . command_dispatch . update ( { self . DIGITAL_MESSAGE : [ self . digital_message , 2 ] } ) self . command_dispatch . update ( { self . ENCODER_DATA : [ self . encoder_data , 3 ] } ) self . command_dispatch . update ( { self . SONAR_DATA : [ self . sonar_data , 3 ] } ) self . command_dispatch . update ( { self . STRING_DATA : [ self . _string_data , 2 ] } ) self . command_dispatch . update ( { self . I2C_REPLY : [ self . i2c_reply , 2 ] } ) self . command_dispatch . update ( { self . CAPABILITY_RESPONSE : [ self . capability_response , 2 ] } ) self . command_dispatch . update ( { self . PIN_STATE_RESPONSE : [ self . pin_state_response , 2 ] } ) self . command_dispatch . update ( { self . ANALOG_MAPPING_RESPONSE : [ self . analog_mapping_response , 2 ] } ) self . command_dispatch . update ( { self . STEPPER_DATA : [ self . stepper_version_response , 2 ] } ) while not self . is_stopped ( ) : if len ( self . pymata . command_deque ) : data = self . pymata . command_deque . popleft ( ) command_data = [ ] if data == self . START_SYSEX : while len ( self . pymata . command_deque ) == 0 : pass sysex_command = self . pymata . command_deque . popleft ( ) dispatch_entry = self . command_dispatch . get ( sysex_command ) method = dispatch_entry [ 0 ] end_of_sysex = False while not end_of_sysex : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) if data != self . END_SYSEX : command_data . append ( data ) else : end_of_sysex = True method ( command_data ) continue elif 0x80 <= data <= 0xff : if 0x90 <= data <= 0x9f : port = data & 0xf command_data . append ( port ) data = 0x90 elif 0xe0 <= data <= 0xef : pin = data & 0xf command_data . append ( pin ) data = 0xe0 else : pass dispatch_entry = self . command_dispatch . get ( data ) method = dispatch_entry [ 0 ] num_args = dispatch_entry [ 1 ] for i in range ( num_args ) : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) command_data . append ( data ) method ( command_data ) continue else : time . sleep ( .1 )
5902	def prehook ( self , ** kwargs ) : cmd = [ 'smpd' , '-s' ] logger . info ( "Starting smpd: " + " " . join ( cmd ) ) rc = subprocess . call ( cmd ) return rc
11982	def is_valid_ip ( self , ip ) : if not isinstance ( ip , ( IPv4Address , CIDR ) ) : if str ( ip ) . find ( '/' ) == - 1 : ip = IPv4Address ( ip ) else : ip = CIDR ( ip ) if isinstance ( ip , IPv4Address ) : if ip < self . _first_ip or ip > self . _last_ip : return False elif isinstance ( ip , CIDR ) : if ip . _nm . _ip_dec == 0xFFFFFFFE and self . _nm . _ip_dec != 0xFFFFFFFE : compare_to_first = self . _net_ip . _ip_dec compare_to_last = self . _bc_ip . _ip_dec else : compare_to_first = self . _first_ip . _ip_dec compare_to_last = self . _last_ip . _ip_dec if ip . _first_ip . _ip_dec < compare_to_first or ip . _last_ip . _ip_dec > compare_to_last : return False return True
6044	def padded_grid_from_shape_psf_shape_and_pixel_scale ( cls , shape , psf_shape , pixel_scale ) : padded_shape = ( shape [ 0 ] + psf_shape [ 0 ] - 1 , shape [ 1 ] + psf_shape [ 1 ] - 1 ) padded_regular_grid = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( padded_shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) padded_mask = msk . Mask . unmasked_for_shape_and_pixel_scale ( shape = padded_shape , pixel_scale = pixel_scale ) return PaddedRegularGrid ( arr = padded_regular_grid , mask = padded_mask , image_shape = shape )
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
2144	def get_prefix ( self , include_version = True ) : host = settings . host if '://' not in host : host = 'https://%s' % host . strip ( '/' ) elif host . startswith ( 'http://' ) and settings . verify_ssl : raise exc . TowerCLIError ( 'Can not verify ssl with non-https protocol. Change the ' 'verify_ssl configuration setting to continue.' ) url_pieces = urlparse ( host ) if url_pieces [ 0 ] not in [ 'http' , 'https' ] : raise exc . ConnectionError ( 'URL must be http(s), {} is not valid' . format ( url_pieces [ 0 ] ) ) prefix = urljoin ( host , '/api/' ) if include_version : prefix = urljoin ( prefix , "{}/" . format ( CUR_API_VERSION ) ) return prefix
911	def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore
5215	def active_futures ( ticker : str , dt ) -> str : t_info = ticker . split ( ) prefix , asset = ' ' . join ( t_info [ : - 1 ] ) , t_info [ - 1 ] info = const . market_info ( f'{prefix[:-1]}1 {asset}' ) f1 , f2 = f'{prefix[:-1]}1 {asset}' , f'{prefix[:-1]}2 {asset}' fut_2 = fut_ticker ( gen_ticker = f2 , dt = dt , freq = info [ 'freq' ] ) fut_1 = fut_ticker ( gen_ticker = f1 , dt = dt , freq = info [ 'freq' ] ) fut_tk = bdp ( tickers = [ fut_1 , fut_2 ] , flds = 'Last_Tradeable_Dt' , cache = True ) if pd . Timestamp ( dt ) . month < pd . Timestamp ( fut_tk . last_tradeable_dt [ 0 ] ) . month : return fut_1 d1 = bdib ( ticker = f1 , dt = dt ) d2 = bdib ( ticker = f2 , dt = dt ) return fut_1 if d1 [ f1 ] . volume . sum ( ) > d2 [ f2 ] . volume . sum ( ) else fut_2
9932	def get_refkey ( self , obj , referent ) : if isinstance ( obj , dict ) : for k , v in obj . items ( ) : if v is referent : return " (via its %r key)" % k for k in dir ( obj ) + [ '__dict__' ] : if getattr ( obj , k , None ) is referent : return " (via its %r attribute)" % k return ""
9002	def _register_instruction_in_defs ( self , instruction ) : type_ = instruction . type color_ = instruction . color instruction_to_svg_dict = self . _instruction_to_svg . instruction_to_svg_dict instruction_id = "{}:{}" . format ( type_ , color_ ) defs_id = instruction_id + ":defs" if instruction_id not in self . _instruction_type_color_to_symbol : svg_dict = instruction_to_svg_dict ( instruction ) self . _compute_scale ( instruction_id , svg_dict ) symbol = self . _make_definition ( svg_dict , instruction_id ) self . _instruction_type_color_to_symbol [ defs_id ] = symbol [ DEFINITION_HOLDER ] . pop ( "defs" , { } ) self . _instruction_type_color_to_symbol [ instruction_id ] = symbol return instruction_id
1703	def outer_left_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
9337	def wait ( self ) : e , r = self . result . get ( ) self . slave . join ( ) self . slave = None self . result = None if isinstance ( e , Exception ) : raise SlaveException ( e , r ) return r
5101	def adjacency2graph ( adjacency , edge_type = None , adjust = 1 , ** kwargs ) : if isinstance ( adjacency , np . ndarray ) : adjacency = _matrix2dict ( adjacency ) elif isinstance ( adjacency , dict ) : adjacency = _dict2dict ( adjacency ) else : msg = ( "If the adjacency parameter is supplied it must be a " "dict, or a numpy.ndarray." ) raise TypeError ( msg ) if edge_type is None : edge_type = { } else : if isinstance ( edge_type , np . ndarray ) : edge_type = _matrix2dict ( edge_type , etype = True ) elif isinstance ( edge_type , dict ) : edge_type = _dict2dict ( edge_type ) for u , ty in edge_type . items ( ) : for v , et in ty . items ( ) : adjacency [ u ] [ v ] [ 'edge_type' ] = et g = nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) ) adjacency = nx . to_dict_of_dicts ( g ) adjacency = _adjacency_adjust ( adjacency , adjust , True ) return nx . from_dict_of_dicts ( adjacency , create_using = nx . DiGraph ( ) )
4119	def twosided_2_onesided ( data ) : assert len ( data ) % 2 == 0 N = len ( data ) psd = np . array ( data [ 0 : N // 2 + 1 ] ) * 2. psd [ 0 ] /= 2. psd [ - 1 ] = data [ - 1 ] return psd
6112	def single_value ( cls , value , shape , pixel_scale , origin = ( 0.0 , 0.0 ) ) : array = np . ones ( shape ) * value return cls ( array , pixel_scale , origin )
6674	def get_owner ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) , warn_only = True ) : result = func ( 'stat -c %%U "%(path)s"' % locals ( ) ) if result . failed and 'stat: illegal option' in result : return func ( 'stat -f %%Su "%(path)s"' % locals ( ) ) return result
9074	def setup_smtp_factory ( ** settings ) : return CustomSMTP ( host = settings . get ( 'mail.host' , 'localhost' ) , port = int ( settings . get ( 'mail.port' , 25 ) ) , user = settings . get ( 'mail.user' ) , password = settings . get ( 'mail.password' ) , timeout = float ( settings . get ( 'mail.timeout' , 60 ) ) , )
226	def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs
7291	def set_fields ( self ) : if self . is_initialized : self . model_map_dict = self . create_document_dictionary ( self . model_instance ) else : self . model_map_dict = self . create_document_dictionary ( self . model ) form_field_dict = self . get_form_field_dict ( self . model_map_dict ) self . set_form_fields ( form_field_dict )
1370	def get_subparser ( parser , command ) : subparsers_actions = [ action for action in parser . _actions if isinstance ( action , argparse . _SubParsersAction ) ] for subparsers_action in subparsers_actions : for choice , subparser in subparsers_action . choices . items ( ) : if choice == command : return subparser return None
2839	def pullup ( self , pin , enabled ) : self . _validate_pin ( pin ) if enabled : self . gppu [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) else : self . gppu [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) self . write_gppu ( )
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
7515	def enter_singles ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : seq = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] nalln = np . all ( seq == "N" , axis = 1 ) nsidx = nalln + smask samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) locuscov [ idx ] += 1 seq = seq [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s . tostring ( ) for name , s in zip ( names , seq ) ] ) snpstring = [ "-" if snp [ i , 0 ] else "*" if snp [ i , 1 ] else " " for i in range ( len ( snp ) ) ] outstr += "\n" + snppad + "" . join ( snpstring ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
11839	def set_board ( self , board = None ) : "Set the board, and find all the words in it." if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
8814	def get_interfaces ( self ) : LOG . debug ( "Getting interfaces from Xapi" ) with self . sessioned ( ) as session : instances = self . get_instances ( session ) recs = session . xenapi . VIF . get_all_records ( ) interfaces = set ( ) for vif_ref , rec in recs . iteritems ( ) : vm = instances . get ( rec [ "VM" ] ) if not vm : continue device_id = vm . uuid interfaces . add ( VIF ( device_id , rec , vif_ref ) ) return interfaces
6861	def prep_root_password ( self , password = None , ** kwargs ) : r = self . database_renderer ( ** kwargs ) r . env . root_password = password or r . genv . get ( 'db_root_password' ) r . sudo ( "DEBIAN_FRONTEND=noninteractive dpkg --configure -a" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password password {root_password}'" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password {root_password}'" )
11713	def instance ( self , counter = None ) : if not counter : history = self . history ( ) if not history : return history else : return Response . _from_json ( history [ 'pipelines' ] [ 0 ] ) return self . _get ( '/instance/{counter:d}' . format ( counter = counter ) )
6236	def set_time ( self , value : float ) : if value < 0 : value = 0 mixer . music . set_pos ( value )
773	def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
2297	def fit ( self , x , y ) : train = np . vstack ( ( np . array ( [ self . featurize_row ( row . iloc [ 0 ] , row . iloc [ 1 ] ) for idx , row in x . iterrows ( ) ] ) , np . array ( [ self . featurize_row ( row . iloc [ 1 ] , row . iloc [ 0 ] ) for idx , row in x . iterrows ( ) ] ) ) ) labels = np . vstack ( ( y , - y ) ) . ravel ( ) verbose = 1 if self . verbose else 0 self . clf = CLF ( verbose = verbose , min_samples_leaf = self . L , n_estimators = self . E , max_depth = self . max_depth , n_jobs = self . n_jobs ) . fit ( train , labels )
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
8933	def run ( cmd , ** kw ) : kw = kw . copy ( ) kw . setdefault ( 'warn' , False ) report_error = kw . pop ( 'report_error' , True ) runner = kw . pop ( 'runner' , invoke_run ) try : return runner ( cmd , ** kw ) except exceptions . Failure as exc : sys . stdout . flush ( ) sys . stderr . flush ( ) if report_error : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise finally : sys . stdout . flush ( ) sys . stderr . flush ( )
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) return len ( errors ) == 0 , errors
1116	def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . _prefix [ 1 ] next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id="difflib_chg_%s_%d"' % ( toprefix , num_chg ) num_chg += 1 next_href [ last ] = '<a href="#difflib_chg_%s_%d">n</a>' % ( toprefix , num_chg ) else : in_change = False if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next_href [ 0 ] = '<a href="#difflib_chg_%s_0">f</a>' % toprefix next_href [ last ] = '<a href="#difflib_chg_%s_top">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id
10533	def update_project ( project ) : try : project_id = project . id project = _forbidden_attributes ( project ) res = _pybossa_req ( 'put' , 'project' , project_id , payload = project . data ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
4160	def _get_data ( url ) : if url . startswith ( 'http://' ) : try : resp = urllib . urlopen ( url ) encoding = resp . headers . dict . get ( 'content-encoding' , 'plain' ) except AttributeError : resp = urllib . request . urlopen ( url ) encoding = resp . headers . get ( 'content-encoding' , 'plain' ) data = resp . read ( ) if encoding == 'plain' : pass elif encoding == 'gzip' : data = StringIO ( data ) data = gzip . GzipFile ( fileobj = data ) . read ( ) else : raise RuntimeError ( 'unknown encoding' ) else : with open ( url , 'r' ) as fid : data = fid . read ( ) return data
11805	def record_conflict ( self , assignment , var , val , delta ) : "Record conflicts caused by addition or deletion of a Queen." n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta
9226	def post_parse ( self ) : if self . result : out = [ ] for pu in self . result : try : out . append ( pu . parse ( self . scope ) ) except SyntaxError as e : self . handle_error ( e , 0 ) self . result = list ( utility . flatten ( out ) )
11042	def request ( self , method , url = None , ** kwargs ) : url = self . _compose_url ( url , kwargs ) kwargs . setdefault ( 'timeout' , self . _timeout ) d = self . _client . request ( method , url , reactor = self . _reactor , ** kwargs ) d . addCallback ( self . _log_request_response , method , url , kwargs ) d . addErrback ( self . _log_request_error , url ) return d
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
9343	def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
10692	def rgb_to_yiq ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 y = ( 0.299 * r ) + ( 0.587 * g ) + ( 0.114 * b ) i = ( 0.596 * r ) - ( 0.275 * g ) - ( 0.321 * b ) q = ( 0.212 * r ) - ( 0.528 * g ) + ( 0.311 * b ) return round ( y , 3 ) , round ( i , 3 ) , round ( q , 3 )
5186	def aggregate_event_counts ( self , summarize_by , query = None , count_by = None , count_filter = None ) : return self . _query ( 'aggregate-event-counts' , query = query , summarize_by = summarize_by , count_by = count_by , count_filter = count_filter )
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) return data . get_bytes ( )
3458	def main ( argv ) : source , target , tag = argv if "a" in tag : bump = "alpha" if "b" in tag : bump = "beta" else : bump = find_bump ( target , tag ) filename = "{}.md" . format ( tag ) destination = copy ( join ( source , filename ) , target ) build_hugo_md ( destination , tag , bump )
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
7441	def set_params ( self , param , newvalue ) : legacy_params = [ "edit_cutsites" , "trim_overhang" ] current_params = self . paramsdict . keys ( ) allowed_params = current_params + legacy_params if not param in allowed_params : raise IPyradParamsError ( "Parameter key not recognized: {}" . format ( param ) ) param = str ( param ) if len ( param ) < 3 : param = self . paramsdict . keys ( ) [ int ( param ) ] try : self = _paramschecker ( self , param , newvalue ) except Exception as inst : raise IPyradWarningExit ( BAD_PARAMETER . format ( param , inst , newvalue ) )
11854	def scanner ( self , j , word ) : "For each edge expecting a word of this category here, extend the edge." for ( i , j , A , alpha , Bb ) in self . chart [ j ] : if Bb and self . grammar . isa ( word , Bb [ 0 ] ) : self . add_edge ( [ i , j + 1 , A , alpha + [ ( Bb [ 0 ] , word ) ] , Bb [ 1 : ] ] )
3243	def get_rules ( security_group , ** kwargs ) : rules = security_group . pop ( 'security_group_rules' , [ ] ) for rule in rules : rule [ 'ip_protocol' ] = rule . pop ( 'protocol' ) rule [ 'from_port' ] = rule . pop ( 'port_range_max' ) rule [ 'to_port' ] = rule . pop ( 'port_range_min' ) rule [ 'cidr_ip' ] = rule . pop ( 'remote_ip_prefix' ) rule [ 'rule_type' ] = rule . pop ( 'direction' ) security_group [ 'rules' ] = sorted ( rules ) return security_group
5340	def __remove_dashboard_menu ( self , kibiter_major ) : logger . info ( "Removing old dashboard menu, if any" ) if kibiter_major == "6" : metadashboard = ".kibana/doc/metadashboard" else : metadashboard = ".kibana/metadashboard/main" menu_url = urijoin ( self . conf [ 'es_enrichment' ] [ 'url' ] , metadashboard ) self . grimoire_con . delete ( menu_url )
12755	def enable_motors ( self , max_force ) : for joint in self . joints : amotor = getattr ( joint , 'amotor' , joint ) amotor . max_forces = max_force if max_force > 0 : amotor . enable_feedback ( ) else : amotor . disable_feedback ( )
10449	def getallstates ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) _obj_states = [ ] if object_handle . AXEnabled : _obj_states . append ( "enabled" ) if object_handle . AXFocused : _obj_states . append ( "focused" ) else : try : if object_handle . AXFocused : _obj_states . append ( "focusable" ) except : pass if re . match ( "AXCheckBox" , object_handle . AXRole , re . M | re . U | re . L ) or re . match ( "AXRadioButton" , object_handle . AXRole , re . M | re . U | re . L ) : if object_handle . AXValue : _obj_states . append ( "checked" ) return _obj_states
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
12232	def register_prefs ( * args , ** kwargs ) : swap_settings_module = bool ( kwargs . get ( 'swap_settings_module' , True ) ) if __PATCHED_LOCALS_SENTINEL not in get_frame_locals ( 2 ) : raise SitePrefsException ( 'Please call `patch_locals()` right before the `register_prefs()`.' ) bind_proxy ( args , ** kwargs ) unpatch_locals ( ) swap_settings_module and proxy_settings_module ( )
3577	def disconnect_devices ( self , service_uuids ) : cbuuids = map ( uuid_to_cbuuid , service_uuids ) for device in self . _central_manager . retrieveConnectedPeripheralsWithServices_ ( cbuuids ) : self . _central_manager . cancelPeripheralConnection_ ( device )
13418	def start ( info ) : cmd = options . paved . django . runserver if cmd == 'runserver_plus' : try : import django_extensions except ImportError : info ( "Could not import django_extensions. Using default runserver." ) cmd = 'runserver' port = options . paved . django . runserver_port if port : cmd = '%s %s' % ( cmd , port ) call_manage ( cmd )
9770	def update ( ctx , name , description , tags ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the job.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . job . update_job ( user , project_name , _job , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job updated." ) get_job_details ( response )
5324	def read_file_from_uri ( self , uri ) : logger . debug ( "Reading %s" % ( uri ) ) self . __check_looks_like_uri ( uri ) try : req = urllib . request . Request ( uri ) req . add_header ( 'Authorization' , 'token %s' % self . token ) r = urllib . request . urlopen ( req ) except urllib . error . HTTPError as err : if err . code == 404 : raise GithubFileNotFound ( 'File %s is not available. Check the URL to ensure it really exists' % uri ) else : raise return r . read ( ) . decode ( "utf-8" )
3218	def get_route_tables ( vpc , ** conn ) : route_tables = describe_route_tables ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) rt_ids = [ ] for r in route_tables : rt_ids . append ( r [ "RouteTableId" ] ) return rt_ids
2546	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True doc . annotations [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'AnnotationComment' ) else : raise OrderError ( 'AnnotationComment' )
11714	def schedule ( self , variables = None , secure_variables = None , materials = None , return_new_instance = False , backoff_time = 1.0 ) : scheduling_args = dict ( variables = variables , secure_variables = secure_variables , material_fingerprint = materials , headers = { "Confirm" : True } , ) scheduling_args = dict ( ( k , v ) for k , v in scheduling_args . items ( ) if v is not None ) if return_new_instance : pipelines = self . history ( ) [ 'pipelines' ] if len ( pipelines ) == 0 : last_run = None else : last_run = pipelines [ 0 ] [ 'counter' ] response = self . _post ( '/schedule' , ok_status = 202 , ** scheduling_args ) if not response : return response max_tries = 10 while max_tries > 0 : current = self . instance ( ) if not last_run and current : return current elif last_run and current [ 'counter' ] > last_run : return current else : time . sleep ( backoff_time ) max_tries -= 1 return response else : return self . _post ( '/schedule' , ok_status = 202 , ** scheduling_args )
13170	def path ( self , include_root = False ) : path = '%s[%d]' % ( self . tagname , self . index or 0 ) p = self . parent while p is not None : if p . parent or include_root : path = '%s[%d]/%s' % ( p . tagname , p . index or 0 , path ) p = p . parent return path
8985	def instruction_to_svg_dict ( self , instruction_or_id , copy_result = True ) : instruction_id = self . get_instruction_id ( instruction_or_id ) if instruction_id in self . _cache : result = self . _cache [ instruction_id ] else : result = self . _instruction_to_svg_dict ( instruction_id ) self . _cache [ instruction_id ] = result if copy_result : result = deepcopy ( result ) return result
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
5350	def compose_projects_json ( projects , data ) : projects = compose_git ( projects , data ) projects = compose_mailing_lists ( projects , data ) projects = compose_bugzilla ( projects , data ) projects = compose_github ( projects , data ) projects = compose_gerrit ( projects ) projects = compose_mbox ( projects ) return projects
3504	def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
4633	def from_privkey ( cls , privkey , prefix = None ) : privkey = PrivateKey ( privkey , prefix = prefix or Prefix . prefix ) secret = unhexlify ( repr ( privkey ) ) order = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . curve . generator . order ( ) p = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . verifying_key . pubkey . point x_str = ecdsa . util . number_to_string ( p . x ( ) , order ) compressed = hexlify ( chr ( 2 + ( p . y ( ) & 1 ) ) . encode ( "ascii" ) + x_str ) . decode ( "ascii" ) return cls ( compressed , prefix = prefix or Prefix . prefix )
10614	def T ( self , T ) : self . _T = T self . _H = self . _calculate_H ( T )
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
12356	def wait ( self ) : interval_seconds = 5 while True : actions = self . actions ( ) slept = False for a in actions : if a [ 'status' ] == 'in-progress' : time . sleep ( interval_seconds ) slept = True break if not slept : break
6555	def copy ( self ) : return self . __class__ ( self . func , self . configurations , self . variables , self . vartype , name = self . name )
7565	def memoize ( func ) : class Memodict ( dict ) : def __getitem__ ( self , * key ) : return dict . __getitem__ ( self , key ) def __missing__ ( self , key ) : ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . __getitem__
6668	def populate_fabfile ( ) : stack = inspect . stack ( ) fab_frame = None for frame_obj , script_fn , line , _ , _ , _ in stack : if 'fabfile.py' in script_fn : fab_frame = frame_obj break if not fab_frame : return try : locals_ = fab_frame . f_locals for module_name , module in sub_modules . items ( ) : locals_ [ module_name ] = module for role_name , role_func in role_commands . items ( ) : assert role_name not in sub_modules , ( 'The role %s conflicts with a built-in submodule. ' 'Please choose a different name.' ) % ( role_name ) locals_ [ role_name ] = role_func locals_ [ 'common' ] = common locals_ [ 'shell' ] = shell for _module_alias in common . post_import_modules : exec ( "import %s" % _module_alias ) locals_ [ _module_alias ] = locals ( ) [ _module_alias ] finally : del stack
12765	def reposition ( self , frame_no ) : for label , j in self . channels . items ( ) : body = self . bodies [ label ] body . position = self . positions [ frame_no , j ] body . linear_velocity = self . velocities [ frame_no , j ]
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
7575	def detect_cpus ( ) : if hasattr ( os , "sysconf" ) : if os . sysconf_names . has_key ( "SC_NPROCESSORS_ONLN" ) : ncpus = os . sysconf ( "SC_NPROCESSORS_ONLN" ) if isinstance ( ncpus , int ) and ncpus > 0 : return ncpus else : return int ( os . popen2 ( "sysctl -n hw.ncpu" ) [ 1 ] . read ( ) ) if os . environ . has_key ( "NUMBER_OF_PROCESSORS" ) : ncpus = int ( os . environ [ "NUMBER_OF_PROCESSORS" ] ) if ncpus > 0 : return ncpus return 1
12969	def get ( self , pk , cascadeFetch = False ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) res = conn . hgetall ( key ) if type ( res ) != dict or not len ( res . keys ( ) ) : return None res [ '_id' ] = pk ret = self . _redisResultToObj ( res ) if cascadeFetch is True : self . _doCascadeFetch ( ret ) return ret
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
5328	def __get_uuids_from_profile_name ( self , profile_name ) : uuids = [ ] with self . db . connect ( ) as session : query = session . query ( Profile ) . filter ( Profile . name == profile_name ) profiles = query . all ( ) if profiles : for p in profiles : uuids . append ( p . uuid ) return uuids
2601	def start ( self ) : if self . mode == "manual" : return if self . ipython_dir != '~/.ipython' : self . ipython_dir = os . path . abspath ( os . path . expanduser ( self . ipython_dir ) ) if self . log : stdout = open ( os . path . join ( self . ipython_dir , "{0}.controller.out" . format ( self . profile ) ) , 'w' ) stderr = open ( os . path . join ( self . ipython_dir , "{0}.controller.err" . format ( self . profile ) ) , 'w' ) else : stdout = open ( os . devnull , 'w' ) stderr = open ( os . devnull , 'w' ) try : opts = [ 'ipcontroller' , '' if self . ipython_dir == '~/.ipython' else '--ipython-dir={}' . format ( self . ipython_dir ) , self . interfaces if self . interfaces is not None else '--ip=*' , '' if self . profile == 'default' else '--profile={0}' . format ( self . profile ) , '--reuse' if self . reuse else '' , '--location={}' . format ( self . public_ip ) if self . public_ip else '' , '--port={}' . format ( self . port ) if self . port is not None else '' ] if self . port_range is not None : opts += [ '--HubFactory.hb={0},{1}' . format ( self . hb_ping , self . hb_pong ) , '--HubFactory.control={0},{1}' . format ( self . control_client , self . control_engine ) , '--HubFactory.mux={0},{1}' . format ( self . mux_client , self . mux_engine ) , '--HubFactory.task={0},{1}' . format ( self . task_client , self . task_engine ) ] logger . debug ( "Starting ipcontroller with '{}'" . format ( ' ' . join ( [ str ( x ) for x in opts ] ) ) ) self . proc = subprocess . Popen ( opts , stdout = stdout , stderr = stderr , preexec_fn = os . setsid ) except FileNotFoundError : msg = "Could not find ipcontroller. Please make sure that ipyparallel is installed and available in your env" logger . error ( msg ) raise ControllerError ( msg ) except Exception as e : msg = "IPPController failed to start: {0}" . format ( e ) logger . error ( msg ) raise ControllerError ( msg )
3341	def parse_xml_body ( environ , allow_empty = False ) : clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) if clHeader == "" : requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
9896	def uptime ( ) : if __boottime is not None : return time . time ( ) - __boottime return { 'amiga' : _uptime_amiga , 'aros12' : _uptime_amiga , 'beos5' : _uptime_beos , 'cygwin' : _uptime_linux , 'darwin' : _uptime_osx , 'haiku1' : _uptime_beos , 'linux' : _uptime_linux , 'linux-armv71' : _uptime_linux , 'linux2' : _uptime_linux , 'mac' : _uptime_mac , 'minix3' : _uptime_minix , 'riscos' : _uptime_riscos , 'sunos5' : _uptime_solaris , 'syllable' : _uptime_syllable , 'win32' : _uptime_windows , 'wince' : _uptime_windows } . get ( sys . platform , _uptime_bsd ) ( ) or _uptime_bsd ( ) or _uptime_plan9 ( ) or _uptime_linux ( ) or _uptime_windows ( ) or _uptime_solaris ( ) or _uptime_beos ( ) or _uptime_amiga ( ) or _uptime_riscos ( ) or _uptime_posix ( ) or _uptime_syllable ( ) or _uptime_mac ( ) or _uptime_osx ( )
35	def setup_mpi_gpus ( ) : if 'CUDA_VISIBLE_DEVICES' not in os . environ : if sys . platform == 'darwin' : ids = [ ] else : lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) ids = [ lrank ] os . environ [ "CUDA_VISIBLE_DEVICES" ] = "," . join ( map ( str , ids ) )
3146	def _build_path ( self , * args ) : return '/' . join ( chain ( ( self . endpoint , ) , map ( str , args ) ) )
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
9745	def connection_made ( self , transport ) : self . transport = transport sock = transport . get_extra_info ( "socket" ) self . port = sock . getsockname ( ) [ 1 ]
4588	def show_image ( setter , width , height , image_path = '' , image_obj = None , offset = ( 0 , 0 ) , bgcolor = COLORS . Off , brightness = 255 ) : bgcolor = color_scale ( bgcolor , brightness ) img = image_obj if image_path and not img : from PIL import Image img = Image . open ( image_path ) elif not img : raise ValueError ( 'Must provide either image_path or image_obj' ) w = min ( width - offset [ 0 ] , img . size [ 0 ] ) h = min ( height - offset [ 1 ] , img . size [ 1 ] ) ox = offset [ 0 ] oy = offset [ 1 ] for x in range ( ox , w + ox ) : for y in range ( oy , h + oy ) : r , g , b , a = ( 0 , 0 , 0 , 255 ) rgba = img . getpixel ( ( x - ox , y - oy ) ) if isinstance ( rgba , int ) : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if len ( rgba ) == 3 : r , g , b = rgba elif len ( rgba ) == 4 : r , g , b , a = rgba else : raise ValueError ( 'Image must be in RGB or RGBA format!' ) if a == 0 : r , g , b = bgcolor else : r , g , b = color_scale ( ( r , g , b ) , a ) if brightness != 255 : r , g , b = color_scale ( ( r , g , b ) , brightness ) setter ( x , y , ( r , g , b ) )
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
11022	def get_node ( self , string_key ) : pos = self . get_node_pos ( string_key ) if pos is None : return None return self . ring [ self . _sorted_keys [ pos ] ]
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) if word [ - 3 : ] == 'que' : if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] noun = word verb = word for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
10682	def H_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : h = ( - self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 2 + tau ** 9 / 15 + tau ** 15 / 40 ) ) / self . _D_mag else : h = - ( tau ** - 5 / 2 + tau ** - 15 / 21 + tau ** - 25 / 60 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * h
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
2659	def initialize_scaling ( self ) : debug_opts = "--debug" if self . worker_debug else "" max_workers = "" if self . max_workers == float ( 'inf' ) else "--max_workers={}" . format ( self . max_workers ) worker_logdir = "{}/{}" . format ( self . run_dir , self . label ) if self . worker_logdir_root is not None : worker_logdir = "{}/{}" . format ( self . worker_logdir_root , self . label ) l_cmd = self . launch_cmd . format ( debug = debug_opts , prefetch_capacity = self . prefetch_capacity , task_url = self . worker_task_url , result_url = self . worker_result_url , cores_per_worker = self . cores_per_worker , max_workers = max_workers , nodes_per_block = self . provider . nodes_per_block , heartbeat_period = self . heartbeat_period , heartbeat_threshold = self . heartbeat_threshold , poll_period = self . poll_period , logdir = worker_logdir ) self . launch_cmd = l_cmd logger . debug ( "Launch command: {}" . format ( self . launch_cmd ) ) self . _scaling_enabled = self . provider . scaling_enabled logger . debug ( "Starting HighThroughputExecutor with provider:\n%s" , self . provider ) if hasattr ( self . provider , 'init_blocks' ) : try : self . scale_out ( blocks = self . provider . init_blocks ) except Exception as e : logger . error ( "Scaling out failed: {}" . format ( e ) ) raise e
7298	def get_form_field_class ( model_field ) : FIELD_MAPPING = { IntField : forms . IntegerField , StringField : forms . CharField , FloatField : forms . FloatField , BooleanField : forms . BooleanField , DateTimeField : forms . DateTimeField , DecimalField : forms . DecimalField , URLField : forms . URLField , EmailField : forms . EmailField } return FIELD_MAPPING . get ( model_field . __class__ , forms . CharField )
11162	def size ( self ) : try : return self . _stat . st_size except : self . _stat = self . stat ( ) return self . size
7922	def __prepare_resource ( data ) : if not data : return None data = unicode ( data ) try : resource = RESOURCEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( resource . encode ( "utf-8" ) ) > 1023 : raise JIDError ( "Resource name too long" ) return resource
46	def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )
6066	def einstein_radius_rescaled ( self ) : return ( ( 3 - self . slope ) / ( 1 + self . axis_ratio ) ) * self . einstein_radius ** ( self . slope - 1 )
13832	def _SkipFieldValue ( tokenizer ) : if tokenizer . TryConsumeByteString ( ) : while tokenizer . TryConsumeByteString ( ) : pass return if ( not tokenizer . TryConsumeIdentifier ( ) and not tokenizer . TryConsumeInt64 ( ) and not tokenizer . TryConsumeUint64 ( ) and not tokenizer . TryConsumeFloat ( ) ) : raise ParseError ( 'Invalid field value: ' + tokenizer . token )
978	def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( "Either i or j don't exist" )
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : shutil . copy2 ( src , dst )
1588	def get_topology_config ( self ) : if self . pplan . topology . HasField ( "topology_config" ) : return self . _get_dict_from_config ( self . pplan . topology . topology_config ) else : return { }
5661	def find_segments ( stops , shape ) : if not shape : return [ ] , 0 break_points = [ ] last_i = 0 cumul_d = 0 badness = 0 d_last_stop = float ( 'inf' ) lstlat , lstlon = None , None break_shape_points = [ ] for stop in stops : stlat , stlon = stop [ 'lat' ] , stop [ 'lon' ] best_d = float ( 'inf' ) if badness > 500 and badness > 30 * len ( break_points ) : return [ ] , badness for i in range ( last_i , len ( shape ) ) : d = wgs84_distance ( stlat , stlon , shape [ i ] [ 'lat' ] , shape [ i ] [ 'lon' ] ) if lstlat : d_last_stop = wgs84_distance ( lstlat , lstlon , shape [ i ] [ 'lat' ] , shape [ i ] [ 'lon' ] ) if d < best_d : best_d = d best_i = i cumul_d += d if ( d_last_stop < d ) or ( d > 500 ) or ( i < best_i + 100 ) : continue else : badness += best_d break_points . append ( best_i ) last_i = best_i lstlat , lstlon = stlat , stlon break_shape_points . append ( shape [ best_i ] ) break else : badness += best_d break_points . append ( best_i ) last_i = best_i lstlat , lstlon = stlat , stlon break_shape_points . append ( shape [ best_i ] ) pass return break_points , badness
6992	def flare_model ( flareparams , times , mags , errs ) : ( amplitude , flare_peak_time , rise_gaussian_stdev , decay_time_constant ) = flareparams zerolevel = np . median ( mags ) modelmags = np . full_like ( times , zerolevel ) modelmags [ times < flare_peak_time ] = ( mags [ times < flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times < flare_peak_time ] - flare_peak_time ) * ( times [ times < flare_peak_time ] - flare_peak_time ) ) / ( 2.0 * rise_gaussian_stdev * rise_gaussian_stdev ) ) ) modelmags [ times > flare_peak_time ] = ( mags [ times > flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times > flare_peak_time ] - flare_peak_time ) ) / ( decay_time_constant ) ) ) return modelmags , times , mags , errs
10363	def has_protein_modification_increases_activity ( graph : BELGraph , source : BaseEntity , target : BaseEntity , key : str , ) -> bool : edge_data = graph [ source ] [ target ] [ key ] return has_protein_modification ( graph , source ) and part_has_modifier ( edge_data , OBJECT , ACTIVITY )
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
5325	def measure_memory ( cls , obj , seen = None ) : size = sys . getsizeof ( obj ) if seen is None : seen = set ( ) obj_id = id ( obj ) if obj_id in seen : return 0 seen . add ( obj_id ) if isinstance ( obj , dict ) : size += sum ( [ cls . measure_memory ( v , seen ) for v in obj . values ( ) ] ) size += sum ( [ cls . measure_memory ( k , seen ) for k in obj . keys ( ) ] ) elif hasattr ( obj , '__dict__' ) : size += cls . measure_memory ( obj . __dict__ , seen ) elif hasattr ( obj , '__iter__' ) and not isinstance ( obj , ( str , bytes , bytearray ) ) : size += sum ( [ cls . measure_memory ( i , seen ) for i in obj ] ) return size
10001	def get_nodes_with ( self , obj ) : result = set ( ) if nx . __version__ [ 0 ] == "1" : nodes = self . nodes_iter ( ) else : nodes = self . nodes for node in nodes : if node [ OBJ ] == obj : result . add ( node ) return result
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
8594	def get_group ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s?depth=%s' % ( group_id , str ( depth ) ) ) return response
5222	def market_info ( ticker : str ) -> dict : t_info = ticker . split ( ) assets = param . load_info ( 'assets' ) if ( t_info [ - 1 ] == 'Equity' ) and ( '=' not in t_info [ 0 ] ) : exch = t_info [ - 2 ] for info in assets . get ( 'Equity' , [ dict ( ) ] ) : if 'exch_codes' not in info : continue if exch in info [ 'exch_codes' ] : return info return dict ( ) if t_info [ - 1 ] == 'Curncy' : for info in assets . get ( 'Curncy' , [ dict ( ) ] ) : if 'tickers' not in info : continue if ( t_info [ 0 ] . split ( '+' ) [ 0 ] in info [ 'tickers' ] ) or ( t_info [ 0 ] [ - 1 ] . isdigit ( ) and ( t_info [ 0 ] [ : - 1 ] in info [ 'tickers' ] ) ) : return info return dict ( ) if t_info [ - 1 ] == 'Comdty' : for info in assets . get ( 'Comdty' , [ dict ( ) ] ) : if 'tickers' not in info : continue if t_info [ 0 ] [ : - 1 ] in info [ 'tickers' ] : return info return dict ( ) if ( t_info [ - 1 ] == 'Index' ) or ( ( t_info [ - 1 ] == 'Equity' ) and ( '=' in t_info [ 0 ] ) ) : if t_info [ - 1 ] == 'Equity' : tck = t_info [ 0 ] . split ( '=' ) [ 0 ] else : tck = ' ' . join ( t_info [ : - 1 ] ) for info in assets . get ( 'Index' , [ dict ( ) ] ) : if 'tickers' not in info : continue if ( tck [ : 2 ] == 'UX' ) and ( 'UX' in info [ 'tickers' ] ) : return info if tck in info [ 'tickers' ] : if t_info [ - 1 ] == 'Equity' : return info if not info . get ( 'is_fut' , False ) : return info if tck [ : - 1 ] . rstrip ( ) in info [ 'tickers' ] : if info . get ( 'is_fut' , False ) : return info return dict ( ) if t_info [ - 1 ] == 'Corp' : for info in assets . get ( 'Corp' , [ dict ( ) ] ) : if 'ticker' not in info : continue return dict ( )
3875	async def _on_state_update ( self , state_update ) : notification_type = state_update . WhichOneof ( 'state_update' ) if state_update . HasField ( 'conversation' ) : try : await self . _handle_conversation_delta ( state_update . conversation ) except exceptions . NetworkError : logger . warning ( 'Discarding %s for %s: Failed to fetch conversation' , notification_type . replace ( '_' , ' ' ) , state_update . conversation . conversation_id . id ) return if notification_type == 'typing_notification' : await self . _handle_set_typing_notification ( state_update . typing_notification ) elif notification_type == 'watermark_notification' : await self . _handle_watermark_notification ( state_update . watermark_notification ) elif notification_type == 'event_notification' : await self . _on_event ( state_update . event_notification . event )
3041	def access_token_expired ( self ) : if self . invalid : return True if not self . token_expiry : return False now = _UTCNOW ( ) if now >= self . token_expiry : logger . info ( 'access_token is expired. Now: %s, token_expiry: %s' , now , self . token_expiry ) return True return False
142	def from_shapely ( polygon_shapely , label = None ) : import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )
9228	def get_all_tags ( self ) : verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching tags..." ) tags = [ ] page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) rc , data = gh . repos [ user ] [ repo ] . tags . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : tags . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if len ( tags ) == 0 : if not self . options . quiet : print ( "Warning: Can't find any tags in repo. Make sure, that " "you push tags to remote repo via 'git push --tags'" ) exit ( ) if verbose > 1 : print ( "Found {} tag(s)" . format ( len ( tags ) ) ) return tags
5410	def build_machine_type ( cls , min_cores , min_ram ) : min_cores = min_cores or job_model . DEFAULT_MIN_CORES min_ram = min_ram or job_model . DEFAULT_MIN_RAM min_ram *= GoogleV2CustomMachine . _MB_PER_GB cores = cls . _validate_cores ( min_cores ) ram = cls . _validate_ram ( min_ram ) memory_to_cpu_ratio = ram / cores if memory_to_cpu_ratio < GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU : adjusted_ram = GoogleV2CustomMachine . _MIN_MEMORY_PER_CPU * cores ram = cls . _validate_ram ( adjusted_ram ) elif memory_to_cpu_ratio > GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU : adjusted_cores = math . ceil ( ram / GoogleV2CustomMachine . _MAX_MEMORY_PER_CPU ) cores = cls . _validate_cores ( adjusted_cores ) else : pass return 'custom-{}-{}' . format ( int ( cores ) , int ( ram ) )
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
5061	def get_enterprise_customer_for_user ( auth_user ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( user_id = auth_user . id ) . enterprise_customer except EnterpriseCustomerUser . DoesNotExist : return None
11126	def dump_copy ( self , path , relativePath , name = None , description = None , replace = False , verbose = False ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' if name is None : _ , name = os . path . split ( path ) self . add_directory ( relativePath ) realPath = os . path . join ( self . __path , relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage if name in dict . __getitem__ ( dirInfoDict , "files" ) : if not replace : if verbose : warnings . warn ( "a file with the name '%s' is already defined in repository dictionary info. Set replace flag to True if you want to replace the existing file" % ( name ) ) return dump = "raise Exception(\"dump is ambiguous for copied file '$FILE_PATH' \")" pull = "raise Exception(\"pull is ambiguous for copied file '$FILE_PATH' \")" try : shutil . copyfile ( path , os . path . join ( realPath , name ) ) except Exception as e : if verbose : warnings . warn ( e ) return klass = None dict . __getitem__ ( dirInfoDict , "files" ) [ name ] = { "dump" : dump , "pull" : pull , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "class" : klass , "description" : description } self . save ( )
9650	def check_integrity ( sakefile , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "Call to check_integrity issued" , level = "verbose" ) if not sakefile : error ( "Sakefile is empty" ) return False if len ( sakefile . keys ( ) ) != len ( set ( sakefile . keys ( ) ) ) : error ( "Sakefile contains duplicate targets" ) return False for target in sakefile : if target == "all" : if not check_target_integrity ( target , sakefile [ "all" ] , all = True ) : error ( "Failed to accept target 'all'" ) return False continue if "formula" not in sakefile [ target ] : if not check_target_integrity ( target , sakefile [ target ] , meta = True ) : errmes = "Failed to accept meta-target '{}'" . format ( target ) error ( errmes ) return False for atom_target in sakefile [ target ] : if atom_target == "help" : continue if not check_target_integrity ( atom_target , sakefile [ target ] [ atom_target ] , parent = target ) : errmes = "Failed to accept target '{}'\n" . format ( atom_target ) error ( errmes ) return False continue if not check_target_integrity ( target , sakefile [ target ] ) : errmes = "Failed to accept target '{}'\n" . format ( target ) error ( errmes ) return False return True
13317	def remove ( name_or_path ) : r = resolve ( name_or_path ) r . resolved [ 0 ] . remove ( ) EnvironmentCache . discard ( r . resolved [ 0 ] ) EnvironmentCache . save ( )
13506	def get_position ( self , position_id ) : url = "/2/positions/%s" % position_id return self . position_from_json ( self . _get_resource ( url ) [ "position" ] )
4970	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerAdminForm , self ) . clean ( ) if 'catalog' in cleaned_data and not cleaned_data [ 'catalog' ] : cleaned_data [ 'catalog' ] = None return cleaned_data
12946	def copy ( self , copyPrimaryKey = False , copyValues = False ) : cpy = self . __class__ ( ** self . asDict ( copyPrimaryKey , forStorage = False ) ) if copyValues is True : for fieldName in cpy . FIELDS : setattr ( cpy , fieldName , copy . deepcopy ( getattr ( cpy , fieldName ) ) ) return cpy
9490	def _get_const_info ( const_index , const_list ) : argval = const_index if const_list is not None : try : argval = const_list [ const_index ] except IndexError : raise ValidationError ( "Consts value out of range: {}" . format ( const_index ) ) from None return argval , repr ( argval )
3029	def verify_id_token ( id_token , audience , http = None , cert_uri = ID_TOKEN_VERIFICATION_CERTS ) : _require_crypto_or_die ( ) if http is None : http = transport . get_cached_http ( ) resp , content = transport . request ( http , cert_uri ) if resp . status == http_client . OK : certs = json . loads ( _helpers . _from_bytes ( content ) ) return crypt . verify_signed_jwt_with_certs ( id_token , certs , audience ) else : raise VerifyJwtTokenError ( 'Status code: {0}' . format ( resp . status ) )
9581	def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
7955	def getpeercert ( self ) : with self . lock : if not self . _socket or self . _tls_state != "connected" : raise ValueError ( "Not TLS-connected" ) return get_certificate_from_ssl_socket ( self . _socket )
13408	def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) self . logButton . setObjectName ( "roundButton" ) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
9397	def run ( self ) : print ( 'Oct2Py speed test' ) print ( '*' * 20 ) time . sleep ( 1 ) print ( 'Raw speed: ' ) avg = timeit . timeit ( self . raw_speed , number = 10 ) / 10 print ( ' {0:0.01f} usec per loop' . format ( avg * 1e6 ) ) sides = [ 1 , 10 , 100 , 1000 ] runs = [ 10 , 10 , 10 , 5 ] for ( side , nruns ) in zip ( sides , runs ) : self . array = np . reshape ( np . arange ( side ** 2 ) , ( - 1 ) ) print ( 'Put {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_put , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) print ( 'Get {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_get , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) self . octave . exit ( ) print ( '*' * 20 ) print ( 'Test complete!' )
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
12806	def attach ( self , observer ) : if not observer in self . _observers : self . _observers . append ( observer ) return self
8816	def update_network ( context , id , network ) : LOG . info ( "update_network %s for tenant %s" % ( id , context . tenant_id ) ) with context . session . begin ( ) : net = db_api . network_find ( context , id = id , scope = db_api . ONE ) if not net : raise n_exc . NetworkNotFound ( net_id = id ) net_dict = network [ "network" ] utils . pop_param ( net_dict , "network_plugin" ) if not context . is_admin and "ipam_strategy" in net_dict : utils . pop_param ( net_dict , "ipam_strategy" ) net = db_api . network_update ( context , net , ** net_dict ) return v . _make_network_dict ( net )
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
9165	def expandvars_dict ( settings ) : return dict ( ( key , os . path . expandvars ( value ) ) for key , value in settings . iteritems ( ) )
9710	def heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
12347	def field_metadata ( self , well_row = 0 , well_column = 0 , field_row = 0 , field_column = 0 ) : def condition ( path ) : attrs = attributes ( path ) return ( attrs . u == well_column and attrs . v == well_row and attrs . x == field_column and attrs . y == field_row ) field = [ f for f in self . fields if condition ( f ) ] if field : field = field [ 0 ] filename = _pattern ( field , 'metadata' , _image , extension = '*.ome.xml' ) filename = glob ( filename ) [ 0 ] return objectify . parse ( filename ) . getroot ( )
11799	def prune ( self , var , value , removals ) : "Rule out var=value." self . curr_domains [ var ] . remove ( value ) if removals is not None : removals . append ( ( var , value ) )
8216	def hide_variables_window ( self ) : if self . var_window is not None : self . var_window . window . destroy ( ) self . var_window = None
12466	def run_cmd ( cmd , echo = False , fail_silently = False , ** kwargs ) : r out , err = None , None if echo : cmd_str = cmd if isinstance ( cmd , string_types ) else ' ' . join ( cmd ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = sys . stdout , sys . stderr print_message ( '$ {0}' . format ( cmd_str ) ) else : out , err = get_temp_streams ( ) kwargs [ 'stdout' ] , kwargs [ 'stderr' ] = out , err try : retcode = subprocess . call ( cmd , ** kwargs ) except subprocess . CalledProcessError as err : if fail_silently : return False print_error ( str ( err ) if IS_PY3 else unicode ( err ) ) finally : if out : out . close ( ) if err : err . close ( ) if retcode and echo and not fail_silently : print_error ( 'Command {0!r} returned non-zero exit status {1}' . format ( cmd_str , retcode ) ) return retcode
2993	def otcSymbolsDF ( token = '' , version = '' ) : df = pd . DataFrame ( otcSymbols ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
5916	def _translate_residue ( self , selection , default_atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( ** vars ( ) ) logger . error ( errmsg ) raise ValueError ( errmsg ) gmx_resid = self . gmx_resid ( int ( m . group ( 'resid' ) ) ) residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx_resname = utilities . convert_aa_code ( residue ) else : gmx_resname = residue gmx_atomname = m . group ( 'atom' ) if gmx_atomname is None : gmx_atomname = default_atomname return { 'resname' : gmx_resname , 'resid' : gmx_resid , 'atomname' : gmx_atomname }
3521	def performable ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return PerformableNode ( )
1991	def ls ( self , glob_str ) : path = os . path . join ( self . uri , glob_str ) return [ os . path . split ( s ) [ 1 ] for s in glob . glob ( path ) ]
11073	def set_nested ( data , value , * keys ) : if len ( keys ) == 1 : data [ keys [ 0 ] ] = value else : if keys [ 0 ] not in data : data [ keys [ 0 ] ] = { } set_nested ( data [ keys [ 0 ] ] , value , * keys [ 1 : ] )
1423	def copy ( self , new_object ) : new_object . classdesc = self . classdesc for name in self . classdesc . fields_names : new_object . __setattr__ ( name , getattr ( self , name ) )
3541	def do_apply ( mutation_pk , dict_synonyms , backup ) : filename , mutation_id = filename_and_mutation_id_from_pk ( int ( mutation_pk ) ) update_line_numbers ( filename ) context = Context ( mutation_id = mutation_id , filename = filename , dict_synonyms = dict_synonyms , ) mutate_file ( backup = backup , context = context , ) if context . number_of_performed_mutations == 0 : raise RuntimeError ( 'No mutations performed.' )
9505	def intersects ( self , i ) : return self . start <= i . end and i . start <= self . end
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
6686	def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
3396	def gapfill ( model , universal = None , lower_bound = 0.05 , penalties = None , demand_reactions = True , exchange_reactions = False , iterations = 1 ) : gapfiller = GapFiller ( model , universal = universal , lower_bound = lower_bound , penalties = penalties , demand_reactions = demand_reactions , exchange_reactions = exchange_reactions ) return gapfiller . fill ( iterations = iterations )
13786	def require ( name , field , data_type ) : if not isinstance ( field , data_type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data_type , field ) raise AssertionError ( msg )
8734	def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( )
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , ** kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
725	def get ( self , number ) : if not number in self . _patterns : raise IndexError ( "Invalid number" ) return self . _patterns [ number ]
1248	def is_action_available ( self , action ) : temp_state = np . rot90 ( self . _state , action ) return self . _is_action_available_left ( temp_state )
12096	def save ( self , * args , ** kwargs ) : current_activable_value = getattr ( self , self . ACTIVATABLE_FIELD_NAME ) is_active_changed = self . id is None or self . __original_activatable_value != current_activable_value self . __original_activatable_value = current_activable_value ret_val = super ( BaseActivatableModel , self ) . save ( * args , ** kwargs ) if is_active_changed : model_activations_changed . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) if self . activatable_field_updated : model_activations_updated . send ( self . __class__ , instance_ids = [ self . id ] , is_active = current_activable_value ) return ret_val
11722	def app_class ( ) : try : pkg_resources . get_distribution ( 'invenio-files-rest' ) from invenio_files_rest . app import Flask as FlaskBase except pkg_resources . DistributionNotFound : from flask import Flask as FlaskBase class Request ( TrustedHostsMixin , FlaskBase . request_class ) : pass class Flask ( FlaskBase ) : request_class = Request return Flask
13309	def fmt ( a , b ) : return 100 * np . min ( [ a , b ] , axis = 0 ) . sum ( ) / np . max ( [ a , b ] , axis = 0 ) . sum ( )
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
13643	def command_list ( ) : from cliez . conf import COMPONENT_ROOT root = COMPONENT_ROOT if root is None : sys . stderr . write ( "cliez.conf.COMPONENT_ROOT not set.\n" ) sys . exit ( 2 ) pass if not os . path . exists ( root ) : sys . stderr . write ( "please set a valid path for `cliez.conf.COMPONENT_ROOT`\n" ) sys . exit ( 2 ) pass try : path = os . listdir ( os . path . join ( root , 'components' ) ) return [ f [ : - 3 ] for f in path if f . endswith ( '.py' ) and f != '__init__.py' ] except FileNotFoundError : return [ ]
4661	def broadcast ( self , tx = None ) : if tx : return self . transactionbuilder_class ( tx , blockchain_instance = self ) . broadcast ( ) else : return self . txbuffer . broadcast ( )
6058	def bin_up_array_2d_using_mean ( array_2d , bin_up_factor ) : padded_array_2d = pad_2d_array_for_binning_up_with_bin_up_factor ( array_2d = array_2d , bin_up_factor = bin_up_factor ) binned_array_2d = np . zeros ( shape = ( padded_array_2d . shape [ 0 ] // bin_up_factor , padded_array_2d . shape [ 1 ] // bin_up_factor ) ) for y in range ( binned_array_2d . shape [ 0 ] ) : for x in range ( binned_array_2d . shape [ 1 ] ) : value = 0.0 for y1 in range ( bin_up_factor ) : for x1 in range ( bin_up_factor ) : padded_y = y * bin_up_factor + y1 padded_x = x * bin_up_factor + x1 value += padded_array_2d [ padded_y , padded_x ] binned_array_2d [ y , x ] = value / ( bin_up_factor ** 2.0 ) return binned_array_2d
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
10377	def one_sided ( value : float , distribution : List [ float ] ) -> float : assert distribution return sum ( value < element for element in distribution ) / len ( distribution )
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
1240	def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )
6376	def dist_abs ( self , src , tar , max_offset = 5 ) : if not src : return len ( tar ) if not tar : return len ( src ) src_len = len ( src ) tar_len = len ( tar ) src_cur = 0 tar_cur = 0 lcss = 0 local_cs = 0 while ( src_cur < src_len ) and ( tar_cur < tar_len ) : if src [ src_cur ] == tar [ tar_cur ] : local_cs += 1 else : lcss += local_cs local_cs = 0 if src_cur != tar_cur : src_cur = tar_cur = max ( src_cur , tar_cur ) for i in range ( max_offset ) : if not ( ( src_cur + i < src_len ) or ( tar_cur + i < tar_len ) ) : break if ( src_cur + i < src_len ) and ( src [ src_cur + i ] == tar [ tar_cur ] ) : src_cur += i local_cs += 1 break if ( tar_cur + i < tar_len ) and ( src [ src_cur ] == tar [ tar_cur + i ] ) : tar_cur += i local_cs += 1 break src_cur += 1 tar_cur += 1 lcss += local_cs return round ( max ( src_len , tar_len ) - lcss )
11536	def map_pin ( self , abstract_pin_id , physical_pin_id ) : if physical_pin_id : self . _pin_mapping [ abstract_pin_id ] = physical_pin_id else : self . _pin_mapping . pop ( abstract_pin_id , None )
537	def run ( self ) : descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) fieldStats = self . _getFieldStats ( ) self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) self . _periodic = self . _initPeriodicActivities ( ) numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , "when iterationCountInferOnly " "is specified, iterationCount must be greater than " "iterationCountInferOnly." learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) self . _finalize ( ) return ( self . _cmpReason , None )
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
8187	def prune ( self , depth = 0 ) : for n in list ( self . nodes ) : if len ( n . links ) <= depth : self . remove_node ( n . id )
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
10239	def count_citations_by_annotation ( graph : BELGraph , annotation : str ) -> Mapping [ str , typing . Counter [ str ] ] : citations = defaultdict ( lambda : defaultdict ( set ) ) for u , v , data in graph . edges ( data = True ) : if not edge_has_annotation ( data , annotation ) or CITATION not in data : continue k = data [ ANNOTATIONS ] [ annotation ] citations [ k ] [ u , v ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return { k : Counter ( itt . chain . from_iterable ( v . values ( ) ) ) for k , v in citations . items ( ) }
11872	def wait ( self , sec = 0.1 ) : sec = max ( sec , 0 ) reps = int ( floor ( sec / 0.1 ) ) commands = [ ] for i in range ( 0 , reps ) : commands . append ( Command ( 0x00 , wait = True ) ) return tuple ( commands )
5050	def from_children ( cls , program_uuid , * children ) : if not children or any ( child is None for child in children ) : return None granted = all ( ( child . granted for child in children ) ) exists = any ( ( child . exists for child in children ) ) usernames = set ( [ child . username for child in children ] ) enterprises = set ( [ child . enterprise_customer for child in children ] ) if not len ( usernames ) == len ( enterprises ) == 1 : raise InvalidProxyConsent ( 'Children used to create a bulk proxy consent object must ' 'share a single common username and EnterpriseCustomer.' ) username = children [ 0 ] . username enterprise_customer = children [ 0 ] . enterprise_customer return cls ( enterprise_customer = enterprise_customer , username = username , program_uuid = program_uuid , exists = exists , granted = granted , child_consents = children )
13766	def parse ( self , string ) : var , eq , values = string . strip ( ) . partition ( '=' ) assert var == 'runtimepath' assert eq == '=' return values . split ( ',' )
6221	def set_position ( self , x , y , z ) : self . position = Vector3 ( [ x , y , z ] )
10073	def build_deposit_schema ( self , record ) : schema_path = current_jsonschemas . url_to_path ( record [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path : return current_jsonschemas . path_to_url ( schema_prefix + schema_path )
10208	def record_view_event_builder ( event , sender_app , pid = None , record = None , ** kwargs ) : event . update ( dict ( timestamp = datetime . datetime . utcnow ( ) . isoformat ( ) , record_id = str ( record . id ) , pid_type = pid . pid_type , pid_value = str ( pid . pid_value ) , referrer = request . referrer , ** get_user ( ) ) ) return event
2720	def wait ( self , update_every_seconds = 1 ) : while self . status == u'in-progress' : sleep ( update_every_seconds ) self . load ( ) return self . status == u'completed'
12511	def load_nipy_img ( nii_file ) : import nipy if not os . path . exists ( nii_file ) : raise FileNotFound ( nii_file ) try : return nipy . load_image ( nii_file ) except Exception as exc : raise Exception ( 'Reading file {0}.' . format ( repr_imgs ( nii_file ) ) ) from exc
4075	def set_cfg_value ( config , section , option , value ) : if isinstance ( value , list ) : value = '\n' . join ( value ) config [ section ] [ option ] = value
3219	def get_network_acls ( vpc , ** conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
11816	def score ( self , code ) : text = permutation_decode ( self . ciphertext , code ) logP = ( sum ( [ log ( self . Pwords [ word ] ) for word in words ( text ) ] ) + sum ( [ log ( self . P1 [ c ] ) for c in text ] ) + sum ( [ log ( self . P2 [ b ] ) for b in bigrams ( text ) ] ) ) return exp ( logP )
13735	def get_api_error ( response ) : error_class = _status_code_to_class . get ( response . status_code , APIError ) return error_class ( response )
5219	def ref_file ( ticker : str , fld : str , has_date = False , cache = False , ext = 'parq' , ** kwargs ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if ( not data_path ) or ( not cache ) : return '' proper_ticker = ticker . replace ( '/' , '_' ) cache_days = kwargs . pop ( 'cache_days' , 10 ) root = f'{data_path}/{ticker.split()[-1]}/{proper_ticker}/{fld}' if len ( kwargs ) > 0 : info = utils . to_str ( kwargs ) [ 1 : - 1 ] . replace ( '|' , '_' ) else : info = 'ovrd=None' if has_date : cur_dt = utils . cur_time ( ) missing = f'{root}/asof={cur_dt}, {info}.{ext}' to_find = re . compile ( rf'{root}/asof=(.*), {info}\.pkl' ) cur_files = list ( filter ( to_find . match , sorted ( files . all_files ( path_name = root , keyword = info , ext = ext ) ) ) ) if len ( cur_files ) > 0 : upd_dt = to_find . match ( cur_files [ - 1 ] ) . group ( 1 ) diff = pd . Timestamp ( 'today' ) - pd . Timestamp ( upd_dt ) if diff >= pd . Timedelta ( days = cache_days ) : return missing return sorted ( cur_files ) [ - 1 ] else : return missing else : return f'{root}/{info}.{ext}'
11795	def min_conflicts ( csp , max_steps = 100000 ) : csp . current = current = { } for var in csp . vars : val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) for i in range ( max_steps ) : conflicted = csp . conflicted_vars ( current ) if not conflicted : return current var = random . choice ( conflicted ) val = min_conflicts_value ( csp , var , current ) csp . assign ( var , val , current ) return None
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : return [ ] else : return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
6632	def islast ( generator ) : next_x = None first = True for x in generator : if not first : yield ( next_x , False ) next_x = x first = False if not first : yield ( next_x , True )
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
8715	def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
4428	async def _now ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) song = 'Nothing' if player . current : position = lavalink . Utils . format_time ( player . position ) if player . current . stream : duration = '🔴 LIVE' else : duration = lavalink . Utils . format_time ( player . current . duration ) song = f'**[{player.current.title}]({player.current.uri})**\n({position}/{duration})' embed = discord . Embed ( color = discord . Color . blurple ( ) , title = 'Now Playing' , description = song ) await ctx . send ( embed = embed )
4369	def send ( self , message , json = False , callback = None ) : pkt = dict ( type = "message" , data = message , endpoint = self . ns_name ) if json : pkt [ 'type' ] = "json" if callback : pkt [ 'ack' ] = True pkt [ 'id' ] = msgid = self . socket . _get_next_msgid ( ) self . socket . _save_ack_callback ( msgid , callback ) self . socket . send_packet ( pkt )
8297	def hexDump ( bytes ) : for i in range ( len ( bytes ) ) : sys . stdout . write ( "%2x " % ( ord ( bytes [ i ] ) ) ) if ( i + 1 ) % 8 == 0 : print repr ( bytes [ i - 7 : i + 1 ] ) if ( len ( bytes ) % 8 != 0 ) : print string . rjust ( "" , 11 ) , repr ( bytes [ i - len ( bytes ) % 8 : i + 1 ] )
13909	def show_version ( self ) : class ShowVersionAction ( argparse . Action ) : def __init__ ( inner_self , nargs = 0 , ** kw ) : super ( ShowVersionAction , inner_self ) . __init__ ( nargs = nargs , ** kw ) def __call__ ( inner_self , parser , args , value , option_string = None ) : print ( "{parser_name} version: {version}" . format ( parser_name = self . config . get ( "parser" , { } ) . get ( "prog" ) , version = self . prog_version ) ) return ShowVersionAction
8869	def read_bgen ( filepath , metafile_filepath = None , samples_filepath = None , verbose = True ) : r assert_file_exist ( filepath ) assert_file_readable ( filepath ) metafile_filepath = _get_valid_metafile_filepath ( filepath , metafile_filepath ) if not os . path . exists ( metafile_filepath ) : if verbose : print ( f"We will create the metafile `{metafile_filepath}`. This file will " "speed up further\nreads and only need to be created once. So, please, " "bear with me." ) create_metafile ( filepath , metafile_filepath , verbose ) samples = get_samples ( filepath , samples_filepath , verbose ) variants = map_metadata ( filepath , metafile_filepath ) genotype = map_genotype ( filepath , metafile_filepath , verbose ) return dict ( variants = variants , samples = samples , genotype = genotype )
11745	def closure ( self , rules ) : closure = set ( ) todo = set ( rules ) while todo : rule = todo . pop ( ) closure . add ( rule ) if rule . at_end : continue symbol = rule . rhs [ rule . pos ] for production in self . nonterminals [ symbol ] : for first in self . first ( rule . rest ) : if EPSILON in production . rhs : new_rule = DottedRule ( production , 1 , first ) else : new_rule = DottedRule ( production , 0 , first ) if new_rule not in closure : todo . add ( new_rule ) return frozenset ( closure )
9514	def is_complete_orf ( self ) : if len ( self ) % 3 != 0 or len ( self ) < 6 : return False orfs = self . orfs ( ) complete_orf = intervals . Interval ( 0 , len ( self ) - 1 ) for orf in orfs : if orf == complete_orf : return True return False
6154	def position_CD ( Ka , out_type = 'fb_exact' ) : rs = 10 / ( 2 * np . pi ) if out_type . lower ( ) == 'open_loop' : b = np . array ( [ Ka * 4000 * rs ] ) a = np . array ( [ 1 , 1275 , 31250 , 0 ] ) elif out_type . lower ( ) == 'fb_approx' : b = np . array ( [ 3.2 * Ka * rs ] ) a = np . array ( [ 1 , 25 , 3.2 * Ka * rs ] ) elif out_type . lower ( ) == 'fb_exact' : b = np . array ( [ 4000 * Ka * rs ] ) a = np . array ( [ 1 , 1250 + 25 , 25 * 1250 , 4000 * Ka * rs ] ) else : raise ValueError ( 'out_type must be: open_loop, fb_approx, or fc_exact' ) return b , a
1941	def map_memory_callback ( self , address , size , perms , name , offset , result ) : logger . info ( ' ' . join ( ( "Mapping Memory @" , hex ( address ) if type ( address ) is int else "0x??" , hr_size ( size ) , "-" , perms , "-" , f"{name}:{hex(offset) if name else ''}" , "->" , hex ( result ) ) ) ) self . _emu . mem_map ( address , size , convert_permissions ( perms ) ) self . copy_memory ( address , size )
8562	def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
9150	def to_indra_statements ( self , * args , ** kwargs ) : graph = self . to_bel ( * args , ** kwargs ) return to_indra_statements ( graph )
12501	def _smooth_data_array ( arr , affine , fwhm , copy = True ) : if arr . dtype . kind == 'i' : if arr . dtype == np . int64 : arr = arr . astype ( np . float64 ) else : arr = arr . astype ( np . float32 ) if copy : arr = arr . copy ( ) arr [ np . logical_not ( np . isfinite ( arr ) ) ] = 0 try : affine = affine [ : 3 , : 3 ] fwhm_sigma_ratio = np . sqrt ( 8 * np . log ( 2 ) ) vox_size = np . sqrt ( np . sum ( affine ** 2 , axis = 0 ) ) sigma = fwhm / ( fwhm_sigma_ratio * vox_size ) for n , s in enumerate ( sigma ) : ndimage . gaussian_filter1d ( arr , s , output = arr , axis = n ) except : raise ValueError ( 'Error smoothing the array.' ) else : return arr
1582	def read ( self , dispatcher ) : try : if not self . is_header_read : to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) self . header += dispatcher . recv ( to_read ) if len ( self . header ) == HeronProtocol . HEADER_SIZE : self . is_header_read = True else : Log . debug ( "Header read incomplete; read %d bytes of header" % len ( self . header ) ) return if self . is_header_read and not self . is_complete : to_read = self . get_datasize ( ) - len ( self . data ) self . data += dispatcher . recv ( to_read ) if len ( self . data ) == self . get_datasize ( ) : self . is_complete = True except socket . error as e : if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : Log . debug ( "Try again error" ) else : Log . debug ( "Fatal error when reading IncomingPacket" ) raise RuntimeError ( "Fatal error occured in IncomingPacket.read()" )
2829	def convert_upsample_bilinear ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) output_size = params [ 'output_size' ] align_corners = params [ 'align_corners' ] > 0 def target_layer ( x , size = output_size , align_corners = align_corners ) : import tensorflow as tf x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ] ) x = tf . image . resize_images ( x , size , align_corners = align_corners ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) return x lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
605	def getVersion ( ) : with open ( os . path . join ( REPO_DIR , "VERSION" ) , "r" ) as versionFile : return versionFile . read ( ) . strip ( )
5003	def handle ( self , * args , ** options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
13823	def end_timing ( self ) : if self . _callback != None : elapsed = time . clock ( ) * 1000 - self . _start self . _callback . end_timing ( self . _counter , elapsed )
12884	def field_type ( self ) : if not self . model : return 'JSON' database = self . model . _meta . database if isinstance ( database , Proxy ) : database = database . obj if Json and isinstance ( database , PostgresqlDatabase ) : return 'JSON' return 'TEXT'
7892	def join ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if self . joined : raise RuntimeError ( "Room is already joined" ) p = MucPresence ( to_jid = self . room_jid ) p . make_join_request ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) self . manager . stream . send ( p )
11524	def mfa_otp_login ( self , temp_token , one_time_pass ) : parameters = dict ( ) parameters [ 'mfaTokenId' ] = temp_token parameters [ 'otp' ] = one_time_pass response = self . request ( 'midas.mfa.otp.login' , parameters ) return response [ 'token' ]
10314	def canonical_circulation ( elements : T , key : Optional [ Callable [ [ T ] , bool ] ] = None ) -> T : return min ( get_circulations ( elements ) , key = key )
12683	def notice_settings ( request ) : notice_types = NoticeType . objects . all ( ) settings_table = [ ] for notice_type in notice_types : settings_row = [ ] for medium_id , medium_display in NOTICE_MEDIA : form_label = "%s_%s" % ( notice_type . label , medium_id ) setting = NoticeSetting . for_user ( request . user , notice_type , medium_id ) if request . method == "POST" : if request . POST . get ( form_label ) == "on" : if not setting . send : setting . send = True setting . save ( ) else : if setting . send : setting . send = False setting . save ( ) settings_row . append ( ( form_label , setting . send ) ) settings_table . append ( { "notice_type" : notice_type , "cells" : settings_row } ) if request . method == "POST" : next_page = request . POST . get ( "next_page" , "." ) return HttpResponseRedirect ( next_page ) settings = { "column_headers" : [ medium_display for medium_id , medium_display in NOTICE_MEDIA ] , "rows" : settings_table , } return render_to_response ( "notification/notice_settings.html" , { "notice_types" : notice_types , "notice_settings" : settings , } , context_instance = RequestContext ( request ) )
11518	def generate_upload_token ( self , token , item_id , filename , checksum = None ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemid' ] = item_id parameters [ 'filename' ] = filename if checksum is not None : parameters [ 'checksum' ] = checksum response = self . request ( 'midas.upload.generatetoken' , parameters ) return response [ 'token' ]
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
220	async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ "method" ] not in ( "GET" , "HEAD" ) : return PlainTextResponse ( "Method Not Allowed" , status_code = 405 ) if path . startswith ( ".." ) : return PlainTextResponse ( "Not Found" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : index_path = os . path . join ( path , "index.html" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ "path" ] . endswith ( "/" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + "/" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : full_path , stat_result = await self . lookup_path ( "404.html" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( "Not Found" , status_code = 404 )
1738	def unify_string_literals ( js_string ) : n = 0 res = '' limit = len ( js_string ) while n < limit : char = js_string [ n ] if char == '\\' : new , n = do_escape ( js_string , n ) res += new else : res += char n += 1 return res
9520	def make_random_contigs ( contigs , length , outfile , name_by_letters = False , prefix = '' , seed = None , first_number = 1 ) : random . seed ( a = seed ) fout = utils . open_file_write ( outfile ) letters = list ( 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' ) letters_index = 0 for i in range ( contigs ) : if name_by_letters : name = letters [ letters_index ] letters_index += 1 if letters_index == len ( letters ) : letters_index = 0 else : name = str ( i + first_number ) fa = sequences . Fasta ( prefix + name , '' . join ( [ random . choice ( 'ACGT' ) for x in range ( length ) ] ) ) print ( fa , file = fout ) utils . close ( fout )
9138	def drop_all ( self , check_first : bool = True ) : self . _metadata . drop_all ( self . engine , checkfirst = check_first ) self . _store_drop ( )
9007	def transfer_to_row ( self , new_row ) : if new_row != self . _row : index = self . get_index_in_row ( ) if index is not None : self . _row . instructions . pop ( index ) self . _row = new_row
2811	def convert_reshape ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reshape ...' ) if names == 'short' : tf_name = 'RESH' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) > 1 : if layers [ inputs [ 1 ] ] [ 0 ] == - 1 : print ( 'Cannot deduct batch size! It will be omitted, but result may be wrong.' ) reshape = keras . layers . Reshape ( layers [ inputs [ 1 ] + '_np' ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : if inputs [ 0 ] in layers : reshape = keras . layers . Reshape ( params [ 'shape' ] [ 1 : ] , name = tf_name ) layers [ scope_name ] = reshape ( layers [ inputs [ 0 ] ] ) else : print ( 'Skip weight matrix transpose, but result may be wrong.' )
913	def read ( cls , proto ) : instance = object . __new__ ( cls ) super ( PreviousValueModel , instance ) . __init__ ( proto = proto . modelBase ) instance . _logger = opf_utils . initLogger ( instance ) if len ( proto . predictedField ) : instance . _predictedField = proto . predictedField else : instance . _predictedField = None instance . _fieldNames = list ( proto . fieldNames ) instance . _fieldTypes = list ( proto . fieldTypes ) instance . _predictionSteps = list ( proto . predictionSteps ) return instance
5715	def _slugify_foreign_key ( schema ) : for foreign_key in schema . get ( 'foreignKeys' , [ ] ) : foreign_key [ 'reference' ] [ 'resource' ] = _slugify_resource_name ( foreign_key [ 'reference' ] . get ( 'resource' , '' ) ) return schema
4437	def build ( self , track , requester ) : try : self . track = track [ 'track' ] self . identifier = track [ 'info' ] [ 'identifier' ] self . can_seek = track [ 'info' ] [ 'isSeekable' ] self . author = track [ 'info' ] [ 'author' ] self . duration = track [ 'info' ] [ 'length' ] self . stream = track [ 'info' ] [ 'isStream' ] self . title = track [ 'info' ] [ 'title' ] self . uri = track [ 'info' ] [ 'uri' ] self . requester = requester return self except KeyError : raise InvalidTrack ( 'An invalid track was passed.' )
518	def _raisePermanenceToThreshold ( self , perm , mask ) : if len ( mask ) < self . _stimulusThreshold : raise Exception ( "This is likely due to a " + "value of stimulusThreshold that is too large relative " + "to the input size. [len(mask) < self._stimulusThreshold]" ) numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) while True : numConnected = numpy . nonzero ( perm > self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] . size if numConnected >= self . _stimulusThreshold : return perm [ mask ] += self . _synPermBelowStimulusInc
4430	async def _remove ( self , ctx , index : int ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'Nothing queued.' ) if index > len ( player . queue ) or index < 1 : return await ctx . send ( f'Index has to be **between** 1 and {len(player.queue)}' ) index -= 1 removed = player . queue . pop ( index ) await ctx . send ( f'Removed **{removed.title}** from the queue.' )
4049	def fulltext_item ( self , itemkey , ** kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
4866	def save ( self ) : enterprise_customer = self . validated_data [ 'enterprise_customer' ] ecu = models . EnterpriseCustomerUser ( user_id = self . user . pk , enterprise_customer = enterprise_customer , ) ecu . save ( )
13438	def _extendrange ( self , start , end ) : range_positions = [ ] for i in range ( start , end ) : if i != 0 : range_positions . append ( str ( i ) ) if i < end : range_positions . append ( self . separator ) return range_positions
4112	def rc2ac ( k , R0 ) : [ a , efinal ] = rc2poly ( k , R0 ) R , u , kr , e = rlevinson ( a , efinal ) return R
8624	def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11078	def start_timer ( self , duration , func , * args ) : t = threading . Timer ( duration , self . _timer_callback , ( func , args ) ) self . _timer_callbacks [ func ] = t t . start ( ) self . log . info ( "Scheduled call to %s in %ds" , func . __name__ , duration )
13799	def log ( self , string ) : self . wfile . write ( json . dumps ( { 'log' : string } ) + NEWLINE )
7114	def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
5395	def _get_input_target_path ( self , local_file_path ) : path , filename = os . path . split ( local_file_path ) if '*' in filename : return path + '/' else : return local_file_path
1218	def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before save" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )
595	def _getTPClass ( temporalImp ) : if temporalImp == 'py' : return backtracking_tm . BacktrackingTM elif temporalImp == 'cpp' : return backtracking_tm_cpp . BacktrackingTMCPP elif temporalImp == 'tm_py' : return backtracking_tm_shim . TMShim elif temporalImp == 'tm_cpp' : return backtracking_tm_shim . TMCPPShim elif temporalImp == 'monitored_tm_py' : return backtracking_tm_shim . MonitoredTMShim else : raise RuntimeError ( "Invalid temporalImp '%s'. Legal values are: 'py', " "'cpp', 'tm_py', 'monitored_tm_py'" % ( temporalImp ) )
6313	def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
5506	def screenshot ( url , * args , ** kwargs ) : phantomscript = os . path . join ( os . path . dirname ( __file__ ) , 'take_screenshot.js' ) directory = kwargs . get ( 'save_dir' , '/tmp' ) image_name = kwargs . get ( 'image_name' , None ) or _image_name_from_url ( url ) ext = kwargs . get ( 'format' , 'png' ) . lower ( ) save_path = os . path . join ( directory , image_name ) + '.' + ext crop_to_visible = kwargs . get ( 'crop_to_visible' , False ) cmd_args = [ 'phantomjs' , '--ssl-protocol=any' , phantomscript , url , '--width' , str ( kwargs [ 'width' ] ) , '--height' , str ( kwargs [ 'height' ] ) , '--useragent' , str ( kwargs [ 'user_agent' ] ) , '--dir' , directory , '--ext' , ext , '--name' , str ( image_name ) , ] if crop_to_visible : cmd_args . append ( '--croptovisible' ) output = subprocess . Popen ( cmd_args , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] return Screenshot ( save_path , directory , image_name + '.' + ext , ext )
8085	def nostroke ( self ) : c = self . _canvas . strokecolor self . _canvas . strokecolor = None return c
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : "Add to chart any rules for B that could help extend this edge." B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
1976	def sys_receive ( self , cpu , fd , buf , count , rx_bytes ) : data = '' if count != 0 : if not self . _is_open ( fd ) : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EBADF" ) return Decree . CGC_EBADF if buf not in cpu . memory : logger . info ( "RECEIVE: buf points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT if fd > 2 and self . files [ fd ] . is_empty ( ) : cpu . PC -= cpu . instruction . size self . wait ( [ fd ] , [ ] , None ) raise RestartSyscall ( ) data = self . files [ fd ] . receive ( count ) self . syscall_trace . append ( ( "_receive" , fd , data ) ) cpu . write_bytes ( buf , data ) self . signal_receive ( fd ) if rx_bytes : if rx_bytes not in cpu . memory : logger . info ( "RECEIVE: Not valid file descriptor on receive. Returning EFAULT" ) return Decree . CGC_EFAULT cpu . write_int ( rx_bytes , len ( data ) , 32 ) logger . info ( "RECEIVE(%d, 0x%08x, %d, 0x%08x) -> <%s> (size:%d)" % ( fd , buf , count , rx_bytes , repr ( data ) [ : min ( count , 10 ) ] , len ( data ) ) ) return 0
1963	def sys_rt_sigprocmask ( self , cpu , how , newset , oldset ) : return self . sys_sigprocmask ( cpu , how , newset , oldset )
7778	def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) != VCARD_NS : raise ValueError ( "Not in the %r namespace" % ( VCARD_NS , ) ) if data . name != "vCard" : raise ValueError ( "Bad root element name: %r" % ( data . name , ) ) n = data . children dns = get_node_ns ( data ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and dns and ns . getContent ( ) != dns . getContent ( ) ) : n = n . next continue if not self . components . has_key ( n . name ) : n = n . next continue cl , tp = self . components [ n . name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( n . name ) : raise ValueError ( "Duplicate %s" % ( n . name , ) ) try : self . content [ n . name ] = cl ( n . name , n ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( n . name ) : self . content [ n . name ] = [ ] try : self . content [ n . name ] . append ( cl ( n . name , n ) ) except Empty : pass n = n . next
8683	def export ( self , output_path = None , decrypt = False ) : self . _assert_valid_stash ( ) all_keys = [ ] for key in self . list ( ) : all_keys . append ( dict ( self . get ( key , decrypt = decrypt ) ) ) if all_keys : if output_path : with open ( output_path , 'w' ) as output_file : output_file . write ( json . dumps ( all_keys , indent = 4 ) ) return all_keys else : raise GhostError ( 'There are no keys to export' )
10803	def tk ( self , k , x ) : weights = np . diag ( np . ones ( k + 1 ) ) [ k ] return np . polynomial . chebyshev . chebval ( self . _x2c ( x ) , weights )
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
523	def _updateBoostFactorsLocal ( self ) : targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) for i in xrange ( self . _numColumns ) : maskNeighbors = self . _getColumnNeighborhood ( i ) targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )
7829	def add_option ( self , value , label ) : if type ( value ) is list : warnings . warn ( ".add_option() accepts single value now." , DeprecationWarning , stacklevel = 1 ) value = value [ 0 ] if self . type not in ( "list-multi" , "list-single" ) : raise ValueError ( "Options are allowed only for list types." ) option = Option ( value , label ) self . options . append ( option ) return option
9718	async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
8305	def live_source_load ( self , source ) : source = source . rstrip ( '\n' ) if source != self . source : self . source = source b64_source = base64 . b64encode ( bytes ( bytearray ( source , "ascii" ) ) ) self . send_command ( CMD_LOAD_BASE64 , b64_source )
2271	def _win32_symlink ( path , link , verbose = 0 ) : from ubelt import util_cmd if os . path . isdir ( path ) : if verbose : print ( '... as directory symlink' ) command = 'mklink /D "{}" "{}"' . format ( link , path ) else : if verbose : print ( '... as file symlink' ) command = 'mklink "{}" "{}"' . format ( link , path ) if command is not None : info = util_cmd . cmd ( command , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format permission_msg = 'You do not have sufficient privledges' if permission_msg not in info [ 'err' ] : print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) return link
12618	def get_shape ( img ) : if hasattr ( img , 'shape' ) : shape = img . shape else : shape = img . get_data ( ) . shape return shape
13153	def dict_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . DICT ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
8201	def settings ( self , ** kwargs ) : for k , v in kwargs . items ( ) : setattr ( self , k , v )
2582	def load_checkpoints ( self , checkpointDirs ) : self . memo_lookup_table = None if not checkpointDirs : return { } if type ( checkpointDirs ) is not list : raise BadCheckpoint ( "checkpointDirs expects a list of checkpoints" ) return self . _load_checkpoints ( checkpointDirs )
9897	def boottime ( ) : global __boottime if __boottime is None : up = uptime ( ) if up is None : return None if __boottime is None : _boottime_linux ( ) if datetime is None : raise RuntimeError ( 'datetime module required.' ) return datetime . fromtimestamp ( __boottime or time . time ( ) - up )
3397	def extend_model ( self , exchange_reactions = False , demand_reactions = True ) : for rxn in self . universal . reactions : rxn . gapfilling_type = 'universal' new_metabolites = self . universal . metabolites . query ( lambda metabolite : metabolite not in self . model . metabolites ) self . model . add_metabolites ( new_metabolites ) existing_exchanges = [ ] for rxn in self . universal . boundary : existing_exchanges = existing_exchanges + [ met . id for met in list ( rxn . metabolites ) ] for met in self . model . metabolites : if exchange_reactions : if met . id not in existing_exchanges : rxn = self . universal . add_boundary ( met , type = 'exchange_smiley' , lb = - 1000 , ub = 0 , reaction_id = 'EX_{}' . format ( met . id ) ) rxn . gapfilling_type = 'exchange' if demand_reactions : rxn = self . universal . add_boundary ( met , type = 'demand_smiley' , lb = 0 , ub = 1000 , reaction_id = 'DM_{}' . format ( met . id ) ) rxn . gapfilling_type = 'demand' new_reactions = self . universal . reactions . query ( lambda reaction : reaction not in self . model . reactions ) self . model . add_reactions ( new_reactions )
4867	def to_representation ( self , instance ) : updated_course = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( updated_course [ 'key' ] ) for course_run in updated_course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_course
9013	def knitting_pattern_set ( self , values ) : self . _start ( ) pattern_collection = self . _new_pattern_collection ( ) self . _fill_pattern_collection ( pattern_collection , values ) self . _create_pattern_set ( pattern_collection , values ) return self . _pattern_set
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
9579	def read_cell_array ( fd , endian , header ) : array = [ list ( ) for i in range ( header [ 'dims' ] [ 0 ] ) ] for row in range ( header [ 'dims' ] [ 0 ] ) : for col in range ( header [ 'dims' ] [ 1 ] ) : vheader , next_pos , fd_var = read_var_header ( fd , endian ) varray = read_var_array ( fd_var , endian , vheader ) array [ row ] . append ( varray ) fd . seek ( next_pos ) if header [ 'dims' ] [ 0 ] == 1 : return squeeze ( array [ 0 ] ) return squeeze ( array )
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
3997	def copy_between_containers ( source_name , source_path , dest_name , dest_path ) : if not container_path_exists ( source_name , source_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( source_path , source_name ) ) temp_path = os . path . join ( tempfile . mkdtemp ( ) , str ( uuid . uuid1 ( ) ) ) with _cleanup_path ( temp_path ) : copy_to_local ( temp_path , source_name , source_path , demote = False ) copy_from_local ( temp_path , dest_name , dest_path , demote = False )
2028	def CALLDATALOAD ( self , offset ) : if issymbolic ( offset ) : if solver . can_be_true ( self . _constraints , offset == self . _used_calldata_size ) : self . constraints . add ( offset == self . _used_calldata_size ) raise ConcretizeArgument ( 1 , policy = 'SAMPLED' ) self . _use_calldata ( offset , 32 ) data_length = len ( self . data ) bytes = [ ] for i in range ( 32 ) : try : c = Operators . ITEBV ( 8 , offset + i < data_length , self . data [ offset + i ] , 0 ) except IndexError : c = 0 bytes . append ( c ) return Operators . CONCAT ( 256 , * bytes )
8383	def update ( self ) : if self . delay > 0 : self . delay -= 1 return if self . fi == 0 : if len ( self . q ) == 1 : self . fn = float ( "inf" ) else : self . fn = len ( self . q [ self . i ] ) / self . speed self . fn = max ( self . fn , self . mf ) self . fi += 1 if self . fi > self . fn : self . fi = 0 self . i = ( self . i + 1 ) % len ( self . q )
5595	def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
11639	def json_write_data ( json_data , filename ) : with open ( filename , 'w' ) as fp : json . dump ( json_data , fp , indent = 4 , sort_keys = True , ensure_ascii = False ) return True return False
6702	def exists ( self , name ) : with self . settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn_only = True ) : return self . run ( 'getent group %(name)s' % locals ( ) ) . succeeded
8941	def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( ** url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( ** data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
3126	def get ( self , template_id , ** queryparams ) : self . template_id = template_id return self . _mc_client . _get ( url = self . _build_path ( template_id ) , ** queryparams )
976	def _newRepresentation ( self , index , newIndex ) : newRepresentation = self . bucketMap [ index ] . copy ( ) ri = newIndex % self . w newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit while newBit in self . bucketMap [ index ] or not self . _newRepresentationOK ( newRepresentation , newIndex ) : self . numTries += 1 newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit return newRepresentation
5205	def proc_elms ( ** kwargs ) -> list : return [ ( ELEM_KEYS . get ( k , k ) , ELEM_VALS . get ( ELEM_KEYS . get ( k , k ) , dict ( ) ) . get ( v , v ) ) for k , v in kwargs . items ( ) if ( k in list ( ELEM_KEYS . keys ( ) ) + list ( ELEM_KEYS . values ( ) ) ) and ( k not in PRSV_COLS ) ]
89	def new_random_state ( seed = None , fully_random = False ) : if seed is None : if not fully_random : seed = CURRENT_RANDOM_STATE . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return np . random . RandomState ( seed )
3832	async def send_chat_message ( self , send_chat_message_request ) : response = hangouts_pb2 . SendChatMessageResponse ( ) await self . _pb_request ( 'conversations/sendchatmessage' , send_chat_message_request , response ) return response
1744	def pythonize_arguments ( arg_str ) : out_args = [ ] if arg_str is None : return out_str args = arg_str . split ( ',' ) for arg in args : components = arg . split ( '=' ) name_and_type = components [ 0 ] . split ( ' ' ) if name_and_type [ - 1 ] == '' and len ( name_and_type ) > 1 : name = name_and_type [ - 2 ] else : name = name_and_type [ - 1 ] if len ( components ) > 1 : name += '=' + components [ 1 ] out_args . append ( name ) return ',' . join ( out_args )
9236	def _signal_handler_map ( self ) : result = { } for signum , handler in self . signal_map . items ( ) : result [ signum ] = self . _get_signal_handler ( handler ) return result
8031	def compareChunks ( handles , chunk_size = CHUNK_SIZE ) : chunks = [ ( path , fh , fh . read ( chunk_size ) ) for path , fh , _ in handles ] more , done = [ ] , [ ] while chunks : matches , non_matches = [ chunks [ 0 ] ] , [ ] for chunk in chunks [ 1 : ] : if matches [ 0 ] [ 2 ] == chunk [ 2 ] : matches . append ( chunk ) else : non_matches . append ( chunk ) if len ( matches ) == 1 or matches [ 0 ] [ 2 ] == "" : for x in matches : x [ 1 ] . close ( ) done . append ( [ x [ 0 ] for x in matches ] ) else : more . append ( matches ) chunks = non_matches return more , done
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
10956	def get ( self , name ) : for c in self . comps : if c . category == name : return c return None
8864	def icon_from_typename ( name , icon_type ) : ICONS = { 'CLASS' : ICON_CLASS , 'IMPORT' : ICON_NAMESPACE , 'STATEMENT' : ICON_VAR , 'FORFLOW' : ICON_VAR , 'FORSTMT' : ICON_VAR , 'WITHSTMT' : ICON_VAR , 'GLOBALSTMT' : ICON_VAR , 'MODULE' : ICON_NAMESPACE , 'KEYWORD' : ICON_KEYWORD , 'PARAM' : ICON_VAR , 'ARRAY' : ICON_VAR , 'INSTANCEELEMENT' : ICON_VAR , 'INSTANCE' : ICON_VAR , 'PARAM-PRIV' : ICON_VAR , 'PARAM-PROT' : ICON_VAR , 'FUNCTION' : ICON_FUNC , 'DEF' : ICON_FUNC , 'FUNCTION-PRIV' : ICON_FUNC_PRIVATE , 'FUNCTION-PROT' : ICON_FUNC_PROTECTED } ret_val = None icon_type = icon_type . upper ( ) if hasattr ( name , "string" ) : name = name . string if icon_type == "FORFLOW" or icon_type == "STATEMENT" : icon_type = "PARAM" if icon_type == "PARAM" or icon_type == "FUNCTION" : if name . startswith ( "__" ) : icon_type += "-PRIV" elif name . startswith ( "_" ) : icon_type += "-PROT" if icon_type in ICONS : ret_val = ICONS [ icon_type ] elif icon_type : _logger ( ) . warning ( "Unimplemented completion icon_type: %s" , icon_type ) return ret_val
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( "--" ) and len ( arg ) > 2 : break if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )
10082	def _prepare_edit ( self , record ) : data = record . dumps ( ) data [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] = record . revision_id data [ '_deposit' ] [ 'status' ] = 'draft' data [ '$schema' ] = self . build_deposit_schema ( record ) return data
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights loss = tf . divide ( tf . reduce_sum ( losses ) , tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
13547	def _setVirtualEnv ( ) : try : activate = options . virtualenv . activate_cmd except AttributeError : activate = None if activate is None : virtualenv = path ( os . environ . get ( 'VIRTUAL_ENV' , '' ) ) if not virtualenv : virtualenv = options . paved . cwd else : virtualenv = path ( virtualenv ) activate = virtualenv / 'bin' / 'activate' if activate . exists ( ) : info ( 'Using default virtualenv at %s' % activate ) options . setdotted ( 'virtualenv.activate_cmd' , 'source %s' % activate )
6673	def is_link ( self , path , use_sudo = False ) : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -L "%(path)s" ]' % locals ( ) ) . succeeded
10769	def matlab_formatter ( level , vertices , codes = None ) : vertices = numpy_formatter ( level , vertices , codes ) if codes is not None : level = level [ 0 ] headers = np . vstack ( ( [ v . shape [ 0 ] for v in vertices ] , [ level ] * len ( vertices ) ) ) . T vertices = np . vstack ( list ( it . __next__ ( ) for it in itertools . cycle ( ( iter ( headers ) , iter ( vertices ) ) ) ) ) return vertices
5482	def retry_auth_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in HTTP_AUTH_ERROR_CODES : _print_error ( 'Retrying...' ) return True return False
2392	def edit_distance ( s1 , s2 ) : d = { } lenstr1 = len ( s1 ) lenstr2 = len ( s2 ) for i in xrange ( - 1 , lenstr1 + 1 ) : d [ ( i , - 1 ) ] = i + 1 for j in xrange ( - 1 , lenstr2 + 1 ) : d [ ( - 1 , j ) ] = j + 1 for i in xrange ( lenstr1 ) : for j in xrange ( lenstr2 ) : if s1 [ i ] == s2 [ j ] : cost = 0 else : cost = 1 d [ ( i , j ) ] = min ( d [ ( i - 1 , j ) ] + 1 , d [ ( i , j - 1 ) ] + 1 , d [ ( i - 1 , j - 1 ) ] + cost , ) if i and j and s1 [ i ] == s2 [ j - 1 ] and s1 [ i - 1 ] == s2 [ j ] : d [ ( i , j ) ] = min ( d [ ( i , j ) ] , d [ i - 2 , j - 2 ] + cost ) return d [ lenstr1 - 1 , lenstr2 - 1 ]
11688	def get_changeset ( changeset ) : url = 'https://www.openstreetmap.org/api/0.6/changeset/{}/download' . format ( changeset ) return ET . fromstring ( requests . get ( url ) . content )
4751	def run ( self , shell = True , cmdline = False , echo = True ) : if env ( ) : return 1 cmd = [ "fio" ] + self . __parse_parms ( ) if cmdline : cij . emph ( "cij.fio.run: shell: %r, cmd: %r" % ( shell , cmd ) ) return cij . ssh . command ( cmd , shell , echo )
494	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
4612	def block_timestamp ( self , block_num ) : return int ( self . block_class ( block_num , blockchain_instance = self . blockchain ) . time ( ) . timestamp ( ) )
8530	def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
1573	def add_verbose ( parser ) : parser . add_argument ( '--verbose' , metavar = '(a boolean; default: "false")' , type = bool , default = False ) return parser
6736	def reboot_or_dryrun ( * args , ** kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) try : if verbose : print ( 'Reconnecting to:' , env . host_string ) connections . connect ( env . host_string ) with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
4138	def scale_image ( in_fname , out_fname , max_width , max_height ) : try : from PIL import Image except ImportError : import Image img = Image . open ( in_fname ) width_in , height_in = img . size scale_w = max_width / float ( width_in ) scale_h = max_height / float ( height_in ) if height_in * scale_w <= max_height : scale = scale_w else : scale = scale_h if scale >= 1.0 and in_fname == out_fname : return width_sc = int ( round ( scale * width_in ) ) height_sc = int ( round ( scale * height_in ) ) img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) pos_insert = ( ( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) thumb . paste ( img , pos_insert ) thumb . save ( out_fname ) if os . environ . get ( 'SKLEARN_DOC_OPTIPNG' , False ) : try : subprocess . call ( [ "optipng" , "-quiet" , "-o" , "9" , out_fname ] ) except Exception : warnings . warn ( 'Install optipng to reduce the size of the \ generated images' )
9079	def decode_timestamp ( data : str ) -> datetime . datetime : year = 2000 + int ( data [ 0 : 2 ] ) month = int ( data [ 2 : 4 ] ) day = int ( data [ 4 : 6 ] ) hour = int ( data [ 6 : 8 ] ) minute = int ( data [ 8 : 10 ] ) second = int ( data [ 10 : 12 ] ) if minute == 60 : minute = 0 hour += 1 return datetime . datetime ( year = year , month = month , day = day , hour = hour , minute = minute , second = second )
3922	def keypress ( self , size , key ) : self . _coroutine_queue . put ( self . _client . set_active ( ) ) self . _coroutine_queue . put ( self . _conversation . update_read_timestamp ( ) ) return super ( ) . keypress ( size , key )
7449	def combinefiles ( filepath ) : fastqs = glob . glob ( filepath ) firsts = [ i for i in fastqs if "_R1_" in i ] if not firsts : raise IPyradWarningExit ( "First read files names must contain '_R1_'." ) seconds = [ ff . replace ( "_R1_" , "_R2_" ) for ff in firsts ] return zip ( firsts , seconds )
10734	def fork ( self , name ) : fork = deepcopy ( self ) self [ name ] = fork return fork
3087	def _get_entity ( self ) : if self . _is_ndb ( ) : return self . _model . get_by_id ( self . _key_name ) else : return self . _model . get_by_key_name ( self . _key_name )
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
2862	def _i2c_write_bytes ( self , data ) : for byte in data : self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) self . _command . append ( '\x22\x00' ) self . _expected += len ( data )
12811	def lineReceived ( self , line ) : while self . _in_header : if line : self . _headers . append ( line ) else : http , status , message = self . _headers [ 0 ] . split ( " " , 2 ) status = int ( status ) if status == 200 : self . factory . get_stream ( ) . connected ( ) else : self . factory . continueTrying = 0 self . transport . loseConnection ( ) self . factory . get_stream ( ) . disconnected ( RuntimeError ( status , message ) ) return self . _in_header = False break else : try : self . _len_expected = int ( line , 16 ) self . setRawMode ( ) except : pass
8345	def find ( self , name = None , attrs = { } , recursive = True , text = None , ** kwargs ) : r = None l = self . findAll ( name , attrs , recursive , text , 1 , ** kwargs ) if l : r = l [ 0 ] return r
13796	def handle_rereduce ( self , reduce_function_names , values ) : reduce_functions = [ ] for reduce_function_name in reduce_function_names : try : reduce_function = get_function ( reduce_function_name ) if getattr ( reduce_function , 'view_decorated' , None ) : reduce_function = reduce_function ( self . log ) reduce_functions . append ( reduce_function ) except Exception , exc : self . log ( repr ( exc ) ) reduce_functions . append ( lambda * args , ** kwargs : None ) results = [ ] for reduce_function in reduce_functions : try : results . append ( reduce_function ( None , values , rereduce = True ) ) except Exception , exc : self . log ( repr ( exc ) ) results . append ( None ) return [ True , results ]
1371	def get_heron_dir ( ) : go_above_dirs = 9 path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) return normalized_class_path ( path )
2260	def group_items ( items , groupids ) : r if callable ( groupids ) : keyfunc = groupids pair_list = ( ( keyfunc ( item ) , item ) for item in items ) else : pair_list = zip ( groupids , items ) groupid_to_items = defaultdict ( list ) for key , item in pair_list : groupid_to_items [ key ] . append ( item ) return groupid_to_items
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
9677	def calculate_bin_boundary ( self , bb ) : return min ( enumerate ( OPC_LOOKUP ) , key = lambda x : abs ( x [ 1 ] - bb ) ) [ 0 ]
2534	def validate_str_fields ( self , fields , optional , messages ) : for field_str in fields : field = getattr ( self , field_str ) if field is not None : attr = getattr ( field , '__str__' , None ) if not callable ( attr ) : messages = messages + [ '{0} must provide __str__ method.' . format ( field ) ] elif not optional : messages = messages + [ 'Package {0} can not be None.' . format ( field_str ) ] return messages
3381	def shared_np_array ( shape , data = None , integer = False ) : size = np . prod ( shape ) if integer : array = Array ( ctypes . c_int64 , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) , dtype = "int64" ) else : array = Array ( ctypes . c_double , int ( size ) ) np_array = np . frombuffer ( array . get_obj ( ) ) np_array = np_array . reshape ( shape ) if data is not None : if len ( shape ) != len ( data . shape ) : raise ValueError ( "`data` must have the same dimensions" "as the created array." ) same = all ( x == y for x , y in zip ( shape , data . shape ) ) if not same : raise ValueError ( "`data` must have the same shape" "as the created array." ) np_array [ : ] = data return np_array
10902	def lbl ( axis , label , size = 22 ) : at = AnchoredText ( label , loc = 2 , prop = dict ( size = size ) , frameon = True ) at . patch . set_boxstyle ( "round,pad=0.,rounding_size=0.0" ) axis . add_artist ( at )
8029	def sizeClassifier ( path , min_size = DEFAULTS [ 'min_size' ] ) : filestat = _stat ( path ) if stat . S_ISLNK ( filestat . st_mode ) : return if filestat . st_size < min_size : return return filestat . st_size
5714	def _validate_zip ( the_zip ) : datapackage_jsons = [ f for f in the_zip . namelist ( ) if f . endswith ( 'datapackage.json' ) ] if len ( datapackage_jsons ) != 1 : msg = 'DataPackage must have only one "datapackage.json" (had {n})' raise exceptions . DataPackageException ( msg . format ( n = len ( datapackage_jsons ) ) )
8242	def outline ( path , colors , precision = 0.4 , continuous = True ) : def _point_count ( path , precision ) : return max ( int ( path . length * precision * 0.5 ) , 10 ) n = sum ( [ _point_count ( contour , precision ) for contour in path . contours ] ) contour_i = 0 contour_n = len ( path . contours ) - 1 if contour_n == 0 : continuous = False i = 0 for contour in path . contours : if not continuous : i = 0 j = _point_count ( contour , precision ) first = True for pt in contour . points ( j ) : if first : first = False else : if not continuous : clr = float ( i ) / j * len ( colors ) else : clr = float ( i ) / n * len ( colors ) - 1 * contour_i / contour_n _ctx . stroke ( colors [ int ( clr ) ] ) _ctx . line ( x0 , y0 , pt . x , pt . y ) x0 = pt . x y0 = pt . y i += 1 pt = contour . point ( 0.9999999 ) _ctx . line ( x0 , y0 , pt . x , pt . y ) contour_i += 1
803	def modelsGetResultAndStatus ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % type ( modelIDs ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getResultAndStatusNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getResultAndStatusNamedTuple . _make ( r ) for r in rows ]
8156	def create_index ( self , table , field , unique = False , ascending = True ) : if unique : u = "unique " else : u = "" if ascending : a = "asc" else : a = "desc" sql = "create " + u + "index index_" + table + "_" + field + " " sql += "on " + table + "(" + field + " " + a + ")" self . _cur . execute ( sql ) self . _con . commit ( )
7472	def get_nloci ( data ) : bseeds = os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) with h5py . File ( bseeds ) as io5 : return io5 [ "seedsarr" ] . shape [ 0 ]
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
6040	def regular_data_1d_from_sub_data_1d ( self , sub_array_1d ) : return np . multiply ( self . sub_grid_fraction , sub_array_1d . reshape ( - 1 , self . sub_grid_length ) . sum ( axis = 1 ) )
11193	def metadata ( proto_dataset_uri , relpath_in_dataset , key , value ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) proto_dataset . add_item_metadata ( handle = relpath_in_dataset , key = key , value = value )
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
921	def log ( self , level , msg , * args , ** kwargs ) : self . _baseLogger . log ( self , level , self . getExtendedMsg ( msg ) , * args , ** kwargs )
10757	def writable_path ( path ) : if os . path . exists ( path ) : return os . access ( path , os . W_OK ) try : with open ( path , 'w' ) : pass except ( OSError , IOError ) : return False else : os . remove ( path ) return True
12477	def merge ( dict_1 , dict_2 ) : return dict ( ( str ( key ) , dict_1 . get ( key ) or dict_2 . get ( key ) ) for key in set ( dict_2 ) | set ( dict_1 ) )
5967	def solvate ( struct = 'top/protein.pdb' , top = 'top/system.top' , distance = 0.9 , boxtype = 'dodecahedron' , concentration = 0 , cation = 'NA' , anion = 'CL' , water = 'tip4p' , solvent_name = 'SOL' , with_membrane = False , ndx = 'main.ndx' , mainselection = '"Protein"' , dirname = 'solvate' , ** kwargs ) : sol = solvate_sol ( struct = struct , top = top , distance = distance , boxtype = boxtype , water = water , solvent_name = solvent_name , with_membrane = with_membrane , dirname = dirname , ** kwargs ) ion = solvate_ion ( struct = sol [ 'struct' ] , top = top , concentration = concentration , cation = cation , anion = anion , solvent_name = solvent_name , ndx = ndx , mainselection = mainselection , dirname = dirname , ** kwargs ) return ion
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
3239	def get_group ( group_name , users = True , client = None , ** kwargs ) : result = client . get_group ( GroupName = group_name , ** kwargs ) if users : if result . get ( 'IsTruncated' ) : kwargs_to_send = { 'GroupName' : group_name } kwargs_to_send . update ( kwargs ) user_list = result [ 'Users' ] kwargs_to_send [ 'Marker' ] = result [ 'Marker' ] result [ 'Users' ] = user_list + _get_users_for_group ( client , ** kwargs_to_send ) else : result . pop ( 'Users' , None ) result . pop ( 'IsTruncated' , None ) result . pop ( 'Marker' , None ) return result
329	def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
7389	def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( group )
3222	def _gcp_client ( project , mod_name , pkg_name , key_file = None , http_auth = None , user_agent = None ) : client = None if http_auth is None : http_auth = _googleauth ( key_file = key_file , user_agent = user_agent ) try : google_module = importlib . import_module ( '.' + mod_name , package = pkg_name ) client = google_module . Client ( use_GAX = USE_GAX , project = project , http = http_auth ) except ImportError as ie : import_err = 'Unable to import %s.%s' % ( pkg_name , mod_name ) raise ImportError ( import_err ) except TypeError : client = google_module . Client ( project = project , http = http_auth ) if user_agent and hasattr ( client , 'user_agent' ) : client . user_agent = user_agent return client
4680	def getAccountsFromPublicKey ( self , pub ) : names = self . rpc . get_key_references ( [ str ( pub ) ] ) [ 0 ] for name in names : yield name
9389	def check_sla ( self , sla , diff_metric ) : try : if sla . display is '%' : diff_val = float ( diff_metric [ 'percent_diff' ] ) else : diff_val = float ( diff_metric [ 'absolute_diff' ] ) except ValueError : return False if not ( sla . check_sla_passed ( diff_val ) ) : self . sla_failures += 1 self . sla_failure_list . append ( DiffSLAFailure ( sla , diff_metric ) ) return True
2284	def predict ( self , a , b , ** kwargs ) : binning_alg = kwargs . get ( 'bins' , 'fd' ) return metrics . adjusted_mutual_info_score ( bin_variable ( a , bins = binning_alg ) , bin_variable ( b , bins = binning_alg ) )
11085	def shutdown ( self , msg , args ) : self . log . info ( "Received shutdown from %s" , msg . user . username ) self . _bot . runnable = False return "Shutting down..."
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
2882	def get_timer_event_definition ( self , timerEventDefinition ) : timeDate = first ( self . xpath ( './/bpmn:timeDate' ) ) return TimerEventDefinition ( self . node . get ( 'name' , timeDate . text ) , self . parser . parse_condition ( timeDate . text , None , None , None , None , self ) )
2316	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_pc ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2362	def t_heredoc ( self , t ) : r'<<\S+\r?\n' t . lexer . is_tabbed = False self . _init_heredoc ( t ) t . lexer . begin ( 'heredoc' )
9748	async def choose_qtm_instance ( interface ) : instances = { } print ( "Available QTM instances:" ) async for i , qtm_instance in AsyncEnumerate ( qtm . Discover ( interface ) , start = 1 ) : instances [ i ] = qtm_instance print ( "{} - {}" . format ( i , qtm_instance . info ) ) try : choice = int ( input ( "Connect to: " ) ) if choice not in instances : raise ValueError except ValueError : LOG . error ( "Invalid choice" ) return None return instances [ choice ] . host
7756	def _set_response_handlers ( self , stanza , res_handler , err_handler , timeout_handler = None , timeout = None ) : self . fix_out_stanza ( stanza ) to_jid = stanza . to_jid if to_jid : to_jid = unicode ( to_jid ) if timeout_handler : def callback ( dummy1 , dummy2 ) : timeout_handler ( ) self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout , callback ) else : self . _iq_response_handlers . set_item ( ( stanza . stanza_id , to_jid ) , ( res_handler , err_handler ) , timeout )
4054	def collections_sub ( self , collection , ** kwargs ) : query_string = "/{t}/{u}/collections/{c}/collections" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _build_query ( query_string )
408	def _tf_repeat ( self , a , repeats ) : if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( "This is not a 1D Tensor" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a
7883	def _make_prefixed ( self , name , is_element , declared_prefixes , declarations ) : namespace , name = self . _split_qname ( name , is_element ) if namespace is None : prefix = None elif namespace in declared_prefixes : prefix = declared_prefixes [ namespace ] elif namespace in self . _prefixes : prefix = self . _prefixes [ namespace ] declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix else : if is_element : prefix = None else : prefix = self . _make_prefix ( declared_prefixes ) declarations [ namespace ] = prefix declared_prefixes [ namespace ] = prefix if prefix : return prefix + u":" + name else : return name
4848	def _partition_items ( self , channel_metadata_item_map ) : items_to_create = { } items_to_update = { } items_to_delete = { } transmission_map = { } export_content_ids = channel_metadata_item_map . keys ( ) for transmission in self . _get_transmissions ( ) : transmission_map [ transmission . content_id ] = transmission if transmission . content_id not in export_content_ids : items_to_delete [ transmission . content_id ] = transmission . channel_metadata for item in channel_metadata_item_map . values ( ) : content_id = item . content_id channel_metadata = item . channel_metadata transmitted_item = transmission_map . get ( content_id , None ) if transmitted_item is not None : if diff ( channel_metadata , transmitted_item . channel_metadata ) : items_to_update [ content_id ] = channel_metadata else : items_to_create [ content_id ] = channel_metadata LOGGER . info ( 'Preparing to transmit creation of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_create ) , self . enterprise_configuration , items_to_create . keys ( ) , ) LOGGER . info ( 'Preparing to transmit update of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_update ) , self . enterprise_configuration , items_to_update . keys ( ) , ) LOGGER . info ( 'Preparing to transmit deletion of [%s] content metadata items with plugin configuration [%s]: [%s]' , len ( items_to_delete ) , self . enterprise_configuration , items_to_delete . keys ( ) , ) return items_to_create , items_to_update , items_to_delete , transmission_map
5631	def long_description ( ) : import argparse parser = argparse . ArgumentParser ( ) parser . add_argument ( '--doc' , dest = "doc" , action = "store_true" , default = False ) args , sys . argv = parser . parse_known_args ( sys . argv ) if args . doc : import doc2md , pypandoc md = doc2md . doc2md ( doc2md . __doc__ , "doc2md" , toc = False ) long_description = pypandoc . convert ( md , 'rst' , format = 'md' ) else : return None
3161	def get ( self , conversation_id , ** queryparams ) : self . conversation_id = conversation_id return self . _mc_client . _get ( url = self . _build_path ( conversation_id ) , ** queryparams )
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
12435	def parse ( cls , path ) : for resource , pattern in cls . meta . patterns : match = re . match ( pattern , path ) if match is not None : return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] return None if not cls . meta . patterns else False
8166	def reload_functions ( self ) : with LiveExecution . lock : if self . edited_source : tree = ast . parse ( self . edited_source ) for f in [ n for n in ast . walk ( tree ) if isinstance ( n , ast . FunctionDef ) ] : self . ns [ f . name ] . __code__ = meta . decompiler . compile_func ( f , self . filename , self . ns ) . __code__
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
6045	def padded_blurred_image_2d_from_padded_image_1d_and_psf ( self , padded_image_1d , psf ) : padded_model_image_1d = self . convolve_array_1d_with_psf ( padded_array_1d = padded_image_1d , psf = psf ) return self . scaled_array_2d_from_array_1d ( array_1d = padded_model_image_1d )
5802	def _convert_filetime_to_timestamp ( filetime ) : hundreds_nano_seconds = struct . unpack ( b'>Q' , struct . pack ( b'>LL' , filetime . dwHighDateTime , filetime . dwLowDateTime ) ) [ 0 ] seconds_since_1601 = hundreds_nano_seconds / 10000000 return seconds_since_1601 - 11644473600
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
10526	def get_google_playlist_songs ( self , playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : logger . info ( "Loading Google Music playlist songs..." ) google_playlist = self . get_google_playlist ( playlist ) if not google_playlist : return [ ] , [ ] playlist_song_ids = [ track [ 'trackId' ] for track in google_playlist [ 'tracks' ] ] playlist_songs = [ song for song in self . api . get_all_songs ( ) if song [ 'id' ] in playlist_song_ids ] matched_songs , filtered_songs = filter_google_songs ( playlist_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Filtered {0} Google playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} Google playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs
12714	def connect_to ( self , joint , other_body , offset = ( 0 , 0 , 0 ) , other_offset = ( 0 , 0 , 0 ) , ** kwargs ) : anchor = self . world . move_next_to ( self , other_body , offset , other_offset ) self . world . join ( joint , self , other_body , anchor = anchor , ** kwargs )
11033	def parse ( self , value : str , type_ : typing . Type [ typing . Any ] = str , subtype : typing . Type [ typing . Any ] = str , ) -> typing . Any : if type_ is bool : return type_ ( value . lower ( ) in self . TRUE_STRINGS ) try : if isinstance ( type_ , type ) and issubclass ( type_ , ( list , tuple , set , frozenset ) ) : return type_ ( self . parse ( v . strip ( " " ) , subtype ) for v in value . split ( "," ) if value . strip ( " " ) ) return type_ ( value ) except ValueError as e : raise ConfigError ( * e . args )
7501	def get_shape ( spans , loci ) : width = 0 for idx in xrange ( loci . shape [ 0 ] ) : width += spans [ loci [ idx ] , 1 ] - spans [ loci [ idx ] , 0 ] return width
13482	def sphinx_make ( * targets ) : sh ( 'make %s' % ' ' . join ( targets ) , cwd = options . paved . docs . path )
3557	def find_device ( cls , timeout_sec = TIMEOUT_SEC ) : return get_provider ( ) . find_device ( service_uuids = cls . ADVERTISED , timeout_sec = timeout_sec )
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice print ( self . _chunksize ) rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break fillsets [ : ] = tmpr del tmpr
11848	def things_near ( self , location , radius = None ) : "Return all things within radius of location." if radius is None : radius = self . perceptible_distance radius2 = radius * radius return [ thing for thing in self . things if distance2 ( location , thing . location ) <= radius2 ]
1109	def compare ( self , a , b ) : r cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
12619	def check_img_compatibility ( one_img , another_img , only_check_3d = False ) : nd_to_check = None if only_check_3d : nd_to_check = 3 if hasattr ( one_img , 'shape' ) and hasattr ( another_img , 'shape' ) : if not have_same_shape ( one_img , another_img , nd_to_check = nd_to_check ) : msg = 'Shape of the first image: \n{}\n is different from second one: \n{}' . format ( one_img . shape , another_img . shape ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg ) if hasattr ( one_img , 'get_affine' ) and hasattr ( another_img , 'get_affine' ) : if not have_same_affine ( one_img , another_img , only_check_3d = only_check_3d ) : msg = 'Affine matrix of the first image: \n{}\n is different ' 'from second one:\n{}' . format ( one_img . get_affine ( ) , another_img . get_affine ( ) ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg )
4959	def get_course_runs_from_program ( program ) : course_runs = set ( ) for course in program . get ( "courses" , [ ] ) : for run in course . get ( "course_runs" , [ ] ) : if "key" in run and run [ "key" ] : course_runs . add ( run [ "key" ] ) return course_runs
2106	def _echo_setting ( key ) : value = getattr ( settings , key ) secho ( '%s: ' % key , fg = 'magenta' , bold = True , nl = False ) secho ( six . text_type ( value ) , bold = True , fg = 'white' if isinstance ( value , six . text_type ) else 'cyan' , )
1739	def in_op ( self , other ) : if not is_object ( other ) : raise MakeError ( 'TypeError' , "You can\'t use 'in' operator to search in non-objects" ) return other . has_property ( to_string ( self ) )
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , ** kwargs )
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) is_compressed = False if compress : compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
4668	def encrypt ( privkey , passphrase ) : if isinstance ( privkey , str ) : privkey = PrivateKey ( privkey ) else : privkey = PrivateKey ( repr ( privkey ) ) privkeyhex = repr ( privkey ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) salt = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) ( derived_half1 , derived_half2 ) = ( key [ : 32 ] , key [ 32 : ] ) aes = AES . new ( derived_half2 , AES . MODE_ECB ) encrypted_half1 = _encrypt_xor ( privkeyhex [ : 32 ] , derived_half1 [ : 16 ] , aes ) encrypted_half2 = _encrypt_xor ( privkeyhex [ 32 : ] , derived_half1 [ 16 : ] , aes ) " flag byte is forced 0xc0 because Graphene only uses compressed keys " payload = b"\x01" + b"\x42" + b"\xc0" + salt + encrypted_half1 + encrypted_half2 " Checksum " checksum = hashlib . sha256 ( hashlib . sha256 ( payload ) . digest ( ) ) . digest ( ) [ : 4 ] privatkey = hexlify ( payload + checksum ) . decode ( "ascii" ) return Base58 ( privatkey )
2861	def _transaction_end ( self ) : self . _command . append ( '\x87' ) self . _ft232h . _write ( '' . join ( self . _command ) ) return bytearray ( self . _ft232h . _poll_read ( self . _expected ) )
11910	def bump_version ( version , which = None ) : try : parts = [ int ( n ) for n in version . split ( '.' ) ] except ValueError : fail ( 'Current version is not numeric' ) if len ( parts ) != 3 : fail ( 'Current version is not semantic versioning' ) PARTS = { 'major' : 0 , 'minor' : 1 , 'patch' : 2 } index = PARTS [ which ] if which in PARTS else 2 before , middle , after = parts [ : index ] , parts [ index ] , parts [ index + 1 : ] middle += 1 return '.' . join ( str ( n ) for n in before + [ middle ] + after )
2657	def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
7020	def generate_hatpi_binnedlc_pkl ( binnedpklf , textlcf , timebinsec , outfile = None ) : binlcdict = read_hatpi_binnedlc ( binnedpklf , textlcf , timebinsec ) if binlcdict : if outfile is None : outfile = os . path . join ( os . path . dirname ( binnedpklf ) , '%s-hplc.pkl' % ( os . path . basename ( binnedpklf ) . replace ( 'sec-lc.pkl.gz' , '' ) ) ) return lcdict_to_pickle ( binlcdict , outfile = outfile ) else : LOGERROR ( 'could not read binned HATPI LC: %s' % binnedpklf ) return None
1570	def submit_fatjar ( cl_args , unknown_args , tmp_dir ) : topology_file = cl_args [ 'topology-file-name' ] main_class = cl_args [ 'topology-class-name' ] res = execute . heron_class ( class_name = main_class , lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) , extra_jars = [ topology_file ] , args = tuple ( unknown_args ) , java_defines = cl_args [ 'topology_main_jvm_property' ] ) result . render ( res ) if not result . is_successful ( res ) : err_context = ( "Failed to create topology definition " "file when executing class '%s' of file '%s'" ) % ( main_class , topology_file ) res . add_context ( err_context ) return res results = launch_topologies ( cl_args , topology_file , tmp_dir ) return results
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
13685	def load_values ( self ) : for config_name , evar in self . evar_defs . items ( ) : if evar . is_required and evar . name not in os . environ : raise RuntimeError ( ( "Missing required environment variable: {evar_name}\n" "{help_txt}" ) . format ( evar_name = evar . name , help_txt = evar . help_txt ) ) if evar . name in os . environ : self [ config_name ] = os . environ . get ( evar . name ) else : self [ config_name ] = evar . default_val for filter in evar . filters : current_val = self . get ( config_name ) new_val = filter ( current_val , evar ) self [ config_name ] = new_val self . _filter_all ( )
9812	def revoke ( username ) : try : PolyaxonClient ( ) . user . revoke_superuser ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not revoke superuser role from user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Superuser role was revoked successfully from user `{}`." . format ( username ) )
5959	def ma ( self ) : a = self . array return numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) )
13830	def _url ( self ) : if self . _api_arg : mypart = str ( self . _api_arg ) else : mypart = self . _name if self . _parent : return '/' . join ( filter ( None , [ self . _parent . _url , mypart ] ) ) else : return mypart
1913	def locked_context ( self , key = None , default = dict ) : keys = [ 'policy' ] if key is not None : keys . append ( key ) with self . _executor . locked_context ( '.' . join ( keys ) , default ) as policy_context : yield policy_context
12934	def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
8471	def _debug ( message , color = None , attrs = None ) : if attrs is None : attrs = [ ] if color is not None : print colored ( message , color , attrs = attrs ) else : if len ( attrs ) > 0 : print colored ( message , "white" , attrs = attrs ) else : print message
1807	def SETBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) , 1 , 0 ) )
6957	def _log_prior_transit ( theta , priorbounds ) : allowed = True for ix , key in enumerate ( np . sort ( list ( priorbounds . keys ( ) ) ) ) : if priorbounds [ key ] [ 0 ] < theta [ ix ] < priorbounds [ key ] [ 1 ] : allowed = True and allowed else : allowed = False if allowed : return 0. return - np . inf
8575	def update_nic ( self , datacenter_id , server_id , nic_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
7993	def _restart_stream ( self ) : self . _input_state = "restart" self . _output_state = "restart" self . features = None self . transport . restart ( ) if self . initiator : self . _send_stream_start ( self . stream_id )
7092	def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
10175	def _format_range_dt ( self , d ) : if not isinstance ( d , six . string_types ) : d = d . isoformat ( ) return '{0}||/{1}' . format ( d , self . dt_rounding_map [ self . aggregation_interval ] )
5637	def largest_finite_distance ( self ) : block_start_distances = [ block . distance_start for block in self . _profile_blocks if block . distance_start < float ( 'inf' ) ] block_end_distances = [ block . distance_end for block in self . _profile_blocks if block . distance_end < float ( 'inf' ) ] distances = block_start_distances + block_end_distances if len ( distances ) > 0 : return max ( distances ) else : return None
698	def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )
4881	def delete_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . filter ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH ) . delete ( )
13048	def f_annotation_filter ( annotations , type_uri , number ) : filtered = [ annotation for annotation in annotations if annotation . type_uri == type_uri ] number = min ( [ len ( filtered ) , number ] ) if number == 0 : return None else : return filtered [ number - 1 ]
13748	def get_counter ( self , name , start = 0 ) : item = self . get_item ( hash_key = name , start = start ) counter = Counter ( dynamo_item = item , pool = self ) return counter
371	def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )
6337	def dist_baystat ( src , tar , min_ss_len = None , left_ext = None , right_ext = None ) : return Baystat ( ) . dist ( src , tar , min_ss_len , left_ext , right_ext )
13361	def save ( self ) : env_data = [ dict ( name = env . name , root = env . path ) for env in self ] encode = yaml . safe_dump ( env_data , default_flow_style = False ) with open ( self . path , 'w' ) as f : f . write ( encode )
5946	def in_dir ( directory , create = True ) : startdir = os . getcwd ( ) try : try : os . chdir ( directory ) logger . debug ( "Working in {directory!r}..." . format ( ** vars ( ) ) ) except OSError as err : if create and err . errno == errno . ENOENT : os . makedirs ( directory ) os . chdir ( directory ) logger . info ( "Working in {directory!r} (newly created)..." . format ( ** vars ( ) ) ) else : logger . exception ( "Failed to start working in {directory!r}." . format ( ** vars ( ) ) ) raise yield os . getcwd ( ) finally : os . chdir ( startdir )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
318	def calc_bootstrap ( func , returns , * args , ** kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , ** kwargs ) else : out [ i ] = func ( returns_i , * args , ** kwargs ) return out
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
5290	def construct_inlines ( self ) : inline_formsets = [ ] for inline_class in self . get_inlines ( ) : inline_instance = inline_class ( self . model , self . request , self . object , self . kwargs , self ) inline_formset = inline_instance . construct_formset ( ) inline_formsets . append ( inline_formset ) return inline_formsets
8578	def list_servers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
11810	def index_document ( self , text , url ) : "Index the text of a document." title = text [ : text . index ( '\n' ) ] . strip ( ) docwords = words ( text ) docid = len ( self . documents ) self . documents . append ( Document ( title , url , len ( docwords ) ) ) for word in docwords : if word not in self . stopwords : self . index [ word ] [ docid ] += 1
10930	def find_LM_updates ( self , grad , do_correct_damping = True , subblock = None ) : if subblock is not None : if ( subblock . sum ( ) == 0 ) or ( subblock . size == 0 ) : CLOG . fatal ( 'Empty subblock in find_LM_updates' ) raise ValueError ( 'Empty sub-block' ) j = self . J [ subblock ] JTJ = np . dot ( j , j . T ) damped_JTJ = self . _calc_damped_jtj ( JTJ , subblock = subblock ) grad = grad [ subblock ] else : damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if self . use_accel : accel_correction = self . calc_accel_correction ( damped_JTJ , delta ) nrm_d0 = np . sqrt ( np . sum ( delta ** 2 ) ) nrm_corr = np . sqrt ( np . sum ( accel_correction ** 2 ) ) CLOG . debug ( '|correction| / |LM step|\t%e' % ( nrm_corr / nrm_d0 ) ) if nrm_corr / nrm_d0 < self . max_accel_correction : delta += accel_correction elif do_correct_damping : CLOG . debug ( 'Untrustworthy step! Increasing damping...' ) self . increase_damping ( ) damped_JTJ = self . _calc_damped_jtj ( self . JTJ , subblock = subblock ) delta = self . _calc_lm_step ( damped_JTJ , grad , subblock = subblock ) if np . any ( np . isnan ( delta ) ) : CLOG . fatal ( 'Calculated steps have nans!?' ) raise FloatingPointError ( 'Calculated steps have nans!?' ) return delta
8794	def set_all ( self , model , ** tags ) : for name , tag in self . tags . items ( ) : if name in tags : value = tags . pop ( name ) if value : try : tag . set ( model , value ) except TagValidationError as e : raise n_exc . BadRequest ( resource = "tags" , msg = "%s" % ( e . message ) )
12758	def labels ( self ) : return sorted ( self . channels , key = lambda c : self . channels [ c ] )
9670	def normalize ( self , string ) : return '' . join ( [ self . _normalize . get ( x , x ) for x in nfd ( string ) ] )
8993	def folder ( self , folder ) : result = [ ] for root , _ , files in os . walk ( folder ) : for file in files : path = os . path . join ( root , file ) if self . _chooses_path ( path ) : result . append ( self . path ( path ) ) return result
11823	def exp_schedule ( k = 20 , lam = 0.005 , limit = 100 ) : "One possible schedule function for simulated annealing" return lambda t : if_ ( t < limit , k * math . exp ( - lam * t ) , 0 )
5418	def format_logging_uri ( uri , job_metadata , task_metadata ) : fmt = str ( uri ) if '{' not in fmt : if uri . endswith ( '.log' ) : fmt = os . path . splitext ( uri ) [ 0 ] else : fmt = os . path . join ( uri , '{job-id}' ) if task_metadata . get ( 'task-id' ) is not None : fmt += '.{task-id}' if task_metadata . get ( 'task-attempt' ) is not None : fmt += '.{task-attempt}' fmt += '.log' return _format_task_uri ( fmt , job_metadata , task_metadata )
8084	def scale ( self , x = 1 , y = None ) : if not y : y = x if x == 0 : x = 1 if y == 0 : y = 1 self . _canvas . scale ( x , y )
10155	def convert ( self , schema_node , definition_handler ) : converted = { 'name' : schema_node . name , 'in' : self . _in , 'required' : schema_node . required } if schema_node . description : converted [ 'description' ] = schema_node . description if schema_node . default : converted [ 'default' ] = schema_node . default schema = definition_handler ( schema_node ) schema . pop ( 'title' , None ) converted . update ( schema ) if schema . get ( 'type' ) == 'array' : converted [ 'items' ] = { 'type' : schema [ 'items' ] [ 'type' ] } return converted
3814	def _get_upload_session_status ( res ) : response = json . loads ( res . body . decode ( ) ) if 'sessionStatus' not in response : try : info = ( response [ 'errorMessage' ] [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) reason = '{} : {}' . format ( info [ 'status' ] , info [ 'message' ] ) except KeyError : reason = 'unknown reason' raise exceptions . NetworkError ( 'image upload failed: {}' . format ( reason ) ) return response [ 'sessionStatus' ]
5649	def write_gtfs ( gtfs , output ) : output = os . path . abspath ( output ) uuid_str = "tmp_" + str ( uuid . uuid1 ( ) ) if output [ - 4 : ] == '.zip' : zip = True out_basepath = os . path . dirname ( os . path . abspath ( output ) ) if not os . path . exists ( out_basepath ) : raise IOError ( out_basepath + " does not exist, cannot write gtfs as a zip" ) tmp_dir = os . path . join ( out_basepath , str ( uuid_str ) ) else : zip = False out_basepath = output tmp_dir = os . path . join ( out_basepath + "_" + str ( uuid_str ) ) os . makedirs ( tmp_dir , exist_ok = True ) gtfs_table_to_writer = { "agency" : _write_gtfs_agencies , "calendar" : _write_gtfs_calendar , "calendar_dates" : _write_gtfs_calendar_dates , "feed_info" : _write_gtfs_feed_info , "routes" : _write_gtfs_routes , "shapes" : _write_gtfs_shapes , "stops" : _write_gtfs_stops , "stop_times" : _write_gtfs_stop_times , "transfers" : _write_gtfs_transfers , "trips" : _write_gtfs_trips , } for table , writer in gtfs_table_to_writer . items ( ) : fname_to_write = os . path . join ( tmp_dir , table + '.txt' ) print ( fname_to_write ) writer ( gtfs , open ( os . path . join ( tmp_dir , table + '.txt' ) , 'w' ) ) if zip : shutil . make_archive ( output [ : - 4 ] , 'zip' , tmp_dir ) shutil . rmtree ( tmp_dir ) else : print ( "moving " + str ( tmp_dir ) + " to " + out_basepath ) os . rename ( tmp_dir , out_basepath )
9950	def get_node ( obj , args , kwargs ) : if args is None and kwargs is None : return ( obj , ) if kwargs is None : kwargs = { } return obj , _bind_args ( obj , args , kwargs )
675	def _getMetrics ( self ) : metric = None if self . metrics is not None : metric = self . metrics ( self . _currentRecordIndex + 1 ) elif self . metricValue is not None : metric = self . metricValue else : raise RuntimeError ( 'No metrics or metric value specified for dummy model' ) return { self . _optimizeKeyPattern : metric }
6243	def load ( self ) : self . _open_image ( ) width , height , depth = self . image . size [ 0 ] , self . image . size [ 1 ] // self . layers , self . layers components , data = image_data ( self . image ) texture = self . ctx . texture_array ( ( width , height , depth ) , components , data , ) texture . extra = { 'meta' : self . meta } if self . meta . mipmap : texture . build_mipmaps ( ) self . _close_image ( ) return texture
2921	def _restart ( self , my_task ) : if not my_task . _has_state ( Task . WAITING ) : raise WorkflowException ( my_task , "Cannot refire a task that is not" "in WAITING state" ) if my_task . _get_internal_data ( 'task_id' ) is not None : if not hasattr ( my_task , 'async_call' ) : task_id = my_task . _get_internal_data ( 'task_id' ) my_task . async_call = default_app . AsyncResult ( task_id ) my_task . deserialized = True my_task . async_call . state async_call = my_task . async_call if async_call . state == 'FAILED' : pass elif async_call . state in [ 'RETRY' , 'PENDING' , 'STARTED' ] : async_call . revoke ( ) LOG . info ( "Celery task '%s' was in %s state and was revoked" % ( async_call . state , async_call ) ) elif async_call . state == 'SUCCESS' : LOG . warning ( "Celery task '%s' succeeded, but a refire was " "requested" % async_call ) self . _clear_celery_task_data ( my_task ) return self . _start ( my_task )
13637	def maybe ( f , default = None ) : @ wraps ( f ) def _maybe ( x , * a , ** kw ) : if x is None : return default return f ( x , * a , ** kw ) return _maybe
9526	def to_fastg ( infile , outfile , circular = None ) : if circular is None : to_circularise = set ( ) elif type ( circular ) is not set : f = utils . open_file_read ( circular ) to_circularise = set ( [ x . rstrip ( ) for x in f . readlines ( ) ] ) utils . close ( f ) else : to_circularise = circular seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) nodes = 1 for seq in seq_reader : new_id = '_' . join ( [ 'NODE' , str ( nodes ) , 'length' , str ( len ( seq ) ) , 'cov' , '1' , 'ID' , seq . id ] ) if seq . id in to_circularise : seq . id = new_id + ':' + new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "':" + new_id + "';" print ( seq , file = fout ) else : seq . id = new_id + ';' print ( seq , file = fout ) seq . revcomp ( ) seq . id = new_id + "';" print ( seq , file = fout ) nodes += 1 utils . close ( fout )
8553	def delete_ipblock ( self , ipblock_id ) : response = self . _perform_request ( url = '/ipblocks/' + ipblock_id , method = 'DELETE' ) return response
8619	def getServerInfo ( pbclient = None , dc_id = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server_info = [ ] servers = pbclient . list_servers ( dc_id , 1 ) for server in servers [ 'items' ] : props = server [ 'properties' ] info = dict ( id = server [ 'id' ] , name = props [ 'name' ] , state = server [ 'metadata' ] [ 'state' ] , vmstate = props [ 'vmState' ] ) server_info . append ( info ) return server_info
13692	def broadcast_tx ( self , address , amount , secret , secondsecret = None , vendorfield = '' ) : peer = random . choice ( self . PEERS ) park = Park ( peer , 4001 , constants . ARK_NETHASH , '1.1.1' ) return park . transactions ( ) . create ( address , str ( amount ) , vendorfield , secret , secondsecret )
10408	def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret
6071	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if self . has_light_profile : return sum ( map ( lambda p : p . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) , self . light_profiles ) ) else : return None
11282	def append ( self , next ) : next . chained = True if self . next : self . next . append ( next ) else : self . next = next
6320	def create_entrypoint ( self ) : with open ( os . path . join ( self . template_dir , 'manage.py' ) , 'r' ) as fd : data = fd . read ( ) . format ( project_name = self . project_name ) with open ( 'manage.py' , 'w' ) as fd : fd . write ( data ) os . chmod ( 'manage.py' , 0o777 )
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
7793	def set_fetcher ( self , fetcher_class ) : self . _lock . acquire ( ) try : self . _fetcher = fetcher_class finally : self . _lock . release ( )
13136	def request ( key , features , query , timeout = 5 ) : data = { } data [ 'key' ] = key data [ 'features' ] = '/' . join ( [ f for f in features if f in FEATURES ] ) data [ 'query' ] = quote ( query ) data [ 'format' ] = 'json' r = requests . get ( API_URL . format ( ** data ) , timeout = timeout ) results = json . loads ( _unicode ( r . content ) ) return results
6773	def uninstall_blacklisted ( self ) : from burlap . system import distrib_family blacklisted_packages = self . env . blacklisted_packages if not blacklisted_packages : print ( 'No blacklisted packages.' ) return else : family = distrib_family ( ) if family == DEBIAN : self . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq purge %s' % ' ' . join ( blacklisted_packages ) ) else : raise NotImplementedError ( 'Unknown family: %s' % family )
7201	def is_ordered ( cat_id ) : url = 'https://rda.geobigdata.io/v1/stripMetadata/{}' . format ( cat_id ) auth = Auth ( ) r = _req_with_retries ( auth . gbdx_connection , url ) if r is not None : return r . status_code == 200 return False
12007	def _get_algorithm_info ( self , algorithm_info ) : if algorithm_info [ 'algorithm' ] not in self . ALGORITHMS : raise Exception ( 'Algorithm not supported: %s' % algorithm_info [ 'algorithm' ] ) algorithm = self . ALGORITHMS [ algorithm_info [ 'algorithm' ] ] algorithm_info . update ( algorithm ) return algorithm_info
10643	def Nu ( L : float , h : float , k : float ) -> float : return h * L / k
13275	def update_desc_lsib_path ( desc ) : if ( desc [ 'sib_seq' ] > 0 ) : lsib_path = copy . deepcopy ( desc [ 'path' ] ) lsib_path [ - 1 ] = desc [ 'sib_seq' ] - 1 desc [ 'lsib_path' ] = lsib_path else : pass return ( desc )
1805	def SETA ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) == False , 1 , 0 ) )
10496	def leftMouseDragged ( self , stopCoord , strCoord = ( 0 , 0 ) , speed = 1 ) : self . _leftMouseDragged ( stopCoord , strCoord , speed )
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
1069	def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
10383	def main ( ) : logging . basicConfig ( level = logging . INFO ) log . setLevel ( logging . INFO ) bms_base = get_bms_base ( ) neurommsig_base = get_neurommsig_base ( ) neurommsig_excel_dir = os . path . join ( neurommsig_base , 'resources' , 'excels' , 'neurommsig' ) nift_values = get_nift_values ( ) log . info ( 'Starting Alzheimers' ) ad_path = os . path . join ( neurommsig_excel_dir , 'alzheimers' , 'alzheimers.xlsx' ) ad_df = preprocess ( ad_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'alzheimers' , 'neurommsigdb_ad.bel' ) , 'w' ) as ad_file : write_neurommsig_bel ( ad_file , ad_df , mesh_alzheimer , nift_values ) log . info ( 'Starting Parkinsons' ) pd_path = os . path . join ( neurommsig_excel_dir , 'parkinsons' , 'parkinsons.xlsx' ) pd_df = preprocess ( pd_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'parkinsons' , 'neurommsigdb_pd.bel' ) , 'w' ) as pd_file : write_neurommsig_bel ( pd_file , pd_df , mesh_parkinson , nift_values )
10297	def get_undefined_namespace_names ( graph : BELGraph , namespace : str ) -> Set [ str ] : return { exc . name for _ , exc , _ in graph . warnings if isinstance ( exc , UndefinedNamespaceWarning ) and exc . namespace == namespace }
10823	def query_requests ( cls , admin , eager = False ) : if hasattr ( admin , 'is_superadmin' ) and admin . is_superadmin : q1 = GroupAdmin . query . with_entities ( GroupAdmin . group_id ) else : q1 = GroupAdmin . query_by_admin ( admin ) . with_entities ( GroupAdmin . group_id ) q2 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q1 ) , ) q3 = Membership . query_by_user ( user = admin , state = MembershipState . ACTIVE ) . with_entities ( Membership . id_group ) q4 = GroupAdmin . query . filter ( GroupAdmin . admin_type == 'Group' , GroupAdmin . admin_id . in_ ( q3 ) ) . with_entities ( GroupAdmin . group_id ) q5 = Membership . query . filter ( Membership . state == MembershipState . PENDING_ADMIN , Membership . id_group . in_ ( q4 ) ) query = q2 . union ( q5 ) return query
11871	def color_from_hex ( value ) : if "#" in value : value = value [ 1 : ] try : unhexed = bytes . fromhex ( value ) except : unhexed = binascii . unhexlify ( value ) return color_from_rgb ( * struct . unpack ( 'BBB' , unhexed ) )
3312	def do_MKCOL ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] provider = self . _davProvider if util . get_content_length ( environ ) != 0 : self . _fail ( HTTP_MEDIATYPE_NOT_SUPPORTED , "The server does not handle any body content." , ) if environ . setdefault ( "HTTP_DEPTH" , "0" ) != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if provider . exists ( path , environ ) : self . _fail ( HTTP_METHOD_NOT_ALLOWED , "MKCOL can only be executed on an unmapped URL." , ) parentRes = provider . get_resource_inst ( util . get_uri_parent ( path ) , environ ) if not parentRes or not parentRes . is_collection : self . _fail ( HTTP_CONFLICT , "Parent must be an existing collection." ) self . _check_write_permission ( parentRes , "0" , environ ) parentRes . create_collection ( util . get_uri_name ( path ) ) return util . send_status_response ( environ , start_response , HTTP_CREATED )
13732	def validate_is_boolean_true ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
862	def isTemporal ( inferenceType ) : if InferenceType . __temporalInferenceTypes is None : InferenceType . __temporalInferenceTypes = set ( [ InferenceType . TemporalNextStep , InferenceType . TemporalClassification , InferenceType . TemporalAnomaly , InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep ] ) return inferenceType in InferenceType . __temporalInferenceTypes
12309	def auto_add ( repo , autooptions , files ) : mapping = { "." : "" } if ( ( 'import' in autooptions ) and ( 'directory-mapping' in autooptions [ 'import' ] ) ) : mapping = autooptions [ 'import' ] [ 'directory-mapping' ] keys = mapping . keys ( ) keys = sorted ( keys , key = lambda k : len ( k ) , reverse = True ) count = 0 params = [ ] for f in files : relativepath = f for k in keys : v = mapping [ k ] if f . startswith ( k + "/" ) : relativepath = f . replace ( k + "/" , v ) break count += files_add ( repo = repo , args = [ f ] , targetdir = os . path . dirname ( relativepath ) ) return count
5586	def output_cleaned ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : if is_numpy_or_masked_array ( process_data ) : return process_data elif is_numpy_or_masked_array_with_tags ( process_data ) : data , tags = process_data return self . output_cleaned ( data ) , tags elif self . METADATA [ "data_type" ] == "vector" : return list ( process_data )
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) if factory is None : factory = count ( ) while len ( nodes ) < stop : G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
10460	def _ldtpize_accessible ( self , acc ) : actual_role = self . _get_role ( acc ) label = self . _get_title ( acc ) if re . match ( "AXWindow" , actual_role , re . M | re . U | re . L ) : strip = r"( |\n)" else : strip = r"( |:|\.|_|\n)" if label : label = re . sub ( strip , u"" , label ) role = abbreviated_roles . get ( actual_role , "ukn" ) if self . _ldtp_debug and role == "ukn" : print ( actual_role , acc ) return role , label
2309	def predict ( self , data , graph = None , nruns = 6 , njobs = None , gpus = 0 , verbose = None , plot = False , plot_generated_pair = False , return_list_results = False ) : verbose , njobs = SETTINGS . get_default ( ( 'verbose' , verbose ) , ( 'nb_jobs' , njobs ) ) if njobs != 1 : list_out = Parallel ( n_jobs = njobs ) ( delayed ( run_SAM ) ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = idx % max ( gpus , 1 ) ) for idx in range ( nruns ) ) else : list_out = [ run_SAM ( data , skeleton = graph , lr_gen = self . lr , lr_disc = self . dlr , regul_param = self . l1 , nh = self . nh , dnh = self . dnh , gpu = bool ( gpus ) , train_epochs = self . train , test_epochs = self . test , batch_size = self . batchsize , plot = plot , verbose = verbose , gpu_no = 0 ) for idx in range ( nruns ) ] if return_list_results : return list_out else : W = list_out [ 0 ] for w in list_out [ 1 : ] : W += w W /= nruns return nx . relabel_nodes ( nx . DiGraph ( W ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11944	def add ( self , level , message , extra_tags = '' ) : if not message : return level = int ( level ) if level < self . level : return if level not in stored_messages_settings . STORE_LEVELS or self . user . is_anonymous ( ) : return super ( StorageMixin , self ) . add ( level , message , extra_tags ) self . added_new = True m = self . backend . create_message ( level , message , extra_tags ) self . backend . archive_store ( [ self . user ] , m ) self . _queued_messages . append ( m )
2379	def _get_rules ( self , cls ) : result = [ ] for rule_class in cls . __subclasses__ ( ) : rule_name = rule_class . __name__ . lower ( ) if rule_name not in self . _rules : rule = rule_class ( self ) self . _rules [ rule_name ] = rule result . append ( self . _rules [ rule_name ] ) return result
5522	def parse_list_line_windows ( self , b ) : line = b . decode ( encoding = self . encoding ) . rstrip ( "\r\n" ) date_time_end = line . index ( "M" ) date_time_str = line [ : date_time_end + 1 ] . strip ( ) . split ( " " ) date_time_str = " " . join ( [ x for x in date_time_str if len ( x ) > 0 ] ) line = line [ date_time_end + 1 : ] . lstrip ( ) with setlocale ( "C" ) : strptime = datetime . datetime . strptime date_time = strptime ( date_time_str , "%m/%d/%Y %I:%M %p" ) info = { } info [ "modify" ] = self . format_date_time ( date_time ) next_space = line . index ( " " ) if line . startswith ( "<DIR>" ) : info [ "type" ] = "dir" else : info [ "type" ] = "file" info [ "size" ] = line [ : next_space ] . replace ( "," , "" ) if not info [ "size" ] . isdigit ( ) : raise ValueError filename = line [ next_space : ] . lstrip ( ) if filename == "." or filename == ".." : raise ValueError return pathlib . PurePosixPath ( filename ) , info
7091	def _cpinfo_key_worker ( task ) : cpfile , keyspeclist = task keystoget = [ x [ 0 ] for x in keyspeclist ] nonesubs = [ x [ - 2 ] for x in keyspeclist ] nansubs = [ x [ - 1 ] for x in keyspeclist ] for i , k in enumerate ( keystoget ) : thisk = k . split ( '.' ) if sys . version_info [ : 2 ] < ( 3 , 4 ) : thisk = [ ( int ( x ) if x . isdigit ( ) else x ) for x in thisk ] else : thisk = [ ( int ( x ) if x . isdecimal ( ) else x ) for x in thisk ] keystoget [ i ] = thisk keystoget . insert ( 0 , [ 'objectid' ] ) nonesubs . insert ( 0 , '' ) nansubs . insert ( 0 , '' ) vals = checkplot_infokey_worker ( ( cpfile , keystoget ) ) for val , nonesub , nansub , valind in zip ( vals , nonesubs , nansubs , range ( len ( vals ) ) ) : if val is None : outval = nonesub elif isinstance ( val , float ) and not np . isfinite ( val ) : outval = nansub elif isinstance ( val , ( list , tuple ) ) : outval = ', ' . join ( val ) else : outval = val vals [ valind ] = outval return vals
10665	def stoichiometry_coefficient ( compound , element ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return stoichiometry [ element ]
4405	def parse_line ( line , document = None ) : result = re . match ( line_pattern , line ) if result : _ , lineno , offset , severity , msg = result . groups ( ) lineno = int ( lineno or 1 ) offset = int ( offset or 0 ) errno = 2 if severity == 'error' : errno = 1 diag = { 'source' : 'mypy' , 'range' : { 'start' : { 'line' : lineno - 1 , 'character' : offset } , 'end' : { 'line' : lineno - 1 , 'character' : offset + 1 } } , 'message' : msg , 'severity' : errno } if document : word = document . word_at_position ( diag [ 'range' ] [ 'start' ] ) if word : diag [ 'range' ] [ 'end' ] [ 'character' ] = ( diag [ 'range' ] [ 'start' ] [ 'character' ] + len ( word ) ) return diag
1950	def update_segment ( self , selector , base , size , perms ) : logger . info ( "Updating selector %s to 0x%02x (%s bytes) (%s)" , selector , base , size , perms ) if selector == 99 : self . set_fs ( base ) else : logger . error ( "No way to write segment: %d" , selector )
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : blockade_id = "default" return blockade_id
4769	def is_instance_of ( self , some_class ) : try : if not isinstance ( self . val , some_class ) : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be instance of class <%s>, but was not.' % ( self . val , t , some_class . __name__ ) ) except TypeError : raise TypeError ( 'given arg must be a class' ) return self
13379	def _join_seq ( d , k , v ) : if k not in d : d [ k ] = list ( v ) elif isinstance ( d [ k ] , list ) : for item in v : if item not in d [ k ] : d [ k ] . insert ( 0 , item ) elif isinstance ( d [ k ] , string_types ) : v . append ( d [ k ] ) d [ k ] = v
11874	def put ( xy , * args ) : cmd = [ TermCursor . save , TermCursor . move ( * xy ) , '' . join ( args ) , TermCursor . restore ] write ( '' . join ( cmd ) )
12950	def connectAlt ( cls , redisConnectionParams ) : if not isinstance ( redisConnectionParams , dict ) : raise ValueError ( 'redisConnectionParams must be a dictionary!' ) hashVal = hashDictOneLevel ( redisConnectionParams ) modelDictCopy = copy . deepcopy ( dict ( cls . __dict__ ) ) modelDictCopy [ 'REDIS_CONNECTION_PARAMS' ] = redisConnectionParams ConnectedIndexedRedisModel = type ( 'AltConnect' + cls . __name__ + str ( hashVal ) , cls . __bases__ , modelDictCopy ) return ConnectedIndexedRedisModel
12880	def _fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except StopIteration : self . buffer . append ( ( EndOfFile , EndOfFile ) ) self . len = len ( self . buffer )
10179	def delete ( self , start_date = None , end_date = None ) : aggs_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . aggregation_doc_type ) . extra ( _source = False ) range_args = { } if start_date : range_args [ 'gte' ] = self . _format_range_dt ( start_date . replace ( microsecond = 0 ) ) if end_date : range_args [ 'lte' ] = self . _format_range_dt ( end_date . replace ( microsecond = 0 ) ) if range_args : aggs_query = aggs_query . filter ( 'range' , timestamp = range_args ) bookmarks_query = Search ( using = self . client , index = self . aggregation_alias , doc_type = self . bookmark_doc_type ) . sort ( { 'date' : { 'order' : 'desc' } } ) if range_args : bookmarks_query = bookmarks_query . filter ( 'range' , date = range_args ) def _delete_actions ( ) : for query in ( aggs_query , bookmarks_query ) : affected_indices = set ( ) for doc in query . scan ( ) : affected_indices . add ( doc . meta . index ) yield dict ( _index = doc . meta . index , _op_type = 'delete' , _id = doc . meta . id , _type = doc . meta . doc_type ) current_search_client . indices . flush ( index = ',' . join ( affected_indices ) , wait_if_ongoing = True ) bulk ( self . client , _delete_actions ( ) , refresh = True )
8329	def findNext ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllNext , name , attrs , text , ** kwargs )
10020	def environment_exists ( self , env_name ) : response = self . ebs . describe_environments ( application_name = self . app_name , environment_names = [ env_name ] , include_deleted = False ) return len ( response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] ) > 0 and response [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] [ 0 ] [ 'Status' ] != 'Terminated'
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
1972	def _interp_total_size ( interp ) : load_segs = [ x for x in interp . iter_segments ( ) if x . header . p_type == 'PT_LOAD' ] last = load_segs [ - 1 ] return last . header . p_vaddr + last . header . p_memsz
8832	def if_ ( * args ) : for i in range ( 0 , len ( args ) - 1 , 2 ) : if args [ i ] : return args [ i + 1 ] if len ( args ) % 2 : return args [ - 1 ] else : return None
9009	def next_instruction_in_row ( self ) : index = self . index_in_row + 1 if index >= len ( self . row_instructions ) : return None return self . row_instructions [ index ]
9969	def value ( self ) : if self . has_value : return self . _impl [ OBJ ] . get_value ( self . _impl [ KEY ] ) else : raise ValueError ( "Value not found" )
8682	def purge ( self , force = False , key_type = None ) : self . _assert_valid_stash ( ) if not force : raise GhostError ( "The `force` flag must be provided to perform a stash purge. " "I mean, you don't really want to just delete everything " "without precautionary measures eh?" ) audit ( storage = self . _storage . db_path , action = 'PURGE' , message = json . dumps ( dict ( ) ) ) for key_name in self . list ( key_type = key_type ) : self . delete ( key_name )
3006	def get_storage ( request ) : storage_model = oauth2_settings . storage_model user_property = oauth2_settings . storage_model_user_property credentials_property = oauth2_settings . storage_model_credentials_property if storage_model : module_name , class_name = storage_model . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) storage_model_class = getattr ( module , class_name ) return storage . DjangoORMStorage ( storage_model_class , user_property , request . user , credentials_property ) else : return dictionary_storage . DictionaryStorage ( request . session , key = _CREDENTIALS_KEY )
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
1801	def LAHF ( cpu ) : used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) is_expression = any ( issymbolic ( x ) for x in used_regs ) def make_flag ( val , offset ) : if is_expression : return Operators . ITEBV ( 8 , val , BitVecConstant ( 8 , 1 << offset ) , BitVecConstant ( 8 , 0 ) ) else : return val << offset cpu . AH = ( make_flag ( cpu . SF , 7 ) | make_flag ( cpu . ZF , 6 ) | make_flag ( 0 , 5 ) | make_flag ( cpu . AF , 4 ) | make_flag ( 0 , 3 ) | make_flag ( cpu . PF , 2 ) | make_flag ( 1 , 1 ) | make_flag ( cpu . CF , 0 ) )
12261	def gradient_optimizer ( coro ) : class GradientOptimizer ( Optimizer ) : @ wraps ( coro ) def __init__ ( self , * args , ** kwargs ) : self . algorithm = coro ( * args , ** kwargs ) self . algorithm . send ( None ) self . operators = [ ] def set_transform ( self , func ) : self . transform = compose ( destruct , func , self . restruct ) def minimize ( self , f_df , x0 , display = sys . stdout , maxiter = 1e3 ) : self . display = display self . theta = x0 xk = self . algorithm . send ( destruct ( x0 ) . copy ( ) ) store = defaultdict ( list ) runtimes = [ ] if len ( self . operators ) == 0 : self . operators = [ proxops . identity ( ) ] obj , grad = wrap ( f_df , x0 ) transform = compose ( destruct , * reversed ( self . operators ) , self . restruct ) self . optional_print ( tp . header ( [ 'Iteration' , 'Objective' , '||Grad||' , 'Runtime' ] ) ) try : for k in count ( ) : tstart = perf_counter ( ) f = obj ( xk ) df = grad ( xk ) xk = transform ( self . algorithm . send ( df ) ) runtimes . append ( perf_counter ( ) - tstart ) store [ 'f' ] . append ( f ) self . optional_print ( tp . row ( [ k , f , np . linalg . norm ( destruct ( df ) ) , tp . humantime ( runtimes [ - 1 ] ) ] ) ) if k >= maxiter : break except KeyboardInterrupt : pass self . optional_print ( tp . bottom ( 4 ) ) self . optional_print ( u'\u279b Final objective: {}' . format ( store [ 'f' ] [ - 1 ] ) ) self . optional_print ( u'\u279b Total runtime: {}' . format ( tp . humantime ( sum ( runtimes ) ) ) ) self . optional_print ( u'\u279b Per iteration runtime: {} +/- {}' . format ( tp . humantime ( np . mean ( runtimes ) ) , tp . humantime ( np . std ( runtimes ) ) , ) ) return OptimizeResult ( { 'x' : self . restruct ( xk ) , 'f' : f , 'df' : self . restruct ( df ) , 'k' : k , 'obj' : np . array ( store [ 'f' ] ) , } ) return GradientOptimizer
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
3978	def _get_referenced_libs ( specs ) : active_libs = set ( ) for app_spec in specs [ 'apps' ] . values ( ) : for lib in app_spec [ 'depends' ] [ 'libs' ] : active_libs . add ( lib ) return active_libs
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
8026	def getPaths ( roots , ignores = None ) : paths , count , ignores = [ ] , 0 , ignores or [ ] ignore_re = multiglob_compile ( ignores , prefix = False ) for root in roots : root = os . path . realpath ( root ) if os . path . isfile ( root ) : paths . append ( root ) continue for fldr in os . walk ( root ) : out . write ( "Gathering file paths to compare... (%d files examined)" % count ) for subdir in fldr [ 1 ] : dirpath = os . path . join ( fldr [ 0 ] , subdir ) if ignore_re . match ( dirpath ) : fldr [ 1 ] . remove ( subdir ) for filename in fldr [ 2 ] : filepath = os . path . join ( fldr [ 0 ] , filename ) if ignore_re . match ( filepath ) : continue paths . append ( filepath ) count += 1 out . write ( "Found %s files to be compared for duplication." % ( len ( paths ) ) , newline = True ) return paths
2860	def _idle ( self ) : self . _ft232h . setup_pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )
1871	def CWDE ( cpu ) : bit = Operators . EXTRACT ( cpu . AX , 15 , 1 ) cpu . EAX = Operators . SEXTEND ( cpu . AX , 16 , 32 ) cpu . EDX = Operators . SEXTEND ( bit , 1 , 32 )
6859	def create_database ( name , owner = None , owner_host = 'localhost' , charset = 'utf8' , collate = 'utf8_general_ci' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE DATABASE %(name)s CHARACTER SET %(charset)s COLLATE %(collate)s;" % { 'name' : name , 'charset' : charset , 'collate' : collate } , ** kwargs ) if owner : query ( "GRANT ALL PRIVILEGES ON %(name)s.* TO '%(owner)s'@'%(owner_host)s' WITH GRANT OPTION;" % { 'name' : name , 'owner' : owner , 'owner_host' : owner_host } , ** kwargs ) puts ( "Created MySQL database '%s'." % name )
12666	def apply_mask_4d ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask , only_check_3d = True ) vol = get_data ( img ) series , mask_data = _apply_mask_to_4d_data ( vol , mask ) return series , mask_data
2767	def get_all_volumes ( self , region = None ) : if region : url = "volumes?region={}" . format ( region ) else : url = "volumes" data = self . get_data ( url ) volumes = list ( ) for jsoned in data [ 'volumes' ] : volume = Volume ( ** jsoned ) volume . token = self . token volumes . append ( volume ) return volumes
280	def plot_annual_returns ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , ** kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( "Annual returns" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax
12507	def voxspace_to_mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm_coords = nib . affines . apply_affine ( affine , coords ) return mm_coords
12189	async def from_api_token ( cls , token = None , api_cls = SlackBotApi ) : api = api_cls . from_env ( ) if token is None else api_cls ( api_token = token ) data = await api . execute_method ( cls . API_AUTH_ENDPOINT ) return cls ( data [ 'user_id' ] , data [ 'user' ] , api )
1512	def distribute_package ( roles , cl_args ) : Log . info ( "Distributing heron package to nodes (this might take a while)..." ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name Log . debug ( "TAR file %s to %s" % ( cl_args [ "heron_dir" ] , tar_file ) ) make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) dist_nodes = masters . union ( slaves ) scp_package ( tar_file , dist_nodes , cl_args )
11856	def extender ( self , edge ) : "See what edges can be extended by this edge." ( j , k , B , _ , _ ) = edge for ( i , j , A , alpha , B1b ) in self . chart [ j ] : if B1b and B == B1b [ 0 ] : self . add_edge ( [ i , k , A , alpha + [ edge ] , B1b [ 1 : ] ] )
8838	def get_var ( data , var_name , not_found = None ) : try : for key in str ( var_name ) . split ( '.' ) : try : data = data [ key ] except TypeError : data = data [ int ( key ) ] except ( KeyError , TypeError , ValueError ) : return not_found else : return data
418	def delete_datasets ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( "[Database] Delete Dataset SUCCESS" )
3070	def request ( http , uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : http_callable = getattr ( http , 'request' , http ) return http_callable ( uri , method = method , body = body , headers = headers , redirections = redirections , connection_type = connection_type )
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
10309	def barv ( d , plt , title = None , rotation = 'vertical' ) : labels = sorted ( d , key = d . get , reverse = True ) index = range ( len ( labels ) ) plt . xticks ( index , labels , rotation = rotation ) plt . bar ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
1193	def task_done ( self ) : self . all_tasks_done . acquire ( ) try : unfinished = self . unfinished_tasks - 1 if unfinished <= 0 : if unfinished < 0 : raise ValueError ( 'task_done() called too many times' ) self . all_tasks_done . notify_all ( ) self . unfinished_tasks = unfinished finally : self . all_tasks_done . release ( )
11031	def get_json_field ( self , field , ** kwargs ) : d = self . request ( 'GET' , headers = { 'Accept' : 'application/json' } , ** kwargs ) d . addCallback ( raise_for_status ) d . addCallback ( raise_for_header , 'Content-Type' , 'application/json' ) d . addCallback ( json_content ) d . addCallback ( self . _get_json_field , field ) return d
8872	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : for t in self . regex : m = t . Regex . search ( l ) if m != None : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}' . format ( str ( truePosition + 1 ) ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . failed = True self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) raise DirectiveException ( self )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
13098	def wait ( self ) : try : self . relay . wait ( ) self . responder . wait ( ) except KeyboardInterrupt : print_notification ( "Stopping" ) finally : self . terminate_processes ( )
615	def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , "Inference Type '%s' needs a predictedField " "specified in the inferenceArgs dictionary" % inferenceType if optionInferenceArgs : if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( "'$REPLACE_ME'" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField
1340	def binarize ( x , values , threshold = None , included_in = 'upper' ) : lower , upper = values if threshold is None : threshold = ( lower + upper ) / 2. x = x . copy ( ) if included_in == 'lower' : x [ x <= threshold ] = lower x [ x > threshold ] = upper elif included_in == 'upper' : x [ x < threshold ] = lower x [ x >= threshold ] = upper else : raise ValueError ( 'included_in must be "lower" or "upper"' ) return x
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
13202	def format_abstract ( self , format = 'html5' , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : if self . abstract is None : return None abstract_latex = self . _prep_snippet_for_pandoc ( self . abstract ) output_text = convert_lsstdoc_tex ( abstract_latex , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
10185	def _events_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_events ) : for cfg in ep . load ( ) ( ) : if cfg [ 'event_type' ] not in self . enabled_events : continue elif cfg [ 'event_type' ] in result : raise DuplicateEventError ( 'Duplicate event {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_events [ cfg [ 'event_type' ] ] or { } ) result [ cfg [ 'event_type' ] ] = cfg return result
7355	def seq_to_str ( obj , sep = "," ) : if isinstance ( obj , string_classes ) : return obj elif isinstance ( obj , ( list , tuple ) ) : return sep . join ( [ str ( x ) for x in obj ] ) else : return str ( obj )
3066	def _apply_user_agent ( headers , user_agent ) : if user_agent is not None : if 'user-agent' in headers : headers [ 'user-agent' ] = ( user_agent + ' ' + headers [ 'user-agent' ] ) else : headers [ 'user-agent' ] = user_agent return headers
10174	def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
10038	def pick_coda_from_letter ( letter ) : try : __ , __ , coda = split_phonemes ( letter , onset = False , nucleus = False , coda = True ) except ValueError : return None else : return coda
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
483	def getSwarmModelParams ( modelID ) : cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
316	def perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' ) : stats = pd . Series ( ) for stat_func in SIMPLE_STAT_FUNCS : stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = stat_func ( returns ) if positions is not None : stats [ 'Gross leverage' ] = gross_lev ( positions ) . mean ( ) if transactions is not None : stats [ 'Daily turnover' ] = get_turnover ( positions , transactions , turnover_denom ) . mean ( ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : res = stat_func ( returns , factor_returns ) stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = res return stats
2348	def seed_url ( self ) : url = self . base_url if self . URL_TEMPLATE is not None : url = urlparse . urljoin ( self . base_url , self . URL_TEMPLATE . format ( ** self . url_kwargs ) ) if not url : return None url_parts = list ( urlparse . urlparse ( url ) ) query = urlparse . parse_qsl ( url_parts [ 4 ] ) for k , v in self . url_kwargs . items ( ) : if v is None : continue if "{{{}}}" . format ( k ) not in str ( self . URL_TEMPLATE ) : for i in iterable ( v ) : query . append ( ( k , i ) ) url_parts [ 4 ] = urlencode ( query ) return urlparse . urlunparse ( url_parts )
2054	def ADR ( cpu , dest , src ) : aligned_pc = ( cpu . instruction . address + 4 ) & 0xfffffffc dest . write ( aligned_pc + src . read ( ) )
10888	def coords ( self , norm = False , form = 'broadcast' ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . arange ( self . l [ i ] , self . r [ i ] ) / norm [ i ] for i in range ( self . dim ) ) return self . _format_vector ( v , form = form )
6996	def spline_fit_magseries ( times , mags , errs , period , knotfraction = 0.01 , maxknots = 30 , sigclip = 30.0 , plotfit = False , ignoreinitfail = False , magsarefluxes = False , verbose = True ) : if errs is None : errs = npfull_like ( mags , 0.005 ) stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip , magsarefluxes = magsarefluxes ) nzind = npnonzero ( serrs ) stimes , smags , serrs = stimes [ nzind ] , smags [ nzind ] , serrs [ nzind ] phase , pmags , perrs , ptimes , mintime = ( get_phased_quantities ( stimes , smags , serrs , period ) ) nobs = len ( phase ) nknots = int ( npfloor ( knotfraction * nobs ) ) nknots = maxknots if nknots > maxknots else nknots splineknots = nplinspace ( phase [ 0 ] + 0.01 , phase [ - 1 ] - 0.01 , num = nknots ) phase_diffs_ind = npdiff ( phase ) > 0.0 incphase_ind = npconcatenate ( ( nparray ( [ True ] ) , phase_diffs_ind ) ) phase , pmags , perrs = ( phase [ incphase_ind ] , pmags [ incphase_ind ] , perrs [ incphase_ind ] ) spl = LSQUnivariateSpline ( phase , pmags , t = splineknots , w = 1.0 / perrs ) fitmags = spl ( phase ) fitchisq = npsum ( ( ( fitmags - pmags ) * ( fitmags - pmags ) ) / ( perrs * perrs ) ) fitredchisq = fitchisq / ( len ( pmags ) - nknots - 1 ) if verbose : LOGINFO ( 'spline fit done. nknots = %s, ' 'chisq = %.5f, reduced chisq = %.5f' % ( nknots , fitchisq , fitredchisq ) ) if not magsarefluxes : fitmagminind = npwhere ( fitmags == npmax ( fitmags ) ) else : fitmagminind = npwhere ( fitmags == npmin ( fitmags ) ) if len ( fitmagminind [ 0 ] ) > 1 : fitmagminind = ( fitmagminind [ 0 ] [ 0 ] , ) magseriesepoch = ptimes [ fitmagminind ] returndict = { 'fittype' : 'spline' , 'fitinfo' : { 'nknots' : nknots , 'fitmags' : fitmags , 'fitepoch' : magseriesepoch } , 'fitchisq' : fitchisq , 'fitredchisq' : fitredchisq , 'fitplotfile' : None , 'magseries' : { 'times' : ptimes , 'phase' : phase , 'mags' : pmags , 'errs' : perrs , 'magsarefluxes' : magsarefluxes } , } if plotfit and isinstance ( plotfit , str ) : make_fit_plot ( phase , pmags , perrs , fitmags , period , mintime , magseriesepoch , plotfit , magsarefluxes = magsarefluxes ) returndict [ 'fitplotfile' ] = plotfit return returndict
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
920	def critical ( self , msg , * args , ** kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
155	def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
4717	def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
5865	def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
2830	def convert_upsample ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting upsample...' ) if params [ 'mode' ] != 'nearest' : raise AssertionError ( 'Cannot convert non-nearest upsampling' ) if names == 'short' : tf_name = 'UPSL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'height_scale' in params : scale = ( params [ 'height_scale' ] , params [ 'width_scale' ] ) elif len ( inputs ) == 2 : scale = layers [ inputs [ - 1 ] + '_np' ] [ - 2 : ] upsampling = keras . layers . UpSampling2D ( size = scale , name = tf_name ) layers [ scope_name ] = upsampling ( layers [ inputs [ 0 ] ] )
1838	def JNB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC )
1800	def CMOVNO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , src . read ( ) , dest . read ( ) ) )
9506	def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
1493	def _get_next_timeout_interval ( self ) : if len ( self . timer_tasks ) == 0 : return sys . maxsize else : next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) return next_timeout_interval
358	def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( "[!] Fail to load %s" % file_path )
10434	def getcellvalue ( self , window_name , object_name , row_index , column = 0 ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] count = len ( cell . AXChildren ) if column < 0 or column > count : raise LdtpServerException ( 'Column index out of range: %d' % column ) obj = cell . AXChildren [ column ] if not re . search ( "AXColumn" , obj . AXRole ) : obj = cell . AXChildren [ column ] return obj . AXValue
12287	def shellcmd ( repo , args ) : with cd ( repo . rootdir ) : result = run ( args ) return result
9003	def _compute_scale ( self , instruction_id , svg_dict ) : bbox = list ( map ( float , svg_dict [ "svg" ] [ "@viewBox" ] . split ( ) ) ) scale = self . _zoom / ( bbox [ 3 ] - bbox [ 1 ] ) self . _symbol_id_to_scale [ instruction_id ] = scale
8660	def is_valid_identifier ( name ) : if not isinstance ( name , str ) : return False if '\n' in name : return False if name . strip ( ) != name : return False try : code = compile ( '\n{0}=None' . format ( name ) , filename = '<string>' , mode = 'single' ) exec ( code ) return True except SyntaxError : return False
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
7214	def preview ( image , ** kwargs ) : try : from IPython . display import Javascript , HTML , display from gbdxtools . rda . interface import RDA from gbdxtools import Interface gbdx = Interface ( ) except : print ( "IPython is required to produce maps." ) return zoom = kwargs . get ( "zoom" , 16 ) bands = kwargs . get ( "bands" ) if bands is None : bands = image . _rgb_bands wgs84_bounds = kwargs . get ( "bounds" , list ( loads ( image . metadata [ "image" ] [ "imageBoundsWGS84" ] ) . bounds ) ) center = kwargs . get ( "center" , list ( shape ( image ) . centroid . bounds [ 0 : 2 ] ) ) if image . proj != 'EPSG:4326' : code = image . proj . split ( ':' ) [ 1 ] conn = gbdx . gbdx_connection proj_info = conn . get ( 'https://ughlicoordinates.geobigdata.io/ughli/v1/projinfo/{}' . format ( code ) ) . json ( ) tfm = partial ( pyproj . transform , pyproj . Proj ( init = 'EPSG:4326' ) , pyproj . Proj ( init = image . proj ) ) bounds = list ( ops . transform ( tfm , box ( * wgs84_bounds ) ) . bounds ) else : proj_info = { } bounds = wgs84_bounds if not image . options . get ( 'dra' ) : rda = RDA ( ) dra = rda . HistogramDRA ( image ) image = dra . aoi ( bbox = image . bounds ) graph_id = image . rda_id node_id = image . rda . graph ( ) [ 'nodes' ] [ 0 ] [ 'id' ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) scales = ',' . join ( [ '1' ] * len ( bands ) ) offsets = ',' . join ( [ '0' ] * len ( bands ) ) display ( HTML ( Template ( ) . substitute ( { "map_id" : map_id } ) ) ) js = Template ( ) . substitute ( { "map_id" : map_id , "proj" : image . proj , "projInfo" : json . dumps ( proj_info ) , "graphId" : graph_id , "bounds" : bounds , "bands" : "," . join ( map ( str , bands ) ) , "nodeId" : node_id , "md" : json . dumps ( image . metadata [ "image" ] ) , "georef" : json . dumps ( image . metadata [ "georef" ] ) , "center" : center , "zoom" : zoom , "token" : gbdx . gbdx_connection . access_token , "scales" : scales , "offsets" : offsets , "url" : VIRTUAL_RDA_URL } ) display ( Javascript ( js ) )
7026	def objectlist_conesearch ( racenter , declcenter , searchradiusarcsec , gaia_mirror = None , columns = ( 'source_id' , 'ra' , 'dec' , 'phot_g_mean_mag' , 'l' , 'b' , 'parallax' , 'parallax_error' , 'pmra' , 'pmra_error' , 'pmdec' , 'pmdec_error' ) , extra_filter = None , returnformat = 'csv' , forcefetch = False , cachedir = '~/.astrobase/gaia-cache' , verbose = True , timeout = 15.0 , refresh = 2.0 , maxtimeout = 300.0 , maxtries = 3 , complete_query_later = True ) : query = ( "select {columns}, " "(DISTANCE(POINT('ICRS', " "{{table}}.ra, {{table}}.dec), " "POINT('ICRS', {ra_center:.5f}, {decl_center:.5f})))*3600.0 " "AS dist_arcsec " "from {{table}} where " "CONTAINS(POINT('ICRS',{{table}}.ra, {{table}}.dec)," "CIRCLE('ICRS',{ra_center:.5f},{decl_center:.5f}," "{search_radius:.6f}))=1 " "{extra_filter_str}" "ORDER by dist_arcsec asc " ) if extra_filter is not None : extra_filter_str = ' and %s ' % extra_filter else : extra_filter_str = '' formatted_query = query . format ( ra_center = racenter , decl_center = declcenter , search_radius = searchradiusarcsec / 3600.0 , extra_filter_str = extra_filter_str , columns = ', ' . join ( columns ) ) return tap_query ( formatted_query , gaia_mirror = gaia_mirror , returnformat = returnformat , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , complete_query_later = complete_query_later )
3737	def molecular_diameter ( Tc = None , Pc = None , Vc = None , Zc = None , omega = None , Vm = None , Vb = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and Pc and omega : methods . append ( TEEGOTOSTEWARD4 ) if Tc and Pc : methods . append ( SILVALIUMACEDO ) methods . append ( BSLC2 ) methods . append ( TEEGOTOSTEWARD3 ) if Vc and Zc : methods . append ( STIELTHODOSMD ) if Vc : methods . append ( FLYNN ) methods . append ( BSLC1 ) if Vb : methods . append ( BSLB ) if Vm : methods . append ( BSLM ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : sigma = sigma_Flynn ( Vc ) elif Method == BSLC1 : sigma = sigma_Bird_Stewart_Lightfoot_critical_1 ( Vc ) elif Method == BSLC2 : sigma = sigma_Bird_Stewart_Lightfoot_critical_2 ( Tc , Pc ) elif Method == TEEGOTOSTEWARD3 : sigma = sigma_Tee_Gotoh_Steward_1 ( Tc , Pc ) elif Method == SILVALIUMACEDO : sigma = sigma_Silva_Liu_Macedo ( Tc , Pc ) elif Method == BSLB : sigma = sigma_Bird_Stewart_Lightfoot_boiling ( Vb ) elif Method == BSLM : sigma = sigma_Bird_Stewart_Lightfoot_melting ( Vm ) elif Method == STIELTHODOSMD : sigma = sigma_Stiel_Thodos ( Vc , Zc ) elif Method == TEEGOTOSTEWARD4 : sigma = sigma_Tee_Gotoh_Steward_2 ( Tc , Pc , omega ) elif Method == MAGALHAES : sigma = float ( MagalhaesLJ_data . at [ CASRN , "sigma" ] ) elif Method == NONE : sigma = None else : raise Exception ( 'Failure in in function' ) return sigma
2730	def get_object ( cls , api_token , domain_name ) : domain = cls ( token = api_token , name = domain_name ) domain . load ( ) return domain
5361	def __execute_initial_load ( self ) : if self . conf [ 'phases' ] [ 'panels' ] : tasks_cls = [ TaskPanels , TaskPanelsMenu ] self . execute_tasks ( tasks_cls ) if self . conf [ 'phases' ] [ 'identities' ] : tasks_cls = [ TaskInitSortingHat ] self . execute_tasks ( tasks_cls ) logger . info ( "Loading projects" ) tasks_cls = [ TaskProjects ] self . execute_tasks ( tasks_cls ) logger . info ( "Done" ) return
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
10341	def main ( graph : BELGraph , xlsx : str , tsvs : str ) : if not xlsx and not tsvs : click . secho ( 'Specify at least one option --xlsx or --tsvs' , fg = 'red' ) sys . exit ( 1 ) spia_matrices = bel_to_spia_matrices ( graph ) if xlsx : spia_matrices_to_excel ( spia_matrices , xlsx ) if tsvs : spia_matrices_to_tsvs ( spia_matrices , tsvs )
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
12094	def proto_VC_50_MT_IV ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = '02-check' , resize = False ) av1 , sd1 = swhlab . plot . IV ( abf , 1.2 , 1.4 , True , 'b' ) swhlab . plot . save ( abf , tag = 'iv' ) Xs = abf . clampValues ( 1.2 ) abf . saveThing ( [ Xs , av1 ] , '01_iv' )
10778	def update_field ( self , poses = None ) : m = np . clip ( self . particle_field , 0 , 1 ) part_color = np . zeros ( self . _image . shape ) for a in range ( 4 ) : part_color [ : , : , : , a ] = self . part_col [ a ] self . field = np . zeros ( self . _image . shape ) for a in range ( 4 ) : self . field [ : , : , : , a ] = m * part_color [ : , : , : , a ] + ( 1 - m ) * self . _image [ : , : , : , a ]
585	def _deleteRangeFromKNN ( self , start = 0 , end = None ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete . tolist ( ) ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
13772	def _default_json_default ( obj ) : if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time ) ) : return obj . strftime ( default_date_fmt ) else : return str ( obj )
2819	def convert_batchnorm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting batchnorm ...' ) if names == 'short' : tf_name = 'BN' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) mean_name = '{0}.running_mean' . format ( w_name ) var_name = '{0}.running_var' . format ( w_name ) if bias_name in weights : beta = weights [ bias_name ] . numpy ( ) if weights_name in weights : gamma = weights [ weights_name ] . numpy ( ) mean = weights [ mean_name ] . numpy ( ) variance = weights [ var_name ] . numpy ( ) eps = params [ 'epsilon' ] momentum = params [ 'momentum' ] if weights_name not in weights : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , center = False , scale = False , weights = [ mean , variance ] , name = tf_name ) else : bn = keras . layers . BatchNormalization ( axis = 1 , momentum = momentum , epsilon = eps , weights = [ gamma , beta , mean , variance ] , name = tf_name ) layers [ scope_name ] = bn ( layers [ inputs [ 0 ] ] )
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
5832	def get ( self , data_view_id ) : failure_message = "Dataview get failed" return self . _get_success_json ( self . _get ( 'v1/data_views/' + data_view_id , None , failure_message = failure_message ) ) [ 'data' ] [ 'data_view' ]
10975	def manage ( group_id ) : group = Group . query . get_or_404 ( group_id ) form = GroupForm ( request . form , obj = group ) if form . validate_on_submit ( ) : if group . can_edit ( current_user ) : try : group . update ( ** form . data ) flash ( _ ( 'Group "%(name)s" was updated' , name = group . name ) , 'success' ) except Exception as e : flash ( str ( e ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , ) else : flash ( _ ( 'You cannot edit group %(group_name)s' , group_name = group . name ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , group = group , )
9046	def rsolve ( A , y ) : from numpy_sugar . linalg import rsolve as _rsolve try : beta = _rsolve ( A , y ) except LinAlgError : msg = "Could not converge to solve Ax=y." msg += " Setting x to zero." warnings . warn ( msg , RuntimeWarning ) beta = zeros ( A . shape [ 0 ] ) return beta
13733	def value_to_python_log_level ( config_val , evar ) : if not config_val : config_val = evar . default_val config_val = config_val . upper ( ) return logging . _checkLevel ( config_val )
8036	def lexrank ( sentences , continuous = False , sim_threshold = 0.1 , alpha = 0.9 , use_divrank = False , divrank_alpha = 0.25 ) : ranker_params = { 'max_iter' : 1000 } if use_divrank : ranker = divrank_scipy ranker_params [ 'alpha' ] = divrank_alpha ranker_params [ 'd' ] = alpha else : ranker = networkx . pagerank_scipy ranker_params [ 'alpha' ] = alpha graph = networkx . DiGraph ( ) sent_tf_list = [ ] for sent in sentences : words = tools . word_segmenter_ja ( sent ) tf = collections . Counter ( words ) sent_tf_list . append ( tf ) sent_vectorizer = DictVectorizer ( sparse = True ) sent_vecs = sent_vectorizer . fit_transform ( sent_tf_list ) sim_mat = 1 - pairwise_distances ( sent_vecs , sent_vecs , metric = 'cosine' ) if continuous : linked_rows , linked_cols = numpy . where ( sim_mat > 0 ) else : linked_rows , linked_cols = numpy . where ( sim_mat >= sim_threshold ) graph . add_nodes_from ( range ( sent_vecs . shape [ 0 ] ) ) for i , j in zip ( linked_rows , linked_cols ) : if i == j : continue weight = sim_mat [ i , j ] if continuous else 1.0 graph . add_edge ( i , j , { 'weight' : weight } ) scores = ranker ( graph , ** ranker_params ) return scores , sim_mat
1057	def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , "code out of range" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return if key in _extension_registry : raise ValueError ( "key %s is already registered with code %s" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( "code %s is already in use for key %s" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key
11100	def select_by_atime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . atime <= max_time return self . select_file ( filters , recursive )
9666	def write_dot_file ( G , filename ) : with io . open ( filename , "w" ) as fh : fh . write ( "strict digraph DependencyDiagram {\n" ) edge_list = G . edges ( ) node_list = set ( G . nodes ( ) ) if edge_list : for edge in sorted ( edge_list ) : source , targ = edge node_list = node_list - set ( source ) node_list = node_list - set ( targ ) line = '"{}" -> "{}";\n' fh . write ( line . format ( source , targ ) ) if node_list : for node in sorted ( node_list ) : line = '"{}"\n' . format ( node ) fh . write ( line ) fh . write ( "}" )
9683	def toggle_laser ( self , state ) : a = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 10e-3 ) if state : b = self . cnxn . xfer ( [ 0x02 ] ) [ 0 ] else : b = self . cnxn . xfer ( [ 0x03 ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x03 else False
3092	def oauth_required ( self , method ) : def check_oauth ( request_handler , * args , ** kwargs ) : if self . _in_error : self . _display_error_message ( request_handler ) return user = users . get_current_user ( ) if not user : request_handler . redirect ( users . create_login_url ( request_handler . request . uri ) ) return self . _create_flow ( request_handler ) self . flow . params [ 'state' ] = _build_state_value ( request_handler , user ) self . credentials = self . _storage_class ( self . _credentials_class , None , self . _credentials_property_name , user = user ) . get ( ) if not self . has_credentials ( ) : return request_handler . redirect ( self . authorize_url ( ) ) try : resp = method ( request_handler , * args , ** kwargs ) except client . AccessTokenRefreshError : return request_handler . redirect ( self . authorize_url ( ) ) finally : self . credentials = None return resp return check_oauth
4634	def derive_private_key ( self , sequence ) : encoded = "%s %d" % ( str ( self ) , sequence ) a = bytes ( encoded , "ascii" ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . pubkey . prefix )
4307	def _validate_file_formats ( input_filepath_list , combine_type ) : _validate_sample_rates ( input_filepath_list , combine_type ) if combine_type == 'concatenate' : _validate_num_channels ( input_filepath_list , combine_type )
10469	def launchAppByBundlePath ( bundlePath , arguments = None ) : if arguments is None : arguments = [ ] bundleUrl = NSURL . fileURLWithPath_ ( bundlePath ) workspace = AppKit . NSWorkspace . sharedWorkspace ( ) arguments_strings = list ( map ( lambda a : NSString . stringWithString_ ( str ( a ) ) , arguments ) ) arguments = NSDictionary . dictionaryWithDictionary_ ( { AppKit . NSWorkspaceLaunchConfigurationArguments : NSArray . arrayWithArray_ ( arguments_strings ) } ) return workspace . launchApplicationAtURL_options_configuration_error_ ( bundleUrl , AppKit . NSWorkspaceLaunchAllowingClassicStartup , arguments , None )
12201	def from_yamlfile ( cls , fp , selector_handler = None , strict = False , debug = False ) : return cls . from_yamlstring ( fp . read ( ) , selector_handler = selector_handler , strict = strict , debug = debug )
11792	def lcv ( var , assignment , csp ) : "Least-constraining-values heuristic." return sorted ( csp . choices ( var ) , key = lambda val : csp . nconflicts ( var , val , assignment ) )
11901	def _run_server ( ) : port = _get_server_port ( ) SocketServer . TCPServer . allow_reuse_address = True server = SocketServer . TCPServer ( ( '' , port ) , SimpleHTTPServer . SimpleHTTPRequestHandler ) print ( 'Your images are at http://127.0.0.1:%d/%s' % ( port , INDEX_FILE_NAME ) ) try : server . serve_forever ( ) except KeyboardInterrupt : print ( 'User interrupted, stopping' ) except Exception as exptn : print ( exptn ) print ( 'Unhandled exception in server, stopping' )
2162	def update ( self , inventory_source , monitor = False , wait = False , timeout = None , ** kwargs ) : debug . log ( 'Asking whether the inventory source can be updated.' , header = 'details' ) r = client . get ( '%s%d/update/' % ( self . endpoint , inventory_source ) ) if not r . json ( ) [ 'can_update' ] : raise exc . BadRequest ( 'Tower says it cannot run an update against this inventory source.' ) debug . log ( 'Updating the inventory source.' , header = 'details' ) r = client . post ( '%s%d/update/' % ( self . endpoint , inventory_source ) , data = { } ) inventory_update_id = r . json ( ) [ 'inventory_update' ] if monitor or wait : if monitor : result = self . monitor ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) elif wait : result = self . wait ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) inventory = client . get ( '/inventory_sources/%d/' % result [ 'inventory_source' ] ) . json ( ) [ 'inventory' ] result [ 'inventory' ] = int ( inventory ) return result return { 'id' : inventory_update_id , 'status' : 'ok' }
7029	def generalized_lsp_value_notau ( times , mags , errs , omega ) : one_over_errs2 = 1.0 / ( errs * errs ) W = npsum ( one_over_errs2 ) wi = one_over_errs2 / W sin_omegat = npsin ( omega * times ) cos_omegat = npcos ( omega * times ) sin2_omegat = sin_omegat * sin_omegat cos2_omegat = cos_omegat * cos_omegat sincos_omegat = sin_omegat * cos_omegat Y = npsum ( wi * mags ) C = npsum ( wi * cos_omegat ) S = npsum ( wi * sin_omegat ) YpY = npsum ( wi * mags * mags ) YpC = npsum ( wi * mags * cos_omegat ) YpS = npsum ( wi * mags * sin_omegat ) CpC = npsum ( wi * cos2_omegat ) CpS = npsum ( wi * sincos_omegat ) YY = YpY - Y * Y YC = YpC - Y * C YS = YpS - Y * S CC = CpC - C * C SS = 1 - CpC - S * S CS = CpS - C * S Domega = CC * SS - CS * CS lspval = ( SS * YC * YC + CC * YS * YS - 2.0 * CS * YC * YS ) / ( YY * Domega ) return lspval
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
9214	def t_t_isopen ( self , t ) : r'"|\'' if t . value [ 0 ] == '"' : t . lexer . push_state ( 'istringquotes' ) elif t . value [ 0 ] == '\'' : t . lexer . push_state ( 'istringapostrophe' ) return t
5788	def _bcrypt_encrypt ( cipher , key , data , iv , padding ) : key_handle = None try : key_handle = _bcrypt_create_key_handle ( cipher , key ) if iv is None : iv_len = 0 else : iv_len = len ( iv ) flags = 0 if padding is True : flags = BcryptConst . BCRYPT_BLOCK_PADDING out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) iv_buffer = buffer_from_bytes ( iv ) if iv else null ( ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , iv_buffer , iv_len , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) finally : if key_handle : bcrypt . BCryptDestroyKey ( key_handle )
8854	def on_save_as ( self ) : path = self . tabWidget . current_widget ( ) . file . path path = os . path . dirname ( path ) if path else '' filename , filter = QtWidgets . QFileDialog . getSaveFileName ( self , 'Save' , path ) if filename : self . tabWidget . save_current ( filename ) self . recent_files_manager . open_file ( filename ) self . menu_recents . update_actions ( ) self . actionRun . setEnabled ( True ) self . actionConfigure_run . setEnabled ( True ) self . _update_status_bar ( self . tabWidget . current_widget ( ) )
12809	def received ( self , messages ) : if messages : if self . _queue : self . _queue . put_nowait ( messages ) if self . _callback : self . _callback ( messages )
7445	def _step4func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 4: Joint estimation of error rate and heterozygosity" ) samples = _get_samples ( self , samples ) if not self . _samples_precheck ( samples , 4 , force ) : raise IPyradError ( FIRST_RUN_3 ) elif not force : if all ( [ i . stats . state >= 4 for i in samples ] ) : print ( JOINTS_EXIST . format ( len ( samples ) ) ) return assemble . jointestimate . run ( self , samples , force , ipyclient )
3568	def stop_scan ( self , timeout_sec = TIMEOUT_SEC ) : self . _scan_stopped . clear ( ) self . _adapter . StopDiscovery ( ) if not self . _scan_stopped . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to stop scanning!' )
10406	def canonical_averages_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'number_of_runs' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'percolation_probability_mean' , 'float64' ) , ( 'percolation_probability_m2' , 'float64' ) , ] ) fields . extend ( [ ( 'max_cluster_size_mean' , 'float64' ) , ( 'max_cluster_size_m2' , 'float64' ) , ( 'moments_mean' , '(5,)float64' ) , ( 'moments_m2' , '(5,)float64' ) , ] ) return _ndarray_dtype ( fields )
1836	def JG ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
4974	def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
5844	def get_data_view ( self , data_view_id ) : url = routes . get_data_view ( data_view_id ) response = self . _get ( url ) . json ( ) result = response [ "data" ] [ "data_view" ] datasets_list = [ ] for dataset in result [ "datasets" ] : datasets_list . append ( Dataset ( name = dataset [ "name" ] , id = dataset [ "id" ] , description = dataset [ "description" ] ) ) columns_list = [ ] for column in result [ "columns" ] : columns_list . append ( ColumnFactory . from_dict ( column ) ) return DataView ( view_id = data_view_id , name = result [ "name" ] , description = result [ "description" ] , datasets = datasets_list , columns = columns_list , )
6860	def conf_path ( self ) : from burlap . system import distrib_id , distrib_release hostname = self . current_hostname if hostname not in self . _conf_cache : self . env . conf_specifics [ hostname ] = self . env . conf_default d_id = distrib_id ( ) d_release = distrib_release ( ) for key in ( ( d_id , d_release ) , ( d_id , ) ) : if key in self . env . conf_specifics : self . _conf_cache [ hostname ] = self . env . conf_specifics [ key ] return self . _conf_cache [ hostname ]
3472	def subtract_metabolites ( self , metabolites , combine = True , reversibly = True ) : self . add_metabolites ( { k : - v for k , v in iteritems ( metabolites ) } , combine = combine , reversibly = reversibly )
9988	def new_cells ( self , name = None , formula = None ) : return self . _impl . new_cells ( name , formula ) . interface
10996	def schedules ( self ) : url = PATHS [ 'GET_SCHEDULES' ] % self . id self . __schedules = self . api . get ( url = url ) return self . __schedules
962	def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results
12695	def contains_all ( set1 , set2 , warn ) : for elem in set2 : if elem not in set1 : raise ValueError ( warn ) return True
5554	def _raw_at_zoom ( config , zooms ) : params_per_zoom = { } for zoom in zooms : params = { } for name , element in config . items ( ) : if name not in _RESERVED_PARAMETERS : out_element = _element_at_zoom ( name , element , zoom ) if out_element is not None : params [ name ] = out_element params_per_zoom [ zoom ] = params return params_per_zoom
9081	def find ( self , query , ** kwargs ) : if 'providers' not in kwargs : providers = self . get_providers ( ) else : pargs = kwargs [ 'providers' ] if isinstance ( pargs , list ) : providers = self . get_providers ( ids = pargs ) else : providers = self . get_providers ( ** pargs ) kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . find ( query , ** kwarguments ) } for p in providers ]
558	def bestModelInSprint ( self , sprintIdx ) : swarms = self . getAllSwarms ( sprintIdx ) bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
7478	def sort_seeds ( uhandle , usort ) : cmd = [ "sort" , "-k" , "2" , uhandle , "-o" , usort ] proc = sps . Popen ( cmd , close_fds = True ) proc . communicate ( )
13598	def tick ( self ) : self . current += 1 if self . current == self . factor : sys . stdout . write ( '+' ) sys . stdout . flush ( ) self . current = 0
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
8843	def unindent ( self ) : if self . tab_always_indent : cursor = self . editor . textCursor ( ) if not cursor . hasSelection ( ) : cursor . select ( cursor . LineUnderCursor ) self . unindent_selection ( cursor ) else : super ( PyIndenterMode , self ) . unindent ( )
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
130	def is_fully_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = True )
6796	def manage_async ( self , command = '' , name = 'process' , site = ALL , exclude_sites = '' , end_message = '' , recipients = '' ) : exclude_sites = exclude_sites . split ( ':' ) r = self . local_renderer for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : if _site in exclude_sites : continue r . env . SITE = _site r . env . command = command r . env . end_email_command = '' r . env . recipients = recipients or '' r . env . end_email_command = '' if end_message : end_message = end_message + ' for ' + _site end_message = end_message . replace ( ' ' , '_' ) r . env . end_message = end_message r . env . end_email_command = r . format ( '{manage_cmd} send_mail --subject={end_message} --recipients={recipients}' ) r . env . name = name . format ( ** r . genv ) r . run ( 'screen -dmS {name} bash -c "export SITE={SITE}; ' 'export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} {command} --traceback; {end_email_command}"; sleep 3;' )
11842	def ModelBasedVacuumAgent ( ) : "An agent that keeps track of what locations are clean or dirty." model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
13462	def video_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) return render ( request , 'video/video_list.html' , { 'event' : event , 'video_list' : event . eventvideo_set . all ( ) } )
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , ** savgol_kwargs ) : if errs is None : errs = 0.001 * mags finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , ** savgol_kwargs ) subtracted = fmags - smoothed series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] for ind , extrema_index in enumerate ( extrema_indices ) : pass
13003	def modify_data ( data ) : with tempfile . NamedTemporaryFile ( 'w' ) as f : for entry in data : f . write ( json . dumps ( entry . to_dict ( include_meta = True ) , default = datetime_handler ) ) f . write ( '\n' ) f . flush ( ) print_success ( "Starting editor" ) subprocess . call ( [ 'nano' , '-' , f . name ] ) with open ( f . name , 'r' ) as f : return f . readlines ( )
2913	def get_state_name ( self ) : state_name = [ ] for state , name in list ( self . state_names . items ( ) ) : if self . _has_state ( state ) : state_name . append ( name ) return '|' . join ( state_name )
2844	def enable_FTDI_driver ( ) : logger . debug ( 'Enabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) _check_running_as_root ( ) subprocess . check_call ( 'kextload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . check_call ( 'kextload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) _check_running_as_root ( ) subprocess . check_call ( 'modprobe -q ftdi_sio' , shell = True ) subprocess . check_call ( 'modprobe -q usbserial' , shell = True )
7959	def handle_hup ( self ) : with self . lock : if self . _state == 'connecting' and self . _dst_addrs : self . _hup = False self . _set_state ( "connect" ) return self . _hup = True
11542	def read ( self , pin ) : if type ( pin ) is list : return [ self . read ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : value = self . _read ( pin_id ) lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'read' ] ) is tuple : read_range = lpin [ 'read' ] value = self . _linear_interpolation ( value , * read_range ) return value else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6103	def luminosities_of_galaxies_within_circles_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_circle_in_units ( radius = radius , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
12015	def define_spotsignal ( self ) : client = kplr . API ( ) star = client . star ( self . kic ) lcs = star . get_light_curves ( short_cadence = False ) time , flux , ferr , qual = [ ] , [ ] , [ ] , [ ] for lc in lcs : with lc . open ( ) as f : hdu_data = f [ 1 ] . data time . append ( hdu_data [ "time" ] ) flux . append ( hdu_data [ "pdcsap_flux" ] ) ferr . append ( hdu_data [ "pdcsap_flux_err" ] ) qual . append ( hdu_data [ "sap_quality" ] ) tout = np . array ( [ ] ) fout = np . array ( [ ] ) eout = np . array ( [ ] ) for i in range ( len ( flux ) ) : t = time [ i ] [ qual [ i ] == 0 ] f = flux [ i ] [ qual [ i ] == 0 ] e = ferr [ i ] [ qual [ i ] == 0 ] t = t [ np . isfinite ( f ) ] e = e [ np . isfinite ( f ) ] f = f [ np . isfinite ( f ) ] e /= np . median ( f ) f /= np . median ( f ) tout = np . append ( tout , t [ 50 : ] + 54833 ) fout = np . append ( fout , f [ 50 : ] ) eout = np . append ( eout , e [ 50 : ] ) self . spot_signal = np . zeros ( 52 ) for i in range ( len ( self . times ) ) : if self . times [ i ] < 55000 : self . spot_signal [ i ] = 1.0 else : self . spot_signal [ i ] = fout [ np . abs ( self . times [ i ] - tout ) == np . min ( np . abs ( self . times [ i ] - tout ) ) ]
7323	def sendmail ( message , sender , recipients , config_filename ) : if not hasattr ( sendmail , "host" ) : config = configparser . RawConfigParser ( ) config . read ( config_filename ) sendmail . host = config . get ( "smtp_server" , "host" ) sendmail . port = config . getint ( "smtp_server" , "port" ) sendmail . username = config . get ( "smtp_server" , "username" ) sendmail . security = config . get ( "smtp_server" , "security" ) print ( ">>> Read SMTP server configuration from {}" . format ( config_filename ) ) print ( ">>> host = {}" . format ( sendmail . host ) ) print ( ">>> port = {}" . format ( sendmail . port ) ) print ( ">>> username = {}" . format ( sendmail . username ) ) print ( ">>> security = {}" . format ( sendmail . security ) ) if not hasattr ( sendmail , "password" ) : if sendmail . security == "Dummy" or sendmail . username == "None" : sendmail . password = None else : prompt = ">>> password for {} on {}: " . format ( sendmail . username , sendmail . host ) sendmail . password = getpass . getpass ( prompt ) if sendmail . security == "SSL/TLS" : smtp = smtplib . SMTP_SSL ( sendmail . host , sendmail . port ) elif sendmail . security == "STARTTLS" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) smtp . ehlo ( ) smtp . starttls ( ) smtp . ehlo ( ) elif sendmail . security == "Never" : smtp = smtplib . SMTP ( sendmail . host , sendmail . port ) elif sendmail . security == "Dummy" : smtp = smtp_dummy . SMTP_dummy ( ) else : raise configparser . Error ( "Unrecognized security type: {}" . format ( sendmail . security ) ) if sendmail . username != "None" : smtp . login ( sendmail . username , sendmail . password ) smtp . sendmail ( sender , recipients , message . as_string ( ) ) smtp . close ( )
7773	def _quote ( data ) : data = data . replace ( b'\\' , b'\\\\' ) data = data . replace ( b'"' , b'\\"' ) return data
8581	def update_server ( self , datacenter_id , server_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : if attr == 'boot_volume' : boot_volume_properties = { "id" : value } boot_volume_entities = { "bootVolume" : boot_volume_properties } data . update ( boot_volume_entities ) else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
8158	def sql ( self , sql ) : self . _cur . execute ( sql ) if sql . lower ( ) . find ( "select" ) >= 0 : matches = [ ] for r in self . _cur : matches . append ( r ) return matches
9781	def get ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : response = PolyaxonClient ( ) . build_job . get_build ( user , project_name , _build ) cache . cache ( config_manager = BuildJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_build_details ( response )
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
4846	def get_content_metadata ( self , enterprise_customer ) : content_metadata = OrderedDict ( ) if enterprise_customer . catalog : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_ENDPOINT , detail_resource = 'courses' , resource_id = str ( enterprise_customer . uuid ) , traverse_pagination = True , ) for course in response [ 'results' ] : for course_run in course [ 'course_runs' ] : course_run [ 'content_type' ] = 'courserun' content_metadata [ course_run [ 'key' ] ] = course_run for enterprise_customer_catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_CATALOGS_ENDPOINT , resource_id = str ( enterprise_customer_catalog . uuid ) , traverse_pagination = True , querystring = { 'page_size' : 1000 } , ) for item in response [ 'results' ] : content_id = utils . get_content_metadata_item_id ( item ) content_metadata [ content_id ] = item return content_metadata . values ( )
2765	def get_droplet_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=droplet" ) return [ Snapshot ( token = self . token , ** snapshot ) for snapshot in data [ 'snapshots' ] ]
2644	def push_file ( self , source , dest_dir ) : local_dest = dest_dir + '/' + os . path . basename ( source ) if os . path . dirname ( source ) != dest_dir : try : shutil . copyfile ( source , local_dest ) os . chmod ( local_dest , 0o777 ) except OSError as e : raise FileCopyException ( e , self . hostname ) return local_dest
13574	def run ( exercise , command ) : Popen ( [ 'nohup' , command , exercise . path ( ) ] , stdout = DEVNULL , stderr = DEVNULL )
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
8283	def _curvepoint ( self , t , x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , handles = False ) : mint = 1 - t x01 = x0 * mint + x1 * t y01 = y0 * mint + y1 * t x12 = x1 * mint + x2 * t y12 = y1 * mint + y2 * t x23 = x2 * mint + x3 * t y23 = y2 * mint + y3 * t out_c1x = x01 * mint + x12 * t out_c1y = y01 * mint + y12 * t out_c2x = x12 * mint + x23 * t out_c2y = y12 * mint + y23 * t out_x = out_c1x * mint + out_c2x * t out_y = out_c1y * mint + out_c2y * t if not handles : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y ) else : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y , x01 , y01 , x23 , y23 )
6960	def read_model_table ( modelfile ) : infd = gzip . open ( modelfile ) model = np . genfromtxt ( infd , names = True ) infd . close ( ) return model
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
10489	def _getBundleId ( self ) : ra = AppKit . NSRunningApplication app = ra . runningApplicationWithProcessIdentifier_ ( self . _getPid ( ) ) return app . bundleIdentifier ( )
5099	def _matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
68	def copy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return BoundingBox ( x1 = self . x1 if x1 is None else x1 , x2 = self . x2 if x2 is None else x2 , y1 = self . y1 if y1 is None else y1 , y2 = self . y2 if y2 is None else y2 , label = self . label if label is None else label )
10709	def _authenticate ( self ) : auth_url = BASE_URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant_type' : 'password' } arequest = requests . post ( auth_url , data = payload , headers = BASIC_HEADERS ) status = arequest . status_code if status != 200 : _LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) _LOGGER . debug ( str ( response ) ) self . token = response . get ( "access_token" ) self . refresh_token = response . get ( "refresh_token" ) _auth = HEADERS . get ( "Authorization" ) _auth = _auth % self . token HEADERS [ "Authorization" ] = _auth _LOGGER . info ( "Authentication was successful, token set." ) return True
8378	def add_color_info ( e , path ) : _ctx . colormode ( RGB , 1.0 ) def _color ( hex , alpha = 1.0 ) : if hex == "none" : return None n = int ( hex [ 1 : ] , 16 ) r = ( n >> 16 ) & 0xff g = ( n >> 8 ) & 0xff b = n & 0xff return _ctx . color ( r / 255.0 , g / 255.0 , b / 255.0 , alpha ) path . fill = ( 0 , 0 , 0 , 0 ) path . stroke = ( 0 , 0 , 0 , 0 ) path . strokewidth = 0 alpha = get_attribute ( e , "opacity" , default = "" ) if alpha == "" : alpha = 1.0 else : alpha = float ( alpha ) try : path . fill = _color ( get_attribute ( e , "fill" , default = "#00000" ) , alpha ) except : pass try : path . stroke = _color ( get_attribute ( e , "stroke" , default = "none" ) , alpha ) except : pass try : path . strokewidth = float ( get_attribute ( e , "stroke-width" , default = "1" ) ) except : pass style = get_attribute ( e , "style" , default = "" ) . split ( ";" ) for s in style : try : if s . startswith ( "fill:" ) : path . fill = _color ( s . replace ( "fill:" , "" ) ) elif s . startswith ( "stroke:" ) : path . stroke = _color ( s . replace ( "stroke:" , "" ) ) elif s . startswith ( "stroke-width:" ) : path . strokewidth = float ( s . replace ( "stroke-width:" , "" ) ) except : pass path . closed = False if path [ 0 ] . x == path [ len ( path ) - 1 ] . x and path [ 0 ] . y == path [ len ( path ) - 1 ] . y : path . closed = True for i in range ( 1 , - 1 ) : if path [ i ] . cmd == MOVETO : path . closed = False return path
9327	def valid_content_type ( self , content_type , accept ) : accept_tokens = accept . replace ( ' ' , '' ) . split ( ';' ) content_type_tokens = content_type . replace ( ' ' , '' ) . split ( ';' ) return ( all ( elem in content_type_tokens for elem in accept_tokens ) and ( content_type_tokens [ 0 ] == 'application/vnd.oasis.taxii+json' or content_type_tokens [ 0 ] == 'application/vnd.oasis.stix+json' ) )
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
3374	def remove_cons_vars_from_problem ( model , what ) : context = get_context ( model ) model . solver . remove ( what ) if context : context ( partial ( model . solver . add , what ) )
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
2318	def predict ( self , data , alpha = 0.01 , max_iter = 2000 , ** kwargs ) : edge_model = GraphLasso ( alpha = alpha , max_iter = max_iter ) edge_model . fit ( data . values ) return nx . relabel_nodes ( nx . DiGraph ( edge_model . get_precision ( ) ) , { idx : i for idx , i in enumerate ( data . columns ) } )
11633	def get_orthographies ( self , _library = library ) : results = [ ] for charset in _library . charsets : if self . _charsets : cn = getattr ( charset , 'common_name' , False ) abbr = getattr ( charset , 'abbreviation' , False ) nn = getattr ( charset , 'short_name' , False ) naive = getattr ( charset , 'native_name' , False ) if cn and cn . lower ( ) in self . _charsets : results . append ( charset ) elif nn and nn . lower ( ) in self . _charsets : results . append ( charset ) elif naive and naive . lower ( ) in self . _charsets : results . append ( charset ) elif abbr and abbr . lower ( ) in self . _charsets : results . append ( charset ) else : results . append ( charset ) for result in results : yield CharsetInfo ( self , result )
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
4909	def _delete ( self , url , data , scope ) : self . _create_session ( scope ) response = self . session . delete ( url , data = data ) return response . status_code , response . text
2744	def load_by_pub_key ( self , public_key ) : data = self . get_data ( "account/keys/" ) for jsoned in data [ 'ssh_keys' ] : if jsoned . get ( 'public_key' , "" ) == public_key : self . id = jsoned [ 'id' ] self . load ( ) return self return None
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
7072	def matthews_correl_coeff ( ntp , ntn , nfp , nfn ) : mcc_top = ( ntp * ntn - nfp * nfn ) mcc_bot = msqrt ( ( ntp + nfp ) * ( ntp + nfn ) * ( ntn + nfp ) * ( ntn + nfn ) ) if mcc_bot > 0 : return mcc_top / mcc_bot else : return np . nan
388	def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
189	def copy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = lss , shape = shape )
12335	def apt ( self , package_names , raise_on_error = False ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "apt-get install -y %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
5609	def tiles_to_affine_shape ( tiles ) : if not tiles : raise TypeError ( "no tiles provided" ) pixel_size = tiles [ 0 ] . pixel_x_size left , bottom , right , top = ( min ( [ t . left for t in tiles ] ) , min ( [ t . bottom for t in tiles ] ) , max ( [ t . right for t in tiles ] ) , max ( [ t . top for t in tiles ] ) , ) return ( Affine ( pixel_size , 0 , left , 0 , - pixel_size , top ) , Shape ( width = int ( round ( ( right - left ) / pixel_size , 0 ) ) , height = int ( round ( ( top - bottom ) / pixel_size , 0 ) ) , ) )
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
7229	def paint ( self ) : snippet = { 'heatmap-radius' : VectorStyle . get_style_value ( self . radius ) , 'heatmap-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'heatmap-color' : VectorStyle . get_style_value ( self . color ) , 'heatmap-intensity' : VectorStyle . get_style_value ( self . intensity ) , 'heatmap-weight' : VectorStyle . get_style_value ( self . weight ) } return snippet
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
5409	def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
8676	def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase , dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )
7656	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json__ , self . __schema__ ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
12558	def drain_rois ( img ) : img_data = get_img_data ( img ) out = np . zeros ( img_data . shape , dtype = img_data . dtype ) krn_dim = [ 3 ] * img_data . ndim kernel = np . ones ( krn_dim , dtype = int ) vals = np . unique ( img_data ) vals = vals [ vals != 0 ] for i in vals : roi = img_data == i hits = scn . binary_hit_or_miss ( roi , kernel ) roi [ hits ] = 0 out [ roi > 0 ] = i return out
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
5130	def find ( self , s ) : pSet = [ s ] parent = self . _leader [ s ] while parent != self . _leader [ parent ] : pSet . append ( parent ) parent = self . _leader [ parent ] if len ( pSet ) > 1 : for a in pSet : self . _leader [ a ] = parent return parent
9292	def python_value ( self , value ) : value = super ( OrderedUUIDField , self ) . python_value ( value ) u = binascii . b2a_hex ( value ) value = u [ 8 : 16 ] + u [ 4 : 8 ] + u [ 0 : 4 ] + u [ 16 : 22 ] + u [ 22 : 32 ] return UUID ( value . decode ( ) )
7664	def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . ndim != 1 or np . any ( times < 0 ) : raise ParameterError ( 'times must be 1-dimensional and non-negative' ) idx = np . argsort ( times ) samples = times [ idx ] values = [ list ( ) for _ in samples ] confidences = [ list ( ) for _ in samples ] for obs in self . data : start = np . searchsorted ( samples , obs . time ) end = np . searchsorted ( samples , obs . time + obs . duration , side = 'right' ) for i in range ( start , end ) : values [ idx [ i ] ] . append ( obs . value ) confidences [ idx [ i ] ] . append ( obs . confidence ) if confidence : return values , confidences else : return values
10656	def count_with_multiplier ( groups , multiplier ) : counts = collections . defaultdict ( float ) for group in groups : for element , count in group . count ( ) . items ( ) : counts [ element ] += count * multiplier return counts
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( ** params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
10112	def rewrite ( fname , visitor , ** kw ) : if not isinstance ( fname , pathlib . Path ) : assert isinstance ( fname , string_types ) fname = pathlib . Path ( fname ) assert fname . is_file ( ) with tempfile . NamedTemporaryFile ( delete = False ) as fp : tmp = pathlib . Path ( fp . name ) with UnicodeReader ( fname , ** kw ) as reader_ : with UnicodeWriter ( tmp , ** kw ) as writer : for i , row in enumerate ( reader_ ) : row = visitor ( i , row ) if row is not None : writer . writerow ( row ) shutil . move ( str ( tmp ) , str ( fname ) )
4892	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , grade = None , is_passing = False ) : LearnerDataTransmissionAudit = apps . get_model ( 'integrated_channel' , 'LearnerDataTransmissionAudit' ) completed_timestamp = None course_completed = False if completed_date is not None : completed_timestamp = parse_datetime_to_epoch_millis ( completed_date ) course_completed = is_passing return [ LearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , course_id = enterprise_enrollment . course_id , course_completed = course_completed , completed_timestamp = completed_timestamp , grade = grade , ) ]
10938	def update_eig_J ( self ) : CLOG . debug ( 'Eigen update.' ) vls , vcs = np . linalg . eigh ( self . JTJ ) res0 = self . calc_residuals ( ) for a in range ( min ( [ self . num_eig_dirs , vls . size ] ) ) : stif_dir = vcs [ - ( a + 1 ) ] dl = self . eig_dl _ = self . update_function ( self . param_vals + dl * stif_dir ) res1 = self . calc_residuals ( ) grad_stif = ( res1 - res0 ) / dl self . _rank_1_J_update ( stif_dir , grad_stif ) self . JTJ = np . dot ( self . J , self . J . T ) _ = self . update_function ( self . param_vals )
3188	def create ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'note' not in data : raise KeyError ( 'The list member note must have a note' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'notes' ) , data = data ) if response is not None : self . note_id = response [ 'id' ] else : self . note_id = None return response
4320	def set_globals ( self , dither = False , guard = False , multithread = False , replay_gain = False , verbosity = 2 ) : if not isinstance ( dither , bool ) : raise ValueError ( 'dither must be a boolean.' ) if not isinstance ( guard , bool ) : raise ValueError ( 'guard must be a boolean.' ) if not isinstance ( multithread , bool ) : raise ValueError ( 'multithread must be a boolean.' ) if not isinstance ( replay_gain , bool ) : raise ValueError ( 'replay_gain must be a boolean.' ) if verbosity not in VERBOSITY_VALS : raise ValueError ( 'Invalid value for VERBOSITY. Must be one {}' . format ( VERBOSITY_VALS ) ) global_args = [ ] if not dither : global_args . append ( '-D' ) if guard : global_args . append ( '-G' ) if multithread : global_args . append ( '--multi-threaded' ) if replay_gain : global_args . append ( '--replay-gain' ) global_args . append ( 'track' ) global_args . append ( '-V{}' . format ( verbosity ) ) self . globals = global_args return self
9814	def start ( ctx , file , u ) : specification = None job_config = None if file : specification = check_polyaxonfile ( file , log = False ) . specification if u : ctx . invoke ( upload , sync = False ) if specification : check_polyaxonfile_kind ( specification = specification , kind = specification . _NOTEBOOK ) job_config = specification . parsed_data user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . start_notebook ( user , project_name , job_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not start notebook project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 200 : Printer . print_header ( "A notebook for this project is already running on:" ) click . echo ( get_notebook_url ( user , project_name ) ) sys . exit ( 0 ) if response . status_code != 201 : Printer . print_error ( 'Something went wrong, Notebook was not created.' ) sys . exit ( 1 ) Printer . print_success ( 'Notebook is being deployed for project `{}`' . format ( project_name ) ) clint . textui . puts ( "It may take some time before you can access the notebook.\n" ) clint . textui . puts ( "Your notebook will be available on:\n" ) with clint . textui . indent ( 4 ) : clint . textui . puts ( get_notebook_url ( user , project_name ) )
11804	def assign ( self , var , val , assignment ) : "Assign var, and keep track of conflicts." oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
992	def sample ( reader , writer , n , start = None , stop = None , tsCol = None , writeSampleOnly = True ) : rows = list ( reader ) if tsCol is not None : ts = rows [ 0 ] [ tsCol ] inc = rows [ 1 ] [ tsCol ] - ts if start is None : start = 0 if stop is None : stop = len ( rows ) - 1 initialN = stop - start + 1 numDeletes = initialN - n for i in xrange ( numDeletes ) : delIndex = random . randint ( start , stop - i ) del rows [ delIndex ] if writeSampleOnly : rows = rows [ start : start + n ] if tsCol is not None : ts = rows [ 0 ] [ tsCol ] for row in rows : if tsCol is not None : row [ tsCol ] = ts ts += inc writer . appendRecord ( row )
8866	def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?)?" dq3string = r'(\b[rRuU])?)?' uf_sq3string = r"(\b[rRuU])?)$" uf_dq3string = r'(\b[rRuU])?)$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
3855	async def _sync_all_conversations ( client ) : conv_states = [ ] sync_timestamp = None request = hangouts_pb2 . SyncRecentConversationsRequest ( request_header = client . get_request_header ( ) , max_conversations = CONVERSATIONS_PER_REQUEST , max_events_per_conversation = 1 , sync_filter = [ hangouts_pb2 . SYNC_FILTER_INBOX , hangouts_pb2 . SYNC_FILTER_ARCHIVED , ] ) for _ in range ( MAX_CONVERSATION_PAGES ) : logger . info ( 'Requesting conversations page %s' , request . last_event_timestamp ) response = await client . sync_recent_conversations ( request ) conv_states = list ( response . conversation_state ) + conv_states sync_timestamp = parsers . from_timestamp ( response . response_header . current_server_time ) if response . continuation_end_timestamp == 0 : logger . info ( 'Reached final conversations page' ) break else : request . last_event_timestamp = response . continuation_end_timestamp else : logger . warning ( 'Exceeded maximum number of conversation pages' ) logger . info ( 'Synced %s total conversations' , len ( conv_states ) ) return conv_states , sync_timestamp
7215	def list ( self ) : r = self . gbdx_connection . get ( self . _base_url ) raise_for_status ( r ) return r . json ( ) [ 'tasks' ]
5548	def validate_values ( config , values ) : if not isinstance ( config , dict ) : raise TypeError ( "config must be a dictionary" ) for value , vtype in values : if value not in config : raise ValueError ( "%s not given" % value ) if not isinstance ( config [ value ] , vtype ) : raise TypeError ( "%s must be %s" % ( value , vtype ) ) return True
3427	def add_metabolites ( self , metabolite_list ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] if len ( metabolite_list ) == 0 : return None metabolite_list = [ x for x in metabolite_list if x . id not in self . metabolites ] bad_ids = [ m for m in metabolite_list if not isinstance ( m . id , string_types ) or len ( m . id ) < 1 ] if len ( bad_ids ) != 0 : raise ValueError ( 'invalid identifiers in {}' . format ( repr ( bad_ids ) ) ) for x in metabolite_list : x . _model = self self . metabolites += metabolite_list to_add = [ ] for met in metabolite_list : if met . id not in self . constraints : constraint = self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) to_add += [ constraint ] self . add_cons_vars ( to_add ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __isub__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , None ) )
7168	def remove_intent ( self , name ) : self . intents . remove ( name ) self . padaos . remove_intent ( name ) self . must_train = True
10766	def get_poll ( self , arg , * , request_policy = None ) : if isinstance ( arg , str ) : match = self . _url_re . match ( arg ) if match : arg = match . group ( 'id' ) return self . _http_client . get ( '{}/{}' . format ( self . _POLLS , arg ) , request_policy = request_policy , cls = strawpoll . Poll )
2372	def variables ( self ) : for table in self . tables : if isinstance ( table , VariableTable ) : for statement in table . rows : if statement [ 0 ] != "" : yield statement
11644	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = not self . copy ) vals = vals . reshape ( - 1 , 1 ) if self . min_eig == 0 : inner = vals > self . min_eig else : with np . errstate ( divide = 'ignore' ) : inner = np . where ( vals >= self . min_eig , 1 , np . where ( vals == 0 , 0 , self . min_eig / vals ) ) self . clip_ = np . dot ( vecs , inner * vecs . T ) return self
6918	def _get_acf_peakheights ( lags , acf , npeaks = 20 , searchinterval = 1 ) : maxinds = argrelmax ( acf , order = searchinterval ) [ 0 ] maxacfs = acf [ maxinds ] maxlags = lags [ maxinds ] mininds = argrelmin ( acf , order = searchinterval ) [ 0 ] minacfs = acf [ mininds ] minlags = lags [ mininds ] relpeakheights = npzeros ( npeaks ) relpeaklags = npzeros ( npeaks , dtype = npint64 ) peakindices = npzeros ( npeaks , dtype = npint64 ) for peakind , mxi in enumerate ( maxinds [ : npeaks ] ) : if npall ( mxi < mininds ) : continue leftminind = mininds [ mininds < mxi ] [ - 1 ] rightminind = mininds [ mininds > mxi ] [ 0 ] relpeakheights [ peakind ] = ( acf [ mxi ] - ( acf [ leftminind ] + acf [ rightminind ] ) / 2.0 ) relpeaklags [ peakind ] = lags [ mxi ] peakindices [ peakind ] = peakind if relpeakheights [ 0 ] > relpeakheights [ 1 ] : bestlag = relpeaklags [ 0 ] bestpeakheight = relpeakheights [ 0 ] bestpeakindex = peakindices [ 0 ] else : bestlag = relpeaklags [ 1 ] bestpeakheight = relpeakheights [ 1 ] bestpeakindex = peakindices [ 1 ] return { 'maxinds' : maxinds , 'maxacfs' : maxacfs , 'maxlags' : maxlags , 'mininds' : mininds , 'minacfs' : minacfs , 'minlags' : minlags , 'relpeakheights' : relpeakheights , 'relpeaklags' : relpeaklags , 'peakindices' : peakindices , 'bestlag' : bestlag , 'bestpeakheight' : bestpeakheight , 'bestpeakindex' : bestpeakindex }
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
11881	def scanProcessForMapping ( pid , searchPortion , isExactMatch = False , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e with open ( '/proc/%d/maps' % ( pid , ) , 'r' ) as f : contents = f . read ( ) lines = contents . split ( '\n' ) matchedMappings = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor == searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) == searchIn . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , searchIn : bool ( searchFor in searchIn ) else : isMatch = lambda searchFor , searchIn : bool ( searchFor . lower ( ) in searchIn . lower ( ) ) for line in lines : portion = ' ' . join ( line . split ( ' ' ) [ 5 : ] ) . lstrip ( ) if isMatch ( searchPortion , portion ) : matchedMappings . append ( '\t' + line ) if len ( matchedMappings ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'matchedMappings' : matchedMappings , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
2823	def convert_lrelu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting lrelu ...' ) if names == 'short' : tf_name = 'lRELU' + random_string ( 3 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) leakyrelu = keras . layers . LeakyReLU ( alpha = params [ 'alpha' ] , name = tf_name ) layers [ scope_name ] = leakyrelu ( layers [ inputs [ 0 ] ] )
7179	def lib2to3_unparse ( node , * , hg = False ) : code = str ( node ) if hg : from retype_hgext import apply_job_security code = apply_job_security ( code ) return code
3077	def has_credentials ( self ) : if not self . credentials : return False elif ( self . credentials . access_token_expired and not self . credentials . refresh_token ) : return False else : return True
3033	def credentials_from_clientsecrets_and_code ( filename , scope , code , message = None , redirect_uri = 'postmessage' , http = None , cache = None , device_uri = None ) : flow = flow_from_clientsecrets ( filename , scope , message = message , cache = cache , redirect_uri = redirect_uri , device_uri = device_uri ) credentials = flow . step2_exchange ( code , http = http ) return credentials
10672	def write_compound_to_auxi_file ( directory , compound ) : file_name = "Compound_" + compound . formula + ".json" with open ( os . path . join ( directory , file_name ) , 'w' ) as f : f . write ( str ( compound ) )
4081	def get_directory ( ) : try : language_check_dir = cache [ 'language_check_dir' ] except KeyError : def version_key ( string ) : return [ int ( e ) if e . isdigit ( ) else e for e in re . split ( r"(\d+)" , string ) ] def get_lt_dir ( base_dir ) : paths = [ path for path in glob . glob ( os . path . join ( base_dir , 'LanguageTool*' ) ) if os . path . isdir ( path ) ] return max ( paths , key = version_key ) if paths else None base_dir = os . path . dirname ( sys . argv [ 0 ] ) language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : try : base_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) except NameError : pass else : language_check_dir = get_lt_dir ( base_dir ) if not language_check_dir : raise PathError ( "can't find LanguageTool directory in {!r}" . format ( base_dir ) ) cache [ 'language_check_dir' ] = language_check_dir return language_check_dir
1632	def CheckForBadCharacters ( filename , lines , error ) : for linenum , line in enumerate ( lines ) : if unicode_escape_decode ( '\ufffd' ) in line : error ( filename , linenum , 'readability/utf8' , 5 , 'Line contains invalid UTF-8 (or Unicode replacement character).' ) if '\0' in line : error ( filename , linenum , 'readability/nul' , 5 , 'Line contains NUL byte.' )
7059	def s3_delete_file ( bucket , filename , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 's3' ) try : resp = client . delete_object ( Bucket = bucket , Key = filename ) if not resp : LOGERROR ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) else : return resp [ 'DeleteMarker' ] except Exception as e : LOGEXCEPTION ( 'could not delete file %s from bucket %s' % ( filename , bucket ) ) if raiseonfail : raise return None
7982	def auth_finish ( self , _unused ) : self . lock . acquire ( ) try : self . __logger . debug ( "Authenticated" ) self . authenticated = True self . state_change ( "authorized" , self . my_jid ) self . _post_auth ( ) finally : self . lock . release ( )
11124	def move_directory ( self , relativePath , relativeDestination , replace = False , verbose = True ) : relativePath = os . path . normpath ( relativePath ) relativeDestination = os . path . normpath ( relativeDestination ) filesInfo = list ( self . walk_files_info ( relativePath = relativePath ) ) dirsPath = list ( self . walk_directories_relative_path ( relativePath = relativePath ) ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage self . remove_directory ( relativePath = relativePath , removeFromSystem = False ) self . add_directory ( relativeDestination ) for RP , info in filesInfo : source = os . path . join ( self . __path , relativePath , RP ) destination = os . path . join ( self . __path , relativeDestination , RP ) newDirRP , fileName = os . path . split ( os . path . join ( relativeDestination , RP ) ) dirInfoDict = self . add_directory ( newDirRP ) if os . path . isfile ( destination ) : if replace : os . remove ( destination ) if verbose : warnings . warn ( "file '%s' is copied replacing existing one in destination '%s'." % ( fileName , newDirRP ) ) else : if verbose : warnings . warn ( "file '%s' is not copied because the same file exists in destination '%s'." % ( fileName , destination ) ) continue os . rename ( source , destination ) dict . __getitem__ ( dirInfoDict , "files" ) [ fileName ] = info self . save ( )
7842	def set_category ( self , category ) : if not category : raise ValueError ( "Category is required in DiscoIdentity" ) category = unicode ( category ) self . xmlnode . setProp ( "category" , category . encode ( "utf-8" ) )
3735	def CoolProp_T_dependent_property ( T , CASRN , prop , phase ) : r if not has_CoolProp : raise Exception ( 'CoolProp library is not installed' ) if CASRN not in coolprop_dict : raise Exception ( 'CASRN not in list of supported fluids' ) Tc = coolprop_fluids [ CASRN ] . Tc T = float ( T ) if phase == 'l' : if T > Tc : raise Exception ( 'For liquid properties, must be under the critical temperature.' ) if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) in [ u'liquid' , u'supercritical_liquid' ] : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'Q' , 0 , CASRN ) elif phase == 'g' : if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) == 'gas' : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : if T < Tc : return PropsSI ( prop , 'T' , T , 'Q' , 1 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : raise Exception ( 'Error in CoolProp property function' )
8404	def squish ( x , range = ( 0 , 1 ) , only_finite = True ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) finite = np . isfinite ( x ) if only_finite else True x [ np . logical_and ( x < range [ 0 ] , finite ) ] = range [ 0 ] x [ np . logical_and ( x > range [ 1 ] , finite ) ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
12961	def count ( self ) : conn = self . _get_connection ( ) numFilters = len ( self . filters ) numNotFilters = len ( self . notFilters ) if numFilters + numNotFilters == 0 : return conn . scard ( self . _get_ids_key ( ) ) if numNotFilters == 0 : if numFilters == 1 : ( filterFieldName , filterValue ) = self . filters [ 0 ] return conn . scard ( self . _get_key_for_index ( filterFieldName , filterValue ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] return len ( conn . sinter ( indexKeys ) ) notIndexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . notFilters ] if numFilters == 0 : return len ( conn . sdiff ( self . _get_ids_key ( ) , * notIndexKeys ) ) indexKeys = [ self . _get_key_for_index ( filterFieldName , filterValue ) for filterFieldName , filterValue in self . filters ] tempKey = self . _getTempKey ( ) pipeline = conn . pipeline ( ) pipeline . sinterstore ( tempKey , * indexKeys ) pipeline . sdiff ( tempKey , * notIndexKeys ) pipeline . delete ( tempKey ) pks = pipeline . execute ( ) [ 1 ] return len ( pks )
9920	def save ( self ) : token = models . PasswordResetToken . objects . get ( key = self . validated_data [ "key" ] ) token . email . user . set_password ( self . validated_data [ "password" ] ) token . email . user . save ( ) logger . info ( "Reset password for %s" , token . email . user ) token . delete ( )
10862	def param_particle_rad ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , 'a' ) for i in ind ]
5097	def _calculate_distance ( latlon1 , latlon2 ) : lat1 , lon1 = latlon1 lat2 , lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * ( np . sin ( dlon / 2 ) ) ** 2 c = 2 * np . pi * R * np . arctan2 ( np . sqrt ( a ) , np . sqrt ( 1 - a ) ) / 180 return c
12540	def is_dicom_file ( filepath ) : if not os . path . exists ( filepath ) : raise IOError ( 'File {} not found.' . format ( filepath ) ) filename = os . path . basename ( filepath ) if filename == 'DICOMDIR' : return False try : _ = dicom . read_file ( filepath ) except Exception as exc : log . debug ( 'Checking if {0} was a DICOM, but returned ' 'False.' . format ( filepath ) ) return False return True
10467	def getAnyAppWithWindow ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) if hasattr ( ref , 'windows' ) and len ( ref . windows ( ) ) > 0 : return ref raise ValueError ( 'No GUI application found.' )
684	def getTotaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n
11585	def _getnodenamefor ( self , name ) : "Return the node name where the ``name`` would land to" return 'node_' + str ( ( abs ( binascii . crc32 ( b ( name ) ) & 0xffffffff ) % self . no_servers ) + 1 )
11507	def item_get ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.get' , parameters ) return response
10299	def group_errors ( graph : BELGraph ) -> Mapping [ str , List [ int ] ] : warning_summary = defaultdict ( list ) for _ , exc , _ in graph . warnings : warning_summary [ str ( exc ) ] . append ( exc . line_number ) return dict ( warning_summary )
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
9519	def interleave ( infile_1 , infile_2 , outfile , suffix1 = None , suffix2 = None ) : seq_reader_1 = sequences . file_reader ( infile_1 ) seq_reader_2 = sequences . file_reader ( infile_2 ) f_out = utils . open_file_write ( outfile ) for seq_1 in seq_reader_1 : try : seq_2 = next ( seq_reader_2 ) except : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_1 . id , ' ... cannot continue' ) if suffix1 is not None and not seq_1 . id . endswith ( suffix1 ) : seq_1 . id += suffix1 if suffix2 is not None and not seq_2 . id . endswith ( suffix2 ) : seq_2 . id += suffix2 print ( seq_1 , file = f_out ) print ( seq_2 , file = f_out ) try : seq_2 = next ( seq_reader_2 ) except : seq_2 = None if seq_2 is not None : utils . close ( f_out ) raise Error ( 'Error getting mate for sequence' , seq_2 . id , ' ... cannot continue' ) utils . close ( f_out )
13461	def event_update_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) updates = Update . objects . filter ( event__slug = slug ) if event . recently_ended ( ) : updates = updates . order_by ( 'id' ) else : updates = updates . order_by ( '-id' ) return render ( request , 'happenings/updates/update_list.html' , { 'event' : event , 'object_list' : updates , } )
8416	def precision ( x ) : from . bounds import zero_range rng = min_max ( x , na_rm = True ) if zero_range ( rng ) : span = np . abs ( rng [ 0 ] ) else : span = np . diff ( rng ) [ 0 ] if span == 0 : return 1 else : return 10 ** int ( np . floor ( np . log10 ( span ) ) )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
1327	def normalized_distance ( self , image ) : return self . __distance ( self . __original_image_for_distance , image , bounds = self . bounds ( ) )
10885	def oslicer ( self , tile ) : mask = None vecs = tile . coords ( form = 'meshed' ) for v in vecs : v [ self . slicer ] = - 1 mask = mask & ( v > 0 ) if mask is not None else ( v > 0 ) return tuple ( np . array ( i ) . astype ( 'int' ) for i in zip ( * [ v [ mask ] for v in vecs ] ) )
6871	def estimate_achievable_tmid_precision ( snr , t_ingress_min = 10 , t_duration_hr = 2.14 ) : t_ingress = t_ingress_min * u . minute t_duration = t_duration_hr * u . hour theta = t_ingress / t_duration sigma_tc = ( 1 / snr * t_duration * np . sqrt ( theta / 2 ) ) LOGINFO ( 'assuming t_ingress = {:.1f}' . format ( t_ingress ) ) LOGINFO ( 'assuming t_duration = {:.1f}' . format ( t_duration ) ) LOGINFO ( 'measured SNR={:.2f}\n\t' . format ( snr ) + ' . format ( sigma_tc . to ( u . minute ) , sigma_tc . to ( u . hour ) , sigma_tc . to ( u . day ) ) ) return sigma_tc . to ( u . day ) . value
8638	def revoke_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'revoke' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRevokedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
13884	def ListFiles ( directory ) : from six . moves . urllib . parse import urlparse directory_url = urlparse ( directory ) if _UrlIsLocal ( directory_url ) : if not os . path . isdir ( directory ) : return None return os . listdir ( directory ) elif directory_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme ) else : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( directory_url . scheme )
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( lcat , ccat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
291	def plot_rolling_volatility ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_vol_ts = timeseries . rolling_volatility ( returns , rolling_window ) rolling_vol_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , ** kwargs ) if factor_returns is not None : rolling_vol_ts_factor = timeseries . rolling_volatility ( factor_returns , rolling_window ) rolling_vol_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , ** kwargs ) ax . set_title ( 'Rolling volatility (6-month)' ) ax . axhline ( rolling_vol_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_ylabel ( 'Volatility' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Volatility' , 'Benchmark volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax
10997	def schedules ( self , schedules ) : url = PATHS [ 'UPDATE_SCHEDULES' ] % self . id data_format = "schedules[0][%s][]=%s&" post_data = "" for format_type , values in schedules . iteritems ( ) : for value in values : post_data += data_format % ( format_type , value ) self . api . post ( url = url , data = post_data )
4793	def is_subset_of ( self , * supersets ) : if not isinstance ( self . val , Iterable ) : raise TypeError ( 'val is not iterable' ) if len ( supersets ) == 0 : raise ValueError ( 'one or more superset args must be given' ) missing = [ ] if hasattr ( self . val , 'keys' ) and callable ( getattr ( self . val , 'keys' ) ) and hasattr ( self . val , '__getitem__' ) : superdict = { } for l , j in enumerate ( supersets ) : self . _check_dict_like ( j , check_values = False , name = 'arg #%d' % ( l + 1 ) ) for k in j . keys ( ) : superdict . update ( { k : j [ k ] } ) for i in self . val . keys ( ) : if i not in superdict : missing . append ( { i : self . val [ i ] } ) elif self . val [ i ] != superdict [ i ] : missing . append ( { i : self . val [ i ] } ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superdict ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) else : superset = set ( ) for j in supersets : try : for k in j : superset . add ( k ) except Exception : superset . add ( j ) for i in self . val : if i not in superset : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to be subset of %s, but %s %s missing.' % ( self . val , self . _fmt_items ( superset ) , self . _fmt_items ( missing ) , 'was' if len ( missing ) == 1 else 'were' ) ) return self
8279	def _append_element ( self , render_func , pe ) : self . _render_funcs . append ( render_func ) self . _elements . append ( pe )
9063	def unfix ( self , param ) : if param == "delta" : self . _unfix ( "logistic" ) else : self . _fix [ param ] = False
10425	def infer_missing_backwards_edge ( graph , u , v , k ) : if u in graph [ v ] : for attr_dict in graph [ v ] [ u ] . values ( ) : if attr_dict == graph [ u ] [ v ] [ k ] : return graph . add_edge ( v , u , key = k , ** graph [ u ] [ v ] [ k ] )
8642	def create_milestone_payment ( session , project_id , bidder_id , amount , reason , description ) : milestone_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'amount' : amount , 'reason' : reason , 'description' : description } response = make_post_request ( session , 'milestones' , json_data = milestone_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_data = json_data [ 'result' ] return Milestone ( milestone_data ) else : raise MilestoneNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) authenticated_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) http . request = new_request http . request . credentials = credentials
3924	def _on_return ( self , text ) : if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : filename = text . split ( ' ' ) [ 1 ] image_file = open ( filename , 'rb' ) text = '' else : image_file = None text = replace_emoticons ( text ) segments = hangups . ChatMessageSegment . from_str ( text ) self . _coroutine_queue . put ( self . _handle_send_message ( self . _conversation . send_message ( segments , image_file = image_file ) ) )
8743	def delete_floatingip ( context , id ) : LOG . info ( 'delete_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . FLOATING )
4597	def index ( self , i , length = None ) : if self . begin <= i <= self . end : index = i - self . BEGIN - self . offset if length is None : length = self . full_range ( ) else : length = min ( length , self . full_range ( ) ) if 0 <= index < length : return index
6887	def parallel_epd_lclist ( lclist , externalparams , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , nworkers = NCPUS , maxworkertasks = 1000 ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = dtimecols if magcols is None : magcols = dmagcols if errcols is None : errcols = derrcols outdict = { } for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , externalparams , lcformat , lcformatdir , epdsmooth_sigclip , epdsmooth_windowsize , epdsmooth_func , epdsmooth_extraparams ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_epd_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
12707	def rotation ( self , rotation ) : if isinstance ( rotation , np . ndarray ) : rotation = rotation . ravel ( ) self . ode_body . setRotation ( tuple ( rotation ) )
8873	def isA ( instance , typeList ) : return any ( map ( lambda iType : isinstance ( instance , iType ) , typeList ) )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
7113	def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator . predict ( x ) y = [ item [ 0 ] for item in y ] y = [ self . _remove_prefix ( label ) for label in y ] if not isinstance ( X , list ) : y = y [ 0 ] return y
9020	def _connect_rows ( self , connections ) : for connection in connections : from_row_id = self . _to_id ( connection [ FROM ] [ ID ] ) from_row = self . _id_cache [ from_row_id ] from_row_start_index = connection [ FROM ] . get ( START , DEFAULT_START ) from_row_number_of_possible_meshes = from_row . number_of_produced_meshes - from_row_start_index to_row_id = self . _to_id ( connection [ TO ] [ ID ] ) to_row = self . _id_cache [ to_row_id ] to_row_start_index = connection [ TO ] . get ( START , DEFAULT_START ) to_row_number_of_possible_meshes = to_row . number_of_consumed_meshes - to_row_start_index meshes = min ( from_row_number_of_possible_meshes , to_row_number_of_possible_meshes ) number_of_meshes = connection . get ( MESHES , meshes ) from_row_stop_index = from_row_start_index + number_of_meshes to_row_stop_index = to_row_start_index + number_of_meshes assert 0 <= from_row_start_index <= from_row_stop_index produced_meshes = from_row . produced_meshes [ from_row_start_index : from_row_stop_index ] assert 0 <= to_row_start_index <= to_row_stop_index consumed_meshes = to_row . consumed_meshes [ to_row_start_index : to_row_stop_index ] assert len ( produced_meshes ) == len ( consumed_meshes ) mesh_pairs = zip ( produced_meshes , consumed_meshes ) for produced_mesh , consumed_mesh in mesh_pairs : produced_mesh . connect_to ( consumed_mesh )
5981	def setup_figure ( figsize , as_subplot ) : if not as_subplot : fig = plt . figure ( figsize = figsize ) return fig
8351	def handle_pi ( self , text ) : if text [ : 3 ] == "xml" : text = u"xml version='1.0' encoding='%SOUP-ENCODING%'" self . _toStringSubclass ( text , ProcessingInstruction )
8681	def delete ( self , key_name ) : self . _assert_valid_stash ( ) if key_name == 'stored_passphrase' : raise GhostError ( '`stored_passphrase` is a reserved ghost key name ' 'which cannot be deleted' ) if not self . get ( key_name ) : raise GhostError ( 'Key `{0}` not found' . format ( key_name ) ) key = self . _storage . get ( key_name ) if key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be deleted ' 'Please unlock the key and try again' . format ( key_name ) ) deleted = self . _storage . delete ( key_name ) audit ( storage = self . _storage . db_path , action = 'DELETE' , message = json . dumps ( dict ( key_name = key_name ) ) ) if not deleted : raise GhostError ( 'Failed to delete {0}' . format ( key_name ) )
1776	def AND ( cpu , dest , src ) : if src . size == 64 and src . type == 'immediate' and dest . size == 64 : arg1 = Operators . SEXTEND ( src . read ( ) , 32 , 64 ) else : arg1 = src . read ( ) res = dest . write ( dest . read ( ) & arg1 ) cpu . _calculate_logic_flags ( dest . size , res )
5330	def get_identities ( config ) : TaskProjects ( config ) . execute ( ) task = TaskIdentitiesMerge ( config ) task . execute ( ) logging . info ( "Merging identities finished!" )
5673	def from_directory_as_inmemory_db ( cls , gtfs_directory ) : from gtfspy . import_gtfs import import_gtfs conn = sqlite3 . connect ( ":memory:" ) import_gtfs ( gtfs_directory , conn , preserve_connection = True , print_progress = False ) return cls ( conn )
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
12004	def _read_header ( self , data ) : version = self . _read_version ( data ) version_info = self . _get_version_info ( version ) header_data = data [ : version_info [ 'header_size' ] ] header = version_info [ 'header' ] header = header . _make ( unpack ( version_info [ 'header_format' ] , header_data ) ) header = dict ( header . _asdict ( ) ) flags = list ( "{0:0>8b}" . format ( header [ 'flags' ] ) ) flags = dict ( version_info [ 'flags' ] . _make ( flags ) . _asdict ( ) ) flags = dict ( ( i , bool ( int ( j ) ) ) for i , j in flags . iteritems ( ) ) header [ 'flags' ] = flags timestamp = None if flags [ 'timestamp' ] : ts_start = version_info [ 'header_size' ] ts_end = ts_start + version_info [ 'timestamp_size' ] timestamp_data = data [ ts_start : ts_end ] timestamp = unpack ( version_info [ 'timestamp_format' ] , timestamp_data ) [ 0 ] header [ 'info' ] = { 'timestamp' : timestamp } return header
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
4344	def stat ( self , input_filepath , scale = None , rms = False ) : effect_args = [ 'channels' , '1' , 'stat' ] if scale is not None : if not is_number ( scale ) or scale <= 0 : raise ValueError ( "scale must be a positive number." ) effect_args . extend ( [ '-s' , '{:f}' . format ( scale ) ] ) if rms : effect_args . append ( '-rms' ) _ , _ , stat_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stat_dict = { } lines = stat_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stat_dict [ key . strip ( ':' ) ] = value return stat_dict
9669	def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
8891	def deserialize ( cls , dict_model ) : kwargs = { } for f in cls . _meta . concrete_fields : if f . attname in dict_model : kwargs [ f . attname ] = dict_model [ f . attname ] return cls ( ** kwargs )
8258	def sort_by_distance ( self , reversed = False ) : if len ( self ) == 0 : return ColorList ( ) root = self [ 0 ] for clr in self [ 1 : ] : if clr . brightness < root . brightness : root = clr stack = [ clr for clr in self ] stack . remove ( root ) sorted = [ root ] while len ( stack ) > 1 : closest , distance = stack [ 0 ] , stack [ 0 ] . distance ( sorted [ - 1 ] ) for clr in stack [ 1 : ] : d = clr . distance ( sorted [ - 1 ] ) if d < distance : closest , distance = clr , d stack . remove ( closest ) sorted . append ( closest ) sorted . append ( stack [ 0 ] ) if reversed : _list . reverse ( sorted ) return ColorList ( sorted )
13270	def all_subclasses ( cls ) : for subclass in cls . __subclasses__ ( ) : yield subclass for subc in all_subclasses ( subclass ) : yield subc
470	def read_words ( filename = "nietzsche.txt" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , "r" ) as f : try : context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list
5249	def bopen ( ** kwargs ) : con = BCon ( ** kwargs ) con . start ( ) try : yield con finally : con . stop ( )
2079	def disassociate_notification_template ( self , job_template , notification_template , status ) : return self . _disassoc ( 'notification_templates_%s' % status , job_template , notification_template )
11496	def get_user_by_email ( self , email ) : parameters = dict ( ) parameters [ 'email' ] = email response = self . request ( 'midas.user.get' , parameters ) return response
851	def rewind ( self ) : super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) self . _recordCount = 0
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
8813	def get_unused_ips ( session , used_ips_counts , ** kwargs ) : LOG . debug ( "Getting unused IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , models . Subnet ) query = _filter ( query , ** kwargs ) query = query . group_by ( models . Subnet . segment_id , models . Subnet . id ) ret = defaultdict ( int ) for segment_id , subnet in query . all ( ) : net_size = netaddr . IPNetwork ( subnet . _cidr ) . size ip_policy = subnet [ "ip_policy" ] or { "size" : 0 } ret [ segment_id ] += net_size - ip_policy [ "size" ] for segment_id in used_ips_counts : ret [ segment_id ] -= used_ips_counts [ segment_id ] return ret
881	def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )
5883	def post_cleanup ( self ) : parse_tags = [ 'p' ] if self . config . parse_lists : parse_tags . extend ( [ 'ul' , 'ol' ] ) if self . config . parse_headers : parse_tags . extend ( [ 'h1' , 'h2' , 'h3' , 'h4' , 'h5' , 'h6' ] ) target_node = self . article . top_node node = self . add_siblings ( target_node ) for elm in self . parser . getChildren ( node ) : e_tag = self . parser . getTag ( elm ) if e_tag not in parse_tags : if ( self . is_highlink_density ( elm ) or self . is_table_and_no_para_exist ( elm ) or not self . is_nodescore_threshold_met ( node , elm ) ) : self . parser . remove ( elm ) return node
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
3141	def get ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id ) )
7341	async def get_size ( media ) : if hasattr ( media , 'seek' ) : await execute ( media . seek ( 0 , os . SEEK_END ) ) size = await execute ( media . tell ( ) ) await execute ( media . seek ( 0 ) ) elif hasattr ( media , 'headers' ) : size = int ( media . headers [ 'Content-Length' ] ) elif isinstance ( media , bytes ) : size = len ( media ) else : raise TypeError ( "Can't get size of media of type:" , type ( media ) . __name__ ) _logger . info ( "media size: %dB" % size ) return size
6920	def _autocorr_func2 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 0 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) autocovarfunc = npsum ( products ) / lagindex . size varfunc = npsum ( ( mags [ lagindex ] - magmed ) * ( mags [ lagindex ] - magmed ) ) / mags . size acorr = autocovarfunc / varfunc return acorr
9056	def sample ( self , random_state = None ) : r from numpy_sugar import epsilon from numpy_sugar . linalg import sum2diag from numpy_sugar . random import multivariate_normal if random_state is None : random_state = RandomState ( ) m = self . _mean . value ( ) K = self . _cov . value ( ) . copy ( ) sum2diag ( K , + epsilon . small , out = K ) return self . _lik . sample ( multivariate_normal ( m , K , random_state ) , random_state )
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
9963	def get_impls ( interfaces ) : if interfaces is None : return None elif isinstance ( interfaces , Mapping ) : return { name : interfaces [ name ] . _impl for name in interfaces } elif isinstance ( interfaces , Sequence ) : return [ interfaces . _impl for interfaces in interfaces ] else : return interfaces . _impl
7257	def get_address_coords ( self , address ) : url = "https://maps.googleapis.com/maps/api/geocode/json?&address=" + address r = requests . get ( url ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] lat = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lat' ] lng = results [ 0 ] [ 'geometry' ] [ 'location' ] [ 'lng' ] return lat , lng
11896	def _create_index_file ( root_dir , location , image_files , dirs , force_no_processing = False ) : header_text = 'imageMe: ' + location + ' [' + str ( len ( image_files ) ) + ' image(s)]' html = [ '<!DOCTYPE html>' , '<html>' , ' <head>' , ' <title>imageMe</title>' ' <style>' , ' html, body {margin: 0;padding: 0;}' , ' .header {text-align: right;}' , ' .content {' , ' padding: 3em;' , ' padding-left: 4em;' , ' padding-right: 4em;' , ' }' , ' .image {max-width: 100%; border-radius: 0.3em;}' , ' td {width: ' + str ( 100.0 / IMAGES_PER_ROW ) + '%;}' , ' </style>' , ' </head>' , ' <body>' , ' <div class="content">' , ' <h2 class="header">' + header_text + '</h2>' ] directories = [ ] if root_dir != location : directories = [ '..' ] directories += dirs if len ( directories ) > 0 : html . append ( '<hr>' ) for directory in directories : link = directory + '/' + INDEX_FILE_NAME html += [ ' <h3 class="header">' , ' <a href="' + link + '">' + directory + '</a>' , ' </h3>' ] table_row_count = 1 html += [ '<hr>' , '<table>' ] for image_file in image_files : if table_row_count == 1 : html . append ( '<tr>' ) img_src = _get_thumbnail_src_from_file ( location , image_file , force_no_processing ) link_target = _get_image_link_target_from_file ( location , image_file , force_no_processing ) html += [ ' <td>' , ' <a href="' + link_target + '">' , ' <img class="image" src="' + img_src + '">' , ' </a>' , ' </td>' ] if table_row_count == IMAGES_PER_ROW : table_row_count = 0 html . append ( '</tr>' ) table_row_count += 1 html += [ '</tr>' , '</table>' ] html += [ ' </div>' , ' </body>' , '</html>' ] index_file_path = _get_index_file_path ( location ) print ( 'Creating index file %s' % index_file_path ) index_file = open ( index_file_path , 'w' ) index_file . write ( '\n' . join ( html ) ) index_file . close ( ) return index_file_path
1402	def setTopologyInfo ( self , topology ) : if not topology . execution_state : Log . info ( "No execution state found for: " + topology . name ) return Log . info ( "Setting topology info for topology: " + topology . name ) has_physical_plan = True if not topology . physical_plan : has_physical_plan = False Log . info ( "Setting topology info for topology: " + topology . name ) has_packing_plan = True if not topology . packing_plan : has_packing_plan = False has_tmaster_location = True if not topology . tmaster : has_tmaster_location = False has_scheduler_location = True if not topology . scheduler_location : has_scheduler_location = False topologyInfo = { "name" : topology . name , "id" : topology . id , "logical_plan" : None , "physical_plan" : None , "packing_plan" : None , "execution_state" : None , "tmaster_location" : None , "scheduler_location" : None , } executionState = self . extract_execution_state ( topology ) executionState [ "has_physical_plan" ] = has_physical_plan executionState [ "has_packing_plan" ] = has_packing_plan executionState [ "has_tmaster_location" ] = has_tmaster_location executionState [ "has_scheduler_location" ] = has_scheduler_location executionState [ "status" ] = topology . get_status ( ) topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) topologyInfo [ "execution_state" ] = executionState topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo
6533	def get_project_config ( project_path , use_cache = True ) : return get_local_config ( project_path , use_cache = use_cache ) or get_user_config ( project_path , use_cache = use_cache ) or get_default_config ( )
10658	def amount_fractions ( masses ) : n = amounts ( masses ) n_total = sum ( n . values ( ) ) return { compound : n [ compound ] / n_total for compound in n . keys ( ) }
3964	def stop_apps_or_services ( app_or_service_names = None , rm_containers = False ) : if app_or_service_names : log_to_client ( "Stopping the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Stopping all running containers associated with Dusty" ) compose . stop_running_services ( app_or_service_names ) if rm_containers : compose . rm_containers ( app_or_service_names )
9177	def _dissect_roles ( metadata ) : for role_key in cnxepub . ATTRIBUTED_ROLE_KEYS : for user in metadata . get ( role_key , [ ] ) : if user [ 'type' ] != 'cnx-id' : raise ValueError ( "Archive only accepts Connexions users." ) uid = parse_user_uri ( user [ 'id' ] ) yield uid , role_key raise StopIteration ( )
8622	def get_user_by_id ( session , user_id , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'users/{}' . format ( user_id ) , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UserNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9393	def check_important_sub_metrics ( self , sub_metric ) : if not self . important_sub_metrics : return False if sub_metric in self . important_sub_metrics : return True items = sub_metric . split ( '.' ) if items [ - 1 ] in self . important_sub_metrics : return True return False
2938	def deserialize_assign_list ( self , workflow , start_node ) : assignments = [ ] for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'assign' : assignments . append ( self . deserialize_assign ( workflow , node ) ) else : _exc ( 'Unknown node: %s' % node . nodeName ) return assignments
4858	def ignore_warning ( warning ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'ignore' , warning ) return func ( * args , ** kwargs ) return wrapper return decorator
11274	def check_pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
819	def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None
6710	def shell ( self , gui = 0 , command = '' , dryrun = None , shell_interactive_cmd_str = None ) : from burlap . common import get_hosts_for_site if dryrun is not None : self . dryrun = dryrun r = self . local_renderer if r . genv . SITE != r . genv . default_site : shell_hosts = get_hosts_for_site ( ) if shell_hosts : r . genv . host_string = shell_hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default_site if int ( gui ) : r . env . shell_default_options . append ( '-X' ) if 'host_string' not in self . genv or not self . genv . host_string : if 'available_sites' in self . genv and r . env . SITE not in r . genv . available_sites : raise Exception ( 'No host_string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host_string set.' ) if '@' in r . genv . host_string : r . env . shell_host_string = r . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' if command : r . env . shell_interactive_cmd_str = command else : r . env . shell_interactive_cmd_str = r . format ( shell_interactive_cmd_str or r . env . shell_interactive_cmd ) r . env . shell_default_options_str = ' ' . join ( r . env . shell_default_options ) if self . is_local : self . vprint ( 'Using direct local.' ) cmd = '{shell_interactive_cmd_str}' elif r . genv . key_filename : self . vprint ( 'Using key filename.' ) port = r . env . shell_host_string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell_host_string = r . env . shell_host_string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell_default_options_str} -i {key_filename} {shell_host_string} "{shell_interactive_cmd_str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' else : self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' r . local ( cmd )
9612	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } data . setdefault ( 'element_id' , self . element_id ) return self . _driver . _execute ( command , data , unpack )
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
11780	def ContinuousXor ( n ) : "2 inputs are chosen uniformly from (0.0 .. 2.0]; output is xor of ints." examples = [ ] for i in range ( n ) : x , y = [ random . uniform ( 0.0 , 2.0 ) for i in '12' ] examples . append ( [ x , y , int ( x ) != int ( y ) ] ) return DataSet ( name = "continuous xor" , examples = examples )
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
13420	def validate ( cls , definition ) : schema_path = os . path . join ( os . path . dirname ( __file__ ) , '../../schema/mapper_definition_schema.json' ) with open ( schema_path , 'r' ) as jsonfp : schema = json . load ( jsonfp ) jsonschema . validate ( definition , schema ) assert definition [ 'main_key' ] in definition [ 'supported_keys' ] , '\'main_key\' must be contained in \'supported_keys\'' assert set ( definition . get ( 'list_valued_keys' , [ ] ) ) <= set ( definition [ 'supported_keys' ] ) , '\'list_valued_keys\' must be a subset of \'supported_keys\'' assert set ( definition . get ( 'disjoint' , [ ] ) ) <= set ( definition . get ( 'list_valued_keys' , [ ] ) ) , '\'disjoint\' must be a subset of \'list_valued_keys\'' assert set ( definition . get ( 'key_synonyms' , { } ) . values ( ) ) <= set ( definition [ 'supported_keys' ] ) , '\'The values of the \'key_synonyms\' mapping must be in \'supported_keys\''
8782	def select_ipam_strategy ( self , network_id , network_strategy , ** kwargs ) : LOG . info ( "Selecting IPAM strategy for network_id:%s " "network_strategy:%s" % ( network_id , network_strategy ) ) net_type = "tenant" if STRATEGY . is_provider_network ( network_id ) : net_type = "provider" strategy = self . _ipam_strategies . get ( net_type , { } ) default = strategy . get ( "default" ) overrides = strategy . get ( "overrides" , { } ) if network_strategy in overrides : LOG . info ( "Selected overridden IPAM strategy: %s" % ( overrides [ network_strategy ] ) ) return overrides [ network_strategy ] if default : LOG . info ( "Selected default IPAM strategy for tenant " "network: %s" % ( default ) ) return default LOG . info ( "Selected network strategy for tenant " "network: %s" % ( network_strategy ) ) return network_strategy
5543	def clip ( self , array , geometries , inverted = False , clip_buffer = 0 ) : return commons_clip . clip_array_with_vector ( array , self . tile . affine , geometries , inverted = inverted , clip_buffer = clip_buffer * self . tile . pixel_x_size )
2344	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{CUTOFF}' ] = str ( self . cutoff ) self . arguments [ '{VARSEL}' ] = str ( self . variablesel ) . upper ( ) self . arguments [ '{SELMETHOD}' ] = self . var_selection [ self . selmethod ] self . arguments [ '{PRUNING}' ] = str ( self . pruning ) . upper ( ) self . arguments [ '{PRUNMETHOD}' ] = self . var_selection [ self . prunmethod ] self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_cam ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
7670	def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise ParameterError ( "on_conflict='{}' is not in ['fail', " "'overwrite', 'ignore']." . format ( on_conflict ) ) if not self . file_metadata == jam . file_metadata : if on_conflict == 'overwrite' : self . file_metadata = jam . file_metadata elif on_conflict == 'fail' : raise JamsError ( "Metadata conflict! " "Resolve manually or force-overwrite it." ) self . annotations . extend ( jam . annotations ) self . sandbox . update ( ** jam . sandbox )
10735	def ld_to_dl ( ld ) : if ld : keys = list ( ld [ 0 ] ) dl = { key : [ d [ key ] for d in ld ] for key in keys } return dl else : return { }
12410	def all_package_versions ( package ) : info = PyPI . package_info ( package ) return info and sorted ( info [ 'releases' ] . keys ( ) , key = lambda x : x . split ( ) , reverse = True ) or [ ]
3497	def total_components_flux ( flux , components , consumption = True ) : direction = 1 if consumption else - 1 c_flux = [ elem * flux * direction for elem in components ] return sum ( [ flux for flux in c_flux if flux > 0 ] )
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
11044	def create_marathon_acme ( client_creator , cert_store , acme_email , allow_multiple_certs , marathon_addrs , marathon_timeout , sse_timeout , mlb_addrs , group , reactor ) : marathon_client = MarathonClient ( marathon_addrs , timeout = marathon_timeout , sse_kwargs = { 'timeout' : sse_timeout } , reactor = reactor ) marathon_lb_client = MarathonLbClient ( mlb_addrs , reactor = reactor ) return MarathonAcme ( marathon_client , group , cert_store , marathon_lb_client , client_creator , reactor , acme_email , allow_multiple_certs )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
5593	def tiles_from_geom ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_geom ( geometry , zoom ) : yield self . tile ( * tile . id )
7663	def pop_data ( self ) : data = self . data self . data = SortedKeyList ( key = self . _key ) return data
7047	def _parallel_bls_worker ( task ) : try : return _bls_runner ( * task ) except Exception as e : LOGEXCEPTION ( 'BLS failed for task %s' % repr ( task [ 2 : ] ) ) return { 'power' : nparray ( [ npnan for x in range ( task [ 2 ] ) ] ) , 'bestperiod' : npnan , 'bestpower' : npnan , 'transdepth' : npnan , 'transduration' : npnan , 'transingressbin' : npnan , 'transegressbin' : npnan }
2678	def get_role_name ( region , account_id , role ) : prefix = ARN_PREFIXES . get ( region , 'aws' ) return 'arn:{0}:iam::{1}:role/{2}' . format ( prefix , account_id , role )
10810	def get_by_name ( cls , name ) : try : return cls . query . filter_by ( name = name ) . one ( ) except NoResultFound : return None
1842	def JNP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . PF , target . read ( ) , cpu . PC )
7639	def get_comments ( jam , ann ) : jam_comments = jam . file_metadata . __json__ ann_comments = ann . annotation_metadata . __json__ return json . dumps ( { 'metadata' : jam_comments , 'annotation metadata' : ann_comments } , indent = 2 )
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
6380	def sim_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . sim ( src , tar , qval , alphabet )
1396	def removeTopology ( self , topology_name , state_manager_name ) : topologies = [ ] for top in self . topologies : if ( top . name == topology_name and top . state_manager_name == state_manager_name ) : if ( topology_name , state_manager_name ) in self . topologyInfos : self . topologyInfos . pop ( ( topology_name , state_manager_name ) ) else : topologies . append ( top ) self . topologies = topologies
11159	def execute_pyfile ( self , py_exe = None ) : import subprocess self . assert_is_dir_and_exists ( ) if py_exe is None : if six . PY2 : py_exe = "python2" elif six . PY3 : py_exe = "python3" for p in self . select_by_ext ( ".py" ) : subprocess . Popen ( '%s "%s"' % ( py_exe , p . abspath ) )
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
8877	def allele_expectation ( bgen , variant_idx ) : r geno = bgen [ "genotype" ] [ variant_idx ] . compute ( ) if geno [ "phased" ] : raise ValueError ( "Allele expectation is define for unphased genotypes only." ) nalleles = bgen [ "variants" ] . loc [ variant_idx , "nalleles" ] . compute ( ) . item ( ) genotypes = get_genotypes ( geno [ "ploidy" ] , nalleles ) expec = [ ] for i in range ( len ( genotypes ) ) : count = asarray ( genotypes_to_allele_counts ( genotypes [ i ] ) , float ) n = count . shape [ 0 ] expec . append ( ( count . T * geno [ "probs" ] [ i , : n ] ) . sum ( 1 ) ) return stack ( expec , axis = 0 )
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
2391	def regenerate_good_tokens ( string ) : toks = nltk . word_tokenize ( string ) pos_string = nltk . pos_tag ( toks ) pos_seq = [ tag [ 1 ] for tag in pos_string ] pos_ngrams = ngrams ( pos_seq , 2 , 4 ) sel_pos_ngrams = f7 ( pos_ngrams ) return sel_pos_ngrams
2679	def get_account_id ( profile_name , aws_access_key_id , aws_secret_access_key , region = None , ) : client = get_client ( 'sts' , profile_name , aws_access_key_id , aws_secret_access_key , region , ) return client . get_caller_identity ( ) . get ( 'Account' )
9339	def loadtxt2 ( fname , dtype = None , delimiter = ' ' , newline = '\n' , comment_character = '#' , skiplines = 0 ) : dtypert = [ None , None , None ] def preparedtype ( dtype ) : dtypert [ 0 ] = dtype flatten = flatten_dtype ( dtype ) dtypert [ 1 ] = flatten dtypert [ 2 ] = numpy . dtype ( [ ( 'a' , ( numpy . int8 , flatten . itemsize ) ) ] ) buf = numpy . empty ( ( ) , dtype = dtypert [ 1 ] ) converters = [ _default_conv [ flatten [ name ] . char ] for name in flatten . names ] return buf , converters , flatten . names def fileiter ( fh ) : converters = [ ] buf = None if dtype is not None : buf , converters , names = preparedtype ( dtype ) yield None for lineno , line in enumerate ( fh ) : if lineno < skiplines : continue if line [ 0 ] in comment_character : if buf is None and line [ 1 ] == '?' : ddtype = pickle . loads ( base64 . b64decode ( line [ 2 : ] ) ) buf , converters , names = preparedtype ( ddtype ) yield None continue for word , c , name in zip ( line . split ( ) , converters , names ) : buf [ name ] = c ( word ) buf2 = buf . copy ( ) . view ( dtype = dtypert [ 2 ] ) yield buf2 if isinstance ( fname , basestring ) : fh = file ( fh , 'r' ) cleanup = lambda : fh . close ( ) else : fh = iter ( fname ) cleanup = lambda : None try : i = fileiter ( fh ) i . next ( ) return numpy . fromiter ( i , dtype = dtypert [ 2 ] ) . view ( dtype = dtypert [ 0 ] ) finally : cleanup ( )
11286	def flush ( self , line ) : sys . stdout . write ( line ) sys . stdout . flush ( )
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
7764	def connect ( self ) : with self . lock : if self . stream : logger . debug ( "Closing the previously used stream." ) self . _close_stream ( ) transport = TCPTransport ( self . settings ) addr = self . settings [ "server" ] if addr : service = None else : addr = self . jid . domain service = self . settings [ "c2s_service" ] transport . connect ( addr , self . settings [ "c2s_port" ] , service ) handlers = self . _base_handlers [ : ] handlers += self . handlers + [ self ] self . clear_response_handlers ( ) self . setup_stanza_handlers ( handlers , "pre-auth" ) stream = ClientStream ( self . jid , self , handlers , self . settings ) stream . initiate ( transport ) self . main_loop . add_handler ( transport ) self . main_loop . add_handler ( stream ) self . _ml_handlers += [ transport , stream ] self . stream = stream self . uplink = stream
2635	def parent_callback ( self , executor_fu ) : with self . _update_lock : if not executor_fu . done ( ) : raise ValueError ( "done callback called, despite future not reporting itself as done" ) if executor_fu != self . parent : if executor_fu . exception ( ) is None and not isinstance ( executor_fu . result ( ) , RemoteExceptionWrapper ) : raise ValueError ( "internal consistency error: AppFuture done callback called without an exception, but parent has been changed since then" ) try : res = executor_fu . result ( ) if isinstance ( res , RemoteExceptionWrapper ) : res . reraise ( ) super ( ) . set_result ( executor_fu . result ( ) ) except Exception as e : if executor_fu . retries_left > 0 : pass else : super ( ) . set_exception ( e )
1774	def invalidate_cache ( cpu , address , size ) : cache = cpu . instruction_cache for offset in range ( size ) : if address + offset in cache : del cache [ address + offset ]
85	def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
9028	def instructions ( self ) : x = self . x y = self . y result = [ ] for instruction in self . _row . instructions : instruction_in_grid = InstructionInGrid ( instruction , Point ( x , y ) ) x += instruction_in_grid . width result . append ( instruction_in_grid ) return result
6479	def _normalised_python ( self ) : dx = ( self . screen . width / float ( len ( self . points ) ) ) oy = ( self . screen . height ) for x , point in enumerate ( self . points ) : y = ( point - self . minimum ) * 4.0 / self . extents * self . size . y yield Point ( ( dx * x , min ( oy , oy - y ) , ) )
6941	def _double_inverted_gaussian ( x , amp1 , loc1 , std1 , amp2 , loc2 , std2 ) : gaussian1 = - _gaussian ( x , amp1 , loc1 , std1 ) gaussian2 = - _gaussian ( x , amp2 , loc2 , std2 ) return gaussian1 + gaussian2
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
10369	def summarize_edge_filter ( graph : BELGraph , edge_predicates : EdgePredicates ) -> None : passed = count_passed_edge_filter ( graph , edge_predicates ) print ( '{}/{} edges passed {}' . format ( passed , graph . number_of_edges ( ) , ( ', ' . join ( edge_filter . __name__ for edge_filter in edge_predicates ) if isinstance ( edge_predicates , Iterable ) else edge_predicates . __name__ ) ) )
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
4513	def drawCircle ( self , x0 , y0 , r , color = None ) : md . draw_circle ( self . set , x0 , y0 , r , color )
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
6550	def from_func ( cls , func , variables , vartype , name = None ) : variables = tuple ( variables ) configurations = frozenset ( config for config in itertools . product ( vartype . value , repeat = len ( variables ) ) if func ( * config ) ) return cls ( func , configurations , variables , vartype , name )
4244	def _get_record ( self , ipnum ) : seek_country = self . _seek_country ( ipnum ) if seek_country == self . _databaseSegments : return { } read_length = ( 2 * self . _recordLength - 1 ) * self . _databaseSegments try : self . _lock . acquire ( ) self . _fp . seek ( seek_country + read_length , os . SEEK_SET ) buf = self . _fp . read ( const . FULL_RECORD_LENGTH ) finally : self . _lock . release ( ) if PY3 and type ( buf ) is bytes : buf = buf . decode ( ENCODING ) record = { 'dma_code' : 0 , 'area_code' : 0 , 'metro_code' : None , 'postal_code' : None } latitude = 0 longitude = 0 char = ord ( buf [ 0 ] ) record [ 'country_code' ] = const . COUNTRY_CODES [ char ] record [ 'country_code3' ] = const . COUNTRY_CODES3 [ char ] record [ 'country_name' ] = const . COUNTRY_NAMES [ char ] record [ 'continent' ] = const . CONTINENT_NAMES [ char ] def read_data ( buf , pos ) : cur = pos while buf [ cur ] != '\0' : cur += 1 return cur , buf [ pos : cur ] if cur > pos else None offset , record [ 'region_code' ] = read_data ( buf , 1 ) offset , record [ 'city' ] = read_data ( buf , offset + 1 ) offset , record [ 'postal_code' ] = read_data ( buf , offset + 1 ) offset = offset + 1 for j in range ( 3 ) : latitude += ( ord ( buf [ offset + j ] ) << ( j * 8 ) ) for j in range ( 3 ) : longitude += ( ord ( buf [ offset + j + 3 ] ) << ( j * 8 ) ) record [ 'latitude' ] = ( latitude / 10000.0 ) - 180.0 record [ 'longitude' ] = ( longitude / 10000.0 ) - 180.0 if self . _databaseType in ( const . CITY_EDITION_REV1 , const . CITY_EDITION_REV1_V6 ) : if record [ 'country_code' ] == 'US' : dma_area = 0 for j in range ( 3 ) : dma_area += ord ( buf [ offset + j + 6 ] ) << ( j * 8 ) record [ 'dma_code' ] = int ( floor ( dma_area / 1000 ) ) record [ 'area_code' ] = dma_area % 1000 record [ 'metro_code' ] = const . DMA_MAP . get ( record [ 'dma_code' ] ) params = ( record [ 'country_code' ] , record [ 'region_code' ] ) record [ 'time_zone' ] = time_zone_by_country_and_region ( * params ) return record
10577	def get_assay ( self ) : masses_sum = sum ( self . compound_masses ) return [ m / masses_sum for m in self . compound_masses ]
11921	def paginator ( self ) : if not hasattr ( self , '_paginator' ) : if self . pagination_class is None : self . _paginator = None else : self . _paginator = self . pagination_class ( ) return self . _paginator
1621	def FindNextMultiLineCommentEnd ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : return lineix lineix += 1 return len ( lines )
6785	def fake ( self , components = None ) : self . init ( ) if components : current_tp = self . get_previous_thumbprint ( ) or { } current_tp . update ( self . get_current_thumbprint ( components = components ) or { } ) else : current_tp = self . get_current_thumbprint ( components = components ) or { } tp_text = yaml . dump ( current_tp ) r = self . local_renderer r . upload_content ( content = tp_text , fn = self . manifest_filename ) self . reset_all_satchels ( )
6987	def _varfeatures_worker ( task ) : try : ( lcfile , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) = task return get_varfeatures ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir ) except Exception as e : return None
12209	def get_cache_key ( user_or_username , size , prefix ) : if isinstance ( user_or_username , get_user_model ( ) ) : user_or_username = user_or_username . username return '%s_%s_%s' % ( prefix , user_or_username , size )
5839	def submit_predict_request ( self , data_view_id , candidates , prediction_source = 'scalar' , use_prior = True ) : data = { "prediction_source" : prediction_source , "use_prior" : use_prior , "candidates" : candidates } failure_message = "Configuration creation failed" post_url = 'v1/data_views/' + str ( data_view_id ) + '/predict/submit' return self . _get_success_json ( self . _post_json ( post_url , data , failure_message = failure_message ) ) [ 'data' ] [ 'uid' ]
11040	def write ( self , path , ** data ) : d = self . request ( 'PUT' , '/v1/' + path , json = data ) return d . addCallback ( self . _handle_response , check_cas = True )
13447	def authorize ( self ) : response = self . client . login ( username = self . USERNAME , password = self . PASSWORD ) self . assertTrue ( response ) self . authed = True
8886	def finalize ( self ) : if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. finalize unusable' ) elif not self . __head_generate : warn ( f'{self.__class__.__name__} already finalized or fitted' ) elif not self . __head_dict : raise NotFittedError ( f'{self.__class__.__name__} instance is not fitted yet' ) else : if self . remove_rare_ratio : self . __clean_head ( * self . __head_rare ) self . __prepare_header ( ) self . __head_rare = None self . __head_generate = False
5948	def remove_legend ( ax = None ) : from pylab import gca , draw if ax is None : ax = gca ( ) ax . legend_ = None draw ( )
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
6149	def firwin_bpf ( N_taps , f1 , f2 , fs = 1.0 , pass_zero = False ) : return signal . firwin ( N_taps , 2 * ( f1 , f2 ) / fs , pass_zero = pass_zero )
1749	def mmap ( self , addr , size , perms , data_init = None , name = None ) : assert addr is None or isinstance ( addr , int ) , 'Address shall be concrete' self . cpu . _publish ( 'will_map_memory' , addr , size , perms , None , None ) if addr is not None : assert addr < self . memory_size , 'Address too big' addr = self . _floor ( addr ) size = self . _ceil ( size ) addr = self . _search ( size , addr ) for i in range ( self . _page ( addr ) , self . _page ( addr + size ) ) : assert i not in self . _page2map , 'Map already used' m = AnonMap ( start = addr , size = size , perms = perms , data_init = data_init , name = name ) self . _add ( m ) logger . debug ( 'New memory map @%x size:%x' , addr , size ) self . cpu . _publish ( 'did_map_memory' , addr , size , perms , None , None , addr ) return addr
12329	def compute_sha256 ( filename ) : try : h = sha256 ( ) fd = open ( filename , 'rb' ) while True : buf = fd . read ( 0x1000000 ) if buf in [ None , "" ] : break h . update ( buf . encode ( 'utf-8' ) ) fd . close ( ) return h . hexdigest ( ) except : output = run ( [ "sha256sum" , "-b" , filename ] ) return output . split ( " " ) [ 0 ]
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
748	def _anomalyCompute ( self ) : inferenceType = self . getInferenceType ( ) inferences = { } sp = self . _getSPRegion ( ) score = None if inferenceType == InferenceType . NontemporalAnomaly : score = sp . getOutputData ( "anomalyScore" ) [ 0 ] elif inferenceType == InferenceType . TemporalAnomaly : tm = self . _getTPRegion ( ) if sp is not None : activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] else : sensor = self . _getSensorRegion ( ) activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] if not self . _predictedFieldName in self . _input : raise ValueError ( "Expected predicted field '%s' in input row, but was not found!" % self . _predictedFieldName ) score = tm . getOutputData ( "anomalyScore" ) [ 0 ] if sp is not None : self . _getAnomalyClassifier ( ) . setParameter ( "activeColumnCount" , len ( activeColumns ) ) self . _getAnomalyClassifier ( ) . prepareInputs ( ) self . _getAnomalyClassifier ( ) . compute ( ) labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) inferences [ InferenceElement . anomalyLabel ] = "%s" % labels inferences [ InferenceElement . anomalyScore ] = score return inferences
6159	def IIR_sos_header ( fname_out , SOS_mat ) : Ns , Mcol = SOS_mat . shape f = open ( fname_out , 'wt' ) f . write ( '//define a IIR SOS CMSIS-DSP coefficient array\n\n' ) f . write ( '#include <stdint.h>\n\n' ) f . write ( '#ifndef STAGES\n' ) f . write ( '#define STAGES %d\n' % Ns ) f . write ( '#endif\n' ) f . write ( '/*********************************************************/\n' ) f . write ( '/* IIR SOS Filter Coefficients */\n' ) f . write ( 'float32_t ba_coeff[%d] = { //b0,b1,b2,a1,a2,... by stage\n' % ( 5 * Ns ) ) for k in range ( Ns ) : if ( k < Ns - 1 ) : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e,\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) else : f . write ( ' %+-13e, %+-13e, %+-13e,\n' % ( SOS_mat [ k , 0 ] , SOS_mat [ k , 1 ] , SOS_mat [ k , 2 ] ) ) f . write ( ' %+-13e, %+-13e\n' % ( - SOS_mat [ k , 4 ] , - SOS_mat [ k , 5 ] ) ) f . write ( '};\n' ) f . write ( '/*********************************************************/\n' ) f . close ( )
4923	def courses ( self , request , enterprise_customer , pk = None ) : catalog_api = CourseCatalogApiClient ( request . user , enterprise_customer . site ) courses = catalog_api . get_paginated_catalog_courses ( pk , request . GET ) self . ensure_data_exists ( request , courses , error_message = ( "Unable to fetch API response for catalog courses from endpoint '{endpoint}'. " "The resource you are looking for does not exist." . format ( endpoint = request . get_full_path ( ) ) ) ) serializer = serializers . EnterpriseCatalogCoursesReadOnlySerializer ( courses ) serializer . update_enterprise_courses ( enterprise_customer , catalog_id = pk ) return get_paginated_response ( serializer . data , request )
12332	def get_diffs ( history ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] for c in curr [ 'changes' ] : path = c [ 'path' ] if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : shutil . rmtree ( temp1 ) continue diff = handler . get_diff ( path1 , path2 ) c [ 'diff' ] = diff except Exception as e : shutil . rmtree ( temp1 )
9443	def call ( self , call_params ) : path = '/' + self . api_version + '/Call/' method = 'POST' return self . request ( path , method , call_params )
8911	def ows_security_tween_factory ( handler , registry ) : security = owssecurity_factory ( registry ) def ows_security_tween ( request ) : try : security . check_request ( request ) return handler ( request ) except OWSException as err : logger . exception ( "security check failed." ) return err except Exception as err : logger . exception ( "unknown error" ) return OWSNoApplicableCode ( "{}" . format ( err ) ) return ows_security_tween
6488	def _get_filter_field ( field_name , field_value ) : filter_field = None if isinstance ( field_value , ValueRange ) : range_values = { } if field_value . lower : range_values . update ( { "gte" : field_value . lower_string } ) if field_value . upper : range_values . update ( { "lte" : field_value . upper_string } ) filter_field = { "range" : { field_name : range_values } } elif _is_iterable ( field_value ) : filter_field = { "terms" : { field_name : field_value } } else : filter_field = { "term" : { field_name : field_value } } return filter_field
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
9275	def apply_exclude_tags_regex ( self , all_tags ) : filtered = [ ] for tag in all_tags : if not re . match ( self . options . exclude_tags_regex , tag [ "name" ] ) : filtered . append ( tag ) if len ( all_tags ) == len ( filtered ) : self . warn_if_nonmatching_regex ( ) return filtered
11442	def _warning ( code ) : if isinstance ( code , str ) : return code message = '' if isinstance ( code , tuple ) : if isinstance ( code [ 0 ] , str ) : message = code [ 1 ] code = code [ 0 ] return CFG_BIBRECORD_WARNING_MSGS . get ( code , '' ) + message
8650	def post_review ( session , review ) : response = make_post_request ( session , 'reviews' , json_data = review ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise ReviewNotPostedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3366	def _process_flux_dataframe ( flux_dataframe , fva , threshold , floatfmt ) : abs_flux = flux_dataframe [ 'flux' ] . abs ( ) flux_threshold = threshold * abs_flux . max ( ) if fva is None : flux_dataframe = flux_dataframe . loc [ abs_flux >= flux_threshold , : ] . copy ( ) else : flux_dataframe = flux_dataframe . loc [ ( abs_flux >= flux_threshold ) | ( flux_dataframe [ 'fmin' ] . abs ( ) >= flux_threshold ) | ( flux_dataframe [ 'fmax' ] . abs ( ) >= flux_threshold ) , : ] . copy ( ) if fva is None : flux_dataframe [ 'is_input' ] = ( flux_dataframe [ 'flux' ] >= 0 ) flux_dataframe [ 'flux' ] = flux_dataframe [ 'flux' ] . abs ( ) else : def get_direction ( flux , fmin , fmax ) : if flux < 0 : return - 1 elif flux > 0 : return 1 elif ( fmax > 0 ) & ( fmin <= 0 ) : return 1 elif ( fmax < 0 ) & ( fmin >= 0 ) : return - 1 elif ( ( fmax + fmin ) / 2 ) < 0 : return - 1 else : return 1 sign = flux_dataframe . apply ( lambda x : get_direction ( x . flux , x . fmin , x . fmax ) , 1 ) flux_dataframe [ 'is_input' ] = sign == 1 flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . multiply ( sign , 0 ) . astype ( 'float' ) . round ( 6 ) flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . applymap ( lambda x : x if abs ( x ) > 1E-6 else 0 ) if fva is not None : flux_dataframe [ 'fva_fmt' ] = flux_dataframe . apply ( lambda x : ( "[{0.fmin:" + floatfmt + "}, {0.fmax:" + floatfmt + "}]" ) . format ( x ) , 1 ) flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'fmax' , 'fmin' , 'id' ] , ascending = [ False , False , False , True ] ) else : flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'id' ] , ascending = [ False , True ] ) return flux_dataframe
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
5498	def is_cached ( self , url ) : try : return True if url in self . cache else False except TypeError : return False
2259	def dzip ( items1 , items2 , cls = dict ) : try : len ( items1 ) except TypeError : items1 = list ( items1 ) try : len ( items2 ) except TypeError : items2 = list ( items2 ) if len ( items1 ) == 0 and len ( items2 ) == 1 : items2 = [ ] if len ( items2 ) == 1 and len ( items1 ) > 1 : items2 = items2 * len ( items1 ) if len ( items1 ) != len ( items2 ) : raise ValueError ( 'out of alignment len(items1)=%r, len(items2)=%r' % ( len ( items1 ) , len ( items2 ) ) ) return cls ( zip ( items1 , items2 ) )
3372	def choose_solver ( model , solver = None , qp = False ) : if solver is None : solver = model . problem else : model . solver = solver if qp and interface_to_str ( solver ) not in qp_solvers : solver = solvers [ get_solver_name ( qp = True ) ] return solver
10639	def extract ( self , other ) : if type ( other ) is float or type ( other ) is numpy . float64 or type ( other ) is numpy . float32 : return self . _extract_mfr ( other ) elif self . _is_compound_mfr_tuple ( other ) : return self . _extract_compound_mfr ( other [ 0 ] , other [ 1 ] ) elif type ( other ) is str : return self . _extract_compound ( other ) elif type ( other ) is Material : return self . _extract_material ( other ) else : raise TypeError ( "Invalid extraction argument." )
7261	def search ( self , searchAreaWkt = None , filters = None , startDate = None , endDate = None , types = None ) : if not types : types = [ 'Acquisition' ] if startDate : startDateTime = datetime . datetime . strptime ( startDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if endDate : endDateTime = datetime . datetime . strptime ( endDate , '%Y-%m-%dT%H:%M:%S.%fZ' ) if startDate and endDate : diff = endDateTime - startDateTime if diff . days < 0 : raise Exception ( "startDate must come before endDate." ) postdata = { "searchAreaWkt" : searchAreaWkt , "types" : types , "startDate" : startDate , "endDate" : endDate , } if filters : postdata [ 'filters' ] = filters if searchAreaWkt : postdata [ 'searchAreaWkt' ] = searchAreaWkt url = '%(base_url)s/search' % { 'base_url' : self . base_url } headers = { 'Content-Type' : 'application/json' } r = self . gbdx_connection . post ( url , headers = headers , data = json . dumps ( postdata ) ) r . raise_for_status ( ) results = r . json ( ) [ 'results' ] return results
2188	def _get_certificate ( self , cfgstr = None ) : certificate = self . cacher . tryload ( cfgstr = cfgstr ) return certificate
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
12363	def get ( self , id , ** kwargs ) : return ( super ( MutableCollection , self ) . get ( ( id , ) , ** kwargs ) . get ( self . singular , None ) )
7391	def draw ( self ) : self . ax . set_xlim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . ax . set_ylim ( - self . plot_radius ( ) , self . plot_radius ( ) ) self . add_axes_and_nodes ( ) self . add_edges ( ) self . ax . axis ( 'off' )
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
2426	def set_doc_spdx_id ( self , doc , doc_spdx_id_line ) : if not self . doc_spdx_id_set : if doc_spdx_id_line == 'SPDXRef-DOCUMENT' : doc . spdx_id = doc_spdx_id_line self . doc_spdx_id_set = True return True else : raise SPDXValueError ( 'Document::SPDXID' ) else : raise CardinalityError ( 'Document::SPDXID' )
662	def checkMatch ( input , prediction , sparse = True , verbosity = 0 ) : if sparse : activeElementsInInput = set ( input ) activeElementsInPrediction = set ( prediction ) else : activeElementsInInput = set ( input . nonzero ( ) [ 0 ] ) activeElementsInPrediction = set ( prediction . nonzero ( ) [ 0 ] ) totalActiveInPrediction = len ( activeElementsInPrediction ) totalActiveInInput = len ( activeElementsInInput ) foundInInput = len ( activeElementsInPrediction . intersection ( activeElementsInInput ) ) missingFromInput = len ( activeElementsInPrediction . difference ( activeElementsInInput ) ) missingFromPrediction = len ( activeElementsInInput . difference ( activeElementsInPrediction ) ) if verbosity >= 1 : print "preds. found in input:" , foundInInput , "out of" , totalActiveInPrediction , print "; preds. missing from input:" , missingFromInput , "out of" , totalActiveInPrediction , print "; unexpected active in input:" , missingFromPrediction , "out of" , totalActiveInInput return ( foundInInput , totalActiveInInput , missingFromInput , totalActiveInPrediction )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
8925	def get_egg_info ( cfg , verbose = False ) : result = Bunch ( ) setup_py = cfg . rootjoin ( 'setup.py' ) if not os . path . exists ( setup_py ) : return result egg_info = shell . capture ( "python {} egg_info" . format ( setup_py ) , echo = True if verbose else None ) for info_line in egg_info . splitlines ( ) : if info_line . endswith ( 'PKG-INFO' ) : pkg_info_file = info_line . split ( None , 1 ) [ 1 ] result [ '__file__' ] = pkg_info_file with io . open ( pkg_info_file , encoding = 'utf-8' ) as handle : lastkey = None for line in handle : if line . lstrip ( ) != line : assert lastkey , "Bad continuation in PKG-INFO file '{}': {}" . format ( pkg_info_file , line ) result [ lastkey ] += '\n' + line else : lastkey , value = line . split ( ':' , 1 ) lastkey = lastkey . strip ( ) . lower ( ) . replace ( '-' , '_' ) value = value . strip ( ) if lastkey in result : try : result [ lastkey ] . append ( value ) except AttributeError : result [ lastkey ] = [ result [ lastkey ] , value ] else : result [ lastkey ] = value for multikey in PKG_INFO_MULTIKEYS : if not isinstance ( result . get ( multikey , [ ] ) , list ) : result [ multikey ] = [ result [ multikey ] ] return result
8284	def _segment_lengths ( self , relative = False , n = 20 ) : lengths = [ ] first = True for el in self . _get_elements ( ) : if first is True : close_x , close_y = el . x , el . y first = False elif el . cmd == MOVETO : close_x , close_y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . _linelength ( x0 , y0 , close_x , close_y ) ) elif el . cmd == LINETO : lengths . append ( self . _linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y lengths . append ( self . _curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : return map ( lambda l : l / length , lengths ) except ZeroDivisionError : return [ 0.0 ] * len ( lengths ) else : return lengths
8588	def start_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/start' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
1977	def sys_deallocate ( self , cpu , addr , size ) : logger . info ( "DEALLOCATE(0x%08x, %d)" % ( addr , size ) ) if addr & 0xfff != 0 : logger . info ( "DEALLOCATE: addr is not page aligned" ) return Decree . CGC_EINVAL if size == 0 : logger . info ( "DEALLOCATE:length is zero" ) return Decree . CGC_EINVAL cpu . memory . munmap ( addr , size ) self . syscall_trace . append ( ( "_deallocate" , - 1 , size ) ) return 0
7211	def stderr ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stderr.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stderr." ) wf = self . workflow . get ( self . id ) stderr_list = [ ] for task in wf [ 'tasks' ] : stderr_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stderr' : self . workflow . get_stderr ( self . id , task [ 'id' ] ) } ) return stderr_list
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
6055	def sparse_grid_from_unmasked_sparse_grid ( unmasked_sparse_grid , sparse_to_unmasked_sparse ) : total_pix_pixels = sparse_to_unmasked_sparse . shape [ 0 ] pix_grid = np . zeros ( ( total_pix_pixels , 2 ) ) for pixel_index in range ( total_pix_pixels ) : pix_grid [ pixel_index , 0 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 0 ] pix_grid [ pixel_index , 1 ] = unmasked_sparse_grid [ sparse_to_unmasked_sparse [ pixel_index ] , 1 ] return pix_grid
1982	def sync ( f ) : def new_function ( self , * args , ** kw ) : self . _lock . acquire ( ) try : return f ( self , * args , ** kw ) finally : self . _lock . release ( ) return new_function
11549	def main ( ) : usage = "Usage: %prog PATH_TO_PACKAGE" parser = optparse . OptionParser ( usage = usage ) parser . add_option ( "-v" , "--verbose" , action = "store_true" , dest = "verbose" , default = False , help = "Show debug output" ) parser . add_option ( "-d" , "--output-dir" , action = "store" , type = "string" , dest = "output_dir" , default = '' , help = "" ) parser . add_option ( "-t" , "--test-args" , action = "store" , type = "string" , dest = "test_args" , default = '' , help = ( "Pass argument on to bin/test. Quote the argument, " + "for instance \"-t '-m somemodule'\"." ) ) ( options , args ) = parser . parse_args ( ) if options . verbose : log_level = logging . DEBUG else : log_level = logging . INFO logging . basicConfig ( level = log_level , format = "%(levelname)s: %(message)s" ) curdir = os . getcwd ( ) testbinary = os . path . join ( curdir , 'bin' , 'test' ) if not os . path . exists ( testbinary ) : raise RuntimeError ( "Test command doesn't exist: %s" % testbinary ) coveragebinary = os . path . join ( curdir , 'bin' , 'coverage' ) if not os . path . exists ( coveragebinary ) : logger . debug ( "Trying globally installed coverage command." ) coveragebinary = 'coverage' logger . info ( "Running tests in coverage mode (can take a long time)" ) parts = [ coveragebinary , 'run' , testbinary ] if options . test_args : parts . append ( options . test_args ) system ( " " . join ( parts ) ) logger . debug ( "Creating coverage reports..." ) if options . output_dir : coverage_dir = options . output_dir open_in_browser = False else : coverage_dir = 'htmlcov' open_in_browser = True system ( "%s html --directory=%s" % ( coveragebinary , coverage_dir ) ) logger . info ( "Wrote coverage files to %s" , coverage_dir ) if open_in_browser : index_file = os . path . abspath ( os . path . join ( coverage_dir , 'index.html' ) ) logger . debug ( "About to open %s in your webbrowser." , index_file ) webbrowser . open ( 'file://' + index_file ) logger . info ( "Opened reports in your browser." )
10754	def iso_name_slugify ( name ) : name = name . encode ( 'ascii' , 'replace' ) . replace ( b'?' , b'_' ) return name . decode ( 'ascii' )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
2898	def get_task ( self , id ) : tasks = [ task for task in self . get_tasks ( ) if task . id == id ] return tasks [ 0 ] if len ( tasks ) == 1 else None
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) config . commit ( ) from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
1995	def _named_stream ( self , name , binary = False ) : with self . _store . save_stream ( self . _named_key ( name ) , binary = binary ) as s : yield s
2223	def _hashable_sequence ( data , types = True ) : r hasher = _HashTracer ( ) _update_hasher ( hasher , data , types = types ) return hasher . sequence
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
1279	def parse_lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )
10414	def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node not in node_set return exclusion_filter
13411	def removeLogbooks ( self , type = None , logs = [ ] ) : if type is not None and type in self . logList : if len ( logs ) == 0 or logs == "All" : del self . logList [ type ] else : for logbook in logs : if logbook in self . logList [ type ] : self . logList [ type ] . remove ( logbook ) self . changeLogType ( )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
5077	def get_closest_course_run ( course_runs ) : if len ( course_runs ) == 1 : return course_runs [ 0 ] now = datetime . datetime . now ( pytz . UTC ) never = now - datetime . timedelta ( days = 3650 ) return min ( course_runs , key = lambda x : abs ( get_course_run_start ( x , never ) - now ) )
10947	def reset ( self ) : inds = list ( range ( self . state . obj_get_positions ( ) . shape [ 0 ] ) ) self . _rad_nms = self . state . param_particle_rad ( inds ) self . _pos_nms = self . state . param_particle_pos ( inds ) self . _initial_rad = np . copy ( self . state . state [ self . _rad_nms ] ) self . _initial_pos = np . copy ( self . state . state [ self . _pos_nms ] ) . reshape ( ( - 1 , 3 ) ) self . param_vals [ self . rscale_mask ] = 0
4460	def geo ( lat , lon , radius , unit = 'km' ) : return GeoValue ( lat , lon , radius , unit )
12322	def to_holvi_dict ( self ) : self . _jsondata [ "items" ] = [ ] for item in self . items : self . _jsondata [ "items" ] . append ( item . to_holvi_dict ( ) ) self . _jsondata [ "issue_date" ] = self . issue_date . isoformat ( ) self . _jsondata [ "due_date" ] = self . due_date . isoformat ( ) self . _jsondata [ "receiver" ] = self . receiver . to_holvi_dict ( ) return { k : v for ( k , v ) in self . _jsondata . items ( ) if k in self . _valid_keys }
1592	def add ( self , stream_id , task_ids , grouping , source_comp_name ) : if stream_id not in self . targets : self . targets [ stream_id ] = [ ] self . targets [ stream_id ] . append ( Target ( task_ids , grouping , source_comp_name ) )
8749	def delete_scalingip ( context , id ) : LOG . info ( 'delete_scalingip %s for tenant %s' % ( id , context . tenant_id ) ) _delete_flip ( context , id , ip_types . SCALING )
4311	def _build_input_args ( input_filepath_list , input_format_list ) : if len ( input_format_list ) != len ( input_filepath_list ) : raise ValueError ( "input_format_list & input_filepath_list are not the same size" ) input_args = [ ] zipped = zip ( input_filepath_list , input_format_list ) for input_file , input_fmt in zipped : input_args . extend ( input_fmt ) input_args . append ( input_file ) return input_args
4392	def adsSyncWriteControlReqEx ( port , address , ads_state , device_state , data , plc_data_type ) : sync_write_control_request = _adsDLL . AdsSyncWriteControlReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state_c = ctypes . c_ulong ( ads_state ) device_state_c = ctypes . c_ulong ( device_state ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( data . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : data = plc_data_type ( data ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_control_request ( port , ams_address_pointer , ads_state_c , device_state_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
11517	def create_link ( self , token , folder_id , url , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'folderid' ] = folder_id parameters [ 'url' ] = url optional_keys = [ 'item_name' , 'length' , 'checksum' ] for key in optional_keys : if key in kwargs : if key == 'item_name' : parameters [ 'itemname' ] = kwargs [ key ] continue parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.link.create' , parameters ) return response
1055	def shuffle ( self , x , random = None ) : if random is None : random = self . random _int = int for i in reversed ( xrange ( 1 , len ( x ) ) ) : j = _int ( random ( ) * ( i + 1 ) ) x [ i ] , x [ j ] = x [ j ] , x [ i ]
4306	def play ( args ) : if args [ 0 ] . lower ( ) != "play" : args . insert ( 0 , "play" ) else : args [ 0 ] = "play" try : logger . info ( "Executing: %s" , " " . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) status = process_handle . wait ( ) if process_handle . stderr is not None : logger . info ( process_handle . stderr ) if status == 0 : return True else : logger . info ( "Play returned with error code %s" , status ) return False except OSError as error_msg : logger . error ( "OSError: Play failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return False
13293	def ensure_pandoc ( func ) : logger = logging . getLogger ( __name__ ) @ functools . wraps ( func ) def _install_and_run ( * args , ** kwargs ) : try : result = func ( * args , ** kwargs ) except OSError : message = "pandoc needed but not found. Now installing it for you." logger . warning ( message ) pypandoc . download_pandoc ( version = '1.19.1' ) logger . debug ( "pandoc download complete" ) result = func ( * args , ** kwargs ) return result return _install_and_run
11625	def generate ( grammar = None , num = 1 , output = sys . stdout , max_recursion = 10 , seed = None ) : if seed is not None : gramfuzz . rand . seed ( seed ) fuzzer = gramfuzz . GramFuzzer ( ) fuzzer . load_grammar ( grammar ) cat_group = os . path . basename ( grammar ) . replace ( ".py" , "" ) results = fuzzer . gen ( cat_group = cat_group , num = num , max_recursion = max_recursion ) for res in results : output . write ( res )
10545	def delete_task ( task_id ) : try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
955	def title ( s = None , additional = '' , stream = sys . stdout ) : if s is None : callable_name , file_name , class_name = getCallerInfo ( 2 ) s = callable_name if class_name is not None : s = class_name + '.' + callable_name lines = ( s + additional ) . split ( '\n' ) length = max ( len ( line ) for line in lines ) print >> stream , '-' * length print >> stream , s + additional print >> stream , '-' * length
5807	def parse_session_info ( server_handshake_bytes , client_handshake_bytes ) : protocol = None cipher_suite = None compression = False session_id = None session_ticket = None server_session_id = None client_session_id = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x02' : continue protocol = { b'\x03\x00' : "SSLv3" , b'\x03\x01' : "TLSv1" , b'\x03\x02' : "TLSv1.1" , b'\x03\x03' : "TLSv1.2" , b'\x03\x04' : "TLSv1.3" , } [ message_data [ 0 : 2 ] ] session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : server_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_bytes = message_data [ cipher_suite_start : cipher_suite_start + 2 ] cipher_suite = CIPHER_SUITE_MAP [ cipher_suite_bytes ] compression_start = cipher_suite_start + 2 compression = message_data [ compression_start : compression_start + 1 ] != b'\x00' extensions_length_start = compression_start + 1 extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "new" break break for record_type , _ , record_data in parse_tls_records ( client_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x01' : continue session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : client_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_length = int_from_bytes ( message_data [ cipher_suite_start : cipher_suite_start + 2 ] ) compression_start = cipher_suite_start + 2 + cipher_suite_length compression_length = int_from_bytes ( message_data [ compression_start : compression_start + 1 ] ) if server_session_id is None and session_ticket is None : extensions_length_start = compression_start + 1 + compression_length extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "reused" break break if server_session_id is not None : if client_session_id is None : session_id = "new" else : if client_session_id != server_session_id : session_id = "new" else : session_id = "reused" return { "protocol" : protocol , "cipher_suite" : cipher_suite , "compression" : compression , "session_id" : session_id , "session_ticket" : session_ticket , }
9608	def format_map ( self , format_string , mapping ) : return self . vformat ( format_string , args = None , kwargs = mapping )
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
6660	def random_forest_error ( forest , X_train , X_test , inbag = None , calibrate = True , memory_constrained = False , memory_limit = None ) : if inbag is None : inbag = calc_inbag ( X_train . shape [ 0 ] , forest ) pred = np . array ( [ tree . predict ( X_test ) for tree in forest ] ) . T pred_mean = np . mean ( pred , 0 ) pred_centered = pred - pred_mean n_trees = forest . n_estimators V_IJ = _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained , memory_limit ) V_IJ_unbiased = _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) if np . max ( inbag ) == 1 : variance_inflation = 1 / ( 1 - np . mean ( inbag ) ) ** 2 V_IJ_unbiased *= variance_inflation if not calibrate : return V_IJ_unbiased if V_IJ_unbiased . shape [ 0 ] <= 20 : print ( "No calibration with n_samples <= 20" ) return V_IJ_unbiased if calibrate : calibration_ratio = 2 n_sample = np . ceil ( n_trees / calibration_ratio ) new_forest = copy . deepcopy ( forest ) new_forest . estimators_ = np . random . permutation ( new_forest . estimators_ ) [ : int ( n_sample ) ] new_forest . n_estimators = int ( n_sample ) results_ss = random_forest_error ( new_forest , X_train , X_test , calibrate = False , memory_constrained = memory_constrained , memory_limit = memory_limit ) sigma2_ss = np . mean ( ( results_ss - V_IJ_unbiased ) ** 2 ) delta = n_sample / n_trees sigma2 = ( delta ** 2 + ( 1 - delta ) ** 2 ) / ( 2 * ( 1 - delta ) ** 2 ) * sigma2_ss V_IJ_calibrated = calibrateEB ( V_IJ_unbiased , sigma2 ) return V_IJ_calibrated
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
9893	def _uptime_plan9 ( ) : try : f = open ( '/dev/time' , 'r' ) s , ns , ct , cf = f . read ( ) . split ( ) f . close ( ) return float ( ct ) / float ( cf ) except ( IOError , ValueError ) : return None
6683	def require ( self , path = None , contents = None , source = None , url = None , md5 = None , use_sudo = False , owner = None , group = '' , mode = None , verify_remote = True , temp_dir = '/tmp' ) : func = use_sudo and run_as_root or self . run if path and not ( contents or source or url ) : assert path if not self . is_file ( path ) : func ( 'touch "%(path)s"' % locals ( ) ) elif url : if not path : path = os . path . basename ( urlparse ( url ) . path ) if not self . is_file ( path ) or md5 and self . md5sum ( path ) != md5 : func ( 'wget --progress=dot:mega "%(url)s" -O "%(path)s"' % locals ( ) ) else : if source : assert not contents t = None else : fd , source = mkstemp ( ) t = os . fdopen ( fd , 'w' ) t . write ( contents ) t . close ( ) if verify_remote : digest = hashlib . md5 ( ) f = open ( source , 'rb' ) try : while True : d = f . read ( BLOCKSIZE ) if not d : break digest . update ( d ) finally : f . close ( ) else : digest = None if ( not self . is_file ( path , use_sudo = use_sudo ) or ( verify_remote and self . md5sum ( path , use_sudo = use_sudo ) != digest . hexdigest ( ) ) ) : with self . settings ( hide ( 'running' ) ) : self . put ( local_path = source , remote_path = path , use_sudo = use_sudo , temp_dir = temp_dir ) if t is not None : os . unlink ( source ) if use_sudo and owner is None : owner = 'root' if ( owner and self . get_owner ( path , use_sudo ) != owner ) or ( group and self . get_group ( path , use_sudo ) != group ) : func ( 'chown %(owner)s:%(group)s "%(path)s"' % locals ( ) ) if use_sudo and mode is None : mode = oct ( 0o666 & ~ int ( self . umask ( use_sudo = True ) , base = 8 ) ) if mode and self . get_mode ( path , use_sudo ) != mode : func ( 'chmod %(mode)s "%(path)s"' % locals ( ) )
4675	def removeAccount ( self , account ) : accounts = self . getAccounts ( ) for a in accounts : if a [ "name" ] == account : self . store . delete ( a [ "pubkey" ] )
12680	def get_formatted_messages ( self , formats , label , context ) : format_templates = { } for fmt in formats : if fmt . endswith ( ".txt" ) : context . autoescape = False format_templates [ fmt ] = render_to_string ( ( "notification/%s/%s" % ( label , fmt ) , "notification/%s" % fmt ) , context_instance = context ) return format_templates
241	def create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , trade_daily_vol_limit = 0.05 , last_n_days = utils . APPROX_BDAYS_PER_MONTH * 6 , days_to_liquidate_limit = 1 , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) print ( "Max days to liquidation is computed for each traded name " "assuming a 20% limit on daily bar consumption \n" "and trailing 5 day mean volume as the available bar volume.\n\n" "Tickers with >1 day liquidation time at a" " constant $1m capital base:" ) max_days_by_ticker = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 ) max_days_by_ticker . index = ( max_days_by_ticker . index . map ( utils . format_asset ) ) print ( "Whole backtest:" ) utils . print_table ( max_days_by_ticker [ max_days_by_ticker . days_to_liquidate > days_to_liquidate_limit ] ) max_days_by_ticker_lnd = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 , last_n_days = last_n_days ) max_days_by_ticker_lnd . index = ( max_days_by_ticker_lnd . index . map ( utils . format_asset ) ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( max_days_by_ticker_lnd [ max_days_by_ticker_lnd . days_to_liquidate > 1 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data ) llt . index = llt . index . map ( utils . format_asset ) print ( 'Tickers with daily transactions consuming >{}% of daily bar \n' 'all backtest:' . format ( trade_daily_vol_limit * 100 ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data , last_n_days = last_n_days ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) bt_starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns . iloc [ 0 ] ) fig , ax_capacity_sweep = plt . subplots ( figsize = ( 14 , 6 ) ) plotting . plot_capacity_sweep ( returns , transactions , market_data , bt_starting_capital , min_pv = 100000 , max_pv = 300000000 , step_size = 1000000 , ax = ax_capacity_sweep )
7329	async def request ( self , method , url , future , headers = None , session = None , encoding = None , ** kwargs ) : await self . setup req_kwargs = await self . headers . prepare_request ( method = method , url = url , headers = headers , proxy = self . proxy , ** kwargs ) if encoding is None : encoding = self . encoding session = session if ( session is not None ) else self . _session logger . debug ( "making request with parameters: %s" % req_kwargs ) async with session . request ( ** req_kwargs ) as response : if response . status < 400 : data = await data_processing . read ( response , self . _loads , encoding = encoding ) future . set_result ( data_processing . PeonyResponse ( data = data , headers = response . headers , url = response . url , request = req_kwargs ) ) else : await exceptions . throw ( response , loads = self . _loads , encoding = encoding , url = url )
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
3303	def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
6345	def stem ( self , word ) : terminate = False intact = True while not terminate : for n in range ( 6 , 0 , - 1 ) : if word [ - n : ] in self . _rule_table [ n ] : accept = False if len ( self . _rule_table [ n ] [ word [ - n : ] ] ) < 4 : for rule in self . _rule_table [ n ] [ word [ - n : ] ] : ( word , accept , intact , terminate , ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : rule = self . _rule_table [ n ] [ word [ - n : ] ] ( word , accept , intact , terminate ) = self . _apply_rule ( word , rule , intact , terminate ) if accept : break else : break return word
6728	def respawn ( name = None , group = None ) : if name is None : name = get_name ( ) delete ( name = name , group = group ) instance = get_or_create ( name = name , group = group ) env . host_string = instance . public_dns_name
11045	def init_storage_dir ( storage_dir ) : storage_path = FilePath ( storage_dir ) default_cert_path = storage_path . child ( 'default.pem' ) if not default_cert_path . exists ( ) : default_cert_path . setContent ( generate_wildcard_pem_bytes ( ) ) unmanaged_certs_path = storage_path . child ( 'unmanaged-certs' ) if not unmanaged_certs_path . exists ( ) : unmanaged_certs_path . createDirectory ( ) certs_path = storage_path . child ( 'certs' ) if not certs_path . exists ( ) : certs_path . createDirectory ( ) return storage_path , certs_path
4697	def cat_file ( path ) : cmd = [ "cat" , path ] status , stdout , _ = cij . ssh . command ( cmd , shell = True , echo = True ) if status : raise RuntimeError ( "cij.nvme.env: cat %s failed" % path ) return stdout . strip ( )
3358	def index ( self , id , * args ) : if isinstance ( id , string_types ) : try : return self . _dict [ id ] except KeyError : raise ValueError ( "%s not found" % id ) try : i = self . _dict [ id . id ] if self [ i ] is not id : raise ValueError ( "Another object with the identical id (%s) found" % id . id ) return i except KeyError : raise ValueError ( "%s not found" % str ( id ) )
8373	def widget_changed ( self , widget , v ) : if v . type is NUMBER : self . bot . _namespace [ v . name ] = widget . get_value ( ) self . bot . _vars [ v . name ] . value = widget . get_value ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is BOOLEAN : self . bot . _namespace [ v . name ] = widget . get_active ( ) self . bot . _vars [ v . name ] . value = widget . get_active ( ) publish_event ( VARIABLE_UPDATED_EVENT , v ) elif v . type is TEXT : self . bot . _namespace [ v . name ] = widget . get_text ( ) self . bot . _vars [ v . name ] . value = widget . get_text ( ) publish_event ( VARIABLE_UPDATED_EVENT , v )
7435	def _zbufcountlines ( filename , gzipped ) : if gzipped : cmd1 = [ "gunzip" , "-c" , filename ] else : cmd1 = [ "cat" , filename ] cmd2 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdout = sps . PIPE , stderr = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , stderr = sps . PIPE ) res = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error zbufcountlines {}:" . format ( res ) ) LOGGER . info ( res ) nlines = int ( res . split ( ) [ 0 ] ) return nlines
4118	def _swapsides ( data ) : N = len ( data ) return np . concatenate ( ( data [ N // 2 + 1 : ] , data [ 0 : N // 2 ] ) )
4256	def get_compressed_filename ( self , filename ) : if not os . path . splitext ( filename ) [ 1 ] [ 1 : ] in self . suffixes_to_compress : return False file_stats = None compressed_stats = None compressed_filename = '{}.{}' . format ( filename , self . suffix ) try : file_stats = os . stat ( filename ) compressed_stats = os . stat ( compressed_filename ) except OSError : pass if file_stats and compressed_stats : return ( compressed_filename if file_stats . st_mtime > compressed_stats . st_mtime else False ) else : return compressed_filename
8754	def get_groups_to_ack ( groups_to_ack , init_sg_states , curr_sg_states ) : security_groups_changed = [ ] for vif in groups_to_ack : initial_state = init_sg_states [ vif ] [ sg_cli . SECURITY_GROUP_HASH_ATTR ] current_state = curr_sg_states [ vif ] [ sg_cli . SECURITY_GROUP_HASH_ATTR ] bad_match_msg = ( 'security group rules were changed for vif "%s" while' ' executing xapi_client.update_interfaces.' ' Will not ack rule.' % vif ) if len ( initial_state ) != len ( current_state ) : security_groups_changed . append ( vif ) LOG . info ( bad_match_msg ) elif len ( initial_state ) > 0 : for rule in current_state : if rule not in initial_state : security_groups_changed . append ( vif ) LOG . info ( bad_match_msg ) break ret = [ group for group in groups_to_ack if group not in security_groups_changed ] return ret
451	def flatten_reshape ( variable , name = 'flatten' ) : dim = 1 for d in variable . get_shape ( ) [ 1 : ] . as_list ( ) : dim *= d return tf . reshape ( variable , shape = [ - 1 , dim ] , name = name )
13780	def FindEnumTypeByName ( self , full_name ) : full_name = _NormalizeFullyQualifiedName ( full_name ) if full_name not in self . _enum_descriptors : self . FindFileContainingSymbol ( full_name ) return self . _enum_descriptors [ full_name ]
9077	def make_downloader ( url : str , path : str ) -> Callable [ [ bool ] , str ] : def download_data ( force_download : bool = False ) -> str : if os . path . exists ( path ) and not force_download : log . info ( 'using cached data at %s' , path ) else : log . info ( 'downloading %s to %s' , url , path ) urlretrieve ( url , path ) return path return download_data
5431	def _get_filtered_mounts ( mounts , mount_param_type ) : return set ( [ mount for mount in mounts if isinstance ( mount , mount_param_type ) ] )
12882	def main ( world_cls , referee_cls , gui_cls , gui_actor_cls , ai_actor_cls , theater_cls = PygletTheater , default_host = DEFAULT_HOST , default_port = DEFAULT_PORT , argv = None ) : import sys , os , docopt , nonstdlib exe_name = os . path . basename ( sys . argv [ 0 ] ) usage = main . __doc__ . format ( ** locals ( ) ) . strip ( ) args = docopt . docopt ( usage , argv or sys . argv [ 1 : ] ) num_guis = int ( args [ '<num_guis>' ] or 1 ) num_ais = int ( args [ '<num_ais>' ] or 0 ) host , port = args [ '--host' ] , int ( args [ '--port' ] ) logging . basicConfig ( format = '%(levelname)s: %(name)s: %(message)s' , level = nonstdlib . verbosity ( args [ '--verbose' ] ) , ) if args [ 'debug' ] : print ( ) game = MultiplayerDebugger ( world_cls , referee_cls , gui_cls , gui_actor_cls , num_guis , ai_actor_cls , num_ais , theater_cls , host , port ) else : game = theater_cls ( ) ai_actors = [ ai_actor_cls ( ) for i in range ( num_ais ) ] if args [ 'sandbox' ] : game . gui = gui_cls ( ) game . initial_stage = UniplayerGameStage ( world_cls ( ) , referee_cls ( ) , gui_actor_cls ( ) , ai_actors ) game . initial_stage . successor = PostgameSplashStage ( ) if args [ 'client' ] : game . gui = gui_cls ( ) game . initial_stage = ClientConnectionStage ( world_cls ( ) , gui_actor_cls ( ) , host , port ) if args [ 'server' ] : game . initial_stage = ServerConnectionStage ( world_cls ( ) , referee_cls ( ) , num_guis , ai_actors , host , port ) game . play ( )
11880	def scanAllProcessesForCwd ( searchPortion , isExactMatch = False ) : pids = getAllRunningPids ( ) cwdResults = [ scanProcessForCwd ( pid , searchPortion , isExactMatch ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if cwdResults [ i ] is not None : ret [ pids [ i ] ] = cwdResults [ i ] return ret
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
12987	def keep_kwargs_partial ( func , * args , ** keywords ) : def newfunc ( * fargs , ** fkeywords ) : newkeywords = fkeywords . copy ( ) newkeywords . update ( keywords ) return func ( * ( args + fargs ) , ** newkeywords ) newfunc . func = func newfunc . args = args newfunc . keywords = keywords return newfunc
4765	def is_not_equal_to ( self , other ) : if self . val == other : self . _err ( 'Expected <%s> to be not equal to <%s>, but was.' % ( self . val , other ) ) return self
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
12372	def chop ( list_ , n ) : "Chop list_ into n chunks. Returns a list." size = len ( list_ ) each = size // n if each == 0 : return [ list_ ] chopped = [ ] for i in range ( n ) : start = i * each end = ( i + 1 ) * each if i == ( n - 1 ) : end = size chopped . append ( list_ [ start : end ] ) return chopped
8909	def fetch_by_url ( self , url ) : service = self . collection . find_one ( { 'url' : url } ) if not service : raise ServiceNotFound return Service ( service )
979	def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap
8148	def _load_namespace ( self , namespace , filename = None ) : from shoebot import data for name in dir ( data ) : namespace [ name ] = getattr ( data , name ) for name in dir ( self ) : if name [ 0 ] != '_' : namespace [ name ] = getattr ( self , name ) namespace [ '_ctx' ] = self namespace [ '__file__' ] = filename
6307	def load_effects_classes ( self ) : self . effect_classes = [ ] for _ , cls in inspect . getmembers ( self . effect_module ) : if inspect . isclass ( cls ) : if cls == Effect : continue if issubclass ( cls , Effect ) : self . effect_classes . append ( cls ) self . effect_class_map [ cls . __name__ ] = cls cls . _name = "{}.{}" . format ( self . effect_module_name , cls . __name__ )
9473	def DFS_prefix ( self , root = None ) : if not root : root = self . _root return self . _DFS_prefix ( root )
12106	def _launch_process_group ( self , process_commands , streams_path ) : processes = [ ] for cmd , tid in process_commands : job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_path = os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) stderr_path = os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) process = { 'tid' : tid , 'cmd' : cmd , 'stdout' : stdout_path , 'stderr' : stderr_path } processes . append ( process ) json_path = os . path . join ( self . root_directory , self . json_name % ( tid ) ) with open ( json_path , 'w' ) as json_file : json . dump ( processes , json_file , sort_keys = True , indent = 4 ) p = subprocess . Popen ( [ self . script_path , json_path , self . batch_name , str ( len ( processes ) ) , str ( self . max_concurrency ) ] ) if p . wait ( ) != 0 : raise EnvironmentError ( "Script command exit with code: %d" % p . poll ( ) )
6431	def dist_abs ( self , src , tar ) : if src == tar : return 6 if src == '' or tar == '' : return 0 src = list ( mra ( src ) ) tar = list ( mra ( tar ) ) if abs ( len ( src ) - len ( tar ) ) > 2 : return 0 length_sum = len ( src ) + len ( tar ) if length_sum < 5 : min_rating = 5 elif length_sum < 8 : min_rating = 4 elif length_sum < 12 : min_rating = 3 else : min_rating = 2 for _ in range ( 2 ) : new_src = [ ] new_tar = [ ] minlen = min ( len ( src ) , len ( tar ) ) for i in range ( minlen ) : if src [ i ] != tar [ i ] : new_src . append ( src [ i ] ) new_tar . append ( tar [ i ] ) src = new_src + src [ minlen : ] tar = new_tar + tar [ minlen : ] src . reverse ( ) tar . reverse ( ) similarity = 6 - max ( len ( src ) , len ( tar ) ) if similarity >= min_rating : return similarity return 0
1711	def _set_name ( self , name ) : if self . own . get ( 'name' ) : self . func_name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
6060	def numpy_array_2d_from_fits ( file_path , hdu ) : hdu_list = fits . open ( file_path ) return np . flipud ( np . array ( hdu_list [ hdu ] . data ) )
3434	def _populate_solver ( self , reaction_list , metabolite_list = None ) : constraint_terms = AutoVivification ( ) to_add = [ ] if metabolite_list is not None : for met in metabolite_list : to_add += [ self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) ] self . add_cons_vars ( to_add ) for reaction in reaction_list : if reaction . id not in self . variables : forward_variable = self . problem . Variable ( reaction . id ) reverse_variable = self . problem . Variable ( reaction . reverse_id ) self . add_cons_vars ( [ forward_variable , reverse_variable ] ) else : reaction = self . reactions . get_by_id ( reaction . id ) forward_variable = reaction . forward_variable reverse_variable = reaction . reverse_variable for metabolite , coeff in six . iteritems ( reaction . metabolites ) : if metabolite . id in self . constraints : constraint = self . constraints [ metabolite . id ] else : constraint = self . problem . Constraint ( Zero , name = metabolite . id , lb = 0 , ub = 0 ) self . add_cons_vars ( constraint , sloppy = True ) constraint_terms [ constraint ] [ forward_variable ] = coeff constraint_terms [ constraint ] [ reverse_variable ] = - coeff self . solver . update ( ) for reaction in reaction_list : reaction = self . reactions . get_by_id ( reaction . id ) reaction . update_variable_bounds ( ) for constraint , terms in six . iteritems ( constraint_terms ) : constraint . set_linear_coefficients ( terms )
10490	def popUpItem ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . _menuItem ( self , * args )
306	def plot_round_trip_lifetimes ( round_trips , disp_amount = 16 , lsize = 18 , ax = None ) : if ax is None : ax = plt . subplot ( ) symbols_sample = round_trips . symbol . unique ( ) np . random . seed ( 1 ) sample = np . random . choice ( round_trips . symbol . unique ( ) , replace = False , size = min ( disp_amount , len ( symbols_sample ) ) ) sample_round_trips = round_trips [ round_trips . symbol . isin ( sample ) ] symbol_idx = pd . Series ( np . arange ( len ( sample ) ) , index = sample ) for symbol , sym_round_trips in sample_round_trips . groupby ( 'symbol' ) : for _ , row in sym_round_trips . iterrows ( ) : c = 'b' if row . long else 'r' y_ix = symbol_idx [ symbol ] + 0.05 ax . plot ( [ row [ 'open_dt' ] , row [ 'close_dt' ] ] , [ y_ix , y_ix ] , color = c , linewidth = lsize , solid_capstyle = 'butt' ) ax . set_yticks ( range ( disp_amount ) ) ax . set_yticklabels ( [ utils . format_asset ( s ) for s in sample ] ) ax . set_ylim ( ( - 0.5 , min ( len ( sample ) , disp_amount ) - 0.5 ) ) blue = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'b' , label = 'Long' ) red = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'r' , label = 'Short' ) leg = ax . legend ( handles = [ blue , red ] , loc = 'lower left' , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . grid ( False ) return ax
12121	def get_data_around ( self , timePoints , thisSweep = False , padding = 0.02 , msDeriv = 0 ) : if not np . array ( timePoints ) . shape : timePoints = [ float ( timePoints ) ] data = None for timePoint in timePoints : if thisSweep : sweep = self . currentSweep else : sweep = int ( timePoint / self . sweepInterval ) timePoint = timePoint - sweep * self . sweepInterval self . setSweep ( sweep ) if msDeriv : dx = int ( msDeriv * self . rate / 1000 ) newData = ( self . dataY [ dx : ] - self . dataY [ : - dx ] ) * self . rate / 1000 / dx else : newData = self . dataY padPoints = int ( padding * self . rate ) pad = np . empty ( padPoints ) * np . nan Ic = timePoint * self . rate newData = np . concatenate ( ( pad , pad , newData , pad , pad ) ) Ic += padPoints * 2 newData = newData [ Ic - padPoints : Ic + padPoints ] newData = newData [ : int ( padPoints * 2 ) ] if data is None : data = [ newData ] else : data = np . vstack ( ( data , newData ) ) return data
8569	def remove_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics/%s' % ( datacenter_id , loadbalancer_id , nic_id ) , method = 'DELETE' ) return response
10730	def signature ( dbus_object , unpack = False ) : if dbus_object . variant_level != 0 and not unpack : return 'v' if isinstance ( dbus_object , dbus . Array ) : sigs = frozenset ( signature ( x ) for x in dbus_object ) len_sigs = len ( sigs ) if len_sigs > 1 : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_sigs == 0 : return 'a' + dbus_object . signature return 'a' + [ x for x in sigs ] [ 0 ] if isinstance ( dbus_object , dbus . Struct ) : sigs = ( signature ( x ) for x in dbus_object ) return '(' + "" . join ( x for x in sigs ) + ')' if isinstance ( dbus_object , dbus . Dictionary ) : key_sigs = frozenset ( signature ( x ) for x in dbus_object . keys ( ) ) value_sigs = frozenset ( signature ( x ) for x in dbus_object . values ( ) ) len_key_sigs = len ( key_sigs ) len_value_sigs = len ( value_sigs ) if len_key_sigs != len_value_sigs : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_key_sigs > 1 : raise IntoDPValueError ( dbus_object , "dbus_object" , "has bad signature" ) if len_key_sigs == 0 : return 'a{' + dbus_object . signature + '}' return 'a{' + [ x for x in key_sigs ] [ 0 ] + [ x for x in value_sigs ] [ 0 ] + '}' if isinstance ( dbus_object , dbus . Boolean ) : return 'b' if isinstance ( dbus_object , dbus . Byte ) : return 'y' if isinstance ( dbus_object , dbus . Double ) : return 'd' if isinstance ( dbus_object , dbus . Int16 ) : return 'n' if isinstance ( dbus_object , dbus . Int32 ) : return 'i' if isinstance ( dbus_object , dbus . Int64 ) : return 'x' if isinstance ( dbus_object , dbus . ObjectPath ) : return 'o' if isinstance ( dbus_object , dbus . Signature ) : return 'g' if isinstance ( dbus_object , dbus . String ) : return 's' if isinstance ( dbus_object , dbus . UInt16 ) : return 'q' if isinstance ( dbus_object , dbus . UInt32 ) : return 'u' if isinstance ( dbus_object , dbus . UInt64 ) : return 't' if isinstance ( dbus_object , dbus . types . UnixFd ) : return 'h' raise IntoDPValueError ( dbus_object , "dbus_object" , "has no signature" )
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
8539	def get_disk_image_by_name ( pbclient , location , image_name ) : all_images = pbclient . list_images ( ) matching = [ i for i in all_images [ 'items' ] if i [ 'properties' ] [ 'name' ] == image_name and i [ 'properties' ] [ 'imageType' ] == "HDD" and i [ 'properties' ] [ 'location' ] == location ] return matching
6772	def install_required ( self , type = None , service = None , list_only = 0 , ** kwargs ) : r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) return state
4559	def next ( self , length ) : return Segment ( self . strip , length , self . offset + self . length )
6187	def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
10993	def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
11281	def clone ( self ) : new_object = copy . copy ( self ) if new_object . next : new_object . next = new_object . next . clone ( ) return new_object
731	def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
943	def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir
8033	def find_dupes ( paths , exact = False , ignores = None , min_size = 0 ) : groups = { '' : getPaths ( paths , ignores ) } groups = groupBy ( groups , sizeClassifier , 'sizes' , min_size = min_size ) groups = groupBy ( groups , hashClassifier , 'header hashes' , limit = HEAD_SIZE ) if exact : groups = groupBy ( groups , groupByContent , fun_desc = 'contents' ) else : groups = groupBy ( groups , hashClassifier , fun_desc = 'hashes' ) return groups
11210	def _set_tzdata ( self , tzobj ) : for attr in _tzfile . attrs : setattr ( self , '_' + attr , getattr ( tzobj , attr ) )
2610	def _restore_buffers ( obj , buffers ) : if isinstance ( obj , CannedObject ) and obj . buffers : for i , buf in enumerate ( obj . buffers ) : if buf is None : obj . buffers [ i ] = buffers . pop ( 0 )
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
6934	def add_cmds_cplist ( cplist , cmdpkl , require_cmd_magcolor = True , save_cmd_pngs = False ) : with open ( cmdpkl , 'rb' ) as infd : cmd = pickle . load ( infd ) for cpf in cplist : add_cmd_to_checkplot ( cpf , cmd , require_cmd_magcolor = require_cmd_magcolor , save_cmd_pngs = save_cmd_pngs )
9240	def fetch_events_for_issues_and_pr ( self ) : self . fetcher . fetch_events_async ( self . issues , "issues" ) self . fetcher . fetch_events_async ( self . pull_requests , "pull requests" )
7741	def _configure_io_handler ( self , handler ) : if self . check_events ( ) : return if handler in self . _unprepared_handlers : old_fileno = self . _unprepared_handlers [ handler ] prepared = self . _prepare_io_handler ( handler ) else : old_fileno = None prepared = True fileno = handler . fileno ( ) if old_fileno is not None and fileno != old_fileno : tag = self . _io_sources . pop ( handler , None ) if tag is not None : glib . source_remove ( tag ) if not prepared : self . _unprepared_handlers [ handler ] = fileno if fileno is None : logger . debug ( " {0!r}.fileno() is None, not polling" . format ( handler ) ) return events = 0 if handler . is_readable ( ) : logger . debug ( " {0!r} readable" . format ( handler ) ) events |= glib . IO_IN | glib . IO_ERR if handler . is_writable ( ) : logger . debug ( " {0!r} writable" . format ( handler ) ) events |= glib . IO_OUT | glib . IO_HUP | glib . IO_ERR if events : logger . debug ( " registering {0!r} handler fileno {1} for" " events {2}" . format ( handler , fileno , events ) ) glib . io_add_watch ( fileno , events , self . _io_callback , handler )
104	def pad_to_aspect_ratio ( arr , aspect_ratio , mode = "constant" , cval = 0 , return_pad_amounts = False ) : pad_top , pad_right , pad_bottom , pad_left = compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) arr_padded = pad ( arr , top = pad_top , right = pad_right , bottom = pad_bottom , left = pad_left , mode = mode , cval = cval ) if return_pad_amounts : return arr_padded , ( pad_top , pad_right , pad_bottom , pad_left ) else : return arr_padded
7936	def _set_state ( self , state ) : logger . debug ( " _set_state({0!r})" . format ( state ) ) self . _state = state self . _state_cond . notify ( )
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) matched_new_or_template = match . group ( 1 ) if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
548	def __deleteOutputCache ( self , modelID ) : if modelID == self . _modelID and self . _predictionLogger is not None : self . _predictionLogger . close ( ) del self . __predictionCache self . _predictionLogger = None self . __predictionCache = None
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
12482	def find_in_sections ( var_name , app_name ) : sections = get_sections ( app_name ) if not sections : raise ValueError ( 'No sections found in {} rcfiles.' . format ( app_name ) ) for s in sections : try : var_value = get_rcfile_variable_value ( var_name , section_name = s , app_name = app_name ) except : pass else : return s , var_value raise KeyError ( 'No variable {} has been found in {} ' 'rcfiles.' . format ( var_name , app_name ) )
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
3090	def locked_put ( self , credentials ) : entity = self . _model . get_or_insert ( self . _key_name ) setattr ( entity , self . _property_name , credentials ) entity . put ( ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) )
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
3907	def _on_event ( self , conv_event ) : conv = self . _conv_list . get ( conv_event . conversation_id ) user = conv . get_user ( conv_event . user_id ) show_notification = all ( ( isinstance ( conv_event , hangups . ChatMessageEvent ) , not user . is_self , not conv . is_quiet , ) ) if show_notification : self . add_conversation_tab ( conv_event . conversation_id ) if self . _discreet_notifications : notification = DISCREET_NOTIFICATION else : notification = notifier . Notification ( user . full_name , get_conv_name ( conv ) , conv_event . text ) self . _notifier . send ( notification )
11425	def record_match_subfields ( rec , tag , ind1 = " " , ind2 = " " , sub_key = None , sub_value = '' , sub_key2 = None , sub_value2 = '' , case_sensitive = True ) : if sub_key is None : raise TypeError ( "None object passed for parameter sub_key." ) if sub_key2 is not None and sub_value2 is '' : raise TypeError ( "Parameter sub_key2 defined but sub_value2 is None, " + "function requires a value for comparrison." ) ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if not case_sensitive : sub_value = sub_value . lower ( ) sub_value2 = sub_value2 . lower ( ) for field in record_get_field_instances ( rec , tag , ind1 , ind2 ) : subfields = dict ( field_get_subfield_instances ( field ) ) if not case_sensitive : for k , v in subfields . iteritems ( ) : subfields [ k ] = v . lower ( ) if sub_key in subfields : if sub_value is '' : return field [ 4 ] else : if sub_value == subfields [ sub_key ] : if sub_key2 is None : return field [ 4 ] else : if sub_key2 in subfields : if sub_value2 == subfields [ sub_key2 ] : return field [ 4 ] return False
3967	def _compose_dict_for_nginx ( port_specs ) : spec = { 'image' : constants . NGINX_IMAGE , 'volumes' : [ '{}:{}' . format ( constants . NGINX_CONFIG_DIR_IN_VM , constants . NGINX_CONFIG_DIR_IN_CONTAINER ) ] , 'command' : 'nginx -g "daemon off;" -c /etc/nginx/conf.d/nginx.primary' , 'container_name' : 'dusty_{}_1' . format ( constants . DUSTY_NGINX_NAME ) } all_host_ports = set ( [ nginx_spec [ 'host_port' ] for nginx_spec in port_specs [ 'nginx' ] ] ) if all_host_ports : spec [ 'ports' ] = [ ] for port in all_host_ports : spec [ 'ports' ] . append ( '{0}:{0}' . format ( port ) ) return { constants . DUSTY_NGINX_NAME : spec }
2786	def create_from_snapshot ( self , * args , ** kwargs ) : data = self . get_data ( 'volumes/' , type = POST , params = { 'name' : self . name , 'snapshot_id' : self . snapshot_id , 'region' : self . region , 'size_gigabytes' : self . size_gigabytes , 'description' : self . description , 'filesystem_type' : self . filesystem_type , 'filesystem_label' : self . filesystem_label } ) if data : self . id = data [ 'volume' ] [ 'id' ] self . created_at = data [ 'volume' ] [ 'created_at' ] return self
12891	def handle_int ( self , item ) : doc = yield from self . handle_get ( item ) if doc is None : return None return int ( doc . value . u8 . text ) or None
1938	def get_func_argument_types ( self , hsh : bytes ) : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) return '()' if sig is None else sig [ sig . find ( '(' ) : ]
8198	def angle ( x1 , y1 , x2 , y2 ) : sign = 1.0 usign = ( x1 * y2 - y1 * x2 ) if usign < 0 : sign = - 1.0 num = x1 * x2 + y1 * y2 den = hypot ( x1 , y1 ) * hypot ( x2 , y2 ) ratio = min ( max ( num / den , - 1.0 ) , 1.0 ) return sign * degrees ( acos ( ratio ) )
1683	def PrintErrorCounts ( self ) : for category , count in sorted ( iteritems ( self . errors_by_category ) ) : self . PrintInfo ( 'Category \'%s\' errors found: %d\n' % ( category , count ) ) if self . error_count > 0 : self . PrintInfo ( 'Total errors found: %d\n' % self . error_count )
4403	def mget ( self , keys , * args ) : args = list_or_args ( keys , args ) server_keys = { } ret_dict = { } for key in args : server_name = self . get_server_name ( key ) server_keys [ server_name ] = server_keys . get ( server_name , [ ] ) server_keys [ server_name ] . append ( key ) for server_name , sub_keys in iteritems ( server_keys ) : values = self . connections [ server_name ] . mget ( sub_keys ) ret_dict . update ( dict ( zip ( sub_keys , values ) ) ) result = [ ] for key in args : result . append ( ret_dict . get ( key , None ) ) return result
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
4572	def hsv2rgb_raw ( hsv ) : HSV_SECTION_3 = 0x40 h , s , v = hsv invsat = 255 - s brightness_floor = ( v * invsat ) // 256 color_amplitude = v - brightness_floor section = h // HSV_SECTION_3 offset = h % HSV_SECTION_3 rampup = offset rampdown = ( HSV_SECTION_3 - 1 ) - offset rampup_amp_adj = ( rampup * color_amplitude ) // ( 256 // 4 ) rampdown_amp_adj = ( rampdown * color_amplitude ) // ( 256 // 4 ) rampup_adj_with_floor = rampup_amp_adj + brightness_floor rampdown_adj_with_floor = rampdown_amp_adj + brightness_floor r , g , b = ( 0 , 0 , 0 ) if section : if section == 1 : r = brightness_floor g = rampdown_adj_with_floor b = rampup_adj_with_floor else : r = rampup_adj_with_floor g = brightness_floor b = rampdown_adj_with_floor else : r = rampdown_adj_with_floor g = rampup_adj_with_floor b = brightness_floor return ( r , g , b )
11362	def convert_html_subscripts_to_latex ( text ) : text = re . sub ( "<sub>(.*?)</sub>" , r"$_{\1}$" , text ) text = re . sub ( "<sup>(.*?)</sup>" , r"$^{\1}$" , text ) return text
4758	def main ( args ) : trun = cij . runner . trun_from_file ( args . trun_fpath ) rehome ( trun [ "conf" ] [ "OUTPUT" ] , args . output , trun ) postprocess ( trun ) cij . emph ( "main: reports are uses tmpl_fpath: %r" % args . tmpl_fpath ) cij . emph ( "main: reports are here args.output: %r" % args . output ) html_fpath = os . sep . join ( [ args . output , "%s.html" % args . tmpl_name ] ) cij . emph ( "html_fpath: %r" % html_fpath ) try : with open ( html_fpath , 'w' ) as html_file : html_file . write ( dset_to_html ( trun , args . tmpl_fpath ) ) except ( IOError , OSError , ValueError ) as exc : import traceback traceback . print_exc ( ) cij . err ( "rprtr:main: exc: %s" % exc ) return 1 return 0
420	def save_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . ValidLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] valid log: " + _log )
4785	def starts_with ( self , prefix ) : if prefix is None : raise TypeError ( 'given prefix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( prefix , str_types ) : raise TypeError ( 'given prefix arg must be a string' ) if len ( prefix ) == 0 : raise ValueError ( 'given prefix arg must not be empty' ) if not self . val . startswith ( prefix ) : self . _err ( 'Expected <%s> to start with <%s>, but did not.' % ( self . val , prefix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) first = next ( iter ( self . val ) ) if first != prefix : self . _err ( 'Expected %s to start with <%s>, but did not.' % ( self . val , prefix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
2105	def version ( ) : click . echo ( 'Tower CLI %s' % __version__ ) click . echo ( 'API %s' % CUR_API_VERSION ) try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
7886	def emit_stanza ( self , element ) : if not self . _head_emitted : raise RuntimeError ( ".emit_head() must be called first." ) string = self . _emit_element ( element , level = 1 , declared_prefixes = self . _root_prefixes ) return remove_evil_characters ( string )
511	def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) )
12745	def pid ( kp = 0. , ki = 0. , kd = 0. , smooth = 0.1 ) : r state = dict ( p = 0 , i = 0 , d = 0 ) def control ( error , dt = 1 ) : state [ 'd' ] = smooth * state [ 'd' ] + ( 1 - smooth ) * ( error - state [ 'p' ] ) / dt state [ 'i' ] += error * dt state [ 'p' ] = error return kp * state [ 'p' ] + ki * state [ 'i' ] + kd * state [ 'd' ] return control
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
6016	def signal_to_noise_map ( self ) : signal_to_noise_map = np . divide ( self . image , self . noise_map ) signal_to_noise_map [ signal_to_noise_map < 0 ] = 0 return signal_to_noise_map
260	def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary
10786	def add_subtract ( st , max_iter = 7 , max_npart = 'calc' , max_mem = 2e8 , always_check_remove = False , ** kwargs ) : if max_npart == 'calc' : max_npart = 0.05 * st . obj_get_positions ( ) . shape [ 0 ] total_changed = 0 _change_since_opt = 0 removed_poses = [ ] added_poses0 = [ ] added_poses = [ ] nr = 1 for _ in range ( max_iter ) : if ( nr != 0 ) or ( always_check_remove ) : nr , rposes = remove_bad_particles ( st , ** kwargs ) na , aposes = add_missing_particles ( st , ** kwargs ) current_changed = na + nr removed_poses . extend ( rposes ) added_poses0 . extend ( aposes ) total_changed += current_changed _change_since_opt += current_changed if current_changed == 0 : break elif _change_since_opt > max_npart : _change_since_opt *= 0 CLOG . info ( 'Start add_subtract optimization.' ) opt . do_levmarq ( st , opt . name_globals ( st , remove_params = st . get ( 'psf' ) . params ) , max_iter = 1 , run_length = 4 , num_eig_dirs = 3 , max_mem = max_mem , eig_update_frequency = 2 , rz_order = 0 , use_accel = True ) CLOG . info ( 'After optimization:\t{:.6}' . format ( st . error ) ) for p in added_poses0 : i = st . obj_closest_particle ( p ) opt . do_levmarq_particles ( st , np . array ( [ i ] ) , max_iter = 2 , damping = 0.3 ) added_poses . append ( st . obj_get_positions ( ) [ i ] ) return total_changed , np . array ( removed_poses ) , np . array ( added_poses )
11129	def stats ( cls , traces ) : data = { } stats = { } for trace in traces : key = trace [ 'key' ] if key not in data : data [ key ] = [ ] stats [ key ] = { } data [ key ] . append ( trace [ 'total_time' ] ) cls . _traces . pop ( trace [ 'id' ] ) for key in data : times = data [ key ] stats [ key ] = dict ( count = len ( times ) , max = max ( times ) , min = min ( times ) , avg = sum ( times ) / len ( times ) ) return stats
12138	def _expand_pattern ( self , pattern ) : ( globpattern , regexp , fields , types ) = self . _decompose_pattern ( pattern ) filelist = glob . glob ( globpattern ) expansion = [ ] for fname in filelist : if fields == [ ] : expansion . append ( ( fname , { } ) ) continue match = re . match ( regexp , fname ) if match is None : continue match_items = match . groupdict ( ) . items ( ) tags = dict ( ( k , types . get ( k , str ) ( v ) ) for ( k , v ) in match_items ) expansion . append ( ( fname , tags ) ) return expansion
7745	def _loop_timeout_cb ( self , main_loop ) : self . _anything_done = True logger . debug ( "_loop_timeout_cb() called" ) main_loop . quit ( )
6816	def optimize_wsgi_processes ( self ) : r = self . local_renderer r . env . wsgi_server_memory_gb = 8 verbose = self . verbose all_sites = list ( self . iter_sites ( site = ALL , setter = self . set_site_specifics ) )
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
9161	def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
11413	def record_replace_field ( rec , tag , new_field , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) replaced = False for position , field in enumerate ( rec [ tag ] ) : if field [ 4 ] == field_position_global : rec [ tag ] [ position ] = new_field replaced = True if not replaced : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the global field position '%d'." % ( tag , field_position_global ) ) else : try : rec [ tag ] [ field_position_local ] = new_field except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
3174	def create_or_update ( self , store_id , product_id , variant_id , data ) : self . store_id = store_id self . product_id = product_id self . variant_id = variant_id if 'id' not in data : raise KeyError ( 'The product variant must have an id' ) if 'title' not in data : raise KeyError ( 'The product variant must have a title' ) return self . _mc_client . _put ( url = self . _build_path ( store_id , 'products' , product_id , 'variants' , variant_id ) , data = data )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
13078	def make_cache_keys ( self , endpoint , kwargs ) : keys = sorted ( kwargs . keys ( ) ) i18n_cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys ] ) if "lang" in keys : cache_key = endpoint + "|" + "|" . join ( [ kwargs [ k ] for k in keys if k != "lang" ] ) else : cache_key = i18n_cache_key return i18n_cache_key , cache_key
5572	def is_valid_with_config ( self , config ) : validate_values ( config , [ ( "schema" , dict ) , ( "path" , str ) ] ) validate_values ( config [ "schema" ] , [ ( "properties" , dict ) , ( "geometry" , str ) ] ) if config [ "schema" ] [ "geometry" ] not in [ "Geometry" , "Point" , "MultiPoint" , "Line" , "MultiLine" , "Polygon" , "MultiPolygon" ] : raise TypeError ( "invalid geometry type" ) return True
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , ** obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
6070	def sersic_constant ( self ) : return ( 2 * self . sersic_index ) - ( 1. / 3. ) + ( 4. / ( 405. * self . sersic_index ) ) + ( 46. / ( 25515. * self . sersic_index ** 2 ) ) + ( 131. / ( 1148175. * self . sersic_index ** 3 ) ) - ( 2194697. / ( 30690717750. * self . sersic_index ** 4 ) )
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
9889	def _uptime_linux ( ) : try : f = open ( '/proc/uptime' , 'r' ) up = float ( f . readline ( ) . split ( ) [ 0 ] ) f . close ( ) return up except ( IOError , ValueError ) : pass try : libc = ctypes . CDLL ( 'libc.so' ) except AttributeError : return None except OSError : try : libc = ctypes . CDLL ( 'libc.so.6' ) except OSError : return None if not hasattr ( libc , 'sysinfo' ) : return None buf = ctypes . create_string_buffer ( 128 ) if libc . sysinfo ( buf ) < 0 : return None up = struct . unpack_from ( '@l' , buf . raw ) [ 0 ] if up < 0 : up = None return up
2444	def set_annotation_spdx_id ( self , doc , spdx_id ) : if len ( doc . annotations ) != 0 : if not self . annotation_spdx_id_set : self . annotation_spdx_id_set = True doc . annotations [ - 1 ] . spdx_id = spdx_id return True else : raise CardinalityError ( 'Annotation::SPDXREF' ) else : raise OrderError ( 'Annotation::SPDXREF' )
13499	def parse ( s ) : try : m = _regex . match ( s ) t = Tag ( int ( m . group ( 'major' ) ) , int ( m . group ( 'minor' ) ) , int ( m . group ( 'patch' ) ) ) return t if m . group ( 'label' ) is None else t . with_revision ( m . group ( 'label' ) , int ( m . group ( 'number' ) ) ) except AttributeError : return None
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
990	def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )
8066	def get_source ( self , doc ) : start_iter = doc . get_start_iter ( ) end_iter = doc . get_end_iter ( ) source = doc . get_text ( start_iter , end_iter , False ) return source
519	def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
6385	def pairwise_similarity_statistics ( src_collection , tar_collection , metric = sim , mean_func = amean , symmetric = False , ) : if not callable ( mean_func ) : raise ValueError ( 'mean_func must be a function' ) if not callable ( metric ) : raise ValueError ( 'metric must be a function' ) if hasattr ( src_collection , 'split' ) : src_collection = src_collection . split ( ) if not hasattr ( src_collection , '__iter__' ) : raise ValueError ( 'src_collection is neither a string nor iterable' ) if hasattr ( tar_collection , 'split' ) : tar_collection = tar_collection . split ( ) if not hasattr ( tar_collection , '__iter__' ) : raise ValueError ( 'tar_collection is neither a string nor iterable' ) src_collection = list ( src_collection ) tar_collection = list ( tar_collection ) pairwise_values = [ ] for src in src_collection : for tar in tar_collection : pairwise_values . append ( metric ( src , tar ) ) if symmetric : pairwise_values . append ( metric ( tar , src ) ) return ( max ( pairwise_values ) , min ( pairwise_values ) , mean_func ( pairwise_values ) , std ( pairwise_values , mean_func , 0 ) , )
2646	def python_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . python import PythonApp def decorator ( func ) : def wrapper ( f ) : return PythonApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
12504	def smooth_img ( imgs , fwhm , ** kwargs ) : if hasattr ( imgs , "__iter__" ) and not isinstance ( imgs , string_types ) : single_img = False else : single_img = True imgs = [ imgs ] ret = [ ] for img in imgs : img = check_niimg ( img ) affine = img . get_affine ( ) filtered = _smooth_array ( img . get_data ( ) , affine , fwhm = fwhm , ensure_finite = True , copy = True , ** kwargs ) ret . append ( new_img_like ( img , filtered , affine , copy_header = True ) ) if single_img : return ret [ 0 ] else : return ret
7687	def multi_segment ( annotation , sr = 22050 , length = None , ** kwargs ) : PENT = [ 1 , 32. / 27 , 4. / 3 , 3. / 2 , 16. / 9 ] DURATION = 0.1 h_int , _ = hierarchy_flatten ( annotation ) if length is None : length = int ( sr * ( max ( np . max ( _ ) for _ in h_int ) + 1. / DURATION ) + 1 ) y = 0.0 for ints , ( oc , scale ) in zip ( h_int , product ( range ( 3 , 3 + len ( h_int ) ) , PENT ) ) : click = mkclick ( 440.0 * scale * oc , sr = sr , duration = DURATION ) y = y + filter_kwargs ( mir_eval . sonify . clicks , np . unique ( ints ) , fs = sr , length = length , click = click ) return y
3661	def calculate ( self , T , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate ( T ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate ( T ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate ( T ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate ( T ) elif method == COOLPROP : return CoolProp_T_dependent_property ( T , self . CASRN , 'CPMOLAR' , 'l' ) elif method == POLING_CONST : return self . POLING_constant elif method == CRCSTD : return self . CRCSTD_constant elif method == ROWLINSON_POLING : Cpgm = self . Cpgm ( T ) if hasattr ( self . Cpgm , '__call__' ) else self . Cpgm return Rowlinson_Poling ( T , self . Tc , self . omega , Cpgm ) elif method == ROWLINSON_BONDI : Cpgm = self . Cpgm ( T ) if hasattr ( self . Cpgm , '__call__' ) else self . Cpgm return Rowlinson_Bondi ( T , self . Tc , self . omega , Cpgm ) elif method == DADGOSTAR_SHAW : Cp = Dadgostar_Shaw ( T , self . similarity_variable ) return property_mass_to_molar ( Cp , self . MW ) elif method in self . tabular_data : return self . interpolate ( T , method ) else : raise Exception ( 'Method not valid' )
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
3864	async def send_message ( self , segments , image_file = None , image_id = None , image_user_id = None ) : async with self . _send_message_lock : if image_file : try : uploaded_image = await self . _client . upload_image ( image_file , return_uploaded_image = True ) except exceptions . NetworkError as e : logger . warning ( 'Failed to upload image: {}' . format ( e ) ) raise image_id = uploaded_image . image_id try : request = hangouts_pb2 . SendChatMessageRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , message_content = hangouts_pb2 . MessageContent ( segment = [ seg . serialize ( ) for seg in segments ] , ) , ) if image_id is not None : request . existing_media . photo . photo_id = image_id if image_user_id is not None : request . existing_media . photo . user_id = image_user_id request . existing_media . photo . is_custom_user_id = True await self . _client . send_chat_message ( request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to send message: {}' . format ( e ) ) raise
3565	def write_value ( self , value , write_type = 0 ) : data = NSData . dataWithBytes_length_ ( value , len ( value ) ) self . _device . _peripheral . writeValue_forCharacteristic_type_ ( data , self . _characteristic , write_type )
13817	def _ConvertFieldValuePair ( js , message ) : names = [ ] message_descriptor = message . DESCRIPTOR for name in js : try : field = message_descriptor . fields_by_camelcase_name . get ( name , None ) if not field : raise ParseError ( 'Message type "{0}" has no field named "{1}".' . format ( message_descriptor . full_name , name ) ) if name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" fields.' . format ( message . DESCRIPTOR . full_name , name ) ) names . append ( name ) if field . containing_oneof is not None : oneof_name = field . containing_oneof . name if oneof_name in names : raise ParseError ( 'Message type "{0}" should not have multiple "{1}" ' 'oneof fields.' . format ( message . DESCRIPTOR . full_name , oneof_name ) ) names . append ( oneof_name ) value = js [ name ] if value is None : message . ClearField ( field . name ) continue if _IsMapEntry ( field ) : message . ClearField ( field . name ) _ConvertMapFieldValue ( value , message , field ) elif field . label == descriptor . FieldDescriptor . LABEL_REPEATED : message . ClearField ( field . name ) if not isinstance ( value , list ) : raise ParseError ( 'repeated field {0} must be in [] which is ' '{1}.' . format ( name , value ) ) if field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : for item in value : sub_message = getattr ( message , field . name ) . add ( ) if ( item is None and sub_message . DESCRIPTOR . full_name != 'google.protobuf.Value' ) : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) _ConvertMessage ( item , sub_message ) else : for item in value : if item is None : raise ParseError ( 'null is not allowed to be used as an element' ' in a repeated field.' ) getattr ( message , field . name ) . append ( _ConvertScalarFieldValue ( item , field ) ) elif field . cpp_type == descriptor . FieldDescriptor . CPPTYPE_MESSAGE : sub_message = getattr ( message , field . name ) _ConvertMessage ( value , sub_message ) else : setattr ( message , field . name , _ConvertScalarFieldValue ( value , field ) ) except ParseError as e : if field and field . containing_oneof is None : raise ParseError ( 'Failed to parse {0} field: {1}' . format ( name , e ) ) else : raise ParseError ( str ( e ) ) except ValueError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) ) except TypeError as e : raise ParseError ( 'Failed to parse {0} field: {1}.' . format ( name , e ) )
320	def get_max_drawdown_underwater ( underwater ) : valley = np . argmin ( underwater ) peak = underwater [ : valley ] [ underwater [ : valley ] == 0 ] . index [ - 1 ] try : recovery = underwater [ valley : ] [ underwater [ valley : ] == 0 ] . index [ 0 ] except IndexError : recovery = np . nan return peak , valley , recovery
10741	def print_memory ( function ) : import memory_profiler def wrapper ( * args , ** kwargs ) : m = StringIO ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , ** kwargs ) print ( m . getvalue ( ) ) m . close ( ) return output return wrapper
6383	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) start = word [ 0 : 1 ] consonant_part = '' vowel_part = '' for char in word [ 1 : ] : if char != start : if char in self . _vowels : if char not in vowel_part : vowel_part += char elif char not in consonant_part : consonant_part += char return start + consonant_part + vowel_part
13081	def register_filters ( self ) : for _filter , instance in self . _filters : if not instance : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( flask_nemo . filters , _filter ) else : self . app . jinja_env . filters [ _filter . replace ( "f_" , "" ) ] = getattr ( instance , _filter . replace ( "_{}" . format ( instance . name ) , "" ) )
13548	def update ( dst , src ) : stack = [ ( dst , src ) ] def isdict ( o ) : return hasattr ( o , 'keys' ) while stack : current_dst , current_src = stack . pop ( ) for key in current_src : if key not in current_dst : current_dst [ key ] = current_src [ key ] else : if isdict ( current_src [ key ] ) and isdict ( current_dst [ key ] ) : stack . append ( ( current_dst [ key ] , current_src [ key ] ) ) else : current_dst [ key ] = current_src [ key ] return dst
12913	def extend ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot extend to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . extend ( item ) return
4364	def encode ( data , json_dumps = default_json_dumps ) : payload = '' msg = str ( MSG_TYPES [ data [ 'type' ] ] ) if msg in [ '0' , '1' ] : msg += '::' + data [ 'endpoint' ] if 'qs' in data and data [ 'qs' ] != '' : msg += ':' + data [ 'qs' ] elif msg == '2' : msg += '::' elif msg in [ '3' , '4' , '5' ] : if msg == '3' : payload = data [ 'data' ] if msg == '4' : payload = json_dumps ( data [ 'data' ] ) if msg == '5' : d = { } d [ 'name' ] = data [ 'name' ] if 'args' in data and data [ 'args' ] != [ ] : d [ 'args' ] = data [ 'args' ] payload = json_dumps ( d ) if 'id' in data : msg += ':' + str ( data [ 'id' ] ) if data [ 'ack' ] == 'data' : msg += '+' msg += ':' else : msg += '::' if 'endpoint' not in data : data [ 'endpoint' ] = '' if payload != '' : msg += data [ 'endpoint' ] + ':' + payload else : msg += data [ 'endpoint' ] elif msg == '6' : msg += '::' + data . get ( 'endpoint' , '' ) + ':' + str ( data [ 'ackId' ] ) if 'args' in data and data [ 'args' ] != [ ] : msg += '+' + json_dumps ( data [ 'args' ] ) elif msg == '7' : msg += ':::' if 'reason' in data and data [ 'reason' ] != '' : msg += str ( ERROR_REASONS [ data [ 'reason' ] ] ) if 'advice' in data and data [ 'advice' ] != '' : msg += '+' + str ( ERROR_ADVICES [ data [ 'advice' ] ] ) msg += data [ 'endpoint' ] elif msg == '8' : msg += '::' return msg
8786	def diag_port ( self , context , port_id , ** kwargs ) : LOG . info ( "diag_port %s" % port_id ) try : port = self . _client . show_port ( port_id ) except Exception as e : msg = "failed fetching downstream port: %s" % ( str ( e ) ) LOG . exception ( msg ) raise IronicException ( msg = msg ) return { "downstream_port" : port }
9569	def build_message ( self , data ) : if not data : return None return Message ( id = data [ 'message' ] [ 'mid' ] , platform = self . platform , text = data [ 'message' ] [ 'text' ] , user = data [ 'sender' ] [ 'id' ] , timestamp = data [ 'timestamp' ] , raw = data , chat = None , )
12033	def averageSweep ( self , sweepFirst = 0 , sweepLast = None ) : if sweepLast is None : sweepLast = self . sweeps - 1 nSweeps = sweepLast - sweepFirst + 1 runningSum = np . zeros ( len ( self . sweepY ) ) self . log . debug ( "averaging sweep %d to %d" , sweepFirst , sweepLast ) for sweep in np . arange ( nSweeps ) + sweepFirst : self . setsweep ( sweep ) runningSum += self . sweepY . flatten ( ) average = runningSum / nSweeps return average
11386	def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , ** kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
9704	def checkSerial ( self ) : for item in self . rxSerial ( self . _TUN . _tun . mtu ) : try : self . _TUN . _tun . write ( item ) except pytun . Error as error : print ( "pytun error writing: {0}" . format ( item ) ) print ( error )
952	def trainTM ( sequence , timeSteps , noiseLevel ) : currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) ts = 0 for t in range ( timeSteps ) : tm . reset ( ) for k in range ( 4 ) : v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] acc = accuracy ( currentColumns , predictedColumns ) x . append ( ts ) y . append ( acc ) ts += 1 predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ]
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
8451	def is_temple_project ( ) : if not os . path . exists ( temple . constants . TEMPLE_CONFIG_FILE ) : msg = 'No {} file found in repository.' . format ( temple . constants . TEMPLE_CONFIG_FILE ) raise temple . exceptions . InvalidTempleProjectError ( msg )
658	def plotHistogram ( freqCounts , title = 'On-Times Histogram' , xLabel = 'On-Time' ) : import pylab pylab . ion ( ) pylab . figure ( ) pylab . bar ( numpy . arange ( len ( freqCounts ) ) - 0.5 , freqCounts ) pylab . title ( title ) pylab . xlabel ( xLabel )
3977	def _expand_libs_in_libs ( specs ) : for lib_name , lib_spec in specs [ 'libs' ] . iteritems ( ) : if 'depends' in lib_spec and 'libs' in lib_spec [ 'depends' ] : lib_spec [ 'depends' ] [ 'libs' ] = _get_dependent ( 'libs' , lib_name , specs , 'libs' )
2403	def gen_feats ( self , e_set ) : bag_feats = self . gen_bag_feats ( e_set ) length_feats = self . gen_length_feats ( e_set ) prompt_feats = self . gen_prompt_feats ( e_set ) overall_feats = numpy . concatenate ( ( length_feats , prompt_feats , bag_feats ) , axis = 1 ) overall_feats = overall_feats . copy ( ) return overall_feats
13281	def parse ( self , source ) : command_regex = self . _make_command_regex ( self . name ) for match in re . finditer ( command_regex , source ) : self . _logger . debug ( match ) start_index = match . start ( 0 ) yield self . _parse_command ( source , start_index )
5954	def tool_factory ( clsname , name , driver , base = GromacsCommand ) : clsdict = { 'command_name' : name , 'driver' : driver , '__doc__' : property ( base . _get_gmx_docs ) } return type ( clsname , ( base , ) , clsdict )
581	def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] ) if encoder [ "type" ] == "RandomDistributedScalarEncoder" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( "numBuckets" ) ) encoder [ "resolution" ] = resolution
1453	def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
695	def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( "Experiment description file %s does not exist or " + "is not a file" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , "descriptionInterface" ) : raise RuntimeError ( "Experiment description file %s does not define %s" % ( descriptionPyPath , "descriptionInterface" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( "Experiment description file %s defines %s but it " + "is not DescriptionIface-based" ) % ( descriptionPyPath , name ) ) return mod
7086	def _single_true ( iterable ) : iterator = iter ( iterable ) has_true = any ( iterator ) has_another_true = any ( iterator ) return has_true and not has_another_true
11367	def locate ( pattern , root = os . curdir ) : for path , dummy , files in os . walk ( os . path . abspath ( root ) ) : for filename in fnmatch . filter ( files , pattern ) : yield os . path . join ( path , filename )
5537	def get_raw_output ( self , tile , _baselevel_readonly = False ) : if not isinstance ( tile , ( BufferedTile , tuple ) ) : raise TypeError ( "'tile' must be a tuple or BufferedTile" ) if isinstance ( tile , tuple ) : tile = self . config . output_pyramid . tile ( * tile ) if _baselevel_readonly : tile = self . config . baselevels [ "tile_pyramid" ] . tile ( * tile . id ) if tile . zoom not in self . config . zoom_levels : return self . config . output . empty ( tile ) if tile . crs != self . config . process_pyramid . crs : raise NotImplementedError ( "reprojection between processes not yet implemented" ) if self . config . mode == "memory" : process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] return self . _extract ( in_tile = process_tile , in_data = self . _execute_using_cache ( process_tile ) , out_tile = tile ) process_tile = self . config . process_pyramid . intersecting ( tile ) [ 0 ] if tile . pixelbuffer > self . config . output . pixelbuffer : output_tiles = list ( self . config . output_pyramid . tiles_from_bounds ( tile . bounds , tile . zoom ) ) else : output_tiles = self . config . output_pyramid . intersecting ( tile ) if self . config . mode == "readonly" or _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . config . output . empty ( tile ) elif self . config . mode == "continue" and not _baselevel_readonly : if self . config . output . tiles_exist ( process_tile ) : return self . _read_existing_output ( tile , output_tiles ) else : return self . _process_and_overwrite_output ( tile , process_tile ) elif self . config . mode == "overwrite" and not _baselevel_readonly : return self . _process_and_overwrite_output ( tile , process_tile )
2197	def log_part ( self ) : self . cap_stdout . seek ( self . _pos ) text = self . cap_stdout . read ( ) self . _pos = self . cap_stdout . tell ( ) self . parts . append ( text ) self . text = text
8028	def groupify ( function ) : @ wraps ( function ) def wrapper ( paths , * args , ** kwargs ) : groups = { } for path in paths : key = function ( path , * args , ** kwargs ) if key is not None : groups . setdefault ( key , set ( ) ) . add ( path ) return groups return wrapper
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
2872	def all_info_files ( self ) : 'Returns a generator of "Path"s' try : for info_file in list_files_in_dir ( self . info_dir ) : if not os . path . basename ( info_file ) . endswith ( '.trashinfo' ) : self . on_non_trashinfo_found ( ) else : yield info_file except OSError : pass
6565	def xor_gate ( variables , vartype = dimod . BINARY , name = 'XOR' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 ) , ( 0 , 1 , 1 ) , ( 1 , 0 , 1 ) , ( 1 , 1 , 0 ) ] ) def func ( in1 , in2 , out ) : return ( in1 != in2 ) == out else : configs = frozenset ( [ ( - 1 , - 1 , - 1 ) , ( - 1 , + 1 , + 1 ) , ( + 1 , - 1 , + 1 ) , ( + 1 , + 1 , - 1 ) ] ) def func ( in1 , in2 , out ) : return ( ( in1 > 0 ) != ( in2 > 0 ) ) == ( out > 0 ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
6932	def colormagdiagram_cplist ( cplist , outpkl , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist_objectids = [ ] cplist_mags = [ ] cplist_colors = [ ] for cpf in cplist : cpd = _read_checkplot_picklefile ( cpf ) cplist_objectids . append ( cpd [ 'objectid' ] ) thiscp_mags = [ ] thiscp_colors = [ ] for cm1 , cm2 , ym in zip ( color_mag1 , color_mag2 , yaxis_mag ) : if ( ym in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ ym ] is not None ) : thiscp_mags . append ( cpd [ 'objectinfo' ] [ ym ] ) else : thiscp_mags . append ( np . nan ) if ( cm1 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm1 ] is not None and cm2 in cpd [ 'objectinfo' ] and cpd [ 'objectinfo' ] [ cm2 ] is not None ) : thiscp_colors . append ( cpd [ 'objectinfo' ] [ cm1 ] - cpd [ 'objectinfo' ] [ cm2 ] ) else : thiscp_colors . append ( np . nan ) cplist_mags . append ( thiscp_mags ) cplist_colors . append ( thiscp_colors ) cplist_objectids = np . array ( cplist_objectids ) cplist_mags = np . array ( cplist_mags ) cplist_colors = np . array ( cplist_colors ) cmddict = { 'objectids' : cplist_objectids , 'mags' : cplist_mags , 'colors' : cplist_colors , 'color_mag1' : color_mag1 , 'color_mag2' : color_mag2 , 'yaxis_mag' : yaxis_mag } with open ( outpkl , 'wb' ) as outfd : pickle . dump ( cmddict , outfd , pickle . HIGHEST_PROTOCOL ) plt . close ( 'all' ) return cmddict
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
2812	def convert_squeeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting squeeze ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert squeeze by multiple dimensions' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) ) : import tensorflow as tf return tf . squeeze ( x , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
2699	def render_ranks ( graph , ranks , dot_file = "graph.dot" ) : if dot_file : write_dot ( graph , ranks , path = dot_file )
8536	def pop_data ( self , nbytes ) : last_timestamp = 0 data = [ ] for packet in self . pop ( nbytes ) : last_timestamp = packet . timestamp data . append ( packet . data . data ) return '' . join ( data ) , last_timestamp
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : r if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] self . user_methods_P = user_methods_P self . forced_P = forced_P if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
6275	def resolve_loader ( self , meta : ResourceDescription ) : meta . loader_cls = self . get_loader ( meta , raise_on_error = True )
12930	def get_pos ( vcf_line ) : if not vcf_line : return None vcf_data = vcf_line . strip ( ) . split ( '\t' ) return_data = dict ( ) return_data [ 'chrom' ] = CHROM_INDEX [ vcf_data [ 0 ] ] return_data [ 'pos' ] = int ( vcf_data [ 1 ] ) return return_data
12555	def sav_to_pandas_savreader ( input_file ) : from savReaderWriter import SavReader lines = [ ] with SavReader ( input_file , returnHeader = True ) as reader : header = next ( reader ) for line in reader : lines . append ( line ) return pd . DataFrame ( data = lines , columns = header )
3418	def load_matlab_model ( infile_path , variable_name = None , inf = inf ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) data = scipy_io . loadmat ( infile_path ) possible_names = [ ] if variable_name is None : meta_vars = { "__globals__" , "__header__" , "__version__" } possible_names = sorted ( i for i in data if i not in meta_vars ) if len ( possible_names ) == 1 : variable_name = possible_names [ 0 ] if variable_name is not None : return from_mat_struct ( data [ variable_name ] , model_id = variable_name , inf = inf ) for possible_name in possible_names : try : return from_mat_struct ( data [ possible_name ] , model_id = possible_name , inf = inf ) except ValueError : pass raise IOError ( "no COBRA model found" )
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : lhs += token expression = matched . group ( 2 ) else : operator = token rhs = matched . group ( 2 ) break else : matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) if not ( lhs and operator and rhs ) : return if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
4277	def build ( self , force = False ) : "Create the image gallery" if not self . albums : self . logger . warning ( "No albums found." ) return def log_func ( x ) : available_length = get_terminal_size ( ) [ 0 ] - 64 if x and available_length > 10 : return x . name [ : available_length ] else : return "" try : with progressbar ( self . albums . values ( ) , label = "Collecting files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : media_list = [ f for album in albums for f in self . process_dir ( album , force = force ) ] except KeyboardInterrupt : sys . exit ( 'Interrupted' ) bar_opt = { 'label' : "Processing files" , 'show_pos' : True , 'file' : self . progressbar_target } failed_files = [ ] if self . pool : try : with progressbar ( length = len ( media_list ) , ** bar_opt ) as bar : for res in self . pool . imap_unordered ( worker , media_list ) : if res : failed_files . append ( res ) bar . update ( 1 ) self . pool . close ( ) self . pool . join ( ) except KeyboardInterrupt : self . pool . terminate ( ) sys . exit ( 'Interrupted' ) except pickle . PicklingError : self . logger . critical ( "Failed to process files with the multiprocessing feature." " This can be caused by some module import or object " "defined in the settings file, which can't be serialized." , exc_info = True ) sys . exit ( 'Abort' ) else : with progressbar ( media_list , ** bar_opt ) as medias : for media_item in medias : res = process_file ( media_item ) if res : failed_files . append ( res ) if failed_files : self . remove_files ( failed_files ) if self . settings [ 'write_html' ] : album_writer = AlbumPageWriter ( self . settings , index_title = self . title ) album_list_writer = AlbumListPageWriter ( self . settings , index_title = self . title ) with progressbar ( self . albums . values ( ) , label = "%16s" % "Writing files" , item_show_func = log_func , show_eta = False , file = self . progressbar_target ) as albums : for album in albums : if album . albums : if album . medias : self . logger . warning ( "Album %s contains sub-albums and images. " "Please move images to their own sub-album. " "Images in album %s will not be visible." , album . title , album . title ) album_list_writer . write ( album ) else : album_writer . write ( album ) print ( '' ) signals . gallery_build . send ( self )
11401	def create_field ( subfields = None , ind1 = ' ' , ind2 = ' ' , controlfield_value = '' , global_position = - 1 ) : if subfields is None : subfields = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) field = ( subfields , ind1 , ind2 , controlfield_value , global_position ) _check_field_validity ( field ) return field
2552	def attr ( * args , ** kwargs ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : dicts = args + ( kwargs , ) for d in dicts : for attr , value in d . items ( ) : ctx [ - 1 ] . tag . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) else : raise ValueError ( 'not in a tag context' )
4371	def spawn ( self , fn , * args , ** kwargs ) : if hasattr ( self , 'exception_handler_decorator' ) : fn = self . exception_handler_decorator ( fn ) new = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( new ) return new
1311	def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
112	def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )
13486	def minify ( self , css ) : css = css . replace ( "\r\n" , "\n" ) for rule in _REPLACERS [ self . level ] : css = re . compile ( rule [ 0 ] , re . MULTILINE | re . UNICODE | re . DOTALL ) . sub ( rule [ 1 ] , css ) return css
10445	def getchild ( self , window_name , child_name = '' , role = '' , parent = '' ) : matches = [ ] if role : role = re . sub ( ' ' , '_' , role ) self . _windows = { } if parent and ( child_name or role ) : _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) obj = self . _get_object_map ( window_name , parent ) def _get_all_children_under_obj ( obj , child_list ) : if role and obj [ 'class' ] == role : child_list . append ( obj [ 'label' ] ) elif child_name and self . _match_name_to_appmap ( child_name , obj ) : child_list . append ( obj [ 'label' ] ) if obj : children = obj [ 'children' ] if not children : return child_list for child in children . split ( ) : return _get_all_children_under_obj ( appmap [ child ] , child_list ) matches = _get_all_children_under_obj ( obj , [ ] ) if not matches : if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches _window_handle , _window_name = self . _get_window_handle ( window_name ) [ 0 : 2 ] if not _window_handle : raise LdtpServerException ( 'Unable to find window "%s"' % window_name ) appmap = self . _get_appmap ( _window_handle , _window_name ) for name in appmap . keys ( ) : obj = appmap [ name ] if role and not child_name and obj [ 'class' ] == role : matches . append ( name ) if parent and child_name and not role and self . _match_name_to_appmap ( parent , obj ) : matches . append ( name ) if child_name and not role and self . _match_name_to_appmap ( child_name , obj ) : return name matches . append ( name ) if role and child_name and obj [ 'class' ] == role and self . _match_name_to_appmap ( child_name , obj ) : matches . append ( name ) if not matches : _name = '' _role = '' _parent = '' if child_name : _name = 'name "%s" ' % child_name if role : _role = 'role "%s" ' % role if parent : _parent = 'parent "%s"' % parent exception = 'Could not find a child %s%s%s' % ( _name , _role , _parent ) raise LdtpServerException ( exception ) return matches
288	def plot_returns ( returns , live_start_date = None , ax = None ) : if ax is None : ax = plt . gca ( ) ax . set_label ( '' ) ax . set_ylabel ( 'Returns' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index >= live_start_date ] is_returns . plot ( ax = ax , color = 'g' ) oos_returns . plot ( ax = ax , color = 'r' ) else : returns . plot ( ax = ax , color = 'g' ) return ax
12359	def format_parameters ( self , ** kwargs ) : req_data = { } for k , v in kwargs . items ( ) : if isinstance ( v , ( list , tuple ) ) : k = k + '[]' req_data [ k ] = v return req_data
7444	def _step2func ( self , samples , force , ipyclient ) : if self . _headers : print ( "\n Step 2: Filtering reads " ) if not self . samples . keys ( ) : raise IPyradWarningExit ( FIRST_RUN_1 ) samples = _get_samples ( self , samples ) if not force : if all ( [ i . stats . state >= 2 for i in samples ] ) : print ( EDITS_EXIST . format ( len ( samples ) ) ) return assemble . rawedit . run2 ( self , samples , force , ipyclient )
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
6601	def put_package ( self , package ) : self . last_package_index += 1 package_index = self . last_package_index package_fullpath = self . package_fullpath ( package_index ) with gzip . open ( package_fullpath , 'wb' ) as f : pickle . dump ( package , f , protocol = pickle . HIGHEST_PROTOCOL ) f . close ( ) result_fullpath = self . result_fullpath ( package_index ) result_dir = os . path . dirname ( result_fullpath ) alphatwirl . mkdir_p ( result_dir ) return package_index
7662	def slice ( self , start_time , end_time , strict = False ) : sliced_ann = self . trim ( start_time , end_time , strict = strict ) raw_data = sliced_ann . pop_data ( ) for obs in raw_data : new_time = max ( 0 , obs . time - start_time ) sliced_ann . append ( time = new_time , duration = obs . duration , value = obs . value , confidence = obs . confidence ) ref_time = sliced_ann . time slice_start = ref_time slice_end = ref_time + sliced_ann . duration if 'slice' not in sliced_ann . sandbox . keys ( ) : sliced_ann . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ] ) else : sliced_ann . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time , 'slice_start' : slice_start , 'slice_end' : slice_end } ) sliced_ann . time = max ( 0 , ref_time - start_time ) return sliced_ann
13899	def PushPopItem ( obj , key , value ) : if key in obj : old_value = obj [ key ] obj [ key ] = value yield value obj [ key ] = old_value else : obj [ key ] = value yield value del obj [ key ]
6595	def poll ( self ) : ret = self . communicationChannel . receive_finished ( ) self . nruns -= len ( ret ) return ret
7615	def get_datetime ( self , timestamp : str , unix = True ) : time = datetime . strptime ( timestamp , '%Y%m%dT%H%M%S.%fZ' ) if unix : return int ( time . timestamp ( ) ) else : return time
3274	def get_user_info ( self ) : if self . value in ERROR_DESCRIPTIONS : s = "{}" . format ( ERROR_DESCRIPTIONS [ self . value ] ) else : s = "{}" . format ( self . value ) if self . context_info : s += ": {}" . format ( self . context_info ) elif self . value in ERROR_RESPONSES : s += ": {}" . format ( ERROR_RESPONSES [ self . value ] ) if self . src_exception : s += "\n Source exception: '{}'" . format ( self . src_exception ) if self . err_condition : s += "\n Error condition: '{}'" . format ( self . err_condition ) return s
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
8123	def textpath ( self , txt , x , y , width = None , height = 1000000 , enableRendering = False , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , ** kwargs ) path = txt . path if draw : path . draw ( ) return path
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
12304	def post ( self , repo ) : datapackage = repo . package url = self . url token = self . token headers = { 'Authorization' : 'Token {}' . format ( token ) , 'Content-Type' : 'application/json' } try : r = requests . post ( url , data = json . dumps ( datapackage ) , headers = headers ) return r except Exception as e : raise NetworkError ( ) return ""
7169	def remove_entity ( self , name ) : self . entities . remove ( name ) self . padaos . remove_entity ( name )
10949	def get_shares ( self ) : self . shares = self . api . get ( url = PATHS [ 'GET_SHARES' ] % self . url ) [ 'shares' ] return self . shares
3403	def find_boundary_types ( model , boundary_type , external_compartment = None ) : if not model . boundary : LOGGER . warning ( "There are no boundary reactions in this model. " "Therefore specific types of boundary reactions such " "as 'exchanges', 'demands' or 'sinks' cannot be " "identified." ) return [ ] if external_compartment is None : external_compartment = find_external_compartment ( model ) return model . reactions . query ( lambda r : is_boundary_type ( r , boundary_type , external_compartment ) )
4807	def create_char_dataframe ( words ) : char_dict = [ ] for word in words : for i , char in enumerate ( word ) : if i == 0 : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : True } ) else : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : False } ) return pd . DataFrame ( char_dict )
12224	def convertShpToExtend ( pathToShp ) : driver = ogr . GetDriverByName ( 'ESRI Shapefile' ) dataset = driver . Open ( pathToShp ) if dataset is not None : layer = dataset . GetLayer ( ) spatialRef = layer . GetSpatialRef ( ) feature = layer . GetNextFeature ( ) geom = feature . GetGeometryRef ( ) spatialRef = geom . GetSpatialReference ( ) outSpatialRef = osr . SpatialReference ( ) outSpatialRef . ImportFromEPSG ( 4326 ) coordTrans = osr . CoordinateTransformation ( spatialRef , outSpatialRef ) env = geom . GetEnvelope ( ) pointMAX = ogr . Geometry ( ogr . wkbPoint ) pointMAX . AddPoint ( env [ 1 ] , env [ 3 ] ) pointMAX . Transform ( coordTrans ) pointMIN = ogr . Geometry ( ogr . wkbPoint ) pointMIN . AddPoint ( env [ 0 ] , env [ 2 ] ) pointMIN . Transform ( coordTrans ) return [ pointMAX . GetPoint ( ) [ 1 ] , pointMIN . GetPoint ( ) [ 0 ] , pointMIN . GetPoint ( ) [ 1 ] , pointMAX . GetPoint ( ) [ 0 ] ] else : exit ( " shapefile not found. Please verify your path to the shapefile" )
4057	def _csljson_processor ( self , retrieved ) : items = [ ] json_kwargs = { } if self . preserve_json_order : json_kwargs [ "object_pairs_hook" ] = OrderedDict for csl in retrieved . entries : items . append ( json . loads ( csl [ "content" ] [ 0 ] [ "value" ] , ** json_kwargs ) ) self . url_params = None return items
12592	def query_reliabledictionary ( client , application_name , service_name , dictionary_name , query_string , partition_key = None , partition_id = None , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) start = time . time ( ) if ( partition_id != None ) : result = dictionary . query ( query_string , PartitionLookup . ID , partition_id ) elif ( partition_key != None ) : result = dictionary . query ( query_string , PartitionLookup . KEY , partition_key ) else : result = dictionary . query ( query_string ) if type ( result ) is str : print ( result ) return else : result = json . dumps ( result . get ( "value" ) , indent = 4 ) print ( "Query took " + str ( time . time ( ) - start ) + " seconds" ) if ( output_file == None ) : output_file = "{}-{}-{}-query-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( ) print ( 'Printed output to: ' + output_file ) print ( result )
4182	def window_blackman_nuttall ( N ) : r a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
13473	def loop_in_background ( interval , callback ) : loop = GeventLoop ( interval , callback ) loop . start ( ) try : yield loop finally : if loop . has_started ( ) : loop . stop ( )
11794	def mac ( csp , var , value , assignment , removals ) : "Maintain arc consistency." return AC3 ( csp , [ ( X , var ) for X in csp . neighbors [ var ] ] , removals )
11582	def retrieve_url ( self , url ) : try : r = requests . get ( url ) except requests . ConnectionError : raise exceptions . RetrieveError ( 'Connection fail' ) if r . status_code >= 400 : raise exceptions . RetrieveError ( 'Connected, but status code is %s' % ( r . status_code ) ) real_url = r . url content = r . content try : content_type = r . headers [ 'Content-Type' ] except KeyError : content_type , encoding = mimetypes . guess_type ( real_url , strict = False ) self . response = r return content_type . lower ( ) , content
6814	def configure ( self ) : print ( 'env.services:' , self . genv . services ) for service in list ( self . genv . services ) : service = service . strip ( ) . upper ( ) funcs = common . service_configurators . get ( service , [ ] ) if funcs : print ( '!' * 80 ) print ( 'Configuring service %s...' % ( service , ) ) for func in funcs : print ( 'Function:' , func ) if not self . dryrun : func ( )
10096	def create_new_locale ( self , template_id , locale , version_name , subject , text = '' , html = '' , timeout = None ) : payload = { 'locale' : locale , 'name' : version_name , 'subject' : subject } if html : payload [ 'html' ] = html if text : payload [ 'text' ] = text return self . _api_request ( self . TEMPLATES_LOCALES_ENDPOINT % template_id , self . HTTP_POST , payload = payload , timeout = timeout )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
7708	def handle_got_features_event ( self , event ) : server_features = set ( ) logger . debug ( "Checking roster-related features" ) if event . features . find ( FEATURE_ROSTERVER ) is not None : logger . debug ( " Roster versioning available" ) server_features . add ( "versioning" ) if event . features . find ( FEATURE_APPROVALS ) is not None : logger . debug ( " Subscription pre-approvals available" ) server_features . add ( "pre-approvals" ) self . server_features = server_features
6244	def draw ( self , projection_matrix = None , view_matrix = None , camera_matrix = None , time = 0 ) : if self . mesh_program : self . mesh_program . draw ( self , projection_matrix = projection_matrix , view_matrix = view_matrix , camera_matrix = camera_matrix , time = time )
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
1957	def _init_arm_kernel_helpers ( self ) : page_data = bytearray ( b'\xf1\xde\xfd\xe7' * 1024 ) preamble = binascii . unhexlify ( 'ff0300ea' + '650400ea' + 'f0ff9fe5' + '430400ea' + '220400ea' + '810400ea' + '000400ea' + '870400ea' ) __kuser_cmpxchg64 = binascii . unhexlify ( '30002de9' + '08c09de5' + '30009ce8' + '010055e1' + '00005401' + '0100a013' + '0000a003' + '0c008c08' + '3000bde8' + '1eff2fe1' ) __kuser_dmb = binascii . unhexlify ( '5bf07ff5' + '1eff2fe1' ) __kuser_cmpxchg = binascii . unhexlify ( '003092e5' + '000053e1' + '0000a003' + '00108205' + '0100a013' + '1eff2fe1' ) self . _arm_tls_memory = self . current . memory . mmap ( None , 4 , 'rw ' ) __kuser_get_tls = binascii . unhexlify ( '04009FE5' + '010090e8' + '1eff2fe1' ) + struct . pack ( '<I' , self . _arm_tls_memory ) tls_area = b'\x00' * 12 version = struct . pack ( '<I' , 5 ) def update ( address , code ) : page_data [ address : address + len ( code ) ] = code update ( 0x000 , preamble ) update ( 0xf60 , __kuser_cmpxchg64 ) update ( 0xfa0 , __kuser_dmb ) update ( 0xfc0 , __kuser_cmpxchg ) update ( 0xfe0 , __kuser_get_tls ) update ( 0xff0 , tls_area ) update ( 0xffc , version ) self . current . memory . mmap ( 0xffff0000 , len ( page_data ) , 'r x' , page_data )
13741	def cached_httpbl_exempt ( view_func ) : def wrapped_view ( * args , ** kwargs ) : return view_func ( * args , ** kwargs ) wrapped_view . cached_httpbl_exempt = True return wraps ( view_func , assigned = available_attrs ( view_func ) ) ( wrapped_view )
2887	def _try_disconnect ( self , ref ) : with self . lock : weak = [ s [ 0 ] for s in self . weak_subscribers ] try : index = weak . index ( ref ) except ValueError : pass else : self . weak_subscribers . pop ( index )
11925	def parse ( self , source ) : rt , title , title_pic , markdown = libparser . parse ( source ) if rt == - 1 : raise SeparatorNotFound elif rt == - 2 : raise PostTitleNotFound title , title_pic , markdown = map ( to_unicode , ( title , title_pic , markdown ) ) html = self . markdown . render ( markdown ) summary = self . markdown . render ( markdown [ : 200 ] ) return { 'title' : title , 'markdown' : markdown , 'html' : html , 'summary' : summary , 'title_pic' : title_pic }
1028	def b64encode ( s , altchars = None ) : encoded = binascii . b2a_base64 ( s ) [ : - 1 ] if altchars is not None : return encoded . translate ( string . maketrans ( b'+/' , altchars [ : 2 ] ) ) return encoded
9409	def _extract ( data , session = None ) : if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) if data . dtype . names : if data . size == 1 : return _create_struct ( data , session ) return StructArray ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
8172	def limit ( self , max = 30 ) : if abs ( self . vx ) > max : self . vx = self . vx / abs ( self . vx ) * max if abs ( self . vy ) > max : self . vy = self . vy / abs ( self . vy ) * max if abs ( self . vz ) > max : self . vz = self . vz / abs ( self . vz ) * max
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
5310	def translate_rgb_to_ansi_code ( red , green , blue , offset , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' if colormode == terminal . ANSI_8_COLORS or colormode == terminal . ANSI_16_COLORS : color_code = ansi . rgb_to_ansi16 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = color_code + offset - ansi . FOREGROUND_COLOR_OFFSET ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . ANSI_256_COLORS : color_code = ansi . rgb_to_ansi256 ( red , green , blue ) start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};5;{code}' . format ( base = 8 + offset , code = color_code ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code if colormode == terminal . TRUE_COLORS : start_code = ansi . ANSI_ESCAPE_CODE . format ( code = '{base};2;{red};{green};{blue}' . format ( base = 8 + offset , red = red , green = green , blue = blue ) ) end_code = ansi . ANSI_ESCAPE_CODE . format ( code = offset + ansi . COLOR_CLOSE_OFFSET ) return start_code , end_code raise ColorfulError ( 'invalid color mode "{0}"' . format ( colormode ) )
9649	def determine_paths ( self , package_name = None , create_package_dir = False , dry_run = False ) : self . project_dir = Path ( os . getenv ( 'PWD' ) or os . getcwd ( ) ) distribution = self . get_distribution ( ) if distribution : self . project_name = distribution . get_name ( ) else : self . project_name = self . project_dir . name if os . path . isdir ( self . project_dir / "src" ) : package_search_dir = self . project_dir / "src" else : package_search_dir = self . project_dir created_package_dir = False if not package_name : package_name = self . project_name . replace ( "-" , "_" ) def get_matches ( name ) : possibles = [ n for n in os . listdir ( package_search_dir ) if os . path . isdir ( package_search_dir / n ) ] return difflib . get_close_matches ( name , possibles , n = 1 , cutoff = 0.8 ) close = get_matches ( package_name ) if not close and "_" in package_name : short_package_name = "_" . join ( package_name . split ( "_" ) [ 1 : ] ) close = get_matches ( short_package_name ) if not close : if create_package_dir : package_dir = package_search_dir / package_name created_package_dir = True if not dry_run : print ( "Creating package directory at %s" % package_dir ) os . mkdir ( package_dir ) else : print ( "Would have created package directory at %s" % package_dir ) else : raise CommandError ( "Could not guess the package name. Specify it using --name." ) else : package_name = close [ 0 ] self . package_name = package_name self . package_dir = package_search_dir / package_name if not os . path . exists ( self . package_dir ) and not created_package_dir : raise CommandError ( "Package directory did not exist at %s. Perhaps specify it using --name" % self . package_dir )
13652	def pre ( self , command , output_dir , vars ) : vars [ 'license_name' ] = 'Apache' vars [ 'year' ] = time . strftime ( '%Y' , time . localtime ( ) )
152	def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]
835	def clear ( self ) : self . _Memory = None self . _numPatterns = 0 self . _M = None self . _categoryList = [ ] self . _partitionIdList = [ ] self . _partitionIdMap = { } self . _finishedLearning = False self . _iterationIdx = - 1 if self . maxStoredPatterns > 0 : assert self . useSparseMemory , ( "Fixed capacity KNN is implemented only " "in the sparse memory mode" ) self . fixedCapacity = True self . _categoryRecencyList = [ ] else : self . fixedCapacity = False self . _protoSizes = None self . _s = None self . _vt = None self . _nc = None self . _mean = None self . _specificIndexTraining = False self . _nextTrainingIndices = None
769	def _addResults ( self , results ) : if self . __isTemporal : shiftedInferences = self . __inferenceShifter . shift ( results ) . inferences self . __currentResult = copy . deepcopy ( results ) self . __currentResult . inferences = shiftedInferences self . __currentInference = shiftedInferences else : self . __currentResult = copy . deepcopy ( results ) self . __currentInference = copy . deepcopy ( results . inferences ) self . __currentGroundTruth = copy . deepcopy ( results )
8801	def run_migrations_online ( ) : engine = create_engine ( neutron_config . database . connection , poolclass = pool . NullPool ) connection = engine . connect ( ) context . configure ( connection = connection , target_metadata = target_metadata ) try : with context . begin_transaction ( ) : context . run_migrations ( ) finally : connection . close ( )
2456	def set_pkg_license_from_file ( self , doc , lic ) : self . assert_package_exists ( ) if validations . validate_lics_from_file ( lic ) : doc . package . licenses_from_files . append ( lic ) return True else : raise SPDXValueError ( 'Package::LicensesFromFile' )
10677	def Cp ( self , T ) : result = 0.0 for c , e in zip ( self . _coefficients , self . _exponents ) : result += c * T ** e return result
7173	def calc_intent ( self , query ) : matches = self . calc_intents ( query ) if len ( matches ) == 0 : return MatchData ( '' , '' ) best_match = max ( matches , key = lambda x : x . conf ) best_matches = ( match for match in matches if match . conf == best_match . conf ) return min ( best_matches , key = lambda x : sum ( map ( len , x . matches . values ( ) ) ) )
6653	def start ( self , builddir , program , forward_args ) : child = None try : prog_path = self . findProgram ( builddir , program ) if prog_path is None : return start_env , start_vars = self . buildProgEnvAndVars ( prog_path , builddir ) if self . getScript ( 'start' ) : cmd = [ os . path . expandvars ( string . Template ( x ) . safe_substitute ( ** start_vars ) ) for x in self . getScript ( 'start' ) ] + forward_args else : cmd = shlex . split ( './' + prog_path ) + forward_args logger . debug ( 'starting program: %s' , cmd ) child = subprocess . Popen ( cmd , cwd = builddir , env = start_env ) child . wait ( ) if child . returncode : return "process exited with status %s" % child . returncode child = None except OSError as e : import errno if e . errno == errno . ENOEXEC : return ( "the program %s cannot be run (perhaps your target " + "needs to define a 'start' script to start it on its " "intended execution target?)" ) % prog_path finally : if child is not None : _tryTerminate ( child )
5421	def _get_job_metadata ( provider , user_id , job_name , script , task_ids , user_project , unique_job_id ) : create_time = dsub_util . replace_timezone ( datetime . datetime . now ( ) , tzlocal ( ) ) user_id = user_id or dsub_util . get_os_user ( ) job_metadata = provider . prepare_job_metadata ( script . name , job_name , user_id , create_time ) if unique_job_id : job_metadata [ 'job-id' ] = uuid . uuid4 ( ) . hex job_metadata [ 'create-time' ] = create_time job_metadata [ 'script' ] = script job_metadata [ 'user-project' ] = user_project if task_ids : job_metadata [ 'task-ids' ] = dsub_util . compact_interval_string ( list ( task_ids ) ) return job_metadata
8436	def map ( cls , x , palette , limits , na_value = None ) : n = len ( limits ) pal = palette ( n ) [ match ( x , limits ) ] try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
5344	def compose_gerrit ( projects ) : git_projects = [ project for project in projects if 'git' in projects [ project ] ] for project in git_projects : repos = [ repo for repo in projects [ project ] [ 'git' ] if 'gitroot' in repo ] if len ( repos ) > 0 : projects [ project ] [ 'gerrit' ] = [ ] for repo in repos : gerrit_project = repo . replace ( "http://git.eclipse.org/gitroot/" , "" ) gerrit_project = gerrit_project . replace ( ".git" , "" ) projects [ project ] [ 'gerrit' ] . append ( "git.eclipse.org_" + gerrit_project ) return projects
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
5345	def compose_git ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'source_repo' ] ) > 0 ] : repos = [ ] for url in data [ p ] [ 'source_repo' ] : if len ( url [ 'url' ] . split ( ) ) > 1 : repo = url [ 'url' ] . split ( ) [ 1 ] . replace ( '/c/' , '/gitroot/' ) else : repo = url [ 'url' ] . replace ( '/c/' , '/gitroot/' ) if repo not in repos : repos . append ( repo ) projects [ p ] [ 'git' ] = repos return projects
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
10565	def exclude_filepaths ( filepaths , exclude_patterns = None ) : if not exclude_patterns : return filepaths , [ ] exclude_re = re . compile ( "|" . join ( pattern for pattern in exclude_patterns ) ) included_songs = [ ] excluded_songs = [ ] for filepath in filepaths : if exclude_patterns and exclude_re . search ( filepath ) : excluded_songs . append ( filepath ) else : included_songs . append ( filepath ) return included_songs , excluded_songs
1222	def processed_shape ( self , shape ) : for processor in self . preprocessors : shape = processor . processed_shape ( shape = shape ) return shape
13889	def ListMappedNetworkDrives ( ) : if sys . platform != 'win32' : raise NotImplementedError drives_list = [ ] netuse = _CallWindowsNetCommand ( [ 'use' ] ) for line in netuse . split ( EOL_STYLE_WINDOWS ) : match = re . match ( "(\w*)\s+(\w:)\s+(.+)" , line . rstrip ( ) ) if match : drives_list . append ( ( match . group ( 2 ) , match . group ( 3 ) , match . group ( 1 ) == 'OK' ) ) return drives_list
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
3375	def add_absolute_expression ( model , expression , name = "abs_var" , ub = None , difference = 0 , add = True ) : Components = namedtuple ( 'Components' , [ 'variable' , 'upper_constraint' , 'lower_constraint' ] ) variable = model . problem . Variable ( name , lb = 0 , ub = ub ) upper_constraint = model . problem . Constraint ( expression - variable , ub = difference , name = "abs_pos_" + name ) , lower_constraint = model . problem . Constraint ( expression + variable , lb = difference , name = "abs_neg_" + name ) to_add = Components ( variable , upper_constraint , lower_constraint ) if add : add_cons_vars_to_problem ( model , to_add ) return to_add
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
2597	def can ( obj ) : import_needed = False for cls , canner in iteritems ( can_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif istype ( obj , cls ) : return canner ( obj ) if import_needed : _import_mapping ( can_map , _original_can_map ) return can ( obj ) return obj
10753	def open_archive ( fs_url , archive ) : it = pkg_resources . iter_entry_points ( 'fs.archive.open_archive' ) entry_point = next ( ( ep for ep in it if archive . endswith ( ep . name ) ) , None ) if entry_point is None : raise UnsupportedProtocol ( 'unknown archive extension: {}' . format ( archive ) ) try : archive_opener = entry_point . load ( ) except pkg_resources . DistributionNotFound as df : six . raise_from ( UnsupportedProtocol ( 'extension {} requires {}' . format ( entry_point . name , df . req ) ) , None ) try : binfile = None archive_fs = None fs = open_fs ( fs_url ) if issubclass ( archive_opener , base . ArchiveFS ) : try : binfile = fs . openbin ( archive , 'r+' ) except errors . ResourceNotFound : binfile = fs . openbin ( archive , 'w' ) except errors . ResourceReadOnly : binfile = fs . openbin ( archive , 'r' ) archive_opener = archive_opener . _read_fs_cls elif issubclass ( archive_opener , base . ArchiveReadFS ) : binfile = fs . openbin ( archive , 'r' ) if not hasattr ( binfile , 'name' ) : binfile . name = basename ( archive ) archive_fs = archive_opener ( binfile ) except Exception : getattr ( archive_fs , 'close' , lambda : None ) ( ) getattr ( binfile , 'close' , lambda : None ) ( ) raise else : return archive_fs
5923	def get_configuration ( filename = CONFIGNAME ) : global cfg , configuration cfg = GMXConfigParser ( filename = filename ) globals ( ) . update ( cfg . configuration ) configuration = cfg . configuration return cfg
3897	def generate_message_doc ( message_descriptor , locations , path , name_prefix = '' ) : prefixed_name = name_prefix + message_descriptor . name print ( make_subsection ( prefixed_name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for field_index , field in enumerate ( message_descriptor . field ) : field_location = locations [ path + ( 2 , field_index ) ] if field . type not in [ 11 , 14 ] : type_str = TYPE_TO_STR [ field . type ] else : type_str = make_link ( field . type_name . lstrip ( '.' ) ) row_tuples . append ( ( make_code ( field . name ) , field . number , type_str , LABEL_TO_STR [ field . label ] , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Field' , 'Number' , 'Type' , 'Label' , 'Description' ) , row_tuples ) nested_types = enumerate ( message_descriptor . nested_type ) for index , nested_message_desc in nested_types : generate_message_doc ( nested_message_desc , locations , path + ( 3 , index ) , name_prefix = prefixed_name + '.' ) for index , nested_enum_desc in enumerate ( message_descriptor . enum_type ) : generate_enum_doc ( nested_enum_desc , locations , path + ( 4 , index ) , name_prefix = prefixed_name + '.' )
5133	def generate_random_graph ( num_vertices = 250 , prob_loop = 0.5 , ** kwargs ) : g = minimal_random_graph ( num_vertices , ** kwargs ) for v in g . nodes ( ) : e = ( v , v ) if not g . is_edge ( e ) : if np . random . uniform ( ) < prob_loop : g . add_edge ( * e ) g = set_types_random ( g , ** kwargs ) return g
2332	def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
11149	def rename ( self , key : Any , new_key : Any ) : if new_key == key : return required_locks = [ self . _key_locks [ key ] , self . _key_locks [ new_key ] ] ordered_required_locks = sorted ( required_locks , key = lambda x : id ( x ) ) for lock in ordered_required_locks : lock . acquire ( ) try : if key not in self . _data : raise KeyError ( "Attribute to rename \"%s\" does not exist" % key ) self . _data [ new_key ] = self [ key ] del self . _data [ key ] finally : for lock in required_locks : lock . release ( )
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
7125	def get_download_total ( rows ) : headers = rows . pop ( 0 ) index = headers . index ( 'download_count' ) total_downloads = sum ( int ( row [ index ] ) for row in rows ) rows . insert ( 0 , headers ) return total_downloads , index
13060	def get_reffs ( self , objectId , subreference = None , collection = None , export_collection = False ) : if collection is not None : text = collection else : text = self . get_collection ( objectId ) reffs = self . chunk ( text , lambda level : self . resolver . getReffs ( objectId , level = level , subreference = subreference ) ) if export_collection is True : return text , reffs return reffs
8137	def brightness ( self , value = 1.0 ) : b = ImageEnhance . Brightness ( self . img ) self . img = b . enhance ( value )
5037	def _handle_bulk_upload ( cls , enterprise_customer , manage_learners_form , request , email_list = None ) : errors = [ ] emails = set ( ) already_linked_emails = [ ] duplicate_emails = [ ] csv_file = manage_learners_form . cleaned_data [ ManageLearnersForm . Fields . BULK_UPLOAD ] if email_list : parsed_csv = [ { ManageLearnersForm . CsvColumns . EMAIL : email } for email in email_list ] else : parsed_csv = parse_csv ( csv_file , expected_columns = { ManageLearnersForm . CsvColumns . EMAIL } ) try : for index , row in enumerate ( parsed_csv ) : email = row [ ManageLearnersForm . CsvColumns . EMAIL ] try : already_linked = validate_email_to_link ( email , ignore_existing = True ) except ValidationError as exc : message = _ ( "Error at line {line}: {message}\n" ) . format ( line = index + 1 , message = exc ) errors . append ( message ) else : if already_linked : already_linked_emails . append ( ( email , already_linked . enterprise_customer ) ) elif email in emails : duplicate_emails . append ( email ) else : emails . add ( email ) except ValidationError as exc : errors . append ( exc ) if errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . GENERAL_ERRORS , ValidationMessages . BULK_LINK_FAILED ) for error in errors : manage_learners_form . add_error ( ManageLearnersForm . Fields . BULK_UPLOAD , error ) return for email in emails : EnterpriseCustomerUser . objects . link_user ( enterprise_customer , email ) count = len ( emails ) messages . success ( request , ungettext ( "{count} new learner was added to {enterprise_customer_name}." , "{count} new learners were added to {enterprise_customer_name}." , count ) . format ( count = count , enterprise_customer_name = enterprise_customer . name ) ) this_customer_linked_emails = [ email for email , customer in already_linked_emails if customer == enterprise_customer ] other_customer_linked_emails = [ email for email , __ in already_linked_emails if email not in this_customer_linked_emails ] if this_customer_linked_emails : messages . warning ( request , _ ( "The following learners were already associated with this Enterprise " "Customer: {list_of_emails}" ) . format ( list_of_emails = ", " . join ( this_customer_linked_emails ) ) ) if other_customer_linked_emails : messages . warning ( request , _ ( "The following learners are already associated with " "another Enterprise Customer. These learners were not " "added to {enterprise_customer_name}: {list_of_emails}" ) . format ( enterprise_customer_name = enterprise_customer . name , list_of_emails = ", " . join ( other_customer_linked_emails ) , ) ) if duplicate_emails : messages . warning ( request , _ ( "The following duplicate email addresses were not added: " "{list_of_emails}" ) . format ( list_of_emails = ", " . join ( duplicate_emails ) ) ) all_processable_emails = list ( emails ) + this_customer_linked_emails return all_processable_emails
2281	def to_csv ( self , fname_radical , ** kwargs ) : if self . data is not None : self . data . to_csv ( fname_radical + '_data.csv' , index = False , ** kwargs ) pd . DataFrame ( self . adjacency_matrix ) . to_csv ( fname_radical + '_target.csv' , index = False , ** kwargs ) else : raise ValueError ( "Graph has not yet been generated. \ Use self.generate() to do so." )
2187	def ensure ( self , func , * args , ** kwargs ) : r data = self . tryload ( ) if data is None : data = func ( * args , ** kwargs ) self . save ( data ) return data
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
8688	def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
5273	def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
2140	def disassociate ( self , group , parent , ** kwargs ) : parent_id = self . lookup_with_inventory ( parent , kwargs . get ( 'inventory' , None ) ) [ 'id' ] group_id = self . lookup_with_inventory ( group , kwargs . get ( 'inventory' , None ) ) [ 'id' ] return self . _disassoc ( 'children' , parent_id , group_id )
11146	def get_file_info ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) fileName = os . path . basename ( relativePath ) isRepoFile , fileOnDisk , infoOnDisk , classOnDisk = self . is_repository_file ( relativePath ) if not isRepoFile : return None , "file is not a registered repository file." if not infoOnDisk : return None , "file is a registered repository file but info file missing" fileInfoPath = os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % fileName ) try : with open ( fileInfoPath , 'rb' ) as fd : info = pickle . load ( fd ) except Exception as err : return None , "Unable to read file info from disk (%s)" % str ( err ) return info , ''
10319	def _microcanonical_average_max_cluster_size ( max_cluster_size , alpha ) : ret = dict ( ) runs = max_cluster_size . size sqrt_n = np . sqrt ( runs ) max_cluster_size_sample_mean = max_cluster_size . mean ( ) ret [ 'max_cluster_size' ] = max_cluster_size_sample_mean max_cluster_size_sample_std = max_cluster_size . std ( ddof = 1 ) if max_cluster_size_sample_std : old_settings = np . seterr ( all = 'raise' ) ret [ 'max_cluster_size_ci' ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = max_cluster_size_sample_mean , scale = max_cluster_size_sample_std / sqrt_n ) np . seterr ( ** old_settings ) else : ret [ 'max_cluster_size_ci' ] = ( max_cluster_size_sample_mean * np . ones ( 2 ) ) return ret
183	def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )
8614	def create_volume ( self , datacenter_id , volume ) : data = ( json . dumps ( self . _create_volume_dict ( volume ) ) ) response = self . _perform_request ( url = '/datacenters/%s/volumes' % datacenter_id , method = 'POST' , data = data ) return response
13729	def balance_over_time ( address ) : forged_blocks = None txhistory = Address . transactions ( address ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) balance_over_time = [ ] balance = 0 block = 0 Balance = namedtuple ( 'balance' , 'timestamp amount' ) for tx in txhistory : if forged_blocks : while forged_blocks [ block ] . timestamp <= tx . timestamp : balance += ( forged_blocks [ block ] . reward + forged_blocks [ block ] . totalFee ) balance_over_time . append ( Balance ( timestamp = forged_blocks [ block ] . timestamp , amount = balance ) ) block += 1 if tx . senderId == address : balance -= ( tx . amount + tx . fee ) res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if tx . recipientId == address : balance += tx . amount res = Balance ( timestamp = tx . timestamp , amount = balance ) balance_over_time . append ( res ) if forged_blocks and block <= len ( forged_blocks ) - 1 : if forged_blocks [ block ] . timestamp > txhistory [ - 1 ] . timestamp : for i in forged_blocks [ block : ] : balance += ( i . reward + i . totalFee ) res = Balance ( timestamp = i . timestamp , amount = balance ) balance_over_time . append ( res ) return balance_over_time
13247	def get_lsst_bibtex ( bibtex_filenames = None ) : logger = logging . getLogger ( __name__ ) if bibtex_filenames is None : bibtex_names = KNOWN_LSSTTEXMF_BIB_NAMES else : bibtex_names = [ ] for filename in bibtex_filenames : name = os . path . basename ( os . path . splitext ( filename ) [ 0 ] ) if name not in KNOWN_LSSTTEXMF_BIB_NAMES : logger . warning ( '%r is not a known lsst-texmf bib file' , name ) continue bibtex_names . append ( name ) uncached_names = [ name for name in bibtex_names if name not in _LSSTTEXMF_BIB_CACHE ] if len ( uncached_names ) > 0 : loop = asyncio . get_event_loop ( ) future = asyncio . ensure_future ( _download_lsst_bibtex ( uncached_names ) ) loop . run_until_complete ( future ) for name , text in zip ( bibtex_names , future . result ( ) ) : _LSSTTEXMF_BIB_CACHE [ name ] = text return { name : _LSSTTEXMF_BIB_CACHE [ name ] for name in bibtex_names }
623	def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index
797	def jobUpdateResults ( self , jobID , results ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), ' ' results=%%s ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ results , jobID ] )
12828	def log_error ( self , text : str ) -> None : if self . log_errors : with self . _log_fp . open ( 'a+' ) as log_file : log_file . write ( f'{text}\n' )
871	def edit ( cls , properties ) : copyOfProperties = copy ( properties ) configFilePath = cls . getPath ( ) try : with open ( configFilePath , 'r' ) as fp : contents = fp . read ( ) except IOError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s reading custom configuration store " "from %s, while editing properties %s." , e . errno , configFilePath , properties ) raise contents = '<configuration/>' try : elements = ElementTree . XML ( contents ) ElementTree . tostring ( elements ) except Exception , e : msg = "File contents of custom configuration is corrupt. File " "location: %s; Contents: '%s'. Original Error (%s): %s." % ( configFilePath , contents , type ( e ) , e ) _getLogger ( ) . exception ( msg ) raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] if elements . tag != 'configuration' : e = "Expected top-level element to be 'configuration' but got '%s'" % ( elements . tag ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyItem in elements . findall ( './property' ) : propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) name = propInfo [ 'name' ] if name in copyOfProperties : foundValues = propertyItem . findall ( './value' ) if len ( foundValues ) > 0 : foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) if not copyOfProperties : break else : e = "Property %s missing value tag." % ( name , ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyName , value in copyOfProperties . iteritems ( ) : newProp = ElementTree . Element ( 'property' ) nameTag = ElementTree . Element ( 'name' ) nameTag . text = propertyName newProp . append ( nameTag ) valueTag = ElementTree . Element ( 'value' ) valueTag . text = str ( value ) newProp . append ( valueTag ) elements . append ( newProp ) try : makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) with open ( configFilePath , 'w' ) as fp : fp . write ( ElementTree . tostring ( elements ) ) except Exception , e : _getLogger ( ) . exception ( "Error while saving custom configuration " "properties %s in %s." , properties , configFilePath ) raise
5694	def create_table ( self , conn ) : cur = conn . cursor ( ) if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : cur . execute ( self . tabledef ) conn . commit ( )
5118	def get_queue_data ( self , queues = None , edge = None , edge_type = None , return_header = False ) : queues = _get_queues ( self . g , queues , edge , edge_type ) data = np . zeros ( ( 0 , 6 ) ) for q in queues : dat = self . edge2queue [ q ] . fetch_data ( ) if len ( dat ) > 0 : data = np . vstack ( ( data , dat ) ) if return_header : return data , 'arrival,service,departure,num_queued,num_total,q_id' return data
9999	def cells_to_series ( cells , args ) : paramlen = len ( cells . formula . parameters ) is_multidx = paramlen > 1 if len ( cells . data ) == 0 : data = { } indexes = None elif paramlen == 0 : data = list ( cells . data . values ( ) ) indexes = [ np . nan ] else : if len ( args ) > 0 : defaults = tuple ( param . default for param in cells . formula . signature . parameters . values ( ) ) updated_args = [ ] for arg in args : if len ( arg ) > paramlen : arg = arg [ : paramlen ] elif len ( arg ) < paramlen : arg += defaults [ len ( arg ) : ] updated_args . append ( arg ) items = [ ( arg , cells . data [ arg ] ) for arg in updated_args if arg in cells . data ] else : items = [ ( key , value ) for key , value in cells . data . items ( ) ] if not is_multidx : items = [ ( key [ 0 ] , value ) for key , value in items ] if len ( items ) == 0 : indexes , data = None , { } else : indexes , data = zip ( * items ) if is_multidx : indexes = pd . MultiIndex . from_tuples ( indexes ) result = pd . Series ( data = data , name = cells . name , index = indexes ) if indexes is not None and any ( i is not np . nan for i in indexes ) : result . index . names = list ( cells . formula . parameters ) return result
3309	def _run_wsgiref ( app , config , mode ) : from wsgiref . simple_server import make_server , software_version version = "WsgiDAV/{} {}" . format ( __version__ , software_version ) _logger . info ( "Running {}..." . format ( version ) ) _logger . warning ( "WARNING: This single threaded server (wsgiref) is not meant for production." ) httpd = make_server ( config [ "host" ] , config [ "port" ] , app ) try : httpd . serve_forever ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
4991	def post ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , ** kwargs )
13707	def squeeze_words ( line , width = 60 ) : while ( ' ' in line ) and ( len ( line ) > width ) : head , _ , tail = line . rpartition ( ' ' ) line = ' ' . join ( ( head , tail ) ) return line
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
1350	def write_json_response ( self , response ) : self . write ( tornado . escape . json_encode ( response ) ) self . set_header ( "Content-Type" , "application/json" )
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size dest . write ( src . read ( ) ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
13910	def check_path_action ( self ) : class CheckPathAction ( argparse . Action ) : def __call__ ( self , parser , args , value , option_string = None ) : if type ( value ) is list : value = value [ 0 ] user_value = value if option_string == 'None' : if not os . path . isdir ( value ) : _current_user = os . path . expanduser ( "~" ) if not value . startswith ( _current_user ) and not value . startswith ( os . getcwd ( ) ) : if os . path . isdir ( os . path . join ( _current_user , value ) ) : value = os . path . join ( _current_user , value ) elif os . path . isdir ( os . path . join ( os . getcwd ( ) , value ) ) : value = os . path . join ( os . getcwd ( ) , value ) else : value = None else : value = None elif option_string == '--template-name' : if not os . path . isdir ( value ) : if not os . path . isdir ( os . path . join ( args . target , value ) ) : value = None if not value : logger . error ( "Could not to find path %s. Please provide " "correct path to %s option" , user_value , option_string ) exit ( 1 ) setattr ( args , self . dest , value ) return CheckPathAction
3895	def print_table ( col_tuple , row_tuples ) : col_widths = [ max ( len ( str ( row [ col ] ) ) for row in [ col_tuple ] + row_tuples ) for col in range ( len ( col_tuple ) ) ] format_str = ' ' . join ( '{{:<{}}}' . format ( col_width ) for col_width in col_widths ) header_border = ' ' . join ( '=' * col_width for col_width in col_widths ) print ( header_border ) print ( format_str . format ( * col_tuple ) ) print ( header_border ) for row_tuple in row_tuples : print ( format_str . format ( * row_tuple ) ) print ( header_border ) print ( )
1334	def backward ( self , gradient , image = None , strict = True ) : assert self . has_gradient ( ) assert gradient . ndim == 1 if image is None : image = self . __original_image assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . backward ( gradient , image ) assert gradient . shape == image . shape return gradient
3579	def clear_cached_data ( self ) : for device in self . list_devices ( ) : if device . is_connected : continue adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) adapter . RemoveDevice ( device . _device . object_path )
409	def _tf_batch_map_coordinates ( self , inputs , coords ) : input_shape = inputs . get_shape ( ) coords_shape = coords . get_shape ( ) batch_channel = tf . shape ( inputs ) [ 0 ] input_h = int ( input_shape [ 1 ] ) input_w = int ( input_shape [ 2 ] ) kernel_n = int ( coords_shape [ 3 ] ) n_coords = input_h * input_w * kernel_n coords_lt = tf . cast ( tf . floor ( coords ) , 'int32' ) coords_rb = tf . cast ( tf . ceil ( coords ) , 'int32' ) coords_lb = tf . stack ( [ coords_lt [ : , : , : , : , 0 ] , coords_rb [ : , : , : , : , 1 ] ] , axis = - 1 ) coords_rt = tf . stack ( [ coords_rb [ : , : , : , : , 0 ] , coords_lt [ : , : , : , : , 1 ] ] , axis = - 1 ) idx = self . _tf_repeat ( tf . range ( batch_channel ) , n_coords ) vals_lt = self . _get_vals_by_coords ( inputs , coords_lt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rb = self . _get_vals_by_coords ( inputs , coords_rb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_lb = self . _get_vals_by_coords ( inputs , coords_lb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rt = self . _get_vals_by_coords ( inputs , coords_rt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) coords_offset_lt = coords - tf . cast ( coords_lt , 'float32' ) vals_t = vals_lt + ( vals_rt - vals_lt ) * coords_offset_lt [ : , : , : , : , 0 ] vals_b = vals_lb + ( vals_rb - vals_lb ) * coords_offset_lt [ : , : , : , : , 0 ] mapped_vals = vals_t + ( vals_b - vals_t ) * coords_offset_lt [ : , : , : , : , 1 ] return mapped_vals
7857	def __error ( self , stanza ) : try : self . error ( stanza . get_error ( ) ) except ProtocolError : from . . error import StanzaErrorNode self . error ( StanzaErrorNode ( "undefined-condition" ) )
3159	def get_metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . RequestException as e : raise e else : r . raise_for_status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . RequestException ( output [ 'error' ] ) return output
735	def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )
12148	def convertImages ( self ) : exts = [ '.jpg' , '.png' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_jpg_" + fname if not fname2 in self . files2 : self . log . info ( "copying over [%s]" % fname2 ) shutil . copy ( os . path . join ( self . folder1 , fname ) , os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname ) exts = [ '.tif' , '.tiff' ] for fname in [ x for x in self . files1 if cm . ext ( x ) in exts ] : ID = "UNKNOWN" if len ( fname ) > 8 and fname [ : 8 ] in self . IDs : ID = fname [ : 8 ] fname2 = ID + "_tif_" + fname + ".jpg" if not fname2 in self . files2 : self . log . info ( "converting micrograph [%s]" % fname2 ) imaging . TIF_to_jpg ( os . path . join ( self . folder1 , fname ) , saveAs = os . path . join ( self . folder2 , fname2 ) ) if not fname [ : 8 ] + ".abf" in self . files1 : self . log . error ( "orphan image: %s" , fname )
3489	def _sbase_notes_dict ( sbase , notes ) : if notes and len ( notes ) > 0 : tokens = [ '<html xmlns = "http://www.w3.org/1999/xhtml" >' ] + [ "<p>{}: {}</p>" . format ( k , v ) for ( k , v ) in notes . items ( ) ] + [ "</html>" ] _check ( sbase . setNotes ( "\n" . join ( tokens ) ) , "Setting notes on sbase: {}" . format ( sbase ) )
9208	def remove_prefix ( bytes_ ) : prefix_int = extract_prefix ( bytes_ ) prefix = varint . encode ( prefix_int ) return bytes_ [ len ( prefix ) : ]
11621	def set_script ( self , i ) : if i in range ( 1 , 10 ) : n = i - 1 else : raise IllegalInput ( "Invalid Value for ATR %s" % ( hex ( i ) ) ) if n > - 1 : self . curr_script = n self . delta = n * DELTA return
2406	def get_algorithms ( algorithm ) : if algorithm == util_functions . AlgorithmTypes . classification : clf = sklearn . ensemble . GradientBoostingClassifier ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) clf2 = sklearn . ensemble . GradientBoostingClassifier ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) else : clf = sklearn . ensemble . GradientBoostingRegressor ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) clf2 = sklearn . ensemble . GradientBoostingRegressor ( n_estimators = 100 , learn_rate = .05 , max_depth = 4 , random_state = 1 , min_samples_leaf = 3 ) return clf , clf2
5507	def _image_name_from_url ( url ) : find = r'https?://|[^\w]' replace = '_' return re . sub ( find , replace , url ) . strip ( '_' )
2950	def _start ( self , my_task , force = False ) : if my_task . _has_state ( Task . COMPLETED ) : return True , None if my_task . _has_state ( Task . READY ) : return True , None if self . split_task is None : return self . _check_threshold_unstructured ( my_task , force ) return self . _check_threshold_structured ( my_task , force )
5211	def bds ( tickers , flds , ** kwargs ) : logger = logs . get_logger ( bds , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) con , _ = create_connection ( ) ovrds = assist . proc_ovrds ( ** kwargs ) logger . info ( f'loading block data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) data = con . bulkref ( tickers = tickers , flds = flds , ovrds = ovrds ) if not kwargs . get ( 'cache' , False ) : return [ data ] qry_data = [ ] for ( ticker , fld ) , grp in data . groupby ( [ 'ticker' , 'field' ] ) : data_file = storage . ref_file ( ticker = ticker , fld = fld , ext = 'pkl' , has_date = kwargs . get ( 'has_date' , True ) , ** kwargs ) if data_file : if not files . exists ( data_file ) : qry_data . append ( grp ) files . create_folder ( data_file , is_file = True ) grp . reset_index ( drop = True ) . to_pickle ( data_file ) return qry_data
1514	def wait_for_job_to_start ( single_master , job ) : i = 0 while True : try : r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : break else : raise RuntimeError ( ) except : Log . debug ( sys . exc_info ( ) [ 0 ] ) Log . info ( "Waiting for %s to come up... %s" % ( job , i ) ) time . sleep ( 1 ) if i > 20 : Log . error ( "Failed to start Nomad Cluster!" ) sys . exit ( - 1 ) i = i + 1
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
2262	def find_duplicates ( items , k = 2 , key = None ) : duplicates = defaultdict ( list ) if key is None : for count , item in enumerate ( items ) : duplicates [ item ] . append ( count ) else : for count , item in enumerate ( items ) : duplicates [ key ( item ) ] . append ( count ) for key in list ( duplicates . keys ( ) ) : if len ( duplicates [ key ] ) < k : del duplicates [ key ] duplicates = dict ( duplicates ) return duplicates
11396	def get_record ( self ) : self . update_system_numbers ( ) self . add_systemnumber ( "CDS" ) self . fields_list = [ "024" , "041" , "035" , "037" , "088" , "100" , "110" , "111" , "242" , "245" , "246" , "260" , "269" , "300" , "502" , "650" , "653" , "693" , "700" , "710" , "773" , "856" , "520" , "500" , "980" ] self . keep_only_fields ( ) self . determine_collections ( ) self . add_cms_link ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_date ( ) self . update_pagenumber ( ) self . update_authors ( ) self . update_subject_categories ( "SzGeCERN" , "INSPIRE" , "categories_inspire" ) self . update_keywords ( ) self . update_experiments ( ) self . update_collaboration ( ) self . update_journals ( ) self . update_links_and_ffts ( ) if 'THESIS' in self . collections : self . update_thesis_supervisors ( ) self . update_thesis_information ( ) if 'NOTE' in self . collections : self . add_notes ( ) for collection in self . collections : record_add_field ( self . record , tag = '980' , subfields = [ ( 'a' , collection ) ] ) self . remove_controlfields ( ) return self . record
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) self . models_name = self . kwargs . get ( 'models_name' , 'models' ) self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
3354	def append ( self , object ) : the_id = object . id self . _check ( the_id ) self . _dict [ the_id ] = len ( self ) list . append ( self , object )
13336	def module_resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , VirtualEnvironment ) : env = resolver . resolved [ 0 ] mod = env . get_module ( path ) if mod : return mod raise ResolveError
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
6720	def init ( self ) : r = self . local_renderer print ( 'Creating new virtual environment...' ) with self . settings ( warn_only = True ) : cmd = '[ ! -d {virtualenv_dir} ] && virtualenv --no-site-packages {virtualenv_dir} || true' if self . is_local : r . run_or_local ( cmd ) else : r . sudo ( cmd )
10410	def finalize_canonical_averages ( number_of_nodes , ps , canonical_averages , alpha , ) : spanning_cluster = ( ( 'percolation_probability_mean' in canonical_averages . dtype . names ) and 'percolation_probability_m2' in canonical_averages . dtype . names ) ret = np . empty_like ( canonical_averages , dtype = finalized_canonical_averages_dtype ( spanning_cluster = spanning_cluster ) , ) n = canonical_averages [ 'number_of_runs' ] sqrt_n = np . sqrt ( canonical_averages [ 'number_of_runs' ] ) ret [ 'number_of_runs' ] = n ret [ 'p' ] = ps ret [ 'alpha' ] = alpha def _transform ( original_key , final_key = None , normalize = False , transpose = False , ) : if final_key is None : final_key = original_key keys_mean = [ '{}_mean' . format ( key ) for key in [ original_key , final_key ] ] keys_std = [ '{}_m2' . format ( original_key ) , '{}_std' . format ( final_key ) , ] key_ci = '{}_ci' . format ( final_key ) ret [ keys_mean [ 1 ] ] = canonical_averages [ keys_mean [ 0 ] ] if normalize : ret [ keys_mean [ 1 ] ] /= number_of_nodes array = canonical_averages [ keys_std [ 0 ] ] result = np . sqrt ( ( array . T if transpose else array ) / ( n - 1 ) ) ret [ keys_std [ 1 ] ] = ( result . T if transpose else result ) if normalize : ret [ keys_std [ 1 ] ] /= number_of_nodes array = ret [ keys_std [ 1 ] ] scale = ( array . T if transpose else array ) / sqrt_n array = ret [ keys_mean [ 1 ] ] mean = ( array . T if transpose else array ) result = scipy . stats . t . interval ( 1 - alpha , df = n - 1 , loc = mean , scale = scale , ) ( ret [ key_ci ] [ ... , 0 ] , ret [ key_ci ] [ ... , 1 ] ) = ( [ my_array . T for my_array in result ] if transpose else result ) if spanning_cluster : _transform ( 'percolation_probability' ) _transform ( 'max_cluster_size' , 'percolation_strength' , normalize = True ) _transform ( 'moments' , normalize = True , transpose = True ) return ret
6687	def install ( packages , repos = None , yes = None , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , six . string_types ) : options = [ options ] if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) if repos : for repo in repos : options . append ( '--enablerepo=%(repo)s' % locals ( ) ) options = " " . join ( options ) if isinstance ( yes , str ) : run_as_root ( 'yes %(yes)s | %(manager)s %(options)s install %(packages)s' % locals ( ) ) else : run_as_root ( '%(manager)s %(options)s install %(packages)s' % locals ( ) )
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
11573	def clear_display_buffer ( self ) : for row in range ( 0 , 8 ) : self . firmata . i2c_write ( 0x70 , row * 2 , 0 , 0 ) self . firmata . i2c_write ( 0x70 , ( row * 2 ) + 1 , 0 , 0 ) for column in range ( 0 , 8 ) : self . display_buffer [ row ] [ column ] = 0
4221	def set_keyring ( keyring ) : global _keyring_backend if not isinstance ( keyring , backend . KeyringBackend ) : raise TypeError ( "The keyring must be a subclass of KeyringBackend" ) _keyring_backend = keyring
5467	def get_action_image ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'imageUri' )
10724	def xformer ( signature ) : funcs = [ f for ( f , _ ) in xformers ( signature ) ] def the_func ( objects ) : if len ( objects ) != len ( funcs ) : raise IntoDPValueError ( objects , "objects" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( objects ) ) ) return [ x for ( x , _ ) in ( f ( a ) for ( f , a ) in zip ( funcs , objects ) ) ] return the_func
13160	def delete ( cls , cur , table : str , where_keys : list ) : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _delete_query . format ( table , where_clause ) yield from cur . execute ( query , values ) return cur . rowcount
8034	def write ( self , text , newline = False ) : if not self . isatty : self . fobj . write ( '%s\n' % text ) return msg_len = len ( text ) self . max_len = max ( self . max_len , msg_len ) self . fobj . write ( "\r%-*s" % ( self . max_len , text ) ) if newline or not self . isatty : self . fobj . write ( '\n' ) self . max_len = 0
4789	def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
5748	def history ( self , ip , days_limit = None ) : all_dates = sorted ( self . routing_db . smembers ( 'imported_dates' ) , reverse = True ) if days_limit is not None : all_dates = all_dates [ : days_limit ] return [ self . date_asn_block ( ip , date ) for date in all_dates ]
2874	def get_process_parser ( self , process_id_or_name ) : if process_id_or_name in self . process_parsers_by_name : return self . process_parsers_by_name [ process_id_or_name ] else : return self . process_parsers [ process_id_or_name ]
983	def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )
3459	def _multi_deletion ( model , entity , element_lists , method = "fba" , solution = None , processes = None , ** kwargs ) : solver = sutil . interface_to_str ( model . problem . __name__ ) if method == "moma" and solver not in sutil . qp_solvers : raise RuntimeError ( "Cannot use MOMA since '{}' is not QP-capable." "Please choose a different solver or use FBA only." . format ( solver ) ) if processes is None : processes = CONFIGURATION . processes with model : if "moma" in method : add_moma ( model , solution = solution , linear = "linear" in method ) elif "room" in method : add_room ( model , solution = solution , linear = "linear" in method , ** kwargs ) args = set ( [ frozenset ( comb ) for comb in product ( * element_lists ) ] ) processes = min ( processes , len ( args ) ) def extract_knockout_results ( result_iter ) : result = pd . DataFrame ( [ ( frozenset ( ids ) , growth , status ) for ( ids , growth , status ) in result_iter ] , columns = [ 'ids' , 'growth' , 'status' ] ) result . set_index ( 'ids' , inplace = True ) return result if processes > 1 : worker = dict ( gene = _gene_deletion_worker , reaction = _reaction_deletion_worker ) [ entity ] chunk_size = len ( args ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , ) ) results = extract_knockout_results ( pool . imap_unordered ( worker , args , chunksize = chunk_size ) ) pool . close ( ) pool . join ( ) else : worker = dict ( gene = _gene_deletion , reaction = _reaction_deletion ) [ entity ] results = extract_knockout_results ( map ( partial ( worker , model ) , args ) ) return results
2335	def clr ( M , ** kwargs ) : R = np . zeros ( M . shape ) Id = [ [ 0 , 0 ] for i in range ( M . shape [ 0 ] ) ] for i in range ( M . shape [ 0 ] ) : mu_i = np . mean ( M [ i , : ] ) sigma_i = np . std ( M [ i , : ] ) Id [ i ] = [ mu_i , sigma_i ] for i in range ( M . shape [ 0 ] ) : for j in range ( i + 1 , M . shape [ 0 ] ) : z_i = np . max ( [ 0 , ( M [ i , j ] - Id [ i ] [ 0 ] ) / Id [ i ] [ 0 ] ] ) z_j = np . max ( [ 0 , ( M [ i , j ] - Id [ j ] [ 0 ] ) / Id [ j ] [ 0 ] ] ) R [ i , j ] = np . sqrt ( z_i ** 2 + z_j ** 2 ) R [ j , i ] = R [ i , j ] return R
7737	def prohibit ( self , data ) : for char in data : for lookup in self . prohibited : if lookup ( char ) : raise StringprepError ( "Prohibited character: {0!r}" . format ( char ) ) return data
7078	def tic_conesearch ( ra , decl , radius_arcmin = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 10.0 , refresh = 5.0 , maxtimeout = 90.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'ra' : ra , 'dec' : decl , 'radius' : radius_arcmin / 60.0 } service = 'Mast.Catalogs.Tic.Cone' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
11427	def record_make_all_subfields_volatile ( rec ) : for tag in rec . keys ( ) : for field_position , field in enumerate ( rec [ tag ] ) : for subfield_position , subfield in enumerate ( field [ 0 ] ) : if subfield [ 1 ] [ : 9 ] != "VOLATILE:" : record_modify_subfield ( rec , tag , subfield [ 0 ] , "VOLATILE:" + subfield [ 1 ] , subfield_position , field_position_local = field_position )
11786	def sanitize ( self , example ) : "Return a copy of example, with non-input attributes replaced by None." return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
11063	def push ( self , message ) : if self . _ignore_event ( message ) : return None , None args = self . _parse_message ( message ) self . log . debug ( "Searching for command using chunks: %s" , args ) cmd , msg_args = self . _find_longest_prefix_command ( args ) if cmd is not None : if message . user is None : self . log . debug ( "Discarded message with no originating user: %s" , message ) return None , None sender = message . user . username if message . channel is not None : sender = "#%s/%s" % ( message . channel . name , sender ) self . log . info ( "Received from %s: %s, args %s" , sender , cmd , msg_args ) f = self . _get_command ( cmd , message . user ) if f : if self . _is_channel_ignored ( f , message . channel ) : self . log . info ( "Channel %s is ignored, discarding command %s" , message . channel , cmd ) return '_ignored_' , "" return cmd , f . execute ( message , msg_args ) return '_unauthorized_' , "Sorry, you are not authorized to run %s" % cmd return None , None
534	def _getRegions ( self ) : def makeRegion ( name , r ) : r = Region ( r , self ) return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions
4681	def getAccountFromPublicKey ( self , pub ) : names = list ( self . getAccountsFromPublicKey ( str ( pub ) ) ) if names : return names [ 0 ]
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
2941	def deserialize_workflow_spec ( self , s_state , filename = None ) : dom = minidom . parseString ( s_state ) node = dom . getElementsByTagName ( 'process-definition' ) [ 0 ] name = node . getAttribute ( 'name' ) if name == '' : _exc ( '%s without a name attribute' % node . nodeName ) workflow_spec = specs . WorkflowSpec ( name , filename ) del workflow_spec . task_specs [ 'Start' ] end = specs . Simple ( workflow_spec , 'End' ) , [ ] read_specs = dict ( end = end ) for child_node in node . childNodes : if child_node . nodeType != minidom . Node . ELEMENT_NODE : continue if child_node . nodeName == 'name' : workflow_spec . name = child_node . firstChild . nodeValue elif child_node . nodeName == 'description' : workflow_spec . description = child_node . firstChild . nodeValue elif child_node . nodeName . lower ( ) in _spec_map : self . deserialize_task_spec ( workflow_spec , child_node , read_specs ) else : _exc ( 'Unknown node: %s' % child_node . nodeName ) workflow_spec . start = read_specs [ 'start' ] [ 0 ] for name in read_specs : spec , successors = read_specs [ name ] for condition , successor_name in successors : if successor_name not in read_specs : _exc ( 'Unknown successor: "%s"' % successor_name ) successor , foo = read_specs [ successor_name ] if condition is None : spec . connect ( successor ) else : spec . connect_if ( condition , successor ) return workflow_spec
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
6447	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) for suffix_len in range ( 11 , 0 , - 1 ) : ending = word [ - suffix_len : ] if ( ending in self . _suffix and len ( word ) - suffix_len >= 2 and ( self . _suffix [ ending ] is None or self . _suffix [ ending ] ( word , suffix_len ) ) ) : word = word [ : - suffix_len ] break if word [ - 2 : ] in { 'bb' , 'dd' , 'gg' , 'll' , 'mm' , 'nn' , 'pp' , 'rr' , 'ss' , 'tt' , } : word = word [ : - 1 ] for ending , replacement in self . _recode : if word . endswith ( ending ) : if callable ( replacement ) : word = replacement ( word ) else : word = word [ : - len ( ending ) ] + replacement return word
4948	def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
8673	def purge_stash ( force , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Purging stash...' ) stash . purge ( force ) click . echo ( 'Purge complete!' ) except GhostError as ex : sys . exit ( ex )
7887	def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : result = [ ] for mechanism in mechanisms : try : if server_side : klass = SERVER_MECHANISMS_D [ mechanism ] else : klass = CLIENT_MECHANISMS_D [ mechanism ] except KeyError : logger . debug ( " skipping {0} - not supported" . format ( mechanism ) ) continue secure = properties . get ( "security-layer" ) if not allow_insecure and not klass . _pyxmpp_sasl_secure and not secure : logger . debug ( " skipping {0}, as it is not secure" . format ( mechanism ) ) continue if not klass . are_properties_sufficient ( properties ) : logger . debug ( " skipping {0}, as the properties are not sufficient" . format ( mechanism ) ) continue result . append ( mechanism ) return result
12282	def get_resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( "Invalid path" )
3971	def _composed_app_dict ( app_name , assembled_specs , port_specs ) : logging . info ( "Compose Compiler: Compiling dict for app {}" . format ( app_name ) ) app_spec = assembled_specs [ 'apps' ] [ app_name ] compose_dict = app_spec [ "compose" ] _apply_env_overrides ( env_overrides_for_app_or_service ( app_name ) , compose_dict ) if 'image' in app_spec and 'build' in app_spec : raise RuntimeError ( "image and build are both specified in the spec for {}" . format ( app_name ) ) elif 'image' in app_spec : logging . info compose_dict [ 'image' ] = app_spec [ 'image' ] elif 'build' in app_spec : compose_dict [ 'build' ] = _get_build_path ( app_spec ) else : raise RuntimeError ( "Neither image nor build was specified in the spec for {}" . format ( app_name ) ) compose_dict [ 'entrypoint' ] = [ ] compose_dict [ 'command' ] = _compile_docker_command ( app_spec ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( app_name ) logging . info ( "Compose Compiler: compiled command {}" . format ( compose_dict [ 'command' ] ) ) compose_dict [ 'links' ] = _links_for_app ( app_spec , assembled_specs ) logging . info ( "Compose Compiler: links {}" . format ( compose_dict [ 'links' ] ) ) compose_dict [ 'volumes' ] = compose_dict [ 'volumes' ] + _get_compose_volumes ( app_name , assembled_specs ) logging . info ( "Compose Compiler: volumes {}" . format ( compose_dict [ 'volumes' ] ) ) port_list = _get_ports_list ( app_name , port_specs ) if port_list : compose_dict [ 'ports' ] = port_list logging . info ( "Compose Compiler: ports {}" . format ( port_list ) ) compose_dict [ 'user' ] = 'root' return compose_dict
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
1853	def SHR ( cpu , dest , src ) : OperandSize = dest . size count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) value = dest . read ( ) res = dest . write ( value >> count ) MASK = ( 1 << OperandSize ) - 1 SIGN_MASK = 1 << ( OperandSize - 1 ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( count != 0 , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
10662	def element_mass_fraction ( compound , element ) : coeff = stoichiometry_coefficient ( compound , element ) if coeff == 0.0 : return 0.0 formula_mass = molar_mass ( compound ) element_mass = molar_mass ( element ) return coeff * element_mass / formula_mass
7537	def branch_assembly ( args , parsedict ) : data = getassembly ( args , parsedict ) bargs = args . branch newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] if len ( bargs ) > 1 : if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass subsamples = bargs [ 1 : ] if bargs [ 1 ] == "-" : fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
11402	def create_records ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : regex = re . compile ( '<record.*?>.*?</record>' , re . DOTALL ) record_xmls = regex . findall ( marcxml ) return [ create_record ( record_xml , verbose = verbose , correct = correct , parser = parser , keep_singletons = keep_singletons ) for record_xml in record_xmls ]
11877	def format ( self , record ) : try : record . message = record . getMessage ( ) except TypeError : if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( ** record . args ) else : record . message = record . msg . format ( record . args ) self . _fmt = self . getfmt ( record . levelname ) if self . usesTime ( ) : record . asctime = self . formatTime ( record , self . datefmt ) s = self . _fmt . format ( ** record . __dict__ ) if record . exc_info : if not record . exc_text : record . exc_text = self . formatException ( record . exc_info ) if record . exc_text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc_text except UnicodeError : s = s + record . exc_text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
9263	def get_filtered_pull_requests ( self , pull_requests ) : pull_requests = self . filter_by_labels ( pull_requests , "pull requests" ) pull_requests = self . filter_merged_pull_requests ( pull_requests ) if self . options . verbose > 1 : print ( "\tremaining pull requests: {}" . format ( len ( pull_requests ) ) ) return pull_requests
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8367	def output_closure ( self , target , file_number = None ) : def output_context ( ctx ) : target_ctx = target target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_surface ( ctx ) : target_ctx = cairo . Context ( target ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return target_ctx def output_file ( ctx ) : root , extension = os . path . splitext ( target ) if file_number : filename = '%s_%04d%s' % ( root , file_number , extension ) else : filename = target extension = extension . lower ( ) if extension == '.png' : surface = ctx . get_target ( ) surface . write_to_png ( target ) elif extension == '.pdf' : target_ctx = cairo . Context ( cairo . PDFSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension in ( '.ps' , '.eps' ) : target_ctx = cairo . Context ( cairo . PSSurface ( filename , * self . size_or_default ( ) ) ) if extension == '.eps' : target_ctx . set_eps ( extension = '.eps' ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) elif extension == '.svg' : target_ctx = cairo . Context ( cairo . SVGSurface ( filename , * self . size_or_default ( ) ) ) target_ctx . set_source_surface ( ctx . get_target ( ) ) target_ctx . paint ( ) return filename if isinstance ( target , cairo . Context ) : return output_context elif isinstance ( target , cairo . Surface ) : return output_surface else : return output_file
8915	def fetch_by_name ( self , name ) : service = self . name_index . get ( name ) if not service : raise ServiceNotFound return Service ( service )
8231	def speed ( self , framerate = None ) : if framerate is not None : self . _speed = framerate self . _dynamic = True else : return self . _speed
6845	def check_ok ( self ) : import requests if not self . env . check_ok : return branch_name = self . _local ( 'git rev-parse --abbrev-ref HEAD' , capture = True ) . strip ( ) check_ok_paths = self . env . check_ok_paths or { } if branch_name in check_ok_paths : check = check_ok_paths [ branch_name ] if 'username' in check : auth = ( check [ 'username' ] , check [ 'password' ] ) else : auth = None ret = requests . get ( check [ 'url' ] , auth = auth ) passed = check [ 'text' ] in ret . content assert passed , 'Check failed: %s' % check [ 'url' ]
4907	def _sync_content_metadata ( self , serialized_data , http_method ) : try : status_code , response_body = getattr ( self , '_' + http_method ) ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . course_api_path ) , serialized_data , self . CONTENT_PROVIDER_SCOPE ) except requests . exceptions . RequestException as exc : raise ClientError ( 'DegreedAPIClient request failed: {error} {message}' . format ( error = exc . __class__ . __name__ , message = str ( exc ) ) ) if status_code >= 400 : raise ClientError ( 'DegreedAPIClient request failed with status {status_code}: {message}' . format ( status_code = status_code , message = response_body ) )
8005	def from_xml ( self , xmlnode ) : if xmlnode . type != "element" : raise ValueError ( "XML node is not a jabber:x:delay element (not an element)" ) ns = get_node_ns_uri ( xmlnode ) if ns and ns != DELAY_NS or xmlnode . name != "x" : raise ValueError ( "XML node is not a jabber:x:delay element" ) stamp = xmlnode . prop ( "stamp" ) if stamp . endswith ( "Z" ) : stamp = stamp [ : - 1 ] if "-" in stamp : stamp = stamp . split ( "-" , 1 ) [ 0 ] try : tm = time . strptime ( stamp , "%Y%m%dT%H:%M:%S" ) except ValueError : raise BadRequestProtocolError ( "Bad timestamp" ) tm = tm [ 0 : 8 ] + ( 0 , ) self . timestamp = datetime . datetime . fromtimestamp ( time . mktime ( tm ) ) delay_from = from_utf8 ( xmlnode . prop ( "from" ) ) if delay_from : try : self . delay_from = JID ( delay_from ) except JIDError : raise JIDMalformedProtocolError ( "Bad JID in the jabber:x:delay 'from' attribute" ) else : self . delay_from = None self . reason = from_utf8 ( xmlnode . getContent ( ) )
11771	def AIMAFile ( components , mode = 'r' ) : "Open a file based at the AIMA root directory." import utils dir = os . path . dirname ( utils . __file__ ) return open ( apply ( os . path . join , [ dir ] + components ) , mode )
7918	def _validate_ip_address ( family , address ) : try : info = socket . getaddrinfo ( address , 0 , family , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , address ) ) raise ValueError ( "Bad IP address" ) if not info : logger . debug ( "getaddrinfo result empty" ) raise ValueError ( "Bad IP address" ) addr = info [ 0 ] [ 4 ] logger . debug ( " got address: {0!r}" . format ( addr ) ) try : return socket . getnameinfo ( addr , socket . NI_NUMERICHOST ) [ 0 ] except socket . gaierror , err : logger . debug ( "gaierror: {0} for {1!r}" . format ( err , addr ) ) raise ValueError ( "Bad IP address" )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : return L_ANY if ( code & ( code - 1 ) ) != 0 : return L_ANY return code
8100	def apply ( self ) : sorted = self . order + self . keys ( ) unique = [ ] [ unique . append ( x ) for x in sorted if x not in unique ] for node in self . graph . nodes : for s in unique : if self . has_key ( s ) and self [ s ] ( self . graph , node ) : node . style = s
1708	def connect ( command , data = None , env = None , cwd = None ) : command_str = expand_args ( command ) . pop ( ) environ = dict ( os . environ ) environ . update ( env or { } ) process = subprocess . Popen ( command_str , universal_newlines = True , shell = False , env = environ , stdin = subprocess . PIPE , stdout = subprocess . PIPE , stderr = subprocess . PIPE , bufsize = 0 , cwd = cwd , ) return ConnectedCommand ( process = process )
4951	def get_required_query_params ( self , request ) : username = get_request_value ( request , self . REQUIRED_PARAM_USERNAME , '' ) course_id = get_request_value ( request , self . REQUIRED_PARAM_COURSE_ID , '' ) program_uuid = get_request_value ( request , self . REQUIRED_PARAM_PROGRAM_UUID , '' ) enterprise_customer_uuid = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER ) if not ( username and ( course_id or program_uuid ) and enterprise_customer_uuid ) : raise ConsentAPIRequestError ( self . get_missing_params_message ( [ ( "'username'" , bool ( username ) ) , ( "'enterprise_customer_uuid'" , bool ( enterprise_customer_uuid ) ) , ( "one of 'course_id' or 'program_uuid'" , bool ( course_id or program_uuid ) ) , ] ) ) return username , course_id , program_uuid , enterprise_customer_uuid
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
4753	def tcase_parse_descr ( tcase ) : descr_short = "SHORT" descr_long = "LONG" try : comment = tcase_comment ( tcase ) except ( IOError , OSError , ValueError ) as exc : comment = [ ] cij . err ( "tcase_parse_descr: failed: %r, tcase: %r" % ( exc , tcase ) ) comment = [ l for l in comment if l . strip ( ) ] for line_number , line in enumerate ( comment ) : if line . startswith ( "#" ) : comment [ line_number ] = line [ 1 : ] if comment : descr_short = comment [ 0 ] if len ( comment ) > 1 : descr_long = "\n" . join ( comment [ 1 : ] ) return descr_short , descr_long
6362	def encode ( self , word , max_length = - 1 ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
11800	def infer_assignment ( self ) : "Return the partial assignment implied by the current inferences." self . support_pruning ( ) return dict ( ( v , self . curr_domains [ v ] [ 0 ] ) for v in self . vars if 1 == len ( self . curr_domains [ v ] ) )
7651	def query_pop ( query , prefix , sep = '.' ) : terms = query . split ( sep ) if terms [ 0 ] == prefix : terms = terms [ 1 : ] return sep . join ( terms )
8584	def attach_volume ( self , datacenter_id , server_id , volume_id ) : data = '{ "id": "' + volume_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/volumes' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
11569	def run ( self ) : while not self . is_stopped ( ) : try : if self . arduino . inWaiting ( ) : c = self . arduino . read ( ) self . command_deque . append ( ord ( c ) ) else : time . sleep ( .1 ) except OSError : pass except IOError : self . stop ( ) self . close ( )
12066	def comments ( abf , minutes = False ) : if not len ( abf . commentTimes ) : return for i in range ( len ( abf . commentTimes ) ) : t , c = abf . commentTimes [ i ] , abf . commentTags [ i ] if minutes : t = t / 60 pylab . axvline ( t , lw = 1 , color = 'r' , ls = "--" , alpha = .5 ) X1 , X2 , Y1 , Y2 = pylab . axis ( ) Y2 = Y2 - abs ( Y2 - Y1 ) * .02 pylab . text ( t , Y2 , c , size = 8 , color = 'r' , rotation = 'vertical' , ha = 'right' , va = 'top' , weight = 'bold' , alpha = .5 ) if minutes : pylab . xlabel ( "minutes" ) else : pylab . xlabel ( "seconds" )
10914	def separate_particles_into_groups ( s , region_size = 40 , bounds = None , doshift = False ) : imtile = s . oshape . translate ( - s . pad ) bounding_tile = ( imtile if bounds is None else Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) rs = ( np . ones ( bounding_tile . dim , dtype = 'int' ) * region_size if np . size ( region_size ) == 1 else np . array ( region_size ) ) n_translate = np . ceil ( bounding_tile . shape . astype ( 'float' ) / rs ) . astype ( 'int' ) particle_groups = [ ] tile = Tile ( left = bounding_tile . l , right = bounding_tile . l + rs ) if doshift == 'rand' : doshift = np . random . choice ( [ True , False ] ) if doshift : shift = rs // 2 n_translate += 1 else : shift = 0 deltas = np . meshgrid ( * [ np . arange ( i ) for i in n_translate ] ) positions = s . obj_get_positions ( ) if bounds is None : positions = np . clip ( positions , imtile . l + 1e-3 , imtile . r - 1e-3 ) groups = list ( map ( lambda * args : find_particles_in_tile ( positions , tile . translate ( np . array ( args ) * rs - shift ) ) , * [ d . ravel ( ) for d in deltas ] ) ) for i in range ( len ( groups ) - 1 , - 1 , - 1 ) : if groups [ i ] . size == 0 : groups . pop ( i ) assert _check_groups ( s , groups ) return groups
2514	def p_file_depends ( self , f_term , predicate ) : for _ , _ , other_file in self . graph . triples ( ( f_term , predicate , None ) ) : name = self . get_file_name ( other_file ) if name is not None : self . builder . add_file_dep ( six . text_type ( name ) ) else : self . error = True msg = 'File depends on file with no name' self . logger . log ( msg )
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
12431	def create_manage_scripts ( self ) : start = '# start script for {0}\n\n' . format ( self . _project_name ) start += 'echo \'Starting uWSGI...\'\n' start += 'sh {0}.uwsgi\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'Starting Nginx...\'\n' start += 'nginx -c {0}_nginx.conf\n' . format ( os . path . join ( self . _conf_dir , self . _project_name ) ) start += 'sleep 1\n' start += 'echo \'{0} started\'\n\n' . format ( self . _project_name ) stop = '# stop script for {0}\n\n' . format ( self . _project_name ) stop += 'if [ -e {0}_nginx.pid ]; then nginx -c {1}_nginx.conf -s stop ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) , os . path . join ( self . _conf_dir , self . _project_name ) ) stop += 'if [ -e {0}_uwsgi.pid ]; then kill -9 `cat {0}_uwsgi.pid` ; rm {0}_uwsgi.pid 2>&1 > /dev/null ; fi\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) stop += 'echo \'{0} stopped\'\n' . format ( self . _project_name ) start_file = '{0}_start.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) stop_file = '{0}_stop.sh' . format ( os . path . join ( self . _script_dir , self . _project_name ) ) f = open ( start_file , 'w' ) f . write ( start ) f . close ( ) f = open ( stop_file , 'w' ) f . write ( stop ) f . close ( ) os . chmod ( start_file , 0754 ) os . chmod ( stop_file , 0754 )
13673	def add_file ( self , * args ) : for file_path in args : self . files . append ( FilePath ( file_path , self ) )
8230	def size ( self , w = None , h = None ) : if not w : w = self . _canvas . width if not h : h = self . _canvas . height if not w and not h : return ( self . _canvas . width , self . _canvas . height ) w , h = self . _canvas . set_size ( ( w , h ) ) self . _namespace [ 'WIDTH' ] = w self . _namespace [ 'HEIGHT' ] = h self . WIDTH = w self . HEIGHT = h
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
9291	def db_value ( self , value ) : if not isinstance ( value , UUID ) : value = UUID ( value ) parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( OrderedUUIDField , self ) . db_value ( value )
325	def rolling_sharpe ( returns , rolling_sharpe_window ) : return returns . rolling ( rolling_sharpe_window ) . mean ( ) / returns . rolling ( rolling_sharpe_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
5626	def write_json ( path , params ) : logger . debug ( "write %s to %s" , params , path ) if path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) logger . debug ( "upload %s" , key ) bucket . put_object ( Key = key , Body = json . dumps ( params , sort_keys = True , indent = 4 ) ) else : makedirs ( os . path . dirname ( path ) ) with open ( path , 'w' ) as dst : json . dump ( params , dst , sort_keys = True , indent = 4 )
4289	def generate_media_pages ( gallery ) : writer = PageWriter ( gallery . settings , index_title = gallery . title ) for album in gallery . albums . values ( ) : medias = album . medias next_medias = medias [ 1 : ] + [ None ] previous_medias = [ None ] + medias [ : - 1 ] media_groups = zip ( medias , next_medias , previous_medias ) for media_group in media_groups : writer . write ( album , media_group )
1048	def print_tb ( tb , limit = None , file = None ) : if file is None : file = sys . stderr if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit n = 0 while tb is not None and ( limit is None or n < limit ) : f = tb . tb_frame lineno = tb . tb_lineno co = f . f_code filename = co . co_filename name = co . co_name _print ( file , ' File "%s", line %d, in %s' % ( filename , lineno , name ) ) linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : _print ( file , ' ' + line . strip ( ) ) tb = tb . tb_next n = n + 1
12952	def _get_connection ( self ) : if self . _connection is None : self . _connection = self . _get_new_connection ( ) return self . _connection
1712	def ConstructArray ( self , py_arr ) : arr = self . NewArray ( len ( py_arr ) ) arr . _init ( py_arr ) return arr
12055	def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( " username:" , username ) print ( " password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
205	def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap
4395	def adsSyncReadByNameEx ( port , address , data_name , data_type , return_ctypes = False ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) value = adsSyncReadReqEx2 ( port , address , ADSIGRP_SYM_VALBYHND , handle , data_type , return_ctypes ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT ) return value
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
1152	def warn ( message , category = None , stacklevel = 1 ) : if isinstance ( message , Warning ) : category = message . __class__ if category is None : category = UserWarning assert issubclass ( category , Warning ) try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = "<string>" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( ".pyc" , ".pyo" ) ) : filename = filename [ : - 1 ] else : if module == "__main__" : try : filename = sys . argv [ 0 ] except AttributeError : filename = '__main__' if not filename : filename = module registry = globals . setdefault ( "__warningregistry__" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )
11445	def _clean_xml ( self , path_to_xml ) : try : if os . path . isfile ( path_to_xml ) : tree = ET . parse ( path_to_xml ) root = tree . getroot ( ) else : root = ET . fromstring ( path_to_xml ) except Exception , e : self . logger . error ( "Could not read OAI XML, aborting filter!" ) raise e strip_xml_namespace ( root ) return root
12595	def get_aad_token ( endpoint , no_verify ) : from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
4090	def _process_event ( self , key , mask ) : self . _logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT_READ and reader is not None : if reader . _cancelled : self . remove_reader ( fileobj ) else : self . _logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . _run ( ) if mask & selectors . EVENT_WRITE and writer is not None : if writer . _cancelled : self . remove_writer ( fileobj ) else : self . _logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . _run ( )
9545	def add_value_predicate ( self , field_name , value_predicate , code = VALUE_PREDICATE_FALSE , message = MESSAGES [ VALUE_PREDICATE_FALSE ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_predicate ) , 'value predicate must be a callable function' t = field_name , value_predicate , code , message , modulus self . _value_predicates . append ( t )
12343	def image ( self , well_row , well_column , field_row , field_column ) : return next ( ( i for i in self . images if attribute ( i , 'u' ) == well_column and attribute ( i , 'v' ) == well_row and attribute ( i , 'x' ) == field_column and attribute ( i , 'y' ) == field_row ) , '' )
3297	def is_collection ( self , path , environ ) : res = self . get_resource_inst ( path , environ ) return res and res . is_collection
8341	def _invert ( h ) : "Cheap function to invert a hash." i = { } for k , v in h . items ( ) : i [ v ] = k return i
6219	def interleaves ( self , info ) : return info . byte_offset == self . component_type . size * self . components
404	def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
13294	def convert_text ( content , from_fmt , to_fmt , deparagraph = False , mathjax = False , smart = True , extra_args = None ) : logger = logging . getLogger ( __name__ ) if extra_args is not None : extra_args = list ( extra_args ) else : extra_args = [ ] if mathjax : extra_args . append ( '--mathjax' ) if smart : extra_args . append ( '--smart' ) if deparagraph : extra_args . append ( '--filter=lsstprojectmeta-deparagraph' ) extra_args . append ( '--wrap=none' ) extra_args = set ( extra_args ) logger . debug ( 'Running pandoc from %s to %s with extra_args %s' , from_fmt , to_fmt , extra_args ) output = pypandoc . convert_text ( content , to_fmt , format = from_fmt , extra_args = extra_args ) return output
11074	def get_by_username ( self , username ) : res = filter ( lambda x : x . username == username , self . users . values ( ) ) if len ( res ) > 0 : return res [ 0 ] return None
6321	def get_template_dir ( self ) : directory = os . path . dirname ( os . path . abspath ( __file__ ) ) directory = os . path . dirname ( os . path . dirname ( directory ) ) directory = os . path . join ( directory , 'project_template' ) return directory
3759	def atom_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count tot = sum ( things . values ( ) ) return { atom : value / tot for atom , value in things . iteritems ( ) }
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
491	def close ( self ) : self . _logger . info ( "Closing" ) if self . _pool is not None : self . _pool . close ( ) self . _pool = None else : self . _logger . warning ( "close() called, but connection policy was alredy closed" ) return
194	def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , "Input images did not fulfill user-defined assertion in AssertLambda." ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , "Input heatmaps did not fulfill user-defined assertion in AssertLambda." ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , "Input keypoints did not fulfill user-defined assertion in AssertLambda." ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , "Input polygons did not fulfill user-defined assertion in AssertLambda." ) return polygons_on_images if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )
1601	def chain ( cmd_list ) : command = ' | ' . join ( map ( lambda x : ' ' . join ( x ) , cmd_list ) ) chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) stdout_builder = proc . async_stdout_builder ( chained_proc ) chained_proc . wait ( ) return { 'command' : command , 'stdout' : stdout_builder . result ( ) }
13375	def ensure_path_exists ( path , * args ) : if os . path . exists ( path ) : return os . makedirs ( path , * args )
13710	def is_suspicious ( self , result = None ) : result = result if result is not None else self . _last_result suspicious = False if result is not None : suspicious = True if result [ 'type' ] > 0 else False return suspicious
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
6483	def _process_pagination_values ( request ) : size = 20 page = 0 from_ = 0 if "page_size" in request . POST : size = int ( request . POST [ "page_size" ] ) max_page_size = getattr ( settings , "SEARCH_MAX_PAGE_SIZE" , 100 ) if not ( 0 < size <= max_page_size ) : raise ValueError ( _ ( 'Invalid page size of {page_size}' ) . format ( page_size = size ) ) if "page_index" in request . POST : page = int ( request . POST [ "page_index" ] ) from_ = page * size return size , from_ , page
2599	def uncan ( obj , g = None ) : import_needed = False for cls , uncanner in iteritems ( uncan_map ) : if isinstance ( cls , string_types ) : import_needed = True break elif isinstance ( obj , cls ) : return uncanner ( obj , g ) if import_needed : _import_mapping ( uncan_map , _original_uncan_map ) return uncan ( obj , g ) return obj
10843	def pending ( self ) : pending_updates = [ ] url = PATHS [ 'GET_PENDING' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : pending_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __pending = pending_updates return self . __pending
5706	def clean ( self ) : cleaned_data = super ( AuthForm , self ) . clean ( ) user = self . get_user ( ) if self . staff_only and ( not user or not user . is_staff ) : raise forms . ValidationError ( 'Sorry, only staff are allowed.' ) if self . superusers_only and ( not user or not user . is_superuser ) : raise forms . ValidationError ( 'Sorry, only superusers are allowed.' ) return cleaned_data
9328	def get ( self , url , headers = None , params = None ) : merged_headers = self . _merge_headers ( headers ) if "Accept" not in merged_headers : merged_headers [ "Accept" ] = MEDIA_TYPE_TAXII_V20 accept = merged_headers [ "Accept" ] resp = self . session . get ( url , headers = merged_headers , params = params ) resp . raise_for_status ( ) content_type = resp . headers [ "Content-Type" ] if not self . valid_content_type ( content_type = content_type , accept = accept ) : msg = "Unexpected Response. Got Content-Type: '{}' for Accept: '{}'" raise TAXIIServiceException ( msg . format ( content_type , accept ) ) return _to_json ( resp )
10621	def get_element_mass_dictionary ( self ) : element_symbols = self . material . elements element_masses = self . get_element_masses ( ) return { s : m for s , m in zip ( element_symbols , element_masses ) }
9895	def _uptime_syllable ( ) : global __boottime try : __boottime = os . stat ( '/dev/pty/mst/pty0' ) . st_mtime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
6135	def _fix_docs ( this_abc , child_class ) : if sys . version_info >= ( 3 , 5 ) : return child_class if not issubclass ( child_class , this_abc ) : raise KappaError ( 'Cannot fix docs of class that is not decendent.' ) for name , child_func in vars ( child_class ) . items ( ) : if callable ( child_func ) and not child_func . __doc__ : if name in this_abc . __abstractmethods__ : parent_func = getattr ( this_abc , name ) child_func . __doc__ = parent_func . __doc__ return child_class
8372	def save_as ( self ) : chooser = ShoebotFileChooserDialog ( _ ( 'Save File' ) , None , Gtk . FileChooserAction . SAVE , ( Gtk . STOCK_SAVE , Gtk . ResponseType . ACCEPT , Gtk . STOCK_CANCEL , Gtk . ResponseType . CANCEL ) ) chooser . set_do_overwrite_confirmation ( True ) chooser . set_transient_for ( self ) saved = chooser . run ( ) == Gtk . ResponseType . ACCEPT if saved : old_filename = self . filename self . source_buffer . filename = chooser . get_filename ( ) if not self . save ( ) : self . filename = old_filename chooser . destroy ( ) return saved
7495	def chunk_to_matrices ( narr , mapcol , nmask ) : mats = np . zeros ( ( 3 , 16 , 16 ) , dtype = np . uint32 ) last_loc = - 1 for idx in xrange ( mapcol . shape [ 0 ] ) : if not nmask [ idx ] : if not mapcol [ idx ] == last_loc : i = narr [ : , idx ] mats [ 0 , ( 4 * i [ 0 ] ) + i [ 1 ] , ( 4 * i [ 2 ] ) + i [ 3 ] ] += 1 last_loc = mapcol [ idx ] x = np . uint8 ( 0 ) for y in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : for z in np . array ( [ 0 , 4 , 8 , 12 ] , dtype = np . uint8 ) : mats [ 1 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) mats [ 2 , y : y + np . uint8 ( 4 ) , z : z + np . uint8 ( 4 ) ] = mats [ 0 , x ] . reshape ( 4 , 4 ) . T x += np . uint8 ( 1 ) return mats
9974	def get_mro ( self , space ) : seqs = [ self . get_mro ( base ) for base in self . get_bases ( space ) ] + [ list ( self . get_bases ( space ) ) ] res = [ ] while True : non_empty = list ( filter ( None , seqs ) ) if not non_empty : res . insert ( 0 , space ) return res for seq in non_empty : candidate = seq [ 0 ] not_head = [ s for s in non_empty if candidate in s [ 1 : ] ] if not_head : candidate = None else : break if not candidate : raise TypeError ( "inconsistent hierarchy, no C3 MRO is possible" ) res . append ( candidate ) for seq in non_empty : if seq [ 0 ] == candidate : del seq [ 0 ]
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
5719	def _convert_path ( path , name ) : table = os . path . splitext ( path ) [ 0 ] table = table . replace ( os . path . sep , '__' ) if name is not None : table = ' ' . join ( [ table , name ] ) table = re . sub ( '[^0-9a-zA-Z_]+' , '_' , table ) table = table . lower ( ) return table
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
800	def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : assert len ( fields ) >= 1 , 'fields is empty' dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( dbFields ) query = 'SELECT model_id, %s FROM %s ' ' WHERE job_id=%%s ' % ( dbFieldsStr , self . modelsTableName ) sqlParams = [ jobID ] if ignoreKilled : query += ' AND (completion_reason IS NULL OR completion_reason != %s)' sqlParams . append ( self . CMPL_REASON_KILLED ) with ConnectionFactory . get ( ) as conn : conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows is None : self . _logger . error ( "Unexpected None result from cursor.fetchall; " "query=%r; Traceback=%r" , query , traceback . format_exc ( ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
10491	def dragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _postQueuedEvents ( interval = interval )
6495	def log_indexing_error ( cls , indexing_errors ) : indexing_errors_log = [ ] for indexing_error in indexing_errors : indexing_errors_log . append ( str ( indexing_error ) ) raise exceptions . ElasticsearchException ( ', ' . join ( indexing_errors_log ) )
5367	def compact_interval_string ( value_list ) : if not value_list : return '' value_list . sort ( ) interval_list = [ ] curr = [ ] for val in value_list : if curr and ( val > curr [ - 1 ] + 1 ) : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) curr = [ val ] else : curr . append ( val ) if curr : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) return ',' . join ( [ '{}-{}' . format ( pair [ 0 ] , pair [ 1 ] ) if pair [ 0 ] != pair [ 1 ] else str ( pair [ 0 ] ) for pair in interval_list ] )
1955	def empty_platform ( cls , arch ) : platform = cls ( None ) platform . _init_cpu ( arch ) platform . _init_std_fds ( ) return platform
2185	def tryload ( self , cfgstr = None , on_error = 'raise' ) : cfgstr = self . _rectify_cfgstr ( cfgstr ) if self . enabled : try : if self . verbose > 1 : self . log ( '[cacher] tryload fname={}' . format ( self . fname ) ) return self . load ( cfgstr ) except IOError : if self . verbose > 0 : self . log ( '[cacher] ... {} cache miss' . format ( self . fname ) ) except Exception : if self . verbose > 0 : self . log ( '[cacher] ... failed to load' ) if on_error == 'raise' : raise elif on_error == 'clear' : self . clear ( cfgstr ) return None else : raise KeyError ( 'Unknown method on_error={}' . format ( on_error ) ) else : if self . verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) return None
1826	def PUSH ( cpu , src ) : size = src . size v = src . read ( ) if size != 64 and size != cpu . address_bit_size // 2 : v = Operators . SEXTEND ( v , size , cpu . address_bit_size ) size = cpu . address_bit_size cpu . push ( v , size )
552	def __setAsOrphaned ( self ) : cmplReason = ClientJobsDAO . CMPL_REASON_ORPHAN cmplMessage = "Killed by Scheduler" self . _jobsDAO . modelSetCompleted ( self . _modelID , cmplReason , cmplMessage )
12783	def speak ( self , message ) : campfire = self . get_campfire ( ) if not isinstance ( message , Message ) : message = Message ( campfire , message ) result = self . _connection . post ( "room/%s/speak" % self . id , { "message" : message . get_data ( ) } , parse_data = True , key = "message" ) if result [ "success" ] : return Message ( campfire , result [ "data" ] ) return result [ "success" ]
12689	def send ( * args , ** kwargs ) : queue_flag = kwargs . pop ( "queue" , False ) now_flag = kwargs . pop ( "now" , False ) assert not ( queue_flag and now_flag ) , "'queue' and 'now' cannot both be True." if queue_flag : return queue ( * args , ** kwargs ) elif now_flag : return send_now ( * args , ** kwargs ) else : if QUEUE_ALL : return queue ( * args , ** kwargs ) else : return send_now ( * args , ** kwargs )
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
6118	def circular_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8207	def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
7786	def timeout ( self ) : if not self . active : return if not self . _try_backup_item ( ) : if self . _timeout_handler : self . _timeout_handler ( self . address ) else : self . _error_handler ( self . address , None ) self . cache . invalidate_object ( self . address ) self . _deactivate ( )
9525	def sort_by_name ( infile , outfile ) : seqs = { } file_to_dict ( infile , seqs ) fout = utils . open_file_write ( outfile ) for name in sorted ( seqs ) : print ( seqs [ name ] , file = fout ) utils . close ( fout )
3653	def represent_pixel_location ( self ) : if self . data is None : return None return self . _data . reshape ( self . height + self . y_padding , int ( self . width * self . _num_components_per_pixel + self . x_padding ) )
7640	def convert_jams ( jams_file , output_prefix , csv = False , comment_char = '#' , namespaces = None ) : if namespaces is None : raise ValueError ( 'No namespaces provided. Try ".*" for all namespaces.' ) jam = jams . load ( jams_file ) counter = collections . Counter ( ) annotations = [ ] for query in namespaces : annotations . extend ( jam . search ( namespace = query ) ) if csv : suffix = 'csv' sep = ',' else : suffix = 'lab' sep = '\t' for ann in annotations : index = counter [ ann . namespace ] counter [ ann . namespace ] += 1 filename = os . path . extsep . join ( [ get_output_name ( output_prefix , ann . namespace , index ) , suffix ] ) comment = get_comments ( jam , ann ) lab_dump ( ann , comment , filename , sep , comment_char )
4178	def window_lanczos ( N ) : r if N == 1 : return ones ( 1 ) n = linspace ( - N / 2. , N / 2. , N ) win = sinc ( 2 * n / ( N - 1. ) ) return win
11449	def send ( self , recipient , message ) : if self . _logindata [ 'login_rufnummer' ] is None or self . _logindata [ 'login_passwort' ] is None : err_mess = "YesssSMS: Login data required" raise self . LoginError ( err_mess ) if not recipient : raise self . NoRecipientError ( "YesssSMS: recipient number missing" ) if not isinstance ( recipient , str ) : raise ValueError ( "YesssSMS: str expected as recipient number" ) if not message : raise self . EmptyMessageError ( "YesssSMS: message is empty" ) with self . _login ( requests . Session ( ) ) as sess : sms_data = { 'to_nummer' : recipient , 'nachricht' : message } req = sess . post ( self . _websms_url , data = sms_data ) if not ( req . status_code == 200 or req . status_code == 302 ) : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) if _UNSUPPORTED_CHARS_STRING in req . text : raise self . UnsupportedCharsError ( "YesssSMS: message contains unsupported character(s)" ) if _SMS_SENDING_SUCCESSFUL_STRING not in req . text : raise self . SMSSendingError ( "YesssSMS: error sending SMS" ) sess . get ( self . _logout_url )
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
10716	def main ( argv = None ) : if len ( argv ) : commands = ' ' . join ( argv ) comPort , commands = commands . split ( None , 1 ) sendCommands ( comPort , commands ) return 0
10286	def get_subgraph_peripheral_nodes ( graph : BELGraph , subgraph : Iterable [ BaseEntity ] , node_predicates : NodePredicates = None , edge_predicates : EdgePredicates = None , ) : node_filter = concatenate_node_predicates ( node_predicates = node_predicates ) edge_filter = and_edge_predicates ( edge_predicates = edge_predicates ) result = defaultdict ( lambda : defaultdict ( lambda : defaultdict ( list ) ) ) for u , v , k , d in get_peripheral_successor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ v ] [ 'predecessor' ] [ u ] . append ( ( k , d ) ) for u , v , k , d in get_peripheral_predecessor_edges ( graph , subgraph ) : if not node_filter ( graph , v ) or not node_filter ( graph , u ) or not edge_filter ( graph , u , v , k ) : continue result [ u ] [ 'successor' ] [ v ] . append ( ( k , d ) ) return result
9420	def is_rarfile ( filename ) : mode = constants . RAR_OM_LIST_INCSPLIT archive = unrarlib . RAROpenArchiveDataEx ( filename , mode = mode ) try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : return False unrarlib . RARCloseArchive ( handle ) return ( archive . OpenResult == constants . SUCCESS )
10081	def publish ( self , pid = None , id_ = None ) : pid = pid or self . pid if not pid . is_registered ( ) : raise PIDInvalidAction ( ) self [ '_deposit' ] [ 'status' ] = 'published' if self [ '_deposit' ] . get ( 'pid' ) is None : self . _publish_new ( id_ = id_ ) else : record = self . _publish_edited ( ) record . commit ( ) self . commit ( ) return self
8661	def from_config ( cls , cfg , default_fg = DEFAULT_FG_16 , default_bg = DEFAULT_BG_16 , default_fg_hi = DEFAULT_FG_256 , default_bg_hi = DEFAULT_BG_256 , max_colors = 2 ** 24 ) : e = PaletteEntry ( mono = default_fg , foreground = default_fg , background = default_bg , foreground_high = default_fg_hi , background_high = default_bg_hi ) if isinstance ( cfg , str ) : e . foreground_high = cfg if e . allowed ( cfg , 16 ) : e . foreground = cfg else : rgb = AttrSpec ( fg = cfg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( cfg , dict ) : bg = cfg . get ( "bg" , None ) if isinstance ( bg , str ) : e . background_high = bg if e . allowed ( bg , 16 ) : e . background = bg else : rgb = AttrSpec ( fg = bg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) elif isinstance ( bg , dict ) : e . background_high = bg . get ( "hi" , default_bg_hi ) if "lo" in bg : if e . allowed ( bg [ "lo" ] , 16 ) : e . background = bg [ "lo" ] else : rgb = AttrSpec ( fg = bg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) fg = cfg . get ( "fg" , cfg ) if isinstance ( fg , str ) : e . foreground_high = fg if e . allowed ( fg , 16 ) : e . foreground = fg else : rgb = AttrSpec ( fg = fg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( fg , dict ) : e . foreground_high = fg . get ( "hi" , default_fg_hi ) if "lo" in fg : if e . allowed ( fg [ "lo" ] , 16 ) : e . foreground = fg [ "lo" ] else : rgb = AttrSpec ( fg = fg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) return e
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
6223	def look_at ( self , vec = None , pos = None ) : if pos is None : vec = Vector3 ( pos ) if vec is None : raise ValueError ( "vector or pos must be set" ) return self . _gl_look_at ( self . position , vec , self . _up )
12629	def recursive_glob ( base_directory , regex = '' ) : files = glob ( op . join ( base_directory , regex ) ) for path , dirlist , filelist in os . walk ( base_directory ) : for dir_name in dirlist : files . extend ( glob ( op . join ( path , dir_name , regex ) ) ) return files
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
4218	def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
2233	def _register_builtin_class_extensions ( self ) : @ self . register ( uuid . UUID ) def _hash_uuid ( data ) : hashable = data . bytes prefix = b'UUID' return prefix , hashable @ self . register ( OrderedDict ) def _hash_ordered_dict ( data ) : hashable = b'' . join ( _hashable_sequence ( list ( data . items ( ) ) ) ) prefix = b'ODICT' return prefix , hashable
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
4693	def cmd ( command ) : env ( ) ipmi = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) command = "ipmitool -U %s -P %s -H %s -p %s %s" % ( ipmi [ "USER" ] , ipmi [ "PASS" ] , ipmi [ "HOST" ] , ipmi [ "PORT" ] , command ) cij . info ( "ipmi.command: %s" % command ) return cij . util . execute ( command , shell = True , echo = True )
8150	def _frame_limit ( self , start_time ) : if self . _speed : completion_time = time ( ) exc_time = completion_time - start_time sleep_for = ( 1.0 / abs ( self . _speed ) ) - exc_time if sleep_for > 0 : sleep ( sleep_for )
8851	def open_file ( self , path , line = None ) : editor = None if path : interpreter , pyserver , args = self . _get_backend_parameters ( ) editor = self . tabWidget . open_document ( path , None , interpreter = interpreter , server_script = pyserver , args = args ) if editor : self . setup_editor ( editor ) self . recent_files_manager . open_file ( path ) self . menu_recents . update_actions ( ) if line is not None : TextHelper ( self . tabWidget . current_widget ( ) ) . goto_line ( line ) return editor
10255	def get_causal_source_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_source ( graph , node ) }
5612	def prepare_array ( data , masked = True , nodata = 0 , dtype = "int16" ) : if isinstance ( data , ( list , tuple ) ) : return _prepare_iterable ( data , masked , nodata , dtype ) elif isinstance ( data , np . ndarray ) and data . ndim == 2 : data = ma . expand_dims ( data , axis = 0 ) if isinstance ( data , ma . MaskedArray ) : return _prepare_masked ( data , masked , nodata , dtype ) elif isinstance ( data , np . ndarray ) : if masked : return ma . masked_values ( data . astype ( dtype , copy = False ) , nodata , copy = False ) else : return data . astype ( dtype , copy = False ) else : raise ValueError ( "data must be array, masked array or iterable containing arrays." )
774	def main ( ) : initLogging ( verbose = True ) initExperimentPrng ( ) @ staticmethod def _mockCreate ( * args , ** kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , ** kwargs ) SDRClassifierFactory . create = _mockCreate runExperiment ( sys . argv [ 1 : ] )
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
10424	def infer_missing_two_way_edges ( graph ) : for u , v , k , d in graph . edges ( data = True , keys = True ) : if d [ RELATION ] in TWO_WAY_RELATIONS : infer_missing_backwards_edge ( graph , u , v , k )
2115	def status ( self , pk = None , detail = False , ** kwargs ) : job = self . last_job_data ( pk , ** kwargs ) if detail : return job return { 'elapsed' : job [ 'elapsed' ] , 'failed' : job [ 'failed' ] , 'status' : job [ 'status' ] , }
3210	def get ( self , key , delete_if_expired = True ) : self . _update_cache_stats ( key , None ) if key in self . _CACHE : ( expiration , obj ) = self . _CACHE [ key ] if expiration > self . _now ( ) : self . _update_cache_stats ( key , 'hit' ) return obj else : if delete_if_expired : self . delete ( key ) self . _update_cache_stats ( key , 'expired' ) return None self . _update_cache_stats ( key , 'miss' ) return None
6227	def _translate_string ( self , data , length ) : for index , char in enumerate ( data ) : if index == length : break yield self . _meta . characters - 1 - self . _ct [ char ]
10153	def _extract_operation_from_view ( self , view , args ) : op = { 'responses' : { 'default' : { 'description' : 'UNDOCUMENTED RESPONSE' } } , } renderer = args . get ( 'renderer' , '' ) if "json" in renderer : produces = [ 'application/json' ] elif renderer == 'xml' : produces = [ 'text/xml' ] else : produces = None if produces : op . setdefault ( 'produces' , produces ) consumes = args . get ( 'content_type' ) if consumes is not None : consumes = to_list ( consumes ) consumes = [ x for x in consumes if not callable ( x ) ] op [ 'consumes' ] = consumes is_colander = self . _is_colander_schema ( args ) if is_colander : schema = self . _extract_transform_colander_schema ( args ) parameters = self . parameters . from_schema ( schema ) else : parameters = None if parameters : op [ 'parameters' ] = parameters if isinstance ( view , six . string_types ) : if 'klass' in args : ob = args [ 'klass' ] view_ = getattr ( ob , view . lower ( ) ) docstring = trim ( view_ . __doc__ ) else : docstring = str ( trim ( view . __doc__ ) ) if docstring and self . summary_docstrings : op [ 'summary' ] = docstring if 'response_schemas' in args : op [ 'responses' ] = self . responses . from_schema_mapping ( args [ 'response_schemas' ] ) if 'tags' in args : op [ 'tags' ] = args [ 'tags' ] if 'operation_id' in args : op [ 'operationId' ] = args [ 'operation_id' ] if 'api_security' in args : op [ 'security' ] = args [ 'api_security' ] return op
3623	def __pre_delete_receiver ( self , instance , ** kwargs ) : logger . debug ( 'RECEIVE pre_delete FOR %s' , instance . __class__ ) self . delete_record ( instance )
11838	def result ( self , state , row ) : "Place the next queen at the given row." col = state . index ( None ) new = state [ : ] new [ col ] = row return new
9206	def get_prefix ( multicodec ) : try : prefix = varint . encode ( NAME_TABLE [ multicodec ] ) except KeyError : raise ValueError ( '{} multicodec is not supported.' . format ( multicodec ) ) return prefix
10618	def get_compound_amounts ( self ) : result = self . _compound_masses * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
8349	def isSelfClosingTag ( self , name ) : return self . SELF_CLOSING_TAGS . has_key ( name ) or self . instanceSelfClosingTags . has_key ( name )
11656	def fit_transform ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) X_new = self . transformer . fit_transform ( X . stacked_features , y , ** params ) return self . _gather_outputs ( X , X_new )
10511	def onwindowcreate ( self , window_name , fn_name , * args ) : self . _pollEvents . _callback [ window_name ] = [ "onwindowcreate" , fn_name , args ] return self . _remote_onwindowcreate ( window_name )
44	def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }
5402	def _get_prepare_env ( self , script , job_descriptor , inputs , outputs , mounts ) : docker_paths = sorted ( [ var . docker_path if var . recursive else os . path . dirname ( var . docker_path ) for var in inputs | outputs | mounts if var . value ] ) env = { _SCRIPT_VARNAME : repr ( script . value ) , _META_YAML_VARNAME : repr ( job_descriptor . to_yaml ( ) ) , 'DIR_COUNT' : str ( len ( docker_paths ) ) } for idx , path in enumerate ( docker_paths ) : env [ 'DIR_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , path ) return env
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
7702	def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result = [ ] for item in self . _items : if item . name == name : result . append ( item ) elif item . name is None : continue elif not case_sensitive and item . name . lower ( ) == name : result . append ( item ) return result
4628	def get_private ( self ) : encoded = "%s %d" % ( self . brainkey , self . sequence ) a = _bytes ( encoded ) s = hashlib . sha256 ( hashlib . sha512 ( a ) . digest ( ) ) . digest ( ) return PrivateKey ( hexlify ( s ) . decode ( "ascii" ) , prefix = self . prefix )
2246	def symlink ( real_path , link_path , overwrite = False , verbose = 0 ) : path = normpath ( real_path ) link = normpath ( link_path ) if not os . path . isabs ( path ) : if _can_symlink ( ) : path = os . path . relpath ( path , os . path . dirname ( link ) ) else : path = os . path . abspath ( path ) if verbose : print ( 'Symlink: {path} -> {link}' . format ( path = path , link = link ) ) if islink ( link ) : if verbose : print ( '... already exists' ) pointed = _readlink ( link ) if pointed == path : if verbose > 1 : print ( '... and points to the right place' ) return link if verbose > 1 : if not exists ( link ) : print ( '... but it is broken and points somewhere else: {}' . format ( pointed ) ) else : print ( '... but it points somewhere else: {}' . format ( pointed ) ) if overwrite : util_io . delete ( link , verbose = verbose > 1 ) elif exists ( link ) : if _win32_links is None : if verbose : print ( '... already exists, but its a file. This will error.' ) raise FileExistsError ( 'cannot overwrite a physical path: "{}"' . format ( path ) ) else : if verbose : print ( '... already exists, and is either a file or hard link. ' 'Assuming it is a hard link. ' 'On non-win32 systems this would error.' ) if _win32_links is None : os . symlink ( path , link ) else : _win32_links . _symlink ( path , link , overwrite = overwrite , verbose = verbose ) return link
1627	def CheckForCopyright ( filename , lines , error ) : for line in range ( 1 , min ( len ( lines ) , 11 ) ) : if re . search ( r'Copyright' , lines [ line ] , re . I ) : break else : error ( filename , 0 , 'legal/copyright' , 5 , 'No copyright message found. ' 'You should have a line: "Copyright [year] <Copyright Owner>"' )
13672	def init_build ( self , asset , builder ) : if not self . abs_path : rel_path = utils . prepare_path ( self . rel_bundle_path ) self . abs_bundle_path = utils . prepare_path ( [ builder . config . input_dir , rel_path ] ) self . abs_path = True self . input_dir = builder . config . input_dir
3057	def _load_credentials_file ( credentials_file ) : try : credentials_file . seek ( 0 ) data = json . load ( credentials_file ) except Exception : logger . warning ( 'Credentials file could not be loaded, will ignore and ' 'overwrite.' ) return { } if data . get ( 'file_version' ) != 2 : logger . warning ( 'Credentials file is not version 2, will ignore and ' 'overwrite.' ) return { } credentials = { } for key , encoded_credential in iteritems ( data . get ( 'credentials' , { } ) ) : try : credential_json = base64 . b64decode ( encoded_credential ) credential = client . Credentials . new_from_json ( credential_json ) credentials [ key ] = credential except : logger . warning ( 'Invalid credential {0} in file, ignoring.' . format ( key ) ) return credentials
33	def reset ( self , ** kwargs ) : if self . was_real_done : obs = self . env . reset ( ** kwargs ) else : obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
791	def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' completion_reason=%%s, ' ' completion_msg=%%s, ' ' end_time=UTC_TIMESTAMP(), ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of jobID=%s to " "completed, but this job could not be found or " "belongs to some other CJM" % ( jobID ) )
10166	def get_md_device ( self , line , personalities = [ ] ) : ret = { } splitted = split ( '\W+' , line ) ret [ 'status' ] = splitted [ 1 ] if splitted [ 2 ] in personalities : ret [ 'type' ] = splitted [ 2 ] ret [ 'components' ] = self . get_components ( line , with_type = True ) else : ret [ 'type' ] = None ret [ 'components' ] = self . get_components ( line , with_type = False ) return ret
13331	def add ( name , path , branch , type ) : if not name and not path : ctx = click . get_current_context ( ) click . echo ( ctx . get_help ( ) ) examples = ( '\nExamples:\n' ' cpenv module add my_module ./path/to/my_module\n' ' cpenv module add my_module git@github.com:user/my_module.git' ' cpenv module add my_module git@github.com:user/my_module.git --branch=master --type=shared' ) click . echo ( examples ) return if not name : click . echo ( 'Missing required argument: name' ) return if not path : click . echo ( 'Missing required argument: path' ) env = cpenv . get_active_env ( ) if type == 'local' : if not env : click . echo ( '\nActivate an environment to add a local module.\n' ) return if click . confirm ( '\nAdd {} to active env {}?' . format ( name , env . name ) ) : click . echo ( 'Adding module...' , nl = False ) try : env . add_module ( name , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) ) return module_paths = cpenv . get_module_paths ( ) click . echo ( '\nAvailable module paths:\n' ) for i , mod_path in enumerate ( module_paths ) : click . echo ( ' {}. {}' . format ( i , mod_path ) ) choice = click . prompt ( 'Where do you want to add your module?' , type = int , default = 0 ) module_root = module_paths [ choice ] module_path = utils . unipath ( module_root , name ) click . echo ( 'Creating module {}...' . format ( module_path ) , nl = False ) try : cpenv . create_module ( module_path , path , branch ) except : click . echo ( bold_red ( 'FAILED' ) ) raise else : click . echo ( bold_green ( 'OK!' ) )
370	def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x
3095	def http ( self , * args , ** kwargs ) : return self . credentials . authorize ( transport . get_http_object ( * args , ** kwargs ) )
12178	def ensureDetection ( self ) : if self . APs == False : self . log . debug ( "analysis attempted before event detection..." ) self . detect ( )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
13298	def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
9157	def stroke_linejoin ( self , linejoin ) : linejoin = getattr ( pgmagick . LineJoin , "%sJoin" % linejoin . title ( ) ) linejoin = pgmagick . DrawableStrokeLineJoin ( linejoin ) self . drawer . append ( linejoin )
9477	def parse_dom ( dom ) : root = dom . getElementsByTagName ( "graphml" ) [ 0 ] graph = root . getElementsByTagName ( "graph" ) [ 0 ] name = graph . getAttribute ( 'id' ) g = Graph ( name ) for node in graph . getElementsByTagName ( "node" ) : n = g . add_node ( id = node . getAttribute ( 'id' ) ) for attr in node . getElementsByTagName ( "data" ) : if attr . firstChild : n [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : n [ attr . getAttribute ( "key" ) ] = "" for edge in graph . getElementsByTagName ( "edge" ) : source = edge . getAttribute ( 'source' ) dest = edge . getAttribute ( 'target' ) e = g . add_edge_by_id ( source , dest ) for attr in edge . getElementsByTagName ( "data" ) : if attr . firstChild : e [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : e [ attr . getAttribute ( "key" ) ] = "" return g
1446	def poll ( self ) : try : ret = self . _buffer . get ( block = False ) if self . _producer_callback is not None : self . _producer_callback ( ) return ret except Queue . Empty : Log . debug ( "%s: Empty in poll()" % str ( self ) ) raise Queue . Empty
13203	def format_authors ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : formatted_authors = [ ] for latex_author in self . authors : formatted_author = convert_lsstdoc_tex ( latex_author , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) formatted_author = formatted_author . strip ( ) formatted_authors . append ( formatted_author ) return formatted_authors
10717	def normalize_housecode ( house_code ) : if house_code is None : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) if not isinstance ( house_code , basestring ) : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) if len ( house_code ) != 1 : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) house_code = house_code . upper ( ) if not ( 'A' <= house_code <= 'P' ) : raise X10InvalidHouseCode ( '%r is not a valid house code' % house_code ) return house_code
1713	def ConstructObject ( self , py_obj ) : obj = self . NewObject ( ) for k , v in py_obj . items ( ) : obj . put ( unicode ( k ) , v ) return obj
6906	def total_proper_motion ( pmra , pmdecl , decl ) : pm = np . sqrt ( pmdecl * pmdecl + pmra * pmra * np . cos ( np . radians ( decl ) ) * np . cos ( np . radians ( decl ) ) ) return pm
3089	def locked_get ( self ) : credentials = None if self . _cache : json = self . _cache . get ( self . _key_name ) if json : credentials = client . Credentials . new_from_json ( json ) if credentials is None : entity = self . _get_entity ( ) if entity is not None : credentials = getattr ( entity , self . _property_name ) if self . _cache : self . _cache . set ( self . _key_name , credentials . to_json ( ) ) if credentials and hasattr ( credentials , 'set_store' ) : credentials . set_store ( self ) return credentials
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
9059	def gradient ( self ) : L = self . L n = self . L . shape [ 0 ] grad = { "Lu" : zeros ( ( n , n , n * self . _L . shape [ 1 ] ) ) } for ii in range ( self . _L . shape [ 0 ] * self . _L . shape [ 1 ] ) : row = ii // self . _L . shape [ 1 ] col = ii % self . _L . shape [ 1 ] grad [ "Lu" ] [ row , : , ii ] = L [ : , col ] grad [ "Lu" ] [ : , row , ii ] += L [ : , col ] return grad
3790	def property_derivative_P ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_P ( P , T , zs , ws , method , order ) except : pass return None
5666	def _run ( self ) : if self . _has_run : raise RuntimeError ( "This spreader instance has already been run: " "create a new Spreader object for a new run." ) i = 1 while self . event_heap . size ( ) > 0 and len ( self . _uninfected_stops ) > 0 : event = self . event_heap . pop_next_event ( ) this_stop = self . _stop_I_to_spreading_stop [ event . from_stop_I ] if event . arr_time_ut > self . start_time_ut + self . max_duration_ut : break if this_stop . can_infect ( event ) : target_stop = self . _stop_I_to_spreading_stop [ event . to_stop_I ] already_visited = target_stop . has_been_visited ( ) target_stop . visit ( event ) if not already_visited : self . _uninfected_stops . remove ( event . to_stop_I ) print ( i , self . event_heap . size ( ) ) transfer_distances = self . gtfs . get_straight_line_transfer_distances ( event . to_stop_I ) self . event_heap . add_walk_events_to_heap ( transfer_distances , event , self . start_time_ut , self . walk_speed , self . _uninfected_stops , self . max_duration_ut ) i += 1 self . _has_run = True
13021	def process_columns ( self , columns ) : if type ( columns ) == list : self . columns = columns elif type ( columns ) == str : self . columns = [ c . strip ( ) for c in columns . split ( ) ] elif type ( columns ) == IntEnum : self . columns = [ str ( c ) for c in columns ] else : raise RawlException ( "Unknown format for columns" )
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
8249	def nearest_hue ( self , primary = False ) : if self . is_black : return "black" elif self . is_white : return "white" elif self . is_grey : return "grey" if primary : hues = primary_hues else : hues = named_hues . keys ( ) nearest , d = "" , 1.0 for hue in hues : if abs ( self . hue - named_hues [ hue ] ) % 1 < d : nearest , d = hue , abs ( self . hue - named_hues [ hue ] ) % 1 return nearest
10015	def add_config_files_to_archive ( directory , filename , config = { } ) : with zipfile . ZipFile ( filename , 'a' ) as zip_file : for conf in config : for conf , tree in list ( conf . items ( ) ) : if 'yaml' in tree : content = yaml . dump ( tree [ 'yaml' ] , default_flow_style = False ) else : content = tree . get ( 'content' , '' ) out ( "Adding file " + str ( conf ) + " to archive " + str ( filename ) ) file_entry = zipfile . ZipInfo ( conf ) file_entry . external_attr = tree . get ( 'permissions' , 0o644 ) << 16 zip_file . writestr ( file_entry , content ) return filename
5605	def write_raster_window ( in_tile = None , in_data = None , out_profile = None , out_tile = None , out_path = None , tags = None , bucket_resource = None ) : if not isinstance ( out_path , str ) : raise TypeError ( "out_path must be a string" ) logger . debug ( "write %s" , out_path ) if out_path == "memoryfile" : raise DeprecationWarning ( "Writing to memoryfile with write_raster_window() is deprecated. " "Please use RasterWindowMemoryFile." ) out_tile = in_tile if out_tile is None else out_tile _validate_write_window_params ( in_tile , out_tile , in_data , out_profile ) window_data = extract_from_array ( in_raster = in_data , in_affine = in_tile . affine , out_tile = out_tile ) if in_tile != out_tile else in_data if "affine" in out_profile : out_profile [ "transform" ] = out_profile . pop ( "affine" ) if window_data . all ( ) is not ma . masked : try : if out_path . startswith ( "s3://" ) : with RasterWindowMemoryFile ( in_tile = out_tile , in_data = window_data , out_profile = out_profile , out_tile = out_tile , tags = tags ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with rasterio . open ( out_path , 'w' , ** out_profile ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . write ( window_data . astype ( out_profile [ "dtype" ] , copy = False ) ) _write_tags ( dst , tags ) except Exception as e : logger . exception ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "array window empty" , out_path ) )
4987	def get_course_run_id ( user , enterprise_customer , course_key ) : try : course = CourseCatalogApiServiceClient ( enterprise_customer . site ) . get_course_details ( course_key ) except ImproperlyConfigured : raise Http404 users_all_enrolled_courses = EnrollmentApiClient ( ) . get_enrolled_courses ( user . username ) users_active_course_runs = get_active_course_runs ( course , users_all_enrolled_courses ) if users_all_enrolled_courses else [ ] course_run = get_current_course_run ( course , users_active_course_runs ) if course_run : course_run_id = course_run [ 'key' ] return course_run_id else : raise Http404
4484	def copyfileobj ( fsrc , fdst , total , length = 16 * 1024 ) : with tqdm ( unit = 'bytes' , total = total , unit_scale = True ) as pbar : while 1 : buf = fsrc . read ( length ) if not buf : break fdst . write ( buf ) pbar . update ( len ( buf ) )
7880	def emit_head ( self , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : self . _root_prefixes = dict ( STANDARD_PREFIXES ) self . _root_prefixes [ self . stanza_namespace ] = None for namespace , prefix in self . _root_prefixes . items ( ) : if not prefix or prefix == "stream" : continue if namespace in STANDARD_PREFIXES or namespace in STANZA_NAMESPACES : continue self . _root_prefixes [ namespace ] = prefix tag = u"<{0}:stream version={1}" . format ( STANDARD_PREFIXES [ STREAM_NS ] , quoteattr ( version ) ) if stream_from : tag += u" from={0}" . format ( quoteattr ( stream_from ) ) if stream_to : tag += u" to={0}" . format ( quoteattr ( stream_to ) ) if stream_id is not None : tag += u" id={0}" . format ( quoteattr ( stream_id ) ) if language is not None : tag += u" xml:lang={0}" . format ( quoteattr ( language ) ) for namespace , prefix in self . _root_prefixes . items ( ) : if prefix == "xml" : continue if prefix : tag += u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) else : tag += u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) tag += u">" self . _head_emitted = True return tag
8376	def parse ( svg , cached = False , _copy = True ) : if not cached : dom = parser . parseString ( svg ) paths = parse_node ( dom , [ ] ) else : id = _cache . id ( svg ) if not _cache . has_key ( id ) : dom = parser . parseString ( svg ) _cache . save ( id , parse_node ( dom , [ ] ) ) paths = _cache . load ( id , _copy ) return paths
365	def affine_transform_keypoints ( coords_list , transform_matrix ) : coords_result_list = [ ] for coords in coords_list : coords = np . asarray ( coords ) coords = coords . transpose ( [ 1 , 0 ] ) coords = np . insert ( coords , 2 , 1 , axis = 0 ) coords_result = np . matmul ( transform_matrix , coords ) coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) coords_result_list . append ( coords_result ) return coords_result_list
8962	def freeze ( ctx , local = False ) : cmd = 'pip --disable-pip-version-check freeze{}' . format ( ' --local' if local else '' ) frozen = ctx . run ( cmd , hide = 'out' ) . stdout . replace ( '\x1b' , '#' ) with io . open ( 'frozen-requirements.txt' , 'w' , encoding = 'ascii' ) as out : out . write ( "# Requirements frozen by 'pip freeze' on {}\n" . format ( isodate ( ) ) ) out . write ( frozen ) notify . info ( "Frozen {} requirements." . format ( len ( frozen . splitlines ( ) ) , ) )
6867	def _pkl_magseries_plot ( stimes , smags , serrs , plotdpi = 100 , magsarefluxes = False ) : scaledplottime = stimes - npmin ( stimes ) magseriesfig = plt . figure ( figsize = ( 7.5 , 4.8 ) , dpi = plotdpi ) plt . plot ( scaledplottime , smags , marker = 'o' , ms = 2.0 , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = plt . ylim ( ) plt . ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) plt . xlim ( ( npmin ( scaledplottime ) - 2.0 , npmax ( scaledplottime ) + 2.0 ) ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' plt . xlabel ( plot_xlabel ) plt . ylabel ( plot_ylabel ) plt . gca ( ) . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) plt . gca ( ) . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) magseriespng = StrIO ( ) magseriesfig . savefig ( magseriespng , pad_inches = 0.05 , format = 'png' ) plt . close ( ) magseriespng . seek ( 0 ) magseriesb64 = base64 . b64encode ( magseriespng . read ( ) ) magseriespng . close ( ) checkplotdict = { 'magseries' : { 'plot' : magseriesb64 , 'times' : stimes , 'mags' : smags , 'errs' : serrs } } return checkplotdict
2791	def get_object ( cls , api_token , cert_id ) : certificate = cls ( token = api_token , id = cert_id ) certificate . load ( ) return certificate
12512	def niftilist_to_array ( img_filelist , outdtype = None ) : try : first_img = img_filelist [ 0 ] vol = get_img_data ( first_img ) except IndexError as ie : raise Exception ( 'Error getting the first item of img_filelis: {}' . format ( repr_imgs ( img_filelist [ 0 ] ) ) ) from ie if not outdtype : outdtype = vol . dtype outmat = np . zeros ( ( len ( img_filelist ) , np . prod ( vol . shape ) ) , dtype = outdtype ) try : for i , img_file in enumerate ( img_filelist ) : vol = get_img_data ( img_file ) outmat [ i , : ] = vol . flatten ( ) except Exception as exc : raise Exception ( 'Error on reading file {0}.' . format ( img_file ) ) from exc return outmat , vol . shape
7012	def lcdict_to_pickle ( lcdict , outfile = None ) : if not outfile and lcdict [ 'objectid' ] : outfile = '%s-hplc.pkl' % lcdict [ 'objectid' ] elif not outfile and not lcdict [ 'objectid' ] : outfile = 'hplc.pkl' with open ( outfile , 'wb' ) as outfd : pickle . dump ( lcdict , outfd , protocol = pickle . HIGHEST_PROTOCOL ) if os . path . exists ( outfile ) : LOGINFO ( 'lcdict for object: %s -> %s OK' % ( lcdict [ 'objectid' ] , outfile ) ) return outfile else : LOGERROR ( 'could not make a pickle for this lcdict!' ) return None
8790	def pop ( self , model ) : tags = self . _pop ( model ) if tags : for tag in tags : value = self . deserialize ( tag ) try : self . validate ( value ) return value except TagValidationError : continue
10351	def get_entrez_gene_data ( entrez_ids : Iterable [ Union [ str , int ] ] ) : url = PUBMED_GENE_QUERY_URL . format ( ',' . join ( str ( x ) . strip ( ) for x in entrez_ids ) ) response = requests . get ( url ) tree = ElementTree . fromstring ( response . content ) return { element . attrib [ 'uid' ] : { 'summary' : _sanitize ( element . find ( 'Summary' ) . text ) , 'description' : element . find ( 'Description' ) . text } for element in tree . findall ( './DocumentSummarySet/DocumentSummary' ) }
9365	def email_address ( user = None ) : if not user : user = user_name ( ) else : user = user . strip ( ) . replace ( ' ' , '_' ) . lower ( ) return user + '@' + domain_name ( )
4743	def exists ( ) : if env ( ) : cij . err ( "cij.nvm.exists: Invalid NVMe ENV." ) return 1 nvm = cij . env_to_dict ( PREFIX , EXPORTED + REQUIRED ) cmd = [ '[[ -b "%s" ]]' % nvm [ "DEV_PATH" ] ] rcode , _ , _ = cij . ssh . command ( cmd , shell = True , echo = False ) return rcode
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
209	def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv
297	def plot_return_quantiles ( returns , live_start_date = None , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ "#4c72B0" , "#55A868" , "#CCB974" ] , ax = ax , ** kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = "red" , marker = "d" , ** kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = "red" , marker = "d" , label = "Out-of-sample data" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax
180	def to_bounding_box ( self ) : from . bbs import BoundingBox if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )
9752	def build_swig ( ) : print ( "Looking for FANN libs..." ) find_fann ( ) print ( "running SWIG..." ) swig_bin = find_swig ( ) swig_cmd = [ swig_bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig_cmd ) . wait ( )
3686	def solve_T ( self , P , V , quick = True ) : def to_solve ( T ) : a_alpha = self . a_alpha_and_derivatives ( T , full = False ) P_calc = R * T / ( V - self . b ) - a_alpha / ( V * V + self . delta * V + self . epsilon ) return P_calc - P return newton ( to_solve , self . Tc * 0.5 )
4703	def compare ( buf_a , buf_b , ignore ) : for field in getattr ( buf_a , '_fields_' ) : name , types = field [ 0 ] , field [ 1 ] if name in ignore : continue val_a = getattr ( buf_a , name ) val_b = getattr ( buf_b , name ) if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a , val_b , ignore ) : return 1 elif isinstance ( types , type ( Array ) ) : for i , _ in enumerate ( val_a ) : if isinstance ( types , ( type ( Union ) , type ( Structure ) ) ) : if compare ( val_a [ i ] , val_b [ i ] , ignore ) : return 1 else : if val_a [ i ] != val_b [ i ] : return 1 else : if val_a != val_b : return 1 return 0
1574	def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
13157	def count ( cls , cur , table : str , where_keys : list = None ) : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _count_query_where . format ( table , where_clause ) q , t = query , values else : query = cls . _count_query . format ( table ) q , t = query , ( ) yield from cur . execute ( q , t ) result = yield from cur . fetchone ( ) return int ( result [ 0 ] )
11937	def create_message ( self , level , msg_text , extra_tags = '' , date = None , url = None ) : if not date : now = timezone . now ( ) else : now = date r = now . isoformat ( ) if now . microsecond : r = r [ : 23 ] + r [ 26 : ] if r . endswith ( '+00:00' ) : r = r [ : - 6 ] + 'Z' fingerprint = r + msg_text msg_id = hashlib . sha256 ( fingerprint . encode ( 'ascii' , 'ignore' ) ) . hexdigest ( ) return Message ( id = msg_id , message = msg_text , level = level , tags = extra_tags , date = r , url = url )
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
5054	def get_identity_provider ( provider_id ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None try : return Registry and Registry . get ( provider_id ) except ValueError : return None
5771	def _advapi32_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : algo = certificate_or_public_key . algorithm if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) decrypted_signature = raw_rsa_public_crypt ( certificate_or_public_key , signature ) key_size = certificate_or_public_key . bit_size if not verify_pss_padding ( hash_algorithm , hash_length , key_size , data , decrypted_signature ) : raise SignatureError ( 'Signature is invalid' ) return if algo == 'rsa' and hash_algorithm == 'raw' : padded_plaintext = raw_rsa_public_crypt ( certificate_or_public_key , signature ) try : plaintext = remove_pkcs1v15_signature_padding ( certificate_or_public_key . byte_size , padded_plaintext ) if not constant_compare ( plaintext , data ) : raise ValueError ( ) except ( ValueError ) : raise SignatureError ( 'Signature is invalid' ) return hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( certificate_or_public_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) if algo == 'dsa' : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) half_len = len ( signature ) // 2 signature = signature [ half_len : ] + signature [ : half_len ] except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) reversed_signature = signature [ : : - 1 ] res = advapi32 . CryptVerifySignatureW ( hash_handle , reversed_signature , len ( signature ) , certificate_or_public_key . key_handle , null ( ) , 0 ) handle_error ( res ) finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
11230	def xafter ( self , dt , count = None , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : def comp ( dc , dtc ) : return dc >= dtc else : def comp ( dc , dtc ) : return dc > dtc n = 0 for d in gen : if comp ( d , dt ) : if count is not None : n += 1 if n > count : break yield d
13172	def last ( self , name = None ) : for c in self . children ( name , reverse = True ) : return c
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
13488	def create ( self , server ) : for chunk in self . __cut_to_size ( ) : server . post ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
12668	def matrix_to_4dvolume ( arr , mask , order = 'C' ) : if mask . dtype != np . bool : raise ValueError ( "mask must be a boolean array" ) if arr . ndim != 2 : raise ValueError ( "X must be a 2-dimensional array" ) if mask . sum ( ) != arr . shape [ 0 ] : raise ValueError ( 'Expected arr of shape ({}, samples). Got {}.' . format ( mask . sum ( ) , arr . shape ) ) data = np . zeros ( mask . shape + ( arr . shape [ 1 ] , ) , dtype = arr . dtype , order = order ) data [ mask , : ] = arr return data
3477	def _dissociate_gene ( self , cobra_gene ) : self . _genes . discard ( cobra_gene ) cobra_gene . _reaction . discard ( self )
7686	def downbeat ( annotation , sr = 22050 , length = None , ** kwargs ) : beat_click = mkclick ( 440 * 2 , sr = sr ) downbeat_click = mkclick ( 440 * 3 , sr = sr ) intervals , values = annotation . to_interval_values ( ) beats , downbeats = [ ] , [ ] for time , value in zip ( intervals [ : , 0 ] , values ) : if value [ 'position' ] == 1 : downbeats . append ( time ) else : beats . append ( time ) if length is None : length = int ( sr * np . max ( intervals ) ) + len ( beat_click ) + 1 y = filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( beats ) , fs = sr , length = length , click = beat_click ) y += filter_kwargs ( mir_eval . sonify . clicks , np . asarray ( downbeats ) , fs = sr , length = length , click = downbeat_click ) return y
9933	def walk ( self , maxresults = 100 , maxdepth = None ) : log . debug ( "step" ) self . seen = { } self . ignore ( self , self . __dict__ , self . obj , self . seen , self . _ignore ) self . ignore_caller ( ) self . maxdepth = maxdepth count = 0 log . debug ( "will iterate results" ) for result in self . _gen ( self . obj ) : log . debug ( "will yeld" ) yield result count += 1 if maxresults and count >= maxresults : yield 0 , 0 , "==== Max results reached ====" return
3690	def solve_T ( self , P , V , quick = True ) : r a , b = self . a , self . b if quick : x1 = - 1.j * 1.7320508075688772 + 1. x2 = V - b x3 = x2 / R x4 = V + b x5 = ( 1.7320508075688772 * ( x2 * x2 * ( - 4. * P * P * P * x3 + 27. * a * a / ( V * V * x4 * x4 ) ) / ( R * R ) ) ** 0.5 - 9. * a * x3 / ( V * x4 ) + 0j ) ** ( 1. / 3. ) return ( 3.3019272488946263 * ( 11.537996562459266 * P * x3 / ( x1 * x5 ) + 1.2599210498948732 * x1 * x5 ) ** 2 / 144.0 ) . real else : return ( ( - ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) / 3 + ( - P * V + P * b ) / ( R * ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) ) ) ** 2 ) . real
4796	def contains_entry ( self , * args , ** kwargs ) : self . _check_dict_like ( self . val , check_values = False ) entries = list ( args ) + [ { k : v } for k , v in kwargs . items ( ) ] if len ( entries ) == 0 : raise ValueError ( 'one or more entry args must be given' ) missing = [ ] for e in entries : if type ( e ) is not dict : raise TypeError ( 'given entry arg must be a dict' ) if len ( e ) != 1 : raise ValueError ( 'given entry args must contain exactly one key-value pair' ) k = next ( iter ( e ) ) if k not in self . val : missing . append ( e ) elif self . val [ k ] != e [ k ] : missing . append ( e ) if missing : self . _err ( 'Expected <%s> to contain entries %s, but did not contain %s.' % ( self . val , self . _fmt_items ( entries ) , self . _fmt_items ( missing ) ) ) return self
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
4453	def alias ( self , alias ) : if alias is FIELDNAME : if not self . _field : raise ValueError ( "Cannot use FIELDNAME alias with no field" ) alias = self . _field [ 1 : ] self . _alias = alias return self
8368	def _parse ( self ) : p1 = "\[.*?\](.*?)\[\/.*?\]" p2 = "\[(.*?)\]" self . links = [ ] for p in ( p1 , p2 ) : for link in re . findall ( p , self . description ) : self . links . append ( link ) self . description = re . sub ( p , "\\1" , self . description ) self . description = self . description . strip ( )
1289	def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments
1594	def choose_tasks ( self , stream_id , values ) : if stream_id not in self . targets : return [ ] ret = [ ] for target in self . targets [ stream_id ] : ret . extend ( target . choose_tasks ( values ) ) return ret
12612	def search_unique ( self , table_name , sample , unique_fields = None ) : return search_unique ( table = self . table ( table_name ) , sample = sample , unique_fields = unique_fields )
3765	def Joule_Thomson ( T , V , Cp , dV_dT = None , beta = None ) : r if dV_dT : return ( T * dV_dT - V ) / Cp elif beta : return V / Cp * ( beta * T - 1. ) else : raise Exception ( 'Either dV_dT or beta is needed' )
1201	def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ]
5276	def _terminalSymbolsGenerator ( self ) : py2 = sys . version [ 0 ] < '3' UPPAs = list ( list ( range ( 0xE000 , 0xF8FF + 1 ) ) + list ( range ( 0xF0000 , 0xFFFFD + 1 ) ) + list ( range ( 0x100000 , 0x10FFFD + 1 ) ) ) for i in UPPAs : if py2 : yield ( unichr ( i ) ) else : yield ( chr ( i ) ) raise ValueError ( "To many input strings." )
7800	def encode ( self ) : if self . data is None : return "" elif not self . data : return "=" else : ret = standard_b64encode ( self . data ) return ret . decode ( "us-ascii" )
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
7202	def deprecate_module_attr ( mod , deprecated ) : deprecated = set ( deprecated ) class Wrapper ( object ) : def __getattr__ ( self , attr ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return getattr ( mod , attr ) def __setattr__ ( self , attr , value ) : if attr in deprecated : warnings . warn ( "Property {} is deprecated" . format ( attr ) , GBDXDeprecation ) return setattr ( mod , attr , value ) return Wrapper ( )
4296	def parse_config_file ( parser , stdin_args ) : config_args = [ ] required_args = [ ] for action in parser . _actions : if action . required : required_args . append ( action ) action . required = False parsed_args = parser . parse_args ( stdin_args ) for action in required_args : action . required = True if not parsed_args . config_file : return config_args config = ConfigParser ( ) if not config . read ( parsed_args . config_file ) : sys . stderr . write ( 'Config file "{0}" doesn\'t exists\n' . format ( parsed_args . config_file ) ) sys . exit ( 7 ) config_args = _convert_config_to_stdin ( config , parser ) return config_args
7246	def launch ( self , workflow ) : try : r = self . gbdx_connection . post ( self . workflows_url , json = workflow ) try : r . raise_for_status ( ) except : print ( "GBDX API Status Code: %s" % r . status_code ) print ( "GBDX API Response: %s" % r . text ) r . raise_for_status ( ) workflow_id = r . json ( ) [ 'id' ] return workflow_id except TypeError : self . logger . debug ( 'Workflow not launched!' )
3098	def validate_token ( key , token , user_id , action_id = "" , current_time = None ) : if not token : return False try : decoded = base64 . urlsafe_b64decode ( token ) token_time = int ( decoded . split ( DELIMITER ) [ - 1 ] ) except ( TypeError , ValueError , binascii . Error ) : return False if current_time is None : current_time = time . time ( ) if current_time - token_time > DEFAULT_TIMEOUT_SECS : return False expected_token = generate_token ( key , user_id , action_id = action_id , when = token_time ) if len ( token ) != len ( expected_token ) : return False different = 0 for x , y in zip ( bytearray ( token ) , bytearray ( expected_token ) ) : different |= x ^ y return not different
3106	def code_challenge ( verifier ) : digest = hashlib . sha256 ( verifier ) . digest ( ) return base64 . urlsafe_b64encode ( digest ) . rstrip ( b'=' )
5845	def kill_design_run ( self , data_view_id , run_uuid ) : url = routes . kill_data_view_design_run ( data_view_id , run_uuid ) response = self . _delete ( url ) . json ( ) return response [ "data" ] [ "uid" ]
2222	def _rectify_base ( base ) : if base is NoParam or base == 'default' : return DEFAULT_ALPHABET elif base in [ 26 , 'abc' , 'alpha' ] : return _ALPHABET_26 elif base in [ 16 , 'hex' ] : return _ALPHABET_16 elif base in [ 10 , 'dec' ] : return _ALPHABET_10 else : if not isinstance ( base , ( list , tuple ) ) : raise TypeError ( 'Argument `base` must be a key, list, or tuple; not {}' . format ( type ( base ) ) ) return base
893	def cellsForColumn ( self , column ) : self . _validateColumn ( column ) start = self . cellsPerColumn * column end = start + self . cellsPerColumn return range ( start , end )
9035	def walk_instructions ( self , mapping = identity ) : instructions = chain ( * self . walk_rows ( lambda row : row . instructions ) ) return map ( mapping , instructions )
11797	def nconflicts ( self , var , val , assignment ) : "Return the number of conflicts var=val has with other variables." def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count_if ( conflict , self . neighbors [ var ] )
9104	def dropbox_post_factory ( request ) : try : max_age = int ( request . registry . settings . get ( 'post_token_max_age_seconds' ) ) except Exception : max_age = 300 try : drop_id = parse_post_token ( token = request . matchdict [ 'token' ] , secret = request . registry . settings [ 'post_secret' ] , max_age = max_age ) except SignatureExpired : raise HTTPGone ( 'dropbox expired' ) except Exception : raise HTTPNotFound ( 'no such dropbox' ) dropbox = request . registry . settings [ 'dropbox_container' ] . get_dropbox ( drop_id ) if dropbox . status_int >= 20 : raise HTTPGone ( 'dropbox already in processing, no longer accepts data' ) return dropbox
8500	def _map_arg ( arg ) : if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : return Unparseable ( )
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
11161	def autopep8 ( self , ** kwargs ) : self . assert_is_dir_and_exists ( ) for p in self . select_by_ext ( ".py" ) : with open ( p . abspath , "rb" ) as f : code = f . read ( ) . decode ( "utf-8" ) formatted_code = autopep8 . fix_code ( code , ** kwargs ) with open ( p . abspath , "wb" ) as f : f . write ( formatted_code . encode ( "utf-8" ) )
10326	def canonical_averages ( ps , microcanonical_averages_arrays ) : num_sites = microcanonical_averages_arrays [ 'N' ] num_edges = microcanonical_averages_arrays [ 'M' ] spanning_cluster = ( 'spanning_cluster' in microcanonical_averages_arrays ) ret = dict ( ) ret [ 'ps' ] = ps ret [ 'N' ] = num_sites ret [ 'M' ] = num_edges ret [ 'max_cluster_size' ] = np . empty ( ps . size ) ret [ 'max_cluster_size_ci' ] = np . empty ( ( ps . size , 2 ) ) if spanning_cluster : ret [ 'spanning_cluster' ] = np . empty ( ps . size ) ret [ 'spanning_cluster_ci' ] = np . empty ( ( ps . size , 2 ) ) ret [ 'moments' ] = np . empty ( ( 5 , ps . size ) ) ret [ 'moments_ci' ] = np . empty ( ( 5 , ps . size , 2 ) ) for p_index , p in enumerate ( ps ) : binomials = _binomial_pmf ( n = num_edges , p = p ) for key , value in microcanonical_averages_arrays . items ( ) : if len ( key ) <= 1 : continue if key in [ 'max_cluster_size' , 'spanning_cluster' ] : ret [ key ] [ p_index ] = np . sum ( binomials * value ) elif key in [ 'max_cluster_size_ci' , 'spanning_cluster_ci' ] : ret [ key ] [ p_index ] = np . sum ( np . tile ( binomials , ( 2 , 1 ) ) . T * value , axis = 0 ) elif key == 'moments' : ret [ key ] [ : , p_index ] = np . sum ( np . tile ( binomials , ( 5 , 1 ) ) * value , axis = 1 ) elif key == 'moments_ci' : ret [ key ] [ : , p_index ] = np . sum ( np . rollaxis ( np . tile ( binomials , ( 5 , 2 , 1 ) ) , 2 , 1 ) * value , axis = 1 ) else : raise NotImplementedError ( '{}-dimensional array' . format ( value . ndim ) ) return ret
8307	def get_command_responses ( self ) : if not self . response_queue . empty ( ) : yield None while not self . response_queue . empty ( ) : line = self . response_queue . get ( ) if line is not None : yield line
219	def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f"Package {package!r} could not be found." assert ( spec . origin is not None ) , "Directory 'statics' in package {package!r} could not be found." directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) assert os . path . isdir ( directory ) , "Directory 'statics' in package {package!r} could not be found." directories . append ( directory ) return directories
8015	async def dispatch_downstream ( self , message , steam_name ) : handler = getattr ( self , get_handler_name ( message ) , None ) if handler : await handler ( message , stream_name = steam_name ) else : await self . base_send ( message )
13904	def parse_hub_key ( key ) : if key is None : raise ValueError ( 'Not a valid key' ) match = re . match ( PATTERN , key ) if not match : match = re . match ( PATTERN_S0 , key ) if not match : raise ValueError ( 'Not a valid key' ) return dict ( map ( normalise_part , zip ( [ p for p in PARTS_S0 . keys ( ) ] , match . groups ( ) ) ) ) return dict ( zip ( PARTS . keys ( ) , match . groups ( ) ) )
2025	def SGT ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) return Operators . ITEBV ( 256 , s0 > s1 , 1 , 0 )
5963	def plot ( self , ** kwargs ) : columns = kwargs . pop ( 'columns' , Ellipsis ) maxpoints = kwargs . pop ( 'maxpoints' , self . maxpoints_default ) transform = kwargs . pop ( 'transform' , lambda x : x ) method = kwargs . pop ( 'method' , "mean" ) ax = kwargs . pop ( 'ax' , None ) if columns is Ellipsis or columns is None : columns = numpy . arange ( self . array . shape [ 0 ] ) if len ( columns ) == 0 : raise MissingDataError ( "plot() needs at least one column of data" ) if len ( self . array . shape ) == 1 or self . array . shape [ 0 ] == 1 : a = numpy . ravel ( self . array ) X = numpy . arange ( len ( a ) ) a = numpy . vstack ( ( X , a ) ) columns = [ 0 ] + [ c + 1 for c in columns ] else : a = self . array color = kwargs . pop ( 'color' , self . default_color_cycle ) try : cmap = matplotlib . cm . get_cmap ( color ) colors = cmap ( matplotlib . colors . Normalize ( ) ( numpy . arange ( len ( columns [ 1 : ] ) , dtype = float ) ) ) except TypeError : colors = cycle ( utilities . asiterable ( color ) ) if ax is None : ax = plt . gca ( ) a = self . decimate ( method , numpy . asarray ( transform ( a ) ) [ columns ] , maxpoints = maxpoints ) ma = numpy . ma . MaskedArray ( a , mask = numpy . logical_not ( numpy . isfinite ( a ) ) ) for column , color in zip ( range ( 1 , len ( columns ) ) , colors ) : if len ( ma [ column ] ) == 0 : warnings . warn ( "No data to plot for column {column:d}" . format ( ** vars ( ) ) , category = MissingDataWarning ) kwargs [ 'color' ] = color ax . plot ( ma [ 0 ] , ma [ column ] , ** kwargs ) return ax
3179	def get ( self , batch_webhook_id , ** queryparams ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _get ( url = self . _build_path ( batch_webhook_id ) , ** queryparams )
13706	def iter_space_block ( self , text = None , width = 60 , fmtfunc = str ) : if width < 1 : width = 1 curline = '' text = ( self . text if text is None else text ) or '' for word in text . split ( ) : possibleline = ' ' . join ( ( curline , word ) ) if curline else word codelen = sum ( len ( s ) for s in get_codes ( possibleline ) ) reallen = len ( possibleline ) - codelen if reallen > width : yield fmtfunc ( curline ) curline = word else : curline = possibleline if curline : yield fmtfunc ( curline )
2365	def RobotFactory ( path , parent = None ) : if os . path . isdir ( path ) : return SuiteFolder ( path , parent ) else : rf = RobotFile ( path , parent ) for table in rf . tables : if isinstance ( table , TestcaseTable ) : rf . __class__ = SuiteFile return rf rf . __class__ = ResourceFile return rf
5381	def _datetime_to_utc_int ( date ) : if date is None : return None epoch = dsub_util . replace_timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total_seconds ( )
6169	def filter ( self , x ) : y = signal . lfilter ( self . b , [ 1 ] , x ) return y
160	def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy )
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
7850	def add_feature ( self , var ) : if self . has_feature ( var ) : return n = self . xmlnode . newChild ( None , "feature" , None ) n . setProp ( "var" , to_utf8 ( var ) )
11994	def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
1584	def yaml_config_reader ( config_path ) : if not config_path . endswith ( ".yaml" ) : raise ValueError ( "Config file not yaml" ) with open ( config_path , 'r' ) as f : config = yaml . load ( f ) return config
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
3777	def calculate_integral ( self , T1 , T2 , method ) : r return float ( quad ( self . calculate , T1 , T2 , args = ( method ) ) [ 0 ] )
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
4840	def get_program_by_title ( self , program_title ) : all_programs = self . _load_data ( self . PROGRAMS_ENDPOINT , default = [ ] ) matching_programs = [ program for program in all_programs if program . get ( 'title' ) == program_title ] if len ( matching_programs ) > 1 : raise MultipleProgramMatchError ( len ( matching_programs ) ) elif len ( matching_programs ) == 1 : return matching_programs [ 0 ] else : return None
9419	def format_docstring ( * args , ** kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , ** kwargs ) return func return decorator
6381	def dist_jaro_winkler ( src , tar , qval = 1 , mode = 'winkler' , long_strings = False , boost_threshold = 0.7 , scaling_factor = 0.1 , ) : return JaroWinkler ( ) . dist ( src , tar , qval , mode , long_strings , boost_threshold , scaling_factor )
2308	def forward ( self , input ) : return th . nn . functional . linear ( input , self . weight . div ( self . weight . pow ( 2 ) . sum ( 0 ) . sqrt ( ) ) )
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : fmt = '' if l > 1 : fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : if num_bytes < 4 : fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) fd . write ( struct . pack ( fmt , * data ) )
7448	def _samples_precheck ( self , samples , mystep , force ) : subsample = [ ] for sample in samples : if sample . stats . state < mystep - 1 : LOGGER . debug ( "Sample {} not in proper state." . format ( sample . name ) ) else : subsample . append ( sample ) return subsample
7992	def _send_stream_error ( self , condition ) : if self . _output_state is "closed" : return if self . _output_state in ( None , "restart" ) : self . _send_stream_start ( ) element = StreamErrorElement ( condition ) . as_xml ( ) self . transport . send_element ( element ) self . transport . disconnect ( ) self . _output_state = "closed"
8122	def error ( message ) : global parser print ( _ ( "Error: " ) + message ) print ( ) parser . print_help ( ) sys . exit ( )
5044	def notify_program_learners ( cls , enterprise_customer , program_details , users ) : program_name = program_details . get ( 'title' ) program_branding = program_details . get ( 'type' ) program_uuid = program_details . get ( 'uuid' ) lms_root_url = get_configuration_value_for_site ( enterprise_customer . site , 'LMS_ROOT_URL' , settings . LMS_ROOT_URL ) program_path = urlquote ( '/dashboard/programs/{program_uuid}/?tpa_hint={tpa_hint}' . format ( program_uuid = program_uuid , tpa_hint = enterprise_customer . identity_provider , ) ) destination_url = '{site}/{login_or_register}?next={program_path}' . format ( site = lms_root_url , login_or_register = '{login_or_register}' , program_path = program_path ) program_type = 'program' program_start = get_earliest_start_date_from_program ( program_details ) with mail . get_connection ( ) as email_conn : for user in users : login_or_register = 'register' if isinstance ( user , PendingEnterpriseCustomerUser ) else 'login' destination_url = destination_url . format ( login_or_register = login_or_register ) send_email_notification_message ( user = user , enrolled_in = { 'name' : program_name , 'url' : destination_url , 'type' : program_type , 'start' : program_start , 'branding' : program_branding , } , enterprise_customer = enterprise_customer , email_connection = email_conn )
2647	def bash_app ( function = None , data_flow_kernel = None , walltime = 60 , cache = False , executors = 'all' ) : from parsl . app . bash import BashApp def decorator ( func ) : def wrapper ( f ) : return BashApp ( f , data_flow_kernel = data_flow_kernel , walltime = walltime , cache = cache , executors = executors ) return wrapper ( func ) if function is not None : return decorator ( function ) return decorator
3954	def remove_exited_dusty_containers ( ) : client = get_docker_client ( ) exited_containers = get_exited_dusty_containers ( ) removed_containers = [ ] for container in exited_containers : log_to_client ( "Removing container {}" . format ( container [ 'Names' ] [ 0 ] ) ) try : client . remove_container ( container [ 'Id' ] , v = True ) removed_containers . append ( container ) except Exception as e : log_to_client ( e . message or str ( e ) ) return removed_containers
2119	def _parent_filter ( self , parent , relationship , ** kwargs ) : if parent is None or relationship is None : return { } parent_filter_kwargs = { } query_params = ( ( self . _reverse_rel_name ( relationship ) , parent ) , ) parent_filter_kwargs [ 'query' ] = query_params if kwargs . get ( 'workflow_job_template' , None ) is None : parent_data = self . read ( pk = parent ) [ 'results' ] [ 0 ] parent_filter_kwargs [ 'workflow_job_template' ] = parent_data [ 'workflow_job_template' ] return parent_filter_kwargs
6212	def plane_xz ( size = ( 10 , 10 ) , resolution = ( 10 , 10 ) ) -> VAO : sx , sz = size rx , rz = resolution dx , dz = sx / rx , sz / rz ox , oz = - sx / 2 , - sz / 2 def gen_pos ( ) : for z in range ( rz ) : for x in range ( rx ) : yield ox + x * dx yield 0 yield oz + z * dz def gen_uv ( ) : for z in range ( rz ) : for x in range ( rx ) : yield x / ( rx - 1 ) yield 1 - z / ( rz - 1 ) def gen_normal ( ) : for _ in range ( rx * rz ) : yield 0.0 yield 1.0 yield 0.0 def gen_index ( ) : for z in range ( rz - 1 ) : for x in range ( rx - 1 ) : yield z * rz + x + 1 yield z * rz + x yield z * rz + x + rx yield z * rz + x + 1 yield z * rz + x + rx yield z * rz + x + rx + 1 pos_data = numpy . fromiter ( gen_pos ( ) , dtype = numpy . float32 ) uv_data = numpy . fromiter ( gen_uv ( ) , dtype = numpy . float32 ) normal_data = numpy . fromiter ( gen_normal ( ) , dtype = numpy . float32 ) index_data = numpy . fromiter ( gen_index ( ) , dtype = numpy . uint32 ) vao = VAO ( "plane_xz" , mode = moderngl . TRIANGLES ) vao . buffer ( pos_data , '3f' , [ 'in_position' ] ) vao . buffer ( uv_data , '2f' , [ 'in_uv' ] ) vao . buffer ( normal_data , '3f' , [ 'in_normal' ] ) vao . index_buffer ( index_data , index_element_size = 4 ) return vao
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
13783	def _MakeFieldDescriptor ( self , field_proto , message_name , index , is_extension = False ) : if message_name : full_name = '.' . join ( ( message_name , field_proto . name ) ) else : full_name = field_proto . name return descriptor . FieldDescriptor ( name = field_proto . name , full_name = full_name , index = index , number = field_proto . number , type = field_proto . type , cpp_type = None , message_type = None , enum_type = None , containing_type = None , label = field_proto . label , has_default_value = False , default_value = None , is_extension = is_extension , extension_scope = None , options = field_proto . options )
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
4086	def _process_events ( self , events ) : for f , callback , transferred , key , ov in events : try : self . _logger . debug ( 'Invoking event callback {}' . format ( callback ) ) value = callback ( transferred , key , ov ) except OSError : self . _logger . warning ( 'Event callback failed' , exc_info = sys . exc_info ( ) ) else : f . set_result ( value )
9604	def raise_for_status ( self ) : if not self . status : return error = find_exception_by_code ( self . status ) message = None screen = None stacktrace = None if isinstance ( self . value , str ) : message = self . value elif isinstance ( self . value , dict ) : message = self . value . get ( 'message' , None ) screen = self . value . get ( 'screen' , None ) stacktrace = self . value . get ( 'stacktrace' , None ) raise WebDriverException ( error , message , screen , stacktrace )
6115	def fit_lens_data_with_sensitivity_tracers ( lens_data , tracer_normal , tracer_sensitive ) : if ( tracer_normal . has_light_profile and tracer_sensitive . has_light_profile ) and ( not tracer_normal . has_pixelization and not tracer_sensitive . has_pixelization ) : return SensitivityProfileFit ( lens_data = lens_data , tracer_normal = tracer_normal , tracer_sensitive = tracer_sensitive ) elif ( not tracer_normal . has_light_profile and not tracer_sensitive . has_light_profile ) and ( tracer_normal . has_pixelization and tracer_sensitive . has_pixelization ) : return SensitivityInversionFit ( lens_data = lens_data , tracer_normal = tracer_normal , tracer_sensitive = tracer_sensitive ) else : raise exc . FittingException ( 'The sensitivity_fit routine did not call a SensitivityFit class - check the ' 'properties of the tracers' )
12733	def move_next_to ( self , body_a , body_b , offset_a , offset_b ) : ba = self . get_body ( body_a ) bb = self . get_body ( body_b ) if ba is None : return bb . relative_offset_to_world ( offset_b ) if bb is None : return ba . relative_offset_to_world ( offset_a ) anchor = ba . relative_offset_to_world ( offset_a ) offset = bb . relative_offset_to_world ( offset_b ) bb . position = bb . position + anchor - offset return anchor
12217	def print_file_info ( ) : tpl = TableLogger ( columns = 'file,created,modified,size' ) for f in os . listdir ( '.' ) : size = os . stat ( f ) . st_size date_created = datetime . fromtimestamp ( os . path . getctime ( f ) ) date_modified = datetime . fromtimestamp ( os . path . getmtime ( f ) ) tpl ( f , date_created , date_modified , size )
967	def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]
7934	def _compute_handshake ( self ) : return hashlib . sha1 ( to_utf8 ( self . stream_id ) + to_utf8 ( self . secret ) ) . hexdigest ( )
5996	def plot_border ( mask , should_plot_border , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if should_plot_border and mask is not None : plt . gca ( ) border_pixels = mask . masked_grid_index_to_pixel [ mask . border_pixels ] if zoom_offset_pixels is not None : border_pixels -= zoom_offset_pixels border_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = border_pixels ) border_units = convert_grid_units ( array = mask , grid_arcsec = border_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = border_units [ : , 0 ] , x = border_units [ : , 1 ] , s = pointsize , c = 'y' )
10892	def translate ( self , dr ) : tile = self . copy ( ) tile . l += dr tile . r += dr return tile
6757	def reboot_or_dryrun ( self , * args , ** kwargs ) : warnings . warn ( 'Use self.run() instead.' , DeprecationWarning , stacklevel = 2 ) self . reboot ( * args , ** kwargs )
3635	def bid ( self , trade_id , bid , fast = False ) : method = 'PUT' url = 'trade/%s/bid' % trade_id if not fast : rc = self . tradeStatus ( trade_id ) [ 0 ] if rc [ 'currentBid' ] >= bid or self . credits < bid : return False data = { 'bid' : bid } try : rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } , fast = fast ) [ 'auctionInfo' ] [ 0 ] except PermissionDenied : return False if rc [ 'bidState' ] == 'highest' or ( rc [ 'tradeState' ] == 'closed' and rc [ 'bidState' ] == 'buyNow' ) : return True else : return False
6404	def get_feature ( vector , feature ) : if feature not in _FEATURE_MASK : raise AttributeError ( "feature must be one of: '" + "', '" . join ( ( 'consonantal' , 'sonorant' , 'syllabic' , 'labial' , 'round' , 'coronal' , 'anterior' , 'distributed' , 'dorsal' , 'high' , 'low' , 'back' , 'tense' , 'pharyngeal' , 'ATR' , 'voice' , 'spread_glottis' , 'constricted_glottis' , 'continuant' , 'strident' , 'lateral' , 'delayed_release' , 'nasal' , ) ) + "'" ) mask = _FEATURE_MASK [ feature ] pos_mask = mask >> 1 retvec = [ ] for char in vector : if char < 0 : retvec . append ( float ( 'NaN' ) ) else : masked = char & mask if masked == 0 : retvec . append ( 0 ) elif masked == mask : retvec . append ( 2 ) elif masked & pos_mask : retvec . append ( 1 ) else : retvec . append ( - 1 ) return retvec
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
3515	def chartbeat_bottom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatBottomNode ( )
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , ** data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
4452	def aggregate ( self , query ) : if isinstance ( query , AggregateRequest ) : has_schema = query . _with_schema has_cursor = bool ( query . _cursor ) cmd = [ self . AGGREGATE_CMD , self . index_name ] + query . build_args ( ) elif isinstance ( query , Cursor ) : has_schema = False has_cursor = True cmd = [ self . CURSOR_CMD , 'READ' , self . index_name ] + query . build_args ( ) else : raise ValueError ( 'Bad query' , query ) raw = self . redis . execute_command ( * cmd ) if has_cursor : if isinstance ( query , Cursor ) : query . cid = raw [ 1 ] cursor = query else : cursor = Cursor ( raw [ 1 ] ) raw = raw [ 0 ] else : cursor = None if query . _with_schema : schema = raw [ 0 ] rows = raw [ 2 : ] else : schema = None rows = raw [ 1 : ] res = AggregateResult ( rows , cursor , schema ) return res
10598	def clear ( self ) : self . solid_density = 1.0 self . H2O_mass = 0.0 self . size_class_masses = self . size_class_masses * 0.0
1107	def get_opcodes ( self ) : if self . opcodes is not None : return self . opcodes i = j = 0 self . opcodes = answer = [ ] for ai , bj , size in self . get_matching_blocks ( ) : tag = '' if i < ai and j < bj : tag = 'replace' elif i < ai : tag = 'delete' elif j < bj : tag = 'insert' if tag : answer . append ( ( tag , i , ai , j , bj ) ) i , j = ai + size , bj + size if size : answer . append ( ( 'equal' , ai , i , bj , j ) ) return answer
2124	def associate_always_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'always' , parent , child , ** kwargs )
3657	def remove_cti_file ( self , file_path : str ) : if file_path in self . _cti_files : self . _cti_files . remove ( file_path ) self . _logger . info ( 'Removed {0} from the CTI file list.' . format ( file_path ) )
4864	def get_groups ( self , obj ) : if obj . user : return [ group . name for group in obj . user . groups . filter ( name__in = ENTERPRISE_PERMISSION_GROUPS ) ] return [ ]
9179	def _validate_license ( model ) : license_mapping = obtain_licenses ( ) try : license_url = model . metadata [ 'license_url' ] except KeyError : raise exceptions . MissingRequiredMetadata ( 'license_url' ) try : license = license_mapping [ license_url ] except KeyError : raise exceptions . InvalidLicense ( license_url ) if not license [ 'is_valid_for_publication' ] : raise exceptions . InvalidLicense ( license_url )
4102	def generate_gallery_rst ( app ) : try : plot_gallery = eval ( app . builder . config . plot_gallery ) except TypeError : plot_gallery = bool ( app . builder . config . plot_gallery ) gallery_conf . update ( app . config . sphinx_gallery_conf ) gallery_conf . update ( plot_gallery = plot_gallery ) gallery_conf . update ( abort_on_example_error = app . builder . config . abort_on_example_error ) app . config . sphinx_gallery_conf = gallery_conf app . config . html_static_path . append ( glr_path_static ( ) ) clean_gallery_out ( app . builder . outdir ) examples_dirs = gallery_conf [ 'examples_dirs' ] gallery_dirs = gallery_conf [ 'gallery_dirs' ] if not isinstance ( examples_dirs , list ) : examples_dirs = [ examples_dirs ] if not isinstance ( gallery_dirs , list ) : gallery_dirs = [ gallery_dirs ] mod_examples_dir = os . path . relpath ( gallery_conf [ 'mod_example_dir' ] , app . builder . srcdir ) seen_backrefs = set ( ) for examples_dir , gallery_dir in zip ( examples_dirs , gallery_dirs ) : examples_dir = os . path . relpath ( examples_dir , app . builder . srcdir ) gallery_dir = os . path . relpath ( gallery_dir , app . builder . srcdir ) for workdir in [ examples_dir , gallery_dir , mod_examples_dir ] : if not os . path . exists ( workdir ) : os . makedirs ( workdir ) fhindex = open ( os . path . join ( gallery_dir , 'index.rst' ) , 'w' ) fhindex . write ( generate_dir_rst ( examples_dir , gallery_dir , gallery_conf , seen_backrefs ) ) for directory in sorted ( os . listdir ( examples_dir ) ) : if os . path . isdir ( os . path . join ( examples_dir , directory ) ) : src_dir = os . path . join ( examples_dir , directory ) target_dir = os . path . join ( gallery_dir , directory ) fhindex . write ( generate_dir_rst ( src_dir , target_dir , gallery_conf , seen_backrefs ) ) fhindex . flush ( )
7908	def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_presence ( stanza ) return True
2570	def send_UDP_message ( self , message ) : x = 0 if self . tracking_enabled : try : proc = udp_messenger ( self . domain_name , self . UDP_IP , self . UDP_PORT , self . sock_timeout , message ) self . procs . append ( proc ) except Exception as e : logger . debug ( "Usage tracking failed: {}" . format ( e ) ) else : x = - 1 return x
8897	def _fsic_queuing_calc ( fsic1 , fsic2 ) : return { instance : fsic2 . get ( instance , 0 ) for instance , counter in six . iteritems ( fsic1 ) if fsic2 . get ( instance , 0 ) < counter }
2369	def keywords ( self ) : for table in self . tables : if isinstance ( table , KeywordTable ) : for keyword in table . keywords : yield keyword
6410	def lehmer_mean ( nums , exp = 2 ) : r return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
2994	def _getJson ( url , token = '' , version = '' ) : if token : return _getJsonIEXCloud ( url , token , version ) return _getJsonOrig ( url )
12954	def _rem_id_from_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_ids_key ( ) , pk )
13099	def getAnnotations ( self , targets , wildcard = "." , include = None , exclude = None , limit = None , start = 1 , expand = False , ** kwargs ) : return 0 , [ ]
1391	def convert_pb_kvs ( kvs , include_non_primitives = True ) : config = { } for kv in kvs : if kv . value : config [ kv . key ] = kv . value elif kv . serialized_value : if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) if jv is not None : config [ kv . key ] = jv else : config [ kv . key ] = _raw_value ( kv ) return config
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
13346	def run ( * args , ** kwargs ) : kwargs . setdefault ( 'env' , os . environ ) kwargs . setdefault ( 'shell' , True ) try : subprocess . check_call ( ' ' . join ( args ) , ** kwargs ) return True except subprocess . CalledProcessError : logger . debug ( 'Error running: {}' . format ( args ) ) return False
417	def find_datasets ( self , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( "[Database] FAIL! Cannot find any dataset: {}" . format ( kwargs ) ) return False print ( "[Database] Find {} datasets SUCCESS, took: {}s" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
477	def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) try : multi_bleu_path , _ = urllib . request . urlretrieve ( "https://raw.githubusercontent.com/moses-smt/mosesdecoder/" "master/scripts/generic/multi-bleu.perl" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : tl . logging . info ( "Unable to fetch multi-bleu.perl script, using local." ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( "\n" . join ( hypotheses ) . encode ( "utf-8" ) ) hypothesis_file . write ( b"\n" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( "\n" . join ( references ) . encode ( "utf-8" ) ) reference_file . write ( b"\n" ) reference_file . flush ( ) with open ( hypothesis_file . name , "r" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ "-lc" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( "utf-8" ) bleu_score = re . search ( r"BLEU = (.+?)," , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( "multi-bleu.perl script returned non-zero exit code" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
5625	def relative_path ( path = None , base_dir = None ) : if path_is_remote ( path ) or not os . path . isabs ( path ) : return path else : return os . path . relpath ( path , base_dir )
5212	def bdh ( tickers , flds = None , start_date = None , end_date = 'today' , adjust = None , ** kwargs ) -> pd . DataFrame : logger = logs . get_logger ( bdh , level = kwargs . pop ( 'log' , logs . LOG_LEVEL ) ) if isinstance ( adjust , str ) and adjust : if adjust == 'all' : kwargs [ 'CshAdjNormal' ] = True kwargs [ 'CshAdjAbnormal' ] = True kwargs [ 'CapChg' ] = True else : kwargs [ 'CshAdjNormal' ] = 'normal' in adjust or 'dvd' in adjust kwargs [ 'CshAdjAbnormal' ] = 'abn' in adjust or 'dvd' in adjust kwargs [ 'CapChg' ] = 'split' in adjust con , _ = create_connection ( ) elms = assist . proc_elms ( ** kwargs ) ovrds = assist . proc_ovrds ( ** kwargs ) if isinstance ( tickers , str ) : tickers = [ tickers ] if flds is None : flds = [ 'Last_Price' ] if isinstance ( flds , str ) : flds = [ flds ] e_dt = utils . fmt_dt ( end_date , fmt = '%Y%m%d' ) if start_date is None : start_date = pd . Timestamp ( e_dt ) - relativedelta ( months = 3 ) s_dt = utils . fmt_dt ( start_date , fmt = '%Y%m%d' ) logger . info ( f'loading historical data from Bloomberg:\n' f'{assist.info_qry(tickers=tickers, flds=flds)}' ) logger . debug ( f'\nflds={flds}\nelms={elms}\novrds={ovrds}\nstart_date={s_dt}\nend_date={e_dt}' ) res = con . bdh ( tickers = tickers , flds = flds , elms = elms , ovrds = ovrds , start_date = s_dt , end_date = e_dt ) res . index . name = None if ( len ( flds ) == 1 ) and kwargs . get ( 'keep_one' , False ) : return res . xs ( flds [ 0 ] , axis = 1 , level = 1 ) return res
5048	def delete ( self , request , customer_uuid ) : enterprise_customer = EnterpriseCustomer . objects . get ( uuid = customer_uuid ) email_to_unlink = request . GET [ "unlink_email" ] try : EnterpriseCustomerUser . objects . unlink_user ( enterprise_customer = enterprise_customer , user_email = email_to_unlink ) except ( EnterpriseCustomerUser . DoesNotExist , PendingEnterpriseCustomerUser . DoesNotExist ) : message = _ ( "Email {email} is not associated with Enterprise " "Customer {ec_name}" ) . format ( email = email_to_unlink , ec_name = enterprise_customer . name ) return HttpResponse ( message , content_type = "application/json" , status = 404 ) return HttpResponse ( json . dumps ( { } ) , content_type = "application/json" )
13360	def load ( self ) : if not os . path . exists ( self . path ) : return with open ( self . path , 'r' ) as f : env_data = yaml . load ( f . read ( ) ) if env_data : for env in env_data : self . add ( VirtualEnvironment ( env [ 'root' ] ) )
116	def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )
4031	def _decrypt ( self , value , encrypted_value ) : if sys . platform == 'win32' : return self . _decrypt_windows_chrome ( value , encrypted_value ) if value or ( encrypted_value [ : 3 ] != b'v10' ) : return value encrypted_value = encrypted_value [ 3 : ] encrypted_value_half_len = int ( len ( encrypted_value ) / 2 ) cipher = pyaes . Decrypter ( pyaes . AESModeOfOperationCBC ( self . key , self . iv ) ) decrypted = cipher . feed ( encrypted_value [ : encrypted_value_half_len ] ) decrypted += cipher . feed ( encrypted_value [ encrypted_value_half_len : ] ) decrypted += cipher . feed ( ) return decrypted . decode ( "utf-8" )
6680	def copy ( self , source , destination , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/cp {0}{1} {2}' . format ( options , quote ( source ) , quote ( destination ) ) )
4196	def TOEPLITZ ( T0 , TC , TR , Z ) : assert len ( TC ) > 0 assert len ( TC ) == len ( TR ) M = len ( TC ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) B = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save1 = TC [ k ] save2 = TR [ k ] beta = X [ 0 ] * TC [ k ] if k == 0 : temp1 = - save1 / P temp2 = - save2 / P else : for j in range ( 0 , k ) : save1 = save1 + A [ j ] * TC [ k - j - 1 ] save2 = save2 + B [ j ] * TR [ k - j - 1 ] beta = beta + X [ j + 1 ] * TC [ k - j - 1 ] temp1 = - save1 / P temp2 = - save2 / P P = P * ( 1. - ( temp1 * temp2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp1 B [ k ] = temp2 alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] continue for j in range ( 0 , k ) : kj = k - j - 1 save1 = A [ j ] A [ j ] = save1 + temp1 * B [ kj ] B [ kj ] = B [ kj ] + temp2 * save1 X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * B [ k - j ] return X
5687	def stop ( self , stop_I ) : return pd . read_sql_query ( "SELECT * FROM stops WHERE stop_I={stop_I}" . format ( stop_I = stop_I ) , self . conn )
6402	def fingerprint ( self , phrase , joiner = ' ' ) : phrase = unicode_normalize ( 'NFKD' , text_type ( phrase . strip ( ) . lower ( ) ) ) phrase = '' . join ( [ c for c in phrase if c . isalnum ( ) or c . isspace ( ) ] ) phrase = joiner . join ( sorted ( list ( set ( phrase . split ( ) ) ) ) ) return phrase
5466	def get_action_environment ( op , name ) : action = _get_action_by_name ( op , name ) if action : return action . get ( 'environment' )
11407	def record_get_field_instances ( rec , tag = "" , ind1 = " " , ind2 = " " ) : if not rec : return [ ] if not tag : return rec . items ( ) else : out = [ ] ind1 , ind2 = _wash_indicators ( ind1 , ind2 ) if '%' in tag : for field_tag in rec : if _tag_matches_pattern ( field_tag , tag ) : for possible_field_instance in rec [ field_tag ] : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) else : for possible_field_instance in rec . get ( tag , [ ] ) : if ( ind1 in ( '%' , possible_field_instance [ 1 ] ) and ind2 in ( '%' , possible_field_instance [ 2 ] ) ) : out . append ( possible_field_instance ) return out
3019	def from_json ( cls , json_data ) : if not isinstance ( json_data , dict ) : json_data = json . loads ( _helpers . _from_bytes ( json_data ) ) private_key_pkcs8_pem = None pkcs12_val = json_data . get ( _PKCS12_KEY ) password = None if pkcs12_val is None : private_key_pkcs8_pem = json_data [ '_private_key_pkcs8_pem' ] signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) else : pkcs12_val = base64 . b64decode ( pkcs12_val ) password = json_data [ '_private_key_password' ] signer = crypt . Signer . from_string ( pkcs12_val , password ) credentials = cls ( json_data [ '_service_account_email' ] , signer , scopes = json_data [ '_scopes' ] , private_key_id = json_data [ '_private_key_id' ] , client_id = json_data [ 'client_id' ] , user_agent = json_data [ '_user_agent' ] , ** json_data [ '_kwargs' ] ) if private_key_pkcs8_pem is not None : credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem if pkcs12_val is not None : credentials . _private_key_pkcs12 = pkcs12_val if password is not None : credentials . _private_key_password = password credentials . invalid = json_data [ 'invalid' ] credentials . access_token = json_data [ 'access_token' ] credentials . token_uri = json_data [ 'token_uri' ] credentials . revoke_uri = json_data [ 'revoke_uri' ] token_expiry = json_data . get ( 'token_expiry' , None ) if token_expiry is not None : credentials . token_expiry = datetime . datetime . strptime ( token_expiry , client . EXPIRY_FORMAT ) return credentials
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )
10348	def export_namespace ( graph , namespace , directory = None , cacheable = False ) : directory = os . getcwd ( ) if directory is None else directory path = os . path . join ( directory , '{}.belns' . format ( namespace ) ) with open ( path , 'w' ) as file : log . info ( 'Outputting to %s' , path ) right_names = get_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d correct names in %s' , len ( right_names ) , namespace ) wrong_names = get_incorrect_names_by_namespace ( graph , namespace ) log . info ( 'Graph has %d incorrect names in %s' , len ( right_names ) , namespace ) undefined_ns_names = get_undefined_namespace_names ( graph , namespace ) log . info ( 'Graph has %d names in missing namespace %s' , len ( right_names ) , namespace ) names = ( right_names | wrong_names | undefined_ns_names ) if 0 == len ( names ) : log . warning ( '%s is empty' , namespace ) write_namespace ( namespace_name = namespace , namespace_keyword = namespace , namespace_domain = 'Other' , author_name = graph . authors , author_contact = graph . contact , citation_name = graph . name , values = names , cacheable = cacheable , file = file )
11131	def stop ( self ) : with self . _status_lock : if self . _running : assert self . _observer is not None self . _observer . stop ( ) self . _running = False self . _origin_mapped_data = dict ( )
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
10053	def post ( self , pid , record ) : uploaded_file = request . files [ 'file' ] key = secure_filename ( request . form . get ( 'name' ) or uploaded_file . filename ) if key in record . files : raise FileAlreadyExists ( ) record . files [ key ] = uploaded_file . stream record . commit ( ) db . session . commit ( ) return self . make_response ( obj = record . files [ key ] . obj , pid = pid , record = record , status = 201 )
1226	def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : if discount is None : discount = self . discount def cumulate ( cumulative , reward_terminal_horizon_subtract ) : rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract return tf . where ( condition = is_terminal , x = rew , y = tf . where ( condition = is_over_horizon , x = ( rew + cumulative * discount - sub ) , y = ( rew + cumulative * discount ) ) ) def len_ ( cumulative , term ) : return tf . where ( condition = term , x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , y = cumulative + 1 ) reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) if horizon > 0 : horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) else : horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) reward = tf . scan ( fn = cumulate , elems = ( reward , terminal , off_horizon , horizon_subtractions ) , initializer = final_reward if horizon != 1 else 0.0 ) return tf . reverse ( tensor = reward , axis = ( 0 , ) )
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
11342	def load_config ( filename = None , section_option_dict = { } ) : config = ConfigParser ( ) config . read ( filename ) working_dict = _prepare_working_dict ( config , section_option_dict ) tmp_dict = { } for section , options in working_dict . iteritems ( ) : tmp_dict [ section ] = { } for option in options : tmp_dict [ section ] [ option ] = config . get ( section , option ) return Bunch ( tmp_dict )
8415	def min_max ( x , na_rm = False , finite = True ) : if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) if na_rm and finite : x = x [ np . isfinite ( x ) ] elif not na_rm and np . any ( np . isnan ( x ) ) : return np . nan , np . nan elif na_rm : x = x [ ~ np . isnan ( x ) ] elif finite : x = x [ ~ np . isinf ( x ) ] if ( len ( x ) ) : return np . min ( x ) , np . max ( x ) else : return float ( '-inf' ) , float ( 'inf' )
7459	def paramname ( param = "" ) : try : name = pinfo [ str ( param ) ] [ 0 ] . strip ( ) . split ( " " ) [ 1 ] except ( KeyError , ValueError ) as err : print ( "\tKey name/number not recognized - " . format ( param ) , err ) raise return name
13618	def get_branches ( self ) : return [ self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) ]
1808	def SETC ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
9825	def groups ( ctx , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiment_groups ( username = user , project_name = project_name , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiment groups for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiment groups for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiment groups found for project `{}/{}`.' . format ( user , project_name ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiment groups:" ) objects . pop ( 'project' , None ) objects . pop ( 'user' , None ) dict_tabulate ( objects , is_list_dict = True )
504	def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
12591	def get_reliabledictionary_schema ( client , application_name , service_name , dictionary_name , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) result = json . dumps ( dictionary . get_information ( ) , indent = 4 ) if ( output_file == None ) : output_file = "{}-{}-{}-schema-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( 'Printed schema information to: ' + output_file ) print ( result )
9685	def read_firmware ( self ) : self . cnxn . xfer ( [ 0x12 ] ) sleep ( 10e-3 ) self . firmware [ 'major' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] self . firmware [ 'minor' ] = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] self . firmware [ 'version' ] = float ( '{}.{}' . format ( self . firmware [ 'major' ] , self . firmware [ 'minor' ] ) ) sleep ( 0.1 ) return self . firmware
2777	def add_forwarding_rules ( self , forwarding_rules ) : rules_dict = [ rule . __dict__ for rule in forwarding_rules ] return self . get_data ( "load_balancers/%s/forwarding_rules/" % self . id , type = POST , params = { "forwarding_rules" : rules_dict } )
12902	def _set_range ( self , start , stop , value , value_len ) : assert stop >= start and value_len >= 0 range_len = stop - start if range_len < value_len : self . _insert_zeros ( stop , stop + value_len - range_len ) self . _copy_to_range ( start , value , value_len ) elif range_len > value_len : self . _del_range ( stop - ( range_len - value_len ) , stop ) self . _copy_to_range ( start , value , value_len ) else : self . _copy_to_range ( start , value , value_len )
11657	def fit ( self , X , y = None ) : X = check_array ( X , copy = self . copy , dtype = [ np . float64 , np . float32 , np . float16 , np . float128 ] ) feature_range = self . feature_range if feature_range [ 0 ] >= feature_range [ 1 ] : raise ValueError ( "Minimum of desired feature range must be smaller" " than maximum. Got %s." % str ( feature_range ) ) if self . fit_feature_range is not None : fit_feature_range = self . fit_feature_range if fit_feature_range [ 0 ] >= fit_feature_range [ 1 ] : raise ValueError ( "Minimum of desired (fit) feature range must " "be smaller than maximum. Got %s." % str ( feature_range ) ) if ( fit_feature_range [ 0 ] < feature_range [ 0 ] or fit_feature_range [ 1 ] > feature_range [ 1 ] ) : raise ValueError ( "fit_feature_range must be a subset of " "feature_range. Got %s, fit %s." % ( str ( feature_range ) , str ( fit_feature_range ) ) ) feature_range = fit_feature_range data_min = np . min ( X , axis = 0 ) data_range = np . max ( X , axis = 0 ) - data_min data_range [ data_range == 0.0 ] = 1.0 self . scale_ = ( feature_range [ 1 ] - feature_range [ 0 ] ) / data_range self . min_ = feature_range [ 0 ] - data_min * self . scale_ self . data_range = data_range self . data_min = data_min return self
1817	def SETNS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) )
4838	def get_course_and_course_run ( self , course_run_id ) : course_id = parse_course_key ( course_run_id ) course = self . get_course_details ( course_id ) course_run = None if course : course_run = None course_runs = [ course_run for course_run in course [ 'course_runs' ] if course_run [ 'key' ] == course_run_id ] if course_runs : course_run = course_runs [ 0 ] return course , course_run
3841	async def sync_all_new_events ( self , sync_all_new_events_request ) : response = hangouts_pb2 . SyncAllNewEventsResponse ( ) await self . _pb_request ( 'conversations/syncallnewevents' , sync_all_new_events_request , response ) return response
5157	def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
7293	def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
6371	def fallout ( self ) : r if self . _fp + self . _tn == 0 : return float ( 'NaN' ) return self . _fp / ( self . _fp + self . _tn )
3656	def add_cti_file ( self , file_path : str ) : if not os . path . exists ( file_path ) : self . _logger . warning ( 'Attempted to add {0} which does not exist.' . format ( file_path ) ) if file_path not in self . _cti_files : self . _cti_files . append ( file_path ) self . _logger . info ( 'Added {0} to the CTI file list.' . format ( file_path ) )
4995	def handle_user_post_save ( sender , ** kwargs ) : created = kwargs . get ( "created" , False ) user_instance = kwargs . get ( "instance" , None ) if user_instance is None : return try : pending_ecu = PendingEnterpriseCustomerUser . objects . get ( user_email = user_instance . email ) except PendingEnterpriseCustomerUser . DoesNotExist : return if not created : try : existing_record = EnterpriseCustomerUser . objects . get ( user_id = user_instance . id ) message_template = "User {user} have changed email to match pending Enterprise Customer link, " "but was already linked to Enterprise Customer {enterprise_customer} - " "deleting pending link record" logger . info ( message_template . format ( user = user_instance , enterprise_customer = existing_record . enterprise_customer ) ) pending_ecu . delete ( ) return except EnterpriseCustomerUser . DoesNotExist : pass enterprise_customer_user = EnterpriseCustomerUser . objects . create ( enterprise_customer = pending_ecu . enterprise_customer , user_id = user_instance . id ) pending_enrollments = list ( pending_ecu . pendingenrollment_set . all ( ) ) if pending_enrollments : def _complete_user_enrollment ( ) : for enrollment in pending_enrollments : enterprise_customer_user . enroll ( enrollment . course_id , enrollment . course_mode , cohort = enrollment . cohort_name ) track_enrollment ( 'pending-admin-enrollment' , user_instance . id , enrollment . course_id ) pending_ecu . delete ( ) transaction . on_commit ( _complete_user_enrollment ) else : pending_ecu . delete ( )
8808	def get_mac_address_range ( context , id , fields = None ) : LOG . info ( "get_mac_address_range %s for tenant %s fields %s" % ( id , context . tenant_id , fields ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) mac_address_range = db_api . mac_address_range_find ( context , id = id , scope = db_api . ONE ) if not mac_address_range : raise q_exc . MacAddressRangeNotFound ( mac_address_range_id = id ) return v . _make_mac_range_dict ( mac_address_range )
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
1804	def SAHF ( cpu ) : eflags_size = 32 val = cpu . AH & 0xD5 | 0x02 cpu . EFLAGS = Operators . ZEXTEND ( val , eflags_size )
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
4144	def CHOLESKY ( A , B , method = 'scipy' ) : if method == 'numpy_solver' : X = _numpy_solver ( A , B ) return X elif method == 'numpy' : X , _L = _numpy_cholesky ( A , B ) return X elif method == 'scipy' : import scipy . linalg L = scipy . linalg . cholesky ( A ) X = scipy . linalg . cho_solve ( ( L , False ) , B ) else : raise ValueError ( 'method must be numpy_solver, numpy_cholesky or cholesky_inplace' ) return X
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
13302	def rmse ( a , b ) : return np . sqrt ( np . square ( a - b ) . mean ( ) )
10403	def microcanonical_statistics_dtype ( spanning_cluster = True ) : fields = list ( ) fields . extend ( [ ( 'n' , 'uint32' ) , ( 'edge' , 'uint32' ) , ] ) if spanning_cluster : fields . extend ( [ ( 'has_spanning_cluster' , 'bool' ) , ] ) fields . extend ( [ ( 'max_cluster_size' , 'uint32' ) , ( 'moments' , '(5,)uint64' ) , ] ) return _ndarray_dtype ( fields )
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
11160	def trail_space ( self , filters = lambda p : p . ext == ".py" ) : self . assert_is_dir_and_exists ( ) for p in self . select_file ( filters ) : try : with open ( p . abspath , "rb" ) as f : lines = list ( ) for line in f : lines . append ( line . decode ( "utf-8" ) . rstrip ( ) ) with open ( p . abspath , "wb" ) as f : f . write ( "\n" . join ( lines ) . encode ( "utf-8" ) ) except Exception as e : raise e
6761	def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
5346	def compose_mailing_lists ( projects , data ) : for p in [ project for project in data if len ( data [ project ] [ 'mailing_lists' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] urls = [ url [ 'url' ] . replace ( 'mailto:' , '' ) for url in data [ p ] [ 'mailing_lists' ] if url [ 'url' ] not in projects [ p ] [ 'mailing_lists' ] ] projects [ p ] [ 'mailing_lists' ] += urls for p in [ project for project in data if len ( data [ project ] [ 'dev_list' ] ) > 0 ] : if 'mailing_lists' not in projects [ p ] : projects [ p ] [ 'mailing_lists' ] = [ ] mailing_list = data [ p ] [ 'dev_list' ] [ 'url' ] . replace ( 'mailto:' , '' ) projects [ p ] [ 'mailing_lists' ] . append ( mailing_list ) return projects
12941	def pprint ( self , stream = None ) : pprint . pprint ( self . asDict ( includeMeta = True , forStorage = False , strKeys = True ) , stream = stream )
8134	def up ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = min ( len ( self . canvas . layers ) , i + 1 ) self . canvas . layers . insert ( i , self )
12451	def deref ( self , data ) : deref = copy . deepcopy ( jsonref . JsonRef . replace_refs ( data ) ) self . write_template ( deref , filename = 'swagger.json' ) return deref
11574	def digital_message ( self , data ) : port = data [ 0 ] port_data = ( data [ self . MSB ] << 7 ) + data [ self . LSB ] pin = port * 8 for pin in range ( pin , min ( pin + 8 , self . total_pins_discovered ) ) : with self . pymata . data_lock : prev_data = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = port_data & 0x01 if prev_data != port_data & 0x01 : callback = self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_CALLBACK ] if callback : callback ( [ self . pymata . DIGITAL , pin , self . digital_response_table [ pin ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] ] ) latching_entry = self . digital_latch_table [ pin ] if latching_entry [ self . LATCH_STATE ] == self . LATCH_ARMED : if latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_LOW : if ( port_data & 0x01 ) == 0 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 0 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_LOW updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass elif latching_entry [ self . LATCHED_THRESHOLD_TYPE ] == self . DIGITAL_LATCH_HIGH : if port_data & 0x01 : if latching_entry [ self . DIGITAL_LATCH_CALLBACK ] is not None : self . digital_latch_table [ pin ] = [ 0 , 0 , 0 , 0 , None ] latching_entry [ self . DIGITAL_LATCH_CALLBACK ] ( [ self . pymata . OUTPUT | self . pymata . LATCH_MODE , pin , 1 , time . time ( ) ] ) else : updated_latch_entry = latching_entry updated_latch_entry [ self . LATCH_STATE ] = self . LATCH_LATCHED updated_latch_entry [ self . DIGITAL_LATCHED_DATA ] = self . DIGITAL_LATCH_HIGH updated_latch_entry [ self . DIGITAL_TIME_STAMP ] = time . time ( ) else : pass else : pass port_data >>= 1
4176	def window_gaussian ( N , alpha = 2.5 ) : r t = linspace ( - ( N - 1 ) / 2. , ( N - 1 ) / 2. , N ) w = exp ( - 0.5 * ( alpha * t / ( N / 2. ) ) ** 2. ) return w
9786	def resources ( ctx , gpu ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : message_handler = Printer . gpu_resources if gpu else Printer . resources PolyaxonClient ( ) . build_job . resources ( user , project_name , _build , message_handler = message_handler ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get resources for build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
12723	def max_forces ( self , max_forces ) : _set_params ( self . ode_obj , 'FMax' , max_forces , self . ADOF + self . LDOF )
8766	def _fix_missing_tenant_id ( self , context , body , key ) : if not body : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) resource = body . get ( key ) if not resource : raise n_exc . BadRequest ( resource = key , msg = "Body malformed" ) if context . tenant_id is None : context . tenant_id = resource . get ( "tenant_id" ) if context . tenant_id is None : msg = _ ( "Running without keystone AuthN requires " "that tenant_id is specified" ) raise n_exc . BadRequest ( resource = key , msg = msg )
9434	def load_savefile ( input_file , layers = 0 , verbose = False , lazy = False ) : global VERBOSE old_verbose = VERBOSE VERBOSE = verbose __TRACE__ ( '[+] attempting to load {:s}' , ( input_file . name , ) ) header = _load_savefile_header ( input_file ) if __validate_header__ ( header ) : __TRACE__ ( '[+] found valid header' ) if lazy : packets = _generate_packets ( input_file , header , layers ) __TRACE__ ( '[+] created packet generator' ) else : packets = _load_packets ( input_file , header , layers ) __TRACE__ ( '[+] loaded {:d} packets' , ( len ( packets ) , ) ) sfile = pcap_savefile ( header , packets ) __TRACE__ ( '[+] finished loading savefile.' ) else : __TRACE__ ( '[!] invalid savefile' ) sfile = None VERBOSE = old_verbose return sfile
2395	def quadratic_weighted_kappa ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] if min_rating is None : min_rating = min ( rater_a + rater_b ) if max_rating is None : max_rating = max ( rater_a + rater_b ) conf_mat = confusion_matrix ( rater_a , rater_b , min_rating , max_rating ) num_ratings = len ( conf_mat ) num_scored_items = float ( len ( rater_a ) ) hist_rater_a = histogram ( rater_a , min_rating , max_rating ) hist_rater_b = histogram ( rater_b , min_rating , max_rating ) numerator = 0.0 denominator = 0.0 if ( num_ratings > 1 ) : for i in range ( num_ratings ) : for j in range ( num_ratings ) : expected_count = ( hist_rater_a [ i ] * hist_rater_b [ j ] / num_scored_items ) d = pow ( i - j , 2.0 ) / pow ( num_ratings - 1 , 2.0 ) numerator += d * conf_mat [ i ] [ j ] / num_scored_items denominator += d * expected_count / num_scored_items return 1.0 - numerator / denominator else : return 1.0
9771	def stop ( ctx , yes ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not yes and not click . confirm ( "Are sure you want to stop " "job `{}`" . format ( _job ) ) : click . echo ( 'Existing without stopping job.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . job . stop ( user , project_name , _job ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Job is being stopped." )
11534	def block_resource_fitnesses ( self , block : block . Block ) : if not block . resources : return { n : 1 for n in self . config . nodes . keys ( ) } node_fitnesses = { } for resource in block . resources : resource_fitnesses = self . resource_fitnesses ( resource ) if not resource_fitnesses : raise UnassignableBlock ( block . name ) max_fit = max ( resource_fitnesses . values ( ) ) min_fit = min ( resource_fitnesses . values ( ) ) for node , fitness in resource_fitnesses . items ( ) : if node not in node_fitnesses : node_fitnesses [ node ] = { } if not fitness : node_fitnesses [ node ] [ resource . describe ( ) ] = False else : if max_fit - min_fit : node_fitnesses [ node ] [ resource . describe ( ) ] = ( fitness - min_fit ) / ( max_fit - min_fit ) else : node_fitnesses [ node ] [ resource . describe ( ) ] = 1.0 res = { } for node , res_fits in node_fitnesses . items ( ) : fit_sum = 0 for res_desc , fit in res_fits . items ( ) : if fit is False : fit_sum = False break fit_sum += fit if fit_sum is False : res [ node ] = False continue res [ node ] = fit_sum return res
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
6454	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _accents ) wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
8516	def _assert_all_finite ( X ) : X = np . asanyarray ( X ) if ( X . dtype . char in np . typecodes [ 'AllFloat' ] and not np . isfinite ( X . sum ( ) ) and not np . isfinite ( X ) . all ( ) ) : raise ValueError ( "Input contains NaN, infinity" " or a value too large for %r." % X . dtype )
6261	def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
4541	def _on_index ( self , old_index ) : if self . animation : log . debug ( '%s: %s' , self . __class__ . __name__ , self . current_animation . title ) self . frames = self . animation . generate_frames ( False )
3285	def end_write ( self , with_errors ) : if not with_errors : commands . add ( self . provider . ui , self . provider . repo , self . localHgPath )
7142	def balance ( self , unlocked = False ) : return self . _backend . balances ( account = self . index ) [ 1 if unlocked else 0 ]
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
9740	def get_2d_markers_linearized ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
4953	def ready ( self ) : from enterprise . signals import handle_user_post_save from django . db . models . signals import pre_migrate , post_save post_save . connect ( handle_user_post_save , sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID ) pre_migrate . connect ( self . _disconnect_user_post_save_for_migrations )
11521	def add_condor_dag ( self , token , batchmaketaskid , dagfilename , dagmanoutfilename ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'batchmaketaskid' ] = batchmaketaskid parameters [ 'dagfilename' ] = dagfilename parameters [ 'outfilename' ] = dagmanoutfilename response = self . request ( 'midas.batchmake.add.condor.dag' , parameters ) return response
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Deserializer = None if format : Deserializer = self . meta . deserializers [ format ] if not Deserializer : media_ranges = request . get ( 'Content-Type' ) if media_ranges : media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : pass if Deserializer : try : deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : pass raise http . exceptions . UnsupportedMediaType ( )
6117	def circular ( cls , shape , pixel_scale , radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
8254	def _context ( self ) : tags1 = None for clr in self : overlap = [ ] if clr . is_black : name = "black" elif clr . is_white : name = "white" elif clr . is_grey : name = "grey" else : name = clr . nearest_hue ( primary = True ) if name == "orange" and clr . brightness < 0.6 : name = "brown" tags2 = context [ name ] if tags1 is None : tags1 = tags2 else : for tag in tags2 : if tag in tags1 : if tag not in overlap : overlap . append ( tag ) tags1 = overlap overlap . sort ( ) return overlap
6043	def regular_to_sparse ( self ) : return mapping_util . regular_to_sparse_from_sparse_mappings ( regular_to_unmasked_sparse = self . regular_to_unmasked_sparse , unmasked_sparse_to_sparse = self . unmasked_sparse_to_sparse ) . astype ( 'int' )
1245	def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( ** experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
3845	def to_participantid ( user_id ) : return hangouts_pb2 . ParticipantId ( chat_id = user_id . chat_id , gaia_id = user_id . gaia_id )
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
10929	def do_internal_run ( self , initial_count = 0 , subblock = None , update_derr = True ) : self . _inner_run_counter = initial_count good_step = True n_good_steps = 0 CLOG . debug ( 'Running...' ) _last_residuals = self . calc_residuals ( ) . copy ( ) while ( ( self . _inner_run_counter < self . run_length ) & good_step & ( not self . check_terminate ( ) ) ) : if self . check_Broyden_J ( ) and self . _inner_run_counter != 0 : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) and self . _inner_run_counter != 0 : self . update_eig_J ( ) er0 = 1 * self . error delta_vals = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False , subblock = subblock ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = er1 < er0 if good_step : n_good_steps += 1 CLOG . debug ( '%f\t%f' % ( er0 , er1 ) ) self . update_param_vals ( delta_vals , incremental = True ) self . _last_residuals = _last_residuals . copy ( ) if update_derr : self . _last_error = er0 self . error = er1 _last_residuals = self . calc_residuals ( ) . copy ( ) else : er0_0 = self . update_function ( self . param_vals ) CLOG . debug ( 'Bad step!' ) if np . abs ( er0 - er0_0 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . _inner_run_counter += 1 return n_good_steps
12098	def show ( self , args , file_handle = None , ** kwargs ) : "Write to file_handle if supplied, othewise print output" full_string = '' info = { 'root_directory' : '<root_directory>' , 'batch_name' : '<batch_name>' , 'batch_tag' : '<batch_tag>' , 'batch_description' : '<batch_description>' , 'launcher' : '<launcher>' , 'timestamp_format' : '<timestamp_format>' , 'timestamp' : tuple ( time . localtime ( ) ) , 'varying_keys' : args . varying_keys , 'constant_keys' : args . constant_keys , 'constant_items' : args . constant_items } quoted_cmds = [ subprocess . list2cmdline ( [ el for el in self ( self . _formatter ( s ) , '<tid>' , info ) ] ) for s in args . specs ] cmd_lines = [ '%d: %s\n' % ( i , qcmds ) for ( i , qcmds ) in enumerate ( quoted_cmds ) ] full_string += '' . join ( cmd_lines ) if file_handle : file_handle . write ( full_string ) file_handle . flush ( ) else : print ( full_string )
1079	def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . _year if month is None : month = self . _month if day is None : day = self . _day return date . __new__ ( type ( self ) , year , month , day )
12481	def get_rcfile_variable_value ( var_name , app_name , section_name = None ) : cfg = get_rcfile_section ( app_name , section_name ) if var_name in cfg : raise KeyError ( 'Option {} not found in {} ' 'section.' . format ( var_name , section_name ) ) return cfg [ var_name ]
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
4534	def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . numLEDs : end = self . numLEDs - 1 for led in range ( start , end + 1 ) : self . _set_base ( led , color )
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
4290	def write ( self , album , media_group ) : from sigal import __url__ as sigal_link file_path = os . path . join ( album . dst_path , media_group [ 0 ] . filename ) page = self . template . render ( { 'album' : album , 'media' : media_group [ 0 ] , 'previous_media' : media_group [ - 1 ] , 'next_media' : media_group [ 1 ] , 'index_title' : self . index_title , 'settings' : self . settings , 'sigal_link' : sigal_link , 'theme' : { 'name' : os . path . basename ( self . theme ) , 'url' : url_from_path ( os . path . relpath ( self . theme_path , album . dst_path ) ) } , } ) output_file = "%s.html" % file_path with open ( output_file , 'w' , encoding = 'utf-8' ) as f : f . write ( page )
9567	def byteswap ( fmt , data , offset = 0 ) : data = BytesIO ( data ) data . seek ( offset ) data_swapped = BytesIO ( ) for f in fmt : swapped = data . read ( int ( f ) ) [ : : - 1 ] data_swapped . write ( swapped ) return data_swapped . getvalue ( )
12312	def create_ingest_point ( self , privateStreamName , publicStreamName ) : return self . protocol . execute ( 'createIngestPoint' , privateStreamName = privateStreamName , publicStreamName = publicStreamName )
9830	def write ( self , filename ) : maxcol = 80 with open ( filename , 'w' ) as outfile : for line in self . comments : comment = '# ' + str ( line ) outfile . write ( comment [ : maxcol ] + '\n' ) for component , object in self . sorted_components ( ) : object . write ( outfile ) DXclass . write ( self , outfile , quote = True ) for component , object in self . sorted_components ( ) : outfile . write ( 'component "%s" value %s\n' % ( component , str ( object . id ) ) )
161	def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx )
12364	def transfer ( self , region ) : action = self . post ( type = 'transfer' , region = region ) [ 'action' ] return self . parent . get ( action [ 'resource_id' ] )
453	def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( "please input a name" ) logging . info ( " [*] geting variables with %s" % name ) if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( " got {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars
1142	def _long2bytesBigEndian ( n , blocksize = 0 ) : s = b'' pack = struct . pack while n > 0 : s = pack ( '>I' , n & 0xffffffff ) + s n = n >> 32 for i in range ( len ( s ) ) : if s [ i ] != '\000' : break else : s = '\000' i = 0 s = s [ i : ] if blocksize > 0 and len ( s ) % blocksize : s = ( blocksize - len ( s ) % blocksize ) * '\000' + s return s
4576	def color_cmp ( a , b ) : if a == b : return 0 a , b = rgb_to_hsv ( a ) , rgb_to_hsv ( b ) return - 1 if a < b else 1
1899	def can_be_true ( self , constraints , expression ) : if isinstance ( expression , bool ) : if not expression : return expression else : self . _reset ( constraints ) return self . _is_sat ( ) assert isinstance ( expression , Bool ) with constraints as temp_cs : temp_cs . add ( expression ) self . _reset ( temp_cs . to_string ( related_to = expression ) ) return self . _is_sat ( )
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
10531	def find_project ( ** kwargs ) : try : res = _pybossa_req ( 'get' , 'project' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Project ( project ) for project in res ] else : return res except : raise
6198	def print_sizes ( self ) : float_size = 4 MB = 1024 * 1024 size_ = self . n_samples * float_size em_size = size_ * self . num_particles / MB pos_size = 3 * size_ * self . num_particles / MB print ( " Number of particles:" , self . num_particles ) print ( " Number of time steps:" , self . n_samples ) print ( " Emission array - 1 particle (float32): %.1f MB" % ( size_ / MB ) ) print ( " Emission array (float32): %.1f MB" % em_size ) print ( " Position array (float32): %.1f MB " % pos_size )
7881	def _split_qname ( self , name , is_element ) : if name . startswith ( u"{" ) : namespace , name = name [ 1 : ] . split ( u"}" , 1 ) if namespace in STANZA_NAMESPACES : namespace = self . stanza_namespace elif is_element : raise ValueError ( u"Element with no namespace: {0!r}" . format ( name ) ) else : namespace = None return namespace , name
8515	def format_timedelta ( td_object ) : def get_total_seconds ( td ) : return ( td . microseconds + ( td . seconds + td . days * 24 * 3600 ) * 1e6 ) / 1e6 seconds = int ( get_total_seconds ( td_object ) ) periods = [ ( 'year' , 60 * 60 * 24 * 365 ) , ( 'month' , 60 * 60 * 24 * 30 ) , ( 'day' , 60 * 60 * 24 ) , ( 'hour' , 60 * 60 ) , ( 'minute' , 60 ) , ( 'second' , 1 ) ] strings = [ ] for period_name , period_seconds in periods : if seconds > period_seconds : period_value , seconds = divmod ( seconds , period_seconds ) if period_value == 1 : strings . append ( "%s %s" % ( period_value , period_name ) ) else : strings . append ( "%s %ss" % ( period_value , period_name ) ) return ", " . join ( strings )
12134	def write_log ( log_path , data , allow_append = True ) : append = os . path . isfile ( log_path ) islist = isinstance ( data , list ) if append and not allow_append : raise Exception ( 'Appending has been disabled' ' and file %s exists' % log_path ) if not ( islist or isinstance ( data , Args ) ) : raise Exception ( 'Can only write Args objects or dictionary' ' lists to log file.' ) specs = data if islist else data . specs if not all ( isinstance ( el , dict ) for el in specs ) : raise Exception ( 'List elements must be dictionaries.' ) log_file = open ( log_path , 'r+' ) if append else open ( log_path , 'w' ) start = int ( log_file . readlines ( ) [ - 1 ] . split ( ) [ 0 ] ) + 1 if append else 0 ascending_indices = range ( start , start + len ( data ) ) log_str = '\n' . join ( [ '%d %s' % ( tid , json . dumps ( el ) ) for ( tid , el ) in zip ( ascending_indices , specs ) ] ) log_file . write ( "\n" + log_str if append else log_str ) log_file . close ( )
6323	def ac_encode ( text , probs ) : coder = Arithmetic ( ) coder . set_probs ( probs ) return coder . encode ( text )
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
11090	def select ( self , filters = all_true , recursive = True ) : self . assert_is_dir_and_exists ( ) if recursive : for p in self . glob ( "**/*" ) : if filters ( p ) : yield p else : for p in self . iterdir ( ) : if filters ( p ) : yield p
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
6823	def maint_up ( self ) : r = self . local_renderer fn = self . render_to_file ( r . env . maintenance_template , extra = { 'current_hostname' : self . current_hostname } ) r . put ( local_path = fn , remote_path = r . env . maintenance_path , use_sudo = True ) r . sudo ( 'chown -R {apache_web_user}:{apache_web_group} {maintenance_path}' )
5866	def organization_data_is_valid ( organization_data ) : if organization_data is None : return False if 'id' in organization_data and not organization_data . get ( 'id' ) : return False if 'name' in organization_data and not organization_data . get ( 'name' ) : return False return True
102	def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
5356	def set_param ( self , section , param , value ) : if section not in self . conf or param not in self . conf [ section ] : logger . error ( 'Config section %s and param %s not exists' , section , param ) else : self . conf [ section ] [ param ] = value
7105	def train ( self ) : for i , model in enumerate ( self . models ) : N = [ int ( i * len ( self . y ) ) for i in self . lc_range ] for n in N : X = self . X [ : n ] y = self . y [ : n ] e = Experiment ( X , y , model . estimator , self . scores , self . validation_method ) e . log_folder = self . log_folder e . train ( )
386	def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) if is_rescale : if is_center : x = 1. - coord [ 0 ] else : x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : x = im . shape [ 1 ] - coord [ 0 ] else : x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )
5827	def dataset_search ( self , dataset_returning_query ) : self . _validate_search_query ( dataset_returning_query ) return self . _execute_search_query ( dataset_returning_query , DatasetSearchResult )
5243	def tz_convert ( dt , to_tz , from_tz = None ) -> str : logger = logs . get_logger ( tz_convert , level = 'info' ) f_tz , t_tz = get_tz ( from_tz ) , get_tz ( to_tz ) from_dt = pd . Timestamp ( str ( dt ) , tz = f_tz ) logger . debug ( f'converting {str(from_dt)} from {f_tz} to {t_tz} ...' ) return str ( pd . Timestamp ( str ( from_dt ) , tz = t_tz ) )
12647	def set_auth ( pem = None , cert = None , key = None , aad = False ) : if any ( [ cert , key ] ) and pem : raise ValueError ( 'Cannot specify both pem and cert or key' ) if any ( [ cert , key ] ) and not all ( [ cert , key ] ) : raise ValueError ( 'Must specify both cert and key' ) if pem : set_config_value ( 'security' , 'pem' ) set_config_value ( 'pem_path' , pem ) elif cert or key : set_config_value ( 'security' , 'cert' ) set_config_value ( 'cert_path' , cert ) set_config_value ( 'key_path' , key ) elif aad : set_config_value ( 'security' , 'aad' ) else : set_config_value ( 'security' , 'none' )
12764	def attach ( self , frame_no ) : assert not self . joints for label , j in self . channels . items ( ) : target = self . targets . get ( label ) if target is None : continue if self . visibility [ frame_no , j ] < 0 : continue if np . linalg . norm ( self . velocities [ frame_no , j ] ) > 10 : continue joint = ode . BallJoint ( self . world . ode_world , self . jointgroup ) joint . attach ( self . bodies [ label ] . ode_body , target . ode_body ) joint . setAnchor1Rel ( [ 0 , 0 , 0 ] ) joint . setAnchor2Rel ( self . offsets [ label ] ) joint . setParam ( ode . ParamCFM , self . cfms [ frame_no , j ] ) joint . setParam ( ode . ParamERP , self . erp ) joint . name = label self . joints [ label ] = joint self . _frame_no = frame_no
8130	def layer ( self , img , x = 0 , y = 0 , name = "" ) : from types import StringType if isinstance ( img , Image . Image ) : img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1 if isinstance ( img , Layer ) : img . canvas = self self . layers . append ( img ) return len ( self . layers ) - 1 if type ( img ) == StringType : img = Image . open ( img ) img = img . convert ( "RGBA" ) self . layers . append ( Layer ( self , img , x , y , name ) ) return len ( self . layers ) - 1
8365	def create_rcontext ( self , size , frame ) : if self . format == 'pdf' : surface = cairo . PDFSurface ( self . _output_file ( frame ) , * size ) elif self . format in ( 'ps' , 'eps' ) : surface = cairo . PSSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'svg' : surface = cairo . SVGSurface ( self . _output_file ( frame ) , * size ) elif self . format == 'surface' : surface = self . target else : surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , * size ) return cairo . Context ( surface )
13544	def formatter ( color , s ) : if no_coloring : return s return "{begin}{s}{reset}" . format ( begin = color , s = s , reset = Colors . RESET )
11843	def step ( self ) : if not self . is_done ( ) : actions = [ agent . program ( self . percept ( agent ) ) for agent in self . agents ] for ( agent , action ) in zip ( self . agents , actions ) : self . execute_action ( agent , action ) self . exogenous_change ( )
12985	def getCompressMod ( self ) : if self . compressMode == COMPRESS_MODE_ZLIB : return zlib if self . compressMode == COMPRESS_MODE_BZ2 : return bz2 if self . compressMode == COMPRESS_MODE_LZMA : global _lzmaMod if _lzmaMod is not None : return _lzmaMod try : import lzma _lzmaMod = lzma return _lzmaMod except : try : from backports import lzma _lzmaMod = lzma return _lzmaMod except : pass try : import lzmaffi as lzma _lzmaMod = lzma return _lzmaMod except : pass raise ImportError ( "Requested compress mode is lzma and could not find a module providing lzma support. Tried: 'lzma', 'backports.lzma', 'lzmaffi' and none of these were available. Please install one of these, or to use an unlisted implementation, set IndexedRedis.fields.compressed._lzmaMod to the module (must implement standard python compression interface)" )
5158	def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
7606	def search_clans ( self , ** params : clansearch ) : url = self . api . CLAN return self . _get_model ( url , PartialClan , ** params )
10432	def selectrowindex ( self , window_name , object_name , row_index ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) count = len ( object_handle . AXRows ) if row_index < 0 or row_index > count : raise LdtpServerException ( 'Row index out of range: %d' % row_index ) cell = object_handle . AXRows [ row_index ] if not cell . AXSelected : object_handle . activate ( ) cell . AXSelected = True else : pass return 1
10910	def name_globals ( s , remove_params = None ) : all_params = s . params for p in s . param_particle ( np . arange ( s . obj_get_positions ( ) . shape [ 0 ] ) ) : all_params . remove ( p ) if remove_params is not None : for p in set ( remove_params ) : all_params . remove ( p ) return all_params
9379	def detect_timestamp_format ( timestamp ) : time_formats = { 'epoch' : re . compile ( r'^[0-9]{10}$' ) , 'epoch_ms' : re . compile ( r'^[0-9]{13}$' ) , 'epoch_fraction' : re . compile ( r'^[0-9]{10}\.[0-9]{3,9}$' ) , '%Y-%m-%d %H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y-%m-%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d %H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%dT%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d_%H:%M:%S' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%Y%m%d %H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9] [0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%dT%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y%m%d_%H:%M:%S.%f' : re . compile ( r'^[0-9]{4}[0-1][0-9][0-3][0-9]_[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%H:%M:%S' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9]$' ) , '%H:%M:%S.%f' : re . compile ( r'^[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+$' ) , '%Y-%m-%dT%H:%M:%S.%f%z' : re . compile ( r'^[0-9]{4}-[0-1][0-9]-[0-3][0-9]T[0-2][0-9]:[0-5][0-9]:[0-5][0-9].[0-9]+[+-][0-9]{4}$' ) } for time_format in time_formats : if re . match ( time_formats [ time_format ] , timestamp ) : return time_format return 'unknown'
7704	def add_item ( self , item , replace = False ) : if item . jid in self . _jids : if replace : self . remove_item ( item . jid ) else : raise ValueError ( "JID already in the roster" ) index = len ( self . _items ) self . _items . append ( item ) self . _jids [ item . jid ] = index
7262	def get_most_recent_images ( self , results , types = [ ] , sensors = [ ] , N = 1 ) : if not len ( results ) : return None if types : results = [ r for r in results if r [ 'type' ] in types ] if sensors : results = [ r for r in results if r [ 'properties' ] . get ( 'sensorPlatformName' ) in sensors ] newlist = sorted ( results , key = lambda k : k [ 'properties' ] . get ( 'timestamp' ) , reverse = True ) return newlist [ : N ]
12381	def delete ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'destroy' ) self . destroy ( ) self . response . status = http . client . NO_CONTENT self . make_response ( )
11502	def folder_get ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.get' , parameters ) return response
11879	def scanProcessForCwd ( pid , searchPortion , isExactMatch = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e cwd = getProcessCwd ( pid ) if not cwd : return None isMatch = False if isExactMatch is True : if searchPortion == cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] == cwd : isMatch = True else : if searchPortion in cwd : isMatch = True else : if searchPortion . endswith ( '/' ) and searchPortion [ : - 1 ] in cwd : isMatch = True if not isMatch : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'cwd' : cwd , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
3501	def assess_precursors ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff , solver )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
7004	def train_rf_classifier ( collected_features , test_fraction = 0.25 , n_crossval_iterations = 20 , n_kfolds = 5 , crossval_scoring_metric = 'f1' , classifier_to_pickle = None , nworkers = - 1 , ) : if ( isinstance ( collected_features , str ) and os . path . exists ( collected_features ) ) : with open ( collected_features , 'rb' ) as infd : fdict = pickle . load ( infd ) elif isinstance ( collected_features , dict ) : fdict = collected_features else : LOGERROR ( "can't figure out the input collected_features arg" ) return None tfeatures = fdict [ 'features_array' ] tlabels = fdict [ 'labels_array' ] tfeaturenames = fdict [ 'availablefeatures' ] tmagcol = fdict [ 'magcol' ] tobjectids = fdict [ 'objectids' ] training_features , testing_features , training_labels , testing_labels = ( train_test_split ( tfeatures , tlabels , test_size = test_fraction , random_state = RANDSEED , stratify = tlabels ) ) clf = RandomForestClassifier ( n_jobs = nworkers , random_state = RANDSEED ) rf_hyperparams = { "max_depth" : [ 3 , 4 , 5 , None ] , "n_estimators" : sp_randint ( 100 , 2000 ) , "max_features" : sp_randint ( 1 , 5 ) , "min_samples_split" : sp_randint ( 2 , 11 ) , "min_samples_leaf" : sp_randint ( 2 , 11 ) , } cvsearch = RandomizedSearchCV ( clf , param_distributions = rf_hyperparams , n_iter = n_crossval_iterations , scoring = crossval_scoring_metric , cv = StratifiedKFold ( n_splits = n_kfolds , shuffle = True , random_state = RANDSEED ) , random_state = RANDSEED ) LOGINFO ( 'running grid-search CV to optimize RF hyperparameters...' ) cvsearch_classifiers = cvsearch . fit ( training_features , training_labels ) _gridsearch_report ( cvsearch_classifiers . cv_results_ ) bestclf = cvsearch_classifiers . best_estimator_ bestclf_score = cvsearch_classifiers . best_score_ bestclf_hyperparams = cvsearch_classifiers . best_params_ test_predicted_labels = bestclf . predict ( testing_features ) recscore = recall_score ( testing_labels , test_predicted_labels ) precscore = precision_score ( testing_labels , test_predicted_labels ) f1score = f1_score ( testing_labels , test_predicted_labels ) confmatrix = confusion_matrix ( testing_labels , test_predicted_labels ) outdict = { 'features' : tfeatures , 'labels' : tlabels , 'feature_names' : tfeaturenames , 'magcol' : tmagcol , 'objectids' : tobjectids , 'kwargs' : { 'test_fraction' : test_fraction , 'n_crossval_iterations' : n_crossval_iterations , 'n_kfolds' : n_kfolds , 'crossval_scoring_metric' : crossval_scoring_metric , 'nworkers' : nworkers } , 'collect_kwargs' : fdict [ 'kwargs' ] , 'testing_features' : testing_features , 'testing_labels' : testing_labels , 'training_features' : training_features , 'training_labels' : training_labels , 'best_classifier' : bestclf , 'best_score' : bestclf_score , 'best_hyperparams' : bestclf_hyperparams , 'best_recall' : recscore , 'best_precision' : precscore , 'best_f1' : f1score , 'best_confmatrix' : confmatrix } if classifier_to_pickle : with open ( classifier_to_pickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
3911	def _on_event ( self , _ ) : self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
12045	def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) return results return [ ]
6539	def matches_masks ( target , masks ) : for mask in masks : if mask . search ( target ) : return True return False
9462	def conference_hangup ( self , call_params ) : path = '/' + self . api_version + '/ConferenceHangup/' method = 'POST' return self . request ( path , method , call_params )
11307	def map_attr ( self , mapping , attr , obj ) : if attr not in mapping and hasattr ( self , attr ) : if not callable ( getattr ( self , attr ) ) : mapping [ attr ] = getattr ( self , attr ) else : mapping [ attr ] = getattr ( self , attr ) ( obj )
11641	def yaml_get_data ( filename ) : with open ( filename , 'rb' ) as fd : yaml_data = yaml . load ( fd ) return yaml_data return False
9744	def on_packet ( packet ) : print ( "Framenumber: {}" . format ( packet . framenumber ) ) header , markers = packet . get_3d_markers ( ) print ( "Component info: {}" . format ( header ) ) for marker in markers : print ( "\t" , marker )
8068	def refresh ( self ) : self . reset ( ) self . parse ( self . source ) return self . output ( )
3495	def reaction_elements ( reaction ) : c_elements = [ coeff * met . elements . get ( 'C' , 0 ) for met , coeff in iteritems ( reaction . metabolites ) ] return [ elem for elem in c_elements if elem != 0 ]
6470	def consume_line ( self , line ) : data = RE_VALUE_KEY . split ( line . strip ( ) , 1 ) if len ( data ) == 1 : return float ( data [ 0 ] ) , None else : return float ( data [ 0 ] ) , data [ 1 ] . strip ( )
12442	def require_accessibility ( self , user , method ) : if method == 'OPTIONS' : return authz = self . meta . authorization if not authz . is_accessible ( user , method , self ) : authz . unaccessible ( )
45	def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1
1415	def create_pplan ( self , topologyName , pplan ) : if not pplan or not pplan . IsInitialized ( ) : raise_ ( StateException ( "Physical Plan protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_pplan_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) pplanString = pplan . SerializeToString ( ) try : self . client . create ( path , value = pplanString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating pplan" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating pplan" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating pplan" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
12597	def _check_xl_path ( xl_path : str ) : xl_path = op . abspath ( op . expanduser ( xl_path ) ) if not op . isfile ( xl_path ) : raise IOError ( "Could not find file in {}." . format ( xl_path ) ) return xl_path , _use_openpyxl_or_xlrf ( xl_path )
5654	def makedirs ( path ) : if not os . path . isdir ( path ) : os . makedirs ( path ) return path
12250	def _get_key_internal ( self , * args , ** kwargs ) : if args [ 1 ] is not None and 'force' in args [ 1 ] : key , res = super ( Bucket , self ) . _get_key_internal ( * args , ** kwargs ) if key : mimicdb . backend . sadd ( tpl . bucket % self . name , key . name ) mimicdb . backend . hmset ( tpl . key % ( self . name , key . name ) , dict ( size = key . size , md5 = key . etag . strip ( '"' ) ) ) return key , res key = None if mimicdb . backend . sismember ( tpl . bucket % self . name , args [ 0 ] ) : key = Key ( self ) key . name = args [ 0 ] return key , None
683	def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding
1660	def CheckCStyleCast ( filename , clean_lines , linenum , cast_type , pattern , error ) : line = clean_lines . elided [ linenum ] match = Search ( pattern , line ) if not match : return False context = line [ 0 : match . start ( 1 ) - 1 ] if Match ( r'.*\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\s*$' , context ) : return False if linenum > 0 : for i in xrange ( linenum - 1 , max ( 0 , linenum - 5 ) , - 1 ) : context = clean_lines . elided [ i ] + context if Match ( r'.*\b[_A-Z][_A-Z0-9]*\s*\((?:\([^()]*\)|[^()])*$' , context ) : return False if context . endswith ( ' operator++' ) or context . endswith ( ' operator--' ) : return False remainder = line [ match . end ( 0 ) : ] if Match ( r'^\s*(?:;|const\b|throw\b|final\b|override\b|[=>{),]|->)' , remainder ) : return False error ( filename , linenum , 'readability/casting' , 4 , 'Using C-style cast. Use %s<%s>(...) instead' % ( cast_type , match . group ( 1 ) ) ) return True
6416	def var ( nums , mean_func = amean , ddof = 0 ) : r x_bar = mean_func ( nums ) return sum ( ( x - x_bar ) ** 2 for x in nums ) / ( len ( nums ) - ddof )
5443	def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
5596	def get_neighbors ( self , connectedness = 8 ) : return [ BufferedTile ( t , self . pixelbuffer ) for t in self . _tile . get_neighbors ( connectedness = connectedness ) ]
9613	def element ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENT , { 'using' : using , 'value' : value } )
9954	def custom_showwarning ( message , category , filename = "" , lineno = - 1 , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return text = "%s: %s\n" % ( category . __name__ , message ) try : file . write ( text ) except OSError : pass
11430	def record_order_subfields ( rec , tag = None ) : if rec is None : return rec if tag is None : tags = rec . keys ( ) for tag in tags : record_order_subfields ( rec , tag ) elif tag in rec : for i in xrange ( len ( rec [ tag ] ) ) : field = rec [ tag ] [ i ] ordered_subfields = sorted ( field [ 0 ] , key = lambda subfield : subfield [ 0 ] ) rec [ tag ] [ i ] = ( ordered_subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] , field [ 4 ] )
7851	def remove_feature ( self , var ) : if not var : raise ValueError ( "var is None" ) if '"' not in var : expr = 'd:feature[@var="%s"]' % ( var , ) elif "'" not in var : expr = "d:feature[@var='%s']" % ( var , ) else : raise ValueError ( "Invalid feature name" ) l = self . xpath_ctxt . xpathEval ( expr ) if not l : return for f in l : f . unlinkNode ( ) f . freeNode ( )
12487	def create_folder ( dirpath , overwrite = False ) : if not overwrite : while op . exists ( dirpath ) : dirpath += '+' os . makedirs ( dirpath , exist_ok = overwrite ) return dirpath
6137	def add_model_file ( self , model_fpath , position = 1 , file_id = None ) : if file_id is None : file_id = self . make_unique_id ( 'file_input' ) ret_data = self . file_create ( File . from_file ( model_fpath , position , file_id ) ) return ret_data
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
6709	def check ( self ) : self . _validate_settings ( ) r = self . local_renderer r . env . alias = r . env . aliases [ 0 ] r . sudo ( r . env . check_command_template )
9647	def is_valid_in_template ( var , attr ) : if attr . startswith ( '_' ) : return False try : value = getattr ( var , attr ) except : return False if isroutine ( value ) : if getattr ( value , 'alters_data' , False ) : return False else : try : argspec = getargspec ( value ) num_args = len ( argspec . args ) if argspec . args else 0 num_defaults = len ( argspec . defaults ) if argspec . defaults else 0 if num_args - num_defaults > 1 : return False except TypeError : pass return True
8514	def dict_merge ( base , top ) : out = dict ( top ) for key in base : if key in top : if isinstance ( base [ key ] , dict ) and isinstance ( top [ key ] , dict ) : out [ key ] = dict_merge ( base [ key ] , top [ key ] ) else : out [ key ] = base [ key ] return out
3125	def verify_signed_jwt_with_certs ( jwt , certs , audience = None ) : jwt = _helpers . _to_bytes ( jwt ) if jwt . count ( b'.' ) != 2 : raise AppIdentityError ( 'Wrong number of segments in token: {0}' . format ( jwt ) ) header , payload , signature = jwt . split ( b'.' ) message_to_sign = header + b'.' + payload signature = _helpers . _urlsafe_b64decode ( signature ) payload_bytes = _helpers . _urlsafe_b64decode ( payload ) try : payload_dict = json . loads ( _helpers . _from_bytes ( payload_bytes ) ) except : raise AppIdentityError ( 'Can\'t parse token: {0}' . format ( payload_bytes ) ) _verify_signature ( message_to_sign , signature , certs . values ( ) ) _verify_time_range ( payload_dict ) _check_audience ( payload_dict , audience ) return payload_dict
3455	def weight ( self ) : try : return sum ( [ count * elements_and_molecular_weights [ element ] for element , count in self . elements . items ( ) ] ) except KeyError as e : warn ( "The element %s does not appear in the periodic table" % e )
4062	def show_condition_operators ( self , condition ) : permitted_operators = self . savedsearch . conditions_operators . get ( condition ) permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
10860	def param_particle ( self , ind ) : ind = self . _vps ( listify ( ind ) ) return [ self . _i2p ( i , j ) for i in ind for j in [ 'z' , 'y' , 'x' , 'a' ] ]
7547	def make ( data , samples ) : outfile = open ( os . path . join ( data . dirs . outfiles , data . name + ".alleles" ) , 'w' ) lines = open ( os . path . join ( data . dirs . outfiles , data . name + ".loci" ) , 'r' ) longname = max ( len ( x ) for x in data . samples . keys ( ) ) name_padding = 5 writing = [ ] loc = 0 for line in lines : if ">" in line : name , seq = line . split ( " " ) [ 0 ] , line . split ( " " ) [ - 1 ] allele1 , allele2 = splitalleles ( seq . strip ( ) ) writing . append ( name + "_0" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele1 ) writing . append ( name + "_1" + " " * ( longname - len ( name ) - 2 + name_padding ) + allele2 ) else : writing . append ( line . strip ( ) ) loc += 1 if not loc % 10000 : outfile . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] outfile . write ( "\n" . join ( writing ) ) outfile . close ( )
1866	def PSHUFD ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) order = Operators . ZEXTEND ( op3 . read ( ) , size ) arg0 = arg0 & 0xffffffffffffffffffffffffffffffff00000000000000000000000000000000 arg0 |= ( ( arg1 >> ( ( ( order >> 0 ) & 3 ) * 32 ) ) & 0xffffffff ) arg0 |= ( ( arg1 >> ( ( ( order >> 2 ) & 3 ) * 32 ) ) & 0xffffffff ) << 32 arg0 |= ( ( arg1 >> ( ( ( order >> 4 ) & 3 ) * 32 ) ) & 0xffffffff ) << 64 arg0 |= ( ( arg1 >> ( ( ( order >> 6 ) & 3 ) * 32 ) ) & 0xffffffff ) << 96 op0 . write ( arg0 )
231	def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures
1859	def CMPS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] dest_reg = { 8 : 'DI' , 32 : 'EDI' , 64 : 'RDI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base dest_addr = cpu . read_register ( dest_reg ) + base size = dest . size arg1 = cpu . read_int ( dest_addr , size ) arg0 = cpu . read_int ( src_addr , size ) res = ( arg0 - arg1 ) & ( ( 1 << size ) - 1 ) cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
13666	def command_handle ( self ) : self . __results = self . execute ( self . args . command ) self . close ( ) self . logger . debug ( "results: {}" . format ( self . __results ) ) if not self . __results : self . unknown ( "{} return nothing." . format ( self . args . command ) ) if len ( self . __results ) != 1 : self . unknown ( "{} return more than one number." . format ( self . args . command ) ) self . __result = int ( self . __results [ 0 ] ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if not isinstance ( self . __result , ( int , long ) ) : self . unknown ( "{} didn't return single number." . format ( self . args . command ) ) status = self . ok if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "{0} return {1}." . format ( self . args . command , self . __result ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{command}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , command = self . args . command ) ) status ( self . output ( long_output_limit = None ) ) self . logger . debug ( "Return status and exit to Nagios." )
1416	def get_execution_state ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_execution_state_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
8483	def set ( self , name , value ) : if not self . settings . get ( 'pyconfig.case_sensitive' , False ) : name = name . lower ( ) log . info ( " %s = %s" , name , repr ( value ) ) with self . mut_lock : self . settings [ name ] = value
8665	def _prettify_list ( items ) : assert isinstance ( items , list ) keys_list = 'Available Keys:' for item in items : keys_list += '\n - {0}' . format ( item ) return keys_list
1349	def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
1821	def SETPE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
6179	def merge_ph_times ( times_list , times_par_list , time_block ) : offsets = np . arange ( len ( times_list ) ) * time_block cum_sizes = np . cumsum ( [ ts . size for ts in times_list ] ) times = np . zeros ( cum_sizes [ - 1 ] ) times_par = np . zeros ( cum_sizes [ - 1 ] , dtype = 'uint8' ) i1 = 0 for i2 , ts , ts_par , offset in zip ( cum_sizes , times_list , times_par_list , offsets ) : times [ i1 : i2 ] = ts + offset times_par [ i1 : i2 ] = ts_par i1 = i2 return times , times_par
4949	def _transform_item ( self , content_metadata_item ) : content_metadata_type = content_metadata_item [ 'content_type' ] transformed_item = { } for integrated_channel_schema_key , edx_data_schema_key in self . DATA_TRANSFORM_MAPPING . items ( ) : transformer = ( getattr ( self , 'transform_{content_type}_{edx_data_schema_key}' . format ( content_type = content_metadata_type , edx_data_schema_key = edx_data_schema_key ) , None ) or getattr ( self , 'transform_{edx_data_schema_key}' . format ( edx_data_schema_key = edx_data_schema_key ) , None ) ) if transformer : transformed_item [ integrated_channel_schema_key ] = transformer ( content_metadata_item ) else : try : transformed_item [ integrated_channel_schema_key ] = content_metadata_item [ edx_data_schema_key ] except KeyError : LOGGER . exception ( 'Failed to transform content metadata item field [%s] for [%s]: [%s]' , edx_data_schema_key , self . enterprise_customer . name , content_metadata_item , ) return transformed_item
6350	def _pnums_with_leading_space ( self , phonetic ) : alt_start = phonetic . find ( '(' ) if alt_start == - 1 : return ' ' + self . _phonetic_number ( phonetic ) prefix = phonetic [ : alt_start ] alt_start += 1 alt_end = phonetic . find ( ')' , alt_start ) alt_string = phonetic [ alt_start : alt_end ] alt_end += 1 suffix = phonetic [ alt_end : ] alt_array = alt_string . split ( '|' ) result = '' for alt in alt_array : result += self . _pnums_with_leading_space ( prefix + alt + suffix ) return result
12551	def dump_raw_data ( filename , data ) : if data . ndim == 3 : data = data . reshape ( [ data . shape [ 0 ] , data . shape [ 1 ] * data . shape [ 2 ] ] ) a = array . array ( 'f' ) for o in data : a . fromlist ( list ( o . flatten ( ) ) ) with open ( filename , 'wb' ) as rawf : a . tofile ( rawf )
6067	def convergence_from_grid ( self , grid ) : surface_density_grid = np . zeros ( grid . shape [ 0 ] ) grid_eta = self . grid_to_elliptical_radii ( grid ) for i in range ( grid . shape [ 0 ] ) : surface_density_grid [ i ] = self . convergence_func ( grid_eta [ i ] ) return surface_density_grid
4950	def get_consent_record ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) return get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = course_id , program_uuid = program_uuid )
1902	def summarized_name ( self , name ) : components = name . split ( '.' ) prefix = '.' . join ( c [ 0 ] for c in components [ : - 1 ] ) return f'{prefix}.{components[-1]}'
10971	def index ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) q = request . args . get ( 'q' , '' ) groups = Group . query_by_user ( current_user , eager = True ) if q : groups = Group . search ( groups , q ) groups = groups . paginate ( page , per_page = per_page ) requests = Membership . query_requests ( current_user ) . count ( ) invitations = Membership . query_invitations ( current_user ) . count ( ) return render_template ( 'invenio_groups/index.html' , groups = groups , requests = requests , invitations = invitations , page = page , per_page = per_page , q = q )
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
12768	def load_skeleton ( self , filename , pid_params = None ) : self . skeleton = skeleton . Skeleton ( self ) self . skeleton . load ( filename , color = ( 0.3 , 0.5 , 0.9 , 0.8 ) ) if pid_params : self . skeleton . set_pid_params ( ** pid_params ) self . skeleton . erp = 0.1 self . skeleton . cfm = 0
8321	def parse_important ( self , markup ) : important = [ ] table_titles = [ table . title for table in self . tables ] m = re . findall ( self . re [ "bold" ] , markup ) for bold in m : bold = self . plain ( bold ) if not bold in table_titles : important . append ( bold . lower ( ) ) return important
3172	def get ( self , store_id , customer_id , ** queryparams ) : self . store_id = store_id self . customer_id = customer_id return self . _mc_client . _get ( url = self . _build_path ( store_id , 'customers' , customer_id ) , ** queryparams )
13608	def unpickle ( filepath ) : arr = [ ] with open ( filepath , 'rb' ) as f : carr = f . read ( blosc . MAX_BUFFERSIZE ) while len ( carr ) > 0 : arr . append ( blosc . decompress ( carr ) ) carr = f . read ( blosc . MAX_BUFFERSIZE ) return pkl . loads ( b"" . join ( arr ) )
8208	def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
1629	def GetHeaderGuardCPPVariable ( filename ) : filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
1987	def save_state ( self , state , key ) : with self . save_stream ( key , binary = True ) as f : self . _serializer . serialize ( state , f )
3796	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r if not hasattr ( self , 'kappas' ) : self . kappas = [ ] for Tc , kappa0 , kappa1 , kappa2 , kappa3 in zip ( self . Tcs , self . kappa0s , self . kappa1s , self . kappa2s , self . kappa3s ) : Tr = T / Tc kappa = kappa0 + ( ( kappa1 + kappa2 * ( kappa3 - Tr ) * ( 1. - Tr ** 0.5 ) ) * ( 1. + Tr ** 0.5 ) * ( 0.7 - Tr ) ) self . kappas . append ( kappa ) ( self . a , self . kappa , self . kappa0 , self . kappa1 , self . kappa2 , self . kappa3 , self . Tc ) = ( self . ais [ i ] , self . kappas [ i ] , self . kappa0s [ i ] , self . kappa1s [ i ] , self . kappa2s [ i ] , self . kappa3s [ i ] , self . Tcs [ i ] )
34	def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
2931	def pre_parse_and_validate_signavio ( self , bpmn , filename ) : self . _check_for_disconnected_boundary_events_signavio ( bpmn , filename ) self . _fix_call_activities_signavio ( bpmn , filename ) return bpmn
1062	def formatdate ( timeval = None ) : if timeval is None : timeval = time . time ( ) timeval = time . gmtime ( timeval ) return "%s, %02d %s %04d %02d:%02d:%02d GMT" % ( ( "Mon" , "Tue" , "Wed" , "Thu" , "Fri" , "Sat" , "Sun" ) [ timeval [ 6 ] ] , timeval [ 2 ] , ( "Jan" , "Feb" , "Mar" , "Apr" , "May" , "Jun" , "Jul" , "Aug" , "Sep" , "Oct" , "Nov" , "Dec" ) [ timeval [ 1 ] - 1 ] , timeval [ 0 ] , timeval [ 3 ] , timeval [ 4 ] , timeval [ 5 ] )
1635	def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] fncall = line for pattern in ( r'\bif\s*\((.*)\)\s*{' , r'\bfor\s*\((.*)\)\s*{' , r'\bwhile\s*\((.*)\)\s*[{;]' , r'\bswitch\s*\((.*)\)\s*{' ) : match = Search ( pattern , line ) if match : fncall = match . group ( 1 ) break if ( not Search ( r'\b(if|for|while|switch|return|new|delete|catch|sizeof)\b' , fncall ) and not Search ( r' \([^)]+\)\([^)]*(\)|,$)' , fncall ) and not Search ( r' \([^)]+\)\[[^\]]+\]' , fncall ) ) : if Search ( r'\w\s*\(\s(?!\s*\\$)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space after ( in function call' ) elif Search ( r'\(\s+(?!(\s*\\)|\()' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space after (' ) if ( Search ( r'\w\s+\(' , fncall ) and not Search ( r'_{0,2}asm_{0,2}\s+_{0,2}volatile_{0,2}\s+\(' , fncall ) and not Search ( r'#\s*define|typedef|using\s+\w+\s*=' , fncall ) and not Search ( r'\w\s+\((\w+::)*\*\w+\)\(' , fncall ) and not Search ( r'\bcase\s+\(' , fncall ) ) : if Search ( r'\boperator_*\b' , line ) : error ( filename , linenum , 'whitespace/parens' , 0 , 'Extra space before ( in function call' ) else : error ( filename , linenum , 'whitespace/parens' , 4 , 'Extra space before ( in function call' ) if Search ( r'[^)]\s+\)\s*[^{\s]' , fncall ) : if Search ( r'^\s+\)' , fncall ) : error ( filename , linenum , 'whitespace/parens' , 2 , 'Closing ) should be moved to the previous line' ) else : error ( filename , linenum , 'whitespace/parens' , 2 , 'Extra space before )' )
7254	def heartbeat ( self ) : url = '%s/heartbeat' % self . base_url r = requests . get ( url ) try : return r . json ( ) == "ok" except : return False
1092	def sub ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . sub ( repl , string , count )
13684	def post ( self , url , params = { } , files = None ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . post ( self . host + url , data = params , files = files ) return self . json_parse ( response . content ) except RequestException as e : return self . json_parse ( e . args )
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
6854	def getdevice_by_uuid ( uuid ) : with settings ( hide ( 'running' , 'warnings' , 'stdout' ) , warn_only = True ) : res = run_as_root ( 'blkid -U %s' % uuid ) if not res . succeeded : return None return res
5620	def get_best_zoom_level ( input_file , tile_pyramid_type ) : tile_pyramid = BufferedTilePyramid ( tile_pyramid_type ) with rasterio . open ( input_file , "r" ) as src : xmin , ymin , xmax , ymax = reproject_geometry ( segmentize_geometry ( box ( src . bounds . left , src . bounds . bottom , src . bounds . right , src . bounds . top ) , get_segmentize_value ( input_file , tile_pyramid ) ) , src_crs = src . crs , dst_crs = tile_pyramid . crs ) . bounds x_dif = xmax - xmin y_dif = ymax - ymin size = float ( src . width + src . height ) avg_resolution = ( ( x_dif / float ( src . width ) ) * ( float ( src . width ) / size ) + ( y_dif / float ( src . height ) ) * ( float ( src . height ) / size ) ) for zoom in range ( 0 , 40 ) : if tile_pyramid . pixel_x_size ( zoom ) <= avg_resolution : return zoom - 1
12550	def write_meta_header ( filename , meta_dict ) : header = '' for tag in MHD_TAGS : if tag in meta_dict . keys ( ) : header += '{} = {}\n' . format ( tag , meta_dict [ tag ] ) with open ( filename , 'w' ) as f : f . write ( header )
3676	def rdkitmol ( self ) : r if self . __rdkitmol : return self . __rdkitmol else : try : self . __rdkitmol = Chem . MolFromSmiles ( self . smiles ) return self . __rdkitmol except : return None
1970	def signal_receive ( self , fd ) : connections = self . connections if connections ( fd ) and self . twait [ connections ( fd ) ] : procid = random . sample ( self . twait [ connections ( fd ) ] , 1 ) [ 0 ] self . awake ( procid )
327	def summarize_paths ( samples , cone_std = ( 1. , 1.5 , 2. ) , starting_value = 1. ) : cum_samples = ep . cum_returns ( samples . T , starting_value = starting_value ) . T cum_mean = cum_samples . mean ( axis = 0 ) cum_std = cum_samples . std ( axis = 0 ) if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] cone_bounds = pd . DataFrame ( columns = pd . Float64Index ( [ ] ) ) for num_std in cone_std : cone_bounds . loc [ : , float ( num_std ) ] = cum_mean + cum_std * num_std cone_bounds . loc [ : , float ( - num_std ) ] = cum_mean - cum_std * num_std return cone_bounds
3038	def put ( self , credentials ) : self . acquire_lock ( ) try : self . locked_put ( credentials ) finally : self . release_lock ( )
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
1223	def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack
9148	def actions ( connection ) : session = _make_session ( connection = connection ) for action in Action . ls ( session = session ) : click . echo ( f'{action.created} {action.action} {action.resource}' )
11313	def update_cnum ( self ) : if "ConferencePaper" not in self . collections : cnums = record_get_field_values ( self . record , '773' , code = "w" ) for cnum in cnums : cnum_subs = [ ( "9" , "INSPIRE-CNUM" ) , ( "a" , cnum ) ] record_add_field ( self . record , "035" , subfields = cnum_subs )
3965	def restart_apps_or_services ( app_or_service_names = None ) : if app_or_service_names : log_to_client ( "Restarting the following apps or services: {}" . format ( ', ' . join ( app_or_service_names ) ) ) else : log_to_client ( "Restarting all active containers associated with Dusty" ) if app_or_service_names : specs = spec_assembler . get_assembled_specs ( ) specs_list = [ specs [ 'apps' ] [ app_name ] for app_name in app_or_service_names if app_name in specs [ 'apps' ] ] repos = set ( ) for spec in specs_list : if spec [ 'repo' ] : repos = repos . union ( spec_assembler . get_same_container_repos_from_spec ( spec ) ) nfs . update_nfs_with_repos ( repos ) else : nfs . update_nfs_with_repos ( spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) ) compose . restart_running_services ( app_or_service_names )
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
6133	def from_string ( cls , content , position = 1 , file_id = None ) : if file_id is None : file_id = 'inlined_input' return cls ( FileMetadata ( file_id , position ) , content )
6862	def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
2196	def flush ( self ) : if self . redirect is not None : self . redirect . flush ( ) super ( TeeStringIO , self ) . flush ( )
1803	def MOVBE ( cpu , dest , src ) : size = dest . size arg0 = dest . read ( ) temp = 0 for pos in range ( 0 , size , 8 ) : temp = ( temp << 8 ) | ( arg0 & 0xff ) arg0 = arg0 >> 8 dest . write ( arg0 )
7863	def is_certificate_valid ( stream , cert ) : try : logger . debug ( "tls_is_certificate_valid(cert = {0!r})" . format ( cert ) ) if not cert : logger . warning ( "No TLS certificate information received." ) return False if not cert . validated : logger . warning ( "TLS certificate not validated." ) return False srv_type = stream . transport . _dst_service if cert . verify_server ( stream . peer , srv_type ) : logger . debug ( " tls: certificate valid for {0!r}" . format ( stream . peer ) ) return True else : logger . debug ( " tls: certificate not valid for {0!r}" . format ( stream . peer ) ) return False except : logger . exception ( "Exception caught while checking a certificate" ) raise
12328	def init_repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive_template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv_filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv_filename , 'w' ) as fd : fd . write ( content ) self . make_hook_executable ( postrecv_filename ) print ( "Wrote to" , postrecv_filename )
427	def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]
11205	def name_from_string ( self , tzname_str ) : if not tzname_str . startswith ( '@' ) : return tzname_str name_splt = tzname_str . split ( ',-' ) try : offset = int ( name_splt [ 1 ] ) except : raise ValueError ( "Malformed timezone string." ) return self . load_name ( offset )
1586	def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
1622	def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : for i in range ( begin , end ) : lines [ i ] = '/**/'
3279	def add_provider ( self , share , provider , readonly = False ) : share = "/" + share . strip ( "/" ) assert share not in self . provider_map if compat . is_basestring ( provider ) : provider = FilesystemProvider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if "provider" in provider : prov_class = dynamic_import_class ( provider [ "provider" ] ) provider = prov_class ( * provider . get ( "args" , [ ] ) , ** provider . get ( "kwargs" , { } ) ) else : provider = FilesystemProvider ( provider [ "root" ] , bool ( provider . get ( "readonly" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise ValueError ( "Provider {}: tuple/list syntax is no longer supported" . format ( provider ) ) if not isinstance ( provider , DAVProvider ) : raise ValueError ( "Invalid provider {}" . format ( provider ) ) provider . set_share_path ( share ) if self . mount_path : provider . set_mount_path ( self . mount_path ) provider . set_lock_manager ( self . lock_manager ) provider . set_prop_manager ( self . prop_manager ) self . provider_map [ share ] = provider self . sorted_share_list = [ s . lower ( ) for s in self . provider_map . keys ( ) ] self . sorted_share_list = sorted ( self . sorted_share_list , key = len , reverse = True ) return provider
53	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : image = np . copy ( image ) if copy else image for keypoint in self . keypoints : image = keypoint . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image
12293	def annotate_metadata_action ( repo ) : package = repo . package print ( "Including history of actions" ) with cd ( repo . rootdir ) : filename = ".dgit/log.json" if os . path . exists ( filename ) : history = open ( filename ) . readlines ( ) actions = [ ] for a in history : try : a = json . loads ( a ) for x in [ 'code' ] : if x not in a or a [ x ] == None : a [ x ] = "..." actions . append ( a ) except : pass package [ 'actions' ] = actions
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
5843	def get_design_run_results ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_results ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) result = response [ "data" ] return DesignResults ( best_materials = result . get ( "best_material_results" ) , next_experiments = result . get ( "next_experiment_results" ) )
5567	def area_at_zoom ( self , zoom = None ) : if zoom is None : if not self . _cache_full_process_area : logger . debug ( "calculate process area ..." ) self . _cache_full_process_area = cascaded_union ( [ self . _area_at_zoom ( z ) for z in self . init_zoom_levels ] ) . buffer ( 0 ) return self . _cache_full_process_area else : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) return self . _area_at_zoom ( zoom )
6437	def dist_abs ( self , src , tar , weights = 'exponential' , max_length = 8 , normalized = False ) : xored = eudex ( src , max_length = max_length ) ^ eudex ( tar , max_length = max_length ) if not weights : binary = bin ( xored ) distance = binary . count ( '1' ) if normalized : return distance / ( len ( binary ) - 2 ) return distance if callable ( weights ) : weights = weights ( ) elif weights == 'exponential' : weights = Eudex . gen_exponential ( ) elif weights == 'fibonacci' : weights = Eudex . gen_fibonacci ( ) if isinstance ( weights , GeneratorType ) : weights = [ next ( weights ) for _ in range ( max_length ) ] [ : : - 1 ] distance = 0 max_distance = 0 while ( xored or normalized ) and weights : max_distance += 8 * weights [ - 1 ] distance += bin ( xored & 0xFF ) . count ( '1' ) * weights . pop ( ) xored >>= 8 if normalized : distance /= max_distance return distance
12905	def copy ( self ) : return self . __class__ ( name = self . name , valueType = self . valueType , defaultValue = self . defaultValue , hashIndex = self . hashIndex )
8961	def build ( ctx , docs = False ) : cfg = config . load ( ) ctx . run ( "python setup.py build" ) if docs : for doc_path in ( 'docs' , 'doc' ) : if os . path . exists ( cfg . rootjoin ( doc_path , 'conf.py' ) ) : break else : doc_path = None if doc_path : ctx . run ( "invoke docs" ) else : notify . warning ( "Cannot find either a 'docs' or 'doc' Sphinx directory!" )
11232	def run_excel_to_html ( ) : parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
2002	def _type_size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytesM' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty_i in ty [ 1 ] : result += ABI . _type_size ( ty_i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 return result raise ValueError
6097	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( radius , dim . Length ) : radius = dim . Length ( value = radius , unit_length = 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
4487	def update ( self , fp ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) url = self . _upload_url if fp . peek ( 1 ) : response = self . _put ( url , data = fp ) else : response = self . _put ( url , data = b'' ) if response . status_code != 200 : msg = ( 'Could not update {} (status ' 'code: {}).' . format ( self . path , response . status_code ) ) raise RuntimeError ( msg )
3968	def get_compose_dict ( assembled_specs , port_specs ) : compose_dict = _compose_dict_for_nginx ( port_specs ) for app_name in assembled_specs [ 'apps' ] . keys ( ) : compose_dict [ app_name ] = _composed_app_dict ( app_name , assembled_specs , port_specs ) for service_spec in assembled_specs [ 'services' ] . values ( ) : compose_dict [ service_spec . name ] = _composed_service_dict ( service_spec ) return compose_dict
3900	def dir_maker ( path ) : directory = os . path . dirname ( path ) if directory != '' and not os . path . isdir ( directory ) : try : os . makedirs ( directory ) except OSError as e : sys . exit ( 'Failed to create directory: {}' . format ( e ) )
3399	def add_switches_and_objective ( self ) : constraints = list ( ) big_m = max ( max ( abs ( b ) for b in r . bounds ) for r in self . model . reactions ) prob = self . model . problem for rxn in self . model . reactions : if not hasattr ( rxn , 'gapfilling_type' ) : continue indicator = prob . Variable ( name = 'indicator_{}' . format ( rxn . id ) , lb = 0 , ub = 1 , type = 'binary' ) if rxn . id in self . penalties : indicator . cost = self . penalties [ rxn . id ] else : indicator . cost = self . penalties [ rxn . gapfilling_type ] indicator . rxn_id = rxn . id self . indicators . append ( indicator ) constraint_lb = prob . Constraint ( rxn . flux_expression - big_m * indicator , ub = 0 , name = 'constraint_lb_{}' . format ( rxn . id ) , sloppy = True ) constraint_ub = prob . Constraint ( rxn . flux_expression + big_m * indicator , lb = 0 , name = 'constraint_ub_{}' . format ( rxn . id ) , sloppy = True ) constraints . extend ( [ constraint_lb , constraint_ub ] ) self . model . add_cons_vars ( self . indicators ) self . model . add_cons_vars ( constraints , sloppy = True ) self . model . objective = prob . Objective ( Zero , direction = 'min' , sloppy = True ) self . model . objective . set_linear_coefficients ( { i : 1 for i in self . indicators } ) self . update_costs ( )
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
8182	def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
13886	def ReplaceInFile ( filename , old , new , encoding = None ) : contents = GetFileContents ( filename , encoding = encoding ) contents = contents . replace ( old , new ) CreateFile ( filename , contents , encoding = encoding ) return contents
10593	def get_date ( date ) : if type ( date ) is str : return datetime . strptime ( date , '%Y-%m-%d' ) . date ( ) else : return date
2175	def refresh_token ( self , token_url , refresh_token = None , body = "" , auth = None , timeout = None , headers = None , verify = True , proxies = None , ** kwargs ) : if not token_url : raise ValueError ( "No token endpoint set for auto_refresh." ) if not is_secure_transport ( token_url ) : raise InsecureTransportError ( ) refresh_token = refresh_token or self . token . get ( "refresh_token" ) log . debug ( "Adding auto refresh key word arguments %s." , self . auto_refresh_kwargs ) kwargs . update ( self . auto_refresh_kwargs ) body = self . _client . prepare_refresh_body ( body = body , refresh_token = refresh_token , scope = self . scope , ** kwargs ) log . debug ( "Prepared refresh token request body %s" , body ) if headers is None : headers = { "Accept" : "application/json" , "Content-Type" : ( "application/x-www-form-urlencoded;charset=UTF-8" ) , } r = self . post ( token_url , data = dict ( urldecode ( body ) ) , auth = auth , timeout = timeout , headers = headers , verify = verify , withhold_token = True , proxies = proxies , ) log . debug ( "Request to refresh token completed with status %s." , r . status_code ) log . debug ( "Response headers were %s and content %s." , r . headers , r . text ) log . debug ( "Invoking %d token response hooks." , len ( self . compliance_hook [ "refresh_token_response" ] ) , ) for hook in self . compliance_hook [ "refresh_token_response" ] : log . debug ( "Invoking hook %s." , hook ) r = hook ( r ) self . token = self . _client . parse_request_body_response ( r . text , scope = self . scope ) if not "refresh_token" in self . token : log . debug ( "No new refresh token given. Re-using old." ) self . token [ "refresh_token" ] = refresh_token return self . token
10111	def iterrows ( lines_or_file , namedtuples = False , dicts = False , encoding = 'utf-8' , ** kw ) : if namedtuples and dicts : raise ValueError ( 'either namedtuples or dicts can be chosen as output format' ) elif namedtuples : _reader = NamedTupleReader elif dicts : _reader = UnicodeDictReader else : _reader = UnicodeReader with _reader ( lines_or_file , encoding = encoding , ** fix_kw ( kw ) ) as r : for item in r : yield item
8176	def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
11832	def mate ( self , other ) : "Return a new individual crossing self and other." c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
12815	def _finish ( self , forced = False ) : if hasattr ( self , "_current_file_handle" ) and self . _current_file_handle : self . _current_file_handle . close ( ) if self . _current_deferred : self . _current_deferred . callback ( self . _sent ) self . _current_deferred = None if not forced and self . _deferred : self . _deferred . callback ( self . _sent )
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
7937	def connect ( self , addr , port = None , service = None ) : with self . lock : self . _connect ( addr , port , service )
8040	def is_public ( self ) : for decorator in self . decorators : if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
5020	def validate_image_size ( image ) : config = get_app_config ( ) valid_max_image_size_in_bytes = config . valid_max_image_size * 1024 if config and not image . size <= valid_max_image_size_in_bytes : raise ValidationError ( _ ( "The logo image file size must be less than or equal to %s KB." ) % config . valid_max_image_size )
2303	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_gies ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
6675	def umask ( self , use_sudo = False ) : func = use_sudo and run_as_root or self . run return func ( 'umask' )
4004	def get_authed_registries ( ) : result = set ( ) if not os . path . exists ( constants . DOCKER_CONFIG_PATH ) : return result config = json . load ( open ( constants . DOCKER_CONFIG_PATH , 'r' ) ) for registry in config . get ( 'auths' , { } ) . iterkeys ( ) : try : parsed = urlparse ( registry ) except Exception : log_to_client ( 'Error parsing registry {} from Docker config, will skip this registry' ) . format ( registry ) result . add ( parsed . netloc ) if parsed . netloc else result . add ( parsed . path ) return result
4941	def unlink_user ( self , enterprise_customer , user_email ) : try : existing_user = User . objects . get ( email = user_email ) link_record = self . get ( enterprise_customer = enterprise_customer , user_id = existing_user . id ) link_record . delete ( ) if update_user : update_user . delay ( sailthru_vars = { 'is_enterprise_learner' : False , 'enterprise_name' : None , } , email = user_email ) except User . DoesNotExist : pending_link = PendingEnterpriseCustomerUser . objects . get ( enterprise_customer = enterprise_customer , user_email = user_email ) pending_link . delete ( ) LOGGER . info ( 'Enterprise learner {%s} successfully unlinked from Enterprise Customer {%s}' , user_email , enterprise_customer . name )
13460	def event_all_comments_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) comments = event . all_comments page = int ( request . GET . get ( 'page' , 99999 ) ) is_paginated = False if comments : paginator = Paginator ( comments , 50 ) try : comments = paginator . page ( page ) except EmptyPage : comments = paginator . page ( paginator . num_pages ) is_paginated = comments . has_other_pages ( ) return render ( request , 'happenings/event_comments.html' , { "event" : event , "comment_list" : comments , "object_list" : comments , "page_obj" : comments , "page" : page , "is_paginated" : is_paginated , "key" : key } )
4298	def _convert_config_to_stdin ( config , parser ) : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) args = [ ] for key , val in config . items ( SECTION ) : keyp = '--{0}' . format ( key ) action = parser . _option_string_actions [ keyp ] if action . const : try : if config . getboolean ( SECTION , key ) : args . append ( keyp ) except ValueError : args . extend ( [ keyp , val ] ) elif any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if val != '' : args . extend ( [ keyp , val ] ) else : args . extend ( [ keyp , val ] ) return args
5934	def to_int64 ( a ) : def promote_i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote_i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
4141	def _arburg2 ( X , order ) : x = np . array ( X ) N = len ( x ) if order <= 0. : raise ValueError ( "order must be > 0" ) rho = sum ( abs ( x ) ** 2. ) / N den = rho * 2. * N ef = np . zeros ( N , dtype = complex ) eb = np . zeros ( N , dtype = complex ) for j in range ( 0 , N ) : ef [ j ] = x [ j ] eb [ j ] = x [ j ] a = np . zeros ( 1 , dtype = complex ) a [ 0 ] = 1 ref = np . zeros ( order , dtype = complex ) temp = 1. E = np . zeros ( order + 1 ) E [ 0 ] = rho for m in range ( 0 , order ) : efp = ef [ 1 : ] ebp = eb [ 0 : - 1 ] num = - 2. * np . dot ( ebp . conj ( ) . transpose ( ) , efp ) den = np . dot ( efp . conj ( ) . transpose ( ) , efp ) den += np . dot ( ebp , ebp . conj ( ) . transpose ( ) ) ref [ m ] = num / den ef = efp + ref [ m ] * ebp eb = ebp + ref [ m ] . conj ( ) . transpose ( ) * efp a . resize ( len ( a ) + 1 ) a = a + ref [ m ] * np . flipud ( a ) . conjugate ( ) E [ m + 1 ] = ( 1 - ref [ m ] . conj ( ) . transpose ( ) * ref [ m ] ) * E [ m ] return a , E [ - 1 ] , ref
7248	def get_stdout ( self , workflow_id , task_id ) : url = '%(wf_url)s/%(wf_id)s/tasks/%(task_id)s/stdout' % { 'wf_url' : self . workflows_url , 'wf_id' : workflow_id , 'task_id' : task_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . text
13103	def create_scan ( self , host_ips ) : now = datetime . datetime . now ( ) data = { "uuid" : self . get_template_uuid ( ) , "settings" : { "name" : "jackal-" + now . strftime ( "%Y-%m-%d %H:%M" ) , "text_targets" : host_ips } } response = requests . post ( self . url + 'scans' , data = json . dumps ( data ) , verify = False , headers = self . headers ) if response : result = json . loads ( response . text ) return result [ 'scan' ] [ 'id' ]
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
12841	def _close ( self , conn ) : super ( PooledAIODatabase , self ) . _close ( conn ) for waiter in self . _waiters : if not waiter . done ( ) : logger . debug ( 'Release a waiter' ) waiter . set_result ( True ) break
2584	def get_tasks ( self , count ) : tasks = [ ] for i in range ( 0 , count ) : try : x = self . pending_task_queue . get ( block = False ) except queue . Empty : break else : tasks . append ( x ) return tasks
11911	def get_version ( filename , pattern ) : with open ( filename ) as f : match = re . search ( r"^(\s*%s\s*=\s*')(.+?)(')(?sm)" % pattern , f . read ( ) ) if match : before , version , after = match . groups ( ) return version fail ( 'Could not find {} in {}' . format ( pattern , filename ) )
9491	def _get_name_info ( name_index , name_list ) : argval = name_index if name_list is not None : try : argval = name_list [ name_index ] except IndexError : raise ValidationError ( "Names value out of range: {}" . format ( name_index ) ) from None argrepr = argval else : argrepr = repr ( argval ) return argval , argrepr
10426	def enrich_internal_unqualified_edges ( graph , subgraph ) : for u , v in itt . combinations ( subgraph , 2 ) : if not graph . has_edge ( u , v ) : continue for k in graph [ u ] [ v ] : if k < 0 : subgraph . add_edge ( u , v , key = k , ** graph [ u ] [ v ] [ k ] )
11261	def resplit ( prev , pattern , * args , ** kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
4225	def load_config ( ) : filename = 'keyringrc.cfg' keyring_cfg = os . path . join ( platform . config_root ( ) , filename ) if not os . path . exists ( keyring_cfg ) : return config = configparser . RawConfigParser ( ) config . read ( keyring_cfg ) _load_keyring_path ( config ) try : if config . has_section ( "backend" ) : keyring_name = config . get ( "backend" , "default-keyring" ) . strip ( ) else : raise configparser . NoOptionError ( 'backend' , 'default-keyring' ) except ( configparser . NoOptionError , ImportError ) : logger = logging . getLogger ( 'keyring' ) logger . warning ( "Keyring config file contains incorrect values.\n" + "Config file: %s" % keyring_cfg ) return return load_keyring ( keyring_name )
9953	def _get_node ( name : str , args : str ) : obj = get_object ( name ) args = ast . literal_eval ( args ) if not isinstance ( args , tuple ) : args = ( args , ) return obj . node ( * args )
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
7727	def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != self . ns : pass elif n . name == "item" : ret . append ( MucItem ( n ) ) elif n . name == "status" : ret . append ( MucStatus ( n ) ) n = n . next return ret
12111	def save ( self , filename , metadata = { } , ** data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
1304	def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )
1084	def time ( self ) : "Return the time part, with tzinfo None." return time ( self . hour , self . minute , self . second , self . microsecond )
12975	def _doSave ( self , obj , isInsert , conn , pipeline = None ) : if pipeline is None : pipeline = conn newDict = obj . asDict ( forStorage = True ) key = self . _get_key_for_id ( obj . _id ) if isInsert is True : for thisField in self . fields : fieldValue = newDict . get ( thisField , thisField . getDefaultValue ( ) ) pipeline . hset ( key , thisField , fieldValue ) if fieldValue == IR_NULL_STR : obj . _origData [ thisField ] = irNull else : obj . _origData [ thisField ] = object . __getattribute__ ( obj , str ( thisField ) ) self . _add_id_to_keys ( obj . _id , pipeline ) for indexedField in self . indexedFields : self . _add_id_to_index ( indexedField , obj . _id , obj . _origData [ indexedField ] , pipeline ) else : updatedFields = obj . getUpdatedFields ( ) for thisField , fieldValue in updatedFields . items ( ) : ( oldValue , newValue ) = fieldValue oldValueForStorage = thisField . toStorage ( oldValue ) newValueForStorage = thisField . toStorage ( newValue ) pipeline . hset ( key , thisField , newValueForStorage ) if thisField in self . indexedFields : self . _rem_id_from_index ( thisField , obj . _id , oldValueForStorage , pipeline ) self . _add_id_to_index ( thisField , obj . _id , newValueForStorage , pipeline ) obj . _origData [ thisField ] = newValue
10086	def clear ( self , * args , ** kwargs ) : super ( Deposit , self ) . clear ( * args , ** kwargs )
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
2075	def score_models ( clf , X , y , encoder , runs = 1 ) : scores = [ ] X_test = None for _ in range ( runs ) : X_test = encoder ( ) . fit_transform ( X , y ) X_test = StandardScaler ( ) . fit_transform ( X_test ) scores . append ( cross_validate ( clf , X_test , y , n_jobs = 1 , cv = 5 ) [ 'test_score' ] ) gc . collect ( ) scores = [ y for z in [ x for x in scores ] for y in z ] return float ( np . mean ( scores ) ) , float ( np . std ( scores ) ) , scores , X_test . shape [ 1 ]
13131	def parse_domain_computers ( filename ) : with open ( filename ) as f : data = json . loads ( f . read ( ) ) hs = HostSearch ( ) count = 0 entry_count = 0 print_notification ( "Parsing {} entries" . format ( len ( data ) ) ) for system in data : entry_count += 1 parsed = parse_single_computer ( system ) if parsed . ip : try : host = hs . id_to_object ( parsed . ip ) host . description . append ( parsed . description ) host . hostname . append ( parsed . dns_hostname ) if parsed . os : host . os = parsed . os host . domain_controller = parsed . dc host . add_tag ( 'domaindump' ) host . save ( ) count += 1 except ValueError : pass sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}] {} resolved" . format ( entry_count , len ( data ) , count ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
5814	def _try_decode ( byte_string ) : try : return str_cls ( byte_string , _encoding ) except ( UnicodeDecodeError ) : for encoding in _fallback_encodings : try : return str_cls ( byte_string , encoding , errors = 'strict' ) except ( UnicodeDecodeError ) : pass return str_cls ( byte_string , errors = 'replace' )
10168	def get_components ( self , line , with_type = True ) : ret = { } line2 = reduce ( lambda x , y : x + y , split ( '\(.+\)' , line ) ) if with_type : splitted = split ( '\W+' , line2 ) [ 3 : ] else : splitted = split ( '\W+' , line2 ) [ 2 : ] ret = dict ( zip ( splitted [ 0 : : 2 ] , splitted [ 1 : : 2 ] ) ) return ret
8563	def delete_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) , method = 'DELETE' ) return response
7718	def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) self . xmlnode . freeNode ( ) self . xmlnode = None
11902	def serve_dir ( dir_path ) : print ( 'Performing first pass index file generation' ) created_files = _create_index_files ( dir_path , True ) if ( PIL_ENABLED ) : print ( 'Performing PIL-enchanced optimised index file generation in background' ) background_indexer = BackgroundIndexFileGenerator ( dir_path ) background_indexer . run ( ) _run_server ( ) _clean_up ( created_files )
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
1520	def get_remote_home ( host , cl_args ) : cmd = "echo ~" if not is_self ( host ) : cmd = ssh_remote_execute ( cmd , host , cl_args ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get home path for remote host %s with output:\n%s" % ( host , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
8474	def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : data = dict ( self . config . items ( section ) ) return data
3428	def remove_metabolites ( self , metabolite_list , destructive = False ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] metabolite_list = [ x for x in metabolite_list if x . id in self . metabolites ] for x in metabolite_list : x . _model = None associated_groups = self . get_associated_groups ( x ) for group in associated_groups : group . remove_members ( x ) if not destructive : for the_reaction in list ( x . _reaction ) : the_coefficient = the_reaction . _metabolites [ x ] the_reaction . subtract_metabolites ( { x : the_coefficient } ) else : for x in list ( x . _reaction ) : x . remove_from_model ( ) self . metabolites -= metabolite_list to_remove = [ self . solver . constraints [ m . id ] for m in metabolite_list ] self . remove_cons_vars ( to_remove ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __iadd__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , self ) )
10019	def create_environment ( self , env_name , version_label = None , solution_stack_name = None , cname_prefix = None , description = None , option_settings = None , tier_name = 'WebServer' , tier_type = 'Standard' , tier_version = '1.1' ) : out ( "Creating environment: " + str ( env_name ) + ", tier_name:" + str ( tier_name ) + ", tier_type:" + str ( tier_type ) ) self . ebs . create_environment ( self . app_name , env_name , version_label = version_label , solution_stack_name = solution_stack_name , cname_prefix = cname_prefix , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
7466	def _load_existing_results ( self , name , workdir ) : path = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) mcmcs = glob . glob ( path + "_r*.mcmc.txt" ) outs = glob . glob ( path + "_r*.out.txt" ) trees = glob . glob ( path + "_r*.tre" ) for mcmcfile in mcmcs : if mcmcfile not in self . files . mcmcfiles : self . files . mcmcfiles . append ( mcmcfile ) for outfile in outs : if outfile not in self . files . outfiles : self . files . outfiles . append ( outfile ) for tree in trees : if tree not in self . files . treefiles : self . files . treefiles . append ( tree )
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
609	def _generateMetricSpecString ( inferenceElement , metric , params = None , field = None , returnLabel = False ) : metricSpecArgs = dict ( metric = metric , field = field , params = params , inferenceElement = inferenceElement ) metricSpecAsString = "MetricSpec(%s)" % ', ' . join ( [ '%s=%r' % ( item [ 0 ] , item [ 1 ] ) for item in metricSpecArgs . iteritems ( ) ] ) if not returnLabel : return metricSpecAsString spec = MetricSpec ( ** metricSpecArgs ) metricLabel = spec . getLabel ( ) return metricSpecAsString , metricLabel
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
1020	def buildOverlappedSequences ( numSequences = 2 , seqLen = 5 , sharedElements = [ 3 , 4 ] , numOnBitsPerPattern = 3 , patternOverlap = 0 , seqOverlap = 0 , ** kwargs ) : numSharedElements = len ( sharedElements ) numUniqueElements = seqLen - numSharedElements numPatterns = numSharedElements + numUniqueElements * numSequences patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] uniquePatternIndices = range ( numSharedElements , numPatterns ) for _ in xrange ( numSequences ) : sequence = [ ] sharedPatternIndices = range ( numSharedElements ) for j in xrange ( seqLen ) : if j in sharedElements : patIdx = sharedPatternIndices . pop ( 0 ) else : patIdx = uniquePatternIndices . pop ( 0 ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print "\nTraining sequences" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )
11476	def _create_or_reuse_item ( local_file , parent_folder_id , reuse_existing = False ) : local_item_name = os . path . basename ( local_file ) item_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) items = children [ 'items' ] for item in items : if item [ 'name' ] == local_item_name : item_id = item [ 'item_id' ] break if item_id is None : new_item = session . communicator . create_item ( session . token , local_item_name , parent_folder_id ) item_id = new_item [ 'item_id' ] return item_id
6186	def get_last_commit_line ( git_path = None ) : if git_path is None : git_path = GIT_PATH output = check_output ( [ git_path , "log" , "--pretty=format:'%ad %h %s'" , "--date=short" , "-n1" ] ) return output . strip ( ) [ 1 : - 1 ]
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
8883	def predict ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix ) <= self . threshold_value
294	def plot_exposures ( returns , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) pos_no_cash = positions . drop ( 'cash' , axis = 1 ) l_exp = pos_no_cash [ pos_no_cash > 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) s_exp = pos_no_cash [ pos_no_cash < 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) net_exp = pos_no_cash . sum ( axis = 1 ) / positions . sum ( axis = 1 ) ax . fill_between ( l_exp . index , 0 , l_exp . values , label = 'Long' , color = 'green' , alpha = 0.5 ) ax . fill_between ( s_exp . index , 0 , s_exp . values , label = 'Short' , color = 'red' , alpha = 0.5 ) ax . plot ( net_exp . index , net_exp . values , label = 'Net' , color = 'black' , linestyle = 'dotted' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( "Exposure" ) ax . set_ylabel ( 'Exposure' ) ax . legend ( loc = 'lower left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
9194	def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
6759	def get_package_list ( self ) : os_version = self . os_version self . vprint ( 'os_version:' , os_version ) req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
7740	def hold_exception ( method ) : @ functools . wraps ( method ) def wrapper ( self , * args , ** kwargs ) : try : return method ( self , * args , ** kwargs ) except Exception : if self . exc_info : raise if not self . _stack : logger . debug ( '@hold_exception wrapped method {0!r} called' ' from outside of the main loop' . format ( method ) ) raise self . exc_info = sys . exc_info ( ) logger . debug ( u"exception in glib main loop callback:" , exc_info = self . exc_info ) main_loop = self . _stack [ - 1 ] if main_loop is not None : main_loop . quit ( ) return False return wrapper
8857	def on_run ( self ) : filename = self . tabWidget . current_widget ( ) . file . path wd = os . path . dirname ( filename ) args = Settings ( ) . get_run_config_for_file ( filename ) self . interactiveConsole . start_process ( Settings ( ) . interpreter , args = [ filename ] + args , cwd = wd ) self . dockWidget . show ( ) self . actionRun . setEnabled ( False ) self . actionConfigure_run . setEnabled ( False )
855	def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )
4757	def rehome ( old , new , struct ) : if old == new : return if isinstance ( struct , list ) : for item in struct : rehome ( old , new , item ) elif isinstance ( struct , dict ) : for key , val in struct . iteritems ( ) : if isinstance ( val , ( dict , list ) ) : rehome ( old , new , val ) elif "conf" in key : continue elif "orig" in key : continue elif "root" in key or "path" in key : struct [ key ] = struct [ key ] . replace ( old , new )
11557	def extended_analog ( self , pin , data ) : analog_data = [ pin , data & 0x7f , ( data >> 7 ) & 0x7f , ( data >> 14 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . EXTENDED_ANALOG , analog_data )
4852	def _transmit_delete ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . delete_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to delete [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _delete_transmissions ( chunk . keys ( ) )
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s ' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) links = self . _parse_link_headers ( request [ 'Link' ] ) for link in links : self . relate ( target , read ( self , link [ 'uri' ] ) ) self . response . status = http . client . NO_CONTENT self . make_response ( )
6162	def MPSK_bb ( N_symb , Ns , M , pulse = 'rect' , alpha = 0.25 , MM = 6 ) : data = np . random . randint ( 0 , M , N_symb ) xs = np . exp ( 1j * 2 * np . pi / M * data ) x = np . hstack ( ( xs . reshape ( N_symb , 1 ) , np . zeros ( ( N_symb , int ( Ns ) - 1 ) ) ) ) x = x . flatten ( ) if pulse . lower ( ) == 'rect' : b = np . ones ( int ( Ns ) ) elif pulse . lower ( ) == 'rc' : b = rc_imp ( Ns , alpha , MM ) elif pulse . lower ( ) == 'src' : b = sqrt_rc_imp ( Ns , alpha , MM ) else : raise ValueError ( 'pulse type must be rec, rc, or src' ) x = signal . lfilter ( b , 1 , x ) if M == 4 : x = x * np . exp ( 1j * np . pi / 4 ) return x , b / float ( Ns ) , data
7949	def send_stream_tail ( self ) : with self . lock : if not self . _socket or self . _hup : logger . debug ( u"Cannot send stream closing tag: already closed" ) return data = self . _serializer . emit_tail ( ) try : self . _write ( data . encode ( "utf-8" ) ) except ( IOError , SystemError , socket . error ) , err : logger . debug ( u"Sending stream closing tag failed: {0}" . format ( err ) ) self . _serializer = None self . _hup = True if self . _tls_state is None : try : self . _socket . shutdown ( socket . SHUT_WR ) except socket . error : pass self . _set_state ( "closing" ) self . _write_queue . clear ( ) self . _write_queue_cond . notify ( )
5838	def _data_analysis ( self , data_view_id ) : failure_message = "Error while retrieving data analysis for data view {}" . format ( data_view_id ) return self . _get_success_json ( self . _get ( routes . data_analysis ( data_view_id ) , failure_message = failure_message ) )
5929	def getLogLevel ( self , section , option ) : return logging . getLevelName ( self . get ( section , option ) . upper ( ) )
1058	def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( "key %s is not registered with code %s" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]
12220	def _make_wrapper ( self , func ) : @ wraps ( func ) def executor ( * args , ** kwargs ) : return self . execute ( args , kwargs ) executor . dispatch = self . dispatch executor . dispatch_first = self . dispatch_first executor . func = func executor . lookup = self . lookup return executor
13785	def generate ( length = DEFAULT_LENGTH ) : return '' . join ( random . SystemRandom ( ) . choice ( ALPHABET ) for _ in range ( length ) )
7532	def trackjobs ( func , results , spacer ) : LOGGER . info ( "inside trackjobs of %s" , func ) asyncs = [ ( i , results [ i ] ) for i in results if i . split ( "-" , 2 ) [ 0 ] == func ] start = time . time ( ) while 1 : ready = [ i [ 1 ] . ready ( ) for i in asyncs ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) printstr = " {} | {} | s3 |" . format ( PRINTSTR [ func ] , elapsed ) progressbar ( len ( ready ) , sum ( ready ) , printstr , spacer = spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break sfails = [ ] errmsgs = [ ] for job in asyncs : if not job [ 1 ] . successful ( ) : sfails . append ( job [ 0 ] ) errmsgs . append ( job [ 1 ] . result ( ) ) return func , sfails , errmsgs
2676	def get_callable_handler_function ( src , handler ) : os . chdir ( src ) module_name , function_name = handler . split ( '.' ) filename = get_handler_filename ( handler ) path_to_module_file = os . path . join ( src , filename ) module = load_source ( module_name , path_to_module_file ) return getattr ( module , function_name )
8269	def contains ( self , clr ) : if not isinstance ( clr , Color ) : return False if not isinstance ( clr , _list ) : clr = [ clr ] for clr in clr : if clr . is_grey and not self . grayscale : return ( self . black . contains ( clr ) or self . white . contains ( clr ) ) for r , v in [ ( self . h , clr . h ) , ( self . s , clr . s ) , ( self . b , clr . brightness ) , ( self . a , clr . a ) ] : if isinstance ( r , _list ) : pass elif isinstance ( r , tuple ) : r = [ r ] else : r = [ ( r , r ) ] for min , max in r : if not ( min <= v <= max ) : return False return True
9853	def centers ( self ) : for idx in numpy . ndindex ( self . grid . shape ) : yield self . delta * numpy . array ( idx ) + self . origin
3836	async def set_conversation_notification_level ( self , set_conversation_notification_level_request ) : response = hangouts_pb2 . SetConversationNotificationLevelResponse ( ) await self . _pb_request ( 'conversations/setconversationnotificationlevel' , set_conversation_notification_level_request , response ) return response
7613	def get_arena_image ( self , obj : BaseAttrDict ) : badge_id = obj . arena . id for i in self . constants . arenas : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/arenas/arena{}.png' . format ( i . arena_id )
613	def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results
5733	def _get_notify_msg_and_payload ( result , stream ) : token = stream . advance_past_chars ( [ "=" , "*" ] ) token = int ( token ) if token != "" else None logger . debug ( "%s" , fmt_green ( "parsing message" ) ) message = stream . advance_past_chars ( [ "," ] ) logger . debug ( "parsed message" ) logger . debug ( "%s" , fmt_green ( message ) ) payload = _parse_dict ( stream ) return token , message . strip ( ) , payload
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) self . forum . on_start_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
4674	def getPrivateKeyForPublicKey ( self , pub ) : if str ( pub ) not in self . store : raise KeyNotFound return self . store . getPrivateKeyForPublicKey ( str ( pub ) )
10095	def create_template ( self , name , subject , html , text = '' , timeout = None ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } return self . _api_request ( self . TEMPLATES_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
2018	def MOD ( self , a , b ) : try : result = Operators . ITEBV ( 256 , b == 0 , 0 , a % b ) except ZeroDivisionError : result = 0 return result
10805	def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
11648	def fit ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) self . train_ = X memory = get_memory ( self . memory ) lo , = memory . cache ( scipy . linalg . eigvalsh ) ( X , eigvals = ( 0 , 0 ) ) self . shift_ = max ( self . min_eig - lo , 0 ) return self
12799	def _url ( self , url = None , parameters = None ) : uri = url or self . _settings [ "url" ] if url and self . _settings [ "base_url" ] : uri = "%s/%s" % ( self . _settings [ "base_url" ] , url ) uri += ".json" if parameters : uri += "?%s" % urllib . urlencode ( parameters ) return uri
2183	def existing_versions ( self ) : import glob pattern = join ( self . dpath , self . fname + '_*' + self . ext ) for fname in glob . iglob ( pattern ) : data_fpath = join ( self . dpath , fname ) yield data_fpath
11335	def prompt ( question , choices = None ) : if not re . match ( "\s$" , question ) : question = "{}: " . format ( question ) while True : if sys . version_info [ 0 ] > 2 : answer = input ( question ) else : answer = raw_input ( question ) if not choices or answer in choices : break return answer
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
10235	def reaction_cartesian_expansion ( graph : BELGraph , accept_unqualified_edges : bool = True ) -> None : for u , v , d in list ( graph . edges ( data = True ) ) : if CITATION not in d and accept_unqualified_edges : _reaction_cartesion_expansion_unqualified_helper ( graph , u , v , d ) continue if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in catalysts or product in catalysts : continue graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in catalysts or product in catalysts : continue graph . add_qualified_edge ( product , reactant , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , Reaction ) : catalysts = _get_catalysts_in_reaction ( u ) for product in u . products : if product in catalysts : continue if v not in u . products and v not in u . reactants : graph . add_increases ( product , v , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for reactant in u . reactants : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , Reaction ) : for reactant in v . reactants : catalysts = _get_catalysts_in_reaction ( v ) if reactant in catalysts : continue if u not in v . products and u not in v . reactants : graph . add_increases ( u , reactant , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) for product in v . products : graph . add_increases ( reactant , product , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_reaction_nodes ( graph )
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . generation = 1 self . divinity = god
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
11560	def i2c_stop_reading ( self , address ) : data = [ address , self . I2C_STOP_READING ] self . _command_handler . send_sysex ( self . _command_handler . I2C_REQUEST , data )
13609	def contact ( request ) : form = ContactForm ( request . POST or None ) if form . is_valid ( ) : subject = form . cleaned_data [ 'subject' ] message = form . cleaned_data [ 'message' ] sender = form . cleaned_data [ 'sender' ] cc_myself = form . cleaned_data [ 'cc_myself' ] recipients = settings . CONTACTFORM_RECIPIENTS if cc_myself : recipients . append ( sender ) send_mail ( getattr ( settings , "CONTACTFORM_SUBJECT_PREFIX" , '' ) + subject , message , sender , recipients ) return render ( request , 'contactform/thanks.html' ) return render ( request , 'contactform/contact.html' , { 'form' : form } )
7288	def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : if document is None : return None current_key , new_key_array = trim_field_key ( document , field_key ) key_array_digit = int ( new_key_array [ - 1 ] ) if new_key_array and has_digit ( new_key_array ) else None new_key = make_key ( new_key_array ) if key_array_digit is not None and len ( new_key_array ) > 0 : if len ( new_key_array ) == 1 : return_data = document . _data . get ( current_key , [ ] ) elif isinstance ( document , BaseList ) : return_list = [ ] if len ( document ) > 0 : return_list = [ get_value ( doc , new_key ) for doc in document ] return_data = return_list else : return_data = get_value ( getattr ( document , current_key ) , new_key ) elif len ( new_key_array ) > 0 : return_data = get_value ( document . _data . get ( current_key ) , new_key ) else : try : return_data = ( document . _data . get ( None , None ) if current_key == "id" else document . _data . get ( current_key , None ) ) except : return_data = document . _data . get ( current_key , None ) return return_data if self . is_initialized : return get_value ( self . model_instance , field_key ) else : return None
5924	def setup ( filename = CONFIGNAME ) : get_configuration ( ) if not os . path . exists ( filename ) : with open ( filename , 'w' ) as configfile : cfg . write ( configfile ) msg = "NOTE: GromacsWrapper created the configuration file \n\t%r\n" " for you. Edit the file to customize the package." % filename print ( msg ) for d in config_directories : utilities . mkdir_p ( d )
7157	def assign_prompter ( self , prompter ) : if is_string ( prompter ) : if prompter not in prompters : eprint ( "Error: '{}' is not a core prompter" . format ( prompter ) ) sys . exit ( ) self . prompter = prompters [ prompter ] else : self . prompter = prompter
12942	def hasUnsavedChanges ( self , cascadeObjects = False ) : if not self . _id or not self . _origData : return True for thisField in self . FIELDS : thisVal = object . __getattribute__ ( self , thisField ) if self . _origData . get ( thisField , '' ) != thisVal : return True if cascadeObjects is True and issubclass ( thisField . __class__ , IRForeignLinkFieldBase ) : if thisVal . objHasUnsavedChanges ( ) : return True return False
7409	def get_order ( tre ) : anode = tre . tree & ">A" sister = anode . get_sisters ( ) [ 0 ] sisters = ( anode . name [ 1 : ] , sister . name [ 1 : ] ) others = [ i for i in list ( "ABCD" ) if i not in sisters ] return sorted ( sisters ) + sorted ( others )
3778	def T_dependent_property_integral ( self , T1 , T2 ) : r Tavg = 0.5 * ( T1 + T2 ) if self . method : if self . test_method_validity ( Tavg , self . method ) : try : return self . calculate_integral ( T1 , T2 , self . method ) except : pass sorted_valid_methods = self . select_valid_methods ( Tavg ) for method in sorted_valid_methods : try : return self . calculate_integral ( T1 , T2 , method ) except : pass return None
10451	def grabfocus ( self , window_name , object_name = None ) : if not object_name : handle , name , app = self . _get_window_handle ( window_name ) else : handle = self . _get_object_handle ( window_name , object_name ) return self . _grabfocus ( handle )
11955	def is_dot ( ip ) : octets = str ( ip ) . split ( '.' ) if len ( octets ) != 4 : return False for i in octets : try : val = int ( i ) except ValueError : return False if val > 255 or val < 0 : return False return True
780	def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID
6820	def configure_modevasive ( self ) : r = self . local_renderer if r . env . modevasive_enabled : self . install_packages ( ) fn = r . render_to_file ( 'apache/apache_modevasive.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/mod-evasive.conf' , use_sudo = True ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/evasive.conf' , use_sudo = True ) self . enable_mod ( 'evasive' ) else : if self . last_manifest . modevasive_enabled : self . disable_mod ( 'evasive' )
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
1878	def MOVSS ( cpu , dest , src ) : if dest . type == 'register' and src . type == 'register' : assert dest . size == 128 and src . size == 128 dest . write ( dest . read ( ) & ~ 0xffffffff | src . read ( ) & 0xffffffff ) elif dest . type == 'memory' : assert src . type == 'register' dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : assert src . type == 'memory' and dest . type == 'register' assert src . size == 32 and dest . size == 128 dest . write ( Operators . ZEXTEND ( src . read ( ) , 128 ) )
7300	def get_context_data ( self , ** kwargs ) : context = super ( DocumentListView , self ) . get_context_data ( ** kwargs ) context = self . set_permissions_in_context ( context ) if not context [ 'has_view_permission' ] : return HttpResponseForbidden ( "You do not have permissions to view this content." ) context [ 'object_list' ] = self . get_queryset ( ) context [ 'document' ] = self . document context [ 'app_label' ] = self . app_label context [ 'document_name' ] = self . document_name context [ 'request' ] = self . request context [ 'page' ] = self . page context [ 'documents_per_page' ] = self . documents_per_page if self . page > 1 : previous_page_number = self . page - 1 else : previous_page_number = None if self . page < self . total_pages : next_page_number = self . page + 1 else : next_page_number = None context [ 'previous_page_number' ] = previous_page_number context [ 'has_previous_page' ] = previous_page_number is not None context [ 'next_page_number' ] = next_page_number context [ 'has_next_page' ] = next_page_number is not None context [ 'total_pages' ] = self . total_pages if self . queryset . count ( ) : context [ 'keys' ] = [ 'id' , ] for key in [ x for x in self . mongoadmin . list_fields if x != 'id' and x in self . document . _fields . keys ( ) ] : if isinstance ( self . document . _fields [ key ] , EmbeddedDocumentField ) : continue if isinstance ( self . document . _fields [ key ] , ListField ) : continue context [ 'keys' ] . append ( key ) if self . mongoadmin . search_fields : context [ 'search_field' ] = True return context
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : if element [ 'required' ] is True : content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : break if element_start is None and element [ 'required' ] is False : continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
8107	def dumps ( obj , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , encoding = 'utf-8' , default = None , ** kw ) : if ( skipkeys is False and ensure_ascii is True and check_circular is True and allow_nan is True and cls is None and indent is None and separators is None and encoding == 'utf-8' and default is None and not kw ) : return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , encoding = encoding , default = default , ** kw ) . encode ( obj )
9797	def delete ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not click . confirm ( "Are sure you want to delete experiment group `{}`" . format ( _group ) ) : click . echo ( 'Existing without deleting experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . delete_experiment_group ( user , project_name , _group ) GroupManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment group `{}` was delete successfully" . format ( _group ) )
11361	def fix_title_capitalization ( title ) : if re . search ( "[A-Z]" , title ) and re . search ( "[a-z]" , title ) : return title word_list = re . split ( ' +' , title ) final = [ word_list [ 0 ] . capitalize ( ) ] for word in word_list [ 1 : ] : if word . upper ( ) in COMMON_ACRONYMS : final . append ( word . upper ( ) ) elif len ( word ) > 3 : final . append ( word . capitalize ( ) ) else : final . append ( word . lower ( ) ) return " " . join ( final )
4641	def reset_counter ( self ) : self . _cnt_retries = 0 for i in self . _url_counter : self . _url_counter [ i ] = 0
6907	def equatorial_to_galactic ( ra , decl , equinox = 'J2000' ) : radecl = SkyCoord ( ra = ra * u . degree , dec = decl * u . degree , equinox = equinox ) gl = radecl . galactic . l . degree gb = radecl . galactic . b . degree return gl , gb
2020	def ADDMOD ( self , a , b , c ) : try : result = Operators . ITEBV ( 256 , c == 0 , 0 , ( a + b ) % c ) except ZeroDivisionError : result = 0 return result
6333	def dist_abs ( self , src , tar ) : return self . _lev . dist_abs ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 9999 , 9999 ) )
6419	def dist ( self , src , tar , probs = None ) : if src == tar : return 0.0 if probs is None : self . _coder . train ( src + tar ) else : self . _coder . set_probs ( probs ) src_comp = self . _coder . encode ( src ) [ 1 ] tar_comp = self . _coder . encode ( tar ) [ 1 ] concat_comp = self . _coder . encode ( src + tar ) [ 1 ] concat_comp2 = self . _coder . encode ( tar + src ) [ 1 ] return ( min ( concat_comp , concat_comp2 ) - min ( src_comp , tar_comp ) ) / max ( src_comp , tar_comp )
11115	def save ( self ) : repoInfoPath = os . path . join ( self . __path , ".pyrepinfo" ) try : fdinfo = open ( repoInfoPath , 'wb' ) except Exception as e : raise Exception ( "unable to open repository info for saving (%s)" % e ) try : pickle . dump ( self , fdinfo , protocol = 2 ) except Exception as e : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) raise Exception ( "Unable to save repository info (%s)" % e ) finally : fdinfo . flush ( ) os . fsync ( fdinfo . fileno ( ) ) fdinfo . close ( ) repoTimePath = os . path . join ( self . __path , ".pyrepstate" ) try : self . __state = ( "%.6f" % time . time ( ) ) . encode ( ) with open ( repoTimePath , 'wb' ) as fdtime : fdtime . write ( self . __state ) fdtime . flush ( ) os . fsync ( fdtime . fileno ( ) ) except Exception as e : raise Exception ( "unable to open repository time stamp for saving (%s)" % e )
2751	def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( ** jsoned ) image . token = self . token images . append ( image ) return images
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
11135	def copyto ( self , new_abspath = None , new_dirpath = None , new_dirname = None , new_basename = None , new_fname = None , new_ext = None , overwrite = False , makedirs = False ) : self . assert_exists ( ) p = self . change ( new_abspath = new_abspath , new_dirpath = new_dirpath , new_dirname = new_dirname , new_basename = new_basename , new_fname = new_fname , new_ext = new_ext , ) if p . is_not_exist_or_allow_overwrite ( overwrite = overwrite ) : if self . abspath != p . abspath : try : shutil . copy ( self . abspath , p . abspath ) except IOError as e : if makedirs : os . makedirs ( p . parent . abspath ) shutil . copy ( self . abspath , p . abspath ) else : raise e return p
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , ** kwargs ) : data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , ** kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
10855	def sphere_analytical_gaussian_trim ( dr , a , alpha = 0.2765 , cut = 1.6 ) : m = np . abs ( dr ) <= cut rr = dr [ m ] t = - rr / ( alpha * np . sqrt ( 2 ) ) q = 0.5 * ( 1 + erf ( t ) ) - np . sqrt ( 0.5 / np . pi ) * ( alpha / ( rr + a + 1e-10 ) ) * np . exp ( - t * t ) ans = 0 * dr ans [ m ] = q ans [ dr > cut ] = 0 ans [ dr < - cut ] = 1 return ans
9088	async def _update_loop ( self ) -> None : await asyncio . sleep ( self . _update_interval ) while not self . _closed : await self . update ( ) await asyncio . sleep ( self . _update_interval )
2375	def report ( self , obj , message , linenum , char_offset = 0 ) : self . controller . report ( linenumber = linenum , filename = obj . path , severity = self . severity , message = message , rulename = self . __class__ . __name__ , char = char_offset )
7284	def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
10342	def overlay_data ( graph : BELGraph , data : Mapping [ BaseEntity , Any ] , label : Optional [ str ] = None , overwrite : bool = False , ) -> None : if label is None : label = WEIGHT for node , value in data . items ( ) : if node not in graph : log . debug ( '%s not in graph' , node ) continue if label in graph . nodes [ node ] and not overwrite : log . debug ( '%s already on %s' , label , node ) continue graph . nodes [ node ] [ label ] = value
2088	def delete ( self , pk = None , fail_on_missing = False , ** kwargs ) : if not pk : existing_data = self . _lookup ( fail_on_missing = fail_on_missing , ** kwargs ) if not existing_data : return { 'changed' : False } pk = existing_data [ 'id' ] url = '%s%s/' % ( self . endpoint , pk ) debug . log ( 'DELETE %s' % url , fg = 'blue' , bold = True ) try : client . delete ( url ) return { 'changed' : True } except exc . NotFound : if fail_on_missing : raise return { 'changed' : False }
638	def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]
7732	def get_join_info ( self ) : x = self . get_muc_child ( ) if not x : return None if not isinstance ( x , MucX ) : return None return x
5429	def _name_for_command ( command ) : r lines = command . splitlines ( ) for line in lines : line = line . strip ( ) if line and not line . startswith ( '#' ) and line != '\\' : return os . path . basename ( re . split ( r'\s' , line ) [ 0 ] ) return 'command'
12639	def move_to_folder ( self , folder_path , groupby_field_name = None ) : try : copy_groups_to_folder ( self . dicom_groups , folder_path , groupby_field_name ) except IOError as ioe : raise IOError ( 'Error moving dicom groups to {}.' . format ( folder_path ) ) from ioe
12642	def get_config_bool ( name ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) return cli_config . getboolean ( 'servicefabric' , name , False )
7977	def _post_connect ( self ) : if not self . initiator : if "plain" in self . auth_methods or "digest" in self . auth_methods : self . set_iq_get_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage1 ) self . set_iq_set_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage2 ) elif self . registration_callback : iq = Iq ( stanza_type = "get" ) iq . set_content ( Register ( ) ) self . set_response_handlers ( iq , self . registration_form_received , self . registration_error ) self . send ( iq ) return ClientStream . _post_connect ( self )
4784	def contains_ignoring_case ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) if isinstance ( self . val , str_types ) : if len ( items ) == 1 : if not isinstance ( items [ 0 ] , str_types ) : raise TypeError ( 'given arg must be a string' ) if items [ 0 ] . lower ( ) not in self . val . lower ( ) : self . _err ( 'Expected <%s> to case-insensitive contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) if i . lower ( ) not in self . val . lower ( ) : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) elif isinstance ( self . val , Iterable ) : missing = [ ] for i in items : if not isinstance ( i , str_types ) : raise TypeError ( 'given args must all be strings' ) found = False for v in self . val : if not isinstance ( v , str_types ) : raise TypeError ( 'val items must all be strings' ) if i . lower ( ) == v . lower ( ) : found = True break if not found : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to case-insensitive contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
7824	def finish ( self , data ) : if not self . _server_first_message : logger . debug ( "Got success too early" ) return Failure ( "bad-success" ) if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : ret = self . _final_challenge ( data ) if isinstance ( ret , Failure ) : return ret if self . _finished : return Success ( { "username" : self . username , "authzid" : self . authzid } ) else : logger . debug ( "Something went wrong when processing additional" " data with success?" ) return Failure ( "bad-success" )
13387	def make_upstream_request ( self ) : "Return request object for calling the upstream" url = self . upstream_url ( self . request . uri ) return tornado . httpclient . HTTPRequest ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
5200	def Select ( self , command , index ) : OutstationApplication . process_point_value ( 'Select' , command , index , None ) return opendnp3 . CommandStatus . SUCCESS
642	def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]
4091	def addSources ( self , * sources ) : self . _sources . extend ( sources ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB source(s): %s' % ', ' . join ( [ str ( x ) for x in self . _sources ] ) ) return self
3479	def _clip ( sid , prefix ) : return sid [ len ( prefix ) : ] if sid . startswith ( prefix ) else sid
4517	def fillScreen ( self , color = None ) : md . fill_rect ( self . set , 0 , 0 , self . width , self . height , color )
4673	def addPrivateKey ( self , wif ) : try : pub = self . publickey_from_wif ( wif ) except Exception : raise InvalidWifError ( "Invalid Key format!" ) if str ( pub ) in self . store : raise KeyAlreadyInStoreException ( "Key already in the store" ) self . store . add ( str ( wif ) , str ( pub ) )
6218	def get_bbox ( self , primitive ) : accessor = primitive . attributes . get ( 'POSITION' ) return accessor . min , accessor . max
11264	def stdout ( prev , endl = '\n' , thru = False ) : for i in prev : sys . stdout . write ( str ( i ) + endl ) if thru : yield i
4131	def get_docstring_and_rest ( filename ) : with open ( filename ) as f : content = f . read ( ) node = ast . parse ( content ) if not isinstance ( node , ast . Module ) : raise TypeError ( "This function only supports modules. " "You provided {0}" . format ( node . __class__ . __name__ ) ) if node . body and isinstance ( node . body [ 0 ] , ast . Expr ) and isinstance ( node . body [ 0 ] . value , ast . Str ) : docstring_node = node . body [ 0 ] docstring = docstring_node . value . s rest = content . split ( '\n' , docstring_node . lineno ) [ - 1 ] return docstring , rest else : raise ValueError ( ( 'Could not find docstring in file "{0}". ' 'A docstring is required by sphinx-gallery' ) . format ( filename ) )
10271	def is_unweighted_source ( graph : BELGraph , node : BaseEntity , key : str ) -> bool : return graph . in_degree ( node ) == 0 and key not in graph . nodes [ node ]
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6754	def all_other_enabled_satchels ( self ) : return dict ( ( name , satchel ) for name , satchel in self . all_satchels . items ( ) if name != self . name . upper ( ) and name . lower ( ) in map ( str . lower , self . genv . services ) )
2848	def _check ( self , command , * args ) : ret = command ( self . _ctx , * args ) logger . debug ( 'Called ftdi_{0} and got response {1}.' . format ( command . __name__ , ret ) ) if ret != 0 : raise RuntimeError ( 'ftdi_{0} failed with error {1}: {2}' . format ( command . __name__ , ret , ftdi . get_error_string ( self . _ctx ) ) )
8765	def opt_args_decorator ( func ) : @ wraps ( func ) def wrapped_dec ( * args , ** kwargs ) : if len ( args ) == 1 and len ( kwargs ) == 0 and callable ( args [ 0 ] ) : return func ( args [ 0 ] ) else : return lambda realf : func ( realf , * args , ** kwargs ) return wrapped_dec
143	def exterior_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , list ) : other = Polygon ( np . float32 ( other ) ) elif ia . is_np_array ( other ) : other = Polygon ( other ) else : assert isinstance ( other , Polygon ) other = other return self . to_line_string ( closed = True ) . coords_almost_equals ( other . to_line_string ( closed = True ) , max_distance = max_distance , points_per_edge = points_per_edge )
3262	def get_workspaces ( self , names = None ) : if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] data = self . get_xml ( "{}/workspaces.xml" . format ( self . service_url ) ) workspaces = [ ] workspaces . extend ( [ workspace_from_index ( self , node ) for node in data . findall ( "workspace" ) ] ) if workspaces and names : return ( [ ws for ws in workspaces if ws . name in names ] ) return workspaces
5664	def interpolate_shape_times ( shape_distances , shape_breaks , stop_times ) : shape_times = np . zeros ( len ( shape_distances ) ) shape_times [ : shape_breaks [ 0 ] ] = stop_times [ 0 ] for i in range ( len ( shape_breaks ) - 1 ) : cur_break = shape_breaks [ i ] cur_time = stop_times [ i ] next_break = shape_breaks [ i + 1 ] next_time = stop_times [ i + 1 ] if cur_break == next_break : shape_times [ cur_break ] = stop_times [ i ] else : cur_distances = shape_distances [ cur_break : next_break + 1 ] norm_distances = ( ( np . array ( cur_distances ) - float ( cur_distances [ 0 ] ) ) / float ( cur_distances [ - 1 ] - cur_distances [ 0 ] ) ) times = ( 1. - norm_distances ) * cur_time + norm_distances * next_time shape_times [ cur_break : next_break ] = times [ : - 1 ] shape_times [ shape_breaks [ - 1 ] : ] = stop_times [ - 1 ] return list ( shape_times )
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
12006	def _read_version ( self , data ) : version = ord ( data [ 0 ] ) if version not in self . VERSIONS : raise Exception ( 'Version not defined: %d' % version ) return version
5300	def with_setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend_colors = extend_colors ) yield colorful
1315	def DeleteLog ( ) -> None : if os . path . exists ( Logger . FileName ) : os . remove ( Logger . FileName )
4926	def transform_title ( self , content_metadata_item ) : title_with_locales = [ ] for locale in self . enterprise_configuration . get_locales ( ) : title_with_locales . append ( { 'locale' : locale , 'value' : content_metadata_item . get ( 'title' , '' ) } ) return title_with_locales
3872	def get_all ( self , include_archived = False ) : return [ conv for conv in self . _conv_dict . values ( ) if not conv . is_archived or include_archived ]
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
4594	def make_matrix_coord_map ( dx , dy , serpentine = True , offset = 0 , rotation = 0 , y_flip = False ) : result = [ ] for y in range ( dy ) : if not serpentine or y % 2 == 0 : result . append ( [ ( dx * y ) + x + offset for x in range ( dx ) ] ) else : result . append ( [ dx * ( y + 1 ) - 1 - x + offset for x in range ( dx ) ] ) result = rotate_and_flip ( result , rotation , y_flip ) return result
946	def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True
9486	def ensure_instruction ( instruction : int ) -> bytes : if PY36 : return instruction . to_bytes ( 2 , byteorder = "little" ) else : return instruction . to_bytes ( 1 , byteorder = "little" )
8482	def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env , default )
3957	def update_running_containers_from_spec ( compose_config , recreate_containers = True ) : write_composefile ( compose_config , constants . COMPOSEFILE_PATH ) compose_up ( constants . COMPOSEFILE_PATH , 'dusty' , recreate_containers = recreate_containers )
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) return invert
1799	def CMOVO ( cpu , dest , src ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) )
10195	def register_templates ( ) : event_templates = [ current_stats . _events_config [ e ] [ 'templates' ] for e in current_stats . _events_config ] aggregation_templates = [ current_stats . _aggregations_config [ a ] [ 'templates' ] for a in current_stats . _aggregations_config ] return event_templates + aggregation_templates
376	def imresize ( x , size = None , interp = 'bicubic' , mode = None ) : if size is None : size = [ 100 , 100 ] if x . shape [ - 1 ] == 1 : x = scipy . misc . imresize ( x [ : , : , 0 ] , size , interp = interp , mode = mode ) return x [ : , : , np . newaxis ] else : return scipy . misc . imresize ( x , size , interp = interp , mode = mode )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
11680	def disconnect ( self ) : logger . info ( u'Disconnecting' ) self . sock . shutdown ( socket . SHUT_RDWR ) self . sock . close ( ) self . state = DISCONNECTED
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
11269	def substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . substitute ( data )
7631	def get_dtypes ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) value_dtype = __get_dtype ( __NAMESPACE__ [ ns_key ] . get ( 'value' , { } ) ) confidence_dtype = __get_dtype ( __NAMESPACE__ [ ns_key ] . get ( 'confidence' , { } ) ) return value_dtype , confidence_dtype
12914	def write_json ( self , fh , pretty = True ) : sjson = json . JSONEncoder ( ) . encode ( self . json ( ) ) if pretty : json . dump ( json . loads ( sjson ) , fh , sort_keys = True , indent = 4 ) else : json . dump ( json . loads ( sjson ) , fh ) return
13246	async def _download_lsst_bibtex ( bibtex_names ) : blob_url_template = ( 'https://raw.githubusercontent.com/lsst/lsst-texmf/master/texmf/' 'bibtex/bib/{name}.bib' ) urls = [ blob_url_template . format ( name = name ) for name in bibtex_names ] tasks = [ ] async with ClientSession ( ) as session : for url in urls : task = asyncio . ensure_future ( _download_text ( url , session ) ) tasks . append ( task ) return await asyncio . gather ( * tasks )
10771	def contour ( self , level ) : if not isinstance ( level , numbers . Number ) : raise TypeError ( ( "'_level' must be of type 'numbers.Number' but is " "'{:s}'" ) . format ( type ( level ) ) ) vertices = self . _contour_generator . create_contour ( level ) return self . formatter ( level , vertices )
13602	def warn_message ( self , message , fh = None , prefix = "[warn]:" , suffix = "..." ) : msg = prefix + message + suffix fh = fh or sys . stdout if fh is sys . stdout : termcolor . cprint ( msg , color = "yellow" ) else : fh . write ( msg ) pass
4590	def serpentine_y ( x , y , matrix ) : if x % 2 : return x , matrix . rows - 1 - y return x , y
3072	def init_app ( self , app , scopes = None , client_secrets_file = None , client_id = None , client_secret = None , authorize_callback = None , storage = None , ** kwargs ) : self . app = app self . authorize_callback = authorize_callback self . flow_kwargs = kwargs if storage is None : storage = dictionary_storage . DictionaryStorage ( session , key = _CREDENTIALS_KEY ) self . storage = storage if scopes is None : scopes = app . config . get ( 'GOOGLE_OAUTH2_SCOPES' , _DEFAULT_SCOPES ) self . scopes = scopes self . _load_config ( client_secrets_file , client_id , client_secret ) app . register_blueprint ( self . _create_blueprint ( ) )
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( ** values )
2525	def get_reviewer ( self , r_term ) : reviewer_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'reviewer' ] , None ) ) ) if len ( reviewer_list ) != 1 : self . error = True msg = 'Review must have exactly one reviewer' self . logger . log ( msg ) return try : return self . builder . create_entity ( self . doc , six . text_type ( reviewer_list [ 0 ] [ 2 ] ) ) except SPDXValueError : self . value_error ( 'REVIEWER_VALUE' , reviewer_list [ 0 ] [ 2 ] )
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
12010	def getTableOfContents ( self ) : self . directory_size = self . getDirectorySize ( ) if self . directory_size > 65536 : self . directory_size += 2 self . requestContentDirectory ( ) directory_start = unpack ( "i" , self . raw_bytes [ self . directory_end + 16 : self . directory_end + 20 ] ) [ 0 ] self . raw_bytes = self . raw_bytes current_start = directory_start - self . start filestart = 0 compressedsize = 0 tableOfContents = [ ] try : while True : zip_n = unpack ( "H" , self . raw_bytes [ current_start + 28 : current_start + 28 + 2 ] ) [ 0 ] zip_m = unpack ( "H" , self . raw_bytes [ current_start + 30 : current_start + 30 + 2 ] ) [ 0 ] zip_k = unpack ( "H" , self . raw_bytes [ current_start + 32 : current_start + 32 + 2 ] ) [ 0 ] filename = self . raw_bytes [ current_start + 46 : current_start + 46 + zip_n ] filestart = unpack ( "I" , self . raw_bytes [ current_start + 42 : current_start + 42 + 4 ] ) [ 0 ] compressedsize = unpack ( "I" , self . raw_bytes [ current_start + 20 : current_start + 20 + 4 ] ) [ 0 ] uncompressedsize = unpack ( "I" , self . raw_bytes [ current_start + 24 : current_start + 24 + 4 ] ) [ 0 ] tableItem = { 'filename' : filename , 'compressedsize' : compressedsize , 'uncompressedsize' : uncompressedsize , 'filestart' : filestart } tableOfContents . append ( tableItem ) current_start = current_start + 46 + zip_n + zip_m + zip_k except : pass self . tableOfContents = tableOfContents return tableOfContents
13193	def geom_to_xml_element ( geom ) : if geom . srs . srid != 4326 : raise NotImplementedError ( "Only WGS 84 lat/long geometries (SRID 4326) are supported." ) return geojson_to_gml ( json . loads ( geom . geojson ) )
2120	def associate_success_node ( self , parent , child = None , ** kwargs ) : return self . _assoc_or_create ( 'success' , parent , child , ** kwargs )
13034	def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
3773	def set_user_methods ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . T_cached = None
11774	def NeuralNetLearner ( dataset , sizes ) : activations = map ( lambda n : [ 0.0 for i in range ( n ) ] , sizes ) weights = [ ] def predict ( example ) : unimplemented ( ) return predict
559	def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True
5183	def node ( self , name ) : nodes = self . nodes ( path = name ) return next ( node for node in nodes )
7198	def describe_images ( self , idaho_image_results ) : results = idaho_image_results [ 'results' ] results = [ r for r in results if 'IDAHOImage' in r [ 'type' ] ] self . logger . debug ( 'Describing %s IDAHO images.' % len ( results ) ) catids = set ( [ r [ 'properties' ] [ 'catalogID' ] for r in results ] ) description = { } for catid in catids : description [ catid ] = { } description [ catid ] [ 'parts' ] = { } images = [ r for r in results if r [ 'properties' ] [ 'catalogID' ] == catid ] for image in images : description [ catid ] [ 'sensorPlatformName' ] = image [ 'properties' ] [ 'sensorPlatformName' ] part = int ( image [ 'properties' ] [ 'vendorDatasetIdentifier' ] . split ( ':' ) [ 1 ] [ - 3 : ] ) color = image [ 'properties' ] [ 'colorInterpretation' ] bucket = image [ 'properties' ] [ 'tileBucketName' ] identifier = image [ 'identifier' ] boundstr = image [ 'properties' ] [ 'footprintWkt' ] try : description [ catid ] [ 'parts' ] [ part ] except : description [ catid ] [ 'parts' ] [ part ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] = { } description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'id' ] = identifier description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'bucket' ] = bucket description [ catid ] [ 'parts' ] [ part ] [ color ] [ 'boundstr' ] = boundstr return description
12790	def create_from_settings ( settings ) : return Connection ( settings [ "url" ] , settings [ "base_url" ] , settings [ "user" ] , settings [ "password" ] , authorizations = settings [ "authorizations" ] , debug = settings [ "debug" ] )
11924	def render_to ( path , template , ** data ) : try : renderer . render_to ( path , template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
7213	def get_proj ( prj_code ) : if prj_code in CUSTOM_PRJ : proj = pyproj . Proj ( CUSTOM_PRJ [ prj_code ] ) else : proj = pyproj . Proj ( init = prj_code ) return proj
12699	def _parse_data_fields ( self , fields , tag_id = "tag" , sub_id = "code" ) : for field in fields : params = field . params if tag_id not in params : continue field_repr = OrderedDict ( [ [ self . i1_name , params . get ( self . i1_name , " " ) ] , [ self . i2_name , params . get ( self . i2_name , " " ) ] , ] ) for subfield in field . find ( "subfield" ) : if sub_id not in subfield . params : continue content = MARCSubrecord ( val = subfield . getContent ( ) . strip ( ) , i1 = field_repr [ self . i1_name ] , i2 = field_repr [ self . i2_name ] , other_subfields = field_repr ) code = subfield . params [ sub_id ] if code in field_repr : field_repr [ code ] . append ( content ) else : field_repr [ code ] = [ content ] tag = params [ tag_id ] if tag in self . datafields : self . datafields [ tag ] . append ( field_repr ) else : self . datafields [ tag ] = [ field_repr ]
2619	def create_session ( self ) : session = None if self . key_file is not None : credfile = os . path . expandvars ( os . path . expanduser ( self . key_file ) ) try : with open ( credfile , 'r' ) as f : creds = json . load ( f ) except json . JSONDecodeError as e : logger . error ( "EC2Provider '{}': json decode error in credential file {}" . format ( self . label , credfile ) ) raise e except Exception as e : logger . debug ( "EC2Provider '{0}' caught exception while reading credential file: {1}" . format ( self . label , credfile ) ) raise e logger . debug ( "EC2Provider '{}': Using credential file to create session" . format ( self . label ) ) session = boto3 . session . Session ( region_name = self . region , ** creds ) elif self . profile is not None : logger . debug ( "EC2Provider '{}': Using profile name to create session" . format ( self . label ) ) session = boto3 . session . Session ( profile_name = self . profile , region_name = self . region ) else : logger . debug ( "EC2Provider '{}': Using environment variables to create session" . format ( self . label ) ) session = boto3 . session . Session ( region_name = self . region ) return session
8252	def image_to_rgb ( self , path , n = 10 ) : from PIL import Image img = Image . open ( path ) p = img . getdata ( ) f = lambda p : choice ( p ) for i in _range ( n ) : rgba = f ( p ) rgba = _list ( rgba ) if len ( rgba ) == 3 : rgba . append ( 255 ) r , g , b , a = [ v / 255.0 for v in rgba ] clr = color ( r , g , b , a , mode = "rgb" ) self . append ( clr )
3425	def get_metabolite_compartments ( self ) : warn ( 'use Model.compartments instead' , DeprecationWarning ) return { met . compartment for met in self . metabolites if met . compartment is not None }
3351	def get_by_any ( self , iterable ) : def get_item ( item ) : if isinstance ( item , int ) : return self [ item ] elif isinstance ( item , string_types ) : return self . get_by_id ( item ) elif item in self : return item else : raise TypeError ( "item in iterable cannot be '%s'" % type ( item ) ) if not isinstance ( iterable , list ) : iterable = [ iterable ] return [ get_item ( item ) for item in iterable ]
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
1837	def JGE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , ( cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC )
12964	def all ( self , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultiple ( matchedKeys , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
8953	def load ( ) : cfg = Bunch ( DEFAULTS ) cfg . project_root = get_project_root ( ) if not cfg . project_root : raise RuntimeError ( "No tasks module is imported, cannot determine project root" ) cfg . rootjoin = lambda * names : os . path . join ( cfg . project_root , * names ) cfg . srcjoin = lambda * names : cfg . rootjoin ( cfg . srcdir , * names ) cfg . testjoin = lambda * names : cfg . rootjoin ( cfg . testdir , * names ) cfg . cwd = os . getcwd ( ) os . chdir ( cfg . project_root ) if cfg . project_root not in sys . path : sys . path . append ( cfg . project_root ) try : from setup import project except ImportError : from setup import setup_args as project cfg . project = Bunch ( project ) return cfg
10389	def calculate_average_scores_on_subgraphs ( subgraphs : Mapping [ H , BELGraph ] , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , tqdm_kwargs : Optional [ Mapping [ str , Any ] ] = None , ) -> Mapping [ H , Tuple [ float , float , float , float , int , int ] ] : results = { } log . info ( 'calculating results for %d candidate mechanisms using %d permutations' , len ( subgraphs ) , runs ) it = subgraphs . items ( ) if use_tqdm : _tqdm_kwargs = dict ( total = len ( subgraphs ) , desc = 'Candidate mechanisms' ) if tqdm_kwargs : _tqdm_kwargs . update ( tqdm_kwargs ) it = tqdm ( it , ** _tqdm_kwargs ) for node , subgraph in it : number_first_neighbors = subgraph . in_degree ( node ) number_first_neighbors = 0 if isinstance ( number_first_neighbors , dict ) else number_first_neighbors mechanism_size = subgraph . number_of_nodes ( ) runners = workflow ( subgraph , node , key = key , tag = tag , default_score = default_score , runs = runs ) scores = [ runner . get_final_score ( ) for runner in runners ] if 0 == len ( scores ) : results [ node ] = ( None , None , None , None , number_first_neighbors , mechanism_size , ) continue scores = np . array ( scores ) average_score = np . average ( scores ) score_std = np . std ( scores ) med_score = np . median ( scores ) chi_2_stat , norm_p = stats . normaltest ( scores ) results [ node ] = ( average_score , score_std , norm_p , med_score , number_first_neighbors , mechanism_size , ) return results
2350	def wait_for_page_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_page_to_load ( page = self ) return self
11182	def release ( self ) : self . _lock . release ( ) with self . _stat_lock : self . _locked = False self . _last_released = datetime . now ( )
5759	def get_jenkins_job_urls ( rosdistro_name , jenkins_url , release_build_name , targets ) : urls = { } for target in targets : view_name = get_release_view_name ( rosdistro_name , release_build_name , target . os_name , target . os_code_name , target . arch ) base_url = jenkins_url + '/view/%s/job/%s__{pkg}__' % ( view_name , view_name ) if target . arch == 'source' : urls [ target ] = base_url + '%s_%s__source' % ( target . os_name , target . os_code_name ) else : urls [ target ] = base_url + '%s_%s_%s__binary' % ( target . os_name , target . os_code_name , target . arch ) return urls
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
7324	def create_sample_input_files ( template_filename , database_filename , config_filename ) : print ( "Creating sample template email {}" . format ( template_filename ) ) if os . path . exists ( template_filename ) : print ( "Error: file exists: " + template_filename ) sys . exit ( 1 ) with io . open ( template_filename , "w" ) as template_file : template_file . write ( u"TO: {{email}}\n" u"SUBJECT: Testing mailmerge\n" u"FROM: My Self <myself@mydomain.com>\n" u"\n" u"Hi, {{name}},\n" u"\n" u"Your number is {{number}}.\n" ) print ( "Creating sample database {}" . format ( database_filename ) ) if os . path . exists ( database_filename ) : print ( "Error: file exists: " + database_filename ) sys . exit ( 1 ) with io . open ( database_filename , "w" ) as database_file : database_file . write ( u'email,name,number\n' u'myself@mydomain.com,"Myself",17\n' u'bob@bobdomain.com,"Bob",42\n' ) print ( "Creating sample config file {}" . format ( config_filename ) ) if os . path . exists ( config_filename ) : print ( "Error: file exists: " + config_filename ) sys . exit ( 1 ) with io . open ( config_filename , "w" ) as config_file : config_file . write ( u"# Example: GMail\n" u"[smtp_server]\n" u"host = smtp.gmail.com\n" u"port = 465\n" u"security = SSL/TLS\n" u"username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: Wide open\n" u"# [smtp_server]\n" u"# host = open-smtp.example.com\n" u"# port = 25\n" u"# security = Never\n" u"# username = None\n" u"#\n" u"# Example: University of Michigan\n" u"# [smtp_server]\n" u"# host = smtp.mail.umich.edu\n" u"# port = 465\n" u"# security = SSL/TLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with STARTTLS security\n" u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = STARTTLS\n" u"# username = YOUR_USERNAME_HERE\n" u"#\n" u"# Example: University of Michigan EECS Dept., with no encryption\n" u"# [smtp_server]\n" u"# host = newman.eecs.umich.edu\n" u"# port = 25\n" u"# security = Never\n" u"# username = YOUR_USERNAME_HERE\n" ) print ( "Edit these files, and then run mailmerge again" )
12361	def send_request ( self , kind , url_components , ** kwargs ) : return self . api . send_request ( kind , self . resource_path , url_components , ** kwargs )
12423	def load ( fp , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : converter = None output = cls ( ) arraykeys = set ( ) for line in fp : if converter is None : if isinstance ( line , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator key , value = line . strip ( ) . split ( separator , 1 ) keyparts = key . split ( index_separator ) try : index = int ( keyparts [ - 1 ] ) endwithint = True except ValueError : endwithint = False if len ( keyparts ) > 1 and endwithint : basekey = key . rsplit ( index_separator , 1 ) [ 0 ] if basekey not in arraykeys : arraykeys . add ( basekey ) if basekey in output : if not isinstance ( output [ basekey ] , dict ) : output [ basekey ] = { - 1 : output [ basekey ] } else : output [ basekey ] = { } output [ basekey ] [ index ] = value else : if key in output and isinstance ( output [ key ] , dict ) : output [ key ] [ - 1 ] = value else : output [ key ] = value for key in arraykeys : output [ key ] = list_cls ( pair [ 1 ] for pair in sorted ( six . iteritems ( output [ key ] ) ) ) return output
6672	def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
3277	def handle_move ( self , dest_path ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
11406	def records_identical ( rec1 , rec2 , skip_005 = True , ignore_field_order = False , ignore_subfield_order = False , ignore_duplicate_subfields = False , ignore_duplicate_controlfields = False ) : rec1_keys = set ( rec1 . keys ( ) ) rec2_keys = set ( rec2 . keys ( ) ) if skip_005 : rec1_keys . discard ( "005" ) rec2_keys . discard ( "005" ) if rec1_keys != rec2_keys : return False for key in rec1_keys : if ignore_duplicate_controlfields and key . startswith ( '00' ) : if set ( field [ 3 ] for field in rec1 [ key ] ) != set ( field [ 3 ] for field in rec2 [ key ] ) : return False continue rec1_fields = rec1 [ key ] rec2_fields = rec2 [ key ] if len ( rec1_fields ) != len ( rec2_fields ) : return False if ignore_field_order : rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 3 ] , elem [ 0 ] ) ) else : rec1_fields = sorted ( rec1_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) rec2_fields = sorted ( rec2_fields , key = lambda elem : ( elem [ 1 ] , elem [ 2 ] , elem [ 4 ] , elem [ 3 ] , elem [ 0 ] ) ) for field1 , field2 in zip ( rec1_fields , rec2_fields ) : if ignore_duplicate_subfields : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or set ( field1 [ 0 ] ) != set ( field2 [ 0 ] ) : return False elif ignore_subfield_order : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] or sorted ( field1 [ 0 ] ) != sorted ( field2 [ 0 ] ) : return False elif field1 [ : 4 ] != field2 [ : 4 ] : return False return True
8127	def search_images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_IMAGES return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
4722	def trun_enter ( trun ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter" ) trun [ "stamp" ] [ "begin" ] = int ( time . time ( ) ) rcode = 0 for hook in trun [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:trun::enter { rcode: %r }" % rcode , rcode ) return rcode
6384	def mean_pairwise_similarity ( collection , metric = sim , mean_func = hmean , symmetric = False ) : if not callable ( mean_func ) : raise ValueError ( 'mean_func must be a function' ) if not callable ( metric ) : raise ValueError ( 'metric must be a function' ) if hasattr ( collection , 'split' ) : collection = collection . split ( ) if not hasattr ( collection , '__iter__' ) : raise ValueError ( 'collection is neither a string nor iterable type' ) elif len ( collection ) < 2 : raise ValueError ( 'collection has fewer than two members' ) collection = list ( collection ) pairwise_values = [ ] for i in range ( len ( collection ) ) : for j in range ( i + 1 , len ( collection ) ) : pairwise_values . append ( metric ( collection [ i ] , collection [ j ] ) ) if symmetric : pairwise_values . append ( metric ( collection [ j ] , collection [ i ] ) ) return mean_func ( pairwise_values )
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
1244	def update_batch ( self , loss_per_instance ) : if self . batch_indices is None : raise TensorForceError ( "Need to call get_batch before each update_batch call." ) for index , loss in zip ( self . batch_indices , loss_per_instance ) : new_priority = ( np . abs ( loss ) + self . prioritization_constant ) ** self . prioritization_weight self . observations . _move ( index , new_priority ) self . none_priority_index += 1
9508	def union_fill_gap ( self , i ) : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) )
6418	def sim_editex ( src , tar , cost = ( 0 , 1 , 2 ) , local = False ) : return Editex ( ) . sim ( src , tar , cost , local )
1403	def getTopologyInfo ( self , topologyName , cluster , role , environ ) : for ( topology_name , _ ) , topologyInfo in self . topologyInfos . items ( ) : executionState = topologyInfo [ "execution_state" ] if ( topologyName == topology_name and cluster == executionState [ "cluster" ] and environ == executionState [ "environ" ] ) : if not role or executionState . get ( "role" ) == role : return topologyInfo if role is not None : Log . info ( "Could not find topology info for topology: %s," "cluster: %s, role: %s, and environ: %s" , topologyName , cluster , role , environ ) else : Log . info ( "Could not find topology info for topology: %s," "cluster: %s and environ: %s" , topologyName , cluster , environ ) raise Exception ( "No topology found" )
12526	def condor_submit ( cmd ) : is_running = subprocess . call ( 'condor_status' , shell = True ) == 0 if not is_running : raise CalledProcessError ( 'HTCondor is not running.' ) sub_cmd = 'condor_qsub -shell n -b y -r y -N ' + cmd . split ( ) [ 0 ] + ' -m n' log . info ( 'Calling: ' + sub_cmd ) return subprocess . call ( sub_cmd + ' ' + cmd , shell = True )
4801	def is_named ( self , filename ) : self . is_file ( ) if not isinstance ( filename , str_types ) : raise TypeError ( 'given filename arg must be a path' ) val_filename = os . path . basename ( os . path . abspath ( self . val ) ) if val_filename != filename : self . _err ( 'Expected filename <%s> to be equal to <%s>, but was not.' % ( val_filename , filename ) ) return self
5403	def _get_localization_env ( self , inputs , user_project ) : non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
1732	def is_empty_object ( n , last ) : if n . strip ( ) : return False last = last . strip ( ) markers = { ')' , ';' , } if not last or last [ - 1 ] in markers : return False return True
10617	def get_compound_mass ( self , compound ) : if compound in self . material . compounds : return self . _compound_masses [ self . material . get_compound_index ( compound ) ] else : return 0.0
8519	def sha1 ( self ) : with open ( self . path , 'rb' ) as f : return hashlib . sha1 ( f . read ( ) ) . hexdigest ( )
9034	def instruction_in_grid ( self , instruction ) : row_position = self . _rows_in_grid [ instruction . row ] . xy x = instruction . index_of_first_consumed_mesh_in_row position = Point ( row_position . x + x , row_position . y ) return InstructionInGrid ( instruction , position )
3594	def search ( self , query ) : if self . authSubToken is None : raise LoginError ( "You need to login before executing any request" ) path = SEARCH_URL + "?c=3&q={}" . format ( requests . utils . quote ( query ) ) self . toc ( ) data = self . executeRequestApi2 ( path ) if utils . hasPrefetch ( data ) : response = data . preFetch [ 0 ] . response else : response = data resIterator = response . payload . listResponse . doc return list ( map ( utils . parseProtobufObj , resIterator ) )
7637	def smkdirs ( dpath , mode = 0o777 ) : if not os . path . exists ( dpath ) : os . makedirs ( dpath , mode = mode )
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
10124	def flip_x ( self , center = None ) : if center is None : self . poly . flip ( ) else : self . poly . flip ( center [ 0 ] )
4847	def _load_data ( self , resource , detail_resource = None , resource_id = None , querystring = None , traverse_pagination = False , default = DEFAULT_VALUE_SAFEGUARD , ) : default_val = default if default != self . DEFAULT_VALUE_SAFEGUARD else { } querystring = querystring if querystring else { } cache_key = utils . get_cache_key ( resource = resource , querystring = querystring , traverse_pagination = traverse_pagination , resource_id = resource_id ) response = cache . get ( cache_key ) if not response : endpoint = getattr ( self . client , resource ) ( resource_id ) endpoint = getattr ( endpoint , detail_resource ) if detail_resource else endpoint response = endpoint . get ( ** querystring ) if traverse_pagination : results = utils . traverse_pagination ( response , endpoint ) response = { 'count' : len ( results ) , 'next' : 'None' , 'previous' : 'None' , 'results' : results , } if response : cache . set ( cache_key , response , settings . ENTERPRISE_API_CACHE_TIMEOUT ) return response or default_val
4398	def adsSyncDelDeviceNotificationReqEx ( port , adr , notification_handle , user_handle ) : adsSyncDelDeviceNotificationReqFct = _adsDLL . AdsSyncDelDeviceNotificationReqEx pAmsAddr = ctypes . pointer ( adr . amsAddrStruct ( ) ) nHNotification = ctypes . c_ulong ( notification_handle ) err_code = adsSyncDelDeviceNotificationReqFct ( port , pAmsAddr , nHNotification ) callback_store . pop ( notification_handle , None ) if err_code : raise ADSError ( err_code ) adsSyncWriteReqEx ( port , adr , ADSIGRP_SYM_RELEASEHND , 0 , user_handle , PLCTYPE_UDINT )
6073	def mass_within_circle_in_units ( self , radius , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . mass_within_circle_in_units ( radius = radius , unit_mass = unit_mass , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
9376	def extract_diff_sla_from_config_file ( obj , options_file ) : rule_strings = { } config_obj = ConfigParser . ConfigParser ( ) config_obj . optionxform = str config_obj . read ( options_file ) for section in config_obj . sections ( ) : rule_strings , kwargs = get_rule_strings ( config_obj , section ) for ( key , val ) in rule_strings . iteritems ( ) : set_sla ( obj , section , key , val )
9544	def add_value_check ( self , field_name , value_check , code = VALUE_CHECK_FAILED , message = MESSAGES [ VALUE_CHECK_FAILED ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_check ) , 'value check must be a callable function' t = field_name , value_check , code , message , modulus self . _value_checks . append ( t )
7752	def process_presence ( self , stanza ) : stanza_type = stanza . stanza_type return self . __try_handlers ( self . _presence_handlers , stanza , stanza_type )
6076	def einstein_mass_in_units ( self , unit_mass = 'angular' , critical_surface_density = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_mass_in_units ( unit_mass = unit_mass , critical_surface_density = critical_surface_density ) , self . mass_profiles ) ) else : return None
9380	def get_standardized_timestamp ( timestamp , ts_format ) : if not timestamp : return None if timestamp == 'now' : timestamp = str ( datetime . datetime . now ( ) ) if not ts_format : ts_format = detect_timestamp_format ( timestamp ) try : if ts_format == 'unknown' : logger . error ( 'Unable to determine timestamp format for : %s' , timestamp ) return - 1 elif ts_format == 'epoch' : ts = int ( timestamp ) * 1000 elif ts_format == 'epoch_ms' : ts = timestamp elif ts_format == 'epoch_fraction' : ts = int ( timestamp [ : 10 ] ) * 1000 + int ( timestamp [ 11 : ] ) elif ts_format in ( '%H:%M:%S' , '%H:%M:%S.%f' ) : date_today = str ( datetime . date . today ( ) ) dt_obj = datetime . datetime . strptime ( date_today + ' ' + timestamp , '%Y-%m-%d ' + ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 else : dt_obj = datetime . datetime . strptime ( timestamp , ts_format ) ts = calendar . timegm ( dt_obj . utctimetuple ( ) ) * 1000 + dt_obj . microsecond / 1000 except ValueError : return - 1 return str ( ts )
8356	def _toUnicode ( self , data , encoding ) : if ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xfe\xff' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16be' data = data [ 2 : ] elif ( len ( data ) >= 4 ) and ( data [ : 2 ] == '\xff\xfe' ) and ( data [ 2 : 4 ] != '\x00\x00' ) : encoding = 'utf-16le' data = data [ 2 : ] elif data [ : 3 ] == '\xef\xbb\xbf' : encoding = 'utf-8' data = data [ 3 : ] elif data [ : 4 ] == '\x00\x00\xfe\xff' : encoding = 'utf-32be' data = data [ 4 : ] elif data [ : 4 ] == '\xff\xfe\x00\x00' : encoding = 'utf-32le' data = data [ 4 : ] newdata = unicode ( data , encoding ) return newdata
4454	def group_by ( self , fields , * reducers ) : group = Group ( fields , reducers ) self . _groups . append ( group ) return self
6061	def convolve_mapping_matrix ( self , mapping_matrix ) : return self . convolve_matrix_jit ( mapping_matrix , self . image_frame_indexes , self . image_frame_psfs , self . image_frame_lengths )
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
11330	def get_record ( self , record ) : self . document = record rec = create_record ( ) language = self . _get_language ( ) if language and language != 'en' : record_add_field ( rec , '041' , subfields = [ ( 'a' , language ) ] ) publisher = self . _get_publisher ( ) date = self . _get_date ( ) if publisher and date : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) , ( 'c' , date ) ] ) elif publisher : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) ] ) elif date : record_add_field ( rec , '260' , subfields = [ ( 'c' , date ) ] ) title = self . _get_title ( ) if title : record_add_field ( rec , '245' , subfields = [ ( 'a' , title ) ] ) record_copyright = self . _get_copyright ( ) if record_copyright : record_add_field ( rec , '540' , subfields = [ ( 'a' , record_copyright ) ] ) subject = self . _get_subject ( ) if subject : record_add_field ( rec , '650' , ind1 = '1' , ind2 = '7' , subfields = [ ( 'a' , subject ) , ( '2' , 'PoS' ) ] ) authors = self . _get_authors ( ) first_author = True for author in authors : subfields = [ ( 'a' , author [ 0 ] ) ] for affiliation in author [ 1 ] : subfields . append ( ( 'v' , affiliation ) ) if first_author : record_add_field ( rec , '100' , subfields = subfields ) first_author = False else : record_add_field ( rec , '700' , subfields = subfields ) identifier = self . get_identifier ( ) conference = identifier . split ( ':' ) [ 2 ] conference = conference . split ( '/' ) [ 0 ] contribution = identifier . split ( ':' ) [ 2 ] contribution = contribution . split ( '/' ) [ 1 ] record_add_field ( rec , '773' , subfields = [ ( 'p' , 'PoS' ) , ( 'v' , conference . replace ( ' ' , '' ) ) , ( 'c' , contribution ) , ( 'y' , date [ : 4 ] ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'ConferencePaper' ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'HEP' ) ] ) return rec
7462	def encode ( self , obj ) : def hint_tuples ( item ) : if isinstance ( item , tuple ) : return { '__tuple__' : True , 'items' : item } if isinstance ( item , list ) : return [ hint_tuples ( e ) for e in item ] if isinstance ( item , dict ) : return { key : hint_tuples ( val ) for key , val in item . iteritems ( ) } else : return item return super ( Encoder , self ) . encode ( hint_tuples ( obj ) )
3544	def compute_exit_code ( config , exception = None ) : code = 0 if exception is not None : code = code | 1 if config . surviving_mutants > 0 : code = code | 2 if config . surviving_mutants_timeout > 0 : code = code | 4 if config . suspicious_mutants > 0 : code = code | 8 return code
13852	def age ( self ) : if self . rounds == 1 : self . do_run = False elif self . rounds > 1 : self . rounds -= 1
9062	def fix ( self , param ) : if param == "delta" : super ( ) . _fix ( "logistic" ) else : self . _fix [ param ] = True
12894	def set_power ( self , value = False ) : power = ( yield from self . handle_set ( self . API . get ( 'power' ) , int ( value ) ) ) return bool ( power )
10608	def _create_element_list ( self ) : element_set = stoich . elements ( self . compounds ) return sorted ( list ( element_set ) )
2505	def get_extr_license_text ( self , extr_lic ) : text_tripples = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'extractedText' ] , None ) ) ) if not text_tripples : self . error = True msg = 'Extracted license must have extractedText property' self . logger . log ( msg ) return if len ( text_tripples ) > 1 : self . more_than_one_error ( 'extracted license text' ) return text_tripple = text_tripples [ 0 ] _s , _p , text = text_tripple return text
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : depth } return modeldict
11878	def getProcessOwner ( pid ) : try : ownerUid = os . stat ( '/proc/' + str ( pid ) ) . st_uid except : return None try : ownerName = pwd . getpwuid ( ownerUid ) . pw_name except : ownerName = None return { 'uid' : ownerUid , 'name' : ownerName }
6309	def draw ( self , time , frametime , target ) : for effect in self . effects : value = effect . rocket_timeline_track . time_value ( time ) if value > 0.5 : effect . draw ( time , frametime , target )
13023	def select ( self , sql_string , cols , * args , ** kwargs ) : working_columns = None if kwargs . get ( 'columns' ) is not None : working_columns = kwargs . pop ( 'columns' ) query = self . _assemble_select ( sql_string , cols , * args , * kwargs ) return self . _execute ( query , working_columns = working_columns )
9095	def _add_annotation_to_graph ( self , graph : BELGraph ) -> None : if 'bio2bel' not in graph . annotation_list : graph . annotation_list [ 'bio2bel' ] = set ( ) graph . annotation_list [ 'bio2bel' ] . add ( self . module_name )
10444	def getobjectproperty ( self , window_name , object_name , prop ) : try : obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } obj_info = self . _get_object_map ( window_name , object_name , wait_for_object = False ) if obj_info and prop != "obj" and prop in obj_info : if prop == "class" : return ldtp_class_type . get ( obj_info [ prop ] , obj_info [ prop ] ) else : return obj_info [ prop ] raise LdtpServerException ( 'Unknown property "%s" in %s' % ( prop , object_name ) )
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
3226	def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
3607	def put ( self , url , name , data , params = None , headers = None , connection = None ) : assert name , 'Snapshot name must be specified' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) return make_put_request ( endpoint , data , params , headers , connection = connection )
10790	def load_wisdom ( wisdomfile ) : if wisdomfile is None : return try : pyfftw . import_wisdom ( pickle . load ( open ( wisdomfile , 'rb' ) ) ) except ( IOError , TypeError ) as e : log . warn ( "No wisdom present, generating some at %r" % wisdomfile ) save_wisdom ( wisdomfile )
1113	def _split_line ( self , data_list , line_num , text ) : if not line_num : data_list . append ( ( line_num , text ) ) return size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 line1 = text [ : i ] line2 = text [ i : ] if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 data_list . append ( ( line_num , line1 ) ) self . _split_line ( data_list , '>' , line2 )
13391	def format_uuid ( uuid , max_length = 10 ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( uuid ) > max_length : uuid = "{}..." . format ( uuid [ 0 : max_length - 3 ] ) return uuid
87	def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )
3322	def clear ( self ) : self . _lock . acquire_write ( ) try : was_closed = self . _dict is None if was_closed : self . open ( ) if len ( self . _dict ) : self . _dict . clear ( ) self . _dict . sync ( ) if was_closed : self . close ( ) finally : self . _lock . release ( )
4563	def to_type_constructor ( value , python_path = None ) : if not value : return value if callable ( value ) : return { 'datatype' : value } value = to_type ( value ) typename = value . get ( 'typename' ) if typename : r = aliases . resolve ( typename ) try : value [ 'datatype' ] = importer . import_symbol ( r , python_path = python_path ) del value [ 'typename' ] except Exception as e : value [ '_exception' ] = e return value
5476	def get_zones ( input_list ) : if not input_list : return [ ] output_list = [ ] for zone in input_list : if zone . endswith ( '*' ) : prefix = zone [ : - 1 ] output_list . extend ( [ z for z in _ZONES if z . startswith ( prefix ) ] ) else : output_list . append ( zone ) return output_list
13768	def collect_files ( self ) : self . files = [ ] for bundle in self . bundles : bundle . init_build ( self , self . builder ) bundle_files = bundle . prepare ( ) self . files . extend ( bundle_files ) return self
11646	def transform ( self , X ) : n = self . flip_ . shape [ 0 ] if X . ndim != 2 or X . shape [ 1 ] != n : msg = "X should have {} columns, the number of samples at fit time" raise TypeError ( msg . format ( self . flip_ . shape [ 0 ] ) ) return np . dot ( X , self . flip_ )
5995	def plot_mask ( mask , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if mask is not None : plt . gca ( ) edge_pixels = mask . masked_grid_index_to_pixel [ mask . edge_pixels ] + 0.5 if zoom_offset_pixels is not None : edge_pixels -= zoom_offset_pixels edge_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = edge_pixels ) edge_units = convert_grid_units ( array = mask , grid_arcsec = edge_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = edge_units [ : , 0 ] , x = edge_units [ : , 1 ] , s = pointsize , c = 'k' )
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
12064	def lazygo ( watchFolder = '../abfs/' , reAnalyze = False , rebuildSite = False , keepGoing = True , matching = False ) : abfsKnown = [ ] while True : print ( ) pagesNeeded = [ ] for fname in glob . glob ( watchFolder + "/*.abf" ) : ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if not fname in abfsKnown : if os . path . exists ( fname . replace ( ".abf" , ".rsv" ) ) : continue if matching and not matching in fname : continue abfsKnown . append ( fname ) if os . path . exists ( os . path . dirname ( fname ) + "/swhlab4/" + os . path . basename ( fname ) . replace ( ".abf" , "_info.pkl" ) ) and reAnalyze == False : print ( "already analyzed" , os . path . basename ( fname ) ) if rebuildSite : pagesNeeded . append ( ID ) else : handleNewABF ( fname ) pagesNeeded . append ( ID ) if len ( pagesNeeded ) : print ( " -- rebuilding index page" ) indexing . genIndex ( os . path . dirname ( fname ) , forceIDs = pagesNeeded ) if not keepGoing : return for i in range ( 50 ) : print ( '.' , end = '' ) time . sleep ( .2 )
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
7041	def list_lc_collections ( lcc_server ) : url = '%s/api/collections' % lcc_server try : LOGINFO ( 'getting list of recent publicly visible ' 'and owned LC collections from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) lcc_list = json . loads ( resp . read ( ) ) [ 'result' ] [ 'collections' ] return lcc_list except HTTPError as e : LOGERROR ( 'could not retrieve list of collections, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
8121	def union ( self , b ) : mx , my = min ( self . x , b . x ) , min ( self . y , b . y ) return Bounds ( mx , my , max ( self . x + self . width , b . x + b . width ) - mx , max ( self . y + self . height , b . y + b . height ) - my )
9658	def get_sinks ( G ) : sinks = [ ] for node in G : if not len ( list ( G . successors ( node ) ) ) : sinks . append ( node ) return sinks
9039	def _dump_to_file ( self , file ) : xmltodict . unparse ( self . object ( ) , file , pretty = True )
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
11851	def add_walls ( self ) : "Put walls around the entire perimeter of the grid." for x in range ( self . width ) : self . add_thing ( Wall ( ) , ( x , 0 ) ) self . add_thing ( Wall ( ) , ( x , self . height - 1 ) ) for y in range ( self . height ) : self . add_thing ( Wall ( ) , ( 0 , y ) ) self . add_thing ( Wall ( ) , ( self . width - 1 , y ) )
13694	def main ( ) : global DEBUG argd = docopt ( USAGESTR , version = VERSIONSTR , script = SCRIPT ) DEBUG = argd [ '--debug' ] width = parse_int ( argd [ '--width' ] or DEFAULT_WIDTH ) or 1 indent = parse_int ( argd [ '--indent' ] or ( argd [ '--INDENT' ] or 0 ) ) prepend = ' ' * ( indent * 4 ) if prepend and argd [ '--indent' ] : width -= len ( prepend ) userprepend = argd [ '--prepend' ] or ( argd [ '--PREPEND' ] or '' ) prepend = '' . join ( ( prepend , userprepend ) ) if argd [ '--prepend' ] : width -= len ( userprepend ) userappend = argd [ '--append' ] or ( argd [ '--APPEND' ] or '' ) if argd [ '--append' ] : width -= len ( userappend ) if argd [ 'WORDS' ] : argd [ 'WORDS' ] = ( ( try_read_file ( w ) if len ( w ) < 256 else w ) for w in argd [ 'WORDS' ] ) words = ' ' . join ( ( w for w in argd [ 'WORDS' ] if w ) ) else : words = read_stdin ( ) block = FormatBlock ( words ) . iter_format_block ( chars = argd [ '--chars' ] , fill = argd [ '--fill' ] , prepend = prepend , strip_first = argd [ '--stripfirst' ] , append = userappend , strip_last = argd [ '--striplast' ] , width = width , newlines = argd [ '--newlines' ] , lstrip = argd [ '--lstrip' ] , ) for i , line in enumerate ( block ) : if argd [ '--enumerate' ] : print ( '{: >3}: {}' . format ( i + 1 , line ) ) else : print ( line ) return 0
10027	def delete_unused_versions ( self , versions_to_keep = 10 ) : environments = self . ebs . describe_environments ( application_name = self . app_name , include_deleted = False ) environments = environments [ 'DescribeEnvironmentsResponse' ] [ 'DescribeEnvironmentsResult' ] [ 'Environments' ] versions_in_use = [ ] for env in environments : versions_in_use . append ( env [ 'VersionLabel' ] ) versions = self . ebs . describe_application_versions ( application_name = self . app_name ) versions = versions [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ] versions = sorted ( versions , reverse = True , key = functools . cmp_to_key ( lambda x , y : ( x [ 'DateCreated' ] > y [ 'DateCreated' ] ) - ( x [ 'DateCreated' ] < y [ 'DateCreated' ] ) ) ) for version in versions [ versions_to_keep : ] : if version [ 'VersionLabel' ] in versions_in_use : out ( "Not deleting " + version [ "VersionLabel" ] + " because it is in use" ) else : out ( "Deleting unused version: " + version [ "VersionLabel" ] ) self . ebs . delete_application_version ( application_name = self . app_name , version_label = version [ 'VersionLabel' ] ) sleep ( 2 )
8694	def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main ( )
5040	def get_users_by_email ( cls , emails ) : users = User . objects . filter ( email__in = emails ) present_emails = users . values_list ( 'email' , flat = True ) missing_emails = list ( set ( emails ) - set ( present_emails ) ) return users , missing_emails
11888	def get_data ( self ) : response = self . send_command ( GET_LIGHTS_COMMAND ) _LOGGER . debug ( "get_data response: %s" , repr ( response ) ) if not response : _LOGGER . debug ( "Empty response: %s" , response ) return { } response = response . strip ( ) if not ( response . startswith ( "GLB" ) and response . endswith ( ";" ) ) : _LOGGER . debug ( "Invalid response: %s" , repr ( response ) ) return { } response = response [ 4 : - 3 ] light_strings = response . split ( ';' ) light_data_by_id = { } for light_string in light_strings : values = light_string . split ( ',' ) try : light_data_by_id [ values [ 0 ] ] = [ int ( values [ 2 ] ) , int ( values [ 4 ] ) , int ( values [ 5 ] ) , int ( values [ 6 ] ) , int ( values [ 7 ] ) ] except ValueError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) except IndexError as error : _LOGGER . error ( "Error %s: %s (%s)" , error , values , response ) return light_data_by_id
7242	def pxbounds ( self , geom , clip = False ) : try : if isinstance ( geom , dict ) : if 'geometry' in geom : geom = shape ( geom [ 'geometry' ] ) else : geom = shape ( geom ) elif isinstance ( geom , BaseGeometry ) : geom = shape ( geom ) else : geom = wkt . loads ( geom ) except : raise TypeError ( "Invalid geometry object" ) if geom . disjoint ( shape ( self ) ) : raise ValueError ( "Geometry outside of image bounds" ) ( xmin , ymin , xmax , ymax ) = ops . transform ( self . __geo_transform__ . rev , geom ) . bounds _nbands , ysize , xsize = self . shape if clip : xmin = max ( xmin , 0 ) ymin = max ( ymin , 0 ) xmax = min ( xmax , xsize ) ymax = min ( ymax , ysize ) return ( xmin , ymin , xmax , ymax )
10884	def slicer ( self ) : return tuple ( np . s_ [ l : r ] for l , r in zip ( * self . bounds ) )
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
13752	def _reference_table ( cls , ref_table ) : cols = [ ( sa . Column ( ) , refcol ) for refcol in ref_table . primary_key ] for col , refcol in cols : setattr ( cls , "%s_%s" % ( ref_table . name , refcol . name ) , col ) cls . __table__ . append_constraint ( sa . ForeignKeyConstraint ( * zip ( * cols ) ) )
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
8050	def load_source ( self ) : if self . filename in self . STDIN_NAMES : self . filename = "stdin" if sys . version_info [ 0 ] < 3 : self . source = sys . stdin . read ( ) else : self . source = TextIOWrapper ( sys . stdin . buffer , errors = "ignore" ) . read ( ) else : handle = tokenize_open ( self . filename ) self . source = handle . read ( ) handle . close ( )
6993	def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( flareparams , times , mags , errs ) return ( mags - modelmags ) / errs
13277	def update_desc_lcin_path ( desc , pdesc_level ) : parent_breadth = desc [ 'parent_breadth_path' ] [ - 1 ] if ( desc [ 'sib_seq' ] == 0 ) : if ( parent_breadth == 0 ) : pass else : parent_lsib_breadth = parent_breadth - 1 plsib_desc = pdesc_level [ parent_lsib_breadth ] if ( plsib_desc [ 'leaf' ] ) : pass else : lcin_path = copy . deepcopy ( plsib_desc [ 'path' ] ) lcin_path . append ( plsib_desc [ 'sons_count' ] - 1 ) desc [ 'lcin_path' ] = lcin_path else : pass return ( desc )
6489	def _process_field_queries ( field_dictionary ) : def field_item ( field ) : return { "match" : { field : field_dictionary [ field ] } } return [ field_item ( field ) for field in field_dictionary ]
5994	def set_colorbar ( cb_ticksize , cb_fraction , cb_pad , cb_tick_values , cb_tick_labels ) : if cb_tick_values is None and cb_tick_labels is None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad ) elif cb_tick_values is not None and cb_tick_labels is not None : cb = plt . colorbar ( fraction = cb_fraction , pad = cb_pad , ticks = cb_tick_values ) cb . ax . set_yticklabels ( cb_tick_labels ) else : raise exc . PlottingException ( 'Only 1 entry of cb_tick_values or cb_tick_labels was input. You must either supply' 'both the values and labels, or neither.' ) cb . ax . tick_params ( labelsize = cb_ticksize )
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : raise
4231	def run_subcommand ( netgear , args ) : subcommand = args . subcommand if subcommand == "block_device" or subcommand == "allow_device" : return netgear . allow_block_device ( args . mac_addr , BLOCK if subcommand == "block_device" else ALLOW ) if subcommand == "attached_devices" : if args . verbose : return netgear . get_attached_devices_2 ( ) else : return netgear . get_attached_devices ( ) if subcommand == 'traffic_meter' : return netgear . get_traffic_meter ( ) if subcommand == 'login' : return netgear . login ( ) print ( "Unknown subcommand" )
412	def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
2561	def recv_result_from_workers ( self ) : info = MPI . Status ( ) result = self . comm . recv ( source = MPI . ANY_SOURCE , tag = RESULT_TAG , status = info ) logger . debug ( "Received result from workers: {}" . format ( result ) ) return result
5780	def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
565	def updateResultsForJob ( self , forceUpdate = True ) : updateInterval = time . time ( ) - self . _lastUpdateAttemptTime if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : return self . logger . info ( "Attempting model selection for jobID=%d: time=%f" " lastUpdate=%f" % ( self . _jobID , time . time ( ) , self . _lastUpdateAttemptTime ) ) timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , self . _MIN_UPDATE_INTERVAL ) if not timestampUpdated : self . logger . info ( "Unable to update selection sweep timestamp: jobID=%d" " updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) if not forceUpdate : return self . _lastUpdateAttemptTime = time . time ( ) self . logger . info ( "Succesfully updated selection sweep timestamp jobid=%d updateTime=%f" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) minUpdateRecords = self . _MIN_UPDATE_THRESHOLD jobResults = self . _getJobResults ( ) if forceUpdate or jobResults is None : minUpdateRecords = 0 candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) self . logger . info ( "Candidate models=%s, metric=%s, jobID=%s" % ( candidateIDs , bestMetric , self . _jobID ) ) if len ( candidateIDs ) == 0 : return self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults )
4937	def transform_description ( self , content_metadata_item ) : full_description = content_metadata_item . get ( 'full_description' ) or '' if 0 < len ( full_description ) <= self . LONG_STRING_LIMIT : return full_description return content_metadata_item . get ( 'short_description' ) or content_metadata_item . get ( 'title' ) or ''
1387	def set_execution_state ( self , execution_state ) : if not execution_state : self . execution_state = None self . cluster = None self . environ = None else : self . execution_state = execution_state cluster , environ = self . get_execution_state_dc_environ ( execution_state ) self . cluster = cluster self . environ = environ self . zone = cluster self . trigger_watches ( )
577	def rUpdate ( original , updates ) : dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v
136	def change_first_point_by_index ( self , point_idx ) : ia . do_assert ( 0 <= point_idx < len ( self . exterior ) ) if point_idx == 0 : return self . deepcopy ( ) exterior = np . concatenate ( ( self . exterior [ point_idx : , : ] , self . exterior [ : point_idx , : ] ) , axis = 0 ) return self . deepcopy ( exterior = exterior )
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
1671	def ProcessConfigOverrides ( filename ) : abs_filename = os . path . abspath ( filename ) cfg_filters = [ ] keep_looking = True while keep_looking : abs_path , base_name = os . path . split ( abs_filename ) if not base_name : break cfg_file = os . path . join ( abs_path , "CPPLINT.cfg" ) abs_filename = abs_path if not os . path . isfile ( cfg_file ) : continue try : with open ( cfg_file ) as file_handle : for line in file_handle : line , _ , _ = line . partition ( '#' ) if not line . strip ( ) : continue name , _ , val = line . partition ( '=' ) name = name . strip ( ) val = val . strip ( ) if name == 'set noparent' : keep_looking = False elif name == 'filter' : cfg_filters . append ( val ) elif name == 'exclude_files' : if base_name : pattern = re . compile ( val ) if pattern . match ( base_name ) : _cpplint_state . PrintInfo ( 'Ignoring "%s": file excluded by ' '"%s". File path component "%s" matches pattern "%s"\n' % ( filename , cfg_file , base_name , val ) ) return False elif name == 'linelength' : global _line_length try : _line_length = int ( val ) except ValueError : _cpplint_state . PrintError ( 'Line length must be numeric.' ) elif name == 'extensions' : global _valid_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _valid_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'headers' : global _header_extensions try : extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] _header_extensions = set ( extensions ) except ValueError : sys . stderr . write ( 'Extensions should be a comma-separated list of values;' 'for example: extensions=hpp,cpp\n' 'This could not be parsed: "%s"' % ( val , ) ) elif name == 'root' : global _root _root = val else : _cpplint_state . PrintError ( 'Invalid configuration option (%s) in file %s\n' % ( name , cfg_file ) ) except IOError : _cpplint_state . PrintError ( "Skipping config file '%s': Can't open for reading\n" % cfg_file ) keep_looking = False for cfg_filter in reversed ( cfg_filters ) : _AddFilters ( cfg_filter ) return True
1230	def tf_optimization ( self , states , internals , actions , terminal , reward , next_states = None , next_internals = None ) : arguments = self . optimizer_arguments ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) return self . optimizer . minimize ( ** arguments )
63	def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None
1049	def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )
12500	def sigma2fwhm ( sigma ) : sigma = np . asarray ( sigma ) return np . sqrt ( 8 * np . log ( 2 ) ) * sigma
13894	def MatchMasks ( filename , masks ) : import fnmatch if not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] for i_mask in masks : if fnmatch . fnmatch ( filename , i_mask ) : return True return False
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
5968	def energy_minimize ( dirname = 'em' , mdp = config . templates [ 'em.mdp' ] , struct = 'solvate/ionized.gro' , top = 'top/system.top' , output = 'em.pdb' , deffnm = "em" , mdrunner = None , mdrun_args = None , ** kwargs ) : structure = realpath ( struct ) topology = realpath ( top ) mdp_template = config . get_template ( mdp ) deffnm = deffnm . strip ( ) mdrun_args = { } if mdrun_args is None else mdrun_args kwargs . setdefault ( 'pp' , 'processed.top' ) kwargs . pop ( 'ndx' , None ) mainselection = kwargs . pop ( 'mainselection' , '"Protein"' ) qtot = kwargs . pop ( 'qtot' , 0 ) mdp = deffnm + '.mdp' tpr = deffnm + '.tpr' logger . info ( "[{dirname!s}] Energy minimization of struct={struct!r}, top={top!r}, mdp={mdp!r} ..." . format ( ** vars ( ) ) ) cbook . add_mdp_includes ( topology , kwargs ) if qtot != 0 : wmsg = "Total charge was reported as qtot = {qtot:g} <> 0; probably a problem." . format ( ** vars ( ) ) logger . warn ( wmsg ) warnings . warn ( wmsg , category = BadParameterWarning ) with in_dir ( dirname ) : unprocessed = cbook . edit_mdp ( mdp_template , new_mdp = mdp , ** kwargs ) check_mdpargs ( unprocessed ) gromacs . grompp ( f = mdp , o = tpr , c = structure , r = structure , p = topology , ** unprocessed ) mdrun_args . update ( v = True , stepout = 10 , deffnm = deffnm , c = output ) if mdrunner is None : mdrun = run . get_double_or_single_prec_mdrun ( ) mdrun ( ** mdrun_args ) else : if type ( mdrunner ) is type : mdrun = mdrunner ( ** mdrun_args ) mdrun . run ( ) else : try : mdrunner . run ( mdrunargs = mdrun_args ) except AttributeError : logger . error ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) raise TypeError ( "mdrunner: Provide a gromacs.run.MDrunner class or instance or a callback with a run() method" ) if not os . path . exists ( output ) : errmsg = "Energy minimized system NOT produced." logger . error ( errmsg ) raise GromacsError ( errmsg ) final_struct = realpath ( output ) logger . info ( "[{dirname!s}] energy minimized structure {final_struct!r}" . format ( ** vars ( ) ) ) return { 'struct' : final_struct , 'top' : topology , 'mainselection' : mainselection , }
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
5244	def missing_info ( ** kwargs ) -> str : func = kwargs . pop ( 'func' , 'unknown' ) if 'ticker' in kwargs : kwargs [ 'ticker' ] = kwargs [ 'ticker' ] . replace ( '/' , '_' ) info = utils . to_str ( kwargs , fmt = '{value}' , sep = '/' ) [ 1 : - 1 ] return f'{func}/{info}'
7208	def task_ids ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get task IDs.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for task IDs." ) wf = self . workflow . get ( self . id ) return [ task [ 'id' ] for task in wf [ 'tasks' ] ]
8550	def update_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id , ** kwargs ) : data = { } for attr , value in kwargs . items ( ) : data [ self . _underscore_to_camelcase ( attr ) ] = value if attr == 'source_mac' : data [ 'sourceMac' ] = value elif attr == 'source_ip' : data [ 'sourceIp' ] = value elif attr == 'target_ip' : data [ 'targetIp' ] = value elif attr == 'port_range_start' : data [ 'portRangeStart' ] = value elif attr == 'port_range_end' : data [ 'portRangeEnd' ] = value elif attr == 'icmp_type' : data [ 'icmpType' ] = value elif attr == 'icmp_code' : data [ 'icmpCode' ] = value else : data [ self . _underscore_to_camelcase ( attr ) ] = value response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
13195	def replace_macros ( tex_source , macros ) : r for macro_name , macro_content in macros . items ( ) : pattern = re . escape ( macro_name ) + r"\\?" tex_source = re . sub ( pattern , lambda _ : macro_content , tex_source ) return tex_source
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
4037	def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : if req . status_code == 429 : delay = backoff . delay if delay > 32 : backoff . reset ( ) raise ze . TooManyRetries ( "Continuing to receive HTTP 429 \responses after 62 seconds. You are being rate-limited, try again later" ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
1399	def extract_tmaster ( self , topology ) : tmasterLocation = { "name" : None , "id" : None , "host" : None , "controller_port" : None , "master_port" : None , "stats_port" : None , } if topology . tmaster : tmasterLocation [ "name" ] = topology . tmaster . topology_name tmasterLocation [ "id" ] = topology . tmaster . topology_id tmasterLocation [ "host" ] = topology . tmaster . host tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port tmasterLocation [ "master_port" ] = topology . tmaster . master_port tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port return tmasterLocation
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
9794	def _ignore_path ( cls , path , ignore_list = None , white_list = None ) : ignore_list = ignore_list or [ ] white_list = white_list or [ ] return ( cls . _matches_patterns ( path , ignore_list ) and not cls . _matches_patterns ( path , white_list ) )
6398	def dist_monge_elkan ( src , tar , sim_func = sim_levenshtein , symmetric = False ) : return MongeElkan ( ) . dist ( src , tar , sim_func , symmetric )
10211	def cmdclass ( path , enable = None , user = None ) : import warnings from setuptools . command . install import install from setuptools . command . develop import develop from os . path import dirname , join , exists , realpath from traceback import extract_stack try : from notebook . nbextensions import install_nbextension from notebook . services . config import ConfigManager except ImportError : try : from IPython . html . nbextensions import install_nbextension from IPython . html . services . config import ConfigManager except ImportError : warnings . warn ( "No jupyter notebook found in your environment. " "Hence jupyter nbextensions were not installed. " "If you would like to have them," "please issue 'pip install jupyter'." ) return { } if user is None : user = not _is_root ( ) calling_file = extract_stack ( ) [ - 2 ] [ 0 ] fullpath = realpath ( calling_file ) if not exists ( fullpath ) : raise Exception ( 'Could not find path of setup file.' ) extension_dir = join ( dirname ( fullpath ) , path ) def run_nbextension_install ( develop ) : import sys sysprefix = hasattr ( sys , 'real_prefix' ) if sysprefix : install_nbextension ( extension_dir , symlink = develop , sys_prefix = sysprefix ) else : install_nbextension ( extension_dir , symlink = develop , user = user ) if enable is not None : print ( "Enabling the extension ..." ) cm = ConfigManager ( ) cm . update ( 'notebook' , { "load_extensions" : { enable : True } } ) class InstallCommand ( install ) : def run ( self ) : print ( "Installing Python module..." ) install . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( False ) class DevelopCommand ( develop ) : def run ( self ) : print ( "Installing Python module..." ) develop . run ( self ) print ( "Installing nbextension ..." ) run_nbextension_install ( True ) return { 'install' : InstallCommand , 'develop' : DevelopCommand , }
2194	def encoding ( self ) : if self . redirect is not None : return self . redirect . encoding else : return super ( TeeStringIO , self ) . encoding
7746	def stanza_factory ( element , return_path = None , language = None ) : tag = element . tag if tag . endswith ( "}iq" ) or tag == "iq" : return Iq ( element , return_path = return_path , language = language ) if tag . endswith ( "}message" ) or tag == "message" : return Message ( element , return_path = return_path , language = language ) if tag . endswith ( "}presence" ) or tag == "presence" : return Presence ( element , return_path = return_path , language = language ) else : return Stanza ( element , return_path = return_path , language = language )
4662	def proposal ( self , proposer = None , proposal_expiration = None , proposal_review = None ) : if not self . _propbuffer : return self . new_proposal ( self . tx ( ) , proposer , proposal_expiration , proposal_review ) if proposer : self . _propbuffer [ 0 ] . set_proposer ( proposer ) if proposal_expiration : self . _propbuffer [ 0 ] . set_expiration ( proposal_expiration ) if proposal_review : self . _propbuffer [ 0 ] . set_review ( proposal_review ) return self . _propbuffer [ 0 ]
5904	def grompp_qtot ( * args , ** kwargs ) : qtot_pattern = re . compile ( 'System has non-zero total charge: *(?P<qtot>[-+]?\d*\.\d+([eE][-+]\d+)?)' ) kwargs [ 'stdout' ] = False kwargs [ 'stderr' ] = False rc , output , error = grompp_warnonly ( * args , ** kwargs ) gmxoutput = "\n" . join ( [ x for x in [ output , error ] if x is not None ] ) if rc != 0 : msg = "grompp_qtot() failed. See warning and screen output for clues." logger . error ( msg ) import sys sys . stderr . write ( "=========== grompp (stdout/stderr) ============\n" ) sys . stderr . write ( gmxoutput ) sys . stderr . write ( "===============================================\n" ) sys . stderr . flush ( ) raise GromacsError ( rc , msg ) qtot = 0 for line in gmxoutput . split ( '\n' ) : m = qtot_pattern . search ( line ) if m : qtot = float ( m . group ( 'qtot' ) ) break logger . info ( "system total charge qtot = {qtot!r}" . format ( ** vars ( ) ) ) return qtot
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
8286	def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
10298	def get_incorrect_names ( graph : BELGraph ) -> Mapping [ str , Set [ str ] ] : return { namespace : get_incorrect_names_by_namespace ( graph , namespace ) for namespace in get_namespaces ( graph ) }
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
8267	def copy ( self , clr = None , d = 0.0 ) : cr = ColorRange ( ) cr . name = self . name cr . h = deepcopy ( self . h ) cr . s = deepcopy ( self . s ) cr . b = deepcopy ( self . b ) cr . a = deepcopy ( self . a ) cr . grayscale = self . grayscale if not self . grayscale : cr . black = self . black . copy ( ) cr . white = self . white . copy ( ) if clr != None : cr . h , cr . a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a return cr
2022	def SIGNEXTEND ( self , size , value ) : testbit = Operators . ITEBV ( 256 , size <= 31 , size * 8 + 7 , 257 ) result1 = ( value | ( TT256 - ( 1 << testbit ) ) ) result2 = ( value & ( ( 1 << testbit ) - 1 ) ) result = Operators . ITEBV ( 256 , ( value & ( 1 << testbit ) ) != 0 , result1 , result2 ) return Operators . ITEBV ( 256 , size <= 31 , result , value )
12704	def make_quaternion ( theta , * axis ) : x , y , z = axis r = np . sqrt ( x * x + y * y + z * z ) st = np . sin ( theta / 2. ) ct = np . cos ( theta / 2. ) return [ x * st / r , y * st / r , z * st / r , ct ]
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
9755	def delete ( ctx ) : user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if not click . confirm ( "Are sure you want to delete experiment `{}`" . format ( _experiment ) ) : click . echo ( 'Existing without deleting experiment.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . experiment . delete_experiment ( user , project_name , _experiment ) ExperimentManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Experiment `{}` was delete successfully" . format ( _experiment ) )
11761	def refresh ( self ) : args = [ ( obj . name , obj . value ) for obj in self . queryset . all ( ) ] super ( SettingDict , self ) . update ( args ) self . empty_cache = False
7045	def all_nonperiodic_features ( times , mags , errs , magsarefluxes = False , stetson_weightbytimediff = True ) : finiteind = npisfinite ( times ) & npisfinite ( mags ) & npisfinite ( errs ) ftimes , fmags , ferrs = times [ finiteind ] , mags [ finiteind ] , errs [ finiteind ] nzind = npnonzero ( ferrs ) ftimes , fmags , ferrs = ftimes [ nzind ] , fmags [ nzind ] , ferrs [ nzind ] xfeatures = nonperiodic_lightcurve_features ( times , mags , errs , magsarefluxes = magsarefluxes ) stetj = stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = stetson_weightbytimediff ) stetk = stetson_kindex ( fmags , ferrs ) xfeatures . update ( { 'stetsonj' : stetj , 'stetsonk' : stetk } ) return xfeatures
4468	def deserialize ( encoded , ** kwargs ) : params = jsonpickle . decode ( encoded , ** kwargs ) return __reconstruct ( params )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
1206	def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env
4362	def _heartbeat ( self ) : interval = self . config [ 'heartbeat_interval' ] while self . connected : gevent . sleep ( interval ) self . put_client_msg ( "2::" )
13259	def combine ( self , members , output_file , dimension = None , start_index = None , stop_index = None , stride = None ) : nco = None try : nco = Nco ( ) except BaseException : raise ImportError ( "NCO not found. The NCO python bindings are required to use 'Collection.combine'." ) if len ( members ) > 0 and hasattr ( members [ 0 ] , 'path' ) : members = [ m . path for m in members ] options = [ '-4' ] options += [ '-L' , '3' ] options += [ '-h' ] if dimension is not None : if start_index is None : start_index = 0 if stop_index is None : stop_index = '' if stride is None : stride = 1 options += [ '-d' , '{0},{1},{2},{3}' . format ( dimension , start_index , stop_index , stride ) ] nco . ncrcat ( input = members , output = output_file , options = options )
1857	def BT ( cpu , dest , src ) : if dest . type == 'register' : cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) value = cpu . read_int ( addr + base , 8 ) cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 else : raise NotImplementedError ( f"Unknown operand for BT: {dest.type}" )
11119	def get_file_info ( self , relativePath , name = None ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' can't be a file name." if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) errorMessage = "" dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) if dirInfoDict is None : return None , errorMessage fileInfo = dict . __getitem__ ( dirInfoDict , "files" ) . get ( name , None ) if fileInfo is None : errorMessage = "file %s does not exist in relative path '%s'" % ( name , relativePath ) return fileInfo , errorMessage
12997	def skyimage_figure ( cluster ) : pf_image = figure ( x_range = ( 0 , 1 ) , y_range = ( 0 , 1 ) , title = 'Image of {0}' . format ( cluster . name ) ) pf_image . image_url ( url = [ cluster . image_path ] , x = 0 , y = 0 , w = 1 , h = 1 , anchor = 'bottom_left' ) pf_image . toolbar_location = None pf_image . axis . visible = False return pf_image
5564	def input ( self ) : delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) raw_inputs = { get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
13319	def deactivate ( ) : if 'CPENV_ACTIVE' not in os . environ or 'CPENV_CLEAN_ENV' not in os . environ : raise EnvironmentError ( 'Can not deactivate environment...' ) utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
423	def create_task ( self , task_name = None , script = None , hyper_parameters = None , saved_result_keys = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) if not isinstance ( script , str ) : raise Exception ( "script should be string" ) if hyper_parameters is None : hyper_parameters = { } if saved_result_keys is None : saved_result_keys = [ ] self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) kwargs . update ( { 'hyper_parameters' : hyper_parameters } ) kwargs . update ( { 'saved_result_keys' : saved_result_keys } ) _script = open ( script , 'rb' ) . read ( ) kwargs . update ( { 'status' : 'pending' , 'script' : _script , 'result' : { } } ) self . db . Task . insert_one ( kwargs ) logging . info ( "[Database] Saved Task - task_name: {} script: {}" . format ( task_name , script ) )
4421	async def set_pause ( self , pause : bool ) : await self . _lavalink . ws . send ( op = 'pause' , guildId = self . guild_id , pause = pause ) self . paused = pause
10684	def S_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : s = 1 - ( self . _B_mag * ( 2 * tau ** 3 / 3 + 2 * tau ** 9 / 27 + 2 * tau ** 15 / 75 ) ) / self . _D_mag else : s = ( 2 * tau ** - 5 / 5 + 2 * tau ** - 15 / 45 + 2 * tau ** - 25 / 125 ) / self . _D_mag return - R * math . log ( self . beta0_mag + 1 ) * s
10958	def update_from_model_change ( self , oldmodel , newmodel , tile ) : self . _loglikelihood -= self . _calc_loglikelihood ( oldmodel , tile = tile ) self . _loglikelihood += self . _calc_loglikelihood ( newmodel , tile = tile ) self . _residuals [ tile . slicer ] = self . _data [ tile . slicer ] - newmodel
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
5375	def simple_pattern_exists_in_gcs ( file_pattern , credentials = None ) : if '*' not in file_pattern : return _file_exists_in_gcs ( file_pattern , credentials ) if not file_pattern . startswith ( 'gs://' ) : raise ValueError ( 'file name must start with gs://' ) gcs_service = _get_storage_service ( credentials ) bucket_name , prefix = file_pattern [ len ( 'gs://' ) : ] . split ( '/' , 1 ) if '*' in bucket_name : raise ValueError ( 'Wildcards may not appear in the bucket name' ) assert '*' in prefix prefix_no_wildcard = prefix [ : prefix . index ( '*' ) ] request = gcs_service . objects ( ) . list ( bucket = bucket_name , prefix = prefix_no_wildcard ) response = request . execute ( ) if 'items' not in response : return False items_list = [ i [ 'name' ] for i in response [ 'items' ] ] return any ( fnmatch . fnmatch ( i , prefix ) for i in items_list )
10405	def bond_canonical_statistics ( microcanonical_statistics , convolution_factors , ** kwargs ) : spanning_cluster = ( 'has_spanning_cluster' in microcanonical_statistics . dtype . names ) ret = np . empty ( 1 , dtype = canonical_statistics_dtype ( spanning_cluster ) ) if spanning_cluster : ret [ 'percolation_probability' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'has_spanning_cluster' ] ) ret [ 'max_cluster_size' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'max_cluster_size' ] ) ret [ 'moments' ] = np . sum ( convolution_factors [ : , np . newaxis ] * microcanonical_statistics [ 'moments' ] , axis = 0 , ) return ret
1760	def read_bytes ( self , where , size , force = False ) : result = [ ] for i in range ( size ) : result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) return result
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
7567	def splitalleles ( consensus ) : allele1 = list ( consensus ) allele2 = list ( consensus ) hidx = [ i for ( i , j ) in enumerate ( consensus ) if j in "RKSWYMrkswym" ] for idx in hidx : hsite = consensus [ idx ] if hsite . isupper ( ) : allele1 [ idx ] = PRIORITY [ hsite ] allele2 [ idx ] = MINOR [ hsite ] else : allele1 [ idx ] = MINOR [ hsite . upper ( ) ] allele2 [ idx ] = PRIORITY [ hsite . upper ( ) ] allele1 = "" . join ( allele1 ) allele2 = "" . join ( allele2 ) return allele1 , allele2
10825	def search ( cls , query , q ) : query = query . join ( User ) . filter ( User . email . like ( '%{0}%' . format ( q ) ) , ) return query
1722	def is_lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER_START : return False return all ( e in IDENTIFIER_PART for e in i )
5795	def cf_dictionary_to_dict ( dictionary ) : dict_length = CoreFoundation . CFDictionaryGetCount ( dictionary ) keys = ( CFTypeRef * dict_length ) ( ) values = ( CFTypeRef * dict_length ) ( ) CoreFoundation . CFDictionaryGetKeysAndValues ( dictionary , _cast_pointer_p ( keys ) , _cast_pointer_p ( values ) ) output = { } for index in range ( 0 , dict_length ) : output [ CFHelpers . native ( keys [ index ] ) ] = CFHelpers . native ( values [ index ] ) return output
5799	def walk_ast ( node , code_lines , sections , md_chunks ) : if isinstance ( node , _ast . FunctionDef ) : key = ( 'function' , node . name ) if key not in sections : return docstring = ast . get_docstring ( node ) def_lineno = node . lineno + len ( node . decorator_list ) definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> ' ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , definition , description_md ) + "\n" md_chunks [ key ] = md_chunk . replace ( '>\n\n' , '' ) elif isinstance ( node , _ast . ClassDef ) : if ( 'class' , node . name ) not in sections : return for subnode in node . body : if isinstance ( subnode , _ast . FunctionDef ) : node_id = node . name + '.' + subnode . name method_key = ( 'method' , node_id ) is_method = method_key in sections attribute_key = ( 'attribute' , node_id ) is_attribute = attribute_key in sections is_constructor = subnode . name == '__init__' if not is_constructor and not is_attribute and not is_method : continue docstring = ast . get_docstring ( subnode ) def_lineno = subnode . lineno + len ( subnode . decorator_list ) if not docstring : continue if is_method or is_constructor : definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> > ' ) if is_constructor : key = ( 'class' , node . name ) class_docstring = ast . get_docstring ( node ) or '' class_description = textwrap . dedent ( class_docstring ) . strip ( ) if class_description : class_description_md = "> %s\n>" % ( class_description . replace ( "\n" , "\n> " ) ) else : class_description_md = '' md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , class_description_md , definition , description_md ) md_chunk = md_chunk . replace ( '\n\n\n' , '\n\n' ) else : key = method_key md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , definition , description_md ) if md_chunk [ - 5 : ] == '\n> >\n' : md_chunk = md_chunk [ 0 : - 5 ] else : key = attribute_key description = textwrap . dedent ( docstring ) . strip ( ) description_md = "> > %s" % ( description . replace ( "\n" , "\n> > " ) ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , description_md ) md_chunks [ key ] = re . sub ( '[ \\t]+\n' , '\n' , md_chunk . rstrip ( ) ) elif isinstance ( node , _ast . If ) : for subast in node . body : walk_ast ( subast , code_lines , sections , md_chunks ) for subast in node . orelse : walk_ast ( subast , code_lines , sections , md_chunks )
7317	def sendmail ( self , msg_from , msg_to , msg ) : SMTP_dummy . msg_from = msg_from SMTP_dummy . msg_to = msg_to SMTP_dummy . msg = msg
9410	def _create_struct ( data , session ) : out = Struct ( ) for name in data . dtype . names : item = data [ name ] if isinstance ( item , np . ndarray ) and item . dtype . kind == 'O' : item = item . squeeze ( ) . tolist ( ) out [ name ] = _extract ( item , session ) return out
2671	def deploy_s3 ( src , requirements = None , local_package = None , config_file = 'config.yaml' , profile_name = None , preserve_vpc = False ) : path_to_config_file = os . path . join ( src , config_file ) cfg = read_cfg ( path_to_config_file , profile_name ) path_to_zip_file = build ( src , config_file = config_file , requirements = requirements , local_package = local_package , ) use_s3 = True s3_file = upload_s3 ( cfg , path_to_zip_file , use_s3 ) existing_config = get_function_config ( cfg ) if existing_config : update_function ( cfg , path_to_zip_file , existing_config , use_s3 = use_s3 , s3_file = s3_file , preserve_vpc = preserve_vpc ) else : create_function ( cfg , path_to_zip_file , use_s3 = use_s3 , s3_file = s3_file )
9315	def _ensure_datetime_to_string ( maybe_dttm ) : if isinstance ( maybe_dttm , datetime . datetime ) : maybe_dttm = _format_datetime ( maybe_dttm ) return maybe_dttm
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
13272	def generic_masked ( arr , attrs = None , minv = None , maxv = None , mask_nan = True ) : attrs = attrs or { } if 'valid_min' in attrs : minv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_min' ] ) if 'valid_max' in attrs : maxv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_max' ] ) if 'valid_range' in attrs : vr = attrs [ 'valid_range' ] minv = safe_attribute_typing ( arr . dtype , vr [ 0 ] ) maxv = safe_attribute_typing ( arr . dtype , vr [ 1 ] ) try : info = np . iinfo ( arr . dtype ) except ValueError : info = np . finfo ( arr . dtype ) minv = minv if minv is not None else info . min maxv = maxv if maxv is not None else info . max if mask_nan is True : arr = np . ma . fix_invalid ( arr ) return np . ma . masked_outside ( arr , minv , maxv )
5470	def _prepare_summary_table ( rows ) : if not rows : return [ ] key_field = 'job-name' if key_field not in rows [ 0 ] : key_field = 'job-id' grouped = collections . defaultdict ( lambda : collections . defaultdict ( lambda : [ ] ) ) for row in rows : grouped [ row . get ( key_field , '' ) ] [ row . get ( 'status' , '' ) ] += [ row ] new_rows = [ ] for job_key in sorted ( grouped . keys ( ) ) : group = grouped . get ( job_key , None ) canonical_status = [ 'RUNNING' , 'SUCCESS' , 'FAILURE' , 'CANCEL' ] for status in canonical_status + sorted ( group . keys ( ) ) : if status not in group : continue task_count = len ( group [ status ] ) del group [ status ] if task_count : summary_row = collections . OrderedDict ( ) summary_row [ key_field ] = job_key summary_row [ 'status' ] = status summary_row [ 'task-count' ] = task_count new_rows . append ( summary_row ) return new_rows
10256	def get_causal_central_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_central ( graph , node ) }
10332	def average_node_annotation ( graph : BELGraph , key : str , annotation : str = 'Subgraph' , aggregator : Optional [ Callable [ [ Iterable [ X ] ] , X ] ] = None , ) -> Mapping [ str , X ] : if aggregator is None : def aggregator ( x ) : return sum ( x ) / len ( x ) result = { } for subgraph , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) : values = [ graph . nodes [ node ] [ key ] for node in nodes if key in graph . nodes [ node ] ] result [ subgraph ] = aggregator ( values ) return result
5438	def validate_submit_args_or_fail ( job_descriptor , provider_name , input_providers , output_providers , logging_providers ) : job_resources = job_descriptor . job_resources job_params = job_descriptor . job_params task_descriptors = job_descriptor . task_descriptors _validate_providers ( [ job_resources . logging ] , 'logging' , logging_providers , provider_name ) _validate_providers ( job_params [ 'inputs' ] , 'input' , input_providers , provider_name ) _validate_providers ( job_params [ 'outputs' ] , 'output' , output_providers , provider_name ) for task_descriptor in task_descriptors : _validate_providers ( task_descriptor . task_params [ 'inputs' ] , 'input' , input_providers , provider_name ) _validate_providers ( task_descriptor . task_params [ 'outputs' ] , 'output' , output_providers , provider_name )
13383	def expand_envvars ( env ) : out_env = { } for k , v in env . iteritems ( ) : out_env [ k ] = Template ( v ) . safe_substitute ( env ) for k , v in out_env . items ( ) : out_env [ k ] = Template ( v ) . safe_substitute ( out_env ) return out_env
2577	def _gather_all_deps ( self , args , kwargs ) : depends = [ ] count = 0 for dep in args : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for key in kwargs : dep = kwargs [ key ] if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for dep in kwargs . get ( 'inputs' , [ ] ) : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) return count , depends
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
5317	def use_style ( self , style_name ) : try : style = getattr ( styles , style_name . upper ( ) ) except AttributeError : raise ColorfulError ( 'the style "{0}" is undefined' . format ( style_name ) ) else : self . colorpalette = style
2873	def trash ( self , file ) : if self . _should_skipped_by_specs ( file ) : self . reporter . unable_to_trash_dot_entries ( file ) return volume_of_file_to_be_trashed = self . volume_of_parent ( file ) self . reporter . volume_of_file ( volume_of_file_to_be_trashed ) candidates = self . _possible_trash_directories_for ( volume_of_file_to_be_trashed ) self . try_trash_file_using_candidates ( file , volume_of_file_to_be_trashed , candidates )
12031	def get_protocol_sequence ( self , sweep ) : self . setsweep ( sweep ) return list ( self . protoSeqX ) , list ( self . protoSeqY )
5282	def construct_formset ( self ) : formset_class = self . get_formset ( ) if hasattr ( self , 'get_extra_form_kwargs' ) : klass = type ( self ) . __name__ raise DeprecationWarning ( 'Calling {0}.get_extra_form_kwargs is no longer supported. ' 'Set `form_kwargs` in {0}.formset_kwargs or override ' '{0}.get_formset_kwargs() directly.' . format ( klass ) , ) return formset_class ( ** self . get_formset_kwargs ( ) )
6831	def get_current_commit ( self ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : s = str ( self . local ( 'git rev-parse HEAD' , capture = True ) ) self . vprint ( 'current commit:' , s ) return s
8951	def get_devpi_url ( ctx ) : cmd = 'devpi use --urls' lines = ctx . run ( cmd , hide = 'out' , echo = False ) . stdout . splitlines ( ) for line in lines : try : line , base_url = line . split ( ':' , 1 ) except ValueError : notify . warning ( 'Ignoring "{}"!' . format ( line ) ) else : if line . split ( ) [ - 1 ] . strip ( ) == 'simpleindex' : return base_url . split ( '\x1b' ) [ 0 ] . strip ( ) . rstrip ( '/' ) raise LookupError ( "Cannot find simpleindex URL in '{}' output:\n {}" . format ( cmd , '\n ' . join ( lines ) , ) )
9472	def validateRequest ( self , uri , postVars , expectedSignature ) : s = uri for k , v in sorted ( postVars . items ( ) ) : s += k + v return ( base64 . encodestring ( hmac . new ( self . auth_token , s , sha1 ) . digest ( ) ) . strip ( ) == expectedSignature )
11082	def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( ** value ) return value
13478	def _sentence_to_interstitial_spacing ( self ) : not_sentence_end_chars = [ ' ' ] abbreviations = [ 'i.e.' , 'e.g.' , ' v.' , ' w.' , ' wh.' ] titles = [ 'Prof.' , 'Mr.' , 'Mrs.' , 'Messrs.' , 'Mmes.' , 'Msgr.' , 'Ms.' , 'Fr.' , 'Rev.' , 'St.' , 'Dr.' , 'Lieut.' , 'Lt.' , 'Capt.' , 'Cptn.' , 'Sgt.' , 'Sjt.' , 'Gen.' , 'Hon.' , 'Cpl.' , 'L-Cpl.' , 'Pvt.' , 'Dvr.' , 'Gnr.' , 'Spr.' , 'Col.' , 'Lt-Col' , 'Lt-Gen.' , 'Mx.' ] for abbrev in abbreviations : for x in not_sentence_end_chars : self . _str_replacement ( abbrev + x , abbrev + '\ ' ) for title in titles : for x in not_sentence_end_chars : self . _str_replacement ( title + x , title + '~' )
5086	def has_implicit_access_to_catalog ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_CATALOG_ADMIN_ROLE , obj )
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : r with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( ** json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
9616	def is_displayed ( target ) : is_displayed = getattr ( target , 'is_displayed' , None ) if not is_displayed or not callable ( is_displayed ) : raise TypeError ( 'Target has no attribute \'is_displayed\' or not callable' ) if not is_displayed ( ) : raise WebDriverException ( 'element not visible' )
677	def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description
128	def project ( self , from_shape , to_shape ) : if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return self . copy ( ) ls_proj = self . to_line_string ( closed = False ) . project ( from_shape , to_shape ) return self . copy ( exterior = ls_proj . coords )
11756	def prop_symbols ( x ) : "Return a list of all propositional symbols in x." if not isinstance ( x , Expr ) : return [ ] elif is_prop_symbol ( x . op ) : return [ x ] else : return list ( set ( symbol for arg in x . args for symbol in prop_symbols ( arg ) ) )
6901	def _parse_xmatch_catalog_header ( xc , xk ) : catdef = [ ] if xc . endswith ( '.gz' ) : infd = gzip . open ( xc , 'rb' ) else : infd = open ( xc , 'rb' ) for line in infd : if line . decode ( ) . startswith ( '#' ) : catdef . append ( line . decode ( ) . replace ( '#' , '' ) . strip ( ) . rstrip ( '\n' ) ) if not line . decode ( ) . startswith ( '#' ) : break if not len ( catdef ) > 0 : LOGERROR ( "catalog definition not parseable " "for catalog: %s, skipping..." % xc ) return None catdef = ' ' . join ( catdef ) catdefdict = json . loads ( catdef ) catdefkeys = [ x [ 'key' ] for x in catdefdict [ 'columns' ] ] catdefdtypes = [ x [ 'dtype' ] for x in catdefdict [ 'columns' ] ] catdefnames = [ x [ 'name' ] for x in catdefdict [ 'columns' ] ] catdefunits = [ x [ 'unit' ] for x in catdefdict [ 'columns' ] ] catcolinds = [ ] catcoldtypes = [ ] catcolnames = [ ] catcolunits = [ ] for xkcol in xk : if xkcol in catdefkeys : xkcolind = catdefkeys . index ( xkcol ) catcolinds . append ( xkcolind ) catcoldtypes . append ( catdefdtypes [ xkcolind ] ) catcolnames . append ( catdefnames [ xkcolind ] ) catcolunits . append ( catdefunits [ xkcolind ] ) return ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits )
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
5106	def _current_color ( self , which = 0 ) : if which == 1 : color = self . colors [ 'edge_loop_color' ] elif which == 2 : color = self . colors [ 'vertex_color' ] else : div = self . coloring_sensitivity * self . num_servers + 1. tmp = 1. - min ( self . num_system / div , 1 ) if self . edge [ 0 ] == self . edge [ 1 ] : color = [ i * tmp for i in self . colors [ 'vertex_fill_color' ] ] color [ 3 ] = 1.0 else : color = [ i * tmp for i in self . colors [ 'edge_color' ] ] color [ 3 ] = 1 / 2. return color
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
7413	def plot ( self ) : if self . results_table == None : return "no results found" else : bb = self . results_table . sort_values ( by = [ "ABCD" , "ACBD" ] , ascending = [ False , True ] , ) import toyplot c = toyplot . Canvas ( width = 600 , height = 200 ) a = c . cartesian ( ) m = a . bars ( bb ) return c , a , m
4477	def norm_remote_path ( path ) : path = os . path . normpath ( path ) if path . startswith ( os . path . sep ) : return path [ 1 : ] else : return path
4335	def oops ( self ) : effect_args = [ 'oops' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'oops' ) return self
10933	def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
224	async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type == "websocket.connect" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ "type" ] assert message_type in { "websocket.receive" , "websocket.disconnect" } if message_type == "websocket.disconnect" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call "receive" once a disconnect message has been received.' )
7906	def __groupchat_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : self . __logger . debug ( "groupchat message from unknown source" ) return False rs . process_groupchat_message ( stanza ) return True
1344	def _get_output ( self , a , image ) : sd = np . square ( self . _input_images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) if mses [ index ] > 0 : raise ValueError ( 'No precomputed output image for this image' ) return self . _output_images [ index ]
11291	def json ( request , * args , ** kwargs ) : params = dict ( request . GET . items ( ) ) callback = params . pop ( 'callback' , None ) url = params . pop ( 'url' , None ) if not url : return HttpResponseBadRequest ( 'Required parameter missing: URL' ) try : provider = oembed . site . provider_for_url ( url ) if not provider . provides : raise OEmbedMissingEndpoint ( ) except OEmbedMissingEndpoint : raise Http404 ( 'No provider found for %s' % url ) query = dict ( [ ( smart_str ( k ) , smart_str ( v ) ) for k , v in params . items ( ) if v ] ) try : resource = oembed . site . embed ( url , ** query ) except OEmbedException , e : raise Http404 ( 'Error embedding %s: %s' % ( url , str ( e ) ) ) response = HttpResponse ( mimetype = 'application/json' ) json = resource . json if callback : response . write ( '%s(%s)' % ( defaultfilters . force_escape ( callback ) , json ) ) else : response . write ( json ) return response
3288	def _get_repo_info ( self , environ , rev , reload = False ) : caches = environ . setdefault ( "wsgidav.hg.cache" , { } ) if caches . get ( compat . to_native ( rev ) ) is not None : _logger . debug ( "_get_repo_info(%s): cache hit." % rev ) return caches [ compat . to_native ( rev ) ] start_time = time . time ( ) self . ui . pushbuffer ( ) commands . manifest ( self . ui , self . repo , rev ) res = self . ui . popbuffer ( ) files = [ ] dirinfos = { } filedict = { } for file in res . split ( "\n" ) : if file . strip ( ) == "" : continue file = file . replace ( "\\" , "/" ) parents = file . split ( "/" ) if len ( parents ) >= 1 : p1 = "" for i in range ( 0 , len ( parents ) - 1 ) : p2 = parents [ i ] dir = dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) if p2 not in dir [ 0 ] : dir [ 0 ] . append ( p2 ) if p1 == "" : p1 = p2 else : p1 = "%s/%s" % ( p1 , p2 ) dirinfos . setdefault ( p1 , ( [ ] , [ ] ) ) [ 1 ] . append ( parents [ - 1 ] ) filedict [ file ] = True files . sort ( ) cache = { "files" : files , "dirinfos" : dirinfos , "filedict" : filedict } caches [ compat . to_native ( rev ) ] = cache _logger . info ( "_getRepoInfo(%s) took %.3f" % ( rev , time . time ( ) - start_time ) ) return cache
12628	def get_all_files ( folder ) : for path , dirlist , filelist in os . walk ( folder ) : for fn in filelist : yield op . join ( path , fn )
8399	def transform ( x ) : try : x = date2num ( x ) except AttributeError : x = [ pd . Timestamp ( item ) for item in x ] x = date2num ( x ) return x
3514	def chartbeat_top ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ChartbeatTopNode ( )
1696	def clone ( self , num_clones ) : retval = [ ] for i in range ( num_clones ) : retval . append ( self . repartition ( self . get_num_partitions ( ) ) ) return retval
13365	def gather ( obj ) : if hasattr ( obj , '__distob_gather__' ) : return obj . __distob_gather__ ( ) elif ( isinstance ( obj , collections . Sequence ) and not isinstance ( obj , string_types ) ) : return [ gather ( subobj ) for subobj in obj ] else : return obj
1620	def FindNextMultiLineCommentStart ( lines , lineix ) : while lineix < len ( lines ) : if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : return lineix lineix += 1 return len ( lines )
2752	def get_all_domains ( self ) : data = self . get_data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( ** jsoned ) domain . token = self . token domains . append ( domain ) return domains
5824	def add_descriptor ( self , descriptor , role = 'ignore' , group_by_key = False ) : descriptor . validate ( ) if descriptor . key in self . configuration [ "roles" ] : raise ValueError ( "Cannot add a descriptor with the same name twice" ) self . configuration [ 'descriptors' ] . append ( descriptor . as_dict ( ) ) self . configuration [ "roles" ] [ descriptor . key ] = role if group_by_key : self . configuration [ "group_by" ] . append ( descriptor . key )
9741	async def await_event ( self , event = None , timeout = None ) : if self . event_future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait_for ( self . _wait_loop ( event ) , timeout ) return result
8797	def apply_rules ( self , device_id , mac_address , rules ) : LOG . info ( "Applying security group rules for device %s with MAC %s" % ( device_id , mac_address ) ) rule_dict = { SECURITY_GROUP_RULE_KEY : rules } redis_key = self . vif_key ( device_id , mac_address ) self . set_field ( redis_key , SECURITY_GROUP_HASH_ATTR , rule_dict ) self . set_field_raw ( redis_key , SECURITY_GROUP_ACK , False )
3284	def handle_error ( self , request , client_address ) : ei = sys . exc_info ( ) e = ei [ 1 ] if e . args [ 0 ] in ( 10053 , 10054 ) : _logger . error ( "*** Caught socket.error: {}" . format ( e ) ) return _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( "<{}> Exception happened during processing of request from {}" . format ( threading . currentThread ( ) . ident , client_address ) ) _logger . error ( client_address , file = sys . stderr ) traceback . print_exc ( ) _logger . error ( "-" * 40 , file = sys . stderr ) _logger . error ( request , file = sys . stderr )
941	def _reportCommandLineUsageErrorAndExit ( parser , message ) : print parser . get_usage ( ) print message sys . exit ( 1 )
8423	def hls_palette ( n_colors = 6 , h = .01 , l = .6 , s = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues -= hues . astype ( int ) palette = [ colorsys . hls_to_rgb ( h_i , l , s ) for h_i in hues ] return palette
1862	def SCAS ( cpu , dest , src ) : dest_reg = dest . reg mem_reg = src . mem . base size = dest . size arg0 = dest . read ( ) arg1 = src . read ( ) res = arg0 - arg1 cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment )
4226	def _data_root_Linux ( ) : fallback = os . path . expanduser ( '~/.local/share' ) root = os . environ . get ( 'XDG_DATA_HOME' , None ) or fallback return os . path . join ( root , 'python_keyring' )
586	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn classifier_indexes = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . _autoDetectWaitRecords ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None classifier . setParameter ( 'inferenceMode' , True ) classifier . setParameter ( 'learningMode' , False ) classifier . getSelf ( ) . compute ( inputs , outputs ) classifier . setParameter ( 'learningMode' , True ) classifier_distances = classifier . getSelf ( ) . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = classifier . getSelf ( ) . getCategoryList ( ) [ indexID ] return category return None
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
1525	def to_table ( result ) : max_count = 20 table , count = [ ] , 0 for role , envs_topos in result . items ( ) : for env , topos in envs_topos . items ( ) : for topo in topos : count += 1 if count > max_count : continue else : table . append ( [ role , env , topo ] ) header = [ 'role' , 'env' , 'topology' ] rest_count = 0 if count <= max_count else count - max_count return table , header , rest_count
11596	def _rc_keys ( self , pattern = '*' ) : "Returns a list of keys matching ``pattern``" result = [ ] for alias , redisent in iteritems ( self . redises ) : if alias . find ( '_slave' ) == - 1 : continue result . extend ( redisent . keys ( pattern ) ) return result
3101	def _SendRecv ( ) : port = int ( os . getenv ( DEVSHELL_ENV , 0 ) ) if port == 0 : raise NoDevshellServer ( ) sock = socket . socket ( ) sock . connect ( ( 'localhost' , port ) ) data = CREDENTIAL_INFO_REQUEST_JSON msg = '{0}\n{1}' . format ( len ( data ) , data ) sock . sendall ( _helpers . _to_bytes ( msg , encoding = 'utf-8' ) ) header = sock . recv ( 6 ) . decode ( ) if '\n' not in header : raise CommunicationError ( 'saw no newline in the first 6 bytes' ) len_str , json_str = header . split ( '\n' , 1 ) to_read = int ( len_str ) - len ( json_str ) if to_read > 0 : json_str += sock . recv ( to_read , socket . MSG_WAITALL ) . decode ( ) return CredentialInfoResponse ( json_str )
6037	def scaled_array_2d_from_array_1d ( self , array_1d ) : return scaled_array . ScaledSquarePixelArray ( array = self . array_2d_from_array_1d ( array_1d ) , pixel_scale = self . mask . pixel_scale , origin = self . mask . origin )
10960	def scramble_positions ( p , delete_frac = 0.1 ) : probs = [ 1 - delete_frac , delete_frac ] m = np . random . choice ( [ True , False ] , p . shape [ 0 ] , p = probs ) jumble = np . random . randn ( m . sum ( ) , 3 ) return p [ m ] + jumble
6184	def get_git_version ( git_path = None ) : if git_path is None : git_path = GIT_PATH git_version = check_output ( [ git_path , "--version" ] ) . split ( ) [ 2 ] return git_version
4791	def is_upper ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . upper ( ) : self . _err ( 'Expected <%s> to contain only uppercase chars, but did not.' % self . val ) return self
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
6114	def resized_scaled_array_from_array ( self , new_shape , new_centre_pixels = None , new_centre_arcsec = None ) : if new_centre_pixels is None and new_centre_arcsec is None : new_centre = ( - 1 , - 1 ) elif new_centre_pixels is not None and new_centre_arcsec is None : new_centre = new_centre_pixels elif new_centre_pixels is None and new_centre_arcsec is not None : new_centre = self . arc_second_coordinates_to_pixel_coordinates ( arc_second_coordinates = new_centre_arcsec ) else : raise exc . DataException ( 'You have supplied two centres (pixels and arc-seconds) to the resize scaled' 'array function' ) return self . new_with_array ( array = array_util . resized_array_2d_from_array_2d_and_resized_shape ( array_2d = self , resized_shape = new_shape , origin = new_centre ) )
2220	def lookup ( self , data ) : for func in self . lazy_init : func ( ) for type , func in self . func_registry . items ( ) : if isinstance ( data , type ) : return func
3147	def _iterate ( self , url , ** queryparams ) : if 'fields' in queryparams : if 'total_items' not in queryparams [ 'fields' ] . split ( ',' ) : queryparams [ 'fields' ] += ',total_items' queryparams . pop ( "offset" , None ) queryparams . pop ( "count" , None ) result = self . _mc_client . _get ( url = url , offset = 0 , count = 1000 , ** queryparams ) total = result [ 'total_items' ] if total > 1000 : for offset in range ( 1 , int ( total / 1000 ) + 1 ) : result = merge_results ( result , self . _mc_client . _get ( url = url , offset = int ( offset * 1000 ) , count = 1000 , ** queryparams ) ) return result else : return result
8989	def first_produced_mesh ( self ) : for instruction in self . instructions : if instruction . produces_meshes ( ) : return instruction . first_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
4571	def adapt_animation_layout ( animation ) : layout = animation . layout required = getattr ( animation , 'LAYOUT_CLASS' , None ) if not required or isinstance ( layout , required ) : return msg = LAYOUT_WARNING % ( type ( animation ) . __name__ , required . __name__ , type ( layout ) . __name__ ) setter = layout . set adaptor = None if required is strip . Strip : if isinstance ( layout , matrix . Matrix ) : width = layout . width def adaptor ( pixel , color = None ) : y , x = divmod ( pixel , width ) setter ( x , y , color or BLACK ) elif isinstance ( layout , cube . Cube ) : lx , ly = layout . x , layout . y def adaptor ( pixel , color = None ) : yz , x = divmod ( pixel , lx ) z , y = divmod ( yz , ly ) setter ( x , y , z , color or BLACK ) elif isinstance ( layout , circle . Circle ) : def adaptor ( pixel , color = None ) : layout . _set_base ( pixel , color or BLACK ) elif required is matrix . Matrix : if isinstance ( layout , strip . Strip ) : width = animation . width def adaptor ( x , y , color = None ) : setter ( x + y * width , color or BLACK ) if not adaptor : raise ValueError ( msg ) log . warning ( msg ) animation . layout . set = adaptor
10611	def _calculate_H_coal ( self , T ) : m_C = 0 m_H = 0 m_O = 0 m_N = 0 m_S = 0 H = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) if stoich . element_mass_fraction ( compound , 'C' ) == 1.0 : m_C += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'H' ) == 1.0 : m_H += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'O' ) == 1.0 : m_O += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'N' ) == 1.0 : m_N += self . _compound_masses [ index ] elif stoich . element_mass_fraction ( compound , 'S' ) == 1.0 : m_S += self . _compound_masses [ index ] else : dH = thermo . H ( compound , T , self . _compound_masses [ index ] ) H += dH m_total = y_C + y_H + y_O + y_N + y_S y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 Hdaf = H - H298 + self . _DH298 Hdaf *= m_total H += Hdaf return H
10787	def add_subtract_misfeatured_tile ( st , tile , rad = 'calc' , max_iter = 3 , invert = 'guess' , max_allowed_remove = 20 , minmass = None , use_tp = False , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) if invert == 'guess' : invert = guess_invert ( st ) initial_error = np . copy ( st . error ) rinds = np . nonzero ( tile . contains ( st . obj_get_positions ( ) ) ) [ 0 ] if rinds . size >= max_allowed_remove : CLOG . fatal ( 'Misfeatured region too large!' ) raise RuntimeError elif rinds . size >= max_allowed_remove / 2 : CLOG . warn ( 'Large misfeatured regions.' ) elif rinds . size > 0 : rpos , rrad = st . obj_remove_particle ( rinds ) n_added = - rinds . size added_poses = [ ] for _ in range ( max_iter ) : if invert : im = 1 - st . residuals [ tile . slicer ] else : im = st . residuals [ tile . slicer ] guess , _ = _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp ) accepts , poses = check_add_particles ( st , guess + tile . l , rad = rad , do_opt = True , ** kwargs ) added_poses . extend ( poses ) n_added += accepts if accepts == 0 : break else : CLOG . warn ( 'Runaway adds or insufficient max_iter' ) ainds = [ ] for p in added_poses : ainds . append ( st . obj_closest_particle ( p ) ) if len ( ainds ) > max_allowed_remove : for i in range ( 0 , len ( ainds ) , max_allowed_remove ) : opt . do_levmarq_particles ( st , np . array ( ainds [ i : i + max_allowed_remove ] ) , include_rad = True , max_iter = 3 ) elif len ( ainds ) > 0 : opt . do_levmarq_particles ( st , ainds , include_rad = True , max_iter = 3 ) did_something = ( rinds . size > 0 ) or ( len ( ainds ) > 0 ) if did_something & ( st . error > initial_error ) : CLOG . info ( 'Failed addsub, Tile {} -> {}' . format ( tile . l . tolist ( ) , tile . r . tolist ( ) ) ) if len ( ainds ) > 0 : _ = st . obj_remove_particle ( ainds ) if rinds . size > 0 : for p , r in zip ( rpos . reshape ( - 1 , 3 ) , rrad . reshape ( - 1 ) ) : _ = st . obj_add_particle ( p , r ) n_added = 0 ainds = [ ] return n_added , ainds
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
4132	def split_code_and_text_blocks ( source_file ) : docstring , rest_of_content = get_docstring_and_rest ( source_file ) blocks = [ ( 'text' , docstring ) ] pattern = re . compile ( r'(?P<header_line>^#{20,}.*)\s(?P<text_content>(?:^#.*\s)*)' , flags = re . M ) pos_so_far = 0 for match in re . finditer ( pattern , rest_of_content ) : match_start_pos , match_end_pos = match . span ( ) code_block_content = rest_of_content [ pos_so_far : match_start_pos ] text_content = match . group ( 'text_content' ) sub_pat = re . compile ( '^#' , flags = re . M ) text_block_content = dedent ( re . sub ( sub_pat , '' , text_content ) ) if code_block_content . strip ( ) : blocks . append ( ( 'code' , code_block_content ) ) if text_block_content . strip ( ) : blocks . append ( ( 'text' , text_block_content ) ) pos_so_far = match_end_pos remaining_content = rest_of_content [ pos_so_far : ] if remaining_content . strip ( ) : blocks . append ( ( 'code' , remaining_content ) ) return blocks
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
13000	def calculate_diagram_ranges ( data ) : data = round_arr_teff_luminosity ( data ) temps = data [ 'temp' ] x_range = [ 1.05 * np . amax ( temps ) , .95 * np . amin ( temps ) ] lums = data [ 'lum' ] y_range = [ .50 * np . amin ( lums ) , 2 * np . amax ( lums ) ] return ( x_range , y_range )
11670	def _get_Ks ( self ) : "Ks as an array and type-checked." Ks = as_integer_type ( self . Ks ) if Ks . ndim != 1 : raise TypeError ( "Ks should be 1-dim, got shape {}" . format ( Ks . shape ) ) if Ks . min ( ) < 1 : raise ValueError ( "Ks should be positive; got {}" . format ( Ks . min ( ) ) ) return Ks
2320	def check_cuda_devices ( ) : import ctypes CUDA_SUCCESS = 0 libnames = ( 'libcuda.so' , 'libcuda.dylib' , 'cuda.dll' ) for libname in libnames : try : cuda = ctypes . CDLL ( libname ) except OSError : continue else : break else : return 0 nGpus = ctypes . c_int ( ) error_str = ctypes . c_char_p ( ) result = cuda . cuInit ( 0 ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 result = cuda . cuDeviceGetCount ( ctypes . byref ( nGpus ) ) if result != CUDA_SUCCESS : cuda . cuGetErrorString ( result , ctypes . byref ( error_str ) ) return 0 return nGpus . value
2184	def clear ( self , cfgstr = None ) : data_fpath = self . get_fpath ( cfgstr ) if self . verbose > 0 : self . log ( '[cacher] clear cache' ) if exists ( data_fpath ) : if self . verbose > 0 : self . log ( '[cacher] removing {}' . format ( data_fpath ) ) os . remove ( data_fpath ) meta_fpath = data_fpath + '.meta' if exists ( meta_fpath ) : os . remove ( meta_fpath ) else : if self . verbose > 0 : self . log ( '[cacher] ... nothing to clear' )
3201	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id ) , data = data )
465	def set_gpu_fraction ( gpu_fraction = 0.3 ) : tl . logging . info ( "[TL]: GPU MEM Fraction %f" % gpu_fraction ) gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) return sess
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
2430	def set_spdx_doc_uri ( self , doc , spdx_doc_uri ) : if validations . validate_doc_namespace ( spdx_doc_uri ) : doc . ext_document_references [ - 1 ] . spdx_document_uri = spdx_doc_uri else : raise SPDXValueError ( 'Document::ExternalDocumentRef' )
7672	def validate ( self , strict = True ) : valid = True try : jsonschema . validate ( self . __json_light__ , schema . JAMS_SCHEMA ) for ann in self . annotations : if isinstance ( ann , Annotation ) : valid &= ann . validate ( strict = strict ) else : msg = '{} is not a well-formed JAMS Annotation' . format ( ann ) valid = False if strict : raise SchemaError ( msg ) else : warnings . warn ( str ( msg ) ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
12109	def _review_all ( self , launchers ) : if self . launch_args is not None : proceed = self . review_args ( self . launch_args , show_repr = True , heading = 'Meta Arguments' ) if not proceed : return False reviewers = [ self . review_args , self . review_command , self . review_launcher ] for ( count , launcher ) in enumerate ( launchers ) : if not all ( reviewer ( launcher ) for reviewer in reviewers ) : print ( "\n == Aborting launch ==" ) return False if len ( launchers ) != 1 and count < len ( launchers ) - 1 : skip_remaining = self . input_options ( [ 'Y' , 'n' , 'quit' ] , '\nSkip remaining reviews?' , default = 'y' ) if skip_remaining == 'y' : break elif skip_remaining == 'quit' : return False if self . input_options ( [ 'y' , 'N' ] , 'Execute?' , default = 'n' ) != 'y' : return False else : return self . _launch_all ( launchers )
4490	def init ( args ) : config = config_from_file ( ) config_ = configparser . ConfigParser ( ) config_ . add_section ( 'osf' ) if 'username' not in config . keys ( ) : config_ . set ( 'osf' , 'username' , '' ) else : config_ . set ( 'osf' , 'username' , config [ 'username' ] ) if 'project' not in config . keys ( ) : config_ . set ( 'osf' , 'project' , '' ) else : config_ . set ( 'osf' , 'project' , config [ 'project' ] ) print ( 'Provide a username for the config file [current username: {}]:' . format ( config_ . get ( 'osf' , 'username' ) ) ) username = input ( ) if username : config_ . set ( 'osf' , 'username' , username ) print ( 'Provide a project for the config file [current project: {}]:' . format ( config_ . get ( 'osf' , 'project' ) ) ) project = input ( ) if project : config_ . set ( 'osf' , 'project' , project ) cfgfile = open ( ".osfcli.config" , "w" ) config_ . write ( cfgfile ) cfgfile . close ( )
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
2200	def ensure_app_data_dir ( appname , * args ) : from ubelt import util_path dpath = get_app_data_dir ( appname , * args ) util_path . ensuredir ( dpath ) return dpath
6521	def directories ( self , filters = None , containing = None ) : filters = compile_masks ( filters or [ r'.*' ] ) contains = compile_masks ( containing ) for dirname , files in iteritems ( self . _found ) : relpath = text_type ( Path ( dirname ) . relative_to ( self . base_path ) ) if matches_masks ( relpath , filters ) : if not contains or self . _contains ( files , contains ) : yield dirname
8235	def split_complementary ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) clr = clr . complement colors . append ( clr . rotate_ryb ( - 30 ) . lighten ( 0.1 ) ) colors . append ( clr . rotate_ryb ( 30 ) . lighten ( 0.1 ) ) return colors
3470	def get_coefficient ( self , metabolite_id ) : if isinstance ( metabolite_id , Metabolite ) : return self . _metabolites [ metabolite_id ] _id_to_metabolites = { m . id : m for m in self . _metabolites } return self . _metabolites [ _id_to_metabolites [ metabolite_id ] ]
13620	def create_patch ( self , from_tag , to_tag ) : return str ( self . _git . diff ( '{}..{}' . format ( from_tag , to_tag ) , _tty_out = False ) )
11495	def list_users ( self , limit = 20 ) : parameters = dict ( ) parameters [ 'limit' ] = limit response = self . request ( 'midas.user.list' , parameters ) return response
10915	def _check_groups ( s , groups ) : ans = [ ] for g in groups : ans . extend ( g ) if np . unique ( ans ) . size != np . size ( ans ) : return False elif np . unique ( ans ) . size != s . obj_get_positions ( ) . shape [ 0 ] : return False else : return ( np . arange ( s . obj_get_radii ( ) . size ) == np . sort ( ans ) ) . all ( )
1569	def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , failing_task_id = self . get_task_id ( ) , fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_fail ( bolt_fail_info )
2377	def list_rules ( self ) : for rule in sorted ( self . all_rules , key = lambda rule : rule . name ) : print ( rule ) if self . args . verbose : for line in rule . doc . split ( "\n" ) : print ( " " , line )
7089	def jd_corr ( jd , ra , dec , obslon = None , obslat = None , obsalt = None , jd_type = 'bjd' ) : if not HAVEKERNEL : LOGERROR ( 'no JPL kernel available, can\'t continue!' ) return rarad = np . radians ( ra ) decrad = np . radians ( dec ) cosra = np . cos ( rarad ) sinra = np . sin ( rarad ) cosdec = np . cos ( decrad ) sindec = np . sin ( decrad ) src_unitvector = np . array ( [ cosdec * cosra , cosdec * sinra , sindec ] ) if ( obslon is None ) or ( obslat is None ) or ( obsalt is None ) : t = astime . Time ( jd , scale = 'utc' , format = 'jd' ) else : t = astime . Time ( jd , scale = 'utc' , format = 'jd' , location = ( '%.5fd' % obslon , '%.5fd' % obslat , obsalt ) ) barycenter_earthmoon = jplkernel [ 0 , 3 ] . compute ( t . tdb . jd ) moonvector = ( jplkernel [ 3 , 301 ] . compute ( t . tdb . jd ) - jplkernel [ 3 , 399 ] . compute ( t . tdb . jd ) ) pos_earth = ( barycenter_earthmoon - moonvector * 1.0 / ( 1.0 + EMRAT ) ) if jd_type == 'bjd' : correction_seconds = np . dot ( pos_earth . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY elif jd_type == 'hjd' : pos_sun = jplkernel [ 0 , 10 ] . compute ( t . tdb . jd ) sun_earth_vec = pos_earth - pos_sun correction_seconds = np . dot ( sun_earth_vec . T , src_unitvector ) / CLIGHT_KPS correction_days = correction_seconds / SEC_P_DAY new_jd = t . tdb . jd + correction_days return new_jd
8392	def usable_class_name ( node ) : name = node . qname ( ) for prefix in [ "__builtin__." , "builtins." , "." ] : if name . startswith ( prefix ) : name = name [ len ( prefix ) : ] return name
13805	def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
9726	async def send_xml ( self , xml ) : return await asyncio . wait_for ( self . _protocol . send_command ( xml , command_type = QRTPacketType . PacketXML ) , timeout = self . _timeout , )
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
11523	def extract_dicommetadata ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'item' ] = item_id response = self . request ( 'midas.dicomextractor.extract' , parameters ) return response
1953	def input_from_cons ( constupl , datas ) : ' solve bytes in |datas| based on ' def make_chr ( c ) : try : return chr ( c ) except Exception : return c newset = constraints_to_constraintset ( constupl ) ret = '' for data in datas : for c in data : ret += make_chr ( solver . get_value ( newset , c ) ) return ret
7485	def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples
7376	async def prepare_request ( self , method , url , headers = None , skip_params = False , proxy = None , ** kwargs ) : if method . lower ( ) == "post" : key = 'data' else : key = 'params' if key in kwargs and not skip_params : request_params = { key : kwargs . pop ( key ) } else : request_params = { } request_params . update ( dict ( method = method . upper ( ) , url = url ) ) coro = self . sign ( ** request_params , skip_params = skip_params , headers = headers ) request_params [ 'headers' ] = await utils . execute ( coro ) request_params [ 'proxy' ] = proxy kwargs . update ( request_params ) return kwargs
12172	def genPNGs ( folder , files = None ) : if files is None : files = glob . glob ( folder + "/*.*" ) new = [ ] for fname in files : ext = os . path . basename ( fname ) . split ( "." ) [ - 1 ] . lower ( ) if ext in [ 'tif' , 'tiff' ] : if not os . path . exists ( fname + ".png" ) : print ( " -- converting %s to PNG..." % os . path . basename ( fname ) ) cm . image_convert ( fname ) new . append ( fname ) else : pass return new
5135	def minimal_random_graph ( num_vertices , seed = None , ** kwargs ) : if isinstance ( seed , numbers . Integral ) : np . random . seed ( seed ) points = np . random . random ( ( num_vertices , 2 ) ) * 10 edges = [ ] for k in range ( num_vertices - 1 ) : for j in range ( k + 1 , num_vertices ) : v = points [ k ] - points [ j ] edges . append ( ( k , j , v [ 0 ] ** 2 + v [ 1 ] ** 2 ) ) mytype = [ ( 'n1' , int ) , ( 'n2' , int ) , ( 'distance' , np . float ) ] edges = np . array ( edges , dtype = mytype ) edges = np . sort ( edges , order = 'distance' ) unionF = UnionFind ( [ k for k in range ( num_vertices ) ] ) g = nx . Graph ( ) for n1 , n2 , dummy in edges : unionF . union ( n1 , n2 ) g . add_edge ( n1 , n2 ) if unionF . nClusters == 1 : break pos = { j : p for j , p in enumerate ( points ) } g = QueueNetworkDiGraph ( g . to_directed ( ) ) g . set_pos ( pos ) return g
4972	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
5882	def nodes_to_check ( self , docs ) : nodes_to_check = [ ] for doc in docs : for tag in [ 'p' , 'pre' , 'td' ] : items = self . parser . getElementsByTag ( doc , tag = tag ) nodes_to_check += items return nodes_to_check
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
12506	def signed_session ( self , session = None ) : from sfctl . config import ( aad_metadata , aad_cache ) if session : session = super ( AdalAuthentication , self ) . signed_session ( session ) else : session = super ( AdalAuthentication , self ) . signed_session ( ) if self . no_verify : session . verify = False authority_uri , cluster_id , client_id = aad_metadata ( ) existing_token , existing_cache = aad_cache ( ) context = adal . AuthenticationContext ( authority_uri , cache = existing_cache ) new_token = context . acquire_token ( cluster_id , existing_token [ 'userId' ] , client_id ) header = "{} {}" . format ( "Bearer" , new_token [ 'accessToken' ] ) session . headers [ 'Authorization' ] = header return session
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
10087	def update ( self , * args , ** kwargs ) : super ( Deposit , self ) . update ( * args , ** kwargs )
8979	def _path ( self , path ) : mode , encoding = self . _mode_and_encoding_for_open ( ) with open ( path , mode , encoding = encoding ) as file : self . __dump_to_file ( file )
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
1384	def trigger_watches ( self ) : to_remove = [ ] for uid , callback in self . watches . items ( ) : try : callback ( self ) except Exception as e : Log . error ( "Caught exception while triggering callback: " + str ( e ) ) Log . debug ( traceback . format_exc ( ) ) to_remove . append ( uid ) for uid in to_remove : self . unregister_watch ( uid )
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
13635	def _splitHeaders ( headers ) : return [ cgi . parse_header ( value ) for value in chain . from_iterable ( s . split ( ',' ) for s in headers if s ) ]
2916	def get_dump ( self , indent = 0 , recursive = True ) : dbg = ( ' ' * indent * 2 ) dbg += '%s/' % self . id dbg += '%s:' % self . thread_id dbg += ' Task of %s' % self . get_name ( ) if self . task_spec . description : dbg += ' (%s)' % self . get_description ( ) dbg += ' State: %s' % self . get_state_name ( ) dbg += ' Children: %s' % len ( self . children ) if recursive : for child in self . children : dbg += '\n' + child . get_dump ( indent + 1 ) return dbg
6798	def set_root_login ( self , r ) : try : r . env . db_root_username = r . env . root_username except AttributeError : pass try : r . env . db_root_password = r . env . root_password except AttributeError : pass key = r . env . get ( 'db_host' ) if self . verbose : print ( 'db.set_root_login.key:' , key ) print ( 'db.set_root_logins:' , r . env . root_logins ) if key in r . env . root_logins : data = r . env . root_logins [ key ] if 'username' in data : r . env . db_root_username = data [ 'username' ] r . genv . db_root_username = data [ 'username' ] if 'password' in data : r . env . db_root_password = data [ 'password' ] r . genv . db_root_password = data [ 'password' ] else : msg = 'Warning: No root login entry found for host %s in role %s.' % ( r . env . get ( 'db_host' ) , self . genv . get ( 'ROLE' ) ) print ( msg , file = sys . stderr )
8195	def load ( self , id ) : self . clear ( ) self . add_node ( id , root = True ) for w , id2 in self . get_links ( id ) : self . add_edge ( id , id2 , weight = w ) if len ( self ) > self . max : break for w , id2 , links in self . get_cluster ( id ) : for id3 in links : self . add_edge ( id3 , id2 , weight = w ) self . add_edge ( id , id3 , weight = w ) if len ( self ) > self . max : break if self . event . clicked : g . add_node ( self . event . clicked )
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
4800	def is_directory ( self ) : self . exists ( ) if not os . path . isdir ( self . val ) : self . _err ( 'Expected <%s> to be a directory, but was not.' % self . val ) return self
12107	def cross_check_launchers ( self , launchers ) : if len ( launchers ) == 0 : raise Exception ( 'Empty launcher list' ) timestamps = [ launcher . timestamp for launcher in launchers ] if not all ( timestamps [ 0 ] == tstamp for tstamp in timestamps ) : raise Exception ( "Launcher timestamps not all equal. " "Consider setting timestamp explicitly." ) root_directories = [ ] for launcher in launchers : command = launcher . command args = launcher . args command . verify ( args ) root_directory = launcher . get_root_directory ( ) if os . path . isdir ( root_directory ) : raise Exception ( "Root directory already exists: %r" % root_directory ) if root_directory in root_directories : raise Exception ( "Each launcher requires a unique root directory" ) root_directories . append ( root_directory )
5971	def MD ( dirname = 'MD' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'MD_POSRES/md.gro' ) kwargs . setdefault ( 'qname' , 'MD_GMX' ) return _setup_MD ( dirname , ** kwargs )
4426	async def _play ( self , ctx , * , query : str ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) query = query . strip ( '<>' ) if not url_rx . match ( query ) : query = f'ytsearch:{query}' tracks = await self . bot . lavalink . get_tracks ( query ) if not tracks : return await ctx . send ( 'Nothing found!' ) embed = discord . Embed ( color = discord . Color . blurple ( ) ) if 'list' in query and 'ytsearch:' not in query : for track in tracks : player . add ( requester = ctx . author . id , track = track ) embed . title = 'Playlist enqueued!' embed . description = f'Imported {len(tracks)} tracks from the playlist!' await ctx . send ( embed = embed ) else : track_title = tracks [ 0 ] [ "info" ] [ "title" ] track_uri = tracks [ 0 ] [ "info" ] [ "uri" ] embed . title = "Track enqueued!" embed . description = f'[{track_title}]({track_uri})' player . add ( requester = ctx . author . id , track = tracks [ 0 ] ) if not player . is_playing : await player . play ( )
1345	def predictions ( self , image ) : return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 )
1823	def SETS ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) )
267	def vectorize ( func ) : def wrapper ( df , * args , ** kwargs ) : if df . ndim == 1 : return func ( df , * args , ** kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , ** kwargs ) return wrapper
5563	def output ( self ) : output_params = dict ( self . _raw [ "output" ] , grid = self . output_pyramid . grid , pixelbuffer = self . output_pyramid . pixelbuffer , metatiling = self . output_pyramid . metatiling ) if "path" in output_params : output_params . update ( path = absolute_path ( path = output_params [ "path" ] , base_dir = self . config_dir ) ) if "format" not in output_params : raise MapcheteConfigError ( "output format not specified" ) if output_params [ "format" ] not in available_output_formats ( ) : raise MapcheteConfigError ( "format %s not available in %s" % ( output_params [ "format" ] , str ( available_output_formats ( ) ) ) ) writer = load_output_writer ( output_params ) try : writer . is_valid_with_config ( output_params ) except Exception as e : logger . exception ( e ) raise MapcheteConfigError ( "driver %s not compatible with configuration: %s" % ( writer . METADATA [ "driver_name" ] , e ) ) return writer
3412	def _fix_type ( value ) : if isinstance ( value , string_types ) : return str ( value ) if isinstance ( value , float_ ) : return float ( value ) if isinstance ( value , bool_ ) : return bool ( value ) if isinstance ( value , set ) : return list ( value ) if isinstance ( value , dict ) : return OrderedDict ( ( key , value [ key ] ) for key in sorted ( value ) ) if value . __class__ . __name__ == "Formula" : return str ( value ) if value is None : return "" return value
1717	def replacement_template ( rep , source , span , npar ) : n = 0 res = '' while n < len ( rep ) - 1 : char = rep [ n ] if char == '$' : if rep [ n + 1 ] == '$' : res += '$' n += 2 continue elif rep [ n + 1 ] == '`' : res += source [ : span [ 0 ] ] n += 2 continue elif rep [ n + 1 ] == '\'' : res += source [ span [ 1 ] : ] n += 2 continue elif rep [ n + 1 ] in DIGS : dig = rep [ n + 1 ] if n + 2 < len ( rep ) and rep [ n + 2 ] in DIGS : dig += rep [ n + 2 ] num = int ( dig ) if not num or num > len ( npar ) : res += '$' + dig else : res += npar [ num - 1 ] if npar [ num - 1 ] else '' n += 1 + len ( dig ) continue res += char n += 1 if n < len ( rep ) : res += rep [ - 1 ] return res
3548	def _descriptor_changed ( self , descriptor ) : desc = descriptor_list ( ) . get ( descriptor ) if desc is not None : desc . _value_read . set ( )
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
5103	def draw_graph ( self , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not HAS_MATPLOTLIB : raise ImportError ( "Matplotlib is required to draw the graph." ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_kwargs , scatter_kwargs = self . lines_scatter_args ( ** mpl_kwargs ) edge_collection = LineCollection ( ** line_kwargs ) ax . add_collection ( edge_collection ) ax . scatter ( ** scatter_kwargs ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) else : ax . set_axis_bgcolor ( kwargs . get ( 'bgcolor' , [ 1 , 1 , 1 , 1 ] ) ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) if 'fname' in kwargs : new_kwargs = { k : v for k , v in kwargs . items ( ) if k in SAVEFIG_KWARGS } fig . savefig ( kwargs [ 'fname' ] , ** new_kwargs ) else : plt . ion ( ) plt . show ( )
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
7134	def run_cell ( self , cell ) : globals = self . ipy_shell . user_global_ns locals = self . ipy_shell . user_ns globals . update ( { "__ipy_scope__" : None , } ) try : with redirect_stdout ( self . stdout ) : self . run ( cell , globals , locals ) except : self . code_error = True if self . options . debug : raise BdbQuit finally : self . finalize ( )
5578	def driver_from_file ( input_file ) : file_ext = os . path . splitext ( input_file ) [ 1 ] . split ( "." ) [ 1 ] if file_ext not in _file_ext_to_driver ( ) : raise MapcheteDriverError ( "no driver could be found for file extension %s" % file_ext ) driver = _file_ext_to_driver ( ) [ file_ext ] if len ( driver ) > 1 : warnings . warn ( DeprecationWarning ( "more than one driver for file found, taking %s" % driver [ 0 ] ) ) return driver [ 0 ]
6316	def reload_programs ( self ) : print ( "Reloading programs:" ) for name , program in self . _programs . items ( ) : if getattr ( program , 'program' , None ) : print ( " - {}" . format ( program . meta . label ) ) program . program = resources . programs . load ( program . meta )
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
12027	def abfProtocol ( fname ) : f = open ( fname , 'rb' ) raw = f . read ( 30 * 1000 ) f . close ( ) raw = raw . decode ( "utf-8" , "ignore" ) raw = raw . split ( "Clampex" ) [ 1 ] . split ( ".pro" ) [ 0 ] protocol = os . path . basename ( raw ) protocolID = protocol . split ( " " ) [ 0 ] return protocolID
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
12585	def spatialimg_to_hdfpath ( file_path , spatial_img , h5path = None , append = True ) : if h5path is None : h5path = '/img' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' with h5py . File ( file_path , mode ) as f : try : h5img = f . create_group ( h5path ) spatialimg_to_hdfgroup ( h5img , spatial_img ) except ValueError as ve : raise Exception ( 'Error creating group ' + h5path ) from ve
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
6769	def install_apt ( self , fn = None , package_name = None , update = 0 , list_only = 0 ) : r = self . local_renderer assert self . genv [ ROLE ] apt_req_fqfn = fn or ( self . env . apt_requirments_fn and self . find_template ( self . env . apt_requirments_fn ) ) if not apt_req_fqfn : return [ ] assert os . path . isfile ( apt_req_fqfn ) lines = list ( self . env . apt_packages or [ ] ) for _ in open ( apt_req_fqfn ) . readlines ( ) : if _ . strip ( ) and not _ . strip ( ) . startswith ( '#' ) and ( not package_name or _ . strip ( ) == package_name ) : lines . extend ( _pkg . strip ( ) for _pkg in _ . split ( ' ' ) if _pkg . strip ( ) ) if list_only : return lines tmp_fn = r . write_temp_file ( '\n' . join ( lines ) ) apt_req_fqfn = tmp_fn if not self . genv . is_local : r . put ( local_path = tmp_fn , remote_path = tmp_fn ) apt_req_fqfn = self . genv . put_remote_path r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq update --fix-missing' ) r . sudo ( 'DEBIAN_FRONTEND=noninteractive apt-get -yq install `cat "%s" | tr "\\n" " "`' % apt_req_fqfn )
4233	def autodetect_url ( ) : for url in [ "http://routerlogin.net:5000" , "https://routerlogin.net" , "http://routerlogin.net" ] : try : r = requests . get ( url + "/soap/server_sa/" , headers = _get_soap_headers ( "Test:1" , "test" ) , verify = False ) if r . status_code == 200 : return url except requests . exceptions . RequestException : pass return None
5806	def parse_alert ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x15' : continue if len ( record_data ) != 2 : return None return ( int_from_bytes ( record_data [ 0 : 1 ] ) , int_from_bytes ( record_data [ 1 : 2 ] ) ) return None
9758	def restart ( ctx , copy , file , u ) : config = None update_code = None if file : config = rhea . read ( file ) if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) try : if copy : response = PolyaxonClient ( ) . experiment . copy ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was copied with id {}' . format ( response . id ) ) else : response = PolyaxonClient ( ) . experiment . restart ( user , project_name , _experiment , config = config , update_code = update_code ) Printer . print_success ( 'Experiment was restarted with id {}' . format ( response . id ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
5627	def read_json ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : return json . loads ( urlopen ( path ) . read ( ) . decode ( ) ) except HTTPError : raise FileNotFoundError ( "%s not found" , path ) elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return json . loads ( obj . get ( ) [ 'Body' ] . read ( ) . decode ( ) ) raise FileNotFoundError ( "%s not found" , path ) else : try : with open ( path , "r" ) as src : return json . loads ( src . read ( ) ) except : raise FileNotFoundError ( "%s not found" , path )
6792	def manage ( self , cmd , * args , ** kwargs ) : r = self . local_renderer environs = kwargs . pop ( 'environs' , '' ) . strip ( ) if environs : environs = ' ' . join ( 'export %s=%s;' % tuple ( _ . split ( '=' ) ) for _ in environs . split ( ',' ) ) environs = ' ' + environs + ' ' r . env . cmd = cmd r . env . SITE = r . genv . SITE or r . genv . default_site r . env . args = ' ' . join ( map ( str , args ) ) r . env . kwargs = ' ' . join ( ( '--%s' % _k if _v in ( True , 'True' ) else '--%s=%s' % ( _k , _v ) ) for _k , _v in kwargs . items ( ) ) r . env . environs = environs if self . is_local : r . env . project_dir = r . env . local_project_dir r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE};{environs} cd {project_dir}; {manage_cmd} {cmd} {args} {kwargs}' )
10311	def prepare_c3 ( data : Union [ List [ Tuple [ str , int ] ] , Mapping [ str , int ] ] , y_axis_label : str = 'y' , x_axis_label : str = 'x' , ) -> str : if not isinstance ( data , list ) : data = sorted ( data . items ( ) , key = itemgetter ( 1 ) , reverse = True ) try : labels , values = zip ( * data ) except ValueError : log . info ( f'no values found for {x_axis_label}, {y_axis_label}' ) labels , values = [ ] , [ ] return json . dumps ( [ [ x_axis_label ] + list ( labels ) , [ y_axis_label ] + list ( values ) , ] )
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , ** kwargs ) return wrapper return decorator
2704	def collect_entities ( sent , ranks , stopwords , spacy_nlp ) : global DEBUG sent_text = " " . join ( [ w . raw for w in sent ] ) if DEBUG : print ( "sent:" , sent_text ) for ent in spacy_nlp ( sent_text ) . ents : if DEBUG : print ( "NER:" , ent . label_ , ent . text ) if ( ent . label_ not in [ "CARDINAL" ] ) and ( ent . text . lower ( ) not in stopwords ) : w_ranks , w_ids = find_entity ( sent , ranks , ent . text . split ( " " ) , 0 ) if w_ranks and w_ids : rl = RankedLexeme ( text = ent . text . lower ( ) , rank = w_ranks , ids = w_ids , pos = "np" , count = 1 ) if DEBUG : print ( rl ) yield rl
7277	def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = "%s %s %s %s" % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , str ( y2 ) ) self . _player_interface . VideoPos ( ObjectPath ( '/not/used' ) , String ( position ) )
5225	def flatten ( iterable , maps = None , unique = False ) -> list : if iterable is None : return [ ] if maps is None : maps = dict ( ) if isinstance ( iterable , ( str , int , float ) ) : return [ maps . get ( iterable , iterable ) ] else : x = [ maps . get ( item , item ) for item in _to_gen_ ( iterable ) ] return list ( set ( x ) ) if unique else x
6527	def get_tools ( ) : if not hasattr ( get_tools , '_CACHE' ) : get_tools . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.tools' ) : try : get_tools . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load tool "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_tools . _CACHE
7820	def flush ( self , dispatch = True ) : if dispatch : while True : event = self . dispatch ( False ) if event in ( None , QUIT ) : return event else : while True : try : self . queue . get ( False ) except Queue . Empty : return None
11111	def synchronize ( self , verbose = False ) : if self . __path is None : return for dirPath in sorted ( list ( self . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , dirPath ) if os . path . isdir ( realPath ) : continue if verbose : warnings . warn ( "%s directory is missing" % realPath ) keys = dirPath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is not None : dict . pop ( dirs , keys [ - 1 ] , None ) for filePath in sorted ( list ( self . walk_files_relative_path ( ) ) ) : realPath = os . path . join ( self . __path , filePath ) if os . path . isfile ( realPath ) : continue if verbose : warnings . warn ( "%s file is missing" % realPath ) keys = filePath . split ( os . sep ) dirInfoDict = self for idx in range ( len ( keys ) - 1 ) : dirs = dict . get ( dirInfoDict , 'directories' , None ) if dirs is None : break dirInfoDict = dict . get ( dirs , keys [ idx ] , None ) if dirInfoDict is None : break if dirInfoDict is not None : files = dict . get ( dirInfoDict , 'files' , None ) if files is not None : dict . pop ( files , keys [ - 1 ] , None )
8875	def allele_frequency ( expec ) : r expec = asarray ( expec , float ) if expec . ndim != 2 : raise ValueError ( "Expectation matrix must be bi-dimensional." ) ploidy = expec . shape [ - 1 ] return expec . sum ( - 2 ) / ploidy
6315	def _add_resource_descriptions_to_pools ( self , meta_list ) : if not meta_list : return for meta in meta_list : getattr ( resources , meta . resource_type ) . add ( meta )
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
5413	def lookup_job_tasks ( self , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user_ids = None if user_ids == { '*' } else user_ids job_ids = None if job_ids == { '*' } else job_ids job_names = None if job_names == { '*' } else job_names task_ids = None if task_ids == { '*' } else task_ids task_attempts = None if task_attempts == { '*' } else task_attempts if labels or create_time_min or create_time_max : raise NotImplementedError ( 'Lookup by labels and create_time not yet supported by stub.' ) operations = [ x for x in self . _operations if ( ( not statuses or x . get_field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user_ids or x . get_field ( 'user' , None ) in user_ids ) and ( not job_ids or x . get_field ( 'job-id' , None ) in job_ids ) and ( not job_names or x . get_field ( 'job-name' , None ) in job_names ) and ( not task_ids or x . get_field ( 'task-id' , None ) in task_ids ) and ( not task_attempts or x . get_field ( 'task-attempt' , None ) in task_attempts ) ) ] if max_tasks > 0 : operations = operations [ : max_tasks ] return operations
7014	def concatenate_textlcs ( lclist , sortby = 'rjd' , normalize = True ) : lcdict = read_hatpi_textlc ( lclist [ 0 ] ) lccounter = 0 lcdict [ 'concatenated' ] = { lccounter : os . path . abspath ( lclist [ 0 ] ) } lcdict [ 'lcn' ] = np . full_like ( lcdict [ 'rjd' ] , lccounter ) if normalize : for col in MAGCOLS : if col in lcdict : thismedval = np . nanmedian ( lcdict [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : lcdict [ col ] = lcdict [ col ] / thismedval else : lcdict [ col ] = lcdict [ col ] - thismedval for lcf in lclist [ 1 : ] : thislcd = read_hatpi_textlc ( lcf ) if thislcd [ 'columns' ] != lcdict [ 'columns' ] : LOGERROR ( 'file %s does not have the ' 'same columns as first file %s, skipping...' % ( lcf , lclist [ 0 ] ) ) continue else : LOGINFO ( 'adding %s (ndet: %s) to %s (ndet: %s)' % ( lcf , thislcd [ 'objectinfo' ] [ 'ndet' ] , lclist [ 0 ] , lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size ) ) lccounter = lccounter + 1 lcdict [ 'concatenated' ] [ lccounter ] = os . path . abspath ( lcf ) lcdict [ 'lcn' ] = np . concatenate ( ( lcdict [ 'lcn' ] , np . full_like ( thislcd [ 'rjd' ] , lccounter ) ) ) for col in lcdict [ 'columns' ] : if normalize and col in MAGCOLS : thismedval = np . nanmedian ( thislcd [ col ] ) if col in ( 'ifl1' , 'ifl2' , 'ifl3' ) : thislcd [ col ] = thislcd [ col ] / thismedval else : thislcd [ col ] = thislcd [ col ] - thismedval lcdict [ col ] = np . concatenate ( ( lcdict [ col ] , thislcd [ col ] ) ) lcdict [ 'objectinfo' ] [ 'ndet' ] = lcdict [ lcdict [ 'columns' ] [ 0 ] ] . size lcdict [ 'objectinfo' ] [ 'stations' ] = [ 'HP%s' % x for x in np . unique ( lcdict [ 'stf' ] ) . tolist ( ) ] lcdict [ 'nconcatenated' ] = lccounter + 1 if sortby and sortby in [ x [ 0 ] for x in COLDEFS ] : LOGINFO ( 'sorting concatenated light curve by %s...' % sortby ) sortind = np . argsort ( lcdict [ sortby ] ) for col in lcdict [ 'columns' ] : lcdict [ col ] = lcdict [ col ] [ sortind ] lcdict [ 'lcn' ] = lcdict [ 'lcn' ] [ sortind ] LOGINFO ( 'done. concatenated light curve has %s detections' % lcdict [ 'objectinfo' ] [ 'ndet' ] ) return lcdict
4211	def compatible_staticpath ( path ) : if VERSION >= ( 1 , 10 ) : return path try : from django . templatetags . static import static return static ( path ) except ImportError : pass try : return '%s/%s' % ( settings . STATIC_URL . rstrip ( '/' ) , path ) except AttributeError : pass try : return '%s/%s' % ( settings . PAGEDOWN_URL . rstrip ( '/' ) , path ) except AttributeError : pass return '%s/%s' % ( settings . MEDIA_URL . rstrip ( '/' ) , path )
12447	def render_to_string ( self ) : values = '' for key , value in self . items ( ) : values += '{}={};' . format ( key , value ) return values
11793	def forward_checking ( csp , var , value , assignment , removals ) : "Prune neighbor values inconsistent with var=value." for B in csp . neighbors [ var ] : if B not in assignment : for b in csp . curr_domains [ B ] [ : ] : if not csp . constraints ( var , value , B , b ) : csp . prune ( B , b , removals ) if not csp . curr_domains [ B ] : return False return True
450	def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha
1644	def GetPreviousNonBlankLine ( clean_lines , linenum ) : prevlinenum = linenum - 1 while prevlinenum >= 0 : prevline = clean_lines . elided [ prevlinenum ] if not IsBlankLine ( prevline ) : return ( prevline , prevlinenum ) prevlinenum -= 1 return ( '' , - 1 )
5921	def strip_fit ( self , ** kwargs ) : kwargs . setdefault ( 'fit' , 'rot+trans' ) kw_fit = { } for k in ( 'xy' , 'fit' , 'fitgroup' , 'input' ) : if k in kwargs : kw_fit [ k ] = kwargs . pop ( k ) kwargs [ 'input' ] = kwargs . pop ( 'strip_input' , [ 'Protein' ] ) kwargs [ 'force' ] = kw_fit [ 'force' ] = kwargs . pop ( 'force' , self . force ) paths = self . strip_water ( ** kwargs ) transformer_nowater = self . nowater [ paths [ 'xtc' ] ] return transformer_nowater . fit ( ** kw_fit )
345	def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) data = data . reshape ( shape ) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) return data logging . info ( "Load or Download {0} > {1}" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test
10633	def get_compound_afrs ( self ) : result = self . _compound_mfrs * 1.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) result [ index ] = stoich . amount ( compound , result [ index ] ) return result
8241	def compound ( clr , flip = False ) : def _wrap ( x , min , threshold , plus ) : if x - min < threshold : return x + plus else : return x - min d = 1 if flip : d = - 1 clr = color ( clr ) colors = colorlist ( clr ) c = clr . rotate_ryb ( 30 * d ) c . brightness = _wrap ( clr . brightness , 0.25 , 0.6 , 0.25 ) colors . append ( c ) c = clr . rotate_ryb ( 30 * d ) c . saturation = _wrap ( clr . saturation , 0.4 , 0.1 , 0.4 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) colors . append ( c ) c = clr . rotate_ryb ( 160 * d ) c . saturation = _wrap ( clr . saturation , 0.25 , 0.1 , 0.25 ) c . brightness = max ( 0.2 , clr . brightness ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.3 , 0.6 , 0.3 ) colors . append ( c ) c = clr . rotate_ryb ( 150 * d ) c . saturation = _wrap ( clr . saturation , 0.1 , 0.8 , 0.1 ) c . brightness = _wrap ( clr . brightness , 0.4 , 0.2 , 0.4 ) return colors
3273	def as_DAVError ( e ) : if isinstance ( e , DAVError ) : return e elif isinstance ( e , Exception ) : return DAVError ( HTTP_INTERNAL_ERROR , src_exception = e ) else : return DAVError ( HTTP_INTERNAL_ERROR , "{}" . format ( e ) )
5440	def get_variable_name ( self , name ) : if not name : name = '%s%s' % ( self . _auto_prefix , self . _auto_index ) self . _auto_index += 1 return name
11920	def get_object ( self ) : dataframe = self . filter_dataframe ( self . get_dataframe ( ) ) assert self . lookup_url_kwarg in self . kwargs , ( 'Expected view %s to be called with a URL keyword argument ' 'named "%s". Fix your URL conf, or set the `.lookup_field` ' 'attribute on the view correctly.' % ( self . __class__ . __name__ , self . lookup_url_kwarg ) ) try : obj = self . index_row ( dataframe ) except ( IndexError , KeyError , ValueError ) : raise Http404 self . check_object_permissions ( self . request , obj ) return obj
12795	def _get_password_url ( self ) : password_url = None if self . _settings [ "user" ] or self . _settings [ "authorization" ] : if self . _settings [ "url" ] : password_url = self . _settings [ "url" ] elif self . _settings [ "base_url" ] : password_url = self . _settings [ "base_url" ] return password_url
720	def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ "hsVersion" ] == "v2" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( "Unsupported hypersearch version \"%s\"" % ( searchJobParams [ "hsVersion" ] ) ) info = search . getOptimizationMetricInfo ( ) return info
2583	def load ( cls , config : Optional [ Config ] = None ) : if cls . _dfk is not None : raise RuntimeError ( 'Config has already been loaded' ) if config is None : cls . _dfk = DataFlowKernel ( Config ( ) ) else : cls . _dfk = DataFlowKernel ( config ) return cls . _dfk
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
591	def compute ( self , inputs , outputs ) : if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : import hotshot if self . _iterations == 10 : print "\n Collecting and sorting internal node profiling stats generated by hotshot..." stats = hotshot . stats . load ( "hotshot.stats" ) stats . strip_dirs ( ) stats . sort_stats ( 'time' , 'calls' ) stats . print_stats ( ) if self . _profileObj is None : print "\n Preparing to capture profile using hotshot..." if os . path . exists ( 'hotshot.stats' ) : os . remove ( 'hotshot.stats' ) self . _profileObj = hotshot . Profile ( "hotshot.stats" , 1 , 1 ) self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) else : self . _compute ( inputs , outputs )
9298	def paginate_query ( self , query , count , offset = None , sort = None ) : assert isinstance ( query , peewee . Query ) assert isinstance ( count , int ) assert isinstance ( offset , ( str , int , type ( None ) ) ) assert isinstance ( sort , ( list , set , tuple , type ( None ) ) ) fields = query . model . _meta . get_primary_keys ( ) if len ( fields ) == 0 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model without primary key' ) if len ( fields ) > 1 : raise peewee . ProgrammingError ( 'Cannot apply pagination on model with compound primary key' ) if offset is not None : query = query . where ( fields [ 0 ] >= offset ) order_bys = [ ] if sort : for field , direction in sort : if not isinstance ( direction , str ) : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) direction = direction . lower ( ) . strip ( ) if direction not in [ 'asc' , 'desc' ] : raise ValueError ( "Invalid sort direction on field '{}'" . format ( field ) ) order_by = peewee . SQL ( field ) order_by = getattr ( order_by , direction ) ( ) order_bys += [ order_by ] order_bys += [ fields [ 0 ] . asc ( ) ] query = query . order_by ( * order_bys ) query = query . limit ( count ) return query
610	def _generateFileFromTemplates ( templateFileNames , outputFilePath , replacementDict ) : installPath = os . path . dirname ( __file__ ) outputFile = open ( outputFilePath , "w" ) outputLines = [ ] inputLines = [ ] firstFile = True for templateFileName in templateFileNames : if not firstFile : inputLines . extend ( [ os . linesep ] * 2 ) firstFile = False inputFilePath = os . path . join ( installPath , templateFileName ) inputFile = open ( inputFilePath ) inputLines . extend ( inputFile . readlines ( ) ) inputFile . close ( ) print "Writing " , len ( inputLines ) , "lines..." for line in inputLines : tempLine = line for k , v in replacementDict . iteritems ( ) : if v is None : v = "None" tempLine = re . sub ( k , v , tempLine ) outputFile . write ( tempLine ) outputFile . close ( )
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
4195	def plot_time_freq ( self , mindB = - 100 , maxdB = None , norm = True , yaxis_label_position = "right" ) : from pylab import subplot , gca subplot ( 1 , 2 , 1 ) self . plot_window ( ) subplot ( 1 , 2 , 2 ) self . plot_frequencies ( mindB = mindB , maxdB = maxdB , norm = norm ) if yaxis_label_position == "left" : try : tight_layout ( ) except : pass else : ax = gca ( ) ax . yaxis . set_label_position ( "right" )
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
6464	def usage_palette ( parser ) : parser . print_usage ( ) print ( '' ) print ( 'available palettes:' ) for palette in sorted ( PALETTE ) : print ( ' %-12s' % ( palette , ) ) return 0
8366	def rendering_finished ( self , size , frame , cairo_ctx ) : surface = cairo_ctx . get_target ( ) if self . format == 'png' : surface . write_to_png ( self . _output_file ( frame ) ) surface . finish ( ) surface . flush ( )
3645	def tradepileDelete ( self , trade_id ) : method = 'DELETE' url = 'trade/%s' % trade_id self . __request__ ( method , url ) return True
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
1891	def _solver_version ( self ) -> Version : self . _reset ( ) if self . _received_version is None : self . _send ( '(get-info :version)' ) self . _received_version = self . _recv ( ) key , version = shlex . split ( self . _received_version [ 1 : - 1 ] ) return Version ( * map ( int , version . split ( '.' ) ) )
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
7468	def multi_muscle_align ( data , samples , ipyclient ) : LOGGER . info ( "starting alignments" ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " aligning clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) path = os . path . join ( data . tmpdir , data . name + ".chunk_*" ) clustbits = glob . glob ( path ) jobs = { } for idx in xrange ( len ( clustbits ) ) : args = [ data , samples , clustbits [ idx ] ] jobs [ idx ] = lbview . apply ( persistent_popen_align3 , * args ) allwait = len ( jobs ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) while 1 : finished = [ i . ready ( ) for i in jobs . values ( ) ] fwait = sum ( finished ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( allwait , fwait , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if all ( finished ) : break keys = jobs . keys ( ) for idx in keys : if not jobs [ idx ] . successful ( ) : LOGGER . error ( "error in persistent_popen_align %s" , jobs [ idx ] . exception ( ) ) raise IPyradWarningExit ( "error in step 6 {}" . format ( jobs [ idx ] . exception ( ) ) ) del jobs [ idx ] print ( "" )
9001	def build_SVG_dict ( self ) : zoom = self . _zoom layout = self . _layout builder = self . _builder bbox = list ( map ( lambda f : f * zoom , layout . bounding_box ) ) builder . bounding_box = bbox flip_x = bbox [ 2 ] + bbox [ 0 ] * 2 flip_y = bbox [ 3 ] + bbox [ 1 ] * 2 instructions = list ( layout . walk_instructions ( lambda i : ( flip_x - ( i . x + i . width ) * zoom , flip_y - ( i . y + i . height ) * zoom , i . instruction ) ) ) instructions . sort ( key = lambda x_y_i : x_y_i [ 2 ] . render_z ) for x , y , instruction in instructions : render_z = instruction . render_z z_id = ( "" if not render_z else "-{}" . format ( render_z ) ) layer_id = "row-{}{}" . format ( instruction . row . id , z_id ) def_id = self . _register_instruction_in_defs ( instruction ) scale = self . _symbol_id_to_scale [ def_id ] group = { "@class" : "instruction" , "@id" : "instruction-{}" . format ( instruction . id ) , "@transform" : "translate({},{}),scale({})" . format ( x , y , scale ) } builder . place_svg_use ( def_id , layer_id , group ) builder . insert_defs ( self . _instruction_type_color_to_symbol . values ( ) ) return builder . get_svg_dict ( )
9495	def _parse_document_id ( elm_tree ) : xpath = '//md:content-id/text()' return [ x for x in elm_tree . xpath ( xpath , namespaces = COLLECTION_NSMAP ) ] [ 0 ]
8462	def set_cmd_env_var ( value ) : def func_decorator ( function ) : @ functools . wraps ( function ) def wrapper ( * args , ** kwargs ) : previous_cmd_env_var = os . getenv ( temple . constants . TEMPLE_ENV_VAR ) os . environ [ temple . constants . TEMPLE_ENV_VAR ] = value try : ret_val = function ( * args , ** kwargs ) finally : if previous_cmd_env_var is None : del os . environ [ temple . constants . TEMPLE_ENV_VAR ] else : os . environ [ temple . constants . TEMPLE_ENV_VAR ] = previous_cmd_env_var return ret_val return wrapper return func_decorator
13703	def expand_words ( self , line , width = 60 ) : if not line . strip ( ) : return line wordi = 1 while len ( strip_codes ( line ) ) < width : wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : wordi = 1 wordendi = self . find_word_end ( line , wordi ) if wordendi < 0 : line = '' . join ( ( ' ' , line ) ) else : line = ' ' . join ( ( line [ : wordendi ] , line [ wordendi : ] ) ) wordi += 1 if ' ' not in strip_codes ( line ) . strip ( ) : return line . replace ( ' ' , '' ) return line
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
342	def create_distributed_session ( task_spec = None , checkpoint_dir = None , scaffold = None , hooks = None , chief_only_hooks = None , save_checkpoint_secs = 600 , save_summaries_steps = object ( ) , save_summaries_secs = object ( ) , config = None , stop_grace_period_secs = 120 , log_step_count_steps = 100 ) : target = task_spec . target ( ) if task_spec is not None else None is_chief = task_spec . is_master ( ) if task_spec is not None else True return tf . train . MonitoredTrainingSession ( master = target , is_chief = is_chief , checkpoint_dir = checkpoint_dir , scaffold = scaffold , save_checkpoint_secs = save_checkpoint_secs , save_summaries_steps = save_summaries_steps , save_summaries_secs = save_summaries_secs , log_step_count_steps = log_step_count_steps , stop_grace_period_secs = stop_grace_period_secs , config = config , hooks = hooks , chief_only_hooks = chief_only_hooks )
4243	def _get_region ( self , ipnum ) : region_code = None country_code = None seek_country = self . _seek_country ( ipnum ) def get_region_code ( offset ) : region1 = chr ( offset // 26 + 65 ) region2 = chr ( offset % 26 + 65 ) return '' . join ( [ region1 , region2 ] ) if self . _databaseType == const . REGION_EDITION_REV0 : seek_region = seek_country - const . STATE_BEGIN_REV0 if seek_region >= 1000 : country_code = 'US' region_code = get_region_code ( seek_region - 1000 ) else : country_code = const . COUNTRY_CODES [ seek_region ] elif self . _databaseType == const . REGION_EDITION_REV1 : seek_region = seek_country - const . STATE_BEGIN_REV1 if seek_region < const . US_OFFSET : pass elif seek_region < const . CANADA_OFFSET : country_code = 'US' region_code = get_region_code ( seek_region - const . US_OFFSET ) elif seek_region < const . WORLD_OFFSET : country_code = 'CA' region_code = get_region_code ( seek_region - const . CANADA_OFFSET ) else : index = ( seek_region - const . WORLD_OFFSET ) // const . FIPS_RANGE if index < len ( const . COUNTRY_CODES ) : country_code = const . COUNTRY_CODES [ index ] elif self . _databaseType in const . CITY_EDITIONS : rec = self . _get_record ( ipnum ) region_code = rec . get ( 'region_code' ) country_code = rec . get ( 'country_code' ) return { 'country_code' : country_code , 'region_code' : region_code }
6207	def merge_da ( self ) : print ( ' - Merging D and A timestamps' , flush = True ) ts_d , ts_par_d = self . S . get_timestamps_part ( self . name_timestamps_d ) ts_a , ts_par_a = self . S . get_timestamps_part ( self . name_timestamps_a ) ts , a_ch , part = merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) assert a_ch . sum ( ) == ts_a . shape [ 0 ] assert ( ~ a_ch ) . sum ( ) == ts_d . shape [ 0 ] assert a_ch . size == ts_a . shape [ 0 ] + ts_d . shape [ 0 ] self . ts , self . a_ch , self . part = ts , a_ch , part self . clk_p = ts_d . attrs [ 'clk_p' ]
9445	def group_call ( self , call_params ) : path = '/' + self . api_version + '/GroupCall/' method = 'POST' return self . request ( path , method , call_params )
11178	def get_separator ( self , i ) : return i and self . separator [ min ( i - 1 , len ( self . separator ) - 1 ) ] or ''
12465	def read_config ( filename , args ) : config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
6950	def jhk_to_sdssi ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSI_JHK , SDSSI_JH , SDSSI_JK , SDSSI_HK , SDSSI_J , SDSSI_H , SDSSI_K )
12404	def bump ( self , bump_reqs = None , ** kwargs ) : bumps = { } for existing_req in sorted ( self . requirements ( ) , key = lambda r : r . project_name ) : if bump_reqs and existing_req . project_name not in bump_reqs : continue bump_reqs . check ( existing_req ) try : bump = self . _bump ( existing_req , bump_reqs . get ( existing_req . project_name ) ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if bump_reqs and bump_reqs . get ( existing_req . project_name ) and all ( r . required_by is None for r in bump_reqs . get ( existing_req . project_name ) ) : raise else : log . warn ( e ) for reqs in bump_reqs . required_requirements ( ) . values ( ) : name = reqs [ 0 ] . project_name if name not in bumps and self . should_add ( name ) : try : bump = self . _bump ( None , reqs ) if bump : bumps [ bump . name ] = bump bump_reqs . check ( bump ) except Exception as e : if all ( r . required_by is None for r in reqs ) : raise else : log . warn ( e ) self . bumps . update ( bumps . values ( ) ) return bumps . values ( )
11306	def store_providers ( self , provider_data ) : if not hasattr ( provider_data , '__iter__' ) : raise OEmbedException ( 'Autodiscovered response not iterable' ) provider_pks = [ ] for provider in provider_data : if 'endpoint' not in provider or 'matches' not in provider : continue resource_type = provider . get ( 'type' ) if resource_type not in RESOURCE_TYPES : continue stored_provider , created = StoredProvider . objects . get_or_create ( wildcard_regex = provider [ 'matches' ] ) if created : stored_provider . endpoint_url = relative_to_full ( provider [ 'endpoint' ] , provider [ 'matches' ] ) stored_provider . resource_type = resource_type stored_provider . save ( ) provider_pks . append ( stored_provider . pk ) return StoredProvider . objects . filter ( pk__in = provider_pks )
8871	def match ( self , subsetLines , offsetOfSubset , fileName ) : for ( offset , l ) in enumerate ( subsetLines ) : column = l . find ( self . literal ) if column != - 1 : truePosition = offset + offsetOfSubset _logger . debug ( 'Found match on line {}, col {}' . format ( str ( truePosition + 1 ) , column ) ) _logger . debug ( 'Line is {}' . format ( l ) ) self . matchLocation = CheckFileParser . FileLocation ( fileName , truePosition + 1 ) return truePosition self . failed = True raise DirectiveException ( self )
10223	def get_regulatory_pairs ( graph : BELGraph ) -> Set [ NodePair ] : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_DECREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( ( u , v ) ) return results
1077	def fromordinal ( cls , n ) : y , m , d = _ord2ymd ( n ) return cls ( y , m , d )
11661	def transform ( self , X ) : self . _check_fitted ( ) X = as_features ( X , stack = True ) assignments = self . kmeans_fit_ . predict ( X . stacked_features ) return self . _group_assignments ( X , assignments )
10097	def create_new_version ( self , name , subject , text = '' , template_id = None , html = None , locale = None , timeout = None ) : if ( html ) : payload = { 'name' : name , 'subject' : subject , 'html' : html , 'text' : text } else : payload = { 'name' : name , 'subject' : subject , 'text' : text } if locale : url = self . TEMPLATES_SPECIFIC_LOCALE_VERSIONS_ENDPOINT % ( template_id , locale ) else : url = self . TEMPLATES_NEW_VERSION_ENDPOINT % template_id return self . _api_request ( url , self . HTTP_POST , payload = payload , timeout = timeout )
7522	def concat_vcf ( data , names , full ) : if not full : writer = open ( data . outfiles . vcf , 'w' ) else : writer = gzip . open ( data . outfiles . VCF , 'w' ) vcfheader ( data , names , writer ) writer . close ( ) vcfchunks = glob . glob ( data . outfiles . vcf + ".*" ) vcfchunks . sort ( key = lambda x : int ( x . rsplit ( "." ) [ - 1 ] ) ) if not full : writer = open ( data . outfiles . vcf , 'a' ) else : writer = gzip . open ( data . outfiles . VCF , 'a' ) if data . paramsdict [ "assembly_method" ] in [ "reference" , "denovo+reference" ] : cmd = [ "cat" ] + vcfchunks + [ " | sort -k 2,2 -n | sort -k 1,1 -s" ] cmd = " " . join ( cmd ) proc = sps . Popen ( cmd , shell = True , stderr = sps . STDOUT , stdout = writer , close_fds = True ) else : proc = sps . Popen ( [ "cat" ] + vcfchunks , stderr = sps . STDOUT , stdout = writer , close_fds = True ) err = proc . communicate ( ) [ 0 ] if proc . returncode : raise IPyradWarningExit ( "err in concat_vcf: %s" , err ) writer . close ( ) for chunk in vcfchunks : os . remove ( chunk )
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) return payload . encode ( 'utf-8' )
13582	def format_to_csv ( filename , skiprows = 0 , delimiter = "" ) : if not delimiter : delimiter = "\t" input_file = open ( filename , "r" ) if skiprows : [ input_file . readline ( ) for _ in range ( skiprows ) ] new_filename = os . path . splitext ( filename ) [ 0 ] + ".csv" output_file = open ( new_filename , "w" ) header = input_file . readline ( ) . split ( ) reader = csv . DictReader ( input_file , fieldnames = header , delimiter = delimiter ) writer = csv . DictWriter ( output_file , fieldnames = header , delimiter = "," ) writer . writerow ( dict ( ( x , x ) for x in header ) ) for line in reader : if None in line : del line [ None ] writer . writerow ( line ) input_file . close ( ) output_file . close ( ) print "Saved %s." % new_filename
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) logger . info ( "DFK cleanup complete" )
7597	def get_top_war_clans ( self , country_key = '' , ** params : keys ) : url = self . api . TOP + '/war/' + str ( country_key ) return self . _get_model ( url , PartialClan , ** params )
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
2834	def platform_detect ( ) : pi = pi_version ( ) if pi is not None : return RASPBERRY_PI plat = platform . platform ( ) if plat . lower ( ) . find ( 'armv7l-with-debian' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-ubuntu' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-glibc2.4' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'tegra-aarch64-with-ubuntu' ) > - 1 : return JETSON_NANO try : import mraa if mraa . getPlatformName ( ) == 'MinnowBoard MAX' : return MINNOWBOARD except ImportError : pass return UNKNOWN
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
588	def compute ( self ) : result = self . _constructClassificationRecord ( ) if result . ROWID >= self . _autoDetectWaitRecords : self . _updateState ( result ) self . saved_states . append ( result ) if len ( self . saved_states ) > self . _history_length : self . saved_states . pop ( 0 ) return result
4505	def get_and_run_edits ( self ) : if self . empty ( ) : return edits = [ ] while True : try : edits . append ( self . get_nowait ( ) ) except queue . Empty : break for e in edits : try : e ( ) except : log . error ( 'Error on edit %s' , e ) traceback . print_exc ( )
13340	def rollaxis ( a , axis , start = 0 ) : if isinstance ( a , np . ndarray ) : return np . rollaxis ( a , axis , start ) if axis not in range ( a . ndim ) : raise ValueError ( 'rollaxis: axis (%d) must be >=0 and < %d' % ( axis , a . ndim ) ) if start not in range ( a . ndim + 1 ) : raise ValueError ( 'rollaxis: start (%d) must be >=0 and < %d' % ( axis , a . ndim + 1 ) ) axes = list ( range ( a . ndim ) ) axes . remove ( axis ) axes . insert ( start , axis ) return transpose ( a , axes )
5107	def fetch_data ( self , return_header = False ) : qdata = [ ] for d in self . data . values ( ) : qdata . extend ( d ) dat = np . zeros ( ( len ( qdata ) , 6 ) ) if len ( qdata ) > 0 : dat [ : , : 5 ] = np . array ( qdata ) dat [ : , 5 ] = self . edge [ 2 ] dType = [ ( 'a' , float ) , ( 's' , float ) , ( 'd' , float ) , ( 'q' , float ) , ( 'n' , float ) , ( 'id' , float ) ] dat = np . array ( [ tuple ( d ) for d in dat ] , dtype = dType ) dat = np . sort ( dat , order = 'a' ) dat = np . array ( [ tuple ( d ) for d in dat ] ) if return_header : return dat , 'arrival,service,departure,num_queued,num_total,q_id' return dat
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" del context del frames return modname , funcname , lineno
7691	def validate ( schema_file = None , jams_files = None ) : schema = load_json ( schema_file ) for jams_file in jams_files : try : jams = load_json ( jams_file ) jsonschema . validate ( jams , schema ) print '{:s} was successfully validated' . format ( jams_file ) except jsonschema . ValidationError as exc : print '{:s} was NOT successfully validated' . format ( jams_file ) print exc
9571	def discovery_view ( self , message ) : for handler in self . registered_handlers : if handler . check ( message ) : return handler . view return None
8498	def _format_call ( call , args ) : out = '' if args . source : out += call . annotation ( ) + '\n' if args . only_keys : out += call . get_key ( ) return out if args . view_call : out += call . as_call ( ) elif args . load_configs : out += call . as_live ( ) else : out += call . as_namespace ( ) return out
1059	def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) return wrapper
11733	def isValidClass ( self , class_ ) : module = inspect . getmodule ( class_ ) valid = ( module in self . _valid_modules or ( hasattr ( module , '__file__' ) and module . __file__ in self . _valid_named_modules ) ) return valid and not private ( class_ )
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , ** kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( ** kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
1679	def CheckNextIncludeOrder ( self , header_type ) : error_message = ( 'Found %s after %s' % ( self . _TYPE_NAMES [ header_type ] , self . _SECTION_NAMES [ self . _section ] ) ) last_section = self . _section if header_type == _C_SYS_HEADER : if self . _section <= self . _C_SECTION : self . _section = self . _C_SECTION else : self . _last_header = '' return error_message elif header_type == _CPP_SYS_HEADER : if self . _section <= self . _CPP_SECTION : self . _section = self . _CPP_SECTION else : self . _last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION else : assert header_type == _OTHER_HEADER self . _section = self . _OTHER_H_SECTION if last_section != self . _section : self . _last_header = '' return ''
12991	def create_hierarchy ( hierarchy , level ) : if level not in hierarchy : hierarchy [ level ] = OrderedDict ( ) return hierarchy [ level ]
7457	def putstats ( pfile , handle , statdicts ) : with open ( pfile , 'r' ) as infile : filestats , samplestats = pickle . load ( infile ) perfile , fsamplehits , fbarhits , fmisses , fdbars = statdicts perfile [ handle ] += filestats samplehits , barhits , misses , dbars = samplestats fsamplehits . update ( samplehits ) fbarhits . update ( barhits ) fmisses . update ( misses ) fdbars . update ( dbars ) statdicts = perfile , fsamplehits , fbarhits , fmisses , fdbars return statdicts
10973	def invitations ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_invitations ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , page = page , per_page = per_page , )
928	def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
7557	def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
4548	def fill_rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : _draw_fast_vline ( setter , i , y , h , color , aa )
3879	async def _handle_set_typing_notification ( self , set_typing_notification ) : conv_id = set_typing_notification . conversation_id . id res = parsers . parse_typing_status_message ( set_typing_notification ) await self . on_typing . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for typing notification: %s' , conv_id ) else : await conv . on_typing . fire ( res )
2218	def _rectify_countdown_or_bool ( count_or_bool ) : if count_or_bool is True or count_or_bool is False : count_or_bool_ = count_or_bool elif isinstance ( count_or_bool , int ) : if count_or_bool == 0 : return 0 elif count_or_bool > 0 : count_or_bool_ = count_or_bool - 1 else : count_or_bool_ = count_or_bool else : count_or_bool_ = False return count_or_bool_
2466	def set_concluded_license ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_conc_lics_set : self . file_conc_lics_set = True if validations . validate_lics_conc ( lic ) : self . file ( doc ) . conc_lics = lic return True else : raise SPDXValueError ( 'File::ConcludedLicense' ) else : raise CardinalityError ( 'File::ConcludedLicense' ) else : raise OrderError ( 'File::ConcludedLicense' )
6494	def set_mappings ( cls , index_name , doc_type , mappings ) : cache . set ( cls . get_cache_item_name ( index_name , doc_type ) , mappings )
11348	def is_instance ( self ) : ret = False val = self . callback if self . is_class ( ) : return False ret = not inspect . isfunction ( val ) and not inspect . ismethod ( val ) return ret
836	def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved
6925	def autocommit ( self ) : if len ( self . cursors . keys ( ) ) == 0 : self . connection . autocommit = True else : raise AttributeError ( 'database cursors are already active, ' 'cannot switch to autocommit now' )
7819	def dispatch ( self , block = False , timeout = None ) : logger . debug ( " dispatching..." ) try : event = self . queue . get ( block , timeout ) except Queue . Empty : logger . debug ( " queue empty" ) return None try : logger . debug ( " event: {0!r}" . format ( event ) ) if event is QUIT : return QUIT handlers = list ( self . _handler_map [ None ] ) klass = event . __class__ if klass in self . _handler_map : handlers += self . _handler_map [ klass ] logger . debug ( " handlers: {0!r}" . format ( handlers ) ) handlers . sort ( key = lambda x : x [ 0 ] ) for dummy , handler in handlers : logger . debug ( u" passing the event to: {0!r}" . format ( handler ) ) result = handler ( event ) if isinstance ( result , Event ) : self . queue . put ( result ) elif result and event is not QUIT : return event return event finally : self . queue . task_done ( )
4504	def put_edit ( self , f , * args , ** kwds ) : self . put_nowait ( functools . partial ( f , * args , ** kwds ) )
5510	def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum_value : raise ValueError ( "Too many releases" )
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
7032	def check_existing_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) if os . path . exists ( APIKEYFILE ) : fileperm = oct ( os . stat ( APIKEYFILE ) [ stat . ST_MODE ] ) if fileperm == '0100600' or fileperm == '0o100600' : with open ( APIKEYFILE ) as infd : apikey , expires = infd . read ( ) . strip ( '\n' ) . split ( ) now = datetime . now ( utc ) if sys . version_info [ : 2 ] < ( 3 , 7 ) : expdt = datetime . strptime ( expires . replace ( 'Z' , '' ) , '%Y-%m-%dT%H:%M:%S.%f' ) . replace ( tzinfo = utc ) else : expdt = datetime . fromisoformat ( expires . replace ( 'Z' , '+00:00' ) ) if now > expdt : LOGERROR ( 'API key has expired. expiry was on: %s' % expires ) return False , apikey , expires else : return True , apikey , expires else : LOGWARNING ( 'The API key file %s has bad permissions ' 'and is insecure, not reading it.\n' '(you need to chmod 600 this file)' % APIKEYFILE ) return False , None , None else : LOGWARNING ( 'No LCC-Server API key ' 'found in: {apikeyfile}' . format ( apikeyfile = APIKEYFILE ) ) return False , None , None
12493	def check_array ( array , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False ) : if isinstance ( accept_sparse , str ) : accept_sparse = [ accept_sparse ] if sp . issparse ( array ) : array = _ensure_sparse_format ( array , accept_sparse , dtype , order , copy , force_all_finite ) else : if ensure_2d : array = np . atleast_2d ( array ) array = np . array ( array , dtype = dtype , order = order , copy = copy ) if not allow_nd and array . ndim >= 3 : raise ValueError ( "Found array with dim %d. Expected <= 2" % array . ndim ) if force_all_finite : _assert_all_finite ( array ) return array
9166	def task ( ** kwargs ) : def wrapper ( wrapped ) : def callback ( scanner , name , obj ) : celery_app = scanner . config . registry . celery_app celery_app . task ( ** kwargs ) ( obj ) venusian . attach ( wrapped , callback ) return wrapped return wrapper
2249	def memoize_property ( fget ) : while hasattr ( fget , 'fget' ) : fget = fget . fget attr_name = '_' + fget . __name__ @ functools . wraps ( fget ) def fget_memoized ( self ) : if not hasattr ( self , attr_name ) : setattr ( self , attr_name , fget ( self ) ) return getattr ( self , attr_name ) return property ( fget_memoized )
10635	def afr ( self ) : result = 0.0 for compound in self . material . compounds : result += self . get_compound_afr ( compound ) return result
2491	def add_file_dependencies_helper ( self , doc_file ) : subj_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( doc_file . name ) ) ) ) if len ( subj_triples ) != 1 : raise InvalidDocumentError ( 'Could not find dependency subject {0}' . format ( doc_file . name ) ) subject_node = subj_triples [ 0 ] [ 0 ] for dependency in doc_file . dependencies : dep_triples = list ( self . graph . triples ( ( None , self . spdx_namespace . fileName , Literal ( dependency ) ) ) ) if len ( dep_triples ) == 1 : dep_node = dep_triples [ 0 ] [ 0 ] dep_triple = ( subject_node , self . spdx_namespace . fileDependency , dep_node ) self . graph . add ( dep_triple ) else : print ( 'Warning could not resolve file dependency {0} -> {1}' . format ( doc_file . name , dependency ) )
4019	def _dusty_vm_exists ( ) : existing_vms = check_output_demoted ( [ 'VBoxManage' , 'list' , 'vms' ] ) for line in existing_vms . splitlines ( ) : if '"{}"' . format ( constants . VM_MACHINE_NAME ) in line : return True return False
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
7340	async def get_media_metadata ( data , path = None ) : if isinstance ( data , bytes ) : media_type = await get_type ( data , path ) else : raise TypeError ( "get_metadata input must be a bytes" ) media_category = get_category ( media_type ) _logger . info ( "media_type: %s, media_category: %s" % ( media_type , media_category ) ) return media_type , media_category
7410	def count_var ( nex ) : arr = np . array ( [ list ( i . split ( ) [ - 1 ] ) for i in nex ] ) miss = np . any ( arr == "N" , axis = 0 ) nomiss = arr [ : , ~ miss ] nsnps = np . invert ( np . all ( nomiss == nomiss [ 0 , : ] , axis = 0 ) ) . sum ( ) return nomiss . shape [ 1 ] , nsnps
1526	def add_context ( self , err_context , succ_context = None ) : self . err_context = err_context self . succ_context = succ_context
12032	def average ( self , t1 = 0 , t2 = None , setsweep = False ) : if setsweep : self . setsweep ( setsweep ) if t2 is None or t2 > self . sweepLength : t2 = self . sweepLength self . log . debug ( "resetting t2 to [%f]" , t2 ) t1 = max ( t1 , 0 ) if t1 > t2 : self . log . error ( "t1 cannot be larger than t2" ) return False I1 , I2 = int ( t1 * self . pointsPerSec ) , int ( t2 * self . pointsPerSec ) if I1 == I2 : return np . nan return np . average ( self . sweepY [ I1 : I2 ] )
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSONEncoder ( self , obj )
2027	def SHA3 ( self , start , size ) : data = self . try_simplify_to_constant ( self . read_buffer ( start , size ) ) if issymbolic ( data ) : known_sha3 = { } self . _publish ( 'on_symbolic_sha3' , data , known_sha3 ) value = 0 known_hashes_cond = False for key , hsh in known_sha3 . items ( ) : assert not issymbolic ( key ) , "Saved sha3 data,hash pairs should be concrete" cond = key == data known_hashes_cond = Operators . OR ( cond , known_hashes_cond ) value = Operators . ITEBV ( 256 , cond , hsh , value ) return value value = sha3 . keccak_256 ( data ) . hexdigest ( ) value = int ( value , 16 ) self . _publish ( 'on_concrete_sha3' , data , value ) logger . info ( "Found a concrete SHA3 example %r -> %x" , data , value ) return value
6028	def neighbors_from_pixelization ( self , pixels , ridge_points ) : return pixelization_util . voronoi_neighbors_from_pixels_and_ridge_points ( pixels = pixels , ridge_points = np . asarray ( ridge_points ) )
12422	def dumps ( obj , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return str ( ) if isinstance ( firstkey , six . text_type ) : io = StringIO ( ) else : io = BytesIO ( ) dump ( obj = obj , fp = io , startindex = startindex , separator = separator , index_separator = index_separator , ) return io . getvalue ( )
6230	def draw_bbox ( self , projection_matrix = None , camera_matrix = None , all = True ) : projection_matrix = projection_matrix . astype ( 'f4' ) . tobytes ( ) camera_matrix = camera_matrix . astype ( 'f4' ) . tobytes ( ) self . bbox_program [ "m_proj" ] . write ( projection_matrix ) self . bbox_program [ "m_view" ] . write ( self . _view_matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "m_cam" ] . write ( camera_matrix ) self . bbox_program [ "bb_min" ] . write ( self . bbox_min . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "bb_max" ] . write ( self . bbox_max . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "color" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox_vao . render ( self . bbox_program ) if not all : return for node in self . root_nodes : node . draw_bbox ( projection_matrix , camera_matrix , self . bbox_program , self . bbox_vao )
9032	def _place_row ( self , row , position ) : self . _rows_in_grid [ row ] = RowInGrid ( row , position )
11344	def request ( self , url , method = "GET" , data = None , params = None , retry = True ) : headers = config . REQUEST_HEADERS if params and self . _session_id : params [ 'sessionid' ] = self . _session_id if method == "GET" : response = requests . get ( url , headers = headers , params = params ) elif method == "POST" : response = requests . post ( url , headers = headers , params = params , data = data ) if response . status_code == 401 and retry : _LOGGER . warn ( "NuHeat APIrequest unauthorized [401]. Try to re-authenticate." ) self . _session_id = None self . authenticate ( ) return self . request ( url , method = method , data = data , params = params , retry = False ) response . raise_for_status ( ) try : return response . json ( ) except ValueError : return response
10631	def clear ( self ) : self . _compound_mfrs = self . _compound_mfrs * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
3416	def _get_id_compartment ( id ) : bracket_search = _bracket_re . findall ( id ) if len ( bracket_search ) == 1 : return bracket_search [ 0 ] [ 1 ] underscore_search = _underscore_re . findall ( id ) if len ( underscore_search ) == 1 : return underscore_search [ 0 ] [ 1 ] return None
3192	def create ( self , list_id , data ) : self . list_id = list_id if 'status' not in data : raise KeyError ( 'The list member must have a status' ) if data [ 'status' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
4768	def is_type_of ( self , some_type ) : if type ( some_type ) is not type and not issubclass ( type ( some_type ) , type ) : raise TypeError ( 'given arg must be a type' ) if type ( self . val ) is not some_type : if hasattr ( self . val , '__name__' ) : t = self . val . __name__ elif hasattr ( self . val , '__class__' ) : t = self . val . __class__ . __name__ else : t = 'unknown' self . _err ( 'Expected <%s:%s> to be of type <%s>, but was not.' % ( self . val , t , some_type . __name__ ) ) return self
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
3132	def create ( self , data ) : if 'name' not in data : raise KeyError ( 'The list must have a name' ) if 'contact' not in data : raise KeyError ( 'The list must have a contact' ) if 'company' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a company' ) if 'address1' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a address1' ) if 'city' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a city' ) if 'state' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a state' ) if 'zip' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a zip' ) if 'country' not in data [ 'contact' ] : raise KeyError ( 'The list contact must have a country' ) if 'permission_reminder' not in data : raise KeyError ( 'The list must have a permission_reminder' ) if 'campaign_defaults' not in data : raise KeyError ( 'The list must have a campaign_defaults' ) if 'from_name' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_name' ) if 'from_email' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a from_email' ) check_email ( data [ 'campaign_defaults' ] [ 'from_email' ] ) if 'subject' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a subject' ) if 'language' not in data [ 'campaign_defaults' ] : raise KeyError ( 'The list campaign_defaults must have a language' ) if 'email_type_option' not in data : raise KeyError ( 'The list must have an email_type_option' ) if data [ 'email_type_option' ] not in [ True , False ] : raise TypeError ( 'The list email_type_option must be True or False' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . list_id = response [ 'id' ] else : self . list_id = None return response
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
10495	def clickMouseButtonRightWithMods ( self , coord , modifiers ) : modFlags = self . _pressModifiers ( modifiers ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _releaseModifiers ( modifiers ) self . _postQueuedEvents ( )
9178	def obtain_licenses ( ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) licenses = { r [ 0 ] : r [ 1 ] for r in cursor . fetchall ( ) } return licenses
2947	def deserialize ( cls , serializer , wf_spec , s_state , ** kwargs ) : return serializer . deserialize_trigger ( wf_spec , s_state , ** kwargs )
6017	def absolute_signal_to_noise_map ( self ) : return np . divide ( np . abs ( self . image ) , self . noise_map )
9617	def PlugIn ( self ) : ids = self . available_ids ( ) if len ( ids ) == 0 : raise MaxInputsReachedError ( 'Max Inputs Reached' ) self . id = ids [ 0 ] _xinput . PlugIn ( self . id ) while self . id in self . available_ids ( ) : pass
10927	def _run1 ( self ) : if self . check_update_J ( ) : self . update_J ( ) else : if self . check_Broyden_J ( ) : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) : self . update_eig_J ( ) delta_vals = self . find_LM_updates ( self . calc_grad ( ) ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if not good_step : er0 = self . update_function ( self . param_vals ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) CLOG . debug ( 'Bad step, increasing damping' ) CLOG . debug ( '\t\t%f\t%f' % ( self . error , er1 ) ) grad = self . calc_grad ( ) for _try in range ( self . _max_inner_loop ) : self . increase_damping ( ) delta_vals = self . find_LM_updates ( grad ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = ( find_best_step ( [ self . error , er1 ] ) == 1 ) if good_step : break else : er0 = self . update_function ( self . param_vals ) CLOG . warn ( 'Stuck!' ) if np . abs ( er0 - self . error ) / er0 > 1e-7 : raise RuntimeError ( 'Function updates are not exact.' ) if good_step : self . _last_error = self . error self . error = er1 CLOG . debug ( 'Good step\t%f\t%f' % ( self . _last_error , self . error ) ) self . update_param_vals ( delta_vals , incremental = True ) self . decrease_damping ( )
2258	def argsort ( indexable , key = None , reverse = False ) : if isinstance ( indexable , collections_abc . Mapping ) : vk_iter = ( ( v , k ) for k , v in indexable . items ( ) ) else : vk_iter = ( ( v , k ) for k , v in enumerate ( indexable ) ) if key is None : indices = [ k for v , k in sorted ( vk_iter , reverse = reverse ) ] else : indices = [ k for v , k in sorted ( vk_iter , key = lambda vk : key ( vk [ 0 ] ) , reverse = reverse ) ] return indices
7153	def many ( prompt , * args , ** kwargs ) : def get_options ( options , chosen ) : return [ options [ i ] for i , c in enumerate ( chosen ) if c ] def get_verbose_options ( verbose_options , chosen ) : no , yes = ' ' , '✔' if sys . version_info < ( 3 , 3 ) : no , yes = ' ' , '@' opts = [ '{} {}' . format ( yes if c else no , verbose_options [ i ] ) for i , c in enumerate ( chosen ) ] return opts + [ '{}{}' . format ( ' ' , kwargs . get ( 'done' , 'done...' ) ) ] options , verbose_options = prepare_options ( args ) chosen = [ False ] * len ( options ) index = kwargs . get ( 'idx' , 0 ) default = kwargs . get ( 'default' , None ) if isinstance ( default , list ) : for idx in default : chosen [ idx ] = True if isinstance ( default , int ) : chosen [ default ] = True while True : try : index = one ( prompt , * get_verbose_options ( verbose_options , chosen ) , return_index = True , idx = index ) except QuestionnaireGoBack : if any ( chosen ) : raise QuestionnaireGoBack ( 0 ) else : raise QuestionnaireGoBack if index == len ( options ) : return get_options ( options , chosen ) chosen [ index ] = not chosen [ index ]
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) tags = [ t for t in tags if t ] return tags
13358	def aot40_vegetation ( df , nb_an ) : return _aot ( df . tshift ( 1 ) , nb_an = nb_an , limite = 80 , mois_debut = 5 , mois_fin = 7 , heure_debut = 8 , heure_fin = 19 )
10200	def run ( self ) : return elasticsearch . helpers . bulk ( self . client , self . actionsiter ( ) , stats_only = True , chunk_size = 50 )
2742	def get_object ( cls , api_token , ssh_key_id ) : ssh_key = cls ( token = api_token , id = ssh_key_id ) ssh_key . load ( ) return ssh_key
5160	def render ( self ) : template_name = '{0}.jinja2' . format ( self . get_name ( ) ) template = self . template_env . get_template ( template_name ) context = getattr ( self . backend , 'intermediate_data' , { } ) output = template . render ( data = context ) return self . cleanup ( output )
3625	def encode ( latitude , longitude , precision = 12 ) : lat_interval , lon_interval = ( - 90.0 , 90.0 ) , ( - 180.0 , 180.0 ) geohash = [ ] bits = [ 16 , 8 , 4 , 2 , 1 ] bit = 0 ch = 0 even = True while len ( geohash ) < precision : if even : mid = ( lon_interval [ 0 ] + lon_interval [ 1 ] ) / 2 if longitude > mid : ch |= bits [ bit ] lon_interval = ( mid , lon_interval [ 1 ] ) else : lon_interval = ( lon_interval [ 0 ] , mid ) else : mid = ( lat_interval [ 0 ] + lat_interval [ 1 ] ) / 2 if latitude > mid : ch |= bits [ bit ] lat_interval = ( mid , lat_interval [ 1 ] ) else : lat_interval = ( lat_interval [ 0 ] , mid ) even = not even if bit < 4 : bit += 1 else : geohash += __base32 [ ch ] bit = 0 ch = 0 return '' . join ( geohash )
11223	def dump_set ( self , obj , class_name = set_class_name ) : return { "$" + class_name : [ self . _json_convert ( item ) for item in obj ] }
9096	def upload_bel_namespace ( self , update : bool = False ) -> Namespace : if not self . is_populated ( ) : self . populate ( ) namespace = self . _get_default_namespace ( ) if namespace is None : log . info ( 'making namespace for %s' , self . _get_namespace_name ( ) ) return self . _make_namespace ( ) if update : self . _update_namespace ( namespace ) return namespace
10028	def describe_events ( self , environment_name , next_token = None , start_time = None ) : events = self . ebs . describe_events ( application_name = self . app_name , environment_name = environment_name , next_token = next_token , start_time = start_time + 'Z' ) return ( events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'Events' ] , events [ 'DescribeEventsResponse' ] [ 'DescribeEventsResult' ] [ 'NextToken' ] )
5690	def read_data_as_dataframe ( self , travel_impedance_measure , from_stop_I = None , to_stop_I = None , statistic = None ) : to_select = [ ] where_clauses = [ ] to_select . append ( "from_stop_I" ) to_select . append ( "to_stop_I" ) if from_stop_I is not None : where_clauses . append ( "from_stop_I=" + str ( int ( from_stop_I ) ) ) if to_stop_I is not None : where_clauses . append ( "to_stop_I=" + str ( int ( to_stop_I ) ) ) where_clause = "" if len ( where_clauses ) > 0 : where_clause = " WHERE " + " AND " . join ( where_clauses ) if not statistic : to_select . extend ( [ "min" , "mean" , "median" , "max" ] ) else : to_select . append ( statistic ) to_select_clause = "," . join ( to_select ) if not to_select_clause : to_select_clause = "*" sql = "SELECT " + to_select_clause + " FROM " + travel_impedance_measure + where_clause + ";" df = pd . read_sql ( sql , self . conn ) return df
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
2655	def isdir ( self , path ) : result = True try : self . sftp_client . lstat ( path ) except FileNotFoundError : result = False return result
5277	def query ( self , i , j ) : "Query the oracle to find out whether i and j should be must-linked" if self . queries_cnt < self . max_queries_cnt : self . queries_cnt += 1 return self . labels [ i ] == self . labels [ j ] else : raise MaximumQueriesExceeded
5499	def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
7956	def _initiate_starttls ( self , ** kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , ** kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
4020	def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
12464	def print_message ( message = None ) : kwargs = { 'stdout' : sys . stdout , 'stderr' : sys . stderr , 'shell' : True } return subprocess . call ( 'echo "{0}"' . format ( message or '' ) , ** kwargs )
5129	def size ( self , s ) : leader = self . find ( s ) return self . _size [ leader ]
7422	def ref_build_and_muscle_chunk ( data , sample ) : regions = bedtools_merge ( data , sample ) . strip ( ) . split ( "\n" ) nregions = len ( regions ) chunksize = ( nregions / 10 ) + ( nregions % 10 ) LOGGER . debug ( "nregions {} chunksize {}" . format ( nregions , chunksize ) ) idx = 0 tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" ) for i in range ( 11 ) : if os . path . exists ( tmpfile . format ( i ) ) : os . remove ( tmpfile . format ( i ) ) fopen = open if data . paramsdict [ "assembly_method" ] == "denovo+reference" : tmpfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) fopen = gzip . open samfile = pysam . AlignmentFile ( sample . files . mapped_reads , 'rb' ) clusts = [ ] nclusts = 0 for region in regions : chrom , pos1 , pos2 = region . split ( ) try : if "pair" in data . paramsdict [ "datatype" ] : clust = fetch_cluster_pairs ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) else : clust = fetch_cluster_se ( data , samfile , chrom , int ( pos1 ) , int ( pos2 ) ) except IndexError as inst : LOGGER . error ( "Bad region chrom:start-end {}:{}-{}" . format ( chrom , pos1 , pos2 ) ) continue if clust : clusts . append ( "\n" . join ( clust ) ) nclusts += 1 if nclusts == chunksize : tmphandle = tmpfile . format ( idx ) with fopen ( tmphandle , 'a' ) as tmp : tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) idx += 1 nclusts = 0 clusts = [ ] if clusts : with fopen ( tmpfile . format ( idx ) , 'a' ) as tmp : tmp . write ( "\n//\n//\n" . join ( clusts ) + "\n//\n//\n" ) clusts = [ ] if not data . paramsdict [ "assembly_method" ] == "denovo+reference" : chunkfiles = glob . glob ( os . path . join ( data . tmpdir , sample . name + "_chunk_*.ali" ) ) LOGGER . info ( "created chunks %s" , chunkfiles ) samfile . close ( )
438	def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) if is_train == True : img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) img = tf . image . random_flip_left_right ( img ) img = tf . image . random_brightness ( img , max_delta = 63 ) img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img = tf . image . per_image_standardization ( img ) elif is_train == False : img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
11336	def get_records ( self , url ) : page = urllib2 . urlopen ( url ) pages = [ BeautifulSoup ( page ) ] numpag = pages [ 0 ] . body . findAll ( 'span' , attrs = { 'class' : 'number-of-pages' } ) if len ( numpag ) > 0 : if re . search ( '^\d+$' , numpag [ 0 ] . string ) : for i in range ( int ( numpag [ 0 ] . string ) - 1 ) : page = urllib2 . urlopen ( '%s/page/%i' % ( url , i + 2 ) ) pages . append ( BeautifulSoup ( page ) ) else : print ( "number of pages %s not an integer" % ( numpag [ 0 ] . string ) ) impl = getDOMImplementation ( ) doc = impl . createDocument ( None , "collection" , None ) links = [ ] for page in pages : links += page . body . findAll ( 'p' , attrs = { 'class' : 'title' } ) links += page . body . findAll ( 'h3' , attrs = { 'class' : 'title' } ) for link in links : record = self . _get_record ( link ) doc . firstChild . appendChild ( record ) return doc . toprettyxml ( )
11345	def handle_starttag ( self , tag , attrs ) : if tag in self . mathml_elements : final_attr = "" for key , value in attrs : final_attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final_attr ) )
1910	def run ( self , procs = 1 , timeout = None , should_profile = False ) : assert not self . running , "Manticore is already running." self . _start_run ( ) self . _last_run_stats [ 'time_started' ] = time . time ( ) with self . shutdown_timeout ( timeout ) : self . _start_workers ( procs , profiling = should_profile ) self . _join_workers ( ) self . _finish_run ( profiling = should_profile )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
11763	def utility ( self , state , player ) : "Return the value to player; 1 for win, -1 for loss, 0 otherwise." return if_ ( player == 'X' , state . utility , - state . utility )
3249	def get_base ( managed_policy , ** conn ) : managed_policy [ '_version' ] = 1 arn = _get_name_from_structure ( managed_policy , 'Arn' ) policy = get_policy ( arn , ** conn ) document = get_managed_policy_document ( arn , policy_metadata = policy , ** conn ) managed_policy . update ( policy [ 'Policy' ] ) managed_policy [ 'Document' ] = document managed_policy [ 'CreateDate' ] = get_iso_string ( managed_policy [ 'CreateDate' ] ) managed_policy [ 'UpdateDate' ] = get_iso_string ( managed_policy [ 'UpdateDate' ] ) return managed_policy
5950	def check_file_exists ( self , filename , resolve = 'exception' , force = None ) : def _warn ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . warn ( msg ) warnings . warn ( msg ) return True def _raise ( x ) : msg = "File {0!r} already exists." . format ( x ) logger . error ( msg ) raise IOError ( errno . EEXIST , x , msg ) solutions = { 'ignore' : lambda x : False , 'indicate' : lambda x : True , 'warn' : _warn , 'warning' : _warn , 'exception' : _raise , 'raise' : _raise , } if force is True : resolve = 'ignore' elif force is False : resolve = 'exception' if not os . path . isfile ( filename ) : return False else : return solutions [ resolve ] ( filename )
3799	def Bahadori_liquid ( T , M ) : r A = [ - 6.48326E-2 , 2.715015E-3 , - 1.08580E-5 , 9.853917E-9 ] B = [ 1.565612E-2 , - 1.55833E-4 , 5.051114E-7 , - 4.68030E-10 ] C = [ - 1.80304E-4 , 1.758693E-6 , - 5.55224E-9 , 5.201365E-12 ] D = [ 5.880443E-7 , - 5.65898E-9 , 1.764384E-11 , - 1.65944E-14 ] X , Y = M , T a = A [ 0 ] + B [ 0 ] * X + C [ 0 ] * X ** 2 + D [ 0 ] * X ** 3 b = A [ 1 ] + B [ 1 ] * X + C [ 1 ] * X ** 2 + D [ 1 ] * X ** 3 c = A [ 2 ] + B [ 2 ] * X + C [ 2 ] * X ** 2 + D [ 2 ] * X ** 3 d = A [ 3 ] + B [ 3 ] * X + C [ 3 ] * X ** 2 + D [ 3 ] * X ** 3 return a + b * Y + c * Y ** 2 + d * Y ** 3
12396	def get_method ( self , * args , ** kwargs ) : for method in self . gen_methods ( * args , ** kwargs ) : return method msg = 'No method was found for %r on %r.' raise self . DispatchError ( msg % ( ( args , kwargs ) , self . inst ) )
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
452	def get_layers_with_name ( net , name = "" , verbose = False ) : logging . info ( " [*] geting layers with %s" % name ) layers = [ ] i = 0 for layer in net . all_layers : if name in layer . name : layers . append ( layer ) if verbose : logging . info ( " got {:3}: {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) ) ) i = i + 1 return layers
13577	def paste ( tid = None , review = False ) : submit ( pastebin = True , tid = tid , review = False )
9078	def make_df_getter ( data_url : str , data_path : str , ** kwargs ) -> Callable [ [ Optional [ str ] , bool , bool ] , pd . DataFrame ] : download_function = make_downloader ( data_url , data_path ) def get_df ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> pd . DataFrame : if url is None and cache : url = download_function ( force_download = force_download ) return pd . read_csv ( url or data_url , ** kwargs ) return get_df
5275	def _edgeLabel ( self , node , parent ) : return self . word [ node . idx + parent . depth : node . idx + node . depth ]
5989	def grid_stack_from_deflection_stack ( grid_stack , deflection_stack ) : if deflection_stack is not None : def minus ( grid , deflections ) : return grid - deflections return grid_stack . map_function ( minus , deflection_stack )
9656	def get_the_node_dict ( G , name ) : for node in G . nodes ( data = True ) : if node [ 0 ] == name : return node [ 1 ]
7528	def aligned_indel_filter ( clust , max_internal_indels ) : lclust = clust . split ( ) try : seq1 = [ i . split ( "nnnn" ) [ 0 ] for i in lclust [ 1 : : 2 ] ] seq2 = [ i . split ( "nnnn" ) [ 1 ] for i in lclust [ 1 : : 2 ] ] intindels1 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] intindels2 = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq2 ] intindels = intindels1 + intindels2 if max ( intindels ) > max_internal_indels : return 1 except IndexError : seq1 = lclust [ 1 : : 2 ] intindels = [ i . rstrip ( "-" ) . lstrip ( "-" ) . count ( "-" ) for i in seq1 ] if max ( intindels ) > max_internal_indels : return 1 return 0
6093	def grid_angle_to_profile ( self , grid_thetas ) : theta_coordinate_to_profile = np . add ( grid_thetas , - self . phi_radians ) return np . cos ( theta_coordinate_to_profile ) , np . sin ( theta_coordinate_to_profile )
7227	def paint ( self ) : snippet = { 'fill-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'fill-color' : VectorStyle . get_style_value ( self . color ) , 'fill-outline-color' : VectorStyle . get_style_value ( self . outline_color ) } if self . translate : snippet [ 'fill-translate' ] = self . translate return snippet
11153	def sha512file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha512 , nbytes = nbytes , chunk_size = chunk_size )
10011	def get ( vals , key , default_val = None ) : val = vals for part in key . split ( '.' ) : if isinstance ( val , dict ) : val = val . get ( part , None ) if val is None : return default_val else : return default_val return val
6598	def end ( self ) : results = self . communicationChannel . receive ( ) if self . nruns != len ( results ) : import logging logger = logging . getLogger ( __name__ ) logger . warning ( 'too few results received: {} results received, {} expected' . format ( len ( results ) , self . nruns ) ) return results
7870	def _decode_error ( self ) : error_qname = self . _ns_prefix + "error" for child in self . _element : if child . tag == error_qname : self . _error = StanzaErrorElement ( child ) return raise BadRequestProtocolError ( "Error element missing in" " an error stanza" )
10025	def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
5259	def _adjust_delay ( self , slot , response ) : if response . status in self . retry_http_codes : new_delay = max ( slot . delay , 1 ) * 4 new_delay = max ( new_delay , self . mindelay ) new_delay = min ( new_delay , self . maxdelay ) slot . delay = new_delay self . stats . inc_value ( 'delay_count' ) elif response . status == 200 : new_delay = max ( slot . delay / 2 , self . mindelay ) if new_delay < 0.01 : new_delay = 0 slot . delay = new_delay
1191	def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
2138	def list ( self , root = False , ** kwargs ) : if kwargs . get ( 'parent' , None ) : self . set_child_endpoint ( parent = kwargs [ 'parent' ] , inventory = kwargs . get ( 'inventory' , None ) ) kwargs . pop ( 'parent' ) if root and not kwargs . get ( 'inventory' , None ) : raise exc . UsageError ( 'The --root option requires specifying an inventory also.' ) if root : inventory_id = kwargs [ 'inventory' ] r = client . get ( '/inventories/%d/root_groups/' % inventory_id ) return r . json ( ) return super ( Resource , self ) . list ( ** kwargs )
3194	def create_or_update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'email_address' not in data : raise KeyError ( 'The list member must have an email_address' ) check_email ( data [ 'email_address' ] ) if 'status_if_new' not in data : raise KeyError ( 'The list member must have a status_if_new' ) if data [ 'status_if_new' ] not in [ 'subscribed' , 'unsubscribed' , 'cleaned' , 'pending' , 'transactional' ] : raise ValueError ( 'The list member status_if_new must be one of "subscribed", "unsubscribed", "cleaned", ' '"pending", or "transactional"' ) return self . _mc_client . _put ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
1690	def UpdatePreprocessor ( self , line ) : if Match ( r'^\s*#\s*(if|ifdef|ifndef)\b' , line ) : self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) elif Match ( r'^\s*#\s*(else|elif)\b' , line ) : if self . pp_stack : if not self . pp_stack [ - 1 ] . seen_else : self . pp_stack [ - 1 ] . seen_else = True self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) else : pass elif Match ( r'^\s*#\s*endif\b' , line ) : if self . pp_stack : if self . pp_stack [ - 1 ] . seen_else : self . stack = self . pp_stack [ - 1 ] . stack_before_else self . pp_stack . pop ( ) else : pass
3674	def draw_3d ( self , width = 300 , height = 500 , style = 'stick' , Hs = True ) : r try : import py3Dmol from IPython . display import display if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol AllChem . EmbedMultipleConfs ( mol ) mb = Chem . MolToMolBlock ( mol ) p = py3Dmol . view ( width = width , height = height ) p . addModel ( mb , 'sdf' ) p . setStyle ( { style : { } } ) p . zoomTo ( ) display ( p . show ( ) ) except : return 'py3Dmol, RDKit, and IPython are required for this feature.'
10293	def expand_internal_causal ( universe : BELGraph , graph : BELGraph ) -> None : expand_internal ( universe , graph , edge_predicates = is_causal_relation )
8180	def add_node ( self , id , radius = 8 , style = style . DEFAULT , category = "" , label = None , root = False , properties = { } ) : if self . has_key ( id ) : return self [ id ] if not isinstance ( style , str ) and style . __dict__ . has_key [ "name" ] : style = style . name n = node ( self , id , radius , style , category , label , properties ) self [ n . id ] = n self . nodes . append ( n ) if root : self . root = n return n
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
1137	def commonprefix ( m ) : "Given a list of pathnames, returns the longest common leading component" if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1
1885	def solve_buffer ( self , addr , nbytes , constrain = False ) : buffer = self . cpu . read_bytes ( addr , nbytes ) result = [ ] with self . _constraints as temp_cs : cs_to_use = self . constraints if constrain else temp_cs for c in buffer : result . append ( self . _solver . get_value ( cs_to_use , c ) ) cs_to_use . add ( c == result [ - 1 ] ) return result
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
4938	def logo_path ( instance , filename ) : extension = os . path . splitext ( filename ) [ 1 ] . lower ( ) instance_id = str ( instance . id ) fullname = os . path . join ( "enterprise/branding/" , instance_id , instance_id + "_logo" + extension ) if default_storage . exists ( fullname ) : default_storage . delete ( fullname ) return fullname
13019	def _assemble_select ( self , sql_str , columns , * args , ** kwargs ) : warnings . warn ( "_assemble_select has been depreciated for _assemble_with_columns. It will be removed in a future version." , DeprecationWarning ) return self . _assemble_with_columns ( sql_str , columns , * args , ** kwargs )
10876	def calculate_linescan_ilm_psf ( y , z , polar_angle = 0. , nlpts = 1 , pinhole_width = 1 , use_laggauss = False , ** kwargs ) : if use_laggauss : x_vals , wts = calc_pts_lag ( ) else : x_vals , wts = calc_pts_hg ( ) xg , yg , zg = [ np . zeros ( list ( y . shape ) + [ x_vals . size ] ) for a in range ( 3 ) ] hilm = np . zeros ( xg . shape ) for a in range ( x_vals . size ) : xg [ ... , a ] = x_vals [ a ] yg [ ... , a ] = y . copy ( ) zg [ ... , a ] = z . copy ( ) y_pinhole , wts_pinhole = np . polynomial . hermite . hermgauss ( nlpts ) y_pinhole *= np . sqrt ( 2 ) * pinhole_width wts_pinhole /= np . sqrt ( np . pi ) for yp , wp in zip ( y_pinhole , wts_pinhole ) : rho = np . sqrt ( xg * xg + ( yg - yp ) * ( yg - yp ) ) phi = np . arctan2 ( yg , xg ) hsym , hasym = get_hsym_asym ( rho , zg , get_hdet = False , ** kwargs ) hilm += wp * ( hsym + np . cos ( 2 * ( phi - polar_angle ) ) * hasym ) for a in range ( x_vals . size ) : hilm [ ... , a ] *= wts [ a ] return hilm . sum ( axis = - 1 ) * 2.
9807	def teardown ( file ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file ) exception = None try : if click . confirm ( 'Would you like to execute pre-delete hooks?' , default = True ) : manager . teardown ( hooks = True ) else : manager . teardown ( hooks = False ) except Exception as e : Printer . print_error ( 'Polyaxon could not teardown the deployment.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
4839	def get_course_details ( self , course_id ) : return self . _load_data ( self . COURSES_ENDPOINT , resource_id = course_id , many = False )
8753	def partition_vifs ( xapi_client , interfaces , security_group_states ) : added = [ ] updated = [ ] removed = [ ] for vif in interfaces : if ( 'floating_ip' in CONF . QUARK . environment_capabilities and is_isonet_vif ( vif ) ) : continue vif_has_groups = vif in security_group_states if vif . tagged and vif_has_groups and security_group_states [ vif ] [ sg_cli . SECURITY_GROUP_ACK ] : continue if vif . tagged : if vif_has_groups : updated . append ( vif ) else : removed . append ( vif ) else : if vif_has_groups : added . append ( vif ) return added , updated , removed
12273	def iso_reference_valid_char ( c , raise_error = True ) : if c in ISO_REFERENCE_VALID : return True if raise_error : raise ValueError ( "'%s' is not in '%s'" % ( c , ISO_REFERENCE_VALID ) ) return False
5065	def filter_audit_course_modes ( enterprise_customer , course_modes ) : audit_modes = getattr ( settings , 'ENTERPRISE_COURSE_ENROLLMENT_AUDIT_MODES' , [ 'audit' ] ) if not enterprise_customer . enable_audit_enrollment : return [ course_mode for course_mode in course_modes if course_mode [ 'mode' ] not in audit_modes ] return course_modes
5334	def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
3315	def _find ( self , url ) : vr = self . db . view ( "properties/by_url" , key = url , include_docs = True ) _logger . debug ( "find(%r) returned %s" % ( url , len ( vr ) ) ) assert len ( vr ) <= 1 , "Found multiple matches for %r" % url for row in vr : assert row . doc return row . doc return None
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
9856	def get_data ( self , ** kwargs ) : limit = int ( kwargs . get ( 'limit' , 288 ) ) end_date = kwargs . get ( 'end_date' , False ) if end_date and isinstance ( end_date , datetime . datetime ) : end_date = self . convert_datetime ( end_date ) if self . mac_address is not None : service_address = 'devices/%s' % self . mac_address self . api_instance . log ( 'SERVICE ADDRESS: %s' % service_address ) data = dict ( limit = limit ) if end_date : data . update ( { 'endDate' : end_date } ) self . api_instance . log ( 'DATA:' ) self . api_instance . log ( data ) return self . api_instance . api_call ( service_address , ** data )
13177	def get_cache_key ( prefix , * args , ** kwargs ) : hash_args_kwargs = hash ( tuple ( kwargs . iteritems ( ) ) + args ) return '{}_{}' . format ( prefix , hash_args_kwargs )
2108	def login ( username , password , scope , client_id , client_secret , verbose ) : if not supports_oauth ( ) : raise exc . TowerCLIError ( 'This version of Tower does not support OAuth2.0. Set credentials using tower-cli config.' ) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client_id and client_secret : HTTPBasicAuth ( client_id , client_secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client_id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "client_id" : client_id , "scope" : scope } , headers = req . headers ) else : HTTPBasicAuth ( username , password ) ( req ) r = client . post ( '/users/{}/personal_tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary_fields' , None ) result . pop ( 'related' , None ) if client_id : token = result . pop ( 'access_token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth_token' , token , '--scope=user' ] )
11725	def camel2word ( string ) : def wordize ( match ) : return ' ' + match . group ( 1 ) . lower ( ) return string [ 0 ] + re . sub ( r'([A-Z])' , wordize , string [ 1 : ] )
12856	def merge_text ( events ) : text = [ ] for obj in events : if obj [ 'type' ] == TEXT : text . append ( obj [ 'text' ] ) else : if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) } text . clear ( ) yield obj if text : yield { 'type' : TEXT , 'text' : '' . join ( text ) }
5354	def get_repos_by_backend_section ( cls , backend_section , raw = True ) : repos = [ ] projects = TaskProjects . get_projects ( ) for pro in projects : if backend_section in projects [ pro ] : if cls . GLOBAL_PROJECT not in projects : repos += projects [ pro ] [ backend_section ] else : if raw : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : not_in_unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL_PROJECT ] [ 0 ] if backend_section not in not_in_unknown : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] else : not_in_unknown_prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL_PROJECT ] not_in_unknown_sections = list ( set ( [ section for prj in not_in_unknown_prj for section in list ( prj . keys ( ) ) ] ) ) if backend_section not in not_in_unknown_sections : repos += projects [ pro ] [ backend_section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend_section , repos , raw ) repos = list ( set ( repos ) ) return repos
393	def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : if rewards is None : raise Exception ( "rewards should be a list" ) discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) running_add = 0 for t in reversed ( xrange ( 0 , rewards . size ) ) : if mode == 0 : if rewards [ t ] != 0 : running_add = 0 running_add = running_add * gamma + rewards [ t ] discounted_r [ t ] = running_add return discounted_r
5182	def nodes ( self , unreported = 2 , with_status = False , ** kwargs ) : nodes = self . _query ( 'nodes' , ** kwargs ) now = datetime . datetime . utcnow ( ) if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
13328	def add ( path ) : click . echo ( '\nAdding {} to cache......' . format ( path ) , nl = False ) try : r = cpenv . resolve ( path ) except Exception as e : click . echo ( bold_red ( 'FAILED' ) ) click . echo ( e ) return if isinstance ( r . resolved [ 0 ] , cpenv . VirtualEnvironment ) : EnvironmentCache . add ( r . resolved [ 0 ] ) EnvironmentCache . save ( ) click . echo ( bold_green ( 'OK!' ) )
2770	def get_firewall ( self , firewall_id ) : return Firewall . get_object ( api_token = self . token , firewall_id = firewall_id , )
2714	def load ( self ) : tags = self . get_data ( "tags/%s" % self . name ) tag = tags [ 'tag' ] for attr in tag . keys ( ) : setattr ( self , attr , tag [ attr ] ) return self
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
13901	def get_db_from_db ( db_string ) : server = get_server_from_db ( db_string ) local_match = PLAIN_RE . match ( db_string ) remote_match = URL_RE . match ( db_string ) if local_match : return server [ local_match . groupdict ( ) [ 'database' ] ] elif remote_match : return server [ remote_match . groupdict ( ) [ 'database' ] ] raise ValueError ( 'Invalid database string: %r' % ( db_string , ) )
3769	def none_and_length_check ( all_inputs , length = None ) : r if not length : length = len ( all_inputs [ 0 ] ) for things in all_inputs : if None in things or len ( things ) != length : return False return True
4241	def ip2long ( ip ) : try : return int ( binascii . hexlify ( socket . inet_aton ( ip ) ) , 16 ) except socket . error : return int ( binascii . hexlify ( socket . inet_pton ( socket . AF_INET6 , ip ) ) , 16 )
6716	def bootstrap ( self , force = 0 ) : force = int ( force ) if self . has_pip ( ) and not force : return r = self . local_renderer if r . env . bootstrap_method == GET_PIP : r . sudo ( 'curl --silent --show-error --retry 5 https://bootstrap.pypa.io/get-pip.py | python' ) elif r . env . bootstrap_method == EZ_SETUP : r . run ( 'wget http://peak.telecommunity.com/dist/ez_setup.py -O /tmp/ez_setup.py' ) with self . settings ( warn_only = True ) : r . sudo ( 'python /tmp/ez_setup.py -U setuptools' ) r . sudo ( 'easy_install -U pip' ) elif r . env . bootstrap_method == PYTHON_PIP : r . sudo ( 'apt-get install -y python-pip' ) else : raise NotImplementedError ( 'Unknown pip bootstrap method: %s' % r . env . bootstrap_method ) r . sudo ( 'pip {quiet_flag} install --upgrade pip' ) r . sudo ( 'pip {quiet_flag} install --upgrade virtualenv' )
11651	def transform ( self , X ) : self . _check_fitted ( ) M = self . smoothness dim = self . dim_ inds = self . inds_ do_check = self . do_bounds_check X = as_features ( X ) if X . dim != dim : msg = "model fit for dimension {} but got dim {}" raise ValueError ( msg . format ( dim , X . dim ) ) Xt = np . empty ( ( len ( X ) , self . inds_ . shape [ 0 ] ) ) Xt . fill ( np . nan ) if self . basis == 'cosine' : coefs = ( np . pi * np . arange ( M + 1 ) ) [ ... , : ] for i , bag in enumerate ( X ) : if do_check : if np . min ( bag ) < 0 or np . max ( bag ) > 1 : raise ValueError ( "Bag {} not in [0, 1]" . format ( i ) ) phi = coefs * bag [ ... , np . newaxis ] np . cos ( phi , out = phi ) phi [ : , : , 1 : ] *= np . sqrt ( 2 ) B = reduce ( op . mul , ( phi [ : , i , inds [ : , i ] ] for i in xrange ( dim ) ) ) Xt [ i , : ] = np . mean ( B , axis = 0 ) else : raise ValueError ( "unknown basis '{}'" . format ( self . basis ) ) return Xt
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
10874	def get_hsym_asym ( rho , z , get_hdet = False , include_K3_det = True , ** kwargs ) : K1 , Kprefactor = get_K ( rho , z , K = 1 , get_hdet = get_hdet , Kprefactor = None , return_Kprefactor = True , ** kwargs ) K2 = get_K ( rho , z , K = 2 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) if get_hdet and not include_K3_det : K3 = 0 * K1 else : K3 = get_K ( rho , z , K = 3 , get_hdet = get_hdet , Kprefactor = Kprefactor , return_Kprefactor = False , ** kwargs ) hsym = K1 * K1 . conj ( ) + K2 * K2 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) hasym = K1 * K2 . conj ( ) + K2 * K1 . conj ( ) + 0.5 * ( K3 * K3 . conj ( ) ) return hsym . real , hasym . real
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
7348	async def get_access_token ( consumer_key , consumer_secret , oauth_token , oauth_token_secret , oauth_verifier , ** kwargs ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , access_token = oauth_token , access_token_secret = oauth_token_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . access_token . get ( _suffix = "" , oauth_verifier = oauth_verifier ) return parse_token ( response )
6748	def collect_genv ( self , include_local = True , include_global = True ) : e = type ( self . genv ) ( ) if include_global : e . update ( self . genv ) if include_local : for k , v in self . lenv . items ( ) : e [ '%s_%s' % ( self . obj . name . lower ( ) , k ) ] = v return e
1983	def save_value ( self , key , value ) : with self . save_stream ( key ) as s : s . write ( value )
4202	def aryule ( X , order , norm = 'biased' , allow_singularity = True ) : r assert norm in [ 'biased' , 'unbiased' ] r = CORRELATION ( X , maxlags = order , norm = norm ) A , P , k = LEVINSON ( r , allow_singularity = allow_singularity ) return A , P , k
5942	def _get_gmx_docs ( self ) : if self . _doc_cache is not None : return self . _doc_cache try : logging . disable ( logging . CRITICAL ) rc , header , docs = self . run ( 'h' , stdout = PIPE , stderr = PIPE , use_input = False ) except : logging . critical ( "Invoking command {0} failed when determining its doc string. Proceed with caution" . format ( self . command_name ) ) self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache finally : logging . disable ( logging . NOTSET ) m = re . match ( self . doc_pattern , docs , re . DOTALL ) if m is None : m = re . match ( self . doc_pattern , header , re . DOTALL ) if m is None : self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache self . _doc_cache = m . group ( 'DOCS' ) return self . _doc_cache
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
11133	def tear_down ( self ) : while len ( self . _temp_directories ) > 0 : directory = self . _temp_directories . pop ( ) shutil . rmtree ( directory , ignore_errors = True ) while len ( self . _temp_files ) > 0 : file = self . _temp_files . pop ( ) try : os . remove ( file ) except OSError : pass
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
7876	def bind ( self , stream , resource ) : self . stream = stream stanza = Iq ( stanza_type = "set" ) payload = ResourceBindingPayload ( resource = resource ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _bind_success , self . _bind_error ) stream . send ( stanza ) stream . event ( BindingResourceEvent ( resource ) )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
8200	def set_bot ( self , bot ) : self . bot = bot self . sink . set_bot ( bot )
6835	def vagrant_settings ( self , name = '' , * args , ** kwargs ) : config = self . ssh_config ( name ) extra_args = self . _settings_dict ( config ) kwargs . update ( extra_args ) return self . settings ( * args , ** kwargs )
4989	def redirect ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) resource_id = course_key or course_run_id or program_uuid path = re . sub ( '{}|{}' . format ( enterprise_customer_uuid , re . escape ( resource_id ) ) , '{}' , request . path ) kwargs . pop ( 'course_key' , None ) return self . VIEWS [ path ] . as_view ( ) ( request , * args , ** kwargs )
3713	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeGases ] return mixing_simple ( zs , Vms ) elif method == IDEAL : return ideal_gas ( T , P ) elif method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP_zs ( T = T , P = P , zs = zs ) return self . eos [ 0 ] . V_g else : raise Exception ( 'Method not valid' )
141	def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
5658	def _validate_no_null_values ( self ) : for table in DB_TABLE_NAMES : null_not_ok_warning = "Null values in must-have columns in table {table}" . format ( table = table ) null_warn_warning = "Null values in good-to-have columns in table {table}" . format ( table = table ) null_not_ok_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_NOT_OK [ table ] null_warn_fields = DB_TABLE_NAME_TO_FIELDS_WHERE_NULL_OK_BUT_WARN [ table ] df = self . gtfs . get_table ( table ) for warning , fields in zip ( [ null_not_ok_warning , null_warn_warning ] , [ null_not_ok_fields , null_warn_fields ] ) : null_unwanted_df = df [ fields ] rows_having_null = null_unwanted_df . isnull ( ) . any ( 1 ) if sum ( rows_having_null ) > 0 : rows_having_unwanted_null = df [ rows_having_null . values ] self . warnings_container . add_warning ( warning , rows_having_unwanted_null , len ( rows_having_unwanted_null ) )
931	def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
7560	def get_total ( tots , node ) : if ( node . is_leaf ( ) or node . is_root ( ) ) : return 0 else : if len ( node . children ) > 2 : down_r = node . children [ 0 ] down_l = node . children [ 1 ] for child in node . children [ 2 : ] : down_l += child else : down_r , down_l = node . children lendr = sum ( 1 for i in down_r . iter_leaves ( ) ) lendl = sum ( 1 for i in down_l . iter_leaves ( ) ) up_r = node . get_sisters ( ) [ 0 ] lenur = sum ( 1 for i in up_r . iter_leaves ( ) ) lenul = tots - ( lendr + lendl + lenur ) return lendr * lendl * lenur * lenul
5755	def get_regressions ( package_descriptors , targets , building_repo_data , testing_repo_data , main_repo_data ) : regressions = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name regressions [ pkg_name ] = { } for target in targets : regressions [ pkg_name ] [ target ] = False main_version = main_repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if main_version is not None : main_ver_loose = LooseVersion ( main_version ) for repo_data in [ building_repo_data , testing_repo_data ] : version = repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) if not version or main_ver_loose > LooseVersion ( version ) : regressions [ pkg_name ] [ target ] = True return regressions
5767	def _advapi32_interpret_rsa_key_blob ( bit_size , blob_struct , blob ) : len1 = bit_size // 8 len2 = bit_size // 16 prime1_offset = len1 prime2_offset = prime1_offset + len2 exponent1_offset = prime2_offset + len2 exponent2_offset = exponent1_offset + len2 coefficient_offset = exponent2_offset + len2 private_exponent_offset = coefficient_offset + len2 public_exponent = blob_struct . rsapubkey . pubexp modulus = int_from_bytes ( blob [ 0 : prime1_offset ] [ : : - 1 ] ) prime1 = int_from_bytes ( blob [ prime1_offset : prime2_offset ] [ : : - 1 ] ) prime2 = int_from_bytes ( blob [ prime2_offset : exponent1_offset ] [ : : - 1 ] ) exponent1 = int_from_bytes ( blob [ exponent1_offset : exponent2_offset ] [ : : - 1 ] ) exponent2 = int_from_bytes ( blob [ exponent2_offset : coefficient_offset ] [ : : - 1 ] ) coefficient = int_from_bytes ( blob [ coefficient_offset : private_exponent_offset ] [ : : - 1 ] ) private_exponent = int_from_bytes ( blob [ private_exponent_offset : private_exponent_offset + len1 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'public_key' : keys . RSAPublicKey ( { 'modulus' : modulus , 'public_exponent' : public_exponent , } ) , } ) rsa_private_key = keys . RSAPrivateKey ( { 'version' : 'two-prime' , 'modulus' : modulus , 'public_exponent' : public_exponent , 'private_exponent' : private_exponent , 'prime1' : prime1 , 'prime2' : prime2 , 'exponent1' : exponent1 , 'exponent2' : exponent2 , 'coefficient' : coefficient , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'rsa' , } ) , 'private_key' : rsa_private_key , } ) return ( public_key_info , private_key_info )
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
352	def load_celebA_dataset ( path = 'data' ) : data_dir = 'celebA' filename , drive_id = "img_align_celeba.zip" , "0B7EVK8r0v71pZjFTYXZWM3FlRnM" save_path = os . path . join ( path , filename ) image_path = os . path . join ( path , data_dir ) if os . path . exists ( image_path ) : logging . info ( '[*] {} already exists' . format ( save_path ) ) else : exists_or_mkdir ( path ) download_file_from_google_drive ( drive_id , save_path ) zip_dir = '' with zipfile . ZipFile ( save_path ) as zf : zip_dir = zf . namelist ( ) [ 0 ] zf . extractall ( path ) os . remove ( save_path ) os . rename ( os . path . join ( path , zip_dir ) , image_path ) data_files = load_file_list ( path = image_path , regx = '\\.jpg' , printable = False ) for i , _v in enumerate ( data_files ) : data_files [ i ] = os . path . join ( image_path , data_files [ i ] ) return data_files
7917	def are_domains_equal ( domain1 , domain2 ) : domain1 = domain1 . encode ( "idna" ) domain2 = domain2 . encode ( "idna" ) return domain1 . lower ( ) == domain2 . lower ( )
3756	def Tautoignition ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) : methods . append ( NFPA ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'Tautoignition' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'Tautoignition' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
9217	def input ( self , file ) : if isinstance ( file , string_types ) : with open ( file ) as f : self . lexer . input ( f . read ( ) ) else : self . lexer . input ( file . read ( ) )
4461	def transform ( self , jam ) : yield jam for jam_out in self . transformer . transform ( jam ) : yield jam_out
8848	def mousePressEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mousePressEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if e . button ( ) == QtCore . Qt . LeftButton : self . open_file_requested . emit ( usd . filename , usd . line )
12375	def allowed_operations ( self ) : if self . slug is not None : return self . meta . detail_allowed_operations return self . meta . list_allowed_operations
6302	def add_package ( self , name ) : name , cls_name = parse_package_string ( name ) if name in self . package_map : return package = EffectPackage ( name ) package . load ( ) self . packages . append ( package ) self . package_map [ package . name ] = package self . polulate ( package . effect_packages )
8204	def snapshot ( self , target , defer = True , file_number = None ) : output_func = self . output_closure ( target , file_number ) if defer : self . _drawqueue . append ( output_func ) else : self . _drawqueue . append_immediate ( output_func )
10100	def get_snippet ( self , snippet_id , timeout = None ) : return self . _api_request ( self . SNIPPET_ENDPOINT % ( snippet_id ) , self . HTTP_GET , timeout = timeout )
6368	def precision_gain ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
6022	def from_fits_with_scale ( cls , file_path , hdu , pixel_scale ) : return cls ( array = array_util . numpy_array_2d_from_fits ( file_path , hdu ) , pixel_scale = pixel_scale )
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
807	def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None
1771	def visualize ( self ) : if os . path . isfile ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_file , args = ( self . workspace , ) ) elif os . path . isdir ( self . workspace ) : t = threading . Thread ( target = self . highlight_from_dir , args = ( self . workspace , ) ) t . start ( )
885	def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )
4879	def get_paginated_response ( data , request ) : url = urlparse ( request . build_absolute_uri ( ) ) . _replace ( query = None ) . geturl ( ) next_page = None previous_page = None if data [ 'next' ] : next_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'next' ] ) . query , ) next_page = next_page . rstrip ( '?' ) if data [ 'previous' ] : previous_page = "{base_url}?{query_parameters}" . format ( base_url = url , query_parameters = urlparse ( data [ 'previous' ] or "" ) . query , ) previous_page = previous_page . rstrip ( '?' ) return Response ( OrderedDict ( [ ( 'count' , data [ 'count' ] ) , ( 'next' , next_page ) , ( 'previous' , previous_page ) , ( 'results' , data [ 'results' ] ) ] ) )
4618	def parse_time ( block_time ) : return datetime . strptime ( block_time , timeFormat ) . replace ( tzinfo = timezone . utc )
1716	def pad ( num , n = 2 , sign = False ) : s = unicode ( abs ( num ) ) if len ( s ) < n : s = '0' * ( n - len ( s ) ) + s if not sign : return s if num >= 0 : return '+' + s else : return '-' + s
9633	def render_to_message ( self , extra_context = None , * args , ** kwargs ) : message = super ( TemplatedHTMLEmailMessageView , self ) . render_to_message ( extra_context , * args , ** kwargs ) if extra_context is None : extra_context = { } context = self . get_context_data ( ** extra_context ) content = self . render_html_body ( context ) message . attach_alternative ( content , mimetype = 'text/html' ) return message
9241	def fetch_tags_dates ( self ) : if self . options . verbose : print ( "Fetching dates for {} tags..." . format ( len ( self . filtered_tags ) ) ) def worker ( tag ) : self . get_time_of_tag ( tag ) threads = [ ] max_threads = 50 cnt = len ( self . filtered_tags ) for i in range ( 0 , ( cnt // max_threads ) + 1 ) : for j in range ( max_threads ) : idx = i * 50 + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( self . filtered_tags [ idx ] , ) ) threads . append ( t ) t . start ( ) if self . options . verbose > 2 : print ( "." , end = "" ) for t in threads : t . join ( ) if self . options . verbose > 2 : print ( "." ) if self . options . verbose > 1 : print ( "Fetched dates for {} tags." . format ( len ( self . tag_times_dict ) ) )
13762	def _check_next ( self ) : if self . is_initial : return True if self . before : if self . before_cursor : return True else : return False else : if self . after_cursor : return True else : return False
9229	def fetch_closed_pull_requests ( self ) : pull_requests = [ ] verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project if verbose : print ( "Fetching closed pull requests..." ) page = 1 while page > 0 : if verbose > 2 : print ( "." , end = "" ) if self . options . release_branch : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , base = self . options . release_branch ) else : rc , data = gh . repos [ user ] [ repo ] . pulls . get ( page = page , per_page = PER_PAGE_NUMBER , state = 'closed' , ) if rc == 200 : pull_requests . extend ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) if verbose > 2 : print ( "." ) if verbose > 1 : print ( "\tfetched {} closed pull requests." . format ( len ( pull_requests ) ) ) return pull_requests
8906	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if self . collection . count_documents ( { 'name' : name } ) > 0 : name = namesgenerator . get_random_name ( retry = True ) if self . collection . count_documents ( { 'name' : name } ) > 0 : if overwrite : self . collection . delete_one ( { 'name' : name } ) else : raise Exception ( "service name already registered." ) self . collection . insert_one ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
9030	def _expand_consumed_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_produced ( ) : return row = mesh . producing_row position = Point ( row_position . x + mesh . index_in_producing_row - mesh_index , row_position . y - INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
12183	def method_exists ( cls , method ) : methods = cls . API_METHODS for key in method . split ( '.' ) : methods = methods . get ( key ) if methods is None : break if isinstance ( methods , str ) : logger . debug ( '%r: %r' , method , methods ) return True return False
4374	def get_messages_payload ( self , socket , timeout = None ) : try : msgs = socket . get_multiple_client_msgs ( timeout = timeout ) data = self . encode_payload ( msgs ) except Empty : data = "" return data
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
1830	def JB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == True , target . read ( ) , cpu . PC )
5800	def system_path ( ) : ca_path = None paths = [ '/usr/lib/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-bundle.crt' , '/etc/pki/tls/certs/ca-bundle.crt' , '/etc/ssl/ca-bundle.pem' , '/usr/local/share/certs/ca-root-nss.crt' , '/etc/ssl/cert.pem' ] if 'SSL_CERT_FILE' in os . environ : paths . insert ( 0 , os . environ [ 'SSL_CERT_FILE' ] ) for path in paths : if os . path . exists ( path ) and os . path . getsize ( path ) > 0 : ca_path = path break if not ca_path : raise OSError ( pretty_message ( ) ) return ca_path
6397	def sim ( self , src , tar , qval = 2 ) : r if src == tar : return 1.0 if not src or not tar : return 0.0 q_src , q_tar = self . _get_qgrams ( src , tar , qval ) q_src_mag = sum ( q_src . values ( ) ) q_tar_mag = sum ( q_tar . values ( ) ) q_intersection_mag = sum ( ( q_src & q_tar ) . values ( ) ) return q_intersection_mag / sqrt ( q_src_mag * q_tar_mag )
1087	def concat ( a , b ) : "Same as a + b, for a and b sequences." if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) return a + b
4347	def tempo ( self , factor , audio_type = None , quick = False ) : if not is_number ( factor ) or factor <= 0 : raise ValueError ( "factor must be a positive number" ) if factor < 0.5 or factor > 2 : logger . warning ( "Using an extreme time stretching factor. " "Quality of results will be poor" ) if abs ( factor - 1.0 ) <= 0.1 : logger . warning ( "For this stretch factor, " "the stretch effect has better performance." ) if audio_type not in [ None , 'm' , 's' , 'l' ] : raise ValueError ( "audio_type must be one of None, 'm', 's', or 'l'." ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'tempo' ] if quick : effect_args . append ( '-q' ) if audio_type is not None : effect_args . append ( '-{}' . format ( audio_type ) ) effect_args . append ( '{:f}' . format ( factor ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'tempo' ) return self
7507	def _renamer ( self , tre ) : names = tre . get_leaves ( ) for name in names : name . name = self . samples [ int ( name . name ) ] return tre . write ( format = 9 )
11155	def print_big_dir ( self , top_n = 5 ) : self . assert_is_dir_and_exists ( ) size_table = sorted ( [ ( p , p . dirsize ) for p in self . select_dir ( recursive = False ) ] , key = lambda x : x [ 1 ] , reverse = True , ) for p , size in size_table [ : top_n ] : print ( "{:<9} {:<9}" . format ( repr_data_size ( size ) , p . abspath ) )
4520	def set ( self , ring , angle , color ) : pixel = self . angleToPixel ( angle , ring ) self . _set_base ( pixel , color )
7571	def revcomp ( sequence ) : "returns reverse complement of a string" sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
2113	def modify ( self , pk = None , create_on_missing = False , ** kwargs ) : if 'job_timeout' in kwargs and 'timeout' not in kwargs : kwargs [ 'timeout' ] = kwargs . pop ( 'job_timeout' ) return super ( Resource , self ) . write ( pk , create_on_missing = create_on_missing , force_on_exists = True , ** kwargs )
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] pweights = 1.0 / perrs phase = phase * 2.0 * pi_value z = npcos ( phase ) + 1.0j * npsin ( phase ) phase = nharmonics * phase psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) zn = 1.0 + 0.0j phi = pweights + 0.0j theta_aov = 0.0 for _ in range ( two_nharmonics ) : phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) alpha = npsum ( pweights * z * phi ) phi_dot_psi = npvdot ( phi , psi ) phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) alpha = alpha / phi_dot_phi theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) phi = phi * z - alpha * zn * phi . conjugate ( ) zn = zn * z theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
6545	def is_connected ( self ) : try : self . exec_command ( b"Query(ConnectionState)" ) return self . status . connection_state . startswith ( b"C(" ) except NotConnectedException : return False
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
8559	def update_lan ( self , datacenter_id , lan_id , name = None , public = None , ip_failover = None ) : data = { } if name : data [ 'name' ] = name if public is not None : data [ 'public' ] = public if ip_failover : data [ 'ipFailover' ] = ip_failover response = self . _perform_request ( url = '/datacenters/%s/lans/%s' % ( datacenter_id , lan_id ) , method = 'PATCH' , data = json . dumps ( data ) ) return response
10159	def fresh_cookies ( ctx , mold = '' ) : mold = mold or "https://github.com/Springerle/py-generic-project.git" tmpdir = os . path . join ( tempfile . gettempdir ( ) , "cc-upgrade-pygments-markdown-lexer" ) if os . path . isdir ( '.git' ) : pass if os . path . isdir ( tmpdir ) : shutil . rmtree ( tmpdir ) if os . path . exists ( mold ) : shutil . copytree ( mold , tmpdir , ignore = shutil . ignore_patterns ( ".git" , ".svn" , "*~" , ) ) else : ctx . run ( "git clone {} {}" . format ( mold , tmpdir ) ) shutil . copy2 ( "project.d/cookiecutter.json" , tmpdir ) with pushd ( '..' ) : ctx . run ( "cookiecutter --no-input {}" . format ( tmpdir ) ) if os . path . exists ( '.git' ) : ctx . run ( "git status" )
13688	def _generate_html_diff ( self , expected_fn , expected_lines , obtained_fn , obtained_lines ) : import difflib differ = difflib . HtmlDiff ( ) return differ . make_file ( fromlines = expected_lines , fromdesc = expected_fn , tolines = obtained_lines , todesc = obtained_fn , )
7503	def _byteify ( data , ignore_dicts = False ) : if isinstance ( data , unicode ) : return data . encode ( "utf-8" ) if isinstance ( data , list ) : return [ _byteify ( item , ignore_dicts = True ) for item in data ] if isinstance ( data , dict ) and not ignore_dicts : return { _byteify ( key , ignore_dicts = True ) : _byteify ( value , ignore_dicts = True ) for key , value in data . iteritems ( ) } return data
763	def createRecordSensor ( network , name , dataSource ) : regionType = "py.RecordSensor" regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) sensorRegion = network . regions [ name ] . getSelf ( ) sensorRegion . encoder = createEncoder ( ) network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) sensorRegion . dataSource = dataSource return sensorRegion
10801	def _distance_matrix ( self , a , b ) : def sq ( x ) : return ( x * x ) matrix = sq ( a [ : , 0 ] [ : , None ] - b [ : , 0 ] [ None , : ] ) for x , y in zip ( a . T [ 1 : ] , b . T [ 1 : ] ) : matrix += sq ( x [ : , None ] - y [ None , : ] ) return matrix
7716	def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roster [ jid ] if jid not in self . roster : raise KeyError ( jid ) item = RosterItem ( jid , subscription = "remove" ) self . _roster_set ( item , callback , error_callback )
4829	def _get_results ( self , identity_provider , param_name , param_value , result_field_name ) : try : kwargs = { param_name : param_value } returned = self . client . providers ( identity_provider ) . users . get ( ** kwargs ) results = returned . get ( 'results' , [ ] ) except HttpNotFoundError : LOGGER . error ( 'username not found for third party provider={provider}, {querystring_param}={id}' . format ( provider = identity_provider , querystring_param = param_name , id = param_value ) ) results = [ ] for row in results : if row . get ( param_name ) == param_value : return row . get ( result_field_name ) return None
2435	def reset_creation_info ( self ) : self . created_date_set = False self . creation_comment_set = False self . lics_list_ver_set = False
12034	def kernel_gaussian ( self , sizeMS , sigmaMS = None , forwardOnly = False ) : sigmaMS = sizeMS / 10 if sigmaMS is None else sigmaMS size , sigma = sizeMS * self . pointsPerMs , sigmaMS * self . pointsPerMs self . kernel = swhlab . common . kernel_gaussian ( size , sigma , forwardOnly ) return self . kernel
1907	def all_events ( cls ) : all_evts = set ( ) for cls , evts in cls . __all_events__ . items ( ) : all_evts . update ( evts ) return all_evts
11012	def write ( context ) : config = context . obj title = click . prompt ( 'Title' ) author = click . prompt ( 'Author' , default = config . get ( 'DEFAULT_AUTHOR' ) ) slug = slugify ( title ) creation_date = datetime . now ( ) basename = '{:%Y-%m-%d}_{}.md' . format ( creation_date , slug ) meta = ( ( 'Title' , title ) , ( 'Date' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Modified' , '{:%Y-%m-%d %H:%M}:00' . format ( creation_date ) ) , ( 'Author' , author ) , ) file_content = '' for key , value in meta : file_content += '{}: {}\n' . format ( key , value ) file_content += '\n\n' file_content += 'Text...\n\n' file_content += '![image description]({filename}/images/my-photo.jpg)\n\n' file_content += 'Text...\n\n' os . makedirs ( config [ 'CONTENT_DIR' ] , exist_ok = True ) path = os . path . join ( config [ 'CONTENT_DIR' ] , basename ) with click . open_file ( path , 'w' ) as f : f . write ( file_content ) click . echo ( path ) click . launch ( path )
4088	def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : asyncio . ensure_future ( fn ( * args , ** kwargs ) ) return wrapper return outer_decorator
6740	def get_packager ( ) : import warnings warnings . filterwarnings ( "ignore" , category = DeprecationWarning ) common_packager = get_rc ( 'common_packager' ) if common_packager : return common_packager with settings ( warn_only = True ) : with hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) : ret = _run ( 'cat /etc/fedora-release' ) if ret . succeeded : common_packager = YUM else : ret = _run ( 'cat /etc/lsb-release' ) if ret . succeeded : common_packager = APT else : for pn in PACKAGERS : ret = _run ( 'which %s' % pn ) if ret . succeeded : common_packager = pn break if not common_packager : raise Exception ( 'Unable to determine packager.' ) set_rc ( 'common_packager' , common_packager ) return common_packager
13307	def gmb ( a , b ) : return np . exp ( np . log ( a ) . mean ( ) - np . log ( b ) . mean ( ) )
129	def find_closest_point_index ( self , x , y , return_distance = False ) : ia . do_assert ( len ( self . exterior ) > 0 ) distances = [ ] for x2 , y2 in self . exterior : d = ( x2 - x ) ** 2 + ( y2 - y ) ** 2 distances . append ( d ) distances = np . sqrt ( distances ) closest_idx = np . argmin ( distances ) if return_distance : return closest_idx , distances [ closest_idx ] return closest_idx
13333	def path_resolver ( resolver , path ) : path = unipath ( path ) if is_environment ( path ) : return VirtualEnvironment ( path ) raise ResolveError
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : keep_line = False else : line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( ** vars ( ) ) )
3233	def list_rules ( client = None , ** kwargs ) : result = client . list_rules ( ** kwargs ) if not result . get ( "Rules" ) : result . update ( { "Rules" : [ ] } ) return result
4293	def _manage_args ( parser , args ) : for item in data . CONFIGURABLE_OPTIONS : action = parser . _option_string_actions [ item ] choices = default = '' input_value = getattr ( args , action . dest ) new_val = None if not args . noinput : if action . choices : choices = ' (choices: {0})' . format ( ', ' . join ( action . choices ) ) if input_value : if type ( input_value ) == list : default = ' [default {0}]' . format ( ', ' . join ( input_value ) ) else : default = ' [default {0}]' . format ( input_value ) while not new_val : prompt = '{0}{1}{2}: ' . format ( action . help , choices , default ) if action . choices in ( 'yes' , 'no' ) : new_val = utils . query_yes_no ( prompt ) else : new_val = compat . input ( prompt ) new_val = compat . clean ( new_val ) if not new_val and input_value : new_val = input_value if new_val and action . dest == 'templates' : if new_val != 'no' and not os . path . isdir ( new_val ) : sys . stdout . write ( 'Given directory does not exists, retry\n' ) new_val = False if new_val and action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) else : if not input_value and action . required : raise ValueError ( 'Option {0} is required when in no-input mode' . format ( action . dest ) ) new_val = input_value if action . dest == 'db' : action ( parser , args , new_val , action . option_strings ) new_val = getattr ( args , action . dest ) if action . dest == 'templates' and ( new_val == 'no' or not os . path . isdir ( new_val ) ) : new_val = False if action . dest in ( 'bootstrap' , 'starting_page' ) : new_val = ( new_val == 'yes' ) setattr ( args , action . dest , new_val ) return args
4096	def KIC ( N , rho , k ) : r from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
5957	def merge_ndx ( * args ) : ndxs = [ ] struct = None for fname in args : if fname . endswith ( '.ndx' ) : ndxs . append ( fname ) else : if struct is not None : raise ValueError ( "only one structure file supported" ) struct = fname fd , multi_ndx = tempfile . mkstemp ( suffix = '.ndx' , prefix = 'multi_' ) os . close ( fd ) atexit . register ( os . unlink , multi_ndx ) if struct : make_ndx = registry [ 'Make_ndx' ] ( f = struct , n = ndxs , o = multi_ndx ) else : make_ndx = registry [ 'Make_ndx' ] ( n = ndxs , o = multi_ndx ) _ , _ , _ = make_ndx ( input = [ 'q' ] , stdout = False , stderr = False ) return multi_ndx
9607	def vformat ( self , format_string , args , kwargs ) : self . _used_kwargs = { } self . _unused_kwargs = { } return super ( MemorizeFormatter , self ) . vformat ( format_string , args , kwargs )
10210	def _is_root ( ) : import os import ctypes try : return os . geteuid ( ) == 0 except AttributeError : return ctypes . windll . shell32 . IsUserAnAdmin ( ) != 0 return False
13262	def task ( func , ** config ) : if func . __name__ == func . __qualname__ : assert not func . __qualname__ in _task_list , "Can not define the same task \"{}\" twice" . format ( func . __qualname__ ) logger . debug ( "Found task %s" , func ) _task_list [ func . __qualname__ ] = Task ( plugin_class = None , func = func , config = config ) else : func . yaz_task_config = config return func
5894	def render ( self , name , value , attrs = { } ) : if value is None : value = '' final_attrs = self . build_attrs ( attrs , name = name ) quill_app = apps . get_app_config ( 'quill' ) quill_config = getattr ( quill_app , self . config ) return mark_safe ( render_to_string ( quill_config [ 'template' ] , { 'final_attrs' : flatatt ( final_attrs ) , 'value' : value , 'id' : final_attrs [ 'id' ] , 'config' : self . config , } ) )
8964	def _get_registered_executable ( exe_name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe_name ) [ 1 ] . lower ( ) != '.exe' : exe_name += '.exe' import _winreg try : key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" + exe_name value = _winreg . QueryValue ( _winreg . HKEY_LOCAL_MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except _winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
4655	def verify_authority ( self ) : try : if not self . blockchain . rpc . verify_authority ( self . json ( ) ) : raise InsufficientAuthorityError except Exception as e : raise e
11654	def fit ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) self . transformer . fit ( X . stacked_features , y , ** params ) return self
13709	def is_threat ( self , result = None , harmless_age = None , threat_score = None , threat_type = None ) : harmless_age = harmless_age if harmless_age is not None else settings . CACHED_HTTPBL_HARMLESS_AGE threat_score = threat_score if threat_score is not None else settings . CACHED_HTTPBL_THREAT_SCORE threat_type = threat_type if threat_type is not None else - 1 result = result if result is not None else self . _last_result threat = False if result is not None : if result [ 'age' ] < harmless_age and result [ 'threat' ] > threat_score : threat = True if threat_type > - 1 : if result [ 'type' ] & threat_type : threat = True else : threat = False return threat
766	def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines )
188	def clip_out_of_image ( self ) : lss_cut = [ ls_clipped for ls in self . line_strings for ls_clipped in ls . clip_out_of_image ( self . shape ) ] return LineStringsOnImage ( lss_cut , shape = self . shape )
10987	def _pick_state_im_name ( state_name , im_name , use_full_path = False ) : initial_dir = os . getcwd ( ) if ( state_name is None ) or ( im_name is None ) : wid = tk . Tk ( ) wid . withdraw ( ) if state_name is None : state_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select pre-featured state' ) os . chdir ( os . path . dirname ( state_name ) ) if im_name is None : im_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select new image' ) if ( not use_full_path ) and ( os . path . dirname ( im_name ) != '' ) : im_path = os . path . dirname ( im_name ) os . chdir ( im_path ) im_name = os . path . basename ( im_name ) else : os . chdir ( initial_dir ) return state_name , im_name
4603	def copy ( self ) : return self . __class__ ( amount = self [ "amount" ] , asset = self [ "asset" ] . copy ( ) , blockchain_instance = self . blockchain , )
1115	def _make_prefix ( self ) : fromprefix = "from%d_" % HtmlDiff . _default_prefix toprefix = "to%d_" % HtmlDiff . _default_prefix HtmlDiff . _default_prefix += 1 self . _prefix = [ fromprefix , toprefix ]
11528	def add_scalar_data ( self , token , community_id , producer_display_name , metric_name , producer_revision , submit_time , value , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time parameters [ 'value' ] = value optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' , 'submission_id' , 'submission_uuid' , 'unit' , 'reproduction_command' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'submission_id' : parameters [ 'submissionId' ] = kwargs [ key ] elif key == 'submission_uuid' : parameters [ 'submissionUuid' ] = kwargs [ key ] elif key == 'unit' : parameters [ 'unit' ] = kwargs [ key ] elif key == 'reproduction_command' : parameters [ 'reproductionCommand' ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.tracker.scalar.add' , parameters ) return response
11682	def _readline ( self ) : line = '' while 1 : readable , _ , __ = select . select ( [ self . sock ] , [ ] , [ ] , 0.5 ) if self . _stop : break if not readable : continue data = readable [ 0 ] . recv ( 1 ) if data == '\n' : break line += unicode ( data , self . encoding ) return line
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
11267	def walk ( prev , inital_path , * args , ** kw ) : for dir_path , dir_names , filenames in os . walk ( inital_path ) : for filename in filenames : yield os . path . join ( dir_path , filename )
5260	def memberness ( context ) : if context : texts = context . xpath ( './/*[local-name()="explicitMember"]/text()' ) . extract ( ) text = str ( texts ) . lower ( ) if len ( texts ) > 1 : return 2 elif 'country' in text : return 2 elif 'member' not in text : return 0 elif 'successor' in text : return 1 elif 'parent' in text : return 2 return 3
11181	def acquire ( self , * args , ** kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , ** kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
1847	def JZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC )
7660	def validate ( self , strict = True ) : ann_schema = schema . namespace_array ( self . namespace ) valid = True try : jsonschema . validate ( self . __json_light__ ( data = False ) , schema . JAMS_SCHEMA ) data_ser = [ serialize_obj ( obs ) for obs in self . data ] jsonschema . validate ( data_ser , ann_schema ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
6652	def findProgram ( self , builddir , program ) : if os . path . isfile ( os . path . join ( builddir , program ) ) : logging . info ( 'found %s' % program ) return program exact_matches = [ ] insensitive_matches = [ ] approx_matches = [ ] for path , dirs , files in os . walk ( builddir ) : if program in files : exact_matches . append ( os . path . relpath ( os . path . join ( path , program ) , builddir ) ) continue files_lower = [ f . lower ( ) for f in files ] if program . lower ( ) in files_lower : insensitive_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( program . lower ( ) ) ] ) , builddir ) ) continue pg_basen_lower_noext = os . path . splitext ( os . path . basename ( program ) . lower ( ) ) [ 0 ] for f in files_lower : if pg_basen_lower_noext in f : approx_matches . append ( os . path . relpath ( os . path . join ( path , files [ files_lower . index ( f ) ] ) , builddir ) ) if len ( exact_matches ) == 1 : logging . info ( 'found %s at %s' , program , exact_matches [ 0 ] ) return exact_matches [ 0 ] elif len ( exact_matches ) > 1 : logging . error ( '%s matches multiple executables, please use a full path (one of %s)' % ( program , ', or ' . join ( [ '"' + os . path . join ( m , program ) + '"' for m in exact_matches ] ) ) ) return None reduced_approx_matches = [ ] for m in approx_matches : root = os . path . splitext ( m ) [ 0 ] if ( m == root ) or ( root not in approx_matches ) : reduced_approx_matches . append ( m ) approx_matches = reduced_approx_matches for matches in ( insensitive_matches , approx_matches ) : if len ( matches ) == 1 : logging . info ( 'found %s at %s' % ( program , matches [ 0 ] ) ) return matches [ 0 ] elif len ( matches ) > 1 : logging . error ( '%s is similar to several executables found. Please use an exact name:\n%s' % ( program , '\n' . join ( matches ) ) ) return None logging . error ( 'could not find program "%s" to debug' % program ) return None
8608	def remove_group_user ( self , group_id , user_id ) : response = self . _perform_request ( url = '/um/groups/%s/users/%s' % ( group_id , user_id ) , method = 'DELETE' ) return response
10637	def get_element_mfr_dictionary ( self ) : element_symbols = self . material . elements element_mfrs = self . get_element_mfrs ( ) result = dict ( ) for s , mfr in zip ( element_symbols , element_mfrs ) : result [ s ] = mfr return result
5876	def check_link_tag ( self ) : node = self . article . raw_doc meta = self . parser . getElementsByTag ( node , tag = 'link' , attr = 'rel' , value = 'image_src' ) for item in meta : src = self . parser . getAttribute ( item , attr = 'href' ) if src : return self . get_image ( src , extraction_type = 'linktag' ) return None
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : r if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
5171	def auto_client ( cls , host , server , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : client = { "mode" : "p2p" , "nobind" : True , "resolv_retry" : "infinite" , "tls_client" : True } port = server . get ( 'port' ) or 1195 client [ 'remote' ] = [ { 'host' : host , 'port' : port } ] if server . get ( 'proto' ) == 'tcp-server' : client [ 'proto' ] = 'tcp-client' else : client [ 'proto' ] = 'udp' if 'server' in server or 'server_bridge' in server : client [ 'pull' ] = True if 'tls_server' not in server or not server [ 'tls_server' ] : client [ 'tls_client' ] = False ns_cert_type = { None : '' , '' : '' , 'client' : 'server' } client [ 'ns_cert_type' ] = ns_cert_type [ server . get ( 'ns_cert_type' ) ] remote_cert_tls = { None : '' , '' : '' , 'client' : 'server' } client [ 'remote_cert_tls' ] = remote_cert_tls [ server . get ( 'remote_cert_tls' ) ] copy_keys = [ 'name' , 'dev_type' , 'dev' , 'comp_lzo' , 'auth' , 'cipher' , 'ca' , 'cert' , 'key' , 'pkcs12' , 'mtu_disc' , 'mtu_test' , 'fragment' , 'mssfix' , 'keepalive' , 'persist_tun' , 'mute' , 'persist_key' , 'script_security' , 'user' , 'group' , 'log' , 'mute_replay_warnings' , 'secret' , 'reneg_sec' , 'tls_timeout' , 'tls_cipher' , 'float' , 'fast_io' , 'verb' ] for key in copy_keys : if key in server : client [ key ] = server [ key ] files = cls . _auto_client_files ( client , ca_path , ca_contents , cert_path , cert_contents , key_path , key_contents ) return { 'openvpn' : [ client ] , 'files' : files }
2448	def set_pkg_file_name ( self , doc , name ) : self . assert_package_exists ( ) if not self . package_file_name_set : self . package_file_name_set = True doc . package . file_name = name return True else : raise CardinalityError ( 'Package::FileName' )
7827	def stream_element_handler ( element_name , usage_restriction = None ) : def decorator ( func ) : func . _pyxmpp_stream_element_handled = element_name func . _pyxmpp_usage_restriction = usage_restriction return func return decorator
13349	def launch ( prompt_prefix = None ) : if prompt_prefix : os . environ [ 'PROMPT' ] = prompt ( prompt_prefix ) subprocess . call ( cmd ( ) , env = os . environ . data )
1966	def syscall ( self ) : index = self . _syscall_abi . syscall_number ( ) try : table = getattr ( linux_syscalls , self . current . machine ) name = table . get ( index , None ) implementation = getattr ( self , name ) except ( AttributeError , KeyError ) : if name is not None : raise SyscallNotImplemented ( index , name ) else : raise Exception ( f"Bad syscall index, {index}" ) return self . _syscall_abi . invoke ( implementation )
7101	def on_marker ( self , mid ) : self . marker = Circle ( __id__ = mid ) self . parent ( ) . markers [ mid ] = self self . marker . setTag ( mid ) d = self . declaration if d . clickable : self . set_clickable ( d . clickable ) del self . options
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
1253	def print_state ( self ) : def tile_string ( value ) : if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
6685	def update ( kernel = False ) : manager = MANAGER cmds = { 'yum -y --color=never' : { False : '--exclude=kernel* update' , True : 'update' } } cmd = cmds [ manager ] [ kernel ] run_as_root ( "%(manager)s %(cmd)s" % locals ( ) )
3308	def _run_flup ( app , config , mode ) : if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
982	def create ( * args , ** kwargs ) : impl = kwargs . pop ( 'implementation' , None ) if impl is None : impl = Configuration . get ( 'nupic.opf.sdrClassifier.implementation' ) if impl == 'py' : return SDRClassifier ( * args , ** kwargs ) elif impl == 'cpp' : return FastSDRClassifier ( * args , ** kwargs ) elif impl == 'diff' : return SDRClassifierDiff ( * args , ** kwargs ) else : raise ValueError ( 'Invalid classifier implementation (%r). Value must be ' '"py", "cpp" or "diff".' % impl )
7944	def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
6363	def dist ( self , src , tar ) : if src == tar : return 0.0 src_comp = self . _rle . encode ( self . _bwt . encode ( src ) ) tar_comp = self . _rle . encode ( self . _bwt . encode ( tar ) ) concat_comp = self . _rle . encode ( self . _bwt . encode ( src + tar ) ) concat_comp2 = self . _rle . encode ( self . _bwt . encode ( tar + src ) ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
3410	def remove_from_model ( self , model = None , make_dependent_reactions_nonfunctional = True ) : warn ( "Use cobra.manipulation.remove_genes instead" ) if model is not None : if model != self . _model : raise Exception ( "%s is a member of %s, not %s" % ( repr ( self ) , repr ( self . _model ) , repr ( model ) ) ) if self . _model is None : raise Exception ( '%s is not in a model' % repr ( self ) ) if make_dependent_reactions_nonfunctional : gene_state = 'False' else : gene_state = 'True' the_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( self . id ) ) associated_groups = self . _model . get_associated_groups ( self ) for group in associated_groups : group . remove_members ( self ) self . _model . genes . remove ( self ) self . _model = None for the_reaction in list ( self . _reaction ) : the_reaction . _gene_reaction_rule = the_gene_re . sub ( gene_state , the_reaction . gene_reaction_rule ) the_reaction . _genes . remove ( self ) the_gene_reaction_relation = the_reaction . gene_reaction_rule for other_gene in the_reaction . _genes : other_gene_re = re . compile ( '(^|(?<=( |\()))%s(?=( |\)|$))' % re . escape ( other_gene . id ) ) the_gene_reaction_relation = other_gene_re . sub ( 'True' , the_gene_reaction_relation ) if not eval ( the_gene_reaction_relation ) : the_reaction . lower_bound = 0 the_reaction . upper_bound = 0 self . _reaction . clear ( )
11818	def expected_utility ( a , s , U , mdp ) : "The expected utility of doing a in state s, according to the MDP and U." return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
5353	def retain_identities ( self , retention_time ) : enrich_es = self . conf [ 'es_enrichment' ] [ 'url' ] sortinghat_db = self . db current_data_source = self . get_backend ( self . backend_section ) active_data_sources = self . config . get_active_data_sources ( ) if retention_time is None : logger . debug ( "[identities retention] Retention policy disabled, no identities will be deleted." ) return if retention_time <= 0 : logger . debug ( "[identities retention] Retention time must be greater than 0." ) return retain_identities ( retention_time , enrich_es , sortinghat_db , current_data_source , active_data_sources )
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
13497	def clone ( self ) : t = Tag ( self . version . major , self . version . minor , self . version . patch ) if self . revision is not None : t . revision = self . revision . clone ( ) return t
12081	def figure_protocols ( self ) : self . log . debug ( "creating overlayed protocols plot" ) self . figure ( ) for sweep in range ( self . abf . sweeps ) : self . abf . setsweep ( sweep ) plt . plot ( self . abf . protoX , self . abf . protoY , color = 'r' ) self . marginX = 0 self . decorate ( protocol = True )
3275	def handle_delete ( self ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) return True
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
9200	def _sort_lows_and_highs ( func ) : "Decorator for extract_cycles" @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : for low , high , mult in func ( * args , ** kwargs ) : if low < high : yield low , high , mult else : yield high , low , mult return wrapper
9585	def write_compressed_var_array ( fd , array , name ) : bd = BytesIO ( ) write_var_array ( bd , array , name ) data = zlib . compress ( bd . getvalue ( ) ) bd . close ( ) fd . write ( struct . pack ( 'b3xI' , etypes [ 'miCOMPRESSED' ] [ 'n' ] , len ( data ) ) ) fd . write ( data )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
7698	def as_xml ( self , parent = None ) : if parent is not None : element = ElementTree . SubElement ( parent , ITEM_TAG ) else : element = ElementTree . Element ( ITEM_TAG ) element . set ( "jid" , unicode ( self . jid ) ) if self . name is not None : element . set ( "name" , self . name ) if self . subscription is not None : element . set ( "subscription" , self . subscription ) if self . ask : element . set ( "ask" , self . ask ) if self . approved : element . set ( "approved" , "true" ) for group in self . groups : ElementTree . SubElement ( element , GROUP_TAG ) . text = group return element
737	def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] with FileRecordStream ( outputFile , write = True , fields = fields ) as o : files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] while not all ( r is None for r in records ) : indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] r = min ( records , key = itemgetter ( * key ) ) o . appendRecord ( r ) index = records . index ( r ) records [ index ] = files [ index ] . getNextRecord ( ) for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )
960	def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , "jsonschema" , opfJsonSchemaFilename ) jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return
11375	def _crawl_elsevier_and_find_issue_xml ( self ) : self . _found_issues = [ ] if not self . path and not self . package_name : for issue in self . conn . _get_issues ( ) : dirname = issue . rstrip ( '/issue.xml' ) try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) else : def visit ( dummy , dirname , names ) : if "issue.xml" in names : try : self . _normalize_issue_dir_with_dtd ( dirname ) self . _found_issues . append ( dirname ) except Exception as err : register_exception ( ) print ( "ERROR: can't normalize %s: %s" % ( dirname , err ) ) walk ( self . path , visit , None )
3881	async def _sync ( self ) : logger . info ( 'Syncing events since {}' . format ( self . _sync_timestamp ) ) try : res = await self . _client . sync_all_new_events ( hangouts_pb2 . SyncAllNewEventsRequest ( request_header = self . _client . get_request_header ( ) , last_sync_timestamp = parsers . to_timestamp ( self . _sync_timestamp ) , max_response_size_bytes = 1048576 , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to sync events, some events may be lost: {}' . format ( e ) ) else : for conv_state in res . conversation_state : conv_id = conv_state . conversation_id . id conv = self . _conv_dict . get ( conv_id , None ) if conv is not None : conv . update_conversation ( conv_state . conversation ) for event_ in conv_state . event : timestamp = parsers . from_timestamp ( event_ . timestamp ) if timestamp > self . _sync_timestamp : await self . _on_event ( event_ ) else : self . _add_conversation ( conv_state . conversation , conv_state . event , conv_state . event_continuation_token )
2925	def _on_ready ( self , my_task ) : assert my_task is not None self . test ( ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) if not mutex . testandset ( ) : return for assignment in self . pre_assign : assignment . assign ( my_task , my_task ) self . _on_ready_before_hook ( my_task ) self . reached_event . emit ( my_task . workflow , my_task ) self . _on_ready_hook ( my_task ) if self . ready_event . emit ( my_task . workflow , my_task ) : for assignment in self . post_assign : assignment . assign ( my_task , my_task ) for lock in self . locks : mutex = my_task . workflow . _get_mutex ( lock ) mutex . unlock ( ) self . finished_event . emit ( my_task . workflow , my_task )
5127	def stop_collecting_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . collect_data = False
3487	def _check ( value , message ) : if value is None : LOGGER . error ( 'Error: LibSBML returned a null value trying ' 'to <' + message + '>.' ) elif type ( value ) is int : if value == libsbml . LIBSBML_OPERATION_SUCCESS : return else : LOGGER . error ( 'Error encountered trying to <' + message + '>.' ) LOGGER . error ( 'LibSBML error code {}: {}' . format ( str ( value ) , libsbml . OperationReturnValue_toString ( value ) . strip ( ) ) ) else : return
12000	def _sign_data ( self , data , options ) : if options [ 'signature_algorithm_id' ] not in self . signature_algorithms : raise Exception ( 'Unknown signature algorithm id: %d' % options [ 'signature_algorithm_id' ] ) signature_algorithm = self . signature_algorithms [ options [ 'signature_algorithm_id' ] ] algorithm = self . _get_algorithm_info ( signature_algorithm ) key_salt = get_random_bytes ( algorithm [ 'salt_size' ] ) key = self . _generate_key ( options [ 'signature_passphrase_id' ] , self . signature_passphrases , key_salt , algorithm ) data = self . _encode ( data , algorithm , key ) return data + key_salt
7787	def _try_backup_item ( self ) : if not self . _backup_state : return False item = self . cache . get_item ( self . address , self . _backup_state ) if item : self . _object_handler ( item . address , item . value , item . state ) return True else : False
9406	def _cleanup ( self ) : self . exit ( ) workspace = osp . join ( os . getcwd ( ) , 'octave-workspace' ) if osp . exists ( workspace ) : os . remove ( workspace )
5689	def get_day_start_ut_span ( self ) : cur = self . conn . cursor ( ) first_day_start_ut , last_day_start_ut = cur . execute ( "SELECT min(day_start_ut), max(day_start_ut) FROM days;" ) . fetchone ( ) return first_day_start_ut , last_day_start_ut
5091	def get_clear_catalog_id_action ( description = None ) : description = description or _ ( "Unlink selected objects from existing course catalogs" ) def clear_catalog_id ( modeladmin , request , queryset ) : queryset . update ( catalog = None ) clear_catalog_id . short_description = description return clear_catalog_id
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
1552	def _get_base_component ( self ) : comp = topology_pb2 . Component ( ) comp . name = self . name comp . spec = topology_pb2 . ComponentObjectSpec . Value ( "PYTHON_CLASS_NAME" ) comp . class_name = self . python_class_path comp . config . CopyFrom ( self . _get_comp_config ( ) ) return comp
363	def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
9381	def set_sla ( obj , metric , sub_metric , rules ) : if not hasattr ( obj , 'sla_map' ) : return False rules_list = rules . split ( ) for rule in rules_list : if '<' in rule : stat , threshold = rule . split ( '<' ) sla = SLA ( metric , sub_metric , stat , threshold , 'lt' ) elif '>' in rule : stat , threshold = rule . split ( '>' ) sla = SLA ( metric , sub_metric , stat , threshold , 'gt' ) else : if hasattr ( obj , 'logger' ) : obj . logger . error ( 'Unsupported SLA type defined : ' + rule ) sla = None obj . sla_map [ metric ] [ sub_metric ] [ stat ] = sla if hasattr ( obj , 'sla_list' ) : obj . sla_list . append ( sla ) return True
10711	def models_preparing ( app ) : def wrapper ( resource , parent ) : if isinstance ( resource , DeclarativeMeta ) : resource = ListResource ( resource ) if not getattr ( resource , '__parent__' , None ) : resource . __parent__ = parent return resource resources_preparing_factory ( app , wrapper )
6144	def DSP_callback_tic ( self ) : if self . Tcapture > 0 : self . DSP_tic . append ( time . time ( ) - self . start_time )
3755	def Carcinogen ( CASRN , AvailableMethods = False , Method = None ) : r methods = [ COMBINED , IARC , NTP ] if AvailableMethods : return methods if not Method : Method = methods [ 0 ] if Method == IARC : if CASRN in IARC_data . index : status = IARC_codes [ IARC_data . at [ CASRN , 'group' ] ] else : status = UNLISTED elif Method == NTP : if CASRN in NTP_data . index : status = NTP_codes [ NTP_data . at [ CASRN , 'Listing' ] ] else : status = UNLISTED elif Method == COMBINED : status = { } for method in methods [ 1 : ] : status [ method ] = Carcinogen ( CASRN , Method = method ) else : raise Exception ( 'Failure in in function' ) return status
3669	def Rachford_Rice_flash_error ( V_over_F , zs , Ks ) : r return sum ( [ zi * ( Ki - 1. ) / ( 1. + V_over_F * ( Ki - 1. ) ) for Ki , zi in zip ( Ks , zs ) ] )
5224	def market_timing ( ticker , dt , timing = 'EOD' , tz = 'local' ) -> str : logger = logs . get_logger ( market_timing ) exch = pd . Series ( exch_info ( ticker = ticker ) ) if any ( req not in exch . index for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return '' mkt_time = { 'BOD' : exch . day [ 0 ] , 'FINISHED' : exch . allday [ - 1 ] } . get ( timing , exch . day [ - 1 ] ) cur_dt = pd . Timestamp ( str ( dt ) ) . strftime ( '%Y-%m-%d' ) if tz == 'local' : return f'{cur_dt} {mkt_time}' return timezone . tz_convert ( f'{cur_dt} {mkt_time}' , to_tz = tz , from_tz = exch . tz )
8812	def get_used_ips ( session , ** kwargs ) : LOG . debug ( "Getting used IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , func . count ( models . IPAddress . address ) ) query = query . group_by ( models . Subnet . segment_id ) query = _filter ( query , ** kwargs ) reuse_window = timeutils . utcnow ( ) - datetime . timedelta ( seconds = cfg . CONF . QUARK . ipam_reuse_after ) query = query . outerjoin ( models . IPAddress , and_ ( models . Subnet . id == models . IPAddress . subnet_id , or_ ( not_ ( models . IPAddress . lock_id . is_ ( None ) ) , models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPAddress . deallocated_at > reuse_window ) ) ) query = query . outerjoin ( models . IPPolicyCIDR , and_ ( models . Subnet . ip_policy_id == models . IPPolicyCIDR . ip_policy_id , models . IPAddress . address >= models . IPPolicyCIDR . first_ip , models . IPAddress . address <= models . IPPolicyCIDR . last_ip ) ) query = query . filter ( or_ ( models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPPolicyCIDR . id . is_ ( None ) ) ) ret = ( ( segment_id , address_count ) for segment_id , address_count in query . all ( ) ) return dict ( ret )
11537	def set_pin_direction ( self , pin , direction ) : if type ( pin ) is list : for p in pin : self . set_pin_direction ( p , direction ) return pin_id = self . _pin_mapping . get ( pin , None ) if pin_id and type ( direction ) is ahio . Direction : self . _set_pin_direction ( pin_id , direction ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7065	def delete_ec2_nodes ( instance_id_list , client = None ) : if not client : client = boto3 . client ( 'ec2' ) resp = client . terminate_instances ( InstanceIds = instance_id_list ) return resp
3785	def interpolate_P ( self , T , P , name ) : r key = ( name , self . interpolation_T , self . interpolation_P , self . interpolation_property , self . interpolation_property_inv ) if key in self . tabular_data_interpolators : extrapolator , spline = self . tabular_data_interpolators [ key ] else : Ts , Ps , properties = self . tabular_data [ name ] if self . interpolation_T : Ts2 = [ self . interpolation_T ( T2 ) for T2 in Ts ] else : Ts2 = Ts if self . interpolation_P : Ps2 = [ self . interpolation_P ( P2 ) for P2 in Ps ] else : Ps2 = Ps if self . interpolation_property : properties2 = [ self . interpolation_property ( p ) for p in properties ] else : properties2 = properties extrapolator = interp2d ( Ts2 , Ps2 , properties2 ) if len ( properties ) >= 5 : spline = interp2d ( Ts2 , Ps2 , properties2 , kind = 'cubic' ) else : spline = None self . tabular_data_interpolators [ key ] = ( extrapolator , spline ) Ts , Ps , properties = self . tabular_data [ name ] if T < Ts [ 0 ] or T > Ts [ - 1 ] or not spline or P < Ps [ 0 ] or P > Ps [ - 1 ] : tool = extrapolator else : tool = spline if self . interpolation_T : T = self . interpolation_T ( T ) if self . interpolation_P : P = self . interpolation_T ( P ) prop = tool ( T , P ) if self . interpolation_property : prop = self . interpolation_property_inv ( prop ) return float ( prop )
6638	def getScript ( self , scriptname ) : script = self . description . get ( 'scripts' , { } ) . get ( scriptname , None ) if script is not None : if isinstance ( script , str ) or isinstance ( script , type ( u'unicode string' ) ) : import shlex script = shlex . split ( script ) if len ( script ) and script [ 0 ] . lower ( ) . endswith ( '.py' ) : if not os . path . isabs ( script [ 0 ] ) : absscript = os . path . abspath ( os . path . join ( self . path , script [ 0 ] ) ) logger . debug ( 'rewriting script %s to be absolute path %s' , script [ 0 ] , absscript ) script [ 0 ] = absscript import sys script = [ sys . executable ] + script return script
5359	def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
232	def plot_sector_exposures_longshort ( long_exposures , short_exposures , sector_dict = SECTORS , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( long_exposures [ 0 ] . index , long_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . stackplot ( long_exposures [ 0 ] . index , short_exposures , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Long and short exposures to sectors' , ylabel = 'Proportion of long/short exposure in sectors' ) ax . legend ( loc = 'upper left' , frameon = True , framealpha = 0.5 ) return ax
2204	def find_exe ( name , multi = False , path = None ) : candidates = find_path ( name , path = path , exact = True ) mode = os . X_OK | os . F_OK results = ( fpath for fpath in candidates if os . access ( fpath , mode ) and not isdir ( fpath ) ) if not multi : for fpath in results : return fpath else : return list ( results )
8772	def _remove_default_tz_bindings ( self , context , network_id ) : default_tz = CONF . NVP . default_tz if not default_tz : LOG . warn ( "additional_default_tz_types specified, " "but no default_tz. Skipping " "_remove_default_tz_bindings()." ) return if not network_id : LOG . warn ( "neutron network_id not specified, skipping " "_remove_default_tz_bindings()" ) return for net_type in CONF . NVP . additional_default_tz_types : if net_type in TZ_BINDINGS : binding = TZ_BINDINGS [ net_type ] binding . remove ( context , default_tz , network_id ) else : LOG . warn ( "Unknown default tz type %s" % ( net_type ) )
1300	def WindowFromPoint ( x : int , y : int ) -> int : return ctypes . windll . user32 . WindowFromPoint ( ctypes . wintypes . POINT ( x , y ) )
11438	def _create_record_lxml ( marcxml , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : parser = etree . XMLParser ( dtd_validation = correct , recover = ( verbose <= 3 ) ) if correct : marcxml = '<?xml version="1.0" encoding="UTF-8"?>\n' '<collection>\n%s\n</collection>' % ( marcxml , ) try : tree = etree . parse ( StringIO ( marcxml ) , parser ) except Exception as e : raise InvenioBibRecordParserError ( str ( e ) ) record = { } field_position_global = 0 controlfield_iterator = tree . iter ( tag = '{*}controlfield' ) for controlfield in controlfield_iterator : tag = controlfield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = ' ' ind2 = ' ' text = controlfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) subfields = [ ] if text or keep_singletons : field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) datafield_iterator = tree . iter ( tag = '{*}datafield' ) for datafield in datafield_iterator : tag = datafield . attrib . get ( 'tag' , '!' ) . encode ( "UTF-8" ) ind1 = datafield . attrib . get ( 'ind1' , '!' ) . encode ( "UTF-8" ) ind2 = datafield . attrib . get ( 'ind2' , '!' ) . encode ( "UTF-8" ) if ind1 in ( '' , '_' ) : ind1 = ' ' if ind2 in ( '' , '_' ) : ind2 = ' ' subfields = [ ] subfield_iterator = datafield . iter ( tag = '{*}subfield' ) for subfield in subfield_iterator : code = subfield . attrib . get ( 'code' , '!' ) . encode ( "UTF-8" ) text = subfield . text if text is None : text = '' else : text = text . encode ( "UTF-8" ) if text or keep_singletons : subfields . append ( ( code , text ) ) if subfields or keep_singletons : text = '' field_position_global += 1 record . setdefault ( tag , [ ] ) . append ( ( subfields , ind1 , ind2 , text , field_position_global ) ) return record
6288	def find_commands ( command_dir : str ) -> List [ str ] : if not command_dir : return [ ] return [ name for _ , name , is_pkg in pkgutil . iter_modules ( [ command_dir ] ) if not is_pkg and not name . startswith ( '_' ) ]
520	def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
3163	def create ( self , workflow_id , email_id , data ) : self . workflow_id = workflow_id self . email_id = email_id if 'email_address' not in data : raise KeyError ( 'The automation email queue must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
9205	def before_constant ( self , constant , key ) : newlines_split = split_on_newlines ( constant ) for c in newlines_split : if is_newline ( c ) : self . current . advance_line ( ) if self . current . line > self . target . line : return self . STOP else : advance_by = len ( c ) if self . is_on_targetted_node ( advance_by ) : self . found_path = deepcopy ( self . current_path ) return self . STOP self . current . advance_columns ( advance_by )
11750	def _register_blueprint ( self , app , bp , bundle_path , child_path , description ) : base_path = sanitize_path ( self . _journey_path + bundle_path + child_path ) app . register_blueprint ( bp , url_prefix = base_path ) return { 'name' : bp . name , 'path' : child_path , 'import_name' : bp . import_name , 'description' : description , 'routes' : self . get_blueprint_routes ( app , base_path ) }
11357	def format_arxiv_id ( arxiv_id ) : if arxiv_id and "/" not in arxiv_id and "arXiv" not in arxiv_id : return "arXiv:%s" % ( arxiv_id , ) elif arxiv_id and '.' not in arxiv_id and arxiv_id . lower ( ) . startswith ( 'arxiv:' ) : return arxiv_id [ 6 : ] else : return arxiv_id
9785	def bookmark ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) try : PolyaxonClient ( ) . build_job . bookmark ( user , project_name , _build ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark build job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build job bookmarked." )
4607	def blacklist ( self , account ) : assert callable ( self . blockchain . account_whitelist ) return self . blockchain . account_whitelist ( account , lists = [ "black" ] , account = self )
399	def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )
5775	def _advapi32_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : algo = private_key . algorithm if algo == 'rsa' and hash_algorithm == 'raw' : padded_data = add_pkcs1v15_signature_padding ( private_key . byte_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) padded_data = add_pss_padding ( hash_algorithm , hash_length , private_key . bit_size , data ) return raw_rsa_private_crypt ( private_key , padded_data ) if private_key . algorithm == 'dsa' and hash_algorithm == 'md5' : raise ValueError ( pretty_message ( ) ) hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( private_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) out_len = new ( advapi32 , 'DWORD *' ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , null ( ) , out_len ) handle_error ( res ) buffer_length = deref ( out_len ) buffer_ = buffer_from_bytes ( buffer_length ) res = advapi32 . CryptSignHashW ( hash_handle , Advapi32Const . AT_SIGNATURE , null ( ) , 0 , buffer_ , out_len ) handle_error ( res ) output = bytes_from_buffer ( buffer_ , deref ( out_len ) ) output = output [ : : - 1 ] if algo == 'dsa' : half_len = len ( output ) // 2 output = output [ half_len : ] + output [ : half_len ] output = algos . DSASignature . from_p1363 ( output ) . dump ( ) return output finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
6553	def fix_variable ( self , v , value ) : variables = self . variables try : idx = variables . index ( v ) except ValueError : raise ValueError ( "given variable {} is not part of the constraint" . format ( v ) ) if value not in self . vartype . value : raise ValueError ( "expected value to be in {}, received {} instead" . format ( self . vartype . value , value ) ) configurations = frozenset ( config [ : idx ] + config [ idx + 1 : ] for config in self . configurations if config [ idx ] == value ) if not configurations : raise UnsatError ( "fixing {} to {} makes this constraint unsatisfiable" . format ( v , value ) ) variables = variables [ : idx ] + variables [ idx + 1 : ] self . configurations = configurations self . variables = variables def func ( * args ) : return args in configurations self . func = func self . name = '{} ({} fixed to {})' . format ( self . name , v , value )
1210	def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table
13810	def MakeDescriptor ( desc_proto , package = '' , build_file_if_cpp = True , syntax = None ) : if api_implementation . Type ( ) == 'cpp' and build_file_if_cpp : from typy . google . protobuf import descriptor_pb2 file_descriptor_proto = descriptor_pb2 . FileDescriptorProto ( ) file_descriptor_proto . message_type . add ( ) . MergeFrom ( desc_proto ) proto_name = str ( uuid . uuid4 ( ) ) if package : file_descriptor_proto . name = os . path . join ( package . replace ( '.' , '/' ) , proto_name + '.proto' ) file_descriptor_proto . package = package else : file_descriptor_proto . name = proto_name + '.proto' _message . default_pool . Add ( file_descriptor_proto ) result = _message . default_pool . FindFileByName ( file_descriptor_proto . name ) if _USE_C_DESCRIPTORS : return result . message_types_by_name [ desc_proto . name ] full_message_name = [ desc_proto . name ] if package : full_message_name . insert ( 0 , package ) enum_types = { } for enum_proto in desc_proto . enum_type : full_name = '.' . join ( full_message_name + [ enum_proto . name ] ) enum_desc = EnumDescriptor ( enum_proto . name , full_name , None , [ EnumValueDescriptor ( enum_val . name , ii , enum_val . number ) for ii , enum_val in enumerate ( enum_proto . value ) ] ) enum_types [ full_name ] = enum_desc nested_types = { } for nested_proto in desc_proto . nested_type : full_name = '.' . join ( full_message_name + [ nested_proto . name ] ) nested_desc = MakeDescriptor ( nested_proto , package = '.' . join ( full_message_name ) , build_file_if_cpp = False , syntax = syntax ) nested_types [ full_name ] = nested_desc fields = [ ] for field_proto in desc_proto . field : full_name = '.' . join ( full_message_name + [ field_proto . name ] ) enum_desc = None nested_desc = None if field_proto . HasField ( 'type_name' ) : type_name = field_proto . type_name full_type_name = '.' . join ( full_message_name + [ type_name [ type_name . rfind ( '.' ) + 1 : ] ] ) if full_type_name in nested_types : nested_desc = nested_types [ full_type_name ] elif full_type_name in enum_types : enum_desc = enum_types [ full_type_name ] field = FieldDescriptor ( field_proto . name , full_name , field_proto . number - 1 , field_proto . number , field_proto . type , FieldDescriptor . ProtoTypeToCppProtoType ( field_proto . type ) , field_proto . label , None , nested_desc , enum_desc , None , False , None , options = field_proto . options , has_default_value = False ) fields . append ( field ) desc_name = '.' . join ( full_message_name ) return Descriptor ( desc_proto . name , desc_name , None , None , fields , list ( nested_types . values ( ) ) , list ( enum_types . values ( ) ) , [ ] , options = desc_proto . options )
13551	def _put_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . putURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 201 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] self [ "expiration" ] = None dict . __init__ ( self , { } )
11315	def update_notes ( self ) : fields = record_get_field_instances ( self . record , '500' ) for field in fields : subs = field_get_subfields ( field ) for sub in subs . get ( 'a' , [ ] ) : sub = sub . strip ( ) if sub . startswith ( "*" ) and sub . endswith ( "*" ) : record_delete_field ( self . record , tag = "500" , field_position_global = field [ 4 ] )
12906	def objHasUnsavedChanges ( self ) : if not self . obj : return False return self . obj . hasUnsavedChanges ( cascadeObjects = True )
10215	def rank_subgraph_by_node_filter ( graph : BELGraph , node_predicates : Union [ NodePredicate , Iterable [ NodePredicate ] ] , annotation : str = 'Subgraph' , reverse : bool = True , ) -> List [ Tuple [ str , int ] ] : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) r2 = count_dict_values ( r1 ) return sorted ( r2 . items ( ) , key = itemgetter ( 1 ) , reverse = reverse )
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
3263	def get_workspace ( self , name ) : workspaces = self . get_workspaces ( names = name ) return self . _return_first_item ( workspaces )
4093	def addBorrowers ( self , * borrowers ) : self . _borrowers . extend ( borrowers ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB borrower(s): %s' % ', ' . join ( [ str ( x ) for x in self . _borrowers ] ) ) return self
9563	def _as_dict ( self , r ) : d = dict ( ) for i , f in enumerate ( self . _field_names ) : d [ f ] = r [ i ] if i < len ( r ) else None return d
10875	def get_polydisp_pts_wts ( kfki , sigkf , dist_type = 'gaussian' , nkpts = 3 ) : if dist_type . lower ( ) == 'gaussian' : pts , wts = np . polynomial . hermite . hermgauss ( nkpts ) kfkipts = np . abs ( kfki + sigkf * np . sqrt ( 2 ) * pts ) elif dist_type . lower ( ) == 'laguerre' or dist_type . lower ( ) == 'gamma' : k_scale = sigkf ** 2 / kfki associated_order = kfki ** 2 / sigkf ** 2 - 1 max_order = 150 if associated_order > max_order or associated_order < ( - 1 + 1e-3 ) : warnings . warn ( 'Numerically unstable sigk, clipping' , RuntimeWarning ) associated_order = np . clip ( associated_order , - 1 + 1e-3 , max_order ) kfkipts , wts = la_roots ( nkpts , associated_order ) kfkipts *= k_scale else : raise ValueError ( 'dist_type must be either gaussian or laguerre' ) return kfkipts , wts / wts . sum ( )
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
2734	def get_object ( cls , api_token , ip ) : floating_ip = cls ( token = api_token , ip = ip ) floating_ip . load ( ) return floating_ip
5540	def open ( self , input_id , ** kwargs ) : if not isinstance ( input_id , str ) : return input_id . open ( self . tile , ** kwargs ) if input_id not in self . params [ "input" ] : raise ValueError ( "%s not found in config as input file" % input_id ) return self . params [ "input" ] [ input_id ] . open ( self . tile , ** kwargs )
8708	def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
10937	def update_Broyden_J ( self ) : CLOG . debug ( 'Broyden update.' ) delta_vals = self . param_vals - self . _last_vals delta_residuals = self . calc_residuals ( ) - self . _last_residuals nrm = np . sqrt ( np . dot ( delta_vals , delta_vals ) ) direction = delta_vals / nrm vals = delta_residuals / nrm self . _rank_1_J_update ( direction , vals ) self . JTJ = np . dot ( self . J , self . J . T )
13455	def _parse_args ( args ) : parser = argparse . ArgumentParser ( description = "Remove and/or rearrange " + "sections from each line of a file(s)." , usage = _usage ( ) [ len ( 'usage: ' ) : ] ) parser . add_argument ( '-b' , "--bytes" , action = 'store' , type = lst , default = [ ] , help = "Bytes to select" ) parser . add_argument ( '-c' , "--chars" , action = 'store' , type = lst , default = [ ] , help = "Character to select" ) parser . add_argument ( '-f' , "--fields" , action = 'store' , type = lst , default = [ ] , help = "Fields to select" ) parser . add_argument ( '-d' , "--delimiter" , action = 'store' , default = "\t" , help = "Sets field delimiter(default is TAB)" ) parser . add_argument ( '-e' , "--regex" , action = 'store_true' , help = 'Enable regular expressions to be used as input ' + 'delimiter' ) parser . add_argument ( '-s' , '--skip' , action = 'store_true' , help = "Skip lines that do not contain input delimiter." ) parser . add_argument ( '-S' , "--separator" , action = 'store' , default = "\t" , help = "Sets field separator for output." ) parser . add_argument ( 'file' , nargs = '*' , default = "-" , help = "File(s) to cut" ) return parser . parse_args ( args )
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
10706	def get_vacations ( ) : arequest = requests . get ( VACATIONS_URL , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
7970	def _add_timeout_handler ( self , handler ) : self . timeout_handlers . append ( handler ) if self . event_thread is None : return self . _run_timeout_threads ( handler )
3903	def _show_menu ( self ) : current_widget = self . _tabbed_window . get_current_widget ( ) if hasattr ( current_widget , 'get_menu_widget' ) : menu_widget = current_widget . get_menu_widget ( self . _hide_menu ) overlay = urwid . Overlay ( menu_widget , self . _tabbed_window , align = 'center' , width = ( 'relative' , 80 ) , valign = 'middle' , height = ( 'relative' , 80 ) ) self . _urwid_loop . widget = overlay
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
1540	def set_config ( self , config ) : if not isinstance ( config , dict ) : raise TypeError ( "Argument to set_config needs to be dict, given: %s" % str ( config ) ) self . _topology_config = config
7210	def stdout ( self ) : if not self . id : raise WorkflowError ( 'Workflow is not running. Cannot get stdout.' ) if self . batch_values : raise NotImplementedError ( "Query Each Workflow Id within the Batch Workflow for stdout." ) wf = self . workflow . get ( self . id ) stdout_list = [ ] for task in wf [ 'tasks' ] : stdout_list . append ( { 'id' : task [ 'id' ] , 'taskType' : task [ 'taskType' ] , 'name' : task [ 'name' ] , 'stdout' : self . workflow . get_stdout ( self . id , task [ 'id' ] ) } ) return stdout_list
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
2112	def parse_requirements ( filename ) : reqs = [ ] version_spec_in_play = None for line in open ( filename , 'r' ) . read ( ) . strip ( ) . split ( '\n' ) : if not line . strip ( ) : continue if not line . startswith ( '#' ) : reqs . append ( line ) continue match = re . search ( r'^# === [Pp]ython (?P<op>[<>=]{1,2}) ' r'(?P<major>[\d])\.(?P<minor>[\d]+) ===[\s]*$' , line ) if match : version_spec_in_play = match . groupdict ( ) for key in ( 'major' , 'minor' ) : version_spec_in_play [ key ] = int ( version_spec_in_play [ key ] ) continue if ' ' not in line [ 1 : ] . strip ( ) and version_spec_in_play : package = line [ 1 : ] . strip ( ) op = version_spec_in_play [ 'op' ] vspec = ( version_spec_in_play [ 'major' ] , version_spec_in_play [ 'minor' ] ) if '=' in op and sys . version_info [ 0 : 2 ] == vspec : reqs . append ( package ) elif '>' in op and sys . version_info [ 0 : 2 ] > vspec : reqs . append ( package ) elif '<' in op and sys . version_info [ 0 : 2 ] < vspec : reqs . append ( package ) return reqs
2806	def convert_elementwise_sub ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting elementwise_sub ...' ) model0 = layers [ inputs [ 0 ] ] model1 = layers [ inputs [ 1 ] ] if names == 'short' : tf_name = 'S' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sub = keras . layers . Subtract ( name = tf_name ) layers [ scope_name ] = sub ( [ model0 , model1 ] )
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
10535	def get_categories ( limit = 20 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) try : res = _pybossa_req ( 'get' , 'category' , params = params ) if type ( res ) . __name__ == 'list' : return [ Category ( category ) for category in res ] else : raise TypeError except : raise
12896	def get_volume_steps ( self ) : if not self . __volume_steps : self . __volume_steps = yield from self . handle_int ( self . API . get ( 'volume_steps' ) ) return self . __volume_steps
1768	def _publish_instruction_as_executed ( self , insn ) : self . _icount += 1 self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn )
8328	def _lastRecursiveChild ( self ) : "Finds the last element beneath this object to be parsed." lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
1935	def get_source_for ( self , asm_offset , runtime = True ) : srcmap = self . get_srcmap ( runtime ) try : beg , size , _ , _ = srcmap [ asm_offset ] except KeyError : return '' output = '' nl = self . source_code [ : beg ] . count ( '\n' ) + 1 snippet = self . source_code [ beg : beg + size ] for l in snippet . split ( '\n' ) : output += ' %s %s\n' % ( nl , l ) nl += 1 return output
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response )
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) return self . _x10_command ( house_code , unit_number , state )
10615	def clone ( self ) : result = copy . copy ( self ) result . _compound_masses = copy . deepcopy ( self . _compound_masses ) return result
8141	def translate ( self , x , y ) : self . x = x self . y = y
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
5663	def get_trip_points ( cur , route_id , offset = 0 , tripid_glob = '' ) : extra_where = '' if tripid_glob : extra_where = "AND trip_id GLOB '%s'" % tripid_glob cur . execute ( 'SELECT seq, lat, lon ' 'FROM (select trip_I from route ' ' LEFT JOIN trips USING (route_I) ' ' WHERE route_id=? %s limit 1 offset ? ) ' 'JOIN stop_times USING (trip_I) ' 'LEFT JOIN stop USING (stop_id) ' 'ORDER BY seq' % extra_where , ( route_id , offset ) ) stop_points = [ dict ( seq = row [ 0 ] , lat = row [ 1 ] , lon = row [ 2 ] ) for row in cur ] return stop_points
13569	def selected_exercise ( func ) : @ wraps ( func ) def inner ( * args , ** kwargs ) : exercise = Exercise . get_selected ( ) return func ( exercise , * args , ** kwargs ) return inner
5652	def create_file ( fname = None , fname_tmp = None , tmpdir = None , save_tmpfile = False , keepext = False ) : if fname == ':memory:' : yield fname return if fname_tmp is None : basename = os . path . basename ( fname ) root , ext = os . path . splitext ( basename ) dir_ = this_dir = os . path . dirname ( fname ) if not keepext : root = root + ext ext = '' if tmpdir : if tmpdir is True : for dir__ in possible_tmpdirs : if os . access ( dir__ , os . F_OK ) : dir_ = dir__ break tmpfile = tempfile . NamedTemporaryFile ( prefix = 'tmp-' + root + '-' , suffix = ext , dir = dir_ , delete = False ) fname_tmp = tmpfile . name try : yield fname_tmp except Exception as e : if save_tmpfile : print ( "Temporary file is '%s'" % fname_tmp ) else : os . unlink ( fname_tmp ) raise try : os . rename ( fname_tmp , fname ) os . chmod ( fname , 0o777 & ~ current_umask ) except OSError as e : tmpfile2 = tempfile . NamedTemporaryFile ( prefix = 'tmp-' + root + '-' , suffix = ext , dir = this_dir , delete = False ) shutil . copy ( fname_tmp , tmpfile2 . name ) os . rename ( tmpfile2 . name , fname ) os . chmod ( fname , 0o666 & ~ current_umask ) os . unlink ( fname_tmp )
11623	def _equivalent ( self , char , prev , next , implicitA ) : result = [ ] if char . isVowel == False : result . append ( char . chr ) if char . isConsonant and ( ( next is not None and next . isConsonant ) or next is None ) : result . append ( DevanagariCharacter . _VIRAMA ) else : if prev is None or prev . isConsonant == False : result . append ( char . chr ) else : if char . _dependentVowel is not None : result . append ( char . _dependentVowel ) return result
6825	def deploy_services ( self , site = None ) : verbose = self . verbose r = self . local_renderer if not r . env . manage_configs : return self . render_paths ( ) supervisor_services = [ ] if r . env . purge_all_confs : r . sudo ( 'rm -Rf /etc/supervisor/conf.d/*' ) self . write_configs ( site = site ) for _site , site_data in self . iter_sites ( site = site , renderer = self . render_paths ) : if verbose : print ( 'deploy_services.site:' , _site ) for cb in self . genv . _supervisor_create_service_callbacks : if self . verbose : print ( 'cb:' , cb ) ret = cb ( site = _site ) if self . verbose : print ( 'ret:' , ret ) if isinstance ( ret , six . string_types ) : supervisor_services . append ( ret ) elif isinstance ( ret , tuple ) : assert len ( ret ) == 2 conf_name , conf_content = ret if self . dryrun : print ( 'supervisor conf filename:' , conf_name ) print ( conf_content ) self . write_to_file ( conf_content ) self . env . services_rendered = '\n' . join ( supervisor_services ) fn = self . render_to_file ( self . env . config_template ) r . put ( local_path = fn , remote_path = self . env . config_path , use_sudo = True ) if not self . is_running ( ) : self . start ( ) r . sudo ( 'supervisorctl update' )
5090	def export_as_csv_action ( description = "Export selected objects as CSV file" , fields = None , header = True ) : def export_as_csv ( modeladmin , request , queryset ) : opts = modeladmin . model . _meta if not fields : field_names = [ field . name for field in opts . fields ] else : field_names = fields response = HttpResponse ( content_type = "text/csv" ) response [ "Content-Disposition" ] = "attachment; filename={filename}.csv" . format ( filename = str ( opts ) . replace ( "." , "_" ) ) writer = unicodecsv . writer ( response , encoding = "utf-8" ) if header : writer . writerow ( field_names ) for obj in queryset : row = [ ] for field_name in field_names : field = getattr ( obj , field_name ) if callable ( field ) : value = field ( ) else : value = field if value is None : row . append ( "[Not Set]" ) elif not value and isinstance ( value , string_types ) : row . append ( "[Empty]" ) else : row . append ( value ) writer . writerow ( row ) return response export_as_csv . short_description = description return export_as_csv
5557	def _strip_zoom ( input_string , strip_string ) : try : return int ( input_string . strip ( strip_string ) ) except Exception as e : raise MapcheteConfigError ( "zoom level could not be determined: %s" % e )
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
1329	def has_gradient ( self ) : try : self . __model . gradient self . __model . predictions_and_gradient except AttributeError : return False else : return True
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
10601	def _httplib2_init ( username , password ) : obj = httplib2 . Http ( ) if username and password : obj . add_credentials ( username , password ) return obj
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
195	def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : k_param = iap . handle_discrete_param ( k , "k" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , "angle" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , "direction" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : from . import geometric as iaa_geometric k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
13116	def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( ** host_config )
3028	def _get_application_default_credential_from_file ( filename ) : with open ( filename ) as file_obj : client_credentials = json . load ( file_obj ) credentials_type = client_credentials . get ( 'type' ) if credentials_type == AUTHORIZED_USER : required_fields = set ( [ 'client_id' , 'client_secret' , 'refresh_token' ] ) elif credentials_type == SERVICE_ACCOUNT : required_fields = set ( [ 'client_id' , 'client_email' , 'private_key_id' , 'private_key' ] ) else : raise ApplicationDefaultCredentialsError ( "'type' field should be defined (and have one of the '" + AUTHORIZED_USER + "' or '" + SERVICE_ACCOUNT + "' values)" ) missing_fields = required_fields . difference ( client_credentials . keys ( ) ) if missing_fields : _raise_exception_for_missing_fields ( missing_fields ) if client_credentials [ 'type' ] == AUTHORIZED_USER : return GoogleCredentials ( access_token = None , client_id = client_credentials [ 'client_id' ] , client_secret = client_credentials [ 'client_secret' ] , refresh_token = client_credentials [ 'refresh_token' ] , token_expiry = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , user_agent = 'Python client library' ) else : from oauth2client import service_account return service_account . _JWTAccessCredentials . from_json_keyfile_dict ( client_credentials )
9122	def make_obo_getter ( data_url : str , data_path : str , * , preparsed_path : Optional [ str ] = None , ) -> Callable [ [ Optional [ str ] , bool , bool ] , MultiDiGraph ] : download_function = make_downloader ( data_url , data_path ) def get_obo ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> MultiDiGraph : if preparsed_path is not None and os . path . exists ( preparsed_path ) : return read_gpickle ( preparsed_path ) if url is None and cache : url = download_function ( force_download = force_download ) result = obonet . read_obo ( url ) if preparsed_path is not None : write_gpickle ( result , preparsed_path ) return result return get_obo
9054	def posteriori_covariance ( self ) : r K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
9184	def _node_to_model ( tree_or_item , metadata = None , parent = None , lucent_id = cnxepub . TRANSLUCENT_BINDER_ID ) : if 'contents' in tree_or_item : tree = tree_or_item binder = cnxepub . TranslucentBinder ( metadata = tree ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder , lucent_id = lucent_id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) result = binder else : item = tree_or_item result = cnxepub . DocumentPointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
11288	def get_request_subfields ( root ) : request = root . find ( 'request' ) responsedate = root . find ( 'responseDate' ) subs = [ ( "9" , request . text ) , ( "h" , responsedate . text ) , ( "m" , request . attrib [ "metadataPrefix" ] ) ] return subs
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
12274	def iso_reference_str2int ( n ) : n = n . upper ( ) numbers = [ ] for c in n : iso_reference_valid_char ( c ) if c in ISO_REFERENCE_VALID_NUMERIC : numbers . append ( c ) else : numbers . append ( str ( iso_reference_char2int ( c ) ) ) return int ( '' . join ( numbers ) )
350	def load_flickr25k_dataset ( tag = 'sky' , path = "data" , n_threads = 50 , printable = False ) : path = os . path . join ( path , 'flickr25k' ) filename = 'mirflickr25k.zip' url = 'http://press.liacs.nl/mirflickr/mirflickr25k/' if folder_exists ( os . path . join ( path , "mirflickr" ) ) is False : logging . info ( "[*] Flickr25k is nonexistent in {}" . format ( path ) ) maybe_download_and_extract ( filename , path , url , extract = True ) del_file ( os . path . join ( path , filename ) ) folder_imgs = os . path . join ( path , "mirflickr" ) path_imgs = load_file_list ( path = folder_imgs , regx = '\\.jpg' , printable = False ) path_imgs . sort ( key = natural_keys ) folder_tags = os . path . join ( path , "mirflickr" , "meta" , "tags" ) path_tags = load_file_list ( path = folder_tags , regx = '\\.txt' , printable = False ) path_tags . sort ( key = natural_keys ) if tag is None : logging . info ( "[Flickr25k] reading all images" ) else : logging . info ( "[Flickr25k] reading images with tag: {}" . format ( tag ) ) images_list = [ ] for idx , _v in enumerate ( path_tags ) : tags = read_file ( os . path . join ( folder_tags , path_tags [ idx ] ) ) . split ( '\n' ) if tag is None or tag in tags : images_list . append ( path_imgs [ idx ] ) images = visualize . read_images ( images_list , folder_imgs , n_threads = n_threads , printable = printable ) return images
575	def loadJsonValueFromFile ( inputFilePath ) : with open ( inputFilePath ) as fileObj : value = json . load ( fileObj ) return value
1486	def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n. ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely. Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
2818	def convert_padding ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting padding...' ) if params [ 'mode' ] == 'constant' : if params [ 'value' ] != 0.0 : raise AssertionError ( 'Cannot convert non-zero padding' ) if names : tf_name = 'PADD' + random_string ( 4 ) else : tf_name = w_name + str ( random . random ( ) ) padding_name = tf_name padding_layer = keras . layers . ZeroPadding2D ( padding = ( ( params [ 'pads' ] [ 2 ] , params [ 'pads' ] [ 6 ] ) , ( params [ 'pads' ] [ 3 ] , params [ 'pads' ] [ 7 ] ) ) , name = padding_name ) layers [ scope_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) elif params [ 'mode' ] == 'reflect' : def target_layer ( x , pads = params [ 'pads' ] ) : layer = tf . pad ( x , [ [ 0 , 0 ] , [ 0 , 0 ] , [ pads [ 2 ] , pads [ 6 ] ] , [ pads [ 3 ] , pads [ 7 ] ] ] , 'REFLECT' ) return layer lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
7417	def update ( assembly , idict , count ) : data = iter ( open ( os . path . join ( assembly . dirs . outfiles , assembly . name + ".phy" ) , 'r' ) ) ntax , nchar = data . next ( ) . strip ( ) . split ( ) for line in data : tax , seq = line . strip ( ) . split ( ) idict [ tax ] = idict [ tax ] [ 100000 : ] idict [ tax ] += seq [ count : count + 100000 ] del line return idict
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) for reaction in pruned : reaction . _model = self for metabolite in list ( reaction . metabolites ) : if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) else : stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) self . _populate_solver ( pruned )
11673	def make_stacked ( self ) : "If unstacked, convert to stacked. If stacked, do nothing." if self . stacked : return self . _boundaries = bounds = np . r_ [ 0 , np . cumsum ( self . n_pts ) ] self . stacked_features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
1485	def run ( self , name , config , builder ) : if not isinstance ( name , str ) : raise RuntimeError ( "Name has to be a string type" ) if not isinstance ( config , Config ) : raise RuntimeError ( "config has to be a Config type" ) if not isinstance ( builder , Builder ) : raise RuntimeError ( "builder has to be a Builder type" ) bldr = TopologyBuilder ( name = name ) builder . build ( bldr ) bldr . set_config ( config . _api_config ) bldr . build_and_submit ( )
12166	def remove_listener ( self , event , listener ) : with contextlib . suppress ( ValueError ) : self . _listeners [ event ] . remove ( listener ) return True with contextlib . suppress ( ValueError ) : self . _once [ event ] . remove ( listener ) return True return False
9097	def drop_bel_namespace ( self ) -> Optional [ Namespace ] : namespace = self . _get_default_namespace ( ) if namespace is not None : for entry in tqdm ( namespace . entries , desc = f'deleting entries in {self._get_namespace_name()}' ) : self . session . delete ( entry ) self . session . delete ( namespace ) log . info ( 'committing deletions' ) self . session . commit ( ) return namespace
2205	def userhome ( username = None ) : if username is None : if 'HOME' in os . environ : userhome_dpath = os . environ [ 'HOME' ] else : if sys . platform . startswith ( 'win32' ) : if 'USERPROFILE' in os . environ : userhome_dpath = os . environ [ 'USERPROFILE' ] elif 'HOMEPATH' in os . environ : drive = os . environ . get ( 'HOMEDRIVE' , '' ) userhome_dpath = join ( drive , os . environ [ 'HOMEPATH' ] ) else : raise OSError ( "Cannot determine the user's home directory" ) else : import pwd userhome_dpath = pwd . getpwuid ( os . getuid ( ) ) . pw_dir else : if sys . platform . startswith ( 'win32' ) : c_users = dirname ( userhome ( ) ) userhome_dpath = join ( c_users , username ) if not exists ( userhome_dpath ) : raise KeyError ( 'Unknown user: {}' . format ( username ) ) else : import pwd try : pwent = pwd . getpwnam ( username ) except KeyError : raise KeyError ( 'Unknown user: {}' . format ( username ) ) userhome_dpath = pwent . pw_dir return userhome_dpath
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
4391	def adsSyncReadDeviceInfoReqEx ( port , address ) : sync_read_device_info_request = _adsDLL . AdsSyncReadDeviceInfoReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) device_name_buffer = ctypes . create_string_buffer ( 20 ) device_name_pointer = ctypes . pointer ( device_name_buffer ) ads_version = SAdsVersion ( ) ads_version_pointer = ctypes . pointer ( ads_version ) error_code = sync_read_device_info_request ( port , ams_address_pointer , device_name_pointer , ads_version_pointer ) if error_code : raise ADSError ( error_code ) return ( device_name_buffer . value . decode ( ) , AdsVersion ( ads_version ) )
6265	def translate_buffer_format ( vertex_format ) : buffer_format = [ ] attributes = [ ] mesh_attributes = [ ] if "T2F" in vertex_format : buffer_format . append ( "2f" ) attributes . append ( "in_uv" ) mesh_attributes . append ( ( "TEXCOORD_0" , "in_uv" , 2 ) ) if "C3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_color" ) mesh_attributes . append ( ( "NORMAL" , "in_color" , 3 ) ) if "N3F" in vertex_format : buffer_format . append ( "3f" ) attributes . append ( "in_normal" ) mesh_attributes . append ( ( "NORMAL" , "in_normal" , 3 ) ) buffer_format . append ( "3f" ) attributes . append ( "in_position" ) mesh_attributes . append ( ( "POSITION" , "in_position" , 3 ) ) return " " . join ( buffer_format ) , attributes , mesh_attributes
5312	def resolve_modifier_to_ansi_code ( modifiername , colormode ) : if colormode == terminal . NO_COLORS : return '' , '' try : start_code , end_code = ansi . MODIFIERS [ modifiername ] except KeyError : raise ColorfulError ( 'the modifier "{0}" is unknown. Use one of: {1}' . format ( modifiername , ansi . MODIFIERS . keys ( ) ) ) else : return ansi . ANSI_ESCAPE_CODE . format ( code = start_code ) , ansi . ANSI_ESCAPE_CODE . format ( code = end_code )
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
7967	def feed ( self , data ) : with self . lock : if self . in_use : raise StreamParseError ( "StreamReader.feed() is not reentrant!" ) self . in_use = True try : if not self . _started : if len ( data ) > 1 : self . parser . feed ( data [ : 1 ] ) data = data [ 1 : ] self . _started = True if data : self . parser . feed ( data ) else : self . parser . close ( ) except ElementTree . ParseError , err : self . handler . stream_parse_error ( unicode ( err ) ) finally : self . in_use = False
13372	def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
11545	def set_pwm_frequency ( self , frequency , pin = None ) : if pin is None : self . _set_pwm_frequency ( frequency , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_pwm_frequency ( frequency , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
6596	def receive_one ( self ) : if self . nruns == 0 : return None ret = self . communicationChannel . receive_one ( ) if ret is not None : self . nruns -= 1 return ret
8244	def shader ( x , y , dx , dy , radius = 300 , angle = 0 , spread = 90 ) : if angle != None : radius *= 2 d = sqrt ( ( dx - x ) ** 2 + ( dy - y ) ** 2 ) a = degrees ( atan2 ( dy - y , dx - x ) ) + 180 if d <= radius : d1 = 1.0 * d / radius else : d1 = 1.0 if angle is None : return 1 - d1 angle = 360 - angle % 360 spread = max ( 0 , min ( spread , 360 ) ) if spread == 0 : return 0.0 d = abs ( a - angle ) if d <= spread / 2 : d2 = d / spread + d1 else : d2 = 1.0 if 360 - angle <= spread / 2 : d = abs ( 360 - angle + a ) if d <= spread / 2 : d2 = d / spread + d1 if angle < spread / 2 : d = abs ( 360 + angle - a ) if d <= spread / 2 : d2 = d / spread + d1 return 1 - max ( 0 , min ( d2 , 1 ) )
5120	def next_event_description ( self ) : if self . _fancy_heap . size == 0 : event_type = 'Nothing' edge_index = None else : s = [ q . _key ( ) for q in self . edge2queue ] s . sort ( ) e = s [ 0 ] [ 1 ] q = self . edge2queue [ e ] event_type = 'Arrival' if q . next_event_description ( ) == 1 else 'Departure' edge_index = q . edge [ 2 ] return event_type , edge_index
10420	def count_unique_relations ( graph : BELGraph ) -> Counter : return Counter ( itt . chain . from_iterable ( get_edge_relations ( graph ) . values ( ) ) )
907	def replaceIterationCycle ( self , phaseSpecs ) : self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return
11893	def retrieve_document ( file_path , directory = 'sec_filings' ) : ftp = FTP ( 'ftp.sec.gov' , timeout = None ) ftp . login ( ) name = file_path . replace ( '/' , '_' ) if not os . path . exists ( directory ) : os . makedirs ( directory ) with tempfile . TemporaryFile ( ) as temp : ftp . retrbinary ( 'RETR %s' % file_path , temp . write ) temp . seek ( 0 ) with open ( '{}/{}' . format ( directory , name ) , 'w+' ) as f : f . write ( temp . read ( ) . decode ( "utf-8" ) ) f . closed records = temp retry = False ftp . close ( )
7371	def main ( args_list = None ) : args = parse_args ( args_list ) binding_predictions = run_predictor ( args ) df = binding_predictions . to_dataframe ( ) logger . info ( '\n%s' , df ) if args . output_csv : df . to_csv ( args . output_csv , index = False ) print ( "Wrote: %s" % args . output_csv )
6228	def init ( window = None , project = None , timeline = None ) : from demosys . effects . registry import Effect from demosys . scene import camera window . timeline = timeline setattr ( Effect , '_window' , window ) setattr ( Effect , '_ctx' , window . ctx ) setattr ( Effect , '_project' , project ) window . sys_camera = camera . SystemCamera ( aspect = window . aspect_ratio , fov = 60.0 , near = 1 , far = 1000 ) setattr ( Effect , '_sys_camera' , window . sys_camera ) print ( "Loading started at" , time . time ( ) ) project . load ( ) timer_cls = import_string ( settings . TIMER ) window . timer = timer_cls ( ) window . timer . start ( )
11479	def _create_bitstream ( file_path , local_file , item_id , log_ind = None ) : checksum = _streaming_file_md5 ( file_path ) upload_token = session . communicator . generate_upload_token ( session . token , item_id , local_file , checksum ) if upload_token != '' : log_trace = 'Uploading bitstream from {0}' . format ( file_path ) session . communicator . perform_upload ( upload_token , local_file , filepath = file_path , itemid = item_id ) else : log_trace = 'Adding a bitstream link in this item to an existing ' 'bitstream from {0}' . format ( file_path ) if log_ind is not None : log_trace += log_ind print ( log_trace )
11679	def connect ( self ) : try : logger . info ( u'Connecting %s:%d' % ( self . host , self . port ) ) self . sock . connect ( ( self . host , self . port ) ) except socket . error : raise ConnectionError ( ) self . state = CONNECTED
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
13671	def strip_codes ( s : Any ) -> str : return codepat . sub ( '' , str ( s ) if ( s or ( s == 0 ) ) else '' )
1167	def _dump_registry ( cls , file = None ) : print >> file , "Class: %s.%s" % ( cls . __module__ , cls . __name__ ) print >> file , "Inv.counter: %s" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( "_abc_" ) : value = getattr ( cls , name ) print >> file , "%s: %r" % ( name , value )
3108	def locked_get ( self ) : query = { self . key_name : self . key_value } entities = self . model_class . objects . filter ( ** query ) if len ( entities ) > 0 : credential = getattr ( entities [ 0 ] , self . property_name ) if getattr ( credential , 'set_store' , None ) is not None : credential . set_store ( self ) return credential else : return None
13057	def get_locale ( self ) : best_match = request . accept_languages . best_match ( [ 'de' , 'fr' , 'en' , 'la' ] ) if best_match is None : if len ( request . accept_languages ) > 0 : best_match = request . accept_languages [ 0 ] [ 0 ] [ : 2 ] else : return self . __default_lang__ lang = self . __default_lang__ if best_match == "de" : lang = "ger" elif best_match == "fr" : lang = "fre" elif best_match == "en" : lang = "eng" elif best_match == "la" : lang = "lat" return lang
2155	def set_or_reset_runtime_param ( self , key , value ) : if self . _runtime . has_option ( 'general' , key ) : self . _runtime = self . _new_parser ( ) if value is None : return settings . _runtime . set ( 'general' , key . replace ( 'tower_' , '' ) , six . text_type ( value ) )
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
6535	def merge_dict ( dict1 , dict2 , merge_lists = False ) : merged = dict ( dict1 ) for key , value in iteritems ( dict2 ) : if isinstance ( merged . get ( key ) , dict ) : merged [ key ] = merge_dict ( merged [ key ] , value ) elif merge_lists and isinstance ( merged . get ( key ) , list ) : merged [ key ] = merge_list ( merged [ key ] , value ) else : merged [ key ] = value return merged
5545	def pyramid ( input_raster , output_dir , pyramid_type = None , output_format = None , resampling_method = None , scale_method = None , zoom = None , bounds = None , overwrite = False , debug = False ) : bounds = bounds if bounds else None options = dict ( pyramid_type = pyramid_type , scale_method = scale_method , output_format = output_format , resampling = resampling_method , zoom = zoom , bounds = bounds , overwrite = overwrite ) raster2pyramid ( input_raster , output_dir , options )
1911	def GetNBits ( value , nbits ) : if isinstance ( value , int ) : return Operators . EXTRACT ( value , 0 , nbits ) elif isinstance ( value , BitVec ) : if value . size < nbits : return Operators . ZEXTEND ( value , nbits ) else : return Operators . EXTRACT ( value , 0 , nbits )
8809	def delete_mac_address_range ( context , id ) : LOG . info ( "delete_mac_address_range %s for tenant %s" % ( id , context . tenant_id ) ) if not context . is_admin : raise n_exc . NotAuthorized ( ) with context . session . begin ( ) : mar = db_api . mac_address_range_find ( context , id = id , scope = db_api . ONE ) if not mar : raise q_exc . MacAddressRangeNotFound ( mac_address_range_id = id ) _delete_mac_address_range ( context , mar )
3144	def update ( self , file_id , data ) : self . file_id = file_id if 'name' not in data : raise KeyError ( 'The file must have a name' ) if 'file_data' not in data : raise KeyError ( 'The file must have file_data' ) return self . _mc_client . _patch ( url = self . _build_path ( file_id ) , data = data )
9887	def _read_all_attribute_info ( self ) : num = copy . deepcopy ( self . _num_attrs ) fname = copy . deepcopy ( self . fname ) out = fortran_cdf . inquire_all_attr ( fname , num , len ( fname ) ) status = out [ 0 ] names = out [ 1 ] . astype ( 'U' ) scopes = out [ 2 ] max_gentries = out [ 3 ] max_rentries = out [ 4 ] max_zentries = out [ 5 ] attr_nums = out [ 6 ] global_attrs_info = { } var_attrs_info = { } if status == 0 : for name , scope , gentry , rentry , zentry , num in zip ( names , scopes , max_gentries , max_rentries , max_zentries , attr_nums ) : name = '' . join ( name ) name = name . rstrip ( ) nug = { } nug [ 'scope' ] = scope nug [ 'max_gentry' ] = gentry nug [ 'max_rentry' ] = rentry nug [ 'max_zentry' ] = zentry nug [ 'attr_num' ] = num flag = ( gentry == 0 ) & ( rentry == 0 ) & ( zentry == 0 ) if not flag : if scope == 1 : global_attrs_info [ name ] = nug elif scope == 2 : var_attrs_info [ name ] = nug self . global_attrs_info = global_attrs_info self . var_attrs_info = var_attrs_info else : raise IOError ( fortran_cdf . statusreporter ( status ) )
9865	def currency ( self ) : try : current_subscription = self . info [ "viewer" ] [ "home" ] [ "currentSubscription" ] return current_subscription [ "priceInfo" ] [ "current" ] [ "currency" ] except ( KeyError , TypeError , IndexError ) : _LOGGER . error ( "Could not find currency." ) return ""
6294	def transform ( self , program : moderngl . Program , buffer : moderngl . Buffer , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . transform ( buffer , mode = mode , vertices = vertices , first = first , instances = instances )
9394	def plot_cdf ( self , graphing_library = 'matplotlib' ) : graphed = False for percentile_csv in self . percentiles_files : csv_filename = os . path . basename ( percentile_csv ) column = self . csv_column_map [ percentile_csv . replace ( ".percentiles." , "." ) ] if not self . check_important_sub_metrics ( column ) : continue column = naarad . utils . sanitize_string ( column ) graph_title = '.' . join ( csv_filename . split ( '.' ) [ 0 : - 1 ] ) if self . sub_metric_description and column in self . sub_metric_description . keys ( ) : graph_title += ' (' + self . sub_metric_description [ column ] + ')' if self . sub_metric_unit and column in self . sub_metric_unit . keys ( ) : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column + ' (' + self . sub_metric_unit [ column ] + ')' , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] else : plot_data = [ PD ( input_csv = percentile_csv , csv_column = 1 , series_name = graph_title , x_label = 'Percentiles' , y_label = column , precision = None , graph_height = 600 , graph_width = 1200 , graph_type = 'line' ) ] graphed , div_file = Metric . graphing_modules [ graphing_library ] . graph_data_on_the_same_graph ( plot_data , self . resource_directory , self . resource_path , graph_title ) if graphed : self . plot_files . append ( div_file ) return True
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
2383	def construct_mapping ( self , node , deep = False ) : mapping = super ( ExtendedSafeConstructor , self ) . construct_mapping ( node , deep ) return { ( str ( key ) if isinstance ( key , int ) else key ) : mapping [ key ] for key in mapping }
13400	def addLogbook ( self , physDef = "LCLS" , mccDef = "MCC" , initialInstance = False ) : if self . logMenuCount < 5 : self . logMenus . append ( LogSelectMenu ( self . logui . multiLogLayout , initialInstance ) ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 1 ] , self . physics_programs , physDef ) self . logMenus [ - 1 ] . addLogbooks ( self . logTypeList [ 0 ] , self . mcc_programs , mccDef ) self . logMenus [ - 1 ] . show ( ) self . logMenuCount += 1 if initialInstance : QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , self . addLogbook ) else : from functools import partial QObject . connect ( self . logMenus [ - 1 ] . logButton , SIGNAL ( "clicked()" ) , partial ( self . removeLogbook , self . logMenus [ - 1 ] ) )
8791	def has_tag ( self , model ) : for tag in model . tags : if self . is_tag ( tag ) : return True return False
1752	def _reg_name ( self , reg_id ) : if reg_id >= X86_REG_ENDING : logger . warning ( "Trying to get register name for a non-register" ) return None cs_reg_name = self . cpu . instruction . reg_name ( reg_id ) if cs_reg_name is None or cs_reg_name . lower ( ) == '(invalid)' : return None return self . cpu . _regfile . _alias ( cs_reg_name . upper ( ) )
10016	def swap_environment_cnames ( self , from_env_name , to_env_name ) : self . ebs . swap_environment_cnames ( source_environment_name = from_env_name , destination_environment_name = to_env_name )
1971	def check_timers ( self ) : if self . _current is None : advance = min ( [ self . clocks ] + [ x for x in self . timers if x is not None ] ) + 1 logger . debug ( f"Advancing the clock from {self.clocks} to {advance}" ) self . clocks = advance for procid in range ( len ( self . timers ) ) : if self . timers [ procid ] is not None : if self . clocks > self . timers [ procid ] : self . procs [ procid ] . PC += self . procs [ procid ] . instruction . size self . awake ( procid )
12734	def set_body_states ( self , states ) : for state in states : self . get_body ( state . name ) . state = state
5574	def available_output_formats ( ) : output_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "w" , "rw" ] ) : output_formats . append ( driver_ . METADATA [ "driver_name" ] ) return output_formats
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
7540	def basecaller ( arrayed , mindepth_majrule , mindepth_statistical , estH , estE ) : cons = np . zeros ( arrayed . shape [ 1 ] , dtype = np . uint8 ) cons . fill ( 78 ) arr = arrayed . view ( np . uint8 ) for col in xrange ( arr . shape [ 1 ] ) : carr = arr [ : , col ] mask = carr == 45 mask += carr == 78 marr = carr [ ~ mask ] if not marr . shape [ 0 ] : cons [ col ] = 78 elif np . all ( marr == marr [ 0 ] ) : cons [ col ] = marr [ 0 ] else : counts = np . bincount ( marr ) pbase = np . argmax ( counts ) nump = counts [ pbase ] counts [ pbase ] = 0 qbase = np . argmax ( counts ) numq = counts [ qbase ] counts [ qbase ] = 0 rbase = np . argmax ( counts ) numr = counts [ rbase ] bidepth = nump + numq if bidepth < mindepth_majrule : cons [ col ] = 78 else : if bidepth > 500 : base1 = int ( 500 * ( nump / float ( bidepth ) ) ) base2 = int ( 500 * ( numq / float ( bidepth ) ) ) else : base1 = nump base2 = numq if bidepth >= mindepth_statistical : ishet , prob = get_binom ( base1 , base2 , estE , estH ) if prob < 0.95 : cons [ col ] = 78 else : if ishet : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase else : if nump == numq : cons [ col ] = TRANS [ ( pbase , qbase ) ] else : cons [ col ] = pbase return cons . view ( "S1" )
1665	def CheckRedundantVirtual ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] virtual = Match ( r'^(.*)(\bvirtual\b)(.*)$' , line ) if not virtual : return if ( Search ( r'\b(public|protected|private)\s+$' , virtual . group ( 1 ) ) or Match ( r'^\s+(public|protected|private)\b' , virtual . group ( 3 ) ) ) : return if Match ( r'^.*[^:]:[^:].*$' , line ) : return end_col = - 1 end_line = - 1 start_col = len ( virtual . group ( 2 ) ) for start_line in xrange ( linenum , min ( linenum + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ start_line ] [ start_col : ] parameter_list = Match ( r'^([^(]*)\(' , line ) if parameter_list : ( _ , end_line , end_col ) = CloseExpression ( clean_lines , start_line , start_col + len ( parameter_list . group ( 1 ) ) ) break start_col = 0 if end_col < 0 : return for i in xrange ( end_line , min ( end_line + 3 , clean_lines . NumLines ( ) ) ) : line = clean_lines . elided [ i ] [ end_col : ] match = Search ( r'\b(override|final)\b' , line ) if match : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"virtual" is redundant since function is ' 'already declared as "%s"' % match . group ( 1 ) ) ) end_col = 0 if Search ( r'[^\w]\s*$' , line ) : break
1354	def get_argument_cluster ( self ) : try : return self . get_argument ( constants . PARAM_CLUSTER ) except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
6007	def load_image ( image_path , image_hdu , pixel_scale ) : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = image_path , hdu = image_hdu , pixel_scale = pixel_scale )
9352	def street_number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )
9332	def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
1187	def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction
2951	def _on_trigger ( self , my_task ) : for task in my_task . workflow . task_tree . _find_any ( self ) : if task . thread_id != my_task . thread_id : continue self . _do_join ( task )
12805	def search ( self , terms ) : messages = self . _connection . get ( "search/%s" % urllib . quote_plus ( terms ) , key = "messages" ) if messages : messages = [ Message ( self , message ) for message in messages ] return messages
4495	def remove ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To remove a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . target ) store = project . storage ( storage ) for f in store . files : if norm_remote_path ( f . path ) == remote_path : f . remove ( )
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
10781	def feature_guess ( st , rad , invert = 'guess' , minmass = None , use_tp = False , trim_edge = False , ** kwargs ) : if invert == 'guess' : invert = guess_invert ( st ) if invert : im = 1 - st . residuals else : im = st . residuals return _feature_guess ( im , rad , minmass = minmass , use_tp = use_tp , trim_edge = trim_edge )
5156	def _get_install_context ( self ) : config = self . config l2vpn = [ ] for vpn in self . config . get ( 'openvpn' , [ ] ) : if vpn . get ( 'dev_type' ) != 'tap' : continue tap = vpn . copy ( ) l2vpn . append ( tap ) bridges = [ ] for interface in self . config . get ( 'interfaces' , [ ] ) : if interface [ 'type' ] != 'bridge' : continue bridge = interface . copy ( ) if bridge . get ( 'addresses' ) : bridge [ 'proto' ] = interface [ 'addresses' ] [ 0 ] . get ( 'proto' ) bridge [ 'ip' ] = interface [ 'addresses' ] [ 0 ] . get ( 'address' ) bridges . append ( bridge ) cron = False for _file in config . get ( 'files' , [ ] ) : path = _file [ 'path' ] if path . startswith ( '/crontabs' ) or path . startswith ( 'crontabs' ) : cron = True break return dict ( hostname = config [ 'general' ] [ 'hostname' ] , l2vpn = l2vpn , bridges = bridges , radios = config . get ( 'radios' , [ ] ) , cron = cron )
13641	def send ( self , use_open_peers = True , queue = True , ** kw ) : if not use_open_peers : ip = kw . get ( 'ip' ) port = kw . get ( 'port' ) peer = 'http://{}:{}' . format ( ip , port ) res = arky . rest . POST . peer . transactions ( peer = peer , transactions = [ self . tx . tx ] ) else : res = arky . core . sendPayload ( self . tx . tx ) if self . tx . success != '0.0%' : self . tx . error = None self . tx . success = True else : self . tx . error = res [ 'messages' ] self . tx . success = False self . tx . tries += 1 self . tx . res = res if queue : self . tx . send = True self . __save ( ) return res
8775	def _discover_via_entrypoints ( self ) : emgr = extension . ExtensionManager ( PLUGIN_EP , invoke_on_load = False ) return ( ( ext . name , ext . plugin ) for ext in emgr )
6952	def aov_theta ( times , mags , errs , frequency , binsize = 0.05 , minbin = 9 ) : period = 1.0 / frequency fold_time = times [ 0 ] phased = phase_magseries ( times , mags , period , fold_time , wrap = False , sort = True ) phases = phased [ 'phase' ] pmags = phased [ 'mags' ] bins = nparange ( 0.0 , 1.0 , binsize ) ndets = phases . size binnedphaseinds = npdigitize ( phases , bins ) bin_s1_tops = [ ] bin_s2_tops = [ ] binndets = [ ] goodbins = 0 all_xbar = npmedian ( pmags ) for x in npunique ( binnedphaseinds ) : thisbin_inds = binnedphaseinds == x thisbin_mags = pmags [ thisbin_inds ] if thisbin_mags . size > minbin : thisbin_ndet = thisbin_mags . size thisbin_xbar = npmedian ( thisbin_mags ) thisbin_s1_top = ( thisbin_ndet * ( thisbin_xbar - all_xbar ) * ( thisbin_xbar - all_xbar ) ) thisbin_s2_top = npsum ( ( thisbin_mags - all_xbar ) * ( thisbin_mags - all_xbar ) ) bin_s1_tops . append ( thisbin_s1_top ) bin_s2_tops . append ( thisbin_s2_top ) binndets . append ( thisbin_ndet ) goodbins = goodbins + 1 bin_s1_tops = nparray ( bin_s1_tops ) bin_s2_tops = nparray ( bin_s2_tops ) binndets = nparray ( binndets ) s1 = npsum ( bin_s1_tops ) / ( goodbins - 1.0 ) s2 = npsum ( bin_s2_tops ) / ( ndets - goodbins ) theta_aov = s1 / s2 return theta_aov
12649	def is_valid_regex ( string ) : try : re . compile ( string ) is_valid = True except re . error : is_valid = False return is_valid
5266	def sentencecase ( string ) : joiner = ' ' string = re . sub ( r"[\-_\.\s]" , joiner , str ( string ) ) if not string : return string return capitalcase ( trimcase ( re . sub ( r"[A-Z]" , lambda matched : joiner + lowercase ( matched . group ( 0 ) ) , string ) ) )
4859	def enterprise_login_required ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : if 'enterprise_uuid' not in kwargs : raise Http404 enterprise_uuid = kwargs [ 'enterprise_uuid' ] enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) if not request . user . is_authenticated : parsed_current_url = urlparse ( request . get_full_path ( ) ) parsed_query_string = parse_qs ( parsed_current_url . query ) parsed_query_string . update ( { 'tpa_hint' : enterprise_customer . identity_provider , FRESH_LOGIN_PARAMETER : 'yes' } ) next_url = '{current_path}?{query_string}' . format ( current_path = quote ( parsed_current_url . path ) , query_string = urlencode ( parsed_query_string , doseq = True ) ) return redirect ( '{login_url}?{params}' . format ( login_url = '/login' , params = urlencode ( { 'next' : next_url } ) ) ) return view ( request , * args , ** kwargs ) return wrapper
7913	def validate_string_list ( value ) : try : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) value = value . decode ( encoding ) return [ x . strip ( ) for x in value . split ( u"," ) ] except ( AttributeError , TypeError , UnicodeError ) : raise ValueError ( "Bad string list" )
3749	def calculate_P ( self , T , P , method ) : r if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
13770	def render_asset ( self , name ) : result = "" if self . has_asset ( name ) : asset = self . get_asset ( name ) if asset . files : for f in asset . files : result += f . render_include ( ) + "\r\n" return result
1687	def _CollapseStrings ( elided ) : if _RE_PATTERN_INCLUDE . match ( elided ) : return elided elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES . sub ( '' , elided ) collapsed = '' while True : match = Match ( r'^([^\'"]*)([\'"])(.*)$' , elided ) if not match : collapsed += elided break head , quote , tail = match . groups ( ) if quote == '"' : second_quote = tail . find ( '"' ) if second_quote >= 0 : collapsed += head + '""' elided = tail [ second_quote + 1 : ] else : collapsed += elided break else : if Search ( r'\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$' , head ) : match_literal = Match ( r'^((?:\'?[0-9a-zA-Z_])*)(.*)$' , "'" + tail ) collapsed += head + match_literal . group ( 1 ) . replace ( "'" , '' ) elided = match_literal . group ( 2 ) else : second_quote = tail . find ( '\'' ) if second_quote >= 0 : collapsed += head + "''" elided = tail [ second_quote + 1 : ] else : collapsed += elided break return collapsed
13661	def _tempfile ( filename ) : return tempfile . NamedTemporaryFile ( mode = 'w' , dir = os . path . dirname ( filename ) , prefix = os . path . basename ( filename ) , suffix = os . fsencode ( '.tmp' ) , delete = False )
197	def Fog ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return CloudLayer ( intensity_mean = ( 220 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.5 ) , intensity_coarse_scale = 2 , alpha_min = ( 0.7 , 0.9 ) , alpha_multiplier = 0.3 , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 4.0 , - 2.0 ) , sparsity = 0.9 , density_multiplier = ( 0.4 , 0.9 ) , name = name , deterministic = deterministic , random_state = random_state )
83	def Pepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
3278	def get_resource_inst ( self , path , environ ) : _logger . info ( "get_resource_inst('%s')" % path ) self . _count_get_resource_inst += 1 root = RootCollection ( environ ) return root . resolve ( "" , path )
3049	def _get_implicit_credentials ( cls ) : environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
838	def infer ( self , inputPattern , computeScores = True , overCategories = True , partitionId = None ) : sparsity = 0.0 if self . minSparsity > 0.0 : sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / len ( inputPattern ) ) if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : winner = None inferenceResult = numpy . zeros ( 1 ) dist = numpy . ones ( 1 ) categoryDist = numpy . ones ( 1 ) else : maxCategoryIdx = max ( self . _categoryList ) inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) dist = self . _getDistances ( inputPattern , partitionId = partitionId ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) if self . exact : exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] if len ( exactMatches ) > 0 : for i in exactMatches [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ i ] ] += 1.0 else : sorted = dist . argsort ( ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 if inferenceResult . any ( ) : winner = inferenceResult . argmax ( ) inferenceResult /= inferenceResult . sum ( ) else : winner = None categoryDist = min_score_per_category ( maxCategoryIdx , self . _categoryList , dist ) categoryDist . clip ( 0 , 1.0 , categoryDist ) if self . verbosity >= 1 : print "%s infer:" % ( g_debugPrefix ) print " active inputs:" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) print " winner category:" , winner print " pct neighbors of each category:" , inferenceResult print " dist of each prototype:" , dist print " dist of each category:" , categoryDist result = ( winner , inferenceResult , dist , categoryDist ) return result
600	def compute ( self , activeColumns , predictedColumns , inputValue = None , timestamp = None ) : anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) if self . _mode == Anomaly . MODE_PURE : score = anomalyScore elif self . _mode == Anomaly . MODE_LIKELIHOOD : if inputValue is None : raise ValueError ( "Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' " "requires 'inputValue' as parameter to compute() method. " ) probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = 1 - probability elif self . _mode == Anomaly . MODE_WEIGHTED : probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = anomalyScore * ( 1 - probability ) if self . _movingAverage is not None : score = self . _movingAverage . next ( score ) if self . _binaryThreshold is not None : if score >= self . _binaryThreshold : score = 1.0 else : score = 0.0 return score
12536	def get_dcm_reader ( store_metadata = True , header_fields = None ) : if not store_metadata : return lambda fpath : fpath if header_fields is None : build_dcm = lambda fpath : DicomFile ( fpath ) else : dicom_header = namedtuple ( 'DicomHeader' , header_fields ) build_dcm = lambda fpath : dicom_header . _make ( DicomFile ( fpath ) . get_attributes ( header_fields ) ) return build_dcm
13414	def removeLayout ( self , layout ) : for cnt in reversed ( range ( layout . count ( ) ) ) : item = layout . takeAt ( cnt ) widget = item . widget ( ) if widget is not None : widget . deleteLater ( ) else : self . removeLayout ( item . layout ( ) )
3612	def do_filter ( qs , keywords , exclude = False ) : and_q = Q ( ) for keyword , value in iteritems ( keywords ) : try : values = value . split ( "," ) if len ( values ) > 0 : or_q = Q ( ) for value in values : or_q |= Q ( ** { keyword : value } ) and_q &= or_q except AttributeError : and_q &= Q ( ** { keyword : value } ) if exclude : qs = qs . exclude ( and_q ) else : qs = qs . filter ( and_q ) return qs
11421	def field_xml_output ( field , tag ) : marcxml = [ ] if field [ 3 ] : marcxml . append ( ' <controlfield tag="%s">%s</controlfield>' % ( tag , MathMLParser . html_to_text ( field [ 3 ] ) ) ) else : marcxml . append ( ' <datafield tag="%s" ind1="%s" ind2="%s">' % ( tag , field [ 1 ] , field [ 2 ] ) ) marcxml += [ _subfield_xml_output ( subfield ) for subfield in field [ 0 ] ] marcxml . append ( ' </datafield>' ) return '\n' . join ( marcxml )
6599	def key_vals_dict_to_tuple_list ( key_vals_dict , fill = float ( 'nan' ) ) : tuple_list = [ ] if not key_vals_dict : return tuple_list vlen = max ( [ len ( vs ) for vs in itertools . chain ( * key_vals_dict . values ( ) ) ] ) for k , vs in key_vals_dict . items ( ) : try : tuple_list . extend ( [ k + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) except TypeError : tuple_list . extend ( [ ( k , ) + tuple ( v ) + ( fill , ) * ( vlen - len ( v ) ) for v in vs ] ) return tuple_list
5721	def _convert_schemas ( mapping , schemas ) : schemas = deepcopy ( schemas ) for schema in schemas : for fk in schema . get ( 'foreignKeys' , [ ] ) : resource = fk [ 'reference' ] [ 'resource' ] if resource != 'self' : if resource not in mapping : message = 'Not resource "%s" for foreign key "%s"' message = message % ( resource , fk ) raise ValueError ( message ) fk [ 'reference' ] [ 'resource' ] = mapping [ resource ] return schemas
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
11543	def set_analog_reference ( self , reference , pin = None ) : if pin is None : self . _set_analog_reference ( reference , None ) else : pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : self . _set_analog_reference ( reference , pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3290	def get_preferred_path ( self ) : if self . path in ( "" , "/" ) : return "/" if self . is_collection and not self . path . endswith ( "/" ) : return self . path + "/" return self . path
7218	def delete ( self , task_name ) : r = self . gbdx_connection . delete ( self . _base_url + '/' + task_name ) raise_for_status ( r ) return r . text
13118	def argument_search ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . search ( ** vars ( arguments ) )
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
8656	def get_threads ( session , query ) : response = make_get_request ( session , 'threads' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ThreadsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2354	def find_element ( self , strategy , locator ) : return self . driver_adapter . find_element ( strategy , locator , root = self . root )
1104	def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None
7147	def address ( addr , label = None ) : addr = str ( addr ) if _ADDR_REGEX . match ( addr ) : netbyte = bytearray ( unhexlify ( base58 . decode ( addr ) ) ) [ 0 ] if netbyte in Address . _valid_netbytes : return Address ( addr , label = label ) elif netbyte in SubAddress . _valid_netbytes : return SubAddress ( addr , label = label ) raise ValueError ( "Invalid address netbyte {nb:x}. Allowed values are: {allowed}" . format ( nb = netbyte , allowed = ", " . join ( map ( lambda b : '%02x' % b , sorted ( Address . _valid_netbytes + SubAddress . _valid_netbytes ) ) ) ) ) elif _IADDR_REGEX . match ( addr ) : return IntegratedAddress ( addr ) raise ValueError ( "Address must be either 95 or 106 characters long base58-encoded string, " "is {addr} ({len} chars length)" . format ( addr = addr , len = len ( addr ) ) )
253	def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips
11377	def _normalize_article_dir_with_dtd ( self , path ) : if exists ( join ( path , 'resolved_main.xml' ) ) : return main_xml_content = open ( join ( path , 'main.xml' ) ) . read ( ) arts = [ 'art501.dtd' , 'art510.dtd' , 'art520.dtd' , 'art540.dtd' ] tmp_extracted = 0 for art in arts : if art in main_xml_content : self . _extract_correct_dtd_package ( art . split ( '.' ) [ 0 ] , path ) tmp_extracted = 1 if not tmp_extracted : message = "It looks like the path " + path message += "does not contain an art501, art510, art520 or art540 in main.xml file" self . logger . error ( message ) raise ValueError ( message ) command = [ "xmllint" , "--format" , "--loaddtd" , join ( path , 'main.xml' ) , "--output" , join ( path , 'resolved_main.xml' ) ] dummy , dummy , cmd_err = run_shell_command ( command ) if cmd_err : message = "Error in cleaning %s: %s" % ( join ( path , 'main.xml' ) , cmd_err ) self . logger . error ( message ) raise ValueError ( message )
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : raise NotImplementedError ( 'Format not implemented: %s' % mode )
7382	def has_edge_within_group ( self , group ) : assert group in self . nodes . keys ( ) , "{0} not one of the group of nodes" . format ( group ) nodelist = self . nodes [ group ] for n1 , n2 in self . simplified_edges ( ) : if n1 in nodelist and n2 in nodelist : return True
12581	def setup_logging ( log_config_file = op . join ( op . dirname ( __file__ ) , 'logger.yml' ) , log_default_level = LOG_LEVEL , env_key = MODULE_NAME . upper ( ) + '_LOG_CFG' ) : path = log_config_file value = os . getenv ( env_key , None ) if value : path = value if op . exists ( path ) : log_cfg = yaml . load ( read ( path ) . format ( MODULE_NAME ) ) logging . config . dictConfig ( log_cfg ) else : logging . basicConfig ( level = log_default_level ) log = logging . getLogger ( __name__ ) log . debug ( 'Start logging.' )
6886	def mdwarf_subtype_from_sdsscolor ( ri_color , iz_color ) : if np . isfinite ( ri_color ) and np . isfinite ( iz_color ) : obj_sti = 0.875274 * ri_color + 0.483628 * ( iz_color + 0.00438 ) obj_sts = - 0.483628 * ri_color + 0.875274 * ( iz_color + 0.00438 ) else : obj_sti = np . nan obj_sts = np . nan if ( np . isfinite ( obj_sti ) and np . isfinite ( obj_sts ) and ( obj_sti > 0.666 ) and ( obj_sti < 3.4559 ) ) : if ( ( obj_sti > 0.6660 ) and ( obj_sti < 0.8592 ) ) : m_class = 'M0' if ( ( obj_sti > 0.8592 ) and ( obj_sti < 1.0822 ) ) : m_class = 'M1' if ( ( obj_sti > 1.0822 ) and ( obj_sti < 1.2998 ) ) : m_class = 'M2' if ( ( obj_sti > 1.2998 ) and ( obj_sti < 1.6378 ) ) : m_class = 'M3' if ( ( obj_sti > 1.6378 ) and ( obj_sti < 2.0363 ) ) : m_class = 'M4' if ( ( obj_sti > 2.0363 ) and ( obj_sti < 2.2411 ) ) : m_class = 'M5' if ( ( obj_sti > 2.2411 ) and ( obj_sti < 2.4126 ) ) : m_class = 'M6' if ( ( obj_sti > 2.4126 ) and ( obj_sti < 2.9213 ) ) : m_class = 'M7' if ( ( obj_sti > 2.9213 ) and ( obj_sti < 3.2418 ) ) : m_class = 'M8' if ( ( obj_sti > 3.2418 ) and ( obj_sti < 3.4559 ) ) : m_class = 'M9' else : m_class = None return m_class , obj_sti , obj_sts
12199	def description ( self ) : if self . _description is None : text = '\n' . join ( self . __doc__ . splitlines ( ) [ 1 : ] ) . strip ( ) lines = [ ] for line in map ( str . strip , text . splitlines ( ) ) : if line and lines : lines [ - 1 ] = ' ' . join ( ( lines [ - 1 ] , line ) ) elif line : lines . append ( line ) else : lines . append ( '' ) self . _description = '\n' . join ( lines ) return self . _description
5525	def grab ( self , bbox = None ) : w = Gdk . get_default_root_window ( ) if bbox is not None : g = [ bbox [ 0 ] , bbox [ 1 ] , bbox [ 2 ] - bbox [ 0 ] , bbox [ 3 ] - bbox [ 1 ] ] else : g = w . get_geometry ( ) pb = Gdk . pixbuf_get_from_window ( w , * g ) if pb . get_bits_per_sample ( ) != 8 : raise ValueError ( 'Expected 8 bits per pixel.' ) elif pb . get_n_channels ( ) != 3 : raise ValueError ( 'Expected RGB image.' ) pixel_bytes = pb . read_pixel_bytes ( ) . get_data ( ) width , height = g [ 2 ] , g [ 3 ] return Image . frombytes ( 'RGB' , ( width , height ) , pixel_bytes , 'raw' , 'RGB' , pb . get_rowstride ( ) , 1 )
12142	def _info ( self , source , key , filetype , ignore ) : specs , mdata = [ ] , { } mdata_clashes = set ( ) for spec in source . specs : if key not in spec : raise Exception ( "Key %r not available in 'source'." % key ) mdata = dict ( ( k , v ) for ( k , v ) in filetype . metadata ( spec [ key ] ) . items ( ) if k not in ignore ) mdata_spec = { } mdata_spec . update ( spec ) mdata_spec . update ( mdata ) specs . append ( mdata_spec ) mdata_clashes = mdata_clashes | ( set ( spec . keys ( ) ) & set ( mdata . keys ( ) ) ) if mdata_clashes : self . warning ( "Loaded metadata keys overriding source keys." ) return specs
2038	def SWAP ( self , * operands ) : a = operands [ 0 ] b = operands [ - 1 ] return ( b , ) + operands [ 1 : - 1 ] + ( a , )
12162	async def _try_catch_coro ( emitter , event , listener , coro ) : try : await coro except Exception as exc : if event == emitter . LISTENER_ERROR_EVENT : raise emitter . emit ( emitter . LISTENER_ERROR_EVENT , event , listener , exc )
2401	def gen_length_feats ( self , e_set ) : text = e_set . _text lengths = [ len ( e ) for e in text ] word_counts = [ max ( len ( t ) , 1 ) for t in e_set . _tokens ] comma_count = [ e . count ( "," ) for e in text ] ap_count = [ e . count ( "'" ) for e in text ] punc_count = [ e . count ( "." ) + e . count ( "?" ) + e . count ( "!" ) for e in text ] chars_per_word = [ lengths [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) good_pos_tag_prop = [ good_pos_tags [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] length_arr = numpy . array ( ( lengths , word_counts , comma_count , ap_count , punc_count , chars_per_word , good_pos_tags , good_pos_tag_prop ) ) . transpose ( ) return length_arr . copy ( )
3357	def _extend_nocheck ( self , iterable ) : current_length = len ( self ) list . extend ( self , iterable ) _dict = self . _dict if current_length is 0 : self . _generate_index ( ) return for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : _dict [ obj . id ] = i
5863	def add_organization_course ( organization_data , course_key ) : _validate_course_key ( course_key ) _validate_organization_data ( organization_data ) data . create_organization_course ( organization = organization_data , course_key = course_key )
1182	def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
2010	def _get_memfee ( self , address , size = 1 ) : if not issymbolic ( size ) and size == 0 : return 0 address = self . safe_add ( address , size ) allocated = self . allocated GMEMORY = 3 GQUADRATICMEMDENOM = 512 old_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( allocated , 31 ) , 32 ) , 512 ) new_size = Operators . ZEXTEND ( Operators . UDIV ( self . safe_add ( address , 31 ) , 32 ) , 512 ) old_totalfee = self . safe_mul ( old_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( old_size , old_size ) , GQUADRATICMEMDENOM ) new_totalfee = self . safe_mul ( new_size , GMEMORY ) + Operators . UDIV ( self . safe_mul ( new_size , new_size ) , GQUADRATICMEMDENOM ) memfee = new_totalfee - old_totalfee flag = Operators . UGT ( new_totalfee , old_totalfee ) return Operators . ITEBV ( 512 , size == 0 , 0 , Operators . ITEBV ( 512 , flag , memfee , 0 ) )
3694	def Tm ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ ] ) : r def list_methods ( ) : methods = [ ] if CASRN in Tm_ON_data . index : methods . append ( OPEN_NTBKM ) if CASRN in CRC_inorganic_data . index and not np . isnan ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_INORG ) if CASRN in CRC_organic_data . index and not np . isnan ( CRC_organic_data . at [ CASRN , 'Tm' ] ) : methods . append ( CRC_ORG ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == OPEN_NTBKM : return float ( Tm_ON_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_INORG : return float ( CRC_inorganic_data . at [ CASRN , 'Tm' ] ) elif Method == CRC_ORG : return float ( CRC_organic_data . at [ CASRN , 'Tm' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
5138	def process_file ( self , file ) : if sys . version_info [ 0 ] >= 3 : nxt = file . __next__ else : nxt = file . next for token in tokenize . generate_tokens ( nxt ) : self . process_token ( * token ) self . make_index ( )
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
12495	def column_or_1d ( y , warn = False ) : shape = np . shape ( y ) if len ( shape ) == 1 : return np . ravel ( y ) if len ( shape ) == 2 and shape [ 1 ] == 1 : if warn : warnings . warn ( "A column-vector y was passed when a 1d array was" " expected. Please change the shape of y to " "(n_samples, ), for example using ravel()." , DataConversionWarning , stacklevel = 2 ) return np . ravel ( y ) raise ValueError ( "bad input shape {0}" . format ( shape ) )
13357	def moyennes_glissantes ( df , sur = 8 , rep = 0.75 ) : return pd . rolling_mean ( df , window = sur , min_periods = rep * sur )
9048	def gradient ( self ) : grad = { } for i , f in enumerate ( self . _covariances ) : for varname , g in f . gradient ( ) . items ( ) : grad [ f"{self._name}[{i}].{varname}" ] = g return grad
7383	def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
7352	def to_dataframe ( self , columns = BindingPrediction . fields + ( "length" , ) ) : return pd . DataFrame . from_records ( [ tuple ( [ getattr ( x , name ) for name in columns ] ) for x in self ] , columns = columns )
12213	def update_field_from_proxy ( field_obj , pref_proxy ) : attr_names = ( 'verbose_name' , 'help_text' , 'default' ) for attr_name in attr_names : setattr ( field_obj , attr_name , getattr ( pref_proxy , attr_name ) )
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
11026	def sort_pem_objects ( pem_objects ) : keys , certs , ca_certs = [ ] , [ ] , [ ] for pem_object in pem_objects : if isinstance ( pem_object , pem . Key ) : keys . append ( pem_object ) else : if _is_ca ( pem_object ) : ca_certs . append ( pem_object ) else : certs . append ( pem_object ) [ key ] , [ cert ] = keys , certs return key , cert , ca_certs
8907	def list_services ( self ) : my_services = [ ] for service in self . collection . find ( ) . sort ( 'name' , pymongo . ASCENDING ) : my_services . append ( Service ( service ) ) return my_services
6948	def jhk_to_sdssg ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSG_JHK , SDSSG_JH , SDSSG_JK , SDSSG_HK , SDSSG_J , SDSSG_H , SDSSG_K )
9750	def find_x ( path1 ) : libs = os . listdir ( path1 ) for lib_dir in libs : if "doublefann" in lib_dir : return True
3714	def calculate ( self , T , method ) : r if method == CRC_INORG_S : Vms = self . CRC_INORG_S_Vm elif method in self . tabular_data : Vms = self . interpolate ( T , method ) return Vms
8513	def fit_and_score_estimator ( estimator , parameters , cv , X , y = None , scoring = None , iid = True , n_jobs = 1 , verbose = 1 , pre_dispatch = '2*n_jobs' ) : scorer = check_scoring ( estimator , scoring = scoring ) n_samples = num_samples ( X ) X , y = check_arrays ( X , y , allow_lists = True , sparse_format = 'csr' , allow_nans = True ) if y is not None : if len ( y ) != n_samples : raise ValueError ( 'Target variable (y) has a different number ' 'of samples (%i) than data (X: %i samples)' % ( len ( y ) , n_samples ) ) cv = check_cv ( cv = cv , y = y , classifier = is_classifier ( estimator ) ) out = Parallel ( n_jobs = n_jobs , verbose = verbose , pre_dispatch = pre_dispatch ) ( delayed ( _fit_and_score ) ( clone ( estimator ) , X , y , scorer , train , test , verbose , parameters , fit_params = None ) for train , test in cv . split ( X , y ) ) assert len ( out ) == cv . n_splits train_scores , test_scores = [ ] , [ ] n_train_samples , n_test_samples = [ ] , [ ] for test_score , n_test , train_score , n_train , _ in out : train_scores . append ( train_score ) test_scores . append ( test_score ) n_test_samples . append ( n_test ) n_train_samples . append ( n_train ) train_scores , test_scores = map ( list , check_arrays ( train_scores , test_scores , warn_nans = True , replace_nans = True ) ) if iid : if verbose > 0 and is_msmbuilder_estimator ( estimator ) : print ( '[CV] Using MSMBuilder API n_samples averaging' ) print ( '[CV] n_train_samples: %s' % str ( n_train_samples ) ) print ( '[CV] n_test_samples: %s' % str ( n_test_samples ) ) mean_test_score = np . average ( test_scores , weights = n_test_samples ) mean_train_score = np . average ( train_scores , weights = n_train_samples ) else : mean_test_score = np . average ( test_scores ) mean_train_score = np . average ( train_scores ) grid_scores = { 'mean_test_score' : mean_test_score , 'test_scores' : test_scores , 'mean_train_score' : mean_train_score , 'train_scores' : train_scores , 'n_test_samples' : n_test_samples , 'n_train_samples' : n_train_samples } return grid_scores
4079	def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
120	def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) while not self . all_finished ( ) : time . sleep ( 0.001 ) if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )
9938	def find ( self , path , all = False ) : matches = [ ] for app in self . apps : app_location = self . storages [ app ] . location if app_location not in searched_locations : searched_locations . append ( app_location ) match = self . find_in_app ( app , path ) if match : if not all : return match matches . append ( match ) return matches
3026	def save_to_well_known_file ( credentials , well_known_file = None ) : if well_known_file is None : well_known_file = _get_well_known_file ( ) config_dir = os . path . dirname ( well_known_file ) if not os . path . isdir ( config_dir ) : raise OSError ( 'Config directory does not exist: {0}' . format ( config_dir ) ) credentials_data = credentials . serialization_data _save_private_file ( well_known_file , credentials_data )
12407	def cons ( collection , value ) : if isinstance ( value , collections . Mapping ) : if collection is None : collection = { } collection . update ( ** value ) elif isinstance ( value , six . string_types ) : if collection is None : collection = [ ] collection . append ( value ) elif isinstance ( value , collections . Iterable ) : if collection is None : collection = [ ] collection . extend ( value ) else : if collection is None : collection = [ ] collection . append ( value ) return collection
9069	def _df ( self ) : if not self . _restricted : return self . nsamples return self . nsamples - self . _X [ "tX" ] . shape [ 1 ]
517	def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )
13163	def serialize_text ( out , text ) : padding = len ( out ) add_padding = padding_adder ( padding ) text = add_padding ( text , ignore_first_line = True ) return out + text
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
7236	def read ( self , bands = None , ** kwargs ) : arr = self if bands is not None : arr = self [ bands , ... ] return arr . compute ( scheduler = threaded_get )
12270	def read_file ( self , filename ) : try : fh = open ( filename , 'rb' ) table_set = any_tableset ( fh ) except : table_set = None return table_set
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : r if self . method : if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
8887	def fit ( self , x , y = None ) : x = iter2array ( x , dtype = ( MoleculeContainer , CGRContainer ) ) if self . __head_less : warn ( f'{self.__class__.__name__} configured to head less mode. fit unusable' ) return self self . _reset ( ) self . __prepare ( x ) return self
8699	def __write ( self , output , binary = False ) : if not binary : log . debug ( 'write: %s' , output ) else : log . debug ( 'write binary: %s' , hexify ( output ) ) self . _port . write ( output ) self . _port . flush ( )
8556	def list_lans ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
11089	def _sort_by ( key ) : @ staticmethod def sort_by ( p_list , reverse = False ) : return sorted ( p_list , key = lambda p : getattr ( p , key ) , reverse = reverse , ) return sort_by
639	def getBool ( cls , prop ) : value = cls . getInt ( prop ) if value not in ( 0 , 1 ) : raise ValueError ( "Expected 0 or 1, but got %r in config property %s" % ( value , prop ) ) return bool ( value )
10646	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return np . polyval ( self . _coeffs , state [ 'T' ] )
767	def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result
12499	def fwhm2sigma ( fwhm ) : fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( 8 * np . log ( 2 ) )
13161	def select ( cls , cur , table : str , order_by : str , columns : list = None , where_keys : list = None , limit = 100 , offset = 0 ) : if columns : columns_string = cls . _COMMA . join ( columns ) if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_selective_column_with_condition . format ( columns_string , table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_selective_column . format ( columns_string , table , order_by , limit , offset ) q , t = query , ( ) else : if where_keys : where_clause , values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _select_all_string_with_condition . format ( table , where_clause , order_by , limit , offset ) q , t = query , values else : query = cls . _select_all_string . format ( table , order_by , limit , offset ) q , t = query , ( ) yield from cur . execute ( q , t ) return ( yield from cur . fetchall ( ) )
11071	def with_proxies ( proxy_map , get_key ) : def wrapper ( cls ) : for label , ProxiedClass in six . iteritems ( proxy_map ) : proxy = proxy_factory ( cls , label , ProxiedClass , get_key ) setattr ( cls , label , proxy ) return cls return wrapper
6969	def _old_epd_diffmags ( coeff , fsv , fdv , fkv , xcc , ycc , bgv , bge , mag ) : return - ( coeff [ 0 ] * fsv ** 2. + coeff [ 1 ] * fsv + coeff [ 2 ] * fdv ** 2. + coeff [ 3 ] * fdv + coeff [ 4 ] * fkv ** 2. + coeff [ 5 ] * fkv + coeff [ 6 ] + coeff [ 7 ] * fsv * fdv + coeff [ 8 ] * fsv * fkv + coeff [ 9 ] * fdv * fkv + coeff [ 10 ] * np . sin ( 2 * np . pi * xcc ) + coeff [ 11 ] * np . cos ( 2 * np . pi * xcc ) + coeff [ 12 ] * np . sin ( 2 * np . pi * ycc ) + coeff [ 13 ] * np . cos ( 2 * np . pi * ycc ) + coeff [ 14 ] * np . sin ( 4 * np . pi * xcc ) + coeff [ 15 ] * np . cos ( 4 * np . pi * xcc ) + coeff [ 16 ] * np . sin ( 4 * np . pi * ycc ) + coeff [ 17 ] * np . cos ( 4 * np . pi * ycc ) + coeff [ 18 ] * bgv + coeff [ 19 ] * bge - mag )
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( ** self . _sharedInstance . config ) return self . _sharedInstance . instance
10847	def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
1444	def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
12430	def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : if self . _user : cfg += 'user {0};\n' . format ( self . _user ) cfg += 'worker_processes 1;\nerror_log {0}-errors.log;\n\pid {1}_ nginx.pid;\n\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' cfg += '\t\t}\n' cfg += '\t}\n' if not self . _shared_hosting : cfg += '}\n' f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
8830	def segment_allocation_find ( context , lock_mode = False , ** filters ) : range_ids = filters . pop ( "segment_allocation_range_ids" , None ) query = context . session . query ( models . SegmentAllocation ) if lock_mode : query = query . with_lockmode ( "update" ) query = query . filter_by ( ** filters ) if range_ids : query . filter ( models . SegmentAllocation . segment_allocation_range_id . in_ ( range_ids ) ) return query
1701	def join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
440	def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( " layer {:3}: {:20} {:15} {}" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )
7859	def make_result_response ( self ) : if self . stanza_type not in ( "set" , "get" ) : raise ValueError ( "Results may only be generated for" " 'set' or 'get' iq" ) stanza = Iq ( stanza_type = "result" , from_jid = self . to_jid , to_jid = self . from_jid , stanza_id = self . stanza_id ) return stanza
445	def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" ) : data_files = [ ] for pattern in file_pattern . split ( "," ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( "Found no input files matching %s" , file_pattern ) else : tl . logging . info ( "Prefetching values from %d files matching %s" , len ( data_files ) , file_pattern ) if is_training : print ( " is_training == True : RandomShuffleQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = "random_" + value_queue_name ) else : print ( " is_training == False : FIFOQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( "queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue
3316	def get_domain_realm ( self , path_info , environ ) : realm = self . _calc_realm_from_path_provider ( path_info , environ ) return realm
8354	def start_meta ( self , attrs ) : httpEquiv = None contentType = None contentTypeIndex = None tagNeedsEncodingSubstitution = False for i in range ( 0 , len ( attrs ) ) : key , value = attrs [ i ] key = key . lower ( ) if key == 'http-equiv' : httpEquiv = value elif key == 'content' : contentType = value contentTypeIndex = i if httpEquiv and contentType : match = self . CHARSET_RE . search ( contentType ) if match : if ( self . declaredHTMLEncoding is not None or self . originalEncoding == self . fromEncoding ) : def rewrite ( match ) : return match . group ( 1 ) + "%SOUP-ENCODING%" newAttr = self . CHARSET_RE . sub ( rewrite , contentType ) attrs [ contentTypeIndex ] = ( attrs [ contentTypeIndex ] [ 0 ] , newAttr ) tagNeedsEncodingSubstitution = True else : newCharset = match . group ( 3 ) if newCharset and newCharset != self . originalEncoding : self . declaredHTMLEncoding = newCharset self . _feed ( self . declaredHTMLEncoding ) raise StopParsing pass tag = self . unknown_starttag ( "meta" , attrs ) if tag and tagNeedsEncodingSubstitution : tag . containsSubstitutions = True
2400	def get_good_pos_ngrams ( self ) : if ( os . path . isfile ( NGRAM_PATH ) ) : good_pos_ngrams = pickle . load ( open ( NGRAM_PATH , 'rb' ) ) elif os . path . isfile ( ESSAY_CORPUS_PATH ) : essay_corpus = open ( ESSAY_CORPUS_PATH ) . read ( ) essay_corpus = util_functions . sub_chars ( essay_corpus ) good_pos_ngrams = util_functions . regenerate_good_tokens ( essay_corpus ) pickle . dump ( good_pos_ngrams , open ( NGRAM_PATH , 'wb' ) ) else : good_pos_ngrams = [ 'NN PRP' , 'NN PRP .' , 'NN PRP . DT' , 'PRP .' , 'PRP . DT' , 'PRP . DT NNP' , '. DT' , '. DT NNP' , '. DT NNP NNP' , 'DT NNP' , 'DT NNP NNP' , 'DT NNP NNP NNP' , 'NNP NNP' , 'NNP NNP NNP' , 'NNP NNP NNP NNP' , 'NNP NNP NNP .' , 'NNP NNP .' , 'NNP NNP . TO' , 'NNP .' , 'NNP . TO' , 'NNP . TO NNP' , '. TO' , '. TO NNP' , '. TO NNP NNP' , 'TO NNP' , 'TO NNP NNP' ] return set ( good_pos_ngrams )
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
6543	def exec_command ( self , cmdstr ) : if self . is_terminated : raise TerminatedError ( "this TerminalClient instance has been terminated" ) log . debug ( "sending command: %s" , cmdstr ) c = Command ( self . app , cmdstr ) start = time . time ( ) c . execute ( ) elapsed = time . time ( ) - start log . debug ( "elapsed execution: {0}" . format ( elapsed ) ) self . status = Status ( c . status_line ) return c
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , ** kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
745	def anomalyRemoveLabels ( self , start , end , labelFilter ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter )
7608	def get_all_cards ( self , timeout : int = None ) : url = self . api . CARDS return self . _get_model ( url , timeout = timeout )
5877	def get_local_image ( self , src ) : return ImageUtils . store_image ( self . fetcher , self . article . link_hash , src , self . config )
4952	def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
4687	def decrypt ( self , message ) : if not message : return None try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
2215	def _join_itemstrs ( itemstrs , itemsep , newlines , _leaf_info , nobraces , trailing_sep , compact_brace , lbr , rbr ) : use_newline = newlines > 0 if newlines < 0 : use_newline = ( - newlines ) < _leaf_info [ 'max_height' ] if use_newline : sep = ',\n' if nobraces : body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = body_str else : if compact_brace : indented = itemstrs else : import ubelt as ub prefix = ' ' * 4 indented = [ ub . indent ( s , prefix ) for s in itemstrs ] body_str = sep . join ( indented ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' if compact_brace : braced_body_str = ( lbr + body_str . replace ( '\n' , '\n ' ) + rbr ) else : braced_body_str = ( lbr + '\n' + body_str + '\n' + rbr ) retstr = braced_body_str else : sep = ',' + itemsep body_str = sep . join ( itemstrs ) if trailing_sep and len ( itemstrs ) > 0 : body_str += ',' retstr = ( lbr + body_str + rbr ) return retstr
1001	def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print "----- computeEnd summary: " print "learn:" , learn print "numBurstingCols: %s, " % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print "curPredScore2: %s, " % ( self . _internalStats [ 'curPredictionScore2' ] ) , print "curFalsePosScore: %s, " % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print "1-curFalseNegScore: %s, " % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print "numSegments: " , self . getNumSegments ( ) , print "avgLearnedSeqLength: " , self . avgLearnedSeqLength print "----- infActiveState (%d on) ------" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print "----- infPredictedState (%d on)-----" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print "----- lrnActiveState (%d on) ------" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print "----- lrnPredictedState (%d on)-----" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print "----- cellConfidence -----" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print "----- colConfidence -----" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print "----- cellConfidence[t-1] for currently active cells -----" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print "Cells, predicted segments only:" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print "TM: learn:" , learn print "TM: active outputs(%d):" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )
13119	def count ( self , * args , ** kwargs ) : search = self . create_search ( * args , ** kwargs ) try : return search . count ( ) except NotFoundError : print_error ( "The index was not found, have you initialized the index?" ) except ( ConnectionError , TransportError ) : print_error ( "Cannot connect to elasticsearch" )
167	def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )
916	def debug ( self , msg , * args , ** kwargs ) : self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs )
3127	def update ( self , template_id , data ) : if 'name' not in data : raise KeyError ( 'The template must have a name' ) if 'html' not in data : raise KeyError ( 'The template must have html' ) self . template_id = template_id return self . _mc_client . _patch ( url = self . _build_path ( template_id ) , data = data )
8251	def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
3063	def parse_unique_urlencoded ( content ) : urlencoded_params = urllib . parse . parse_qs ( content ) params = { } for key , value in six . iteritems ( urlencoded_params ) : if len ( value ) != 1 : msg = ( 'URL-encoded content contains a repeated value:' '%s -> %s' % ( key , ', ' . join ( value ) ) ) raise ValueError ( msg ) params [ key ] = value [ 0 ] return params
2111	def empty ( organization = None , user = None , team = None , credential_type = None , credential = None , notification_template = None , inventory_script = None , inventory = None , project = None , job_template = None , workflow = None , all = None , no_color = False ) : from tower_cli . cli . transfer . cleaner import Cleaner destroyer = Cleaner ( no_color ) assets_to_export = { } for asset_type in SEND_ORDER : assets_to_export [ asset_type ] = locals ( ) [ asset_type ] destroyer . go_ham ( all = all , asset_input = assets_to_export )
8087	def font ( self , fontpath = None , fontsize = None ) : if fontpath is not None : self . _canvas . fontfile = fontpath else : return self . _canvas . fontfile if fontsize is not None : self . _canvas . fontsize = fontsize
12425	def reverse ( self ) : if not self . test_drive and self . bumps : map ( lambda b : b . reverse ( ) , self . bumpers )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
5589	def hillshade ( elevation , tile , azimuth = 315.0 , altitude = 45.0 , z = 1.0 , scale = 1.0 ) : azimuth = float ( azimuth ) altitude = float ( altitude ) z = float ( z ) scale = float ( scale ) xres = tile . tile . pixel_x_size yres = - tile . tile . pixel_y_size slope , aspect = calculate_slope_aspect ( elevation , xres , yres , z = z , scale = scale ) deg2rad = math . pi / 180.0 shaded = np . sin ( altitude * deg2rad ) * np . sin ( slope ) + np . cos ( altitude * deg2rad ) * np . cos ( slope ) * np . cos ( ( azimuth - 90.0 ) * deg2rad - aspect ) shaded = ( ( ( shaded + 1.0 ) / 2 ) * - 255.0 ) . astype ( "uint8" ) return ma . masked_array ( data = np . pad ( shaded , 1 , mode = 'edge' ) , mask = elevation . mask )
7438	def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) return pd . DataFrame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
12955	def _add_id_to_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_key_for_index ( indexedField , val ) , pk )
4078	def write_py2k_header ( file_list ) : if not isinstance ( file_list , list ) : file_list = [ file_list ] python_re = re . compile ( br"^(#!.*\bpython)(.*)([\r\n]+)$" ) coding_re = re . compile ( br"coding[:=]\s*([-\w.]+)" ) new_line_re = re . compile ( br"([\r\n]+)$" ) version_3 = LooseVersion ( '3' ) for file in file_list : if not os . path . getsize ( file ) : continue rewrite_needed = False python_found = False coding_found = False lines = [ ] f = open ( file , 'rb' ) try : while len ( lines ) < 2 : line = f . readline ( ) match = python_re . match ( line ) if match : python_found = True version = LooseVersion ( match . group ( 2 ) . decode ( ) or '2' ) try : version_test = version >= version_3 except TypeError : version_test = True if version_test : line = python_re . sub ( br"\g<1>2\g<3>" , line ) rewrite_needed = True elif coding_re . search ( line ) : coding_found = True lines . append ( line ) if not coding_found : match = new_line_re . search ( lines [ 0 ] ) newline = match . group ( 1 ) if match else b"\n" line = b"# -*- coding: utf-8 -*-" + newline lines . insert ( 1 if python_found else 0 , line ) rewrite_needed = True if rewrite_needed : lines += f . readlines ( ) finally : f . close ( ) if rewrite_needed : f = open ( file , 'wb' ) try : f . writelines ( lines ) finally : f . close ( )
13410	def addLogbooks ( self , type = None , logs = [ ] , default = "" ) : if type is not None and len ( logs ) != 0 : if type in self . logList : for logbook in logs : if logbook not in self . logList . get ( type ) [ 0 ] : self . logList . get ( type ) [ 0 ] . append ( logbook ) else : self . logList [ type ] = [ ] self . logList [ type ] . append ( logs ) if len ( self . logList [ type ] ) > 1 and default != "" : self . logList . get ( type ) [ 1 ] == default else : self . logList . get ( type ) . append ( default ) self . logType . clear ( ) self . logType . addItems ( list ( self . logList . keys ( ) ) ) self . changeLogType ( )
1681	def AddFilters ( self , filters ) : for filt in filters . split ( ',' ) : clean_filt = filt . strip ( ) if clean_filt : self . filters . append ( clean_filt ) for filt in self . filters : if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : raise ValueError ( 'Every filter in --filters must start with + or -' ' (%s does not)' % filt )
8268	def color ( self , clr = None , d = 0.035 ) : if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
7241	def aoi ( self , ** kwargs ) : g = self . _parse_geoms ( ** kwargs ) if g is None : return self else : return self [ g ]
7808	def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) except AttributeError : return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject_name = data . get ( 'subject' ) cert . alt_names = defaultdict ( list ) if 'subjectAltName' in data : for name , value in data [ 'subjectAltName' ] : cert . alt_names [ name ] . append ( value ) if 'notAfter' in data : tstamp = ssl . cert_time_to_seconds ( data [ 'notAfter' ] ) cert . not_after = datetime . utcfromtimestamp ( tstamp ) if sys . version_info . major < 3 : cert . _decode_names ( ) cert . common_names = [ ] if cert . subject_name : for part in cert . subject_name : for name , value in part : if name == 'commonName' : cert . common_names . append ( value ) return cert
2989	def serialize_options ( opts ) : options = ( opts or { } ) . copy ( ) for key in opts . keys ( ) : if key not in DEFAULT_OPTIONS : LOG . warning ( "Unknown option passed to Flask-CORS: %s" , key ) options [ 'origins' ] = sanitize_regex_param ( options . get ( 'origins' ) ) options [ 'allow_headers' ] = sanitize_regex_param ( options . get ( 'allow_headers' ) ) if r'.*' in options [ 'origins' ] and options [ 'supports_credentials' ] and options [ 'send_wildcard' ] : raise ValueError ( "Cannot use supports_credentials in conjunction with" "an origin string of '*'. See: " "http://www.w3.org/TR/cors/#resource-requests" ) serialize_option ( options , 'expose_headers' ) serialize_option ( options , 'methods' , upper = True ) if isinstance ( options . get ( 'max_age' ) , timedelta ) : options [ 'max_age' ] = str ( int ( options [ 'max_age' ] . total_seconds ( ) ) ) return options
4299	def create_project ( config_data ) : env = deepcopy ( dict ( os . environ ) ) env [ str ( 'DJANGO_SETTINGS_MODULE' ) ] = str ( '{0}.settings' . format ( config_data . project_name ) ) env [ str ( 'PYTHONPATH' ) ] = str ( os . pathsep . join ( map ( shlex_quote , sys . path ) ) ) kwargs = { } args = [ ] if config_data . template : kwargs [ 'template' ] = config_data . template args . append ( config_data . project_name ) if config_data . project_directory : args . append ( config_data . project_directory ) if not os . path . exists ( config_data . project_directory ) : os . makedirs ( config_data . project_directory ) base_cmd = 'django-admin.py' start_cmds = [ os . path . join ( os . path . dirname ( sys . executable ) , base_cmd ) ] start_cmd_pnodes = [ 'Scripts' ] start_cmds . extend ( [ os . path . join ( os . path . dirname ( sys . executable ) , pnode , base_cmd ) for pnode in start_cmd_pnodes ] ) start_cmd = [ base_cmd ] for p in start_cmds : if os . path . exists ( p ) : start_cmd = [ sys . executable , p ] break cmd_args = start_cmd + [ 'startproject' ] + args if config_data . verbose : sys . stdout . write ( 'Project creation command: {0}\n' . format ( ' ' . join ( cmd_args ) ) ) try : output = subprocess . check_output ( cmd_args , stderr = subprocess . STDOUT ) sys . stdout . write ( output . decode ( 'utf-8' ) ) except subprocess . CalledProcessError as e : if config_data . verbose : sys . stdout . write ( e . output . decode ( 'utf-8' ) ) raise
3641	def tradepile ( self ) : method = 'GET' url = 'tradepile' rc = self . __request__ ( method , url ) events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer List - List View' ) ] if rc . get ( 'auctionInfo' ) : events . append ( self . pin . event ( 'page_view' , 'Item - Detail View' ) ) self . pin . send ( events ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
3376	def fix_objective_as_constraint ( model , fraction = 1 , bound = None , name = 'fixed_objective_{}' ) : fix_objective_name = name . format ( model . objective . name ) if fix_objective_name in model . constraints : model . solver . remove ( fix_objective_name ) if bound is None : bound = model . slim_optimize ( error_value = None ) * fraction if model . objective . direction == 'max' : ub , lb = None , bound else : ub , lb = bound , None constraint = model . problem . Constraint ( model . objective . expression , name = fix_objective_name , ub = ub , lb = lb ) add_cons_vars_to_problem ( model , constraint , sloppy = True ) return bound
7180	def reapply_all ( ast_node , lib2to3_node ) : late_processing = reapply ( ast_node , lib2to3_node ) for lazy_func in reversed ( late_processing ) : lazy_func ( )
5667	def add_walk_distances_to_db_python ( gtfs , osm_path , cutoff_distance_m = 1000 ) : if isinstance ( gtfs , str ) : gtfs = GTFS ( gtfs ) assert ( isinstance ( gtfs , GTFS ) ) print ( "Reading in walk network" ) walk_network = create_walk_network_from_osm ( osm_path ) print ( "Matching stops to the OSM network" ) stop_I_to_nearest_osm_node , stop_I_to_nearest_osm_node_distance = match_stops_to_nodes ( gtfs , walk_network ) transfers = gtfs . get_straight_line_transfer_distances ( ) from_I_to_to_stop_Is = { stop_I : set ( ) for stop_I in stop_I_to_nearest_osm_node } for transfer_tuple in transfers . itertuples ( ) : from_I = transfer_tuple . from_stop_I to_I = transfer_tuple . to_stop_I from_I_to_to_stop_Is [ from_I ] . add ( to_I ) print ( "Computing walking distances" ) for from_I , to_stop_Is in from_I_to_to_stop_Is . items ( ) : from_node = stop_I_to_nearest_osm_node [ from_I ] from_dist = stop_I_to_nearest_osm_node_distance [ from_I ] shortest_paths = networkx . single_source_dijkstra_path_length ( walk_network , from_node , cutoff = cutoff_distance_m - from_dist , weight = "distance" ) for to_I in to_stop_Is : to_distance = stop_I_to_nearest_osm_node_distance [ to_I ] to_node = stop_I_to_nearest_osm_node [ to_I ] osm_distance = shortest_paths . get ( to_node , float ( 'inf' ) ) total_distance = from_dist + osm_distance + to_distance from_stop_I_transfers = transfers [ transfers [ 'from_stop_I' ] == from_I ] straigth_distance = from_stop_I_transfers [ from_stop_I_transfers [ "to_stop_I" ] == to_I ] [ "d" ] . values [ 0 ] assert ( straigth_distance < total_distance + 2 ) if total_distance <= cutoff_distance_m : gtfs . conn . execute ( "UPDATE stop_distances " "SET d_walk = " + str ( int ( total_distance ) ) + " WHERE from_stop_I=" + str ( from_I ) + " AND to_stop_I=" + str ( to_I ) ) gtfs . conn . commit ( )
7542	def storealleles ( consens , hidx , alleles ) : bigbase = PRIORITY [ consens [ hidx [ 0 ] ] ] bigallele = [ i for i in alleles if i [ 0 ] == bigbase ] [ 0 ] for hsite , pbase in zip ( hidx [ 1 : ] , bigallele [ 1 : ] ) : if PRIORITY [ consens [ hsite ] ] != pbase : consens [ hsite ] = consens [ hsite ] . lower ( ) return consens
964	def export ( self ) : graph = nx . MultiDiGraph ( ) regions = self . network . getRegions ( ) for idx in xrange ( regions . getCount ( ) ) : regionPair = regions . getByIndex ( idx ) regionName = regionPair [ 0 ] graph . add_node ( regionName , label = regionName ) for linkName , link in self . network . getLinks ( ) : graph . add_edge ( link . getSrcRegionName ( ) , link . getDestRegionName ( ) , src = link . getSrcOutputName ( ) , dest = link . getDestInputName ( ) ) return graph
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : def gen_groups ( items ) : j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) if len ( groups ) == 1 : return False normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian_distance < normal_distance else : return False
11329	def main ( ) : argparser = ArgumentParser ( ) subparsers = argparser . add_subparsers ( dest = 'selected_subparser' ) all_parser = subparsers . add_parser ( 'all' ) elsevier_parser = subparsers . add_parser ( 'elsevier' ) oxford_parser = subparsers . add_parser ( 'oxford' ) springer_parser = subparsers . add_parser ( 'springer' ) all_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--run-locally' , action = 'store_true' ) elsevier_parser . add_argument ( '--package-name' ) elsevier_parser . add_argument ( '--path' ) elsevier_parser . add_argument ( '--CONSYN' , action = 'store_true' ) elsevier_parser . add_argument ( '--update-credentials' , action = 'store_true' ) elsevier_parser . add_argument ( '--extract-nations' , action = 'store_true' ) oxford_parser . add_argument ( '--dont-empty-ftp' , action = 'store_true' ) oxford_parser . add_argument ( '--package-name' ) oxford_parser . add_argument ( '--path' ) oxford_parser . add_argument ( '--update-credentials' , action = 'store_true' ) oxford_parser . add_argument ( '--extract-nations' , action = 'store_true' ) springer_parser . add_argument ( '--package-name' ) springer_parser . add_argument ( '--path' ) springer_parser . add_argument ( '--update-credentials' , action = 'store_true' ) springer_parser . add_argument ( '--extract-nations' , action = 'store_true' ) settings = Bunch ( vars ( argparser . parse_args ( ) ) ) call_package ( settings )
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
2890	def create_task ( self ) : return self . spec_class ( self . spec , self . get_task_spec_name ( ) , lane = self . get_lane ( ) , description = self . node . get ( 'name' , None ) )
8572	def list_nics ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
10463	def menuitemenabled ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) if menu_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
10273	def remove_unweighted_sources ( graph : BELGraph , key : Optional [ str ] = None ) -> None : nodes = list ( get_unweighted_sources ( graph , key = key ) ) graph . remove_nodes_from ( nodes )
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) raise Z3NotFoundError for cfg in self . _init : self . _send ( cfg )
2100	def read ( self , * args , ** kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , ** kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
6776	def _configure_users ( self , site = None , full = 0 , only_data = 0 ) : site = site or ALL full = int ( full ) if full and not only_data : packager = self . get_satchel ( 'packager' ) packager . install_required ( type = SYSTEM , service = self . name ) r = self . local_renderer params = self . get_user_vhosts ( site = site ) with settings ( warn_only = True ) : self . add_admin_user ( ) params = sorted ( list ( params ) ) if not only_data : for user , password , vhost in params : r . env . broker_user = user r . env . broker_password = password r . env . broker_vhost = vhost with settings ( warn_only = True ) : r . sudo ( 'rabbitmqctl add_user {broker_user} {broker_password}' ) r . sudo ( 'rabbitmqctl add_vhost {broker_vhost}' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {broker_user} ".*" ".*" ".*"' ) r . sudo ( 'rabbitmqctl set_permissions -p {broker_vhost} {admin_username} ".*" ".*" ".*"' ) return params
12769	def load_markers ( self , filename , attachments , max_frames = 1e100 ) : self . markers = Markers ( self ) fn = filename . lower ( ) if fn . endswith ( '.c3d' ) : self . markers . load_c3d ( filename , max_frames = max_frames ) elif fn . endswith ( '.csv' ) or fn . endswith ( '.csv.gz' ) : self . markers . load_csv ( filename , max_frames = max_frames ) else : logging . fatal ( '%s: not sure how to load markers!' , filename ) self . markers . load_attachments ( attachments , self . skeleton )
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
8892	def get_default ( self ) : if self . has_default ( ) : if callable ( self . default ) : default = self . default ( ) if isinstance ( default , uuid . UUID ) : return default . hex return default if isinstance ( self . default , uuid . UUID ) : return self . default . hex return self . default return None
3732	def mixture_from_any ( ID ) : if type ( ID ) == list : if len ( ID ) == 1 : ID = ID [ 0 ] else : raise Exception ( 'If the input is a list, the list must contain only one item.' ) ID = ID . lower ( ) . strip ( ) ID2 = ID . replace ( ' ' , '' ) ID3 = ID . replace ( '-' , '' ) for i in [ ID , ID2 , ID3 ] : if i in _MixtureDictLookup : return _MixtureDictLookup [ i ] raise Exception ( 'Mixture name not recognized' )
7204	def set ( self , ** kwargs ) : for port_name , port_value in kwargs . items ( ) : if hasattr ( port_value , 'value' ) : port_value = port_value . value self . inputs . __setattr__ ( port_name , port_value )
5411	def build_machine ( network = None , machine_type = None , preemptible = None , service_account = None , boot_disk_size_gb = None , disks = None , accelerators = None , labels = None , cpu_platform = None , nvidia_driver_version = None ) : return { 'network' : network , 'machineType' : machine_type , 'preemptible' : preemptible , 'serviceAccount' : service_account , 'bootDiskSizeGb' : boot_disk_size_gb , 'disks' : disks , 'accelerators' : accelerators , 'labels' : labels , 'cpuPlatform' : cpu_platform , 'nvidiaDriverVersion' : nvidia_driver_version , }
13324	def info ( ) : env = cpenv . get_active_env ( ) modules = [ ] if env : modules = env . get_modules ( ) active_modules = cpenv . get_active_modules ( ) if not env and not modules and not active_modules : click . echo ( '\nNo active modules...' ) return click . echo ( bold ( '\nActive modules' ) ) if env : click . echo ( format_objects ( [ env ] + active_modules ) ) available_modules = set ( modules ) - set ( active_modules ) if available_modules : click . echo ( bold ( '\nInactive modules in {}\n' ) . format ( cyan ( env . name ) ) ) click . echo ( format_objects ( available_modules , header = False ) ) else : click . echo ( format_objects ( active_modules ) ) available_shared_modules = set ( cpenv . get_modules ( ) ) - set ( active_modules ) if not available_shared_modules : return click . echo ( bold ( '\nInactive shared modules \n' ) ) click . echo ( format_objects ( available_shared_modules , header = False ) )
2909	def _find_any ( self , task_spec ) : tasks = [ ] if self . task_spec == task_spec : tasks . append ( self ) for child in self : if child . task_spec != task_spec : continue tasks . append ( child ) return tasks
7919	def __from_unicode ( cls , data , check = True ) : parts1 = data . split ( u"/" , 1 ) parts2 = parts1 [ 0 ] . split ( u"@" , 1 ) if len ( parts2 ) == 2 : local = parts2 [ 0 ] domain = parts2 [ 1 ] if check : local = cls . __prepare_local ( local ) domain = cls . __prepare_domain ( domain ) else : local = None domain = parts2 [ 0 ] if check : domain = cls . __prepare_domain ( domain ) if len ( parts1 ) == 2 : resource = parts1 [ 1 ] if check : resource = cls . __prepare_resource ( parts1 [ 1 ] ) else : resource = None if not domain : raise JIDError ( "Domain is required in JID." ) return ( local , domain , resource )
7414	def plot_pairwise_dist ( self , labels = None , ax = None , cmap = None , cdict = None , metric = "euclidean" ) : allele_counts = self . genotypes . to_n_alt ( ) dist = allel . pairwise_distance ( allele_counts , metric = metric ) if not ax : fig = plt . figure ( figsize = ( 5 , 5 ) ) ax = fig . add_subplot ( 1 , 1 , 1 ) if isinstance ( labels , bool ) : if labels : labels = list ( self . samples_vcforder ) elif isinstance ( labels , type ( None ) ) : pass else : if not len ( labels ) == len ( self . samples_vcforder ) : raise IPyradError ( LABELS_LENGTH_ERROR . format ( len ( labels ) , len ( self . samples_vcforder ) ) ) allel . plot . pairwise_distance ( dist , labels = labels , ax = ax , colorbar = False )
2569	def construct_end_message ( self ) : app_count = self . dfk . task_count site_count = len ( [ x for x in self . dfk . config . executors if x . managed ] ) app_fails = len ( [ t for t in self . dfk . tasks if self . dfk . tasks [ t ] [ 'status' ] in FINAL_FAILURE_STATES ] ) message = { 'uuid' : self . uuid , 'end' : time . time ( ) , 't_apps' : app_count , 'sites' : site_count , 'c_time' : None , 'failed' : app_fails , 'test' : self . test_mode , } return json . dumps ( message )
1401	def extract_packing_plan ( self , topology ) : packingPlan = { "id" : "" , "container_plans" : [ ] } if not topology . packing_plan : return packingPlan container_plans = topology . packing_plan . container_plans containers = [ ] for container_plan in container_plans : instances = [ ] for instance_plan in container_plan . instance_plans : instance_resources = { "cpu" : instance_plan . resource . cpu , "ram" : instance_plan . resource . ram , "disk" : instance_plan . resource . disk } instance = { "component_name" : instance_plan . component_name , "task_id" : instance_plan . task_id , "component_index" : instance_plan . component_index , "instance_resources" : instance_resources } instances . append ( instance ) required_resource = { "cpu" : container_plan . requiredResource . cpu , "ram" : container_plan . requiredResource . ram , "disk" : container_plan . requiredResource . disk } scheduled_resource = { } if container_plan . scheduledResource : scheduled_resource = { "cpu" : container_plan . scheduledResource . cpu , "ram" : container_plan . scheduledResource . ram , "disk" : container_plan . scheduledResource . disk } container = { "id" : container_plan . id , "instances" : instances , "required_resources" : required_resource , "scheduled_resources" : scheduled_resource } containers . append ( container ) packingPlan [ "id" ] = topology . packing_plan . id packingPlan [ "container_plans" ] = containers return json . dumps ( packingPlan )
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
11808	def samples ( self , nwords ) : n = self . n nminus1gram = ( '' , ) * ( n - 1 ) output = [ ] for i in range ( nwords ) : if nminus1gram not in self . cond_prob : nminus1gram = ( '' , ) * ( n - 1 ) wn = self . cond_prob [ nminus1gram ] . sample ( ) output . append ( wn ) nminus1gram = nminus1gram [ 1 : ] + ( wn , ) return ' ' . join ( output )
5436	def parse_pair_args ( labels , argclass ) : label_data = set ( ) for arg in labels : name , value = split_pair ( arg , '=' , nullable_idx = 1 ) label_data . add ( argclass ( name , value ) ) return label_data
228	def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos
10387	def build_database ( manager : pybel . Manager , annotation_url : Optional [ str ] = None ) -> None : annotation_url = annotation_url or NEUROMMSIG_DEFAULT_URL annotation = manager . get_namespace_by_url ( annotation_url ) if annotation is None : raise RuntimeError ( 'no graphs in database with given annotation' ) networks = get_networks_using_annotation ( manager , annotation ) dtis = ... for network in networks : graph = network . as_bel ( ) scores = epicom_on_graph ( graph , dtis ) for ( drug_name , subgraph_name ) , score in scores . items ( ) : drug_model = get_drug_model ( manager , drug_name ) subgraph_model = manager . get_annotation_entry ( annotation_url , subgraph_name ) score_model = Score ( network = network , annotation = subgraph_model , drug = drug_model , score = score ) manager . session . add ( score_model ) t = time . time ( ) logger . info ( 'committing scores' ) manager . session . commit ( ) logger . info ( 'committed scores in %.2f seconds' , time . time ( ) - t )
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
7769	def _stream_disconnected ( self , event ) : with self . lock : if event . stream != self . stream : return if self . stream is not None and event . stream == self . stream : if self . stream . transport in self . _ml_handlers : self . _ml_handlers . remove ( self . stream . transport ) self . main_loop . remove_handler ( self . stream . transport ) self . stream = None self . uplink = None
12951	def _get_new_connection ( self ) : pool = getRedisPool ( self . mdl . REDIS_CONNECTION_PARAMS ) return redis . Redis ( connection_pool = pool )
9318	def wait_until_final ( self , poll_interval = 1 , timeout = 60 ) : start_time = time . time ( ) elapsed = 0 while ( self . status != "complete" and ( timeout <= 0 or elapsed < timeout ) ) : time . sleep ( poll_interval ) self . refresh ( ) elapsed = time . time ( ) - start_time
13892	def _AssertIsLocal ( path ) : from six . moves . urllib . parse import urlparse if not _UrlIsLocal ( urlparse ( path ) ) : from . _exceptions import NotImplementedForRemotePathError raise NotImplementedForRemotePathError
4728	def gen_to_dev ( self , address ) : cmd = [ "nvm_addr gen2dev" , self . envs [ "DEV_PATH" ] , "0x{:x}" . format ( address ) ] status , stdout , _ = cij . ssh . command ( cmd , shell = True ) if status : raise RuntimeError ( "cij.liblight.gen_to_dev: cmd fail" ) return int ( re . findall ( r"dev: ([0-9a-fx]+)" , stdout ) [ 0 ] , 16 )
313	def sharpe_ratio ( returns , risk_free = 0 , period = DAILY ) : return ep . sharpe_ratio ( returns , risk_free = risk_free , period = period )
10288	def enrich_composites ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , COMPOSITE ) ) for u in nodes : for v in u . members : graph . add_has_component ( u , v )
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] stopper = threading . Event ( ) if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) if len ( global_tasks ) > 0 : gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) stopper . set ( ) for t in threads : t . join ( ) self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
4212	def main ( argv = None ) : if argv is None : argv = sys . argv [ 1 : ] cli = CommandLineTool ( ) return cli . run ( argv )
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
8427	def brewer_pal ( type = 'seq' , palette = 1 ) : def full_type_name ( text ) : abbrevs = { 'seq' : 'Sequential' , 'qual' : 'Qualitative' , 'div' : 'Diverging' } text = abbrevs . get ( text , text ) return text . title ( ) def number_to_palette_name ( ctype , n ) : n -= 1 palettes = sorted ( colorbrewer . COLOR_MAPS [ ctype ] . keys ( ) ) if n < len ( palettes ) : return palettes [ n ] raise ValueError ( "There are only '{}' palettes of type {}. " "You requested palette no. {}" . format ( len ( palettes ) , ctype , n + 1 ) ) def max_palette_colors ( type , palette_name ) : if type == 'Sequential' : return 9 elif type == 'Diverging' : return 11 else : qlimit = { 'Accent' : 8 , 'Dark2' : 8 , 'Paired' : 12 , 'Pastel1' : 9 , 'Pastel2' : 8 , 'Set1' : 9 , 'Set2' : 8 , 'Set3' : 12 } return qlimit [ palette_name ] type = full_type_name ( type ) if isinstance ( palette , int ) : palette_name = number_to_palette_name ( type , palette ) else : palette_name = palette nmax = max_palette_colors ( type , palette_name ) def _brewer_pal ( n ) : _n = n if n <= nmax else nmax try : bmap = colorbrewer . get_map ( palette_name , type , _n ) except ValueError as err : if 0 <= _n < 3 : bmap = colorbrewer . get_map ( palette_name , type , 3 ) else : raise err hex_colors = bmap . hex_colors [ : n ] if n > nmax : msg = ( "Warning message:" "Brewer palette {} has a maximum of {} colors" "Returning the palette you asked for with" "that many colors" . format ( palette_name , nmax ) ) warnings . warn ( msg ) hex_colors = hex_colors + [ None ] * ( n - nmax ) return hex_colors return _brewer_pal
9965	def _to_attrdict ( self , attrs = None ) : result = self . _baseattrs for attr in attrs : if hasattr ( self , attr ) : result [ attr ] = getattr ( self , attr ) . _to_attrdict ( attrs ) return result
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
8555	def get_lan ( self , datacenter_id , lan_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/lans/%s?depth=%s' % ( datacenter_id , lan_id , str ( depth ) ) ) return response
2012	def instruction ( self ) : try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
13700	def init_app ( self , app ) : app . config . setdefault ( "TRACY_REQUIRE_CLIENT" , False ) if not hasattr ( app , 'extensions' ) : app . extensions = { } app . extensions [ 'restpoints' ] = self app . before_request ( self . _before ) app . after_request ( self . _after )
6947	def jhk_to_sdssu ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSU_JHK , SDSSU_JH , SDSSU_JK , SDSSU_HK , SDSSU_J , SDSSU_H , SDSSU_K )
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
12041	def matrixToHTML ( data , names = None , units = None , bookName = None , sheetName = None , xCol = None ) : if not names : names = [ "" ] * len ( data [ 0 ] ) if data . dtype . names : names = list ( data . dtype . names ) if not units : units = [ "" ] * len ( data [ 0 ] ) for i in range ( len ( units ) ) : if names [ i ] in UNITS . keys ( ) : units [ i ] = UNITS [ names [ i ] ] if 'recarray' in str ( type ( data ) ) : data = data . view ( float ) . reshape ( data . shape + ( - 1 , ) ) if xCol and xCol in names : xCol = names . index ( xCol ) names . insert ( 0 , names [ xCol ] ) units . insert ( 0 , units [ xCol ] ) data = np . insert ( data , 0 , data [ : , xCol ] , 1 ) htmlFname = tempfile . gettempdir ( ) + "/swhlab/WKS-%s.%s.html" % ( bookName , sheetName ) html = html += "<h1>FauxRigin</h1>" if bookName or sheetName : html += '<code><b>%s / %s</b></code><br><br>' % ( bookName , sheetName ) html += "<table>" colNames = [ '' ] for i in range ( len ( units ) ) : label = "%s (%d)" % ( chr ( i + ord ( 'A' ) ) , i ) colNames . append ( label ) html += htmlListToTR ( colNames , 'labelCol' , 'labelCol' ) html += htmlListToTR ( [ 'Long Name' ] + list ( names ) , 'name' , td1Class = 'labelRow' ) html += htmlListToTR ( [ 'Units' ] + list ( units ) , 'units' , td1Class = 'labelRow' ) cutOff = False for y in range ( len ( data ) ) : html += htmlListToTR ( [ y + 1 ] + list ( data [ y ] ) , trClass = 'data%d' % ( y % 2 ) , td1Class = 'labelRow' ) if y >= 200 : cutOff = True break html += "</table>" html = html . replace ( ">nan<" , ">--<" ) html = html . replace ( ">None<" , "><" ) if cutOff : html += "<h3>... showing only %d of %d rows ...</h3>" % ( y , len ( data ) ) html += "</body></html>" with open ( htmlFname , 'w' ) as f : f . write ( html ) webbrowser . open ( htmlFname ) return
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
3139	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The promo rule must have an id' ) if 'description' not in data : raise KeyError ( 'This promo rule must have a description' ) if 'amount' not in data : raise KeyError ( 'This promo rule must have an amount' ) if 'target' not in data : raise KeyError ( 'This promo rule must apply to a target (example per_item, total, or shipping' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'promo-rules' ) , data = data ) if response is not None : return response
6471	def update ( self , points , values = None ) : self . values = values or [ None ] * len ( points ) if np is None : if self . option . function : warnings . warn ( 'numpy not available, function ignored' ) self . points = points self . minimum = min ( self . points ) self . maximum = max ( self . points ) self . current = self . points [ - 1 ] else : self . points = self . apply_function ( points ) self . minimum = np . min ( self . points ) self . maximum = np . max ( self . points ) self . current = self . points [ - 1 ] if self . maximum == self . minimum : self . extents = 1 else : self . extents = ( self . maximum - self . minimum ) self . extents = ( self . maximum - self . minimum )
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
1507	def print_cluster_info ( cl_args ) : parsed_roles = read_and_parse_roles ( cl_args ) masters = list ( parsed_roles [ Role . MASTERS ] ) slaves = list ( parsed_roles [ Role . SLAVES ] ) zookeepers = list ( parsed_roles [ Role . ZOOKEEPERS ] ) cluster = list ( parsed_roles [ Role . CLUSTER ] ) info = OrderedDict ( ) info [ 'numNodes' ] = len ( cluster ) info [ 'nodes' ] = cluster roles = OrderedDict ( ) roles [ 'masters' ] = masters roles [ 'slaves' ] = slaves roles [ 'zookeepers' ] = zookeepers urls = OrderedDict ( ) urls [ 'serviceUrl' ] = get_service_url ( cl_args ) urls [ 'heronUi' ] = get_heron_ui_url ( cl_args ) urls [ 'heronTracker' ] = get_heron_tracker_url ( cl_args ) info [ 'roles' ] = roles info [ 'urls' ] = urls print json . dumps ( info , indent = 2 )
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
13552	def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
2690	def new_compiler ( * args , ** kwargs ) : make_silent = kwargs . pop ( 'silent' , True ) cc = _new_compiler ( * args , ** kwargs ) if is_msvc ( cc ) : from distutils . msvc9compiler import get_build_version if get_build_version ( ) == 10 : cc . initialize ( ) for ldflags in [ cc . ldflags_shared , cc . ldflags_shared_debug ] : unique_extend ( ldflags , [ '/MANIFEST' ] ) elif get_build_version ( ) == 14 : make_silent = False if make_silent : cc . spawn = _CCompiler_spawn_silent return cc
11789	def sample ( self ) : "Return a random sample from the distribution." if self . sampler is None : self . sampler = weighted_sampler ( self . dictionary . keys ( ) , self . dictionary . values ( ) ) return self . sampler ( )
5871	def fetch_course_organizations ( course_key ) : queryset = internal . OrganizationCourse . objects . filter ( course_id = text_type ( course_key ) , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
9977	def fix_lamdaline ( source ) : strio = io . StringIO ( source ) gen = tokenize . generate_tokens ( strio . readline ) tkns = [ ] try : for t in gen : tkns . append ( t ) except tokenize . TokenError : pass lambda_pos = [ ( t . type , t . string ) for t in tkns ] . index ( ( tokenize . NAME , "lambda" ) ) tkns = tkns [ lambda_pos : ] lastop_pos = ( len ( tkns ) - 1 - [ t . type for t in tkns [ : : - 1 ] ] . index ( tokenize . OP ) ) lastop = tkns [ lastop_pos ] fiedlineno = lastop . start [ 0 ] fixedline = lastop . line [ : lastop . start [ 1 ] ] + lastop . line [ lastop . end [ 1 ] : ] tkns = tkns [ : lastop_pos ] fixedlines = "" last_lineno = 0 for t in tkns : if last_lineno == t . start [ 0 ] : continue elif t . start [ 0 ] == fiedlineno : fixedlines += fixedline last_lineno = t . start [ 0 ] else : fixedlines += t . line last_lineno = t . start [ 0 ] return fixedlines
10920	def do_levmarq_particles ( s , particles , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , max_iter = 2 , ** kwargs ) : lp = LMParticles ( s , particles , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . get_termination_stats ( )
424	def run_top_task ( self , task_name = None , sort = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { 'status' : 'pending' } ) task = self . db . Task . find_one_and_update ( kwargs , { '$set' : { 'status' : 'running' } } , sort = sort ) try : if task is None : logging . info ( "[Database] Find Task FAIL: key: {} sort: {}" . format ( task_name , sort ) ) return False else : logging . info ( "[Database] Find Task SUCCESS: key: {} sort: {}" . format ( task_name , sort ) ) _datetime = task [ 'time' ] _script = task [ 'script' ] _id = task [ '_id' ] _hyper_parameters = task [ 'hyper_parameters' ] _saved_result_keys = task [ 'saved_result_keys' ] logging . info ( " hyper parameters:" ) for key in _hyper_parameters : globals ( ) [ key ] = _hyper_parameters [ key ] logging . info ( " {}: {}" . format ( key , _hyper_parameters [ key ] ) ) s = time . time ( ) logging . info ( "[Database] Start Task: key: {} sort: {} push time: {}" . format ( task_name , sort , _datetime ) ) _script = _script . decode ( 'utf-8' ) with tf . Graph ( ) . as_default ( ) : exec ( _script , globals ( ) ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'finished' } } ) __result = { } for _key in _saved_result_keys : logging . info ( " result: {}={} {}" . format ( _key , globals ( ) [ _key ] , type ( globals ( ) [ _key ] ) ) ) __result . update ( { "%s" % _key : globals ( ) [ _key ] } ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'result' : __result } } , return_document = pymongo . ReturnDocument . AFTER ) logging . info ( "[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s" . format ( task_name , sort , _datetime , time . time ( ) - s ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) logging . info ( "[Database] Fail to run task" ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'pending' } } ) return False
2388	def spell_correct ( string ) : f = tempfile . NamedTemporaryFile ( mode = 'w' ) f . write ( string ) f . flush ( ) f_path = os . path . abspath ( f . name ) try : p = os . popen ( aspell_path + " -a < " + f_path + " --sug-mode=ultra" ) incorrect = p . readlines ( ) p . close ( ) except Exception : log . exception ( "aspell process failed; could not spell check" ) return string , 0 , string finally : f . close ( ) incorrect_words = list ( ) correct_spelling = list ( ) for i in range ( 1 , len ( incorrect ) ) : if ( len ( incorrect [ i ] ) > 10 ) : match = re . search ( ":" , incorrect [ i ] ) if hasattr ( match , "start" ) : begstring = incorrect [ i ] [ 2 : match . start ( ) ] begmatch = re . search ( " " , begstring ) begword = begstring [ 0 : begmatch . start ( ) ] sugstring = incorrect [ i ] [ match . start ( ) + 2 : ] sugmatch = re . search ( "," , sugstring ) if hasattr ( sugmatch , "start" ) : sug = sugstring [ 0 : sugmatch . start ( ) ] incorrect_words . append ( begword ) correct_spelling . append ( sug ) newstring = string markup_string = string already_subbed = [ ] for i in range ( 0 , len ( incorrect_words ) ) : sub_pat = r"\b" + incorrect_words [ i ] + r"\b" sub_comp = re . compile ( sub_pat ) newstring = re . sub ( sub_comp , correct_spelling [ i ] , newstring ) if incorrect_words [ i ] not in already_subbed : markup_string = re . sub ( sub_comp , '<bs>' + incorrect_words [ i ] + "</bs>" , markup_string ) already_subbed . append ( incorrect_words [ i ] ) return newstring , len ( incorrect_words ) , markup_string
469	def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : tl . logging . info ( "Creating vocabulary." ) counter = Counter ( ) for c in sentences : counter . update ( c ) tl . logging . info ( " Total words: %d" % len ( counter ) ) word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) word_counts = [ ( "<PAD>" , 0 ) ] + word_counts tl . logging . info ( " Words in vocabulary: %d" % len ( word_counts ) ) with tf . gfile . FastGFile ( word_counts_output_file , "w" ) as f : f . write ( "\n" . join ( [ "%s %d" % ( w , c ) for w , c in word_counts ] ) ) tl . logging . info ( " Wrote vocabulary file: %s" % word_counts_output_file ) reverse_vocab = [ x [ 0 ] for x in word_counts ] unk_id = len ( reverse_vocab ) vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) vocab = SimpleVocabulary ( vocab_dict , unk_id ) return vocab
11899	def _get_src_from_image ( img , fallback_image_file ) : if img is None : return fallback_image_file target_format = img . format if target_format . lower ( ) in [ 'tif' , 'tiff' ] : target_format = 'JPEG' try : bytesio = io . BytesIO ( ) img . save ( bytesio , target_format ) byte_value = bytesio . getvalue ( ) b64 = base64 . b64encode ( byte_value ) return 'data:image/%s;base64,%s' % ( target_format . lower ( ) , b64 ) except IOError as exptn : print ( 'IOError while saving image bytes: %s' % exptn ) return fallback_image_file
9924	def create ( self , * args , ** kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , ** kwargs ) if is_primary : email . set_primary ( ) return email
12924	def safe_repr ( obj ) : try : obj_repr = repr ( obj ) except : obj_repr = "({0}<{1}> repr error)" . format ( type ( obj ) , id ( obj ) ) return obj_repr
12700	def get_i_name ( self , num , is_oai = None ) : if num not in ( 1 , 2 ) : raise ValueError ( "`num` parameter have to be 1 or 2!" ) if is_oai is None : is_oai = self . oai_marc i_name = "ind" if not is_oai else "i" return i_name + str ( num )
7699	def verify_roster_push ( self , fix = False ) : self . _verify ( ( None , u"from" , u"to" , u"both" , u"remove" ) , fix )
8534	def read ( cls , data , protocol = None , fallback_protocol = TBinaryProtocol , finagle_thrift = False , max_fields = MAX_FIELDS , max_list_size = MAX_LIST_SIZE , max_map_size = MAX_MAP_SIZE , max_set_size = MAX_SET_SIZE , read_values = False ) : if len ( data ) < cls . MIN_MESSAGE_SIZE : raise ValueError ( 'not enough data' ) if protocol is None : protocol = cls . detect_protocol ( data , fallback_protocol ) trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) header = None if finagle_thrift : try : header = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) except : trans = TTransport . TMemoryBuffer ( data ) proto = protocol ( trans ) method , mtype , seqid = proto . readMessageBegin ( ) mtype = cls . message_type_to_str ( mtype ) if len ( method ) == 0 or method . isspace ( ) or method . startswith ( ' ' ) : raise ValueError ( 'no method name' ) if len ( method ) > cls . MAX_METHOD_LENGTH : raise ValueError ( 'method name too long' ) valid = range ( 33 , 127 ) if any ( ord ( char ) not in valid for char in method ) : raise ValueError ( 'invalid method name' % method ) args = ThriftStruct . read ( proto , max_fields , max_list_size , max_map_size , max_set_size , read_values ) proto . readMessageEnd ( ) msglen = trans . _buffer . tell ( ) return cls ( method , mtype , seqid , args , header , msglen ) , msglen
7412	def run_tree_inference ( self , nexus , idx ) : tmpdir = tempfile . tempdir tmpfile = os . path . join ( tempfile . NamedTemporaryFile ( delete = False , prefix = str ( idx ) , dir = tmpdir , ) ) tmpfile . write ( nexus ) tmpfile . flush ( ) rax = raxml ( name = str ( idx ) , data = tmpfile . name , workdir = tmpdir , N = 1 , T = 2 ) rax . run ( force = True , block = True , quiet = True ) tmpfile . close ( ) order = get_order ( toytree . tree ( rax . trees . bestTree ) ) return "" . join ( order )
2867	def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
9482	def to_bytes_35 ( self , previous : bytes ) : bc = b"" it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . generate_bytecode_from_obb ( tokens . GET_ITER , b"" ) prev_len = len ( previous ) + len ( bc ) body_bc = b"" for op in self . _body : padded_bc = previous padded_bc += b"\x00\x00\x00" padded_bc += bc padded_bc += b"\x00\x00\x00" padded_bc += body_bc body_bc += util . generate_bytecode_from_obb ( op , padded_bc ) body_bc += util . generate_simple_call ( tokens . JUMP_ABSOLUTE , prev_len + 3 ) body_bc += util . generate_bytecode_from_obb ( tokens . POP_BLOCK , b"" ) body_bc = util . generate_simple_call ( tokens . FOR_ITER , len ( body_bc ) - 1 ) + body_bc bc = util . generate_simple_call ( tokens . SETUP_LOOP , prev_len + len ( body_bc ) - 6 ) + bc + body_bc return bc
11185	def publish ( quiet , dataset_uri ) : access_uri = http_publish ( dataset_uri ) if not quiet : click . secho ( "Dataset accessible at " , nl = False , fg = "green" ) click . secho ( access_uri )
3608	def put_async ( self , url , name , data , callback = None , params = None , headers = None ) : if name is None : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_put_request , args = ( endpoint , data , params , headers ) , callback = callback )
6366	def population ( self ) : return self . _tp + self . _tn + self . _fp + self . _fn
9518	def count_sequences ( infile ) : seq_reader = sequences . file_reader ( infile ) n = 0 for seq in seq_reader : n += 1 return n
13220	def settings ( self ) : stmt = "select {fields} from pg_settings" . format ( fields = ', ' . join ( SETTINGS_FIELDS ) ) settings = [ ] for row in self . _iter_results ( stmt ) : row [ 'setting' ] = self . _vartype_map [ row [ 'vartype' ] ] ( row [ 'setting' ] ) settings . append ( Settings ( ** row ) ) return settings
5145	def _merge_config ( self , config , templates ) : if not templates : return config if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
873	def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )
1757	def write_int ( self , where , expression , size = None , force = False ) : if size is None : size = self . address_bit_size assert size in SANE_SIZES self . _publish ( 'will_write_memory' , where , expression , size ) data = [ Operators . CHR ( Operators . EXTRACT ( expression , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] self . _memory . write ( where , data , force ) self . _publish ( 'did_write_memory' , where , expression , size )
12378	def get ( self , request , response ) : self . assert_operations ( 'read' ) items = self . read ( ) if not items : raise http . exceptions . NotFound ( ) if ( isinstance ( items , Iterable ) and not isinstance ( items , six . string_types ) ) and items : items = pagination . paginate ( self . request , self . response , items ) self . make_response ( items )
6909	def xieta_from_radecl ( inra , indecl , incenterra , incenterdecl , deg = True ) : if deg : ra = np . radians ( inra ) decl = np . radians ( indecl ) centerra = np . radians ( incenterra ) centerdecl = np . radians ( incenterdecl ) else : ra = inra decl = indecl centerra = incenterra centerdecl = incenterdecl cdecc = np . cos ( centerdecl ) sdecc = np . sin ( centerdecl ) crac = np . cos ( centerra ) srac = np . sin ( centerra ) uu = np . cos ( decl ) * np . cos ( ra ) vv = np . cos ( decl ) * np . sin ( ra ) ww = np . sin ( decl ) uun = uu * cdecc * crac + vv * cdecc * srac + ww * sdecc vvn = - uu * srac + vv * crac wwn = - uu * sdecc * crac - vv * sdecc * srac + ww * cdecc denom = vvn * vvn + wwn * wwn aunn = np . zeros_like ( uun ) aunn [ uun >= 1.0 ] = 0.0 aunn [ uun < 1.0 ] = np . arccos ( uun ) xi , eta = np . zeros_like ( aunn ) , np . zeros_like ( aunn ) xi [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 eta [ ( aunn <= 0.0 ) | ( denom <= 0.0 ) ] = 0.0 sdenom = np . sqrt ( denom ) xi [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * vvn / sdenom eta [ ( aunn > 0.0 ) | ( denom > 0.0 ) ] = aunn * wwn / sdenom if deg : return np . degrees ( xi ) , np . degrees ( eta ) else : return xi , eta
1956	def _execve ( self , program , argv , envp ) : argv = [ ] if argv is None else argv envp = [ ] if envp is None else envp logger . debug ( f"Loading {program} as a {self.arch} elf" ) self . load ( program , envp ) self . _arch_specific_init ( ) self . _stack_top = self . current . STACK self . setup_stack ( [ program ] + argv , envp ) nprocs = len ( self . procs ) nfiles = len ( self . files ) assert nprocs > 0 self . running = list ( range ( nprocs ) ) self . timers = [ None ] * nprocs self . rwait = [ set ( ) for _ in range ( nfiles ) ] self . twait = [ set ( ) for _ in range ( nfiles ) ] for proc in self . procs : self . forward_events_from ( proc )
1221	def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor
12411	def close ( self ) : self . require_not_closed ( ) if not self . streaming or self . asynchronous : if 'Content-Length' not in self . headers : self . headers [ 'Content-Length' ] = self . tell ( ) self . flush ( ) self . _closed = True
13166	def parse_query ( query ) : parts = query . split ( '/' ) norm = [ ] for p in parts : p = p . strip ( ) if p : norm . append ( p ) elif '' not in norm : norm . append ( '' ) return norm
4516	def bresenham_line ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : md . bresenham_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc )
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
5022	def on_init ( app ) : docs_path = os . path . abspath ( os . path . dirname ( __file__ ) ) root_path = os . path . abspath ( os . path . join ( docs_path , '..' ) ) apidoc_path = 'sphinx-apidoc' if hasattr ( sys , 'real_prefix' ) : bin_path = os . path . abspath ( os . path . join ( sys . prefix , 'bin' ) ) apidoc_path = os . path . join ( bin_path , apidoc_path ) check_call ( [ apidoc_path , '-o' , docs_path , os . path . join ( root_path , 'enterprise' ) , os . path . join ( root_path , 'enterprise/migrations' ) ] )
1613	def IsErrorSuppressedByNolint ( category , linenum ) : return ( _global_error_suppressions . get ( category , False ) or linenum in _error_suppressions . get ( category , set ( ) ) or linenum in _error_suppressions . get ( None , set ( ) ) )
3592	def encryptPassword ( self , login , passwd ) : binaryKey = b64decode ( config . GOOGLE_PUBKEY ) i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
1488	def save_module ( self , obj ) : self . modules . add ( obj ) self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False copy_logs_cmd = textwrap . dedent ( ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) body = textwrap . dedent ( ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
829	def encodedBitDescription ( self , bitOffset , formatted = False ) : ( prevFieldName , prevFieldOffset ) = ( None , None ) description = self . getDescription ( ) for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if formatted : offset = offset + i if bitOffset == offset - 1 : prevFieldName = "separator" prevFieldOffset = bitOffset break if bitOffset < offset : break ( prevFieldName , prevFieldOffset ) = ( name , offset ) width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) if prevFieldOffset is None or bitOffset > self . getWidth ( ) : raise IndexError ( "Bit is outside of allowable range: [0 - %d]" % width ) return ( prevFieldName , bitOffset - prevFieldOffset )
9113	def message ( self ) : try : with open ( join ( self . fs_path , u'message' ) ) as message_file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message_file . readlines ( ) ] ) except IOError : return u''
6592	def poll ( self ) : self . runid_to_return . extend ( self . dispatcher . poll ( ) ) ret = self . _collect_all_finished_pkgidx_result_pairs ( ) return ret
956	def getArgumentDescriptions ( f ) : argspec = inspect . getargspec ( f ) docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue indentLevel = lines [ i ] . index ( stripped [ 0 ] ) firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : break argLines . append ( lines [ i ] . strip ( ) ) i += 1 descriptions [ argName ] = ' ' . join ( argLines ) else : i += 1 args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , "" ) ) ) return args
8270	def _xml ( self ) : grouped = self . _weight_by_hue ( ) xml = "<colors query=\"" + self . name + "\" tags=\"" + ", " . join ( self . tags ) + "\">\n\n" for total_weight , normalized_weight , hue , ranges in grouped : if hue == self . blue : hue = "blue" clr = color ( hue ) xml += "\t<color name=\"" + clr . name + "\" weight=\"" + str ( normalized_weight ) + "\">\n " xml += "\t\t<rgb r=\"" + str ( clr . r ) + "\" g=\"" + str ( clr . g ) + "\" " xml += "b=\"" + str ( clr . b ) + "\" a=\"" + str ( clr . a ) + "\" />\n " for clr , rng , wgt in ranges : xml += "\t\t<shade name=\"" + str ( rng ) + "\" weight=\"" + str ( wgt / total_weight ) + "\" />\n " xml = xml . rstrip ( " " ) + "\t</color>\n\n" xml += "</colors>" return xml
7837	def remove ( self ) : if self . disco is None : return self . xmlnode . unlinkNode ( ) oldns = self . xmlnode . ns ( ) ns = self . xmlnode . newNs ( oldns . getContent ( ) , None ) self . xmlnode . replaceNs ( oldns , ns ) common_root . addChild ( self . xmlnode ( ) ) self . disco = None
9006	def to_svg ( self , converter = None ) : if converter is None : from knittingpattern . convert . InstructionSVGCache import default_svg_cache converter = default_svg_cache ( ) return converter . to_svg ( self )
5825	def _patch ( self , route , data , headers = None , failure_message = None ) : headers = self . _get_headers ( headers ) response_lambda = ( lambda : requests . patch ( self . _get_qualified_route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check_for_rate_limiting ( response_lambda ( ) , response_lambda ) return self . _handle_response ( response , failure_message )
10655	def run ( self , clock ) : if clock . timestep_ix >= self . period_count : return for c in self . components : c . run ( clock , self . gl ) self . _perform_year_end_procedure ( clock )
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now return time_delta . days * 86400 + time_delta . seconds else : return 0
6424	def sim ( self , src , tar , qval = 2 ) : r return super ( self . __class__ , self ) . sim ( src , tar , qval , 1 , 1 )
7103	def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
8835	def less_or_equal ( a , b , * args ) : return ( less ( a , b ) or soft_equals ( a , b ) ) and ( not args or less_or_equal ( b , * args ) )
11158	def mirror_to ( self , dst ) : self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
10160	def ci ( ctx ) : opts = [ '' ] if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
9153	def get_exif_info ( self ) : _dict = { } for tag in _EXIF_TAGS : ret = self . img . attribute ( "EXIF:%s" % tag ) if ret and ret != 'unknown' : _dict [ tag ] = ret return _dict
9186	def get_moderation ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
6732	def add_class_methods_as_module_level_functions_for_fabric ( instance , module_name , method_name , module_alias = None ) : import imp from . decorators import task_or_dryrun module_obj = sys . modules [ module_name ] module_alias = re . sub ( '[^a-zA-Z0-9]+' , '' , module_alias or '' ) method_obj = getattr ( instance , method_name ) if not method_name . startswith ( '_' ) : func = getattr ( instance , method_name ) if not hasattr ( func , 'is_task_or_dryrun' ) : func = task_or_dryrun ( func ) if module_name == module_alias or ( module_name . startswith ( 'satchels.' ) and module_name . endswith ( module_alias ) ) : setattr ( module_obj , method_name , func ) else : _module_obj = module_obj module_obj = create_module ( module_alias ) setattr ( module_obj , method_name , func ) post_import_modules . add ( module_alias ) fabric_name = '%s.%s' % ( module_alias or module_name , method_name ) func . wrapped . __func__ . fabric_name = fabric_name return func
12830	def set_data ( self , data = { } , datetime_fields = [ ] ) : if datetime_fields : for field in datetime_fields : if field in data : data [ field ] = self . _parse_datetime ( data [ field ] ) super ( CampfireEntity , self ) . set_data ( data )
4313	def silent ( input_filepath , threshold = 0.001 ) : validate_input_file ( input_filepath ) stat_dictionary = stat ( input_filepath ) mean_norm = stat_dictionary [ 'Mean norm' ] if mean_norm is not float ( 'nan' ) : if mean_norm >= threshold : return False else : return True else : return True
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
1973	def sys_openat ( self , dirfd , buf , flags , mode ) : if issymbolic ( dirfd ) : logger . debug ( "Ask to read from a symbolic directory file descriptor!!" ) self . constraints . add ( dirfd >= 0 ) self . constraints . add ( dirfd <= len ( self . files ) ) raise ConcretizeArgument ( self , 0 ) if issymbolic ( buf ) : logger . debug ( "Ask to read to a symbolic buffer" ) raise ConcretizeArgument ( self , 1 ) return super ( ) . sys_openat ( dirfd , buf , flags , mode )
2453	def set_pkg_verif_code ( self , doc , code ) : self . assert_package_exists ( ) if not self . package_verif_set : self . package_verif_set = True match = self . VERIF_CODE_REGEX . match ( code ) if match : doc . package . verif_code = match . group ( self . VERIF_CODE_CODE_GRP ) if match . group ( self . VERIF_CODE_EXC_FILES_GRP ) is not None : doc . package . verif_exc_files = match . group ( self . VERIF_CODE_EXC_FILES_GRP ) . split ( ',' ) return True else : raise SPDXValueError ( 'Package::VerificationCode' ) else : raise CardinalityError ( 'Package::VerificationCode' )
12120	def generate_colormap ( self , colormap = None , reverse = False ) : if colormap is None : colormap = pylab . cm . Dark2 self . cm = colormap self . colormap = [ ] for i in range ( self . sweeps ) : self . colormap . append ( colormap ( i / self . sweeps ) ) if reverse : self . colormap . reverse ( )
5235	def filter_by_dates ( files_or_folders : list , date_fmt = DATE_FMT ) -> list : r = re . compile ( f'.*{date_fmt}.*' ) return list ( filter ( lambda vv : r . match ( vv . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] ) is not None , files_or_folders , ) )
6544	def terminate ( self ) : if not self . is_terminated : log . debug ( "terminal client terminated" ) try : self . exec_command ( b"Quit" ) except BrokenPipeError : pass except socket . error as e : if e . errno != errno . ECONNRESET : raise self . app . close ( ) self . is_terminated = True
10349	def lint_file ( in_file , out_file = None ) : for line in in_file : print ( line . strip ( ) , file = out_file )
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
1235	def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent
6682	def remove ( self , path , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )
4984	def extend_course ( course , enterprise_customer , request ) : course_run_id = course [ 'course_runs' ] [ 0 ] [ 'key' ] try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : error_code = 'ENTPEV000' LOGGER . error ( 'CourseCatalogApiServiceClient is improperly configured. ' 'Returned error code {error_code} to user {userid} ' 'and enterprise_customer {enterprise_customer} ' 'for course_run_id {course_run_id}' . format ( error_code = error_code , userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) course_details , course_run_details = catalog_api_client . get_course_and_course_run ( course_run_id ) if not course_details or not course_run_details : error_code = 'ENTPEV001' LOGGER . error ( 'User {userid} of enterprise customer {enterprise_customer} encountered an error.' 'No course_details or course_run_details found for ' 'course_run_id {course_run_id}. ' 'The following error code reported to the user: {error_code}' . format ( userid = request . user . id , enterprise_customer = enterprise_customer . uuid , course_run_id = course_run_id , error_code = error_code , ) ) messages . add_generic_error_message_with_code ( request , error_code ) return ( { } , error_code ) weeks_to_complete = course_run_details [ 'weeks_to_complete' ] course_run_image = course_run_details [ 'image' ] or { } course . update ( { 'course_image_uri' : course_run_image . get ( 'src' , '' ) , 'course_title' : course_run_details [ 'title' ] , 'course_level_type' : course_run_details . get ( 'level_type' , '' ) , 'course_short_description' : course_run_details [ 'short_description' ] or '' , 'course_full_description' : clean_html_for_template_rendering ( course_run_details [ 'full_description' ] or '' ) , 'expected_learning_items' : course_details . get ( 'expected_learning_items' , [ ] ) , 'staff' : course_run_details . get ( 'staff' , [ ] ) , 'course_effort' : ungettext_min_max ( '{} hour per week' , '{} hours per week' , '{}-{} hours per week' , course_run_details [ 'min_effort' ] or None , course_run_details [ 'max_effort' ] or None , ) or '' , 'weeks_to_complete' : ungettext ( '{} week' , '{} weeks' , weeks_to_complete ) . format ( weeks_to_complete ) if weeks_to_complete else '' , } ) return course , None
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
8389	def write ( self , text , hashline = b"# {}" ) : u if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
9763	def upload ( sync = True ) : project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
10674	def load_data_auxi ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.json' ) ) for file in files : compound = Compound . read ( file ) compounds [ compound . formula ] = compound
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
12429	def create_virtualenv ( self ) : if check_command ( 'virtualenv' ) : ve_dir = os . path . join ( self . _ve_dir , self . _project_name ) if os . path . exists ( ve_dir ) : if self . _force : logging . warn ( 'Removing existing virtualenv' ) shutil . rmtree ( ve_dir ) else : logging . warn ( 'Found existing virtualenv; not creating (use --force to overwrite)' ) return logging . info ( 'Creating virtualenv' ) p = subprocess . Popen ( 'virtualenv --no-site-packages {0} > /dev/null' . format ( ve_dir ) , shell = True ) os . waitpid ( p . pid , 0 ) for m in self . _modules : self . log . info ( 'Installing module {0}' . format ( m ) ) p = subprocess . Popen ( '{0} install {1} > /dev/null' . format ( os . path . join ( self . _ve_dir , self . _project_name ) + os . sep + 'bin' + os . sep + 'pip' , m ) , shell = True ) os . waitpid ( p . pid , 0 )
2593	def get_last_checkpoint ( rundir = "runinfo" ) : if not os . path . isdir ( rundir ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) if len ( dirs ) == 0 : return [ ] last_runid = dirs [ - 1 ] last_checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , last_runid ) ) if ( not ( os . path . isdir ( last_checkpoint ) ) ) : return [ ] return [ last_checkpoint ]
1974	def sys_allocate ( self , cpu , length , isX , addr ) : if addr not in cpu . memory : logger . info ( "ALLOCATE: addr points to invalid address. Returning EFAULT" ) return Decree . CGC_EFAULT perms = [ 'rw ' , 'rwx' ] [ bool ( isX ) ] try : result = cpu . memory . mmap ( None , length , perms ) except Exception as e : logger . info ( "ALLOCATE exception %s. Returning ENOMEM %r" , str ( e ) , length ) return Decree . CGC_ENOMEM cpu . write_int ( addr , result , 32 ) logger . info ( "ALLOCATE(%d, %s, 0x%08x) -> 0x%08x" % ( length , perms , addr , result ) ) self . syscall_trace . append ( ( "_allocate" , - 1 , length ) ) return 0
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
3780	def load_all_methods ( self ) : r methods = [ ] Tmins , Tmaxs = [ ] , [ ] if self . CASRN in [ '7732-18-5' , '67-56-1' , '64-17-5' ] : methods . append ( TEST_METHOD_1 ) self . TEST_METHOD_1_Tmin = 200. self . TEST_METHOD_1_Tmax = 350 self . TEST_METHOD_1_coeffs = [ 1 , .002 ] Tmins . append ( self . TEST_METHOD_1_Tmin ) Tmaxs . append ( self . TEST_METHOD_1_Tmax ) if self . CASRN in [ '67-56-1' ] : methods . append ( TEST_METHOD_2 ) self . TEST_METHOD_2_Tmin = 300. self . TEST_METHOD_2_Tmax = 400 self . TEST_METHOD_2_coeffs = [ 1 , .003 ] Tmins . append ( self . TEST_METHOD_2_Tmin ) Tmaxs . append ( self . TEST_METHOD_2_Tmax ) self . all_methods = set ( methods ) if Tmins and Tmaxs : self . Tmin = min ( Tmins ) self . Tmax = max ( Tmaxs )
4177	def window_cosine ( N ) : r if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) win = sin ( pi * n / ( N - 1. ) ) return win
11551	def get_exception_from_status_and_error_codes ( status_code , error_code , value ) : if status_code == requests . codes . bad_request : exception = BadRequest ( value ) elif status_code == requests . codes . unauthorized : exception = Unauthorized ( value ) elif status_code == requests . codes . forbidden : exception = Unauthorized ( value ) elif status_code in [ requests . codes . not_found , requests . codes . gone ] : exception = NotFound ( value ) elif status_code == requests . codes . method_not_allowed : exception = MethodNotAllowed ( value ) elif status_code >= requests . codes . bad_request : exception = HTTPError ( value ) else : exception = ResponseError ( value ) if error_code == - 100 : exception = InternalError ( value ) elif error_code == - 101 : exception = InvalidToken ( value ) elif error_code == - 105 : exception = UploadFailed ( value ) elif error_code == - 140 : exception = UploadTokenGenerationFailed ( value ) elif error_code == - 141 : exception = InvalidUploadToken ( value ) elif error_code == - 150 : exception = InvalidParameter ( value ) elif error_code == - 151 : exception = InvalidPolicy ( value ) return exception
8266	def _cache ( self ) : n = self . steps if len ( self . _colors ) == 1 : ColorList . __init__ ( self , [ self . _colors [ 0 ] for i in _range ( n ) ] ) return colors = self . _interpolate ( self . _colors , 40 ) left = colors [ : len ( colors ) / 2 ] right = colors [ len ( colors ) / 2 : ] left . append ( right [ 0 ] ) right . insert ( 0 , left [ - 1 ] ) gradient = self . _interpolate ( left , int ( n * self . spread ) ) [ : - 1 ] gradient . extend ( self . _interpolate ( right , n - int ( n * self . spread ) ) [ 1 : ] ) if self . spread > 1 : gradient = gradient [ : n ] if self . spread < 0 : gradient = gradient [ - n : ] ColorList . __init__ ( self , gradient )
12072	def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group_ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group_ind ) ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . __class__ . __name__ )
4390	def adsSyncReadStateReqEx ( port , address ) : sync_read_state_request = _adsDLL . AdsSyncReadStateReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state = ctypes . c_int ( ) ads_state_pointer = ctypes . pointer ( ads_state ) device_state = ctypes . c_int ( ) device_state_pointer = ctypes . pointer ( device_state ) error_code = sync_read_state_request ( port , ams_address_pointer , ads_state_pointer , device_state_pointer ) if error_code : raise ADSError ( error_code ) return ( ads_state . value , device_state . value )
1051	def format_exception_only ( etype , value ) : if ( isinstance ( etype , BaseException ) or etype is None or type ( etype ) is str ) : return [ _format_final_exc_line ( etype , value ) ] stype = etype . __name__ if not issubclass ( etype , SyntaxError ) : return [ _format_final_exc_line ( stype , value ) ] lines = [ ] try : msg , ( filename , lineno , offset , badline ) = value . args except Exception : pass else : filename = filename or "<string>" lines . append ( ' File "%s", line %d\n' % ( filename , lineno ) ) if badline is not None : lines . append ( ' %s\n' % badline . strip ( ) ) if offset is not None : caretspace = badline . rstrip ( '\n' ) offset = min ( len ( caretspace ) , offset ) - 1 caretspace = caretspace [ : offset ] . lstrip ( ) caretspace = ( ( c . isspace ( ) and c or ' ' ) for c in caretspace ) lines . append ( ' %s^\n' % '' . join ( caretspace ) ) value = msg lines . append ( _format_final_exc_line ( stype , value ) ) return lines
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( ** jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
10603	def calculate ( self , ** state ) : T = state [ 'T' ] y_C = state [ 'y_C' ] y_H = state [ 'y_H' ] y_O = state [ 'y_O' ] y_N = state [ 'y_N' ] y_S = state [ 'y_S' ] a = self . _calc_a ( y_C , y_H , y_O , y_N , y_S ) / 1000 result = ( R / a ) * ( 380 * self . _calc_g0 ( 380 / T ) + 3600 * self . _calc_g0 ( 1800 / T ) ) return result
743	def writeToFile ( self , f , packed = True ) : schema = self . getSchema ( ) proto = schema . new_message ( ) self . write ( proto ) if packed : proto . write_packed ( f ) else : proto . write ( f )
3197	def pause ( self , workflow_id , email_id ) : self . workflow_id = workflow_id self . email_id = email_id return self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'actions/pause' ) )
5465	def _get_action_by_name ( op , name ) : actions = get_actions ( op ) for action in actions : if action . get ( 'name' ) == name : return action
9629	def split_docstring ( value ) : docstring = textwrap . dedent ( getattr ( value , '__doc__' , '' ) ) if not docstring : return None pieces = docstring . strip ( ) . split ( '\n\n' , 1 ) try : body = pieces [ 1 ] except IndexError : body = None return Docstring ( pieces [ 0 ] , body )
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
9125	def _store_helper ( model : Action , session : Optional [ Session ] = None ) -> None : if session is None : session = _make_session ( ) session . add ( model ) session . commit ( ) session . close ( )
10666	def stoichiometry_coefficients ( compound , elements ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return [ stoichiometry [ element ] for element in elements ]
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
11098	def select_by_size ( self , min_size = 0 , max_size = 1 << 40 , recursive = True ) : def filters ( p ) : return min_size <= p . size <= max_size return self . select_file ( filters , recursive )
3854	def add_color_to_scheme ( scheme , name , foreground , background , palette_colors ) : if foreground is None and background is None : return scheme new_scheme = [ ] for item in scheme : if item [ 0 ] == name : if foreground is None : foreground = item [ 1 ] if background is None : background = item [ 2 ] if palette_colors > 16 : new_scheme . append ( ( name , '' , '' , '' , foreground , background ) ) else : new_scheme . append ( ( name , foreground , background ) ) else : new_scheme . append ( item ) return new_scheme
11892	def update ( self ) : bulbs = self . _hub . get_lights ( ) if not bulbs : _LOGGER . debug ( "%s is offline, send command failed" , self . _zid ) self . _online = False
13753	def prepare_path ( path ) : if type ( path ) == list : return os . path . join ( * path ) return path
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
10162	def setup ( app ) : lexer = MarkdownLexer ( ) for alias in lexer . aliases : app . add_lexer ( alias , lexer ) return dict ( version = __version__ )
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
13554	def create_shift ( self , params = { } ) : url = "/2/shifts/" body = params data = self . _post_resource ( url , body ) shift = self . shift_from_json ( data [ "shift" ] ) return shift
5415	def parse_args ( parser , provider_required_args , argv ) : epilog = 'Provider-required arguments:\n' for provider in provider_required_args : epilog += ' %s: %s\n' % ( provider , provider_required_args [ provider ] ) parser . epilog = epilog args = parser . parse_args ( argv ) for arg in provider_required_args [ args . provider ] : if not args . __getattribute__ ( arg ) : parser . error ( 'argument --%s is required' % arg ) return args
2895	def _on_complete_hook ( self , my_task ) : outputs = [ ] for condition , output in self . cond_task_specs : if self . choice is not None and output not in self . choice : continue if condition is None : outputs . append ( self . _wf_spec . get_task_spec_from_name ( output ) ) continue if not condition . _matches ( my_task ) : continue outputs . append ( self . _wf_spec . get_task_spec_from_name ( output ) ) my_task . _sync_children ( outputs , Task . FUTURE ) for child in my_task . children : child . task_spec . _update ( child )
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
12727	def stop_erps ( self , stop_erps ) : _set_params ( self . ode_obj , 'StopERP' , stop_erps , self . ADOF + self . LDOF )
13215	def dump ( self , name , filename ) : if not self . exists ( name ) : raise DatabaseError ( 'database %s does not exist!' ) log . info ( 'dumping %s to %s' % ( name , filename ) ) self . _run_cmd ( 'pg_dump' , '--verbose' , '--blobs' , '--format=custom' , '--file=%s' % filename , name )
12730	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis1 ( ) ) , np . array ( self . ode_obj . getAxis2 ( ) ) ]
2283	def check_R_package ( self , package ) : test_package = not bool ( launch_R_script ( "{}/R_templates/test_import.R" . format ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) , { "{package}" : package } , verbose = True ) ) return test_package
12643	def set_config_value ( name , value ) : cli_config = CLIConfig ( SF_CLI_CONFIG_DIR , SF_CLI_ENV_VAR_PREFIX ) cli_config . set_value ( 'servicefabric' , name , value )
11134	def is_not_exist_or_allow_overwrite ( self , overwrite = False ) : if self . exists ( ) and overwrite is False : return False else : return True
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
7779	def __from_rfc2426 ( self , data ) : data = from_utf8 ( data ) lines = data . split ( "\n" ) started = 0 current = None for l in lines : if not l : continue if l [ - 1 ] == "\r" : l = l [ : - 1 ] if not l : continue if l [ 0 ] in " \t" : if current is None : continue current += l [ 1 : ] continue if not started and current and current . upper ( ) . strip ( ) == "BEGIN:VCARD" : started = 1 elif started and current . upper ( ) . strip ( ) == "END:VCARD" : current = None break elif current and started : self . _process_rfc2425_record ( current ) current = l if started and current : self . _process_rfc2425_record ( current )
11417	def record_modify_controlfield ( rec , tag , controlfield_value , field_position_global = None , field_position_local = None ) : field = record_get_field ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) new_field = ( field [ 0 ] , field [ 1 ] , field [ 2 ] , controlfield_value , field [ 4 ] ) record_replace_field ( rec , tag , new_field , field_position_global = field_position_global , field_position_local = field_position_local )
9322	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : self . refresh_information ( accept ) self . refresh_collections ( accept )
9736	def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
9798	def update ( ctx , name , description , tags ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the experiment group.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . experiment_group . update_experiment_group ( user , project_name , _group , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update experiment group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiment group updated." ) get_group_details ( response )
8099	def copy ( self , graph ) : s = styles ( graph ) s . guide = self . guide . copy ( graph ) dict . __init__ ( s , [ ( v . name , v . copy ( ) ) for v in self . values ( ) ] ) return s
3362	def to_yaml ( model , sort = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ "version" ] = YAML_SPEC return yaml . dump ( obj , ** kwargs )
8062	def do_help ( self , arg ) : print ( self . response_prompt , file = self . stdout ) return cmd . Cmd . do_help ( self , arg )
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , ** saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
4946	def send_course_enrollment_statement ( lrs_configuration , course_enrollment ) : user_details = LearnerInfoSerializer ( course_enrollment . user ) course_details = CourseInfoSerializer ( course_enrollment . course ) statement = LearnerCourseEnrollmentStatement ( course_enrollment . user , course_enrollment . course , user_details . data , course_details . data , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
2275	def _win32_is_hardlinked ( fpath1 , fpath2 ) : def get_read_handle ( fpath ) : if os . path . isdir ( fpath ) : dwFlagsAndAttributes = jwfs . api . FILE_FLAG_BACKUP_SEMANTICS else : dwFlagsAndAttributes = 0 hFile = jwfs . api . CreateFile ( fpath , jwfs . api . GENERIC_READ , jwfs . api . FILE_SHARE_READ , None , jwfs . api . OPEN_EXISTING , dwFlagsAndAttributes , None ) return hFile def get_unique_id ( hFile ) : info = jwfs . api . BY_HANDLE_FILE_INFORMATION ( ) res = jwfs . api . GetFileInformationByHandle ( hFile , info ) jwfs . handle_nonzero_success ( res ) unique_id = ( info . volume_serial_number , info . file_index_high , info . file_index_low ) return unique_id hFile1 = get_read_handle ( fpath1 ) hFile2 = get_read_handle ( fpath2 ) try : are_equal = ( get_unique_id ( hFile1 ) == get_unique_id ( hFile2 ) ) except Exception : raise finally : jwfs . api . CloseHandle ( hFile1 ) jwfs . api . CloseHandle ( hFile2 ) return are_equal
5729	def main ( verbose = True ) : find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) gdbmi = GdbController ( verbose = verbose ) responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) responses = gdbmi . write ( "-file-list-exec-source-files" ) responses = gdbmi . write ( "-break-insert main" ) responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) gdbmi . exit ( )
7524	def _collapse_outgroup ( tree , taxdicts ) : outg = taxdicts [ 0 ] [ "p4" ] if not all ( [ i [ "p4" ] == outg for i in taxdicts ] ) : raise Exception ( "no good" ) tre = ete . Tree ( tree . write ( format = 1 ) ) alltax = [ i for i in tre . get_leaf_names ( ) if i not in outg ] alltax += [ outg [ 0 ] ] tre . prune ( alltax ) tre . search_nodes ( name = outg [ 0 ] ) [ 0 ] . name = "outgroup" tre . ladderize ( ) taxd = copy . deepcopy ( taxdicts ) newtaxdicts = [ ] for test in taxd : test [ "p4" ] = [ "outgroup" ] newtaxdicts . append ( test ) return tre , newtaxdicts
4111	def rc2poly ( kr , r0 = None ) : from . levinson import levup p = len ( kr ) a = numpy . array ( [ 1 , kr [ 0 ] ] ) e = numpy . zeros ( len ( kr ) ) if r0 is None : e0 = 0 else : e0 = r0 e [ 0 ] = e0 * ( 1. - numpy . conj ( numpy . conjugate ( kr [ 0 ] ) * kr [ 0 ] ) ) for k in range ( 1 , p ) : [ a , e [ k ] ] = levup ( a , kr [ k ] , e [ k - 1 ] ) efinal = e [ - 1 ] return a , efinal
1633	def CheckForNewlineAtEOF ( filename , lines , error ) : if len ( lines ) < 3 or lines [ - 2 ] : error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 'Could not find a newline character at the end of the file.' )
6210	def print_children ( data_file , group = '/' ) : base = data_file . get_node ( group ) print ( 'Groups in:\n %s\n' % base ) for node in base . _f_walk_groups ( ) : if node is not base : print ( ' %s' % node ) print ( '\nLeaf-nodes in %s:' % group ) for node in base . _v_leaves . itervalues ( ) : info = node . shape if len ( info ) == 0 : info = node . read ( ) print ( '\t%s, %s' % ( node . name , info ) ) if len ( node . title ) > 0 : print ( '\t %s' % node . title )
2142	def process_extra_vars ( extra_vars_list , force_json = True ) : extra_vars = { } extra_vars_yaml = "" for extra_vars_opt in extra_vars_list : if extra_vars_opt . startswith ( "@" ) : with open ( extra_vars_opt [ 1 : ] , 'r' ) as f : extra_vars_opt = f . read ( ) opt_dict = string_to_dict ( extra_vars_opt , allow_kv = False ) else : opt_dict = string_to_dict ( extra_vars_opt , allow_kv = True ) if any ( line . startswith ( "#" ) for line in extra_vars_opt . split ( '\n' ) ) : extra_vars_yaml += extra_vars_opt + "\n" elif extra_vars_opt != "" : extra_vars_yaml += yaml . dump ( opt_dict , default_flow_style = False ) + "\n" extra_vars . update ( opt_dict ) if not force_json : try : try_dict = yaml . load ( extra_vars_yaml , Loader = yaml . SafeLoader ) assert type ( try_dict ) is dict debug . log ( 'Using unprocessed YAML' , header = 'decision' , nl = 2 ) return extra_vars_yaml . rstrip ( ) except Exception : debug . log ( 'Failed YAML parsing, defaulting to JSON' , header = 'decison' , nl = 2 ) if extra_vars == { } : return "" return json . dumps ( extra_vars , ensure_ascii = False )
9827	def download ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : PolyaxonClient ( ) . project . download_repo ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not download code for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( 'Files downloaded.' )
378	def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( "Unsupported channels %d" % x . shape [ channel_index ] )
7823	def _final_challenge ( self , challenge ) : if self . _finished : return Failure ( "extra-challenge" ) match = SERVER_FINAL_MESSAGE_RE . match ( challenge ) if not match : logger . debug ( "Bad final message syntax: {0!r}" . format ( challenge ) ) return Failure ( "bad-challenge" ) error = match . group ( "error" ) if error : logger . debug ( "Server returned SCRAM error: {0!r}" . format ( error ) ) return Failure ( u"scram-" + error . decode ( "utf-8" ) ) verifier = match . group ( "verifier" ) if not verifier : logger . debug ( "No verifier value in the final message" ) return Failure ( "bad-succes" ) server_key = self . HMAC ( self . _salted_password , b"Server Key" ) server_signature = self . HMAC ( server_key , self . _auth_message ) if server_signature != a2b_base64 ( verifier ) : logger . debug ( "Server verifier does not match" ) return Failure ( "bad-succes" ) self . _finished = True return Response ( None )
2452	def set_pkg_home ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_home_set : self . package_home_set = True if validations . validate_pkg_homepage ( location ) : doc . package . homepage = location return True else : raise SPDXValueError ( 'Package::HomePage' ) else : raise CardinalityError ( 'Package::HomePage' )
8881	def fit ( self , X , y = None ) : X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
77	def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj
4584	def image_to_colorlist ( image , container = list ) : deprecated . deprecated ( 'util.gif.image_to_colorlist' ) return container ( convert_mode ( image ) . getdata ( ) )
10959	def set_mem_level ( self , mem_level = 'hi' ) : key = '' . join ( [ c if c in 'mlh' else '' for c in mem_level ] ) if key not in [ 'h' , 'mh' , 'm' , 'ml' , 'm' , 'l' ] : raise ValueError ( 'mem_level must be one of hi, med-hi, med, med-lo, lo.' ) mem_levels = { 'h' : [ np . float64 , np . float64 ] , 'mh' : [ np . float64 , np . float32 ] , 'm' : [ np . float32 , np . float32 ] , 'ml' : [ np . float32 , np . float16 ] , 'l' : [ np . float16 , np . float16 ] } hi_lvl , lo_lvl = mem_levels [ key ] cat_lvls = { 'obj' : lo_lvl , 'ilm' : hi_lvl , 'bkg' : lo_lvl } self . image . float_precision = hi_lvl self . image . image = self . image . image . astype ( lo_lvl ) self . set_image ( self . image ) for cat in cat_lvls . keys ( ) : obj = self . get ( cat ) if hasattr ( obj , 'comps' ) : for c in obj . comps : c . float_precision = lo_lvl else : obj . float_precision = lo_lvl self . _model = self . _model . astype ( hi_lvl ) self . _residuals = self . _model . astype ( hi_lvl ) self . reset ( )
9923	def save ( self ) : try : email = models . EmailAddress . objects . get ( email = self . validated_data [ "email" ] , is_verified = False ) logger . debug ( "Resending verification email to %s" , self . validated_data [ "email" ] , ) email . send_confirmation ( ) except models . EmailAddress . DoesNotExist : logger . debug ( "Not resending verification email to %s because the address " "doesn't exist in the database." , self . validated_data [ "email" ] , )
11001	def _kpad ( self , field , finalshape , zpad = False , norm = True ) : currshape = np . array ( field . shape ) if any ( finalshape < currshape ) : raise IndexError ( "PSF tile size is less than minimum support size" ) d = finalshape - currshape o = d % 2 d = np . floor_divide ( d , 2 ) if not zpad : o [ 0 ] = 0 axes = None pad = tuple ( ( d [ i ] + o [ i ] , d [ i ] ) for i in [ 0 , 1 , 2 ] ) rpsf = np . pad ( field , pad , mode = 'constant' , constant_values = 0 ) rpsf = np . fft . ifftshift ( rpsf , axes = axes ) kpsf = fft . rfftn ( rpsf , ** fftkwargs ) if norm : kpsf /= kpsf [ 0 , 0 , 0 ] return kpsf
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
13906	def apply_defaults ( self , commands ) : for command in commands : if 'action' in command and "()" in command [ 'action' ] : command [ 'action' ] = eval ( "self.{}" . format ( command [ 'action' ] ) ) if command [ 'keys' ] [ 0 ] . startswith ( '-' ) : if 'required' not in command : command [ 'required' ] = False
6485	def do_search ( request , course_id = None ) : SearchInitializer . set_search_enviroment ( request = request , course_id = course_id ) results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : if not search_term : raise ValueError ( _ ( 'No search term provided for search' ) ) size , from_ , page = _process_pagination_values ( request ) track . emit ( 'edx.course.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = perform_search ( search_term , user = request . user , size = size , from_ = from_ , course_id = course_id ) status_code = 200 track . emit ( 'edx.course.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
10839	def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post_data = "text=%s&" % text if now : post_data += "now=%s&" % now if utc : post_data += "utc=%s&" % utc if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) return Update ( api = self . api , raw_response = response [ 'update' ] )
5712	def retrieve_descriptor ( descriptor ) : the_descriptor = descriptor if the_descriptor is None : the_descriptor = { } if isinstance ( the_descriptor , six . string_types ) : try : if os . path . isfile ( the_descriptor ) : with open ( the_descriptor , 'r' ) as f : the_descriptor = json . load ( f ) else : req = requests . get ( the_descriptor ) req . raise_for_status ( ) req . encoding = 'utf8' the_descriptor = req . json ( ) except ( IOError , requests . exceptions . RequestException ) as error : message = 'Unable to load JSON at "%s"' % descriptor six . raise_from ( exceptions . DataPackageException ( message ) , error ) except ValueError as error : message = 'Unable to parse JSON at "%s". %s' % ( descriptor , error ) six . raise_from ( exceptions . DataPackageException ( message ) , error ) if hasattr ( the_descriptor , 'read' ) : try : the_descriptor = json . load ( the_descriptor ) except ValueError as e : six . raise_from ( exceptions . DataPackageException ( str ( e ) ) , e ) if not isinstance ( the_descriptor , dict ) : msg = 'Data must be a \'dict\', but was a \'{0}\'' raise exceptions . DataPackageException ( msg . format ( type ( the_descriptor ) . __name__ ) ) return the_descriptor
4498	def guid ( self , guid ) : return self . _json ( self . _get ( self . _build_url ( 'guids' , guid ) ) , 200 ) [ 'data' ] [ 'type' ]
9515	def to_Fastq ( self , qual_scores ) : if len ( self ) != len ( qual_scores ) : raise Error ( 'Error making Fastq from Fasta, lengths differ.' , self . id ) return Fastq ( self . id , self . seq , '' . join ( [ chr ( max ( 0 , min ( x , 93 ) ) + 33 ) for x in qual_scores ] ) )
190	def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )
12245	def get_all_buckets ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : buckets = super ( S3Connection , self ) . get_all_buckets ( * args , ** kwargs ) for bucket in buckets : mimicdb . backend . sadd ( tpl . connection , bucket . name ) return buckets return [ Bucket ( self , bucket ) for bucket in mimicdb . backend . smembers ( tpl . connection ) ]
7671	def save ( self , path_or_file , strict = True , fmt = 'auto' ) : self . validate ( strict = strict ) with _open ( path_or_file , mode = 'w' , fmt = fmt ) as fdesc : json . dump ( self . __json__ , fdesc , indent = 2 )
8651	def get_jobs ( session , job_ids , seo_details , lang ) : get_jobs_data = { 'jobs[]' : job_ids , 'seo_details' : seo_details , 'lang' : lang , } response = make_get_request ( session , 'jobs' , params_data = get_jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise JobsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
7267	def run ( self , * args , ** kw ) : log . debug ( '[operator] run "{}" with arguments: {}' . format ( self . __class__ . __name__ , args ) ) if self . kind == OperatorTypes . ATTRIBUTE : return self . match ( self . ctx ) else : return self . run_matcher ( * args , ** kw )
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
10636	def get_element_mfrs ( self , elements = None ) : if elements is None : elements = self . material . elements result = numpy . zeros ( len ( elements ) ) for compound in self . material . compounds : result += self . get_compound_mfr ( compound ) * stoich . element_mass_fractions ( compound , elements ) return result
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
11378	def get_publication_date ( self , xml_doc ) : start_date = get_value_in_tag ( xml_doc , "prism:coverDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , "prism:coverDisplayDate" ) if not start_date : start_date = get_value_in_tag ( xml_doc , 'oa:openAccessEffective' ) if start_date : start_date = datetime . datetime . strptime ( start_date , "%Y-%m-%dT%H:%M:%SZ" ) return start_date . strftime ( "%Y-%m-%d" ) import dateutil . parser start_date = re . sub ( '([A-Z][a-z]+)[\s\-][A-Z][a-z]+ (\d{4})' , r'\1 \2' , start_date ) try : date = dateutil . parser . parse ( start_date ) except ValueError : return '' if len ( start_date . split ( " " ) ) == 3 : return date . strftime ( "%Y-%m-%d" ) else : return date . strftime ( "%Y-%m" ) else : if len ( start_date ) is 8 : start_date = time . strftime ( '%Y-%m-%d' , time . strptime ( start_date , '%Y%m%d' ) ) elif len ( start_date ) is 6 : start_date = time . strftime ( '%Y-%m' , time . strptime ( start_date , '%Y%m' ) ) return start_date
6077	def hyper_noise_from_contributions ( self , noise_map , contributions ) : return self . noise_factor * ( noise_map * contributions ) ** self . noise_power
1436	def update_count ( self , name , incr_by = 1 , key = None ) : if name not in self . metrics : Log . error ( "In update_count(): %s is not registered in the metric" , name ) if key is None and isinstance ( self . metrics [ name ] , CountMetric ) : self . metrics [ name ] . incr ( incr_by ) elif key is not None and isinstance ( self . metrics [ name ] , MultiCountMetric ) : self . metrics [ name ] . incr ( key , incr_by ) else : Log . error ( "In update_count(): %s is registered but not supported with this method" , name )
13567	def linspacestep ( start , stop , step = 1 ) : numsteps = _np . int ( ( stop - start ) / step ) return _np . linspace ( start , start + step * numsteps , numsteps + 1 )
11314	def update_hidden_notes ( self ) : if not self . tag_as_cern : notes = record_get_field_instances ( self . record , tag = "595" ) for field in notes : for dummy , value in field [ 0 ] : if value == "CDS" : self . tag_as_cern = True record_delete_fields ( self . record , tag = "595" )
132	def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
9917	def validate ( self , data ) : user = self . _confirmation . email . user if ( app_settings . EMAIL_VERIFICATION_PASSWORD_REQUIRED and not user . check_password ( data [ "password" ] ) ) : raise serializers . ValidationError ( _ ( "The provided password is invalid." ) ) data [ "email" ] = self . _confirmation . email . email return data
5549	def get_hash ( x ) : if isinstance ( x , str ) : return hash ( x ) elif isinstance ( x , dict ) : return hash ( yaml . dump ( x ) )
4903	def link_to_modal ( link_text , index , autoescape = True ) : link = ( '<a' ' href="#!"' ' class="text-underline view-course-details-link"' ' id="view-course-details-link-{index}"' ' data-toggle="modal"' ' data-target="#course-details-modal-{index}"' '>{link_text}</a>' ) . format ( index = index , link_text = link_text , ) return mark_safe ( link )
5990	def constant_regularization_matrix_from_pixel_neighbors ( coefficients , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( pixel_neighbors ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_coefficient = coefficients [ 0 ] ** 2.0 for i in range ( pixels ) : regularization_matrix [ i , i ] += 1e-8 for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_coefficient regularization_matrix [ i , neighbor_index ] -= regularization_coefficient return regularization_matrix
12810	def connectionMade ( self ) : headers = [ "GET %s HTTP/1.1" % ( "/room/%s/live.json" % self . factory . get_stream ( ) . get_room_id ( ) ) ] connection_headers = self . factory . get_stream ( ) . get_connection ( ) . get_headers ( ) for header in connection_headers : headers . append ( "%s: %s" % ( header , connection_headers [ header ] ) ) headers . append ( "Host: streaming.campfirenow.com" ) self . transport . write ( "\r\n" . join ( headers ) + "\r\n\r\n" ) self . factory . get_stream ( ) . set_protocol ( self )
3235	def list_buckets ( client = None , ** kwargs ) : buckets = client . list_buckets ( ** kwargs ) return [ b . __dict__ for b in buckets ]
6299	def parse_package_string ( path ) : parts = path . split ( '.' ) if parts [ - 1 ] [ 0 ] . isupper ( ) : return "." . join ( parts [ : - 1 ] ) , parts [ - 1 ] return path , ""
9333	def full_like ( array , value , dtype = None ) : shared = empty_like ( array , dtype ) shared [ : ] = value return shared
8260	def cluster_sort ( self , cmp1 = "hue" , cmp2 = "brightness" , reversed = False , n = 12 ) : sorted = self . sort ( cmp1 ) clusters = ColorList ( ) d = 1.0 i = 0 for j in _range ( len ( sorted ) ) : if getattr ( sorted [ j ] , cmp1 ) < d : clusters . extend ( sorted [ i : j ] . sort ( cmp2 ) ) d -= 1.0 / n i = j clusters . extend ( sorted [ i : ] . sort ( cmp2 ) ) if reversed : _list . reverse ( clusters ) return clusters
11141	def load_repository ( self , path , verbose = True , ntrials = 3 ) : assert isinstance ( ntrials , int ) , "ntrials must be integer" assert ntrials > 0 , "ntrials must be >0" repo = None for _trial in range ( ntrials ) : try : self . __load_repository ( path = path , verbose = True ) except Exception as err1 : try : from . OldRepository import Repository REP = Repository ( path ) except Exception as err2 : error = "Unable to load repository using neiher new style (%s) nor old style (%s)" % ( err1 , err2 ) if self . DEBUG_PRINT_FAILED_TRIALS : print ( "Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute" % ( _trial , inspect . stack ( ) [ 1 ] [ 3 ] , str ( error ) ) ) else : error = None repo = REP break else : error = None repo = self break assert error is None , error return repo
10588	def _get_account_and_descendants_ ( self , account , result ) : result . append ( account ) for child in account . accounts : self . _get_account_and_descendants_ ( child , result )
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
6972	def _epd_residual2 ( coeffs , times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : f = _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) residual = mags - f return residual
10049	def create_error_handlers ( blueprint ) : blueprint . errorhandler ( PIDInvalidAction ) ( create_api_errorhandler ( status = 403 , message = 'Invalid action' ) ) records_rest_error_handlers ( blueprint )
7781	def rfc2426 ( self ) : ret = "begin:VCARD\r\n" ret += "version:3.0\r\n" for _unused , value in self . content . items ( ) : if value is None : continue if type ( value ) is list : for v in value : ret += v . rfc2426 ( ) else : v = value . rfc2426 ( ) ret += v return ret + "end:VCARD\r\n"
10227	def get_triangles ( graph : DiGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ a , b , c ] , key = str ) ) for a , b in graph . edges ( ) for c in graph . successors ( b ) if graph . has_edge ( c , a ) }
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
1462	def new_source ( self , source ) : source_streamlet = None if callable ( source ) : source_streamlet = SupplierStreamlet ( source ) elif isinstance ( source , Generator ) : source_streamlet = GeneratorStreamlet ( source ) else : raise RuntimeError ( "Builder's new source has to be either a Generator or a function" ) self . _sources . append ( source_streamlet ) return source_streamlet
10229	def summarize_stability ( graph : BELGraph ) -> Mapping [ str , int ] : regulatory_pairs = get_regulatory_pairs ( graph ) chaotic_pairs = get_chaotic_pairs ( graph ) dampened_pairs = get_dampened_pairs ( graph ) contraditory_pairs = get_contradiction_summary ( graph ) separately_unstable_triples = get_separate_unstable_correlation_triples ( graph ) mutually_unstable_triples = get_mutually_unstable_correlation_triples ( graph ) jens_unstable_triples = get_jens_unstable ( graph ) increase_mismatch_triples = get_increase_mismatch_triplets ( graph ) decrease_mismatch_triples = get_decrease_mismatch_triplets ( graph ) chaotic_triples = get_chaotic_triplets ( graph ) dampened_triples = get_dampened_triplets ( graph ) return { 'Regulatory Pairs' : _count_or_len ( regulatory_pairs ) , 'Chaotic Pairs' : _count_or_len ( chaotic_pairs ) , 'Dampened Pairs' : _count_or_len ( dampened_pairs ) , 'Contradictory Pairs' : _count_or_len ( contraditory_pairs ) , 'Separately Unstable Triples' : _count_or_len ( separately_unstable_triples ) , 'Mutually Unstable Triples' : _count_or_len ( mutually_unstable_triples ) , 'Jens Unstable Triples' : _count_or_len ( jens_unstable_triples ) , 'Increase Mismatch Triples' : _count_or_len ( increase_mismatch_triples ) , 'Decrease Mismatch Triples' : _count_or_len ( decrease_mismatch_triples ) , 'Chaotic Triples' : _count_or_len ( chaotic_triples ) , 'Dampened Triples' : _count_or_len ( dampened_triples ) }
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
7077	def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nworkers = None ) : pfpkldir = os . path . join ( simbasedir , 'periodfinding' ) if not os . path . exists ( pfpkldir ) : LOGERROR ( 'no "periodfinding" subdirectory in %s, can\'t continue' % simbasedir ) return None pfpkl_list = glob . glob ( os . path . join ( pfpkldir , '*periodfinding*pkl*' ) ) if len ( pfpkl_list ) > 0 : if liststartind : pfpkl_list = pfpkl_list [ liststartind : ] if listmaxobjects : pfpkl_list = pfpkl_list [ : listmaxobjects ] tasks = [ ( x , simbasedir , period_tolerance ) for x in pfpkl_list ] pool = mp . Pool ( nworkers ) results = pool . map ( periodrec_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { x [ 'objectid' ] : x for x in results if x is not None } actual_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and x [ 'actual_vartype' ] in PERIODIC_VARTYPES ) ] , dtype = np . unicode_ ) recovered_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'actual' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_twice_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'twice' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_half_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'half' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) all_objectids = [ x [ 'objectid' ] for x in results ] outdict = { 'simbasedir' : os . path . abspath ( simbasedir ) , 'objectids' : all_objectids , 'period_tolerance' : period_tolerance , 'actual_periodicvars' : actual_periodicvars , 'recovered_periodicvars' : recovered_periodicvars , 'alias_twice_periodicvars' : alias_twice_periodicvars , 'alias_half_periodicvars' : alias_half_periodicvars , 'details' : resdict } outfile = os . path . join ( simbasedir , 'periodicvar-recovery.pkl' ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict else : LOGERROR ( 'no periodfinding result pickles found in %s, can\'t continue' % pfpkldir ) return None
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
1042	def compare ( left , right , compare_locs = False ) : if type ( left ) != type ( right ) : return False if isinstance ( left , ast . AST ) : for field in left . _fields : if not compare ( getattr ( left , field ) , getattr ( right , field ) ) : return False if compare_locs : for loc in left . _locs : if getattr ( left , loc ) != getattr ( right , loc ) : return False return True elif isinstance ( left , list ) : if len ( left ) != len ( right ) : return False for left_elt , right_elt in zip ( left , right ) : if not compare ( left_elt , right_elt ) : return False return True else : return left == right
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
13504	def update ( self , server ) : return server . put ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
1921	def _hook_callback ( self , state , pc , instruction ) : 'Invoke all registered generic hooks' if issymbolic ( pc ) : return for cb in self . _hooks . get ( pc , [ ] ) : cb ( state ) for cb in self . _hooks . get ( None , [ ] ) : cb ( state )
3264	def md_link ( node ) : mimetype = node . find ( "type" ) mdtype = node . find ( "metadataType" ) content = node . find ( "content" ) if None in [ mimetype , mdtype , content ] : return None else : return ( mimetype . text , mdtype . text , content . text )
6047	def map_to_2d_keep_padded ( self , padded_array_1d ) : return mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = padded_array_1d , shape = self . mask . shape )
7170	def train ( self , debug = True , force = False , single_thread = False , timeout = 20 ) : if not self . must_train and not force : return self . padaos . compile ( ) self . train_thread = Thread ( target = self . _train , kwargs = dict ( debug = debug , single_thread = single_thread , timeout = timeout ) , daemon = True ) self . train_thread . start ( ) self . train_thread . join ( timeout ) self . must_train = False return not self . train_thread . is_alive ( )
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
8281	def _linepoint ( self , t , x0 , y0 , x1 , y1 ) : out_x = x0 + t * ( x1 - x0 ) out_y = y0 + t * ( y1 - y0 ) return ( out_x , out_y )
1856	def BSR ( cpu , dest , src ) : value = src . read ( ) flag = Operators . EXTRACT ( value , src . size - 1 , 1 ) == 1 res = 0 for pos in reversed ( range ( 0 , src . size ) ) : res = Operators . ITEBV ( dest . size , flag , res , pos ) flag = Operators . OR ( flag , ( Operators . EXTRACT ( value , pos , 1 ) == 1 ) ) cpu . PF = cpu . _calculate_parity_flag ( res ) cpu . ZF = value == 0 dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) )
13494	def write ( args ) : logging . info ( "Writing configure file: %s" % args . config_file ) if args . config_file is None : return config = cparser . ConfigParser ( ) config . add_section ( "lrcloud" ) for p in [ x for x in dir ( args ) if not x . startswith ( "_" ) ] : if p in IGNORE_ARGS : continue value = getattr ( args , p ) if value is not None : config . set ( 'lrcloud' , p , str ( value ) ) with open ( args . config_file , 'w' ) as f : config . write ( f )
5992	def plot_figure ( array , as_subplot , units , kpc_per_arcsec , figsize , aspect , cmap , norm , norm_min , norm_max , linthresh , linscale , xticks_manual , yticks_manual ) : fig = plotter_util . setup_figure ( figsize = figsize , as_subplot = as_subplot ) norm_min , norm_max = get_normalization_min_max ( array = array , norm_min = norm_min , norm_max = norm_max ) norm_scale = get_normalization_scale ( norm = norm , norm_min = norm_min , norm_max = norm_max , linthresh = linthresh , linscale = linscale ) extent = get_extent ( array = array , units = units , kpc_per_arcsec = kpc_per_arcsec , xticks_manual = xticks_manual , yticks_manual = yticks_manual ) plt . imshow ( array , aspect = aspect , cmap = cmap , norm = norm_scale , extent = extent ) return fig
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
853	def appendRecord ( self , record ) : assert self . _file is not None assert self . _mode == self . _FILE_WRITE_MODE assert isinstance ( record , ( list , tuple ) ) , "unexpected record type: " + repr ( type ( record ) ) assert len ( record ) == self . _fieldCount , "len(record): %s, fieldCount: %s" % ( len ( record ) , self . _fieldCount ) if self . _recordCount == 0 : names , types , specials = zip ( * self . getFields ( ) ) for line in names , types , specials : self . _writer . writerow ( line ) self . _updateSequenceInfo ( record ) line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] self . _writer . writerow ( line ) self . _recordCount += 1
9159	def delete_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_license_requests ( cursor , uuid_ , posted_uids ) resp = request . response resp . status_int = 200 return resp
13833	def ParseInteger ( text , is_signed = False , is_long = False ) : try : if is_long : result = long ( text , 0 ) else : result = int ( text , 0 ) except ValueError : raise ValueError ( 'Couldn\'t parse integer: %s' % text ) checker = _INTEGER_CHECKERS [ 2 * int ( is_long ) + int ( is_signed ) ] checker . CheckValue ( result ) return result
12297	def plugins_show ( what = None , name = None , version = None , details = False ) : global pluginmgr return pluginmgr . show ( what , name , version , details )
727	def numbersForBit ( self , bit ) : if bit >= self . _n : raise IndexError ( "Invalid bit" ) numbers = set ( ) for index , pattern in self . _patterns . iteritems ( ) : if bit in pattern : numbers . add ( index ) return numbers
12885	def python_value ( self , value ) : if self . field_type == 'TEXT' and isinstance ( value , str ) : return self . loads ( value ) return value
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
9909	def set_primary ( self ) : query = EmailAddress . objects . filter ( is_primary = True , user = self . user ) query = query . exclude ( pk = self . pk ) with transaction . atomic ( ) : query . update ( is_primary = False ) self . is_primary = True self . save ( ) logger . info ( "Set %s as the primary email address for %s." , self . email , self . user , )
8435	def map ( cls , x , palette , limits , na_value = None , oob = censor ) : x = oob ( rescale ( x , _from = limits ) ) pal = palette ( x ) try : pal [ pd . isnull ( x ) ] = na_value except TypeError : pal = [ v if not pd . isnull ( v ) else na_value for v in pal ] return pal
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : numPrevPatterns = len ( self . _prevLrnPatterns ) currentTimeStepsOffset = numPrevPatterns - 1 if not readOnly : self . segmentUpdates = { } if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) inSequence = True for offset in range ( startOffset , numPrevPatterns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] inputColumns = self . _prevLrnPatterns [ offset ] if not readOnly : self . _processSegmentUpdates ( inputColumns ) if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) if not inSequence or offset == currentTimeStepsOffset : break if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) return inSequence
13338	def redirect_resolver ( resolver , path ) : if not os . path . exists ( path ) : raise ResolveError if os . path . isfile ( path ) : path = os . path . dirname ( path ) for root , _ , _ in walk_up ( path ) : if is_redirecting ( root ) : env_paths = redirect_to_env_paths ( unipath ( root , '.cpenv' ) ) r = Resolver ( * env_paths ) return r . resolve ( ) raise ResolveError
688	def saveRecords ( self , path = 'myOutput' ) : numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) import csv with open ( path + '.csv' , 'wb' ) as f : writer = csv . writer ( f ) writer . writerow ( self . getAllFieldNames ( ) ) writer . writerow ( self . getAllDataTypes ( ) ) writer . writerow ( self . getAllFlags ( ) ) writer . writerows ( self . getAllRecords ( ) ) if self . verbosity > 0 : print '******' , numRecords , 'records exported in numenta format to file:' , path , '******\n'
996	def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc
7418	def make ( assembly , samples ) : longname = max ( [ len ( i ) for i in assembly . samples . keys ( ) ] ) names = [ i . name for i in samples ] partitions = makephy ( assembly , samples , longname ) makenex ( assembly , names , longname , partitions )
11683	def _readblock ( self ) : block = '' while not self . _stop : line = self . _readline ( ) if line == '.' : break block += line return block
2723	def take_snapshot ( self , snapshot_name , return_dict = True , power_off = False ) : if power_off is True and self . status != "off" : action = self . power_off ( return_dict = False ) action . wait ( ) self . load ( ) return self . _perform_action ( { "type" : "snapshot" , "name" : snapshot_name } , return_dict )
6566	def halfadder_gate ( variables , vartype = dimod . BINARY , name = 'HALF_ADDER' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 , 0 ) , ( 0 , 1 , 1 , 0 ) , ( 1 , 0 , 1 , 0 ) , ( 1 , 1 , 0 , 1 ) ] ) else : configs = frozenset ( [ ( - 1 , - 1 , - 1 , - 1 ) , ( - 1 , + 1 , + 1 , - 1 ) , ( + 1 , - 1 , + 1 , - 1 ) , ( + 1 , + 1 , - 1 , + 1 ) ] ) def func ( augend , addend , sum_ , carry ) : total = ( augend > 0 ) + ( addend > 0 ) if total == 0 : return ( sum_ <= 0 ) and ( carry <= 0 ) elif total == 1 : return ( sum_ > 0 ) and ( carry <= 0 ) elif total == 2 : return ( sum_ <= 0 ) and ( carry > 0 ) else : raise ValueError ( "func recieved unexpected values" ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
12662	def load_mask ( image , allow_empty = True ) : img = check_img ( image , make_it_3d = True ) values = np . unique ( img . get_data ( ) ) if len ( values ) == 1 : if values [ 0 ] == 0 and not allow_empty : raise ValueError ( 'Given mask is invalid because it masks all data' ) elif len ( values ) == 2 : if 0 not in values : raise ValueError ( 'Background of the mask must be represented with 0.' ' Given mask contains: {}.' . format ( values ) ) elif len ( values ) != 2 : raise ValueError ( 'Given mask is not made of 2 values: {}. ' 'Cannot interpret as true or false' . format ( values ) ) return nib . Nifti1Image ( as_ndarray ( get_img_data ( img ) , dtype = bool ) , img . get_affine ( ) , img . get_header ( ) )
729	def prettyPrintPattern ( self , bits , verbosity = 1 ) : numberMap = self . numberMapForBits ( bits ) text = "" numberList = [ ] numberItems = sorted ( numberMap . iteritems ( ) , key = lambda ( number , bits ) : len ( bits ) , reverse = True ) for number , bits in numberItems : if verbosity > 2 : strBits = [ str ( n ) for n in bits ] numberText = "{0} (bits: {1})" . format ( number , "," . join ( strBits ) ) elif verbosity > 1 : numberText = "{0} ({1} bits)" . format ( number , len ( bits ) ) else : numberText = str ( number ) numberList . append ( numberText ) text += "[{0}]" . format ( ", " . join ( numberList ) ) return text
9799	def stop ( ctx , yes , pending ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) if not yes and not click . confirm ( "Are sure you want to stop experiments " "in group `{}`" . format ( _group ) ) : click . echo ( 'Existing without stopping experiments in group.' ) sys . exit ( 0 ) try : PolyaxonClient ( ) . experiment_group . stop ( user , project_name , _group , pending = pending ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not stop experiments in group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments in group are being stopped." )
2269	def _symlink ( path , link , overwrite = 0 , verbose = 0 ) : if exists ( link ) and not os . path . islink ( link ) : if verbose : print ( 'link location already exists' ) is_junc = _win32_is_junction ( link ) if os . path . isdir ( link ) : if is_junc : pointed = _win32_read_junction ( link ) if path == pointed : if verbose : print ( '...and is a junction that points to the same place' ) return link else : if verbose : if not exists ( pointed ) : print ( '...and is a broken junction that points somewhere else' ) else : print ( '...and is a junction that points somewhere else' ) else : if verbose : print ( '...and is an existing real directory!' ) raise IOError ( 'Cannot overwrite a real directory' ) elif os . path . isfile ( link ) : if _win32_is_hardlinked ( link , path ) : if verbose : print ( '...and is a hard link that points to the same place' ) return link else : if verbose : print ( '...and is a hard link that points somewhere else' ) if _win32_can_symlink ( ) : raise IOError ( 'Cannot overwrite potentially real file if we can symlink' ) if overwrite : if verbose : print ( '...overwriting' ) util_io . delete ( link , verbose > 1 ) else : if exists ( link ) : raise IOError ( 'Link already exists' ) _win32_symlink2 ( path , link , verbose = verbose )
6042	def sparse_to_unmasked_sparse ( self ) : return mapping_util . sparse_to_unmasked_sparse_from_mask_and_pixel_centres ( total_sparse_pixels = self . total_sparse_pixels , mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres ) . astype ( 'int' )
10303	def tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )
3518	def kiss_insights ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return KissInsightsNode ( )
11624	def from_devanagari ( self , data ) : from indic_transliteration import sanscript return sanscript . transliterate ( data = data , _from = sanscript . DEVANAGARI , _to = self . name )
3784	def TP_dependent_property ( self , T , P ) : r if self . method_P : if self . test_method_validity_P ( T , P , self . method_P ) : try : prop = self . calculate_P ( T , P , self . method_P ) if self . test_property_validity ( prop ) : return prop except : pass self . sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method_P in self . sorted_valid_methods_P : try : prop = self . calculate_P ( T , P , method_P ) if self . test_property_validity ( prop ) : self . method_P = method_P return prop except : pass return None
10457	def getaccesskey ( self , window_name , object_name ) : menu_handle = self . _get_menu_handle ( window_name , object_name ) key = menu_handle . AXMenuItemCmdChar modifiers = menu_handle . AXMenuItemCmdModifiers glpyh = menu_handle . AXMenuItemCmdGlyph virtual_key = menu_handle . AXMenuItemCmdVirtualKey modifiers_type = "" if modifiers == 0 : modifiers_type = "<command>" elif modifiers == 1 : modifiers_type = "<shift><command>" elif modifiers == 2 : modifiers_type = "<option><command>" elif modifiers == 3 : modifiers_type = "<option><shift><command>" elif modifiers == 4 : modifiers_type = "<ctrl><command>" elif modifiers == 6 : modifiers_type = "<ctrl><option><command>" if virtual_key == 115 and glpyh == 102 : modifiers = "<option>" key = "<cursor_left>" elif virtual_key == 119 and glpyh == 105 : modifiers = "<option>" key = "<right>" elif virtual_key == 116 and glpyh == 98 : modifiers = "<option>" key = "<up>" elif virtual_key == 121 and glpyh == 107 : modifiers = "<option>" key = "<down>" elif virtual_key == 126 and glpyh == 104 : key = "<up>" elif virtual_key == 125 and glpyh == 106 : key = "<down>" elif virtual_key == 124 and glpyh == 101 : key = "<right>" elif virtual_key == 123 and glpyh == 100 : key = "<left>" elif virtual_key == 53 and glpyh == 27 : key = "<escape>" if not key : raise LdtpServerException ( "No access key associated" ) return modifiers_type + key
12796	def parse ( self , text , key = None ) : try : data = json . loads ( text ) except ValueError as e : raise ValueError ( "%s: Value: [%s]" % ( e , text ) ) if data and key : if key not in data : raise ValueError ( "Invalid response (key %s not found): %s" % ( key , data ) ) data = data [ key ] return data
5688	def get_transit_events ( self , start_time_ut = None , end_time_ut = None , route_type = None ) : table_name = self . _get_day_trips_table_name ( ) event_query = "SELECT stop_I, seq, trip_I, route_I, routes.route_id AS route_id, routes.type AS route_type, " "shape_id, day_start_ut+dep_time_ds AS dep_time_ut, day_start_ut+arr_time_ds AS arr_time_ut " "FROM " + table_name + " " "JOIN trips USING(trip_I) " "JOIN routes USING(route_I) " "JOIN stop_times USING(trip_I)" where_clauses = [ ] if end_time_ut : where_clauses . append ( table_name + ".start_time_ut< {end_time_ut}" . format ( end_time_ut = end_time_ut ) ) where_clauses . append ( "dep_time_ut <={end_time_ut}" . format ( end_time_ut = end_time_ut ) ) if start_time_ut : where_clauses . append ( table_name + ".end_time_ut > {start_time_ut}" . format ( start_time_ut = start_time_ut ) ) where_clauses . append ( "arr_time_ut >={start_time_ut}" . format ( start_time_ut = start_time_ut ) ) if route_type is not None : assert route_type in ALL_ROUTE_TYPES where_clauses . append ( "routes.type={route_type}" . format ( route_type = route_type ) ) if len ( where_clauses ) > 0 : event_query += " WHERE " for i , where_clause in enumerate ( where_clauses ) : if i is not 0 : event_query += " AND " event_query += where_clause event_query += " ORDER BY trip_I, day_start_ut+dep_time_ds;" events_result = pd . read_sql_query ( event_query , self . conn ) from_indices = numpy . nonzero ( ( events_result [ 'trip_I' ] [ : - 1 ] . values == events_result [ 'trip_I' ] [ 1 : ] . values ) * ( events_result [ 'seq' ] [ : - 1 ] . values < events_result [ 'seq' ] [ 1 : ] . values ) ) [ 0 ] to_indices = from_indices + 1 assert ( events_result [ 'trip_I' ] [ from_indices ] . values == events_result [ 'trip_I' ] [ to_indices ] . values ) . all ( ) trip_Is = events_result [ 'trip_I' ] [ from_indices ] from_stops = events_result [ 'stop_I' ] [ from_indices ] to_stops = events_result [ 'stop_I' ] [ to_indices ] shape_ids = events_result [ 'shape_id' ] [ from_indices ] dep_times = events_result [ 'dep_time_ut' ] [ from_indices ] arr_times = events_result [ 'arr_time_ut' ] [ to_indices ] route_types = events_result [ 'route_type' ] [ from_indices ] route_ids = events_result [ 'route_id' ] [ from_indices ] route_Is = events_result [ 'route_I' ] [ from_indices ] durations = arr_times . values - dep_times . values assert ( durations >= 0 ) . all ( ) from_seqs = events_result [ 'seq' ] [ from_indices ] to_seqs = events_result [ 'seq' ] [ to_indices ] data_tuples = zip ( from_stops , to_stops , dep_times , arr_times , shape_ids , route_types , route_ids , trip_Is , durations , from_seqs , to_seqs , route_Is ) columns = [ "from_stop_I" , "to_stop_I" , "dep_time_ut" , "arr_time_ut" , "shape_id" , "route_type" , "route_id" , "trip_I" , "duration" , "from_seq" , "to_seq" , "route_I" ] df = pd . DataFrame . from_records ( data_tuples , columns = columns ) return df
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
7494	def count_snps ( mat ) : snps = np . zeros ( 4 , dtype = np . uint32 ) snps [ 0 ] = np . uint32 ( mat [ 0 , 5 ] + mat [ 0 , 10 ] + mat [ 0 , 15 ] + mat [ 5 , 0 ] + mat [ 5 , 10 ] + mat [ 5 , 15 ] + mat [ 10 , 0 ] + mat [ 10 , 5 ] + mat [ 10 , 15 ] + mat [ 15 , 0 ] + mat [ 15 , 5 ] + mat [ 15 , 10 ] ) for i in range ( 16 ) : if i % 5 : snps [ 1 ] += mat [ i , i ] snps [ 2 ] = mat [ 1 , 4 ] + mat [ 2 , 8 ] + mat [ 3 , 12 ] + mat [ 4 , 1 ] + mat [ 6 , 9 ] + mat [ 7 , 13 ] + mat [ 8 , 2 ] + mat [ 9 , 6 ] + mat [ 11 , 14 ] + mat [ 12 , 3 ] + mat [ 13 , 7 ] + mat [ 14 , 11 ] snps [ 3 ] = ( mat . sum ( ) - np . diag ( mat ) . sum ( ) ) - snps [ 2 ] return snps
1288	def tf_baseline_loss ( self , states , internals , reward , update , reference = None ) : if self . baseline_mode == 'states' : loss = self . baseline . loss ( states = states , internals = internals , reward = reward , update = update , reference = reference ) elif self . baseline_mode == 'network' : loss = self . baseline . loss ( states = self . network . apply ( x = states , internals = internals , update = update ) , internals = internals , reward = reward , update = update , reference = reference ) regularization_loss = self . baseline . regularization_loss ( ) if regularization_loss is not None : loss += regularization_loss return loss
9803	def set ( verbose , host , http_port , ws_port , use_https , verify_ssl ) : _config = GlobalConfigManager . get_config_or_default ( ) if verbose is not None : _config . verbose = verbose if host is not None : _config . host = host if http_port is not None : _config . http_port = http_port if ws_port is not None : _config . ws_port = ws_port if use_https is not None : _config . use_https = use_https if verify_ssl is False : _config . verify_ssl = verify_ssl GlobalConfigManager . set_config ( _config ) Printer . print_success ( 'Config was updated.' ) CliConfigManager . purge ( )
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
11669	def topological_sort ( deps ) : order = [ ] available = set ( ) def _move_available ( ) : to_delete = [ ] for n , parents in iteritems ( deps ) : if not parents : available . add ( n ) to_delete . append ( n ) for n in to_delete : del deps [ n ] _move_available ( ) while available : n = available . pop ( ) order . append ( n ) for parents in itervalues ( deps ) : parents . discard ( n ) _move_available ( ) if available : raise ValueError ( "dependency cycle found" ) return order
1731	def call ( self , this , args = ( ) ) : if self . is_native : _args = SpaceTuple ( args ) _args . space = self . space return self . code ( this , _args ) else : return self . space . exe . _call ( self , this , args )
13542	def update ( self , server ) : return server . put ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
3683	def calculate ( self , T , method ) : r if method == WAGNER_MCGARRY : Psat = Wagner_original ( T , self . WAGNER_MCGARRY_Tc , self . WAGNER_MCGARRY_Pc , * self . WAGNER_MCGARRY_coefs ) elif method == WAGNER_POLING : Psat = Wagner ( T , self . WAGNER_POLING_Tc , self . WAGNER_POLING_Pc , * self . WAGNER_POLING_coefs ) elif method == ANTOINE_EXTENDED_POLING : Psat = TRC_Antoine_extended ( T , * self . ANTOINE_EXTENDED_POLING_coefs ) elif method == ANTOINE_POLING : A , B , C = self . ANTOINE_POLING_coefs Psat = Antoine ( T , A , B , C , base = 10.0 ) elif method == DIPPR_PERRY_8E : Psat = EQ101 ( T , * self . Perrys2_8_coeffs ) elif method == VDI_PPDS : Psat = Wagner ( T , self . VDI_PPDS_Tc , self . VDI_PPDS_Pc , * self . VDI_PPDS_coeffs ) elif method == COOLPROP : Psat = PropsSI ( 'P' , 'T' , T , 'Q' , 0 , self . CASRN ) elif method == BOILING_CRITICAL : Psat = boiling_critical_relation ( T , self . Tb , self . Tc , self . Pc ) elif method == LEE_KESLER_PSAT : Psat = Lee_Kesler ( T , self . Tc , self . Pc , self . omega ) elif method == AMBROSE_WALTON : Psat = Ambrose_Walton ( T , self . Tc , self . Pc , self . omega ) elif method == SANJARI : Psat = Sanjari ( T , self . Tc , self . Pc , self . omega ) elif method == EDALAT : Psat = Edalat ( T , self . Tc , self . Pc , self . omega ) elif method == EOS : Psat = self . eos [ 0 ] . Psat ( T ) elif method in self . tabular_data : Psat = self . interpolate ( T , method ) return Psat
7683	def display_multi ( annotations , fig_kw = None , meta = True , ** kwargs ) : if fig_kw is None : fig_kw = dict ( ) fig_kw . setdefault ( 'sharex' , True ) fig_kw . setdefault ( 'squeeze' , True ) display_annotations = [ ] for ann in annotations : for namespace in VIZ_MAPPING : if can_convert ( ann , namespace ) : display_annotations . append ( ann ) break if not len ( display_annotations ) : raise ParameterError ( 'No displayable annotations found' ) fig , axs = plt . subplots ( nrows = len ( display_annotations ) , ncols = 1 , ** fig_kw ) if len ( display_annotations ) == 1 : axs = [ axs ] for ann , ax in zip ( display_annotations , axs ) : kwargs [ 'ax' ] = ax display ( ann , meta = meta , ** kwargs ) return fig , axs
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
2364	def append ( self , linenumber , raw_text , cells ) : self . rows . append ( Row ( linenumber , raw_text , cells ) )
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
11194	def freeze ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) num_items = len ( list ( proto_dataset . _identifiers ( ) ) ) max_files_limit = int ( dtoolcore . utils . get_config_value ( "DTOOL_MAX_FILES_LIMIT" , CONFIG_PATH , 10000 ) ) assert isinstance ( max_files_limit , int ) if num_items > max_files_limit : click . secho ( "Too many items ({} > {}) in proto dataset" . format ( num_items , max_files_limit ) , fg = "red" ) click . secho ( "1. Consider splitting the dataset into smaller datasets" ) click . secho ( "2. Consider packaging small files using tar" ) click . secho ( "3. Increase the limit using the DTOOL_MAX_FILES_LIMIT" ) click . secho ( " environment variable" ) sys . exit ( 2 ) handles = [ h for h in proto_dataset . _storage_broker . iter_item_handles ( ) ] for h in handles : if not valid_handle ( h ) : click . secho ( "Invalid item name: {}" . format ( h ) , fg = "red" ) click . secho ( "1. Consider renaming the item" ) click . secho ( "2. Consider removing the item" ) sys . exit ( 3 ) with click . progressbar ( length = len ( list ( proto_dataset . _identifiers ( ) ) ) , label = "Generating manifest" ) as progressbar : try : proto_dataset . freeze ( progressbar = progressbar ) except dtoolcore . storagebroker . DiskStorageBrokerValidationWarning as e : click . secho ( "" ) click . secho ( str ( e ) , fg = "red" , nl = False ) sys . exit ( 4 ) click . secho ( "Dataset frozen " , nl = False , fg = "green" ) click . secho ( proto_dataset_uri )
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams forder = len ( famps ) iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] modelmags = np . median ( mags ) for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
10654	def prepare_to_run ( self , clock , period_count ) : self . period_count = period_count self . _exec_year_end_datetime = clock . get_datetime_at_period_ix ( period_count ) self . _prev_year_end_datetime = clock . start_datetime self . _curr_year_end_datetime = clock . start_datetime + relativedelta ( years = 1 ) del self . gl . transactions [ : ] for c in self . components : c . prepare_to_run ( clock , period_count ) self . negative_income_tax_total = 0
6898	def serial_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , starfeatures = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( fileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformatdir , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) for task in tqdm ( tasks ) : _periodicfeatures_worker ( task )
10094	def get_template ( self , template_id , version = None , timeout = None ) : if ( version ) : return self . _api_request ( self . TEMPLATES_VERSION_ENDPOINT % ( template_id , version ) , self . HTTP_GET , timeout = timeout ) else : return self . _api_request ( self . TEMPLATES_SPECIFIC_ENDPOINT % template_id , self . HTTP_GET , timeout = timeout )
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
1546	def get_component_metrics ( component , cluster , env , topology , role ) : all_queries = metric_queries ( ) try : result = get_topology_metrics ( cluster , env , topology , component , [ ] , all_queries , [ 0 , - 1 ] , role ) return result [ "metrics" ] except Exception : Log . debug ( traceback . format_exc ( ) ) raise
1211	def run ( self ) : if not self . state . document . settings . file_insertion_enabled : raise self . warning ( '"%s" directive disabled.' % self . name ) source = self . state_machine . input_lines . source ( self . lineno - self . state_machine . input_offset - 1 ) source_dir = os . path . dirname ( os . path . abspath ( source ) ) path = rst . directives . path ( self . arguments [ 0 ] ) path = os . path . normpath ( os . path . join ( source_dir , path ) ) path = utils . relative_path ( None , path ) path = nodes . reprunicode ( path ) encoding = self . options . get ( 'encoding' , self . state . document . settings . input_encoding ) e_handler = self . state . document . settings . input_encoding_error_handler tab_width = self . options . get ( 'tab-width' , self . state . document . settings . tab_width ) try : self . state . document . settings . record_dependencies . add ( path ) include_file = io . FileInput ( source_path = path , encoding = encoding , error_handler = e_handler ) except UnicodeEncodeError as error : raise self . severe ( 'Problems with "%s" directive path:\n' 'Cannot encode input file path "%s" ' '(wrong locale?).' % ( self . name , SafeString ( path ) ) ) except IOError as error : raise self . severe ( 'Problems with "%s" directive path:\n%s.' % ( self . name , ErrorString ( error ) ) ) try : rawtext = include_file . read ( ) except UnicodeError as error : raise self . severe ( 'Problem with "%s" directive:\n%s' % ( self . name , ErrorString ( error ) ) ) config = self . state . document . settings . env . config converter = M2R ( no_underscore_emphasis = config . no_underscore_emphasis ) include_lines = statemachine . string2lines ( converter ( rawtext ) , tab_width , convert_whitespace = True ) self . state_machine . insert_input ( include_lines , path ) return [ ]
884	def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ]
11014	def set_real_value_class ( self ) : if self . value_class is not None and isinstance ( self . value_class , str ) : module_name , dot , class_name = self . value_class . rpartition ( "." ) module = __import__ ( module_name , fromlist = [ class_name ] ) self . value_class = getattr ( module , class_name ) self . _initialized = True
11285	def _list_networks ( ) : output = core . run ( "virsh net-list --all" ) networks = { } net_lines = [ n . strip ( ) for n in output . splitlines ( ) [ 2 : ] ] for line in net_lines : if not line : continue name , state , auto = line . split ( ) networks [ name ] = state == "active" return networks
2261	def dict_hist ( item_list , weight_list = None , ordered = False , labels = None ) : if labels is None : hist_ = defaultdict ( lambda : 0 ) else : hist_ = { k : 0 for k in labels } if weight_list is None : weight_list = it . repeat ( 1 ) for item , weight in zip ( item_list , weight_list ) : hist_ [ item ] += weight if ordered : getval = op . itemgetter ( 1 ) hist = OrderedDict ( [ ( key , value ) for ( key , value ) in sorted ( hist_ . items ( ) , key = getval ) ] ) else : hist = dict ( hist_ ) return hist
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
2157	def _auto_help_text ( self , help_text ) : api_doc_delimiter = '=====API DOCS=====' begin_api_doc = help_text . find ( api_doc_delimiter ) if begin_api_doc >= 0 : end_api_doc = help_text . rfind ( api_doc_delimiter ) + len ( api_doc_delimiter ) help_text = help_text [ : begin_api_doc ] + help_text [ end_api_doc : ] an_prefix = ( 'a' , 'e' , 'i' , 'o' ) if not self . resource_name . lower ( ) . startswith ( an_prefix ) : help_text = help_text . replace ( 'an object' , 'a %s' % self . resource_name ) if self . resource_name . lower ( ) . endswith ( 'y' ) : help_text = help_text . replace ( 'objects' , '%sies' % self . resource_name [ : - 1 ] , ) help_text = help_text . replace ( 'object' , self . resource_name ) help_text = help_text . replace ( 'keyword argument' , 'option' ) help_text = help_text . replace ( 'raise an exception' , 'abort with an error' ) for match in re . findall ( r'`([\w_]+)`' , help_text ) : option = '--%s' % match . replace ( '_' , '-' ) help_text = help_text . replace ( '`%s`' % match , option ) return help_text
7255	def get ( self , catID , includeRelationships = False ) : url = '%(base_url)s/record/%(catID)s' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
6735	def write_temp_file_or_dryrun ( content , * args , ** kwargs ) : dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) if dryrun : fd , tmp_fn = tempfile . mkstemp ( ) os . remove ( tmp_fn ) cmd_run = 'local' cmd = 'cat <<EOT >> %s\n%s\nEOT' % ( tmp_fn , content ) if BURLAP_COMMAND_PREFIX : print ( '%s %s: %s' % ( render_command_prefix ( ) , cmd_run , cmd ) ) else : print ( cmd ) else : fd , tmp_fn = tempfile . mkstemp ( ) fout = open ( tmp_fn , 'w' ) fout . write ( content ) fout . close ( ) return tmp_fn
3524	def intercom ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return IntercomNode ( )
4877	def validate_course_run_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) if not enterprise_customer . catalog_contains_course ( value ) : raise serializers . ValidationError ( 'The course run id {course_run_id} is not in the catalog ' 'for Enterprise Customer {enterprise_customer}' . format ( course_run_id = value , enterprise_customer = enterprise_customer . name , ) ) return value
11180	def exchange_token ( self , code ) : access_token_url = OAUTH_ROOT + '/access_token' params = { 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'redirect_uri' : self . redirect_uri , 'code' : code , } resp = requests . get ( access_token_url , params = params ) if not resp . ok : raise MixcloudOauthError ( "Could not get access token." ) return resp . json ( ) [ 'access_token' ]
3803	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return mixing_simple ( zs , ks ) elif method == DIPPR_9H : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return DIPPR9H ( ws , ks ) elif method == FILIPPOV : ks = [ i ( T , P ) for i in self . ThermalConductivityLiquids ] return Filippov ( ws , ks ) elif method == MAGOMEDOV : k_w = self . ThermalConductivityLiquids [ self . index_w ] ( T , P ) ws = list ( ws ) ws . pop ( self . index_w ) return thermal_conductivity_Magomedov ( T , P , ws , self . wCASs , k_w ) else : raise Exception ( 'Method not valid' )
10258	def count_top_centrality ( graph : BELGraph , number : Optional [ int ] = 30 ) -> Mapping [ BaseEntity , int ] : dd = nx . betweenness_centrality ( graph ) dc = Counter ( dd ) return dict ( dc . most_common ( number ) )
454	def initialize_rnn_state ( state , feed_dict = None ) : if isinstance ( state , LSTMStateTuple ) : c = state . c . eval ( feed_dict = feed_dict ) h = state . h . eval ( feed_dict = feed_dict ) return c , h else : new_state = state . eval ( feed_dict = feed_dict ) return new_state
2060	def add ( self , constraint , check = False ) : if isinstance ( constraint , bool ) : constraint = BoolConstant ( constraint ) assert isinstance ( constraint , Bool ) constraint = simplify ( constraint ) if self . _child is not None : raise Exception ( 'ConstraintSet is frozen' ) if isinstance ( constraint , BoolConstant ) : if not constraint . value : logger . info ( "Adding an impossible constant constraint" ) self . _constraints = [ constraint ] else : return self . _constraints . append ( constraint ) if check : from . . . core . smtlib import solver if not solver . check ( self ) : raise ValueError ( "Added an impossible constraint" )
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
10061	def schemaforms ( self ) : _schemaforms = { k : v [ 'schemaform' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'schemaform' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_SCHEMAFORM' ] , _schemaforms )
11962	def _dot_to_dec ( ip , check = True ) : if check and not is_dot ( ip ) : raise ValueError ( '_dot_to_dec: invalid IP: "%s"' % ip ) octets = str ( ip ) . split ( '.' ) dec = 0 dec |= int ( octets [ 0 ] ) << 24 dec |= int ( octets [ 1 ] ) << 16 dec |= int ( octets [ 2 ] ) << 8 dec |= int ( octets [ 3 ] ) return dec
11140	def reset ( self ) : self . __path = None self . __repo = { 'repository_unique_name' : str ( uuid . uuid1 ( ) ) , 'create_utctime' : time . time ( ) , 'last_update_utctime' : None , 'pyrep_version' : str ( __version__ ) , 'repository_information' : '' , 'walk_repo' : [ ] }
11671	def _flann_args ( self , X = None ) : "The dictionary of arguments to give to FLANN." args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n {}" raise TypeError ( msg . format ( e ) ) return args
13134	def import_domaindump ( ) : parser = argparse . ArgumentParser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs" ) parser . add_argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse_args ( ) domain_users_file = '' domain_groups_file = '' computer_count = 0 user_count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain_computers.json' ) : print_notification ( 'Parsing domain computers' ) computer_count = parse_domain_computers ( filename ) if computer_count : stats [ 'hosts' ] = computer_count print_success ( "{} hosts imported" . format ( computer_count ) ) elif filename . endswith ( 'domain_users.json' ) : domain_users_file = filename elif filename . endswith ( 'domain_groups.json' ) : domain_groups_file = filename if domain_users_file : print_notification ( "Parsing domain users" ) user_count = parse_domain_users ( domain_users_file , domain_groups_file ) if user_count : print_success ( "{} users imported" . format ( user_count ) ) stats [ 'users' ] = user_count Logger ( ) . log ( "import_domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user_count , computer_count ) , stats )
11489	def _download_folder_recursive ( folder_id , path = '.' ) : session . token = verify_credentials ( ) cur_folder = session . communicator . folder_get ( session . token , folder_id ) folder_path = os . path . join ( path , cur_folder [ 'name' ] . replace ( '/' , '_' ) ) print ( 'Creating folder at {0}' . format ( folder_path ) ) try : os . mkdir ( folder_path ) except OSError as e : if e . errno == errno . EEXIST and session . allow_existing_download_paths : pass else : raise cur_children = session . communicator . folder_children ( session . token , folder_id ) for item in cur_children [ 'items' ] : _download_item ( item [ 'item_id' ] , folder_path , item = item ) for folder in cur_children [ 'folders' ] : _download_folder_recursive ( folder [ 'folder_id' ] , folder_path ) for callback in session . folder_download_callbacks : callback ( session . communicator , session . token , cur_folder , folder_path )
3646	def sendToWatchlist ( self , trade_id ) : method = 'PUT' url = 'watchlist' data = { 'auctionInfo' : [ { 'id' : trade_id } ] } return self . __request__ ( method , url , data = json . dumps ( data ) )
11254	def attrs ( prev , attr_names ) : for obj in prev : attr_values = [ ] for name in attr_names : if hasattr ( obj , name ) : attr_values . append ( getattr ( obj , name ) ) yield attr_values
4869	def to_representation ( self , instance ) : updated_program = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_program [ 'enrollment_url' ] = enterprise_customer_catalog . get_program_enrollment_url ( updated_program [ 'uuid' ] ) for course in updated_program [ 'courses' ] : course [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_enrollment_url ( course [ 'key' ] ) for course_run in course [ 'course_runs' ] : course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( course_run [ 'key' ] ) return updated_program
4166	def tf2zp ( b , a ) : from numpy import roots assert len ( b ) == len ( a ) , "length of the vectors a and b must be identical. fill with zeros if needed." g = b [ 0 ] / a [ 0 ] z = roots ( b ) p = roots ( a ) return z , p , g
1484	def start_state_manager_watches ( self ) : Log . info ( "Start state manager watches" ) statemgr_config = StateMgrConfig ( ) statemgr_config . set_state_locations ( configloader . load_state_manager_locations ( self . cluster , state_manager_config_file = self . state_manager_config_file , overrides = { "heron.statemgr.connection.string" : self . state_manager_connection } ) ) try : self . state_managers = statemanagerfactory . get_all_state_managers ( statemgr_config ) for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_packing_plan_watch ( state_manager , new_packing_plan ) : Log . debug ( "State watch triggered for PackingPlan update on shard %s. Existing: %s, New: %s" % ( self . shard , str ( self . packing_plan ) , str ( new_packing_plan ) ) ) if self . packing_plan != new_packing_plan : Log . info ( "PackingPlan change detected on shard %s, relaunching effected processes." % self . shard ) self . update_packing_plan ( new_packing_plan ) Log . info ( "Updating executor processes" ) self . launch ( ) else : Log . info ( "State watch triggered for PackingPlan update but plan not changed so not relaunching." ) for state_manager in self . state_managers : onPackingPlanWatch = functools . partial ( on_packing_plan_watch , state_manager ) state_manager . get_packing_plan ( self . topology_name , onPackingPlanWatch ) Log . info ( "Registered state watch for packing plan changes with state manager %s." % str ( state_manager ) )
11584	def background_image_finder ( pipeline_index , soup , finder_image_urls = [ ] , * args , ** kwargs ) : now_finder_image_urls = [ ] for tag in soup . find_all ( style = True ) : style_string = tag [ 'style' ] if 'background-image' in style_string . lower ( ) : style = cssutils . parseStyle ( style_string ) background_image = style . getProperty ( 'background-image' ) if background_image : for property_value in background_image . propertyValue : background_image_url = str ( property_value . value ) if background_image_url : if ( background_image_url not in finder_image_urls ) and ( background_image_url not in now_finder_image_urls ) : now_finder_image_urls . append ( background_image_url ) output = { } output [ 'finder_image_urls' ] = finder_image_urls + now_finder_image_urls return output
5857	def get_available_columns ( self , dataset_ids ) : if not isinstance ( dataset_ids , list ) : dataset_ids = [ dataset_ids ] data = { "dataset_ids" : dataset_ids } failure_message = "Failed to get available columns in dataset(s) {}" . format ( dataset_ids ) return self . _get_success_json ( self . _post_json ( 'v1/datasets/get-available-columns' , data , failure_message = failure_message ) ) [ 'data' ]
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
4685	def unlock_wallet ( self , * args , ** kwargs ) : self . blockchain . wallet . unlock ( * args , ** kwargs ) return self
12981	def string ( html , start_on = None , ignore = ( ) , use_short = True , ** queries ) : if use_short : html = grow_short ( html ) return _to_template ( fromstring ( html ) , start_on = start_on , ignore = ignore , ** queries )
2780	def get_object ( cls , api_token , domain , record_id ) : record = cls ( token = api_token , domain = domain , id = record_id ) record . load ( ) return record
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
10689	def _create_air ( ) : name = "Air" namel = name . lower ( ) mm = 28.9645 ds_dict = _create_ds_dict ( [ "dataset-air-lienhard2015" , "dataset-air-lienhard2018" ] ) active_ds = "dataset-air-lienhard2018" model_dict = { "rho" : IgRhoT ( mm , 101325.0 ) , "beta" : IgBetaT ( ) } model_type = "polynomialmodelt" for property in [ "Cp" , "mu" , "k" ] : name = f"data/{namel}-{property.lower()}-{model_type}-{active_ds}.json" model_dict [ property ] = PolynomialModelT . read ( _path ( name ) ) material = Material ( name , StateOfMatter . gas , model_dict ) return material , ds_dict
8143	def rotate ( self , angle ) : from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
4445	def create_index ( self , fields , no_term_offsets = False , no_field_flags = False , stopwords = None ) : args = [ self . CREATE_CMD , self . index_name ] if no_term_offsets : args . append ( self . NOOFFSETS ) if no_field_flags : args . append ( self . NOFIELDS ) if stopwords is not None and isinstance ( stopwords , ( list , tuple , set ) ) : args += [ self . STOPWORDS , len ( stopwords ) ] if len ( stopwords ) > 0 : args += list ( stopwords ) args . append ( 'SCHEMA' ) args += list ( itertools . chain ( * ( f . redis_args ( ) for f in fields ) ) ) return self . redis . execute_command ( * args )
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
4710	def get_chunk_information ( self , chk , lun , chunk_name ) : cmd = [ "nvm_cmd rprt_lun" , self . envs , "%d %d > %s" % ( chk , lun , chunk_name ) ] status , _ , _ = cij . ssh . command ( cmd , shell = True ) return status
6753	def local_renderer ( self ) : if not self . _local_renderer : r = self . create_local_renderer ( ) self . _local_renderer = r return self . _local_renderer
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
1243	def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
680	def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values
8093	def node_label ( s , node , alpha = 1.0 ) : if s . text : s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize ) s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha ) try : p = node . _textpath except : txt = node . label try : txt = unicode ( txt ) except : try : txt = txt . decode ( "utf-8" ) except : pass dx , dy = 0 , 0 if s . align == 2 : dx = - s . _ctx . textwidth ( txt , s . textwidth ) / 2 dy = s . _ctx . textheight ( txt ) / 2 node . _textpath = s . _ctx . textpath ( txt , dx , dy , width = s . textwidth ) p = node . _textpath if s . depth : try : __colors . shadow ( dx = 2 , dy = 4 , blur = 5 , alpha = 0.3 * alpha ) except : pass s . _ctx . push ( ) s . _ctx . translate ( node . x , node . y ) s . _ctx . scale ( alpha ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
3611	def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
11935	def reuse ( context , block_list , ** kwargs ) : try : block_context = context . render_context [ BLOCK_CONTEXT_KEY ] except KeyError : block_context = BlockContext ( ) if not isinstance ( block_list , ( list , tuple ) ) : block_list = [ block_list ] for block in block_list : block = block_context . get_block ( block ) if block : break else : return '' with context . push ( kwargs ) : return block . render ( context )
9222	def reverse_guard ( lst ) : rev = { '<' : '>=' , '>' : '=<' , '>=' : '<' , '=<' : '>' } return [ rev [ l ] if l in rev else l for l in lst ]
9671	def _from_name ( self , string ) : components = string . split ( ' ' ) if frozenset ( components ) in self . features : return self . features [ frozenset ( components ) ] rest , sound_class = components [ : - 1 ] , components [ - 1 ] if sound_class in [ 'diphthong' , 'cluster' ] : if string . startswith ( 'from ' ) and 'to ' in string : extension = { 'diphthong' : 'vowel' , 'cluster' : 'consonant' } [ sound_class ] string_ = ' ' . join ( string . split ( ' ' ) [ 1 : - 1 ] ) from_ , to_ = string_ . split ( ' to ' ) v1 , v2 = frozenset ( from_ . split ( ' ' ) + [ extension ] ) , frozenset ( to_ . split ( ' ' ) + [ extension ] ) if v1 in self . features and v2 in self . features : s1 , s2 = ( self . features [ v1 ] , self . features [ v2 ] ) if sound_class == 'diphthong' : return Diphthong . from_sounds ( s1 + s2 , s1 , s2 , self ) else : return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) else : s1 , s2 = self . _from_name ( from_ + ' ' + extension ) , self . _from_name ( to_ + ' ' + extension ) if not ( isinstance ( s1 , UnknownSound ) or isinstance ( s2 , UnknownSound ) ) : if sound_class == 'diphthong' : return Diphthong . from_sounds ( s1 + s2 , s1 , s2 , self ) return Cluster . from_sounds ( s1 + s2 , s1 , s2 , self ) raise ValueError ( 'components could not be found in system' ) else : raise ValueError ( 'name string is erroneously encoded' ) if sound_class not in self . sound_classes : raise ValueError ( 'no sound class specified' ) args = { self . _feature_values . get ( comp , '?' ) : comp for comp in rest } if '?' in args : raise ValueError ( 'string contains unknown features' ) args [ 'grapheme' ] = '' args [ 'ts' ] = self sound = self . sound_classes [ sound_class ] ( ** args ) if sound . featureset not in self . features : sound . generated = True return sound return self . features [ sound . featureset ]
6824	def restart ( self ) : n = 60 sleep_n = int ( self . env . max_restart_wait_minutes / 10. * 60 ) for _ in xrange ( n ) : self . stop ( ) if self . dryrun or not self . is_running ( ) : break print ( 'Waiting for supervisor to stop (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) self . start ( ) for _ in xrange ( n ) : if self . dryrun or self . is_running ( ) : return print ( 'Waiting for supervisor to start (%i of %i)...' % ( _ , n ) ) time . sleep ( sleep_n ) raise Exception ( 'Failed to restart service %s!' % self . name )
2093	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : stdout_url = '%s%s/stdout/' % ( self . unified_job_type , pk ) payload = { 'format' : 'json' , 'content_encoding' : 'base64' , 'content_format' : 'ansi' } if start_line : payload [ 'start_line' ] = start_line if end_line : payload [ 'end_line' ] = end_line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout_url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
4419	async def play_previous ( self ) : if not self . previous : raise NoPreviousTrack self . queue . insert ( 0 , self . previous ) await self . play ( ignore_shuffle = True )
272	def clip_returns_to_benchmark ( rets , benchmark_rets ) : if ( rets . index [ 0 ] < benchmark_rets . index [ 0 ] ) or ( rets . index [ - 1 ] > benchmark_rets . index [ - 1 ] ) : clipped_rets = rets [ benchmark_rets . index ] else : clipped_rets = rets return clipped_rets
11278	def parse_address_list ( addrs ) : for addr in addrs . split ( ',' ) : elem = addr . split ( '-' ) if len ( elem ) == 1 : yield int ( elem [ 0 ] ) elif len ( elem ) == 2 : start , end = list ( map ( int , elem ) ) for i in range ( start , end + 1 ) : yield i else : raise ValueError ( 'format error in %s' % addr )
9724	async def load_project ( self , project_path ) : cmd = "loadproject %s" % project_path return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
9551	def _init_unique_sets ( self ) : ks = dict ( ) for t in self . _unique_checks : key = t [ 0 ] ks [ key ] = set ( ) return ks
11473	def parse_data ( self , text , maxwidth , maxheight , template_dir , context , urlize_all_links ) : block_parser = TextBlockParser ( ) lines = text . splitlines ( ) parsed = [ ] for line in lines : if STANDALONE_URL_RE . match ( line ) : user_url = line . strip ( ) try : resource = oembed . site . embed ( user_url , maxwidth = maxwidth , maxheight = maxheight ) context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) except OEmbedException : if urlize_all_links : line = '<a href="%(LINK)s">%(LINK)s</a>' % { 'LINK' : user_url } else : context [ 'minwidth' ] = min ( maxwidth , resource . width ) context [ 'minheight' ] = min ( maxheight , resource . height ) line = self . render_oembed ( resource , user_url , template_dir = template_dir , context = context ) else : line = block_parser . parse ( line , maxwidth , maxheight , 'inline' , context , urlize_all_links ) parsed . append ( line ) return mark_safe ( '\n' . join ( parsed ) )
12327	def update ( globalvars ) : global config profileini = getprofileini ( ) config = configparser . ConfigParser ( ) config . read ( profileini ) defaults = { } if globalvars is not None : defaults = { a [ 0 ] : a [ 1 ] for a in globalvars } generic_configs = [ { 'name' : 'User' , 'nature' : 'generic' , 'description' : "General information" , 'variables' : [ 'user.email' , 'user.name' , 'user.fullname' ] , 'defaults' : { 'user.email' : { 'value' : defaults . get ( 'user.email' , '' ) , 'description' : "Email address" , 'validator' : EmailValidator ( ) } , 'user.fullname' : { 'value' : defaults . get ( 'user.fullname' , '' ) , 'description' : "Full Name" , 'validator' : NonEmptyValidator ( ) } , 'user.name' : { 'value' : defaults . get ( 'user.name' , getpass . getuser ( ) ) , 'description' : "Name" , 'validator' : NonEmptyValidator ( ) } , } } ] mgr = plugins_get_mgr ( ) extra_configs = mgr . gather_configs ( ) allconfigs = generic_configs + extra_configs for c in allconfigs : name = c [ 'name' ] for v in c [ 'variables' ] : try : c [ 'defaults' ] [ v ] [ 'value' ] = config [ name ] [ v ] except : continue for c in allconfigs : print ( "" ) print ( c [ 'description' ] ) print ( "==================" ) if len ( c [ 'variables' ] ) == 0 : print ( "Nothing to do. Enabled by default" ) continue name = c [ 'name' ] config [ name ] = { } config [ name ] [ 'nature' ] = c [ 'nature' ] for v in c [ 'variables' ] : value = '' description = v + " " helptext = "" validator = None if v in c [ 'defaults' ] : value = c [ 'defaults' ] [ v ] . get ( 'value' , '' ) helptext = c [ 'defaults' ] [ v ] . get ( "description" , "" ) validator = c [ 'defaults' ] [ v ] . get ( 'validator' , None ) if helptext != "" : description += "(" + helptext + ")" while True : choice = input_with_default ( description , value ) if validator is not None : if validator . is_valid ( choice ) : break else : print ( "Invalid input. Expected input is {}" . format ( validator . message ) ) else : break config [ name ] [ v ] = choice if v == 'enable' and choice == 'n' : break with open ( profileini , 'w' ) as fd : config . write ( fd ) print ( "Updated profile file:" , config )
7432	def _write_nex ( self , mdict , nlocus ) : max_name_len = max ( [ len ( i ) for i in mdict ] ) namestring = "{:<" + str ( max_name_len + 1 ) + "} {}\n" matrix = "" for i in mdict . items ( ) : matrix += namestring . format ( i [ 0 ] , i [ 1 ] ) minidir = os . path . realpath ( os . path . join ( self . workdir , self . name ) ) if not os . path . exists ( minidir ) : os . makedirs ( minidir ) handle = os . path . join ( minidir , "{}.nex" . format ( nlocus ) ) with open ( handle , 'w' ) as outnex : outnex . write ( NEXBLOCK . format ( ** { "ntax" : len ( mdict ) , "nchar" : len ( mdict . values ( ) [ 0 ] ) , "matrix" : matrix , "ngen" : self . params . mb_mcmc_ngen , "sfreq" : self . params . mb_mcmc_sample_freq , "burnin" : self . params . mb_mcmc_burnin , } ) )
1765	def pop_int ( self , force = False ) : value = self . read_int ( self . STACK , force = force ) self . STACK += self . address_bit_size // 8 return value
1943	def protect_memory_callback ( self , start , size , perms ) : logger . info ( f"Changing permissions on {hex(start)}:{hex(start + size)} to {perms}" ) self . _emu . mem_protect ( start , size , convert_permissions ( perms ) )
7398	def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) bib [ - 1 ] [ key ] = value return bib
6571	def register_chooser ( self , chooser , ** kwargs ) : if not issubclass ( chooser , Chooser ) : return self . register_simple_chooser ( chooser , ** kwargs ) self . choosers [ chooser . model ] = chooser ( ** kwargs ) return chooser
7673	def trim ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'trimming can be performed.' ) if not ( 0 <= start_time <= end_time <= float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_trimmed = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_trimmed . annotations = self . annotations . trim ( start_time , end_time , strict = strict ) if 'trim' not in jam_trimmed . sandbox . keys ( ) : jam_trimmed . sandbox . update ( trim = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_trimmed . sandbox . trim . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_trimmed
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
3767	def zs_to_ws ( zs , MWs ) : r Mavg = sum ( zi * MWi for zi , MWi in zip ( zs , MWs ) ) ws = [ zi * MWi / Mavg for zi , MWi in zip ( zs , MWs ) ] return ws
7675	def pprint_jobject ( obj , ** kwargs ) : obj_simple = { k : v for k , v in six . iteritems ( obj . __json__ ) if v } string = json . dumps ( obj_simple , ** kwargs ) string = re . sub ( r'[{}"]' , '' , string ) string = re . sub ( r',\n' , '\n' , string ) string = re . sub ( r'^\s*$' , '' , string ) return string
2857	def read ( self , length ) : if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
13846	def __get_numbered_paths ( filepath ) : format = '%s (%%d)%s' % splitext_files_only ( filepath ) return map ( lambda n : format % n , itertools . count ( 1 ) )
13144	def pack_triples_numpy ( triples ) : if len ( triples ) == 0 : return np . array ( [ ] , dtype = np . int64 ) return np . stack ( list ( map ( _transform_triple_numpy , triples ) ) , axis = 0 )
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : try : if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) self . _set_vdbconfig_path ( ) if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : self . _restore_vdbconfig_path ( ) sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : raise IPyradWarningExit ( "Download failed. Exiting." ) for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) shutil . rmtree ( sradir ) if ipyclient : try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass if not ipyclient . outstanding : ipyclient . purge_everything ( ) else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
6670	def task ( * args , ** kwargs ) : precursors = kwargs . pop ( 'precursors' , None ) post_callback = kwargs . pop ( 'post_callback' , False ) if args and callable ( args [ 0 ] ) : return _task ( * args ) def wrapper ( meth ) : if precursors : meth . deploy_before = list ( precursors ) if post_callback : meth . is_post_callback = True return _task ( meth ) return wrapper
1886	def _hook_xfer_mem ( self , uc , access , address , size , value , data ) : assert access in ( UC_MEM_WRITE , UC_MEM_READ , UC_MEM_FETCH ) if access == UC_MEM_WRITE : self . _cpu . write_int ( address , value , size * 8 ) elif access == UC_MEM_READ : value = self . _cpu . read_bytes ( address , size ) if address in self . _should_be_written : return True self . _should_be_written [ address ] = value self . _should_try_again = True return False return True
6141	def in_out_check ( self ) : devices = available_devices ( ) if not self . in_idx in devices : raise OSError ( "Input device is unavailable" ) in_check = devices [ self . in_idx ] if not self . out_idx in devices : raise OSError ( "Output device is unavailable" ) out_check = devices [ self . out_idx ] if ( ( in_check [ 'inputs' ] == 0 ) and ( out_check [ 'outputs' ] == 0 ) ) : raise StandardError ( 'Invalid input and output devices' ) elif ( in_check [ 'inputs' ] == 0 ) : raise ValueError ( 'Selected input device has no inputs' ) elif ( out_check [ 'outputs' ] == 0 ) : raise ValueError ( 'Selected output device has no outputs' ) return True
8525	def log_callback ( wrapped_function ) : def debug_log ( message ) : logger . debug ( message . encode ( 'unicode_escape' ) . decode ( ) ) @ functools . wraps ( wrapped_function ) def _wrapper ( parser , match , ** kwargs ) : func_name = wrapped_function . __name__ debug_log ( u'{func_name} <- {matched_string}' . format ( func_name = func_name , matched_string = match . group ( ) , ) ) try : result = wrapped_function ( parser , match , ** kwargs ) except IgnoredMatchException : debug_log ( u'{func_name} -> IGNORED' . format ( func_name = func_name ) ) raise debug_log ( u'{func_name} -> {result}' . format ( func_name = func_name , result = result , ) ) return result return _wrapper
3075	def callback_view ( self ) : if 'error' in request . args : reason = request . args . get ( 'error_description' , request . args . get ( 'error' , '' ) ) reason = markupsafe . escape ( reason ) return ( 'Authorization failed: {0}' . format ( reason ) , httplib . BAD_REQUEST ) try : encoded_state = request . args [ 'state' ] server_csrf = session [ _CSRF_KEY ] code = request . args [ 'code' ] except KeyError : return 'Invalid request' , httplib . BAD_REQUEST try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return 'Invalid request state' , httplib . BAD_REQUEST if client_csrf != server_csrf : return 'Invalid request state' , httplib . BAD_REQUEST flow = _get_flow_for_token ( server_csrf ) if flow is None : return 'Invalid request state' , httplib . BAD_REQUEST try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : current_app . logger . exception ( exchange_error ) content = 'An error occurred: {0}' . format ( exchange_error ) return content , httplib . BAD_REQUEST self . storage . put ( credentials ) if self . authorize_callback : self . authorize_callback ( credentials ) return redirect ( return_url )
11858	def make_factor ( var , e , bn ) : node = bn . variable_node ( var ) vars = [ X for X in [ var ] + node . parents if X not in e ] cpt = dict ( ( event_values ( e1 , vars ) , node . p ( e1 [ var ] , e1 ) ) for e1 in all_events ( vars , bn , e ) ) return Factor ( vars , cpt )
10381	def _get_drug_target_interactions ( manager : Optional [ 'bio2bel_drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel_drugbank manager = bio2bel_drugbank . Manager ( ) if not manager . is_populated ( ) : manager . populate ( ) return manager . get_drug_to_hgnc_symbols ( )
10442	def getobjectlist ( self , window_name ) : try : window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) except atomac . _a11y . ErrorInvalidUIElement : self . _windows = { } window_handle , name , app = self . _get_window_handle ( window_name , True ) object_list = self . _get_appmap ( window_handle , name , True ) return object_list . keys ( )
5112	def animate ( self , out = None , t = None , line_kwargs = None , scatter_kwargs = None , ** kwargs ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if not HAS_MATPLOTLIB : msg = "Matplotlib is necessary to animate a simulation." raise ImportError ( msg ) self . _update_all_colors ( ) kwargs . setdefault ( 'bgcolor' , self . colors [ 'bgcolor' ] ) fig = plt . figure ( figsize = kwargs . get ( 'figsize' , ( 7 , 7 ) ) ) ax = fig . gca ( ) mpl_kwargs = { 'line_kwargs' : line_kwargs , 'scatter_kwargs' : scatter_kwargs , 'pos' : kwargs . get ( 'pos' ) } line_args , scat_args = self . g . lines_scatter_args ( ** mpl_kwargs ) lines = LineCollection ( ** line_args ) lines = ax . add_collection ( lines ) scatt = ax . scatter ( ** scat_args ) t = np . infty if t is None else t now = self . _t def update ( frame_number ) : if t is not None : if self . _t > now + t : return False self . _simulate_next_event ( slow = True ) lines . set_color ( line_args [ 'colors' ] ) scatt . set_edgecolors ( scat_args [ 'edgecolors' ] ) scatt . set_facecolor ( scat_args [ 'c' ] ) if hasattr ( ax , 'set_facecolor' ) : ax . set_facecolor ( kwargs [ 'bgcolor' ] ) else : ax . set_axis_bgcolor ( kwargs [ 'bgcolor' ] ) ax . get_xaxis ( ) . set_visible ( False ) ax . get_yaxis ( ) . set_visible ( False ) animation_args = { 'fargs' : None , 'event_source' : None , 'init_func' : None , 'frames' : None , 'blit' : False , 'interval' : 10 , 'repeat' : None , 'func' : update , 'repeat_delay' : None , 'fig' : fig , 'save_count' : None , } for key , value in kwargs . items ( ) : if key in animation_args : animation_args [ key ] = value animation = FuncAnimation ( ** animation_args ) if 'filename' not in kwargs : plt . ioff ( ) plt . show ( ) else : save_args = { 'filename' : None , 'writer' : None , 'fps' : None , 'dpi' : None , 'codec' : None , 'bitrate' : None , 'extra_args' : None , 'metadata' : None , 'extra_anim' : None , 'savefig_kwargs' : None } for key , value in kwargs . items ( ) : if key in save_args : save_args [ key ] = value animation . save ( ** save_args )
9976	def alter_freevars ( func , globals_ = None , ** vars ) : if globals_ is None : globals_ = func . __globals__ frees = tuple ( vars . keys ( ) ) oldlocs = func . __code__ . co_names newlocs = tuple ( name for name in oldlocs if name not in frees ) code = _alter_code ( func . __code__ , co_freevars = frees , co_names = newlocs , co_flags = func . __code__ . co_flags | inspect . CO_NESTED ) closure = _create_closure ( * vars . values ( ) ) return FunctionType ( code , globals_ , closure = closure )
2095	def monitor ( self , pk , parent_pk = None , timeout = None , interval = 0.5 , outfile = sys . stdout , ** kwargs ) : if pk is None : pk = self . last_job_data ( parent_pk , ** kwargs ) [ 'id' ] job_endpoint = '%s%s/' % ( self . unified_job_type , pk ) self . wait ( pk , exit_on = [ 'running' , 'successful' ] , outfile = outfile ) start = time . time ( ) start_line = 0 result = client . get ( job_endpoint ) . json ( ) click . echo ( '\033[0;91m------Starting Standard Out Stream------\033[0m' , nl = 2 , file = outfile ) while not result [ 'failed' ] and result [ 'status' ] != 'successful' : result = client . get ( job_endpoint ) . json ( ) time . sleep ( interval ) content = self . lookup_stdout ( pk , start_line , full = False ) if not content . startswith ( "Waiting for results" ) : line_count = len ( content . splitlines ( ) ) start_line += line_count click . echo ( content , nl = 0 , file = outfile ) if timeout and time . time ( ) - start > timeout : raise exc . Timeout ( 'Monitoring aborted due to timeout.' ) if self . endpoint == '/workflow_jobs/' : click . echo ( self . lookup_stdout ( pk , start_line , full = True ) , nl = 1 ) click . echo ( '\033[0;91m------End of Standard Out Stream--------\033[0m' , nl = 2 , file = outfile ) if result [ 'failed' ] : raise exc . JobFailure ( 'Job failed.' ) answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , pk ) ) ) answer . update ( result ) if parent_pk : answer [ 'id' ] = parent_pk else : answer [ 'id' ] = pk return answer
8709	def write_lines ( self , data ) : lines = data . replace ( '\r' , '' ) . split ( '\n' ) for line in lines : self . __exchange ( line )
5753	def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e else : raise e else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
8773	def get_lswitch_ids_for_network ( self , context , network_id ) : lswitches = self . _lswitches_for_network ( context , network_id ) . results ( ) return [ s [ 'uuid' ] for s in lswitches [ "results" ] ]
1743	def save_image ( tensor , filename , nrow = 8 , padding = 2 , pad_value = 0 ) : from PIL import Image grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value ) im = Image . fromarray ( pre_pillow_float_img_process ( grid ) ) im . save ( filename )
5416	def get_dstat_provider_args ( provider , project ) : provider_name = get_provider_name ( provider ) args = [ ] if provider_name == 'google' : args . append ( '--project %s' % project ) elif provider_name == 'google-v2' : args . append ( '--project %s' % project ) elif provider_name == 'local' : pass elif provider_name == 'test-fails' : pass else : assert False , 'Provider %s needs get_dstat_provider_args support' % provider args . insert ( 0 , '--provider %s' % provider_name ) return ' ' . join ( args )
8160	def remove ( self , id , operator = "=" , key = None ) : if key == None : key = self . _key try : id = unicode ( id ) except : pass sql = "delete from " + self . _name + " where " + key + " " + operator + " ?" self . _db . _cur . execute ( sql , ( id , ) )
7425	def bedtools_merge ( data , sample ) : LOGGER . info ( "Entering bedtools_merge: %s" , sample . name ) mappedreads = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) cmd1 = [ ipyrad . bins . bedtools , "bamtobed" , "-i" , mappedreads ] cmd2 = [ ipyrad . bins . bedtools , "merge" , "-i" , "-" ] if 'pair' in data . paramsdict [ "datatype" ] : check_insert_size ( data , sample ) cmd2 . insert ( 2 , str ( data . _hackersonly [ "max_inner_mate_distance" ] ) ) cmd2 . insert ( 2 , "-d" ) else : cmd2 . insert ( 2 , str ( - 1 * data . _hackersonly [ "min_SE_refmap_overlap" ] ) ) cmd2 . insert ( 2 , "-d" ) LOGGER . info ( "stdv: bedtools merge cmds: %s %s" , cmd1 , cmd2 ) proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE , stdin = proc1 . stdout ) result = proc2 . communicate ( ) [ 0 ] proc1 . stdout . close ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in %s: %s" , cmd2 , result ) if os . path . exists ( ipyrad . __debugflag__ ) : with open ( os . path . join ( data . dirs . refmapping , sample . name + ".bed" ) , 'w' ) as outfile : outfile . write ( result ) nregions = len ( result . strip ( ) . split ( "\n" ) ) LOGGER . info ( "bedtools_merge: Got # regions: %s" , nregions ) return result
6394	def sim_levenshtein ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 1 , 1 ) ) : return Levenshtein ( ) . sim ( src , tar , mode , cost )
11737	def route ( bp , * args , ** kwargs ) : kwargs [ 'strict_slashes' ] = kwargs . pop ( 'strict_slashes' , False ) body = _validate_schema ( kwargs . pop ( '_body' , None ) ) query = _validate_schema ( kwargs . pop ( '_query' , None ) ) output = _validate_schema ( kwargs . pop ( 'marshal_with' , None ) ) validate = kwargs . pop ( 'validate' , True ) def decorator ( f ) : @ bp . route ( * args , ** kwargs ) @ wraps ( f ) def wrapper ( * inner_args , ** inner_kwargs ) : try : if query is not None : query . strict = validate url = furl ( request . url ) inner_kwargs [ '_query' ] = query . load ( data = url . args ) if body is not None : body . strict = validate json_data = request . get_json ( ) if json_data is None : json_data = { } inner_kwargs [ '_body' ] = body . load ( data = json_data ) except ValidationError as err : return jsonify ( err . messages ) , 422 if output : data = output . dump ( f ( * inner_args , ** inner_kwargs ) ) return jsonify ( data [ 0 ] ) return f ( * inner_args , ** inner_kwargs ) return f return decorator
4416	async def play ( self , track_index : int = 0 , ignore_shuffle : bool = False ) : if self . repeat and self . current : self . queue . append ( self . current ) self . previous = self . current self . current = None self . position = 0 self . paused = False if not self . queue : await self . stop ( ) await self . _lavalink . dispatch_event ( QueueEndEvent ( self ) ) else : if self . shuffle and not ignore_shuffle : track = self . queue . pop ( randrange ( len ( self . queue ) ) ) else : track = self . queue . pop ( min ( track_index , len ( self . queue ) - 1 ) ) self . current = track await self . _lavalink . ws . send ( op = 'play' , guildId = self . guild_id , track = track . track ) await self . _lavalink . dispatch_event ( TrackStartEvent ( self , track ) )
3431	def remove_reactions ( self , reactions , remove_orphans = False ) : if isinstance ( reactions , string_types ) or hasattr ( reactions , "id" ) : warn ( "need to pass in a list" ) reactions = [ reactions ] context = get_context ( self ) for reaction in reactions : try : reaction = self . reactions [ self . reactions . index ( reaction ) ] except ValueError : warn ( '%s not in %s' % ( reaction , self ) ) else : forward = reaction . forward_variable reverse = reaction . reverse_variable if context : obj_coef = reaction . objective_coefficient if obj_coef != 0 : context ( partial ( self . solver . objective . set_linear_coefficients , { forward : obj_coef , reverse : - obj_coef } ) ) context ( partial ( self . _populate_solver , [ reaction ] ) ) context ( partial ( setattr , reaction , '_model' , self ) ) context ( partial ( self . reactions . add , reaction ) ) self . remove_cons_vars ( [ forward , reverse ] ) self . reactions . remove ( reaction ) reaction . _model = None for met in reaction . _metabolites : if reaction in met . _reaction : met . _reaction . remove ( reaction ) if context : context ( partial ( met . _reaction . add , reaction ) ) if remove_orphans and len ( met . _reaction ) == 0 : self . remove_metabolites ( met ) for gene in reaction . _genes : if reaction in gene . _reaction : gene . _reaction . remove ( reaction ) if context : context ( partial ( gene . _reaction . add , reaction ) ) if remove_orphans and len ( gene . _reaction ) == 0 : self . genes . remove ( gene ) if context : context ( partial ( self . genes . add , gene ) ) associated_groups = self . get_associated_groups ( reaction ) for group in associated_groups : group . remove_members ( reaction )
9083	def get_by_uri ( self , uri ) : if not is_uri ( uri ) : raise ValueError ( '%s is not a valid URI.' % uri ) csuris = [ csuri for csuri in self . concept_scheme_uri_map . keys ( ) if uri . startswith ( csuri ) ] for csuri in csuris : c = self . get_provider ( csuri ) . get_by_uri ( uri ) if c : return c for p in self . providers . values ( ) : c = p . get_by_uri ( uri ) if c : return c return False
8920	def _get_request_type ( self ) : value = self . document . tag . lower ( ) if value in allowed_request_types [ self . params [ 'service' ] ] : self . params [ "request" ] = value else : raise OWSInvalidParameterValue ( "Request type %s is not supported" % value , value = "request" ) return self . params [ "request" ]
5581	def _get_contour_values ( min_val , max_val , base = 0 , interval = 100 ) : i = base out = [ ] if min_val < base : while i >= min_val : i -= interval while i <= max_val : if i >= min_val : out . append ( i ) i += interval return out
1547	def configure ( level = logging . INFO , logfile = None ) : for handler in Log . handlers : if isinstance ( handler , logging . StreamHandler ) : Log . handlers . remove ( handler ) Log . setLevel ( level ) if logfile is not None : log_format = "[%(asctime)s] [%(levelname)s]: %(message)s" formatter = logging . Formatter ( fmt = log_format , datefmt = date_format ) file_handler = logging . FileHandler ( logfile ) file_handler . setFormatter ( formatter ) Log . addHandler ( file_handler ) else : log_format = "[%(asctime)s] %(log_color)s[%(levelname)s]%(reset)s: %(message)s" formatter = colorlog . ColoredFormatter ( fmt = log_format , datefmt = date_format ) stream_handler = logging . StreamHandler ( ) stream_handler . setFormatter ( formatter ) Log . addHandler ( stream_handler )
12681	def copy_attributes ( source , destination , ignore_patterns = [ ] ) : for attr in _wildcard_filter ( dir ( source ) , * ignore_patterns ) : setattr ( destination , attr , getattr ( source , attr ) )
12827	def add_data ( self , data ) : if not self . _data : self . _data = { } self . _data . update ( data )
5481	def retry_api_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in TRANSIENT_HTTP_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , socket . error ) : if exception . errno in TRANSIENT_SOCKET_ERROR_CODES : _print_error ( 'Retrying...' ) return True if isinstance ( exception , oauth2client . client . AccessTokenRefreshError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , SSLError ) : _print_error ( 'Retrying...' ) return True if isinstance ( exception , ServerNotFoundError ) : _print_error ( 'Retrying...' ) return True return False
13464	def add_memory ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) form = MemoryForm ( request . POST or None , request . FILES or None ) if form . is_valid ( ) : instance = form . save ( commit = False ) instance . user = request . user instance . event = event instance . save ( ) msg = "Your thoughts were added. " if request . FILES : photo_list = request . FILES . getlist ( 'photos' ) photo_count = len ( photo_list ) for upload_file in photo_list : process_upload ( upload_file , instance , form , event , request ) if photo_count > 1 : msg += "{} images were added and should appear soon." . format ( photo_count ) else : msg += "{} image was added and should appear soon." . format ( photo_count ) messages . success ( request , msg ) return HttpResponseRedirect ( '../' ) return render ( request , 'happenings/add_memories.html' , { 'form' : form , 'event' : event } )
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
11310	def get_object ( self , url , month_format = '%b' , day_format = '%d' ) : params = self . get_params ( url ) try : year = params [ self . _meta . year_part ] month = params [ self . _meta . month_part ] day = params [ self . _meta . day_part ] except KeyError : try : year , month , day = params [ '_0' ] , params [ '_1' ] , params [ '_2' ] except KeyError : raise OEmbedException ( 'Error extracting date from url parameters' ) try : tt = time . strptime ( '%s-%s-%s' % ( year , month , day ) , '%s-%s-%s' % ( '%Y' , month_format , day_format ) ) date = datetime . date ( * tt [ : 3 ] ) except ValueError : raise OEmbedException ( 'Error parsing date from: %s' % url ) if isinstance ( self . _meta . model . _meta . get_field ( self . _meta . date_field ) , DateTimeField ) : min_date = datetime . datetime . combine ( date , datetime . time . min ) max_date = datetime . datetime . combine ( date , datetime . time . max ) query = { '%s__range' % self . _meta . date_field : ( min_date , max_date ) } else : query = { self . _meta . date_field : date } for key , value in self . _meta . fields_to_match . iteritems ( ) : try : query [ value ] = params [ key ] except KeyError : raise OEmbedException ( '%s was not found in the urlpattern parameters. Valid names are: %s' % ( key , ', ' . join ( params . keys ( ) ) ) ) try : obj = self . get_queryset ( ) . get ( ** query ) except self . _meta . model . DoesNotExist : raise OEmbedException ( 'Requested object not found' ) return obj
3107	def _retrieve_info ( self , http ) : if self . invalid : info = _metadata . get_service_account_info ( http , service_account = self . service_account_email or 'default' ) self . invalid = False self . service_account_email = info [ 'email' ] self . scopes = info [ 'scopes' ]
8018	async def disconnect ( self , code ) : try : await asyncio . wait ( self . application_futures . values ( ) , return_when = asyncio . ALL_COMPLETED , timeout = self . application_close_timeout ) except asyncio . TimeoutError : pass
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
1835	def JRCXZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . RCX == 0 , target . read ( ) , cpu . PC )
5737	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-shared' . format ( PUBSUB_OBJECT_PREFIX , self . name ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating shared subscription {}" . format ( subscription_name ) ) try : self . subscriber_client . create_subscription ( subscription_path , topic = topic_path ) except google . cloud . exceptions . Conflict : pass return subscription_path
3643	def quickSell ( self , item_id ) : method = 'DELETE' url = 'item' if not isinstance ( item_id , ( list , tuple ) ) : item_id = ( item_id , ) item_id = ( str ( i ) for i in item_id ) params = { 'itemIds' : ',' . join ( item_id ) } self . __request__ ( method , url , params = params ) return True
4795	def does_not_contain_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) else : found = [ ] for v in values : if v in self . val . values ( ) : found . append ( v ) if found : self . _err ( 'Expected <%s> to not contain values %s, but did contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( found ) ) ) return self
4170	def zpk2ss ( z , p , k ) : import scipy . signal return scipy . signal . zpk2ss ( z , p , k )
4121	def twosided_2_centerdc ( data ) : N = len ( data ) newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
4098	def FPE ( N , rho , k = None ) : r fpe = rho * ( N + k + 1. ) / ( N - k - 1 ) return fpe
5096	def refresh_persistent_maps ( self ) : for robot in self . _robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/persistent_maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _persistent_maps . update ( { robot . serial : resp2 . json ( ) } )
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , ** kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , ** kwargs )
336	def run_model ( model , returns_train , returns_test = None , bmark = None , samples = 500 , ppc = False , progressbar = True ) : if model == 'alpha_beta' : model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model == 't' : model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model == 'normal' : model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model == 'best' : model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else : raise NotImplementedError ( 'Model {} not found.' 'Use alpha_beta, t, normal, or best.' . format ( model ) ) if ppc : ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ 'returns' ] return trace
9042	def eigh ( self ) : from numpy . linalg import svd if self . _cache [ "eig" ] is not None : return self . _cache [ "eig" ] U , S = svd ( self . L ) [ : 2 ] S *= S S += self . _epsilon self . _cache [ "eig" ] = S , U return self . _cache [ "eig" ]
1294	def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
7940	def _got_srv ( self , addrs ) : with self . lock : if not addrs : self . _dst_service = None if self . _dst_port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] else : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Could not resolve SRV for service {0!r}" " on host {1!r} and fallback port number not given" . format ( self . _dst_service , self . _dst_name ) ) elif addrs == [ ( "." , 0 ) ] : self . _dst_nameports = [ ] self . _set_state ( "aborted" ) raise DNSError ( "Service {0!r} not available on host {1!r}" . format ( self . _dst_service , self . _dst_name ) ) else : self . _dst_nameports = addrs self . _set_state ( "resolve-hostname" )
11603	def parse_byteranges ( cls , environ ) : r = [ ] s = environ . get ( cls . header_range , '' ) . replace ( ' ' , '' ) . lower ( ) if s : l = s . split ( '=' ) if len ( l ) == 2 : unit , vals = tuple ( l ) if unit == 'bytes' and vals : gen_rng = ( tuple ( rng . split ( '-' ) ) for rng in vals . split ( ',' ) if '-' in rng ) for start , end in gen_rng : if start or end : r . append ( ( int ( start ) if start else None , int ( end ) if end else None ) ) return r
11773	def information_content ( values ) : "Number of bits to represent the probability distribution in values." probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
4318	def _stat_call ( filepath ) : validate_input_file ( filepath ) args = [ 'sox' , filepath , '-n' , 'stat' ] _ , _ , stat_output = sox ( args ) return stat_output
12457	def iteritems ( data , ** kwargs ) : return iter ( data . items ( ** kwargs ) ) if IS_PY3 else data . iteritems ( ** kwargs )
8485	def load ( self , clear = False ) : if clear : self . settings = { } defer = [ ] for conf in pkg_resources . iter_entry_points ( 'pyconfig' ) : if conf . attrs : raise RuntimeError ( "config must be a module" ) mod_name = conf . module_name base_name = conf . name if conf . name != 'any' else None log . info ( "Loading module '%s'" , mod_name ) mod_dict = runpy . run_module ( mod_name ) if mod_dict . get ( 'deferred' , None ) is deferred : log . info ( "Deferring module '%s'" , mod_name ) mod_dict . pop ( 'deferred' ) defer . append ( ( mod_name , base_name , mod_dict ) ) continue self . _update ( mod_dict , base_name ) for mod_name , base_name , mod_dict in defer : log . info ( "Loading deferred module '%s'" , mod_name ) self . _update ( mod_dict , base_name ) if etcd ( ) . configured : mod_dict = etcd ( ) . load ( ) if mod_dict : self . _update ( mod_dict ) mod_dict = None try : mod_dict = runpy . run_module ( 'localconfig' ) except ImportError : pass except ValueError as err : if getattr ( err , 'message' ) != '__package__ set to non-string' : raise mod_name = 'localconfig' if sys . version_info < ( 2 , 7 ) : loader , code , fname = runpy . _get_module_details ( mod_name ) else : _ , loader , code , fname = runpy . _get_module_details ( mod_name ) mod_dict = runpy . _run_code ( code , { } , { } , mod_name , fname , loader , pkg_name = None ) if mod_dict : log . info ( "Loading module 'localconfig'" ) self . _update ( mod_dict ) self . call_reload_hooks ( )
7544	def run ( data , samples , force , ipyclient ) : data . dirs . consens = os . path . join ( data . dirs . project , data . name + "_consens" ) if not os . path . exists ( data . dirs . consens ) : os . mkdir ( data . dirs . consens ) tmpcons = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcats = glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpfile in tmpcons + tmpcats : os . remove ( tmpfile ) samples = get_subsamples ( data , samples , force ) lbview = ipyclient . load_balanced_view ( ) data . cpus = data . _ipcluster [ "cores" ] if not data . cpus : data . cpus = len ( ipyclient . ids ) inst = "" try : samples = calculate_depths ( data , samples , lbview ) lasyncs = make_chunks ( data , samples , lbview ) process_chunks ( data , samples , lasyncs , lbview ) except KeyboardInterrupt as inst : raise inst finally : tmpcons = glob . glob ( os . path . join ( data . dirs . clusts , "tmp_*.[0-9]*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcons.*" ) ) tmpcons += glob . glob ( os . path . join ( data . dirs . consens , "*_tmpcats.*" ) ) for tmpchunk in tmpcons : os . remove ( tmpchunk ) data . _checkpoint = 0
2621	def security_group ( self , vpc ) : sg = vpc . create_security_group ( GroupName = "private-subnet" , Description = "security group for remote executors" ) ip_ranges = [ { 'CidrIp' : '10.0.0.0/16' } ] in_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'ICMP' , 'FromPort' : - 1 , 'ToPort' : - 1 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 22 , 'ToPort' : 22 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } ] out_permissions = [ { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : [ { 'CidrIp' : '0.0.0.0/0' } ] , } , { 'IpProtocol' : 'TCP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , { 'IpProtocol' : 'UDP' , 'FromPort' : 0 , 'ToPort' : 65535 , 'IpRanges' : ip_ranges , } , ] sg . authorize_ingress ( IpPermissions = in_permissions ) sg . authorize_egress ( IpPermissions = out_permissions ) self . sg_id = sg . id return sg
1430	def run ( command , parser , cl_args , unknown_args ) : Log . debug ( "Update Args: %s" , cl_args ) extra_lib_jars = jars . packing_jars ( ) action = "update topology%s" % ( ' in dry-run mode' if cl_args [ "dry_run" ] else '' ) dict_extra_args = { } try : dict_extra_args = build_extra_args_dict ( cl_args ) except Exception as err : return SimpleResult ( Status . InvocationError , err . message ) if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) else : list_extra_args = convert_args_dict_to_list ( dict_extra_args ) return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars )
10955	def get_update_io_tiles ( self , params , values ) : otile = self . get_update_tile ( params , values ) if otile is None : return [ None ] * 3 ptile = self . get_padding_size ( otile ) or util . Tile ( 0 , dim = otile . dim ) otile = util . Tile . intersection ( otile , self . oshape ) if ( otile . shape <= 0 ) . any ( ) : raise UpdateError ( "update triggered invalid tile size" ) if ( ptile . shape < 0 ) . any ( ) or ( ptile . shape > self . oshape . shape ) . any ( ) : raise UpdateError ( "update triggered invalid padding tile size" ) outer = otile . pad ( ( ptile . shape + 1 ) // 2 ) inner , outer = outer . reflect_overhang ( self . oshape ) iotile = inner . translate ( - outer . l ) outer = util . Tile . intersection ( outer , self . oshape ) inner = util . Tile . intersection ( inner , self . oshape ) return outer , inner , iotile
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
13004	def modify_input ( ) : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : objects = [ obj for obj in doc_mapper . get_pipe ( ) ] modified = modify_data ( objects ) for line in modified : obj = doc_mapper . line_to_object ( line ) obj . save ( ) print_success ( "Object(s) successfully changed" ) else : print_error ( "Please use this tool with pipes" )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
8612	def list_volumes ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/volumes?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8021	async def websocket_close ( self , message , stream_name ) : if stream_name in self . applications_accepting_frames : self . applications_accepting_frames . remove ( stream_name ) if self . closing : return if not self . applications_accepting_frames : await self . close ( message . get ( "code" ) )
10427	def boilerplate ( name , contact , description , pmids , version , copyright , authors , licenses , disclaimer , output ) : from . document_utils import write_boilerplate write_boilerplate ( name = name , version = version , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , pmids = pmids , file = output , )
10216	def to_jupyter ( graph : BELGraph , chart : Optional [ str ] = None ) -> Javascript : with open ( os . path . join ( HERE , 'render_with_javascript.js' ) , 'rt' ) as f : js_template = Template ( f . read ( ) ) return Javascript ( js_template . render ( ** _get_context ( graph , chart = chart ) ) )
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
2813	def convert_unsqueeze ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting unsqueeze ...' ) if names == 'short' : tf_name = 'UNSQ' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) def target_layer ( x ) : import keras return keras . backend . expand_dims ( x ) lambda_layer = keras . layers . Lambda ( target_layer , name = tf_name + 'E' ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
7978	def _post_auth ( self ) : ClientStream . _post_auth ( self ) if not self . initiator : self . unset_iq_get_handler ( "query" , "jabber:iq:auth" ) self . unset_iq_set_handler ( "query" , "jabber:iq:auth" )
311	def sortino_ratio ( returns , required_return = 0 , period = DAILY ) : return ep . sortino_ratio ( returns , required_return = required_return )
10925	def reset ( self , new_damping = None ) : self . _num_iter = 0 self . _inner_run_counter = 0 self . _J_update_counter = self . update_J_frequency self . _fresh_JTJ = False self . _has_run = False if new_damping is not None : self . damping = np . array ( new_damping ) . astype ( 'float' ) self . _set_err_paramvals ( )
3195	def delete ( self , list_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _delete ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) )
4270	def get_iptc_data ( filename ) : logger = logging . getLogger ( __name__ ) iptc_data = { } raw_iptc = { } try : img = _read_image ( filename ) raw_iptc = IptcImagePlugin . getiptcinfo ( img ) except SyntaxError : logger . info ( 'IPTC Error in %s' , filename ) if raw_iptc and ( 2 , 5 ) in raw_iptc : iptc_data [ "title" ] = raw_iptc [ ( 2 , 5 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 120 ) in raw_iptc : iptc_data [ "description" ] = raw_iptc [ ( 2 , 120 ) ] . decode ( 'utf-8' , errors = 'replace' ) if raw_iptc and ( 2 , 105 ) in raw_iptc : iptc_data [ "headline" ] = raw_iptc [ ( 2 , 105 ) ] . decode ( 'utf-8' , errors = 'replace' ) return iptc_data
2345	def forward ( self , x ) : features = self . conv ( x ) . mean ( dim = 2 ) return self . dense ( features )
9883	def inquire ( self ) : name = copy . deepcopy ( self . fname ) stats = fortran_cdf . inquire ( name ) status = stats [ 0 ] if status == 0 : self . _num_dims = stats [ 1 ] self . _dim_sizes = stats [ 2 ] self . _encoding = stats [ 3 ] self . _majority = stats [ 4 ] self . _max_rec = stats [ 5 ] self . _num_r_vars = stats [ 6 ] self . _num_z_vars = stats [ 7 ] self . _num_attrs = stats [ 8 ] else : raise IOError ( fortran_cdf . statusreporter ( status ) )
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
6689	def groupuninstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupremove "%(group)s"' % locals ( ) )
2349	def open ( self ) : if self . seed_url : self . driver_adapter . open ( self . seed_url ) self . wait_for_page_to_load ( ) return self raise UsageError ( "Set a base URL or URL_TEMPLATE to open this page." )
13512	def is_colour ( value ) : global PREDEFINED , HEX_MATCH , RGB_MATCH , RGBA_MATCH , HSL_MATCH , HSLA_MATCH value = value . strip ( ) if HEX_MATCH . match ( value ) or RGB_MATCH . match ( value ) or RGBA_MATCH . match ( value ) or HSL_MATCH . match ( value ) or HSLA_MATCH . match ( value ) or value in PREDEFINED : return True return False
1480	def _start_processes ( self , commands ) : Log . info ( "Start processes" ) processes_to_monitor = { } for ( name , command ) in commands . items ( ) : p = self . _run_process ( name , command ) processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) log_pid_for_process ( name , p . pid ) with self . process_lock : self . processes_to_monitor . update ( processes_to_monitor )
1916	def get ( self ) : if self . is_shutdown ( ) : return None while len ( self . _states ) == 0 : if self . running == 0 : return None if self . is_shutdown ( ) : return None logger . debug ( "Waiting for available states" ) self . _lock . wait ( ) state_id = self . _policy . choice ( list ( self . _states ) ) if state_id is None : return None del self . _states [ self . _states . index ( state_id ) ] return state_id
574	def clippedObj ( obj , maxElementSize = 64 ) : if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
5974	def isMine ( self , scriptname ) : suffix = os . path . splitext ( scriptname ) [ 1 ] . lower ( ) if suffix . startswith ( '.' ) : suffix = suffix [ 1 : ] return self . suffix == suffix
801	def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : assert len ( fields ) >= 1 , "fields is empty" with ConnectionFactory . get ( ) as conn : dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] dbFieldStr = ", " . join ( dbFields ) query = 'SELECT model_id, {fields} from {models}' ' WHERE job_id=%s AND model_checkpoint_id IS NOT NULL' . format ( fields = dbFieldStr , models = self . modelsTableName ) conn . cursor . execute ( query , [ jobID ] ) rows = conn . cursor . fetchall ( ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
9902	def with_data ( path , data ) : if isinstance ( data , str ) : data = json . loads ( data ) if os . path . exists ( path ) : raise ValueError ( "File exists, not overwriting data. Set the " "'data' attribute on a normally-initialized " "'livejson.File' instance if you really " "want to do this." ) else : f = File ( path ) f . data = data return f
10815	def invite_by_emails ( self , emails ) : assert emails is None or isinstance ( emails , list ) results = [ ] for email in emails : try : user = User . query . filter_by ( email = email ) . one ( ) results . append ( self . invite ( user ) ) except NoResultFound : results . append ( None ) return results
1292	def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : return self . demo_memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
6461	def _ends_in_cvc ( self , term ) : return len ( term ) > 2 and ( term [ - 1 ] not in self . _vowels and term [ - 2 ] in self . _vowels and term [ - 3 ] not in self . _vowels and term [ - 1 ] not in tuple ( 'wxY' ) )
4308	def _validate_sample_rates ( input_filepath_list , combine_type ) : sample_rates = [ file_info . sample_rate ( f ) for f in input_filepath_list ] if not core . all_equal ( sample_rates ) : raise IOError ( "Input files do not have the same sample rate. The {} combine " "type requires that all files have the same sample rate" . format ( combine_type ) )
7276	def set_position ( self , position ) : self . _player_interface . SetPosition ( ObjectPath ( "/not/used" ) , Int64 ( position * 1000.0 * 1000 ) ) self . positionEvent ( self , position )
1343	def onehot_like ( a , index , value = 1 ) : x = np . zeros_like ( a ) x [ index ] = value return x
7890	def get_user ( self , nick_or_jid , create = False ) : if isinstance ( nick_or_jid , JID ) : if not nick_or_jid . resource : return None for u in self . users . values ( ) : if nick_or_jid in ( u . room_jid , u . real_jid ) : return u if create : return MucRoomUser ( nick_or_jid ) else : return None return self . users . get ( nick_or_jid )
7904	def join ( self , room , nick , handler , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if not room . node or room . resource : raise ValueError ( "Invalid room JID" ) room_jid = JID ( room . node , room . domain , nick ) cur_rs = self . rooms . get ( room_jid . bare ( ) . as_unicode ( ) ) if cur_rs and cur_rs . joined : raise RuntimeError ( "Room already joined" ) rs = MucRoomState ( self , self . stream . me , room_jid , handler ) self . rooms [ room_jid . bare ( ) . as_unicode ( ) ] = rs rs . join ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) return rs
9964	def update_lazyevals ( self ) : if self . lazy_evals is None : return elif isinstance ( self . lazy_evals , LazyEval ) : self . lazy_evals . get_updated ( ) else : for lz in self . lazy_evals : lz . get_updated ( )
12821	def _str_to_path ( s , result_type ) : assert isinstance ( s , str ) if isinstance ( s , bytes ) and result_type is text_type : return s . decode ( 'ascii' ) elif isinstance ( s , text_type ) and result_type is bytes : return s . encode ( 'ascii' ) return s
7893	def leave ( self ) : if self . joined : p = MucPresence ( to_jid = self . room_jid , stanza_type = "unavailable" ) self . manager . stream . send ( p )
2457	def set_pkg_license_declared ( self , doc , lic ) : self . assert_package_exists ( ) if not self . package_license_declared_set : self . package_license_declared_set = True if validations . validate_lics_conc ( lic ) : doc . package . license_declared = lic return True else : raise SPDXValueError ( 'Package::LicenseDeclared' ) else : raise CardinalityError ( 'Package::LicenseDeclared' )
3320	def refresh ( self , token , timeout ) : assert token in self . _dict , "Lock must exist" assert timeout == - 1 or timeout > 0 if timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX self . _lock . acquire_write ( ) try : lock = self . _dict [ token ] lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout self . _dict [ token ] = lock self . _flush ( ) finally : self . _lock . release ( ) return lock
2221	def _rectify_hasher ( hasher ) : if xxhash is not None : if hasher in { 'xxh32' , 'xx32' , 'xxhash' } : return xxhash . xxh32 if hasher in { 'xxh64' , 'xx64' } : return xxhash . xxh64 if hasher is NoParam or hasher == 'default' : hasher = DEFAULT_HASHER elif isinstance ( hasher , six . string_types ) : if hasher not in hashlib . algorithms_available : raise KeyError ( 'unknown hasher: {}' . format ( hasher ) ) else : hasher = getattr ( hashlib , hasher ) elif isinstance ( hasher , HASH ) : return lambda : hasher return hasher
12374	def take_snapshot ( droplet , name ) : print "powering off" droplet . power_off ( ) droplet . wait ( ) print "taking snapshot" droplet . take_snapshot ( name ) droplet . wait ( ) snapshots = droplet . snapshots ( ) print "Current snapshots" print snapshots
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
11468	def rm ( self , filename ) : try : self . _ftp . delete ( filename ) except error_perm : try : current_folder = self . _ftp . pwd ( ) self . cd ( filename ) except error_perm : print ( '550 Delete operation failed %s ' 'does not exist!' % ( filename , ) ) else : self . cd ( current_folder ) print ( '550 Delete operation failed %s ' 'is a folder. Use rmdir function ' 'to delete it.' % ( filename , ) )
10723	def xformers ( sig ) : return [ ( _wrapper ( f ) , l ) for ( f , l ) in _XFORMER . PARSER . parseString ( sig , parseAll = True ) ]
9769	def delete ( ctx ) : user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) if not click . confirm ( "Are sure you want to delete job `{}`" . format ( _job ) ) : click . echo ( 'Existing without deleting job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . job . delete_job ( user , project_name , _job ) JobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Job `{}` was delete successfully" . format ( _job ) )
10393	def workflow_all_aggregate ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , aggregator : Optional [ Callable [ [ Iterable [ float ] ] , float ] ] = None , ) : results = { } bioprocess_nodes = list ( get_nodes_by_function ( graph , BIOPROCESS ) ) for bioprocess_node in tqdm ( bioprocess_nodes ) : subgraph = generate_mechanism ( graph , bioprocess_node , key = key ) try : results [ bioprocess_node ] = workflow_aggregate ( graph = subgraph , node = bioprocess_node , key = key , tag = tag , default_score = default_score , runs = runs , aggregator = aggregator ) except Exception : log . exception ( 'could not run on %' , bioprocess_node ) return results
6403	def ipa_to_features ( ipa ) : features = [ ] pos = 0 ipa = normalize ( 'NFD' , text_type ( ipa . lower ( ) ) ) maxsymlen = max ( len ( _ ) for _ in _PHONETIC_FEATURES ) while pos < len ( ipa ) : found_match = False for i in range ( maxsymlen , 0 , - 1 ) : if ( pos + i - 1 <= len ( ipa ) and ipa [ pos : pos + i ] in _PHONETIC_FEATURES ) : features . append ( _PHONETIC_FEATURES [ ipa [ pos : pos + i ] ] ) pos += i found_match = True if not found_match : features . append ( - 1 ) pos += 1 return features
2459	def set_pkg_summary ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_summary_set : self . package_summary_set = True if validations . validate_pkg_summary ( text ) : doc . package . summary = str_from_text ( text ) else : raise SPDXValueError ( 'Package::Summary' ) else : raise CardinalityError ( 'Package::Summary' )
10595	def Nu_L ( self , L , theta , Ts , ** statef ) : return self . Nu_x ( L , theta , Ts , ** statef ) / 0.75
13719	def create_tree ( endpoints ) : tree = { } for method , url , doc in endpoints : path = [ p for p in url . strip ( '/' ) . split ( '/' ) ] here = tree version = path [ 0 ] here . setdefault ( version , { } ) here = here [ version ] for p in path [ 1 : ] : part = _camelcase_to_underscore ( p ) here . setdefault ( part , { } ) here = here [ part ] if not 'METHODS' in here : here [ 'METHODS' ] = [ [ method , doc ] ] else : if not method in here [ 'METHODS' ] : here [ 'METHODS' ] . append ( [ method , doc ] ) return tree
13296	def decode_jsonld ( jsonld_text ) : decoder = json . JSONDecoder ( object_pairs_hook = _decode_object_pairs ) return decoder . decode ( jsonld_text )
5763	def topological_order_packages ( packages ) : from catkin_pkg . topological_order import _PackageDecorator from catkin_pkg . topological_order import _sort_decorated_packages decorators_by_name = { } for path , package in packages . items ( ) : decorators_by_name [ package . name ] = _PackageDecorator ( package , path ) for decorator in decorators_by_name . values ( ) : decorator . depends_for_topological_order = set ( [ ] ) all_depends = decorator . package . build_depends + decorator . package . buildtool_depends + decorator . package . run_depends + decorator . package . test_depends unique_depend_names = set ( [ d . name for d in all_depends if d . name in decorators_by_name . keys ( ) ] ) for name in unique_depend_names : if name in decorator . depends_for_topological_order : continue decorators_by_name [ name ] . _add_recursive_run_depends ( decorators_by_name , decorator . depends_for_topological_order ) ordered_pkg_tuples = _sort_decorated_packages ( decorators_by_name ) for pkg_path , pkg in ordered_pkg_tuples : if pkg_path is None : raise RuntimeError ( 'Circular dependency in: %s' % pkg ) return ordered_pkg_tuples
7648	def deprecated ( version , version_removed ) : def __wrapper ( func , * args , ** kwargs ) : code = six . get_function_code ( func ) warnings . warn_explicit ( "{:s}.{:s}\n\tDeprecated as of JAMS version {:s}." "\n\tIt will be removed in JAMS version {:s}." . format ( func . __module__ , func . __name__ , version , version_removed ) , category = DeprecationWarning , filename = code . co_filename , lineno = code . co_firstlineno + 1 ) return func ( * args , ** kwargs ) return decorator ( __wrapper )
497	def _constructClassificationRecord ( self , inputs ) : allSPColumns = inputs [ "spBottomUpOut" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( "Classification vector type must be either 'tpc' or" " 'sp_tpe', current value is %s" % ( self . classificationVectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result
5032	def _build_admin_context ( request , customer ) : opts = customer . _meta codename = get_permission_codename ( 'change' , opts ) has_change_permission = request . user . has_perm ( '%s.%s' % ( opts . app_label , codename ) ) return { 'has_change_permission' : has_change_permission , 'opts' : opts }
3934	def _get_session_cookies ( session , access_token ) : headers = { 'Authorization' : 'Bearer {}' . format ( access_token ) } try : r = session . get ( ( 'https://accounts.google.com/accounts/OAuthLogin' '?source=hangups&issueuberauth=1' ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'OAuthLogin request failed: {}' . format ( e ) ) uberauth = r . text try : r = session . get ( ( 'https://accounts.google.com/MergeSession?' 'service=mail&' 'continue=http://www.google.com&uberauth={}' ) . format ( uberauth ) , headers = headers ) r . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'MergeSession request failed: {}' . format ( e ) ) cookies = session . cookies . get_dict ( domain = '.google.com' ) if cookies == { } : raise GoogleAuthError ( 'Failed to find session cookies' ) return cookies
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
12370	def get ( self , id , ** kwargs ) : return super ( DomainRecords , self ) . get ( id , ** kwargs )
3228	def gce_list_aggregated ( service = None , key_name = 'name' , ** kwargs ) : resp_list = [ ] req = service . aggregatedList ( ** kwargs ) while req is not None : resp = req . execute ( ) for location , item in resp [ 'items' ] . items ( ) : if key_name in item : resp_list . extend ( item [ key_name ] ) req = service . aggregatedList_next ( previous_request = req , previous_response = resp ) return resp_list
8742	def update_floatingip ( context , id , content ) : LOG . info ( 'update_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) if 'port_id' not in content : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'port_id is required.' ) requested_ports = [ ] if content . get ( 'port_id' ) : requested_ports = [ { 'port_id' : content . get ( 'port_id' ) } ] flip = _update_flip ( context , id , ip_types . FLOATING , requested_ports ) return v . _make_floating_ip_dict ( flip )
290	def plot_rolling_beta ( returns , factor_returns , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) ax . set_title ( "Rolling portfolio beta to " + str ( factor_returns . name ) ) ax . set_ylabel ( 'Beta' ) rb_1 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) rb_1 . plot ( color = 'steelblue' , lw = 3 , alpha = 0.6 , ax = ax , ** kwargs ) rb_2 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 12 ) rb_2 . plot ( color = 'grey' , lw = 3 , alpha = 0.4 , ax = ax , ** kwargs ) ax . axhline ( rb_1 . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_xlabel ( '' ) ax . legend ( [ '6-mo' , '12-mo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_ylim ( ( - 1.0 , 1.0 ) ) return ax
9371	def password ( at_least = 6 , at_most = 12 , lowercase = True , uppercase = True , digits = True , spaces = False , punctuation = False ) : return text ( at_least = at_least , at_most = at_most , lowercase = lowercase , uppercase = uppercase , digits = digits , spaces = spaces , punctuation = punctuation )
12089	def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) swhlab . plot . save ( abf , tag = '05-gain' )
10986	def get_particles_featuring ( feature_rad , state_name = None , im_name = None , use_full_path = False , actual_rad = None , invert = True , featuring_params = { } , ** kwargs ) : state_name , im_name = _pick_state_im_name ( state_name , im_name , use_full_path = use_full_path ) s = states . load ( state_name ) if actual_rad is None : actual_rad = np . median ( s . obj_get_radii ( ) ) im = util . RawImage ( im_name , tile = s . image . tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) _ = s . obj_remove_particle ( np . arange ( s . obj_get_radii ( ) . size ) ) s . obj_add_particle ( pos , np . ones ( pos . shape [ 0 ] ) * actual_rad ) s . set_image ( im ) _translate_particles ( s , invert = invert , ** kwargs ) return s
12319	def permalink ( self , repo , path ) : if not os . path . exists ( path ) : return ( None , None ) cwd = os . getcwd ( ) if os . path . isfile ( path ) : os . chdir ( os . path . dirname ( path ) ) rootdir = self . _run ( [ "rev-parse" , "--show-toplevel" ] ) if "fatal" in rootdir : return ( None , None ) os . chdir ( rootdir ) relpath = os . path . relpath ( path , rootdir ) sha1 = self . _run ( [ "log" , "-n" , "1" , "--format=format:%H" , relpath ] ) remoteurl = self . _run ( [ "config" , "--get" , "remote.origin.url" ] ) os . chdir ( cwd ) m = re . search ( '^git@([^:\/]+):([^/]+)/([^/]+)' , remoteurl ) if m is None : m = re . search ( '^https://([^:/]+)/([^/]+)/([^/]+)' , remoteurl ) if m is not None : domain = m . group ( 1 ) username = m . group ( 2 ) project = m . group ( 3 ) if project . endswith ( ".git" ) : project = project [ : - 4 ] permalink = "https://{}/{}/{}/blob/{}/{}" . format ( domain , username , project , sha1 , relpath ) return ( relpath , permalink ) else : return ( None , None )
11384	def module ( self ) : if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : self . _module = imp . load_source ( 'captain_script' , self . path ) return self . _module
8579	def delete_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s' % ( datacenter_id , server_id ) , method = 'DELETE' ) return response
12646	def set_aad_metadata ( uri , resource , client ) : set_config_value ( 'authority_uri' , uri ) set_config_value ( 'aad_resource' , resource ) set_config_value ( 'aad_client' , client )
4387	def adsAddRoute ( net_id , ip_address ) : add_route = _adsDLL . AdsAddRoute add_route . restype = ctypes . c_long ip_address_p = ctypes . c_char_p ( ip_address . encode ( "utf-8" ) ) error_code = add_route ( net_id , ip_address_p ) if error_code : raise ADSError ( error_code )
11953	def _parse_dumb_push_output ( self , string ) : stack = 0 json_list = [ ] tmp_json = '' for char in string : if not char == '\r' and not char == '\n' : tmp_json += char if char == '{' : stack += 1 elif char == '}' : stack -= 1 if stack == 0 : if not len ( tmp_json ) == 0 : json_list . append ( tmp_json ) tmp_json = '' return json_list
10924	def fit_comp ( new_comp , old_comp , ** kwargs ) : new_cat = new_comp . category new_comp . category = 'ilm' fake_s = states . ImageState ( Image ( old_comp . get ( ) . copy ( ) ) , [ new_comp ] , pad = 0 , mdl = mdl . SmoothFieldModel ( ) ) do_levmarq ( fake_s , new_comp . params , ** kwargs ) new_comp . category = new_cat
8340	def toEncoding ( self , s , encoding = None ) : if isinstance ( s , unicode ) : if encoding : s = s . encode ( encoding ) elif isinstance ( s , str ) : if encoding : s = s . encode ( encoding ) else : s = unicode ( s ) else : if encoding : s = self . toEncoding ( str ( s ) , encoding ) else : s = unicode ( s ) return s
12912	def append ( self , item ) : if self . meta_type == 'dict' : raise AssertionError ( 'Cannot append to object of `dict` base type!' ) if self . meta_type == 'list' : self . _list . append ( item ) return
2300	def predict_undirected_graph ( self , data ) : graph = Graph ( ) for idx_i , i in enumerate ( data . columns ) : for idx_j , j in enumerate ( data . columns [ idx_i + 1 : ] ) : score = self . predict ( data [ i ] . values , data [ j ] . values ) if abs ( score ) > 0.001 : graph . add_edge ( i , j , weight = score ) return graph
1619	def CleanseRawStrings ( raw_lines ) : delimiter = None lines_without_raw_strings = [ ] for line in raw_lines : if delimiter : end = line . find ( delimiter ) if end >= 0 : leading_space = Match ( r'^(\s*)\S' , line ) line = leading_space . group ( 1 ) + '""' + line [ end + len ( delimiter ) : ] delimiter = None else : line = '""' while delimiter is None : matched = Match ( r'^(.*?)\b(?:R|u8R|uR|UR|LR)"([^\s\\()]*)\((.*)$' , line ) if ( matched and not Match ( r'^([^\'"]|\'(\\.|[^\'])*\'|"(\\.|[^"])*")*//' , matched . group ( 1 ) ) ) : delimiter = ')' + matched . group ( 2 ) + '"' end = matched . group ( 3 ) . find ( delimiter ) if end >= 0 : line = ( matched . group ( 1 ) + '""' + matched . group ( 3 ) [ end + len ( delimiter ) : ] ) delimiter = None else : line = matched . group ( 1 ) + '""' else : break lines_without_raw_strings . append ( line ) return lines_without_raw_strings
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , ** kw ) : return assertion . test ( subject , expected , * args , ** kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
8167	def run_tenuous ( self ) : with LiveExecution . lock : ns_snapshot = copy . copy ( self . ns ) try : source = self . edited_source self . edited_source = None self . do_exec ( source , ns_snapshot ) self . known_good = source self . call_good_cb ( ) return True , None except Exception as ex : tb = traceback . format_exc ( ) self . call_bad_cb ( tb ) self . ns . clear ( ) self . ns . update ( ns_snapshot ) return False , ex
9261	def filter_wo_labels ( self , all_issues ) : issues_wo_labels = [ ] if not self . options . add_issues_wo_labels : for issue in all_issues : if not issue [ 'labels' ] : issues_wo_labels . append ( issue ) return issues_wo_labels
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
8930	def workdir_is_clean ( self , quiet = False ) : self . run ( 'git update-index -q --ignore-submodules --refresh' , ** RUN_KWARGS ) unchanged = True try : self . run ( 'git diff-files --quiet --ignore-submodules --' , report_error = False , ** RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'You have unstaged changes!' ) self . run ( 'git diff-files --name-status -r --ignore-submodules -- >&2' , ** RUN_KWARGS ) try : self . run ( 'git diff-index --cached --quiet HEAD --ignore-submodules --' , report_error = False , ** RUN_KWARGS ) except exceptions . Failure : unchanged = False if not quiet : notify . warning ( 'Your index contains uncommitted changes!' ) self . run ( 'git diff-index --cached --name-status -r --ignore-submodules HEAD -- >&2' , ** RUN_KWARGS ) return unchanged
10777	def make_clean_figure ( figsize , remove_tooltips = False , remove_keybindings = False ) : tooltip = mpl . rcParams [ 'toolbar' ] if remove_tooltips : mpl . rcParams [ 'toolbar' ] = 'None' fig = pl . figure ( figsize = figsize ) mpl . rcParams [ 'toolbar' ] = tooltip if remove_keybindings : fig . canvas . mpl_disconnect ( fig . canvas . manager . key_press_handler_id ) return fig
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
8839	def missing ( data , * args ) : not_found = object ( ) if args and isinstance ( args [ 0 ] , list ) : args = args [ 0 ] ret = [ ] for arg in args : if get_var ( data , arg , not_found ) is not_found : ret . append ( arg ) return ret
13371	def redirect_to_env_paths ( path ) : with open ( path , 'r' ) as f : redirected = f . read ( ) return shlex . split ( redirected )
6610	def getArrays ( self , tree , branchName ) : itsArray = self . _getArray ( tree , branchName ) if itsArray is None : return None , None itsCountArray = self . _getCounterArray ( tree , branchName ) return itsArray , itsCountArray
9664	def construct_graph ( sakefile , settings ) : verbose = settings [ "verbose" ] sprint = settings [ "sprint" ] G = nx . DiGraph ( ) sprint ( "Going to construct Graph" , level = "verbose" ) for target in sakefile : if target == "all" : continue if "formula" not in sakefile [ target ] : for atomtarget in sakefile [ target ] : if atomtarget == "help" : continue sprint ( "Adding '{}'" . format ( atomtarget ) , level = "verbose" ) data_dict = sakefile [ target ] [ atomtarget ] data_dict [ "parent" ] = target G . add_node ( atomtarget , ** data_dict ) else : sprint ( "Adding '{}'" . format ( target ) , level = "verbose" ) G . add_node ( target , ** sakefile [ target ] ) sprint ( "Nodes are built\nBuilding connections" , level = "verbose" ) for node in G . nodes ( data = True ) : sprint ( "checking node {} for dependencies" . format ( node [ 0 ] ) , level = "verbose" ) for k , v in node [ 1 ] . items ( ) : if v is None : node [ 1 ] [ k ] = [ ] if "output" in node [ 1 ] : for index , out in enumerate ( node [ 1 ] [ 'output' ] ) : node [ 1 ] [ 'output' ] [ index ] = clean_path ( node [ 1 ] [ 'output' ] [ index ] ) if "dependencies" not in node [ 1 ] : continue sprint ( "it has dependencies" , level = "verbose" ) connects = [ ] for index , dep in enumerate ( node [ 1 ] [ 'dependencies' ] ) : dep = os . path . normpath ( dep ) shrt = "dependencies" node [ 1 ] [ 'dependencies' ] [ index ] = clean_path ( node [ 1 ] [ shrt ] [ index ] ) for node in G . nodes ( data = True ) : connects = [ ] if "dependencies" not in node [ 1 ] : continue for dep in node [ 1 ] [ 'dependencies' ] : matches = check_for_dep_in_outputs ( dep , verbose , G ) if not matches : continue for match in matches : sprint ( "Appending {} to matches" . format ( match ) , level = "verbose" ) connects . append ( match ) if connects : for connect in connects : G . add_edge ( connect , node [ 0 ] ) return G
7225	def delete ( self , project_id ) : self . logger . debug ( 'Deleting project by id: ' + project_id ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project_id } r = self . gbdx_connection . delete ( url ) r . raise_for_status ( )
2618	def write_state_file ( self ) : fh = open ( 'awsproviderstate.json' , 'w' ) state = { } state [ 'vpcID' ] = self . vpc_id state [ 'sgID' ] = self . sg_id state [ 'snIDs' ] = self . sn_ids state [ 'instances' ] = self . instances state [ "instanceState" ] = self . instance_states fh . write ( json . dumps ( state , indent = 4 ) )
11971	def _detect ( ip , _isnm ) : ip = str ( ip ) if len ( ip ) > 1 : if ip [ 0 : 2 ] == '0x' : if _CHECK_FUNCT [ IP_HEX ] [ _isnm ] ( ip ) : return IP_HEX elif ip [ 0 ] == '0' : if _CHECK_FUNCT [ IP_OCT ] [ _isnm ] ( ip ) : return IP_OCT if _CHECK_FUNCT [ IP_DOT ] [ _isnm ] ( ip ) : return IP_DOT elif _isnm and _CHECK_FUNCT [ NM_BITS ] [ _isnm ] ( ip ) : return NM_BITS elif _CHECK_FUNCT [ IP_DEC ] [ _isnm ] ( ip ) : return IP_DEC elif _isnm and _CHECK_FUNCT [ NM_WILDCARD ] [ _isnm ] ( ip ) : return NM_WILDCARD elif _CHECK_FUNCT [ IP_BIN ] [ _isnm ] ( ip ) : return IP_BIN return IP_UNKNOWN
1755	def read_register ( self , register ) : self . _publish ( 'will_read_register' , register ) value = self . _regfile . read ( register ) self . _publish ( 'did_read_register' , register , value ) return value
671	def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , "r" ) as f : modelParams = yaml . safe_load ( f ) [ "modelParams" ] network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) sensorRegion = network . regions [ "sensor" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) sensorRegion . dataSource = dataSource modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) createSensorToClassifierLinks ( network , "sensor" , "classifier" ) createDataOutLink ( network , "sensor" , "SP" ) createFeedForwardLink ( network , "SP" , "TM" ) createFeedForwardLink ( network , "TM" , "classifier" ) createResetLink ( network , "sensor" , "SP" ) createResetLink ( network , "sensor" , "TM" ) network . initialize ( ) return network
168	def is_partly_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default mask = self . get_pointwise_inside_image_mask ( image ) if np . any ( mask ) : return True return len ( self . clip_out_of_image ( image ) ) > 0
3717	def economic_status ( CASRN , Method = None , AvailableMethods = False ) : load_economic_data ( ) CASi = CAS2int ( CASRN ) def list_methods ( ) : methods = [ ] methods . append ( 'Combined' ) if CASRN in _EPACDRDict : methods . append ( EPACDR ) if CASRN in _ECHATonnageDict : methods . append ( ECHA ) if CASi in HPV_data . index : methods . append ( OECD ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == EPACDR : status = 'US public: ' + str ( _EPACDRDict [ CASRN ] ) elif Method == ECHA : status = _ECHATonnageDict [ CASRN ] elif Method == OECD : status = 'OECD HPV Chemicals' elif Method == 'Combined' : status = [ ] if CASRN in _EPACDRDict : status += [ 'US public: ' + str ( _EPACDRDict [ CASRN ] ) ] if CASRN in _ECHATonnageDict : status += _ECHATonnageDict [ CASRN ] if CASi in HPV_data . index : status += [ 'OECD HPV Chemicals' ] elif Method == NONE : status = None else : raise Exception ( 'Failure in in function' ) return status
3937	def submit_form ( self , form_selector , input_dict ) : logger . info ( 'Submitting form on page %r' , self . _page . url . split ( '?' ) [ 0 ] ) logger . info ( 'Page contains forms: %s' , [ elem . get ( 'id' ) for elem in self . _page . soup . select ( 'form' ) ] ) try : form = self . _page . soup . select ( form_selector ) [ 0 ] except IndexError : raise GoogleAuthError ( 'Failed to find form {!r} in page' . format ( form_selector ) ) logger . info ( 'Page contains inputs: %s' , [ elem . get ( 'id' ) for elem in form . select ( 'input' ) ] ) for selector , value in input_dict . items ( ) : try : form . select ( selector ) [ 0 ] [ 'value' ] = value except IndexError : raise GoogleAuthError ( 'Failed to find input {!r} in form' . format ( selector ) ) try : self . _page = self . _browser . submit ( form , self . _page . url ) self . _page . raise_for_status ( ) except requests . RequestException as e : raise GoogleAuthError ( 'Failed to submit form: {}' . format ( e ) )
12325	def untokenize ( tokens ) : text = '' previous_line = '' last_row = 0 last_column = - 1 last_non_whitespace_token_type = None for ( token_type , token_string , start , end , line ) in tokens : if TOKENIZE_HAS_ENCODING and token_type == tokenize . ENCODING : continue ( start_row , start_column ) = start ( end_row , end_column ) = end if ( last_non_whitespace_token_type != tokenize . COMMENT and start_row > last_row and previous_line . endswith ( ( '\\\n' , '\\\r\n' , '\\\r' ) ) ) : text += previous_line [ len ( previous_line . rstrip ( ' \t\n\r\\' ) ) : ] if start_row > last_row : last_column = 0 if start_column > last_column : text += line [ last_column : start_column ] text += token_string previous_line = line last_row = end_row last_column = end_column if token_type not in WHITESPACE_TOKENS : last_non_whitespace_token_type = token_type return text
6695	def upgrade ( safe = True ) : manager = MANAGER if safe : cmd = 'upgrade' else : cmd = 'dist-upgrade' run_as_root ( "%(manager)s --assume-yes %(cmd)s" % locals ( ) , pty = False )
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
3537	def crazy_egg ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return CrazyEggNode ( )
7075	def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = ( { } , { } , { 'startp' : 1.0 , 'maxtransitduration' : 0.3 } ) , getblssnr = False , sigclip = 5.0 , nperiodworkers = 10 , ncontrolworkers = 4 , liststartindex = None , listmaxobjects = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] pfdir = os . path . join ( simbasedir , 'periodfinding' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) if liststartindex : lcfpaths = lcfpaths [ liststartindex : ] if listmaxobjects : lcfpaths = lcfpaths [ : listmaxobjects ] pfinfo = periodsearch . parallel_pf ( lcfpaths , pfdir , lcformat = fakelc_formatkey , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers ) with open ( os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' ) , 'wb' ) as outfd : pickle . dump ( pfinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' )
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) return ( pmags - modelmags ) / perrs
6904	def hms_to_decimal ( hours , minutes , seconds , returndeg = True ) : if hours > 24 : return None else : dec_hours = fabs ( hours ) + fabs ( minutes ) / 60.0 + fabs ( seconds ) / 3600.0 if returndeg : dec_deg = dec_hours * 15.0 if dec_deg < 0 : dec_deg = dec_deg + 360.0 dec_deg = dec_deg % 360.0 return dec_deg else : return dec_hours
8502	def as_live ( self ) : key = self . get_key ( ) default = pyconfig . get ( key ) if default : default = repr ( default ) else : default = self . _default ( ) or NotSet ( ) return "%s = %s" % ( key , default )
3325	def _generate_lock ( self , principal , lock_type , lock_scope , lock_depth , lock_owner , path , timeout ) : if timeout is None : timeout = LockManager . LOCK_TIME_OUT_DEFAULT elif timeout < 0 : timeout = - 1 lock_dict = { "root" : path , "type" : lock_type , "scope" : lock_scope , "depth" : lock_depth , "owner" : lock_owner , "timeout" : timeout , "principal" : principal , } self . storage . create ( path , lock_dict ) return lock_dict
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
9428	def printdir ( self ) : print ( "%-46s %19s %12s" % ( "File Name" , "Modified " , "Size" ) ) for rarinfo in self . filelist : date = "%d-%02d-%02d %02d:%02d:%02d" % rarinfo . date_time [ : 6 ] print ( "%-46s %s %12d" % ( rarinfo . filename , date , rarinfo . file_size ) )
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _connecting = True self . connect ( self . endpoint )
8598	def list_shares ( self , group_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares?depth=%s' % ( group_id , str ( depth ) ) ) return response
12557	def cli ( ) : return VersionedCLI ( cli_name = SF_CLI_NAME , config_dir = SF_CLI_CONFIG_DIR , config_env_var_prefix = SF_CLI_ENV_VAR_PREFIX , commands_loader_cls = SFCommandLoader , help_cls = SFCommandHelp )
1723	def translate_file ( input_path , output_path ) : js = get_file_contents ( input_path ) py_code = translate_js ( js ) lib_name = os . path . basename ( output_path ) . split ( '.' ) [ 0 ] head = '__all__ = [%s]\n\n# Don\'t look below, you will not understand this Python code :) I don\'t.\n\n' % repr ( lib_name ) tail = '\n\n# Add lib to the module scope\n%s = var.to_python()' % lib_name out = head + py_code + tail write_file_contents ( output_path , out )
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
3813	async def upload_image ( self , image_file , filename = None , * , return_uploaded_image = False ) : image_filename = filename or os . path . basename ( image_file . name ) image_data = image_file . read ( ) res = await self . _base_request ( IMAGE_UPLOAD_URL , 'application/x-www-form-urlencoded;charset=UTF-8' , 'json' , json . dumps ( { "protocolVersion" : "0.8" , "createSessionRequest" : { "fields" : [ { "external" : { "name" : "file" , "filename" : image_filename , "put" : { } , "size" : len ( image_data ) } } ] } } ) ) try : upload_url = self . _get_upload_session_status ( res ) [ 'externalFieldTransfers' ] [ 0 ] [ 'putInfo' ] [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not acquire an upload url' ) res = await self . _base_request ( upload_url , 'application/octet-stream' , 'json' , image_data ) try : raw_info = ( self . _get_upload_session_status ( res ) [ 'additionalInfo' ] [ 'uploader_service.GoogleRupioAdditionalInfo' ] [ 'completionInfo' ] [ 'customerSpecificInfo' ] ) image_id = raw_info [ 'photoid' ] url = raw_info [ 'url' ] except KeyError : raise exceptions . NetworkError ( 'image upload failed: can not fetch upload info' ) result = UploadedImage ( image_id = image_id , url = url ) return result if return_uploaded_image else result . image_id
9635	def execute_from_command_line ( argv = None ) : parser = argparse . ArgumentParser ( description = __doc__ ) parser . add_argument ( '--monitors-dir' , default = MONITORS_DIR ) parser . add_argument ( '--alerts-dir' , default = ALERTS_DIR ) parser . add_argument ( '--config' , default = SMA_INI_FILE ) parser . add_argument ( '--warning' , help = 'set logging to warning' , action = 'store_const' , dest = 'loglevel' , const = logging . WARNING , default = logging . INFO ) parser . add_argument ( '--quiet' , help = 'set logging to ERROR' , action = 'store_const' , dest = 'loglevel' , const = logging . ERROR , default = logging . INFO ) parser . add_argument ( '--debug' , help = 'set logging to DEBUG' , action = 'store_const' , dest = 'loglevel' , const = logging . DEBUG , default = logging . INFO ) parser . add_argument ( '--verbose' , help = 'set logging to COMM' , action = 'store_const' , dest = 'loglevel' , const = 5 , default = logging . INFO ) parser . sub = parser . add_subparsers ( ) parse_service = parser . sub . add_parser ( 'service' , help = 'Run SMA as service (daemon).' ) parse_service . set_defaults ( which = 'service' ) parse_oneshot = parser . sub . add_parser ( 'one-shot' , help = 'Run SMA once and exit' ) parse_oneshot . set_defaults ( which = 'one-shot' ) parse_alerts = parser . sub . add_parser ( 'alerts' , help = 'Alerts options.' ) parse_alerts . set_defaults ( which = 'alerts' ) parse_alerts . add_argument ( '--test' , help = 'Test alert' , action = 'store_true' ) parse_alerts . add_argument ( 'alert_section' , nargs = '?' , help = 'Alert section to see' ) parse_results = parser . sub . add_parser ( 'results' , help = 'Monitors results' ) parse_results . set_defaults ( which = 'results' ) parser . set_default_subparser ( 'one-shot' ) args = parser . parse_args ( argv [ 1 : ] ) create_logger ( 'sma' , args . loglevel ) if not getattr ( args , 'which' , None ) or args . which == 'one-shot' : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . evaluate_and_alert ( ) elif args . which == 'service' : sma = SMAService ( args . monitors_dir , args . alerts_dir , args . config ) sma . start ( ) elif args . which == 'alerts' and args . test : sma = SMA ( args . monitors_dir , args . alerts_dir , args . config ) sma . alerts . test ( ) elif args . which == 'results' : print ( SMA ( args . monitors_dir , args . alerts_dir , args . config ) . results )
4872	def to_representation ( self , data ) : return [ self . child . to_representation ( item ) if 'detail' in item else item for item in data ]
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : image = common . pad ( image , extrapad , 0 ) s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
10685	def G_mag ( self , T ) : tau = T / self . Tc_mag if tau <= 1.0 : g = 1 - ( self . _A_mag / tau + self . _B_mag * ( tau ** 3 / 6 + tau ** 9 / 135 + tau ** 15 / 600 ) ) / self . _D_mag else : g = - ( tau ** - 5 / 10 + tau ** - 15 / 315 + tau ** - 25 / 1500 ) / self . _D_mag return R * T * math . log ( self . beta0_mag + 1 ) * g
8408	def expand_range_distinct ( range , expand = ( 0 , 0 , 0 , 0 ) , zero_width = 1 ) : if len ( expand ) == 2 : expand = tuple ( expand ) * 2 lower = expand_range ( range , expand [ 0 ] , expand [ 1 ] , zero_width ) [ 0 ] upper = expand_range ( range , expand [ 2 ] , expand [ 3 ] , zero_width ) [ 1 ] return ( lower , upper )
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
6516	def execute_tools ( config , path , progress = None ) : progress = progress or QuietProgress ( ) progress . on_start ( ) manager = SyncManager ( ) manager . start ( ) num_tools = 0 tools = manager . Queue ( ) for name , cls in iteritems ( get_tools ( ) ) : if config [ name ] [ 'use' ] and cls . can_be_used ( ) : num_tools += 1 tools . put ( { 'name' : name , 'config' : config [ name ] , } ) collector = Collector ( config ) if not num_tools : progress . on_finish ( ) return collector notifications = manager . Queue ( ) environment = manager . dict ( { 'finder' : Finder ( path , config ) , } ) workers = [ ] for _ in range ( config [ 'workers' ] ) : worker = Worker ( args = ( tools , notifications , environment , ) , ) worker . start ( ) workers . append ( worker ) while num_tools : try : notification = notifications . get ( True , 0.25 ) except Empty : pass else : if notification [ 'type' ] == 'start' : progress . on_tool_start ( notification [ 'tool' ] ) elif notification [ 'type' ] == 'complete' : collector . add_issues ( notification [ 'issues' ] ) progress . on_tool_finish ( notification [ 'tool' ] ) num_tools -= 1 progress . on_finish ( ) return collector
8193	def crown ( self , depth = 2 ) : nodes = [ ] for node in self . leaves : nodes += node . flatten ( depth - 1 ) return cluster . unique ( nodes )
6808	def configure_hdmi ( self ) : r = self . local_renderer r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_force_hotplug' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_drive' , value = 2 , use_sudo = True , )
10854	def sphere_triangle_cdf ( dr , a , alpha ) : p0 = ( dr + alpha ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 > dr ) * ( dr > - alpha ) p1 = 1 * ( dr > 0 ) - ( alpha - dr ) ** 2 / ( 2 * alpha ** 2 ) * ( 0 < dr ) * ( dr < alpha ) return ( 1 - np . clip ( p0 + p1 , 0 , 1 ) )
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
13676	def add_prepare_handler ( self , prepare_handlers ) : if not isinstance ( prepare_handlers , static_bundle . BUNDLE_ITERABLE_TYPES ) : prepare_handlers = [ prepare_handlers ] if self . prepare_handlers_chain is None : self . prepare_handlers_chain = [ ] for handler in prepare_handlers : self . prepare_handlers_chain . append ( handler )
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : r valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
5455	def numeric_task_id ( task_id ) : if task_id is not None : if task_id . startswith ( 'task-' ) : return int ( task_id [ len ( 'task-' ) : ] ) else : return int ( task_id )
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
9403	def _isobject ( self , name , exist ) : if exist in [ 2 , 5 ] : return False cmd = 'isobject(%s)' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) return resp == 'ans = 1'
6523	def issue_count ( self , include_unclean = False ) : if include_unclean : return len ( self . _all_issues ) self . _ensure_cleaned_issues ( ) return len ( self . _cleaned_issues )
4197	def HERMTOEP ( T0 , T , Z ) : assert len ( T ) > 0 M = len ( T ) X = numpy . zeros ( M + 1 , dtype = complex ) A = numpy . zeros ( M , dtype = complex ) P = T0 if P == 0 : raise ValueError ( "P must be different from zero" ) X [ 0 ] = Z [ 0 ] / T0 for k in range ( 0 , M ) : save = T [ k ] beta = X [ 0 ] * T [ k ] if k == 0 : temp = - save / P else : for j in range ( 0 , k ) : save = save + A [ j ] * T [ k - j - 1 ] beta = beta + X [ j + 1 ] * T [ k - j - 1 ] temp = - save / P P = P * ( 1. - ( temp . real ** 2 + temp . imag ** 2 ) ) if P <= 0 : raise ValueError ( "singular matrix" ) A [ k ] = temp alpha = ( Z [ k + 1 ] - beta ) / P if k == 0 : X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) continue khalf = ( k + 1 ) // 2 for j in range ( 0 , khalf ) : kj = k - j - 1 save = A [ j ] A [ j ] = save + temp * A [ kj ] . conjugate ( ) if j != kj : A [ kj ] = A [ kj ] + temp * save . conjugate ( ) X [ k + 1 ] = alpha for j in range ( 0 , k + 1 ) : X [ j ] = X [ j ] + alpha * A [ k - j ] . conjugate ( ) return X
5746	def asn ( self , ip , announce_date = None ) : assignations , announce_date , _ = self . run ( ip , announce_date ) return next ( ( assign for assign in assignations if assign is not None ) , None ) , announce_date
6213	def load ( self ) : self . path = self . find_scene ( self . meta . path ) if not self . path : raise ValueError ( "Scene '{}' not found" . format ( self . meta . path ) ) self . scene = Scene ( self . path ) if self . path . suffix == '.gltf' : self . load_gltf ( ) if self . path . suffix == '.glb' : self . load_glb ( ) self . meta . check_version ( ) self . meta . check_extensions ( self . supported_extensions ) self . load_images ( ) self . load_samplers ( ) self . load_textures ( ) self . load_materials ( ) self . load_meshes ( ) self . load_nodes ( ) self . scene . calc_scene_bbox ( ) self . scene . prepare ( ) return self . scene
7430	def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . append ( [ _AMBIGS [ i ] [ rand ] for i in col ] ) return np . array ( N )
11587	def object ( self , infotype , key ) : "Return the encoding, idletime, or refcount about the key" redisent = self . redises [ self . _getnodenamefor ( key ) + '_slave' ] return getattr ( redisent , 'object' ) ( infotype , key )
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
12049	def scanABFfolder ( abfFolder ) : assert os . path . isdir ( abfFolder ) filesABF = forwardSlash ( sorted ( glob . glob ( abfFolder + "/*.*" ) ) ) filesSWH = [ ] if os . path . exists ( abfFolder + "/swhlab4/" ) : filesSWH = forwardSlash ( sorted ( glob . glob ( abfFolder + "/swhlab4/*.*" ) ) ) groups = getABFgroups ( filesABF ) return filesABF , filesSWH , groups
3801	def calculate ( self , T , method ) : r if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
644	def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : if doForeground and doBackground : return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) else : if doForeground : return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) if doBackground : return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) return input
536	def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance
8807	def _make_job_dict ( job ) : body = { "id" : job . get ( 'id' ) , "action" : job . get ( 'action' ) , "completed" : job . get ( 'completed' ) , "tenant_id" : job . get ( 'tenant_id' ) , "created_at" : job . get ( 'created_at' ) , "transaction_id" : job . get ( 'transaction_id' ) , "parent_id" : job . get ( 'parent_id' , None ) } if not body [ 'transaction_id' ] : body [ 'transaction_id' ] = job . get ( 'id' ) completed = 0 for sub in job . subtransactions : if sub . get ( 'completed' ) : completed += 1 pct = 100 if job . get ( 'completed' ) else 0 if len ( job . subtransactions ) > 0 : pct = float ( completed ) / len ( job . subtransactions ) * 100.0 body [ 'transaction_percent' ] = int ( pct ) body [ 'completed_subtransactions' ] = completed body [ 'subtransactions' ] = len ( job . subtransactions ) return body
5879	def store_image ( cls , http_client , link_hash , src , config ) : image = cls . read_localfile ( link_hash , src , config ) if image : return image if src . startswith ( 'data:image' ) : image = cls . write_localfile_base64 ( link_hash , src , config ) return image data = http_client . fetch ( src ) if data : image = cls . write_localfile ( data , link_hash , src , config ) if image : return image return None
5653	def execute ( cur , * args ) : stmt = args [ 0 ] if len ( args ) > 1 : stmt = stmt . replace ( '%' , '%%' ) . replace ( '?' , '%r' ) print ( stmt % ( args [ 1 ] ) ) return cur . execute ( * args )
12636	def dist_percentile_threshold ( dist_matrix , perc_thr = 0.05 , k = 1 ) : triu_idx = np . triu_indices ( dist_matrix . shape [ 0 ] , k = k ) upper = np . zeros_like ( dist_matrix ) upper [ triu_idx ] = dist_matrix [ triu_idx ] < np . percentile ( dist_matrix [ triu_idx ] , perc_thr ) return upper
950	def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = "uint32" ) bitsToSwap = int ( noiseLevel * numActiveCols ) for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2
7475	def dask_chroms ( data , samples ) : h5s = [ os . path . join ( data . dirs . across , s . name + ".tmp.h5" ) for s in samples ] handles = [ h5py . File ( i ) for i in h5s ] dsets = [ i [ '/ichrom' ] for i in handles ] arrays = [ da . from_array ( dset , chunks = ( 10000 , 3 ) ) for dset in dsets ] stack = da . stack ( arrays , axis = 2 ) maxchrom = da . max ( stack , axis = 2 ) [ : , 0 ] maxpos = da . max ( stack , axis = 2 ) [ : , 2 ] mask = stack == 0 stack [ mask ] = 9223372036854775807 minpos = da . min ( stack , axis = 2 ) [ : , 1 ] final = da . stack ( [ maxchrom , minpos , maxpos ] , axis = 1 ) final . to_hdf5 ( data . clust_database , "/chroms" ) _ = [ i . close ( ) for i in handles ]
9783	def update ( ctx , name , description , tags ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the build.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . build_job . update_build ( user , project_name , _build , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update build `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build updated." ) get_build_details ( response )
301	def plot_daily_turnover_hist ( transactions , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) turnover = txn . get_turnover ( positions , transactions ) sns . distplot ( turnover , ax = ax , ** kwargs ) ax . set_title ( 'Distribution of daily turnover rates' ) ax . set_xlabel ( 'Turnover rate' ) return ax
9360	def _to_lower_alpha_only ( s ) : s = re . sub ( r'\n' , ' ' , s . lower ( ) ) return re . sub ( r'[^a-z\s]' , '' , s )
13245	async def _download_text ( url , session ) : logger = logging . getLogger ( __name__ ) async with session . get ( url ) as response : logger . info ( 'Downloading %r' , url ) return await response . text ( )
10991	def makestate ( im , pos , rad , slab = None , mem_level = 'hi' ) : if slab is not None : o = comp . ComponentCollection ( [ objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) , slab ] , category = 'obj' ) else : o = objs . PlatonicSpheresCollection ( pos , rad , zscale = zscale ) p = exactpsf . FixedSSChebLinePSF ( ) npts , iorder = _calc_ilm_order ( im . get_image ( ) . shape ) i = ilms . BarnesStreakLegPoly2P1D ( npts = npts , zorder = iorder ) b = ilms . LegendrePoly2P1D ( order = ( 9 , 3 , 5 ) , category = 'bkg' ) c = comp . GlobalScalar ( 'offset' , 0.0 ) s = states . ImageState ( im , [ o , i , b , c , p ] ) runner . link_zscale ( s ) if mem_level != 'hi' : s . set_mem_level ( mem_level ) opt . do_levmarq ( s , [ 'ilm-scale' ] , max_iter = 1 , run_length = 6 , max_mem = 1e4 ) return s
13656	def routedResource ( f , routerAttribute = 'router' ) : return wraps ( f ) ( lambda * a , ** kw : getattr ( f ( * a , ** kw ) , routerAttribute ) . resource ( ) )
10740	def add_runtime ( function ) : def wrapper ( * args , ** kwargs ) : pr = cProfile . Profile ( ) pr . enable ( ) output = function ( * args , ** kwargs ) pr . disable ( ) return pr , output return wrapper
10849	def set_verbosity ( self , verbosity = 'vvv' , handlers = None ) : self . verbosity = sanitize ( verbosity ) self . set_level ( v2l [ verbosity ] , handlers = handlers ) self . set_formatter ( v2f [ verbosity ] , handlers = handlers )
11558	def get_stepper_version ( self , timeout = 20 ) : start_time = time . time ( ) while self . _command_handler . stepper_library_version <= 0 : if time . time ( ) - start_time > timeout : if self . verbose is True : print ( "Stepper Library Version Request timed-out. " "Did you send a stepper_request_library_version command?" ) return else : pass return self . _command_handler . stepper_library_version
10218	def plot_summary_axes ( graph : BELGraph , lax , rax , logx = True ) : ntc = count_functions ( graph ) etc = count_relations ( graph ) df = pd . DataFrame . from_dict ( dict ( ntc ) , orient = 'index' ) df_ec = pd . DataFrame . from_dict ( dict ( etc ) , orient = 'index' ) df . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = lax ) lax . set_title ( 'Number of nodes: {}' . format ( graph . number_of_nodes ( ) ) ) df_ec . sort_values ( 0 , ascending = True ) . plot ( kind = 'barh' , logx = logx , ax = rax ) rax . set_title ( 'Number of edges: {}' . format ( graph . number_of_edges ( ) ) )
550	def __checkCancelation ( self ) : print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( "Model %s canceled because Job %s was stopped." , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( "Model %s canceled because it was killed by hypersearch" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( "Model %s stopped because hypersearch ended" , self . _modelID ) else : raise RuntimeError ( "Unexpected stop reason encountered: %s" % ( stopReason ) )
3597	def list ( self , cat , ctr = None , nb_results = None , offset = None ) : path = LIST_URL + "?c=3&cat={}" . format ( requests . utils . quote ( cat ) ) if ctr is not None : path += "&ctr={}" . format ( requests . utils . quote ( ctr ) ) if nb_results is not None : path += "&n={}" . format ( requests . utils . quote ( str ( nb_results ) ) ) if offset is not None : path += "&o={}" . format ( requests . utils . quote ( str ( offset ) ) ) data = self . executeRequestApi2 ( path ) clusters = [ ] docs = [ ] if ctr is None : for pf in data . preFetch : for cluster in pf . response . payload . listResponse . doc : clusters . extend ( cluster . child ) return [ c . docid for c in clusters ] else : apps = [ ] for d in data . payload . listResponse . doc : for c in d . child : for a in c . child : apps . append ( utils . parseProtobufObj ( a ) ) return apps
3691	def solve_T ( self , P , V , quick = True ) : r if self . S2 == 0 : self . m = self . S1 return SRK . solve_T ( self , P , V , quick = quick ) else : Tc , a , b , S1 , S2 = self . Tc , self . a , self . b , self . S1 , self . S2 if quick : x2 = R / ( V - b ) x3 = ( V * ( V + b ) ) def to_solve ( T ) : x0 = ( T / Tc ) ** 0.5 x1 = x0 - 1. return ( x2 * T - a * ( S1 * x1 + S2 * x1 / x0 - 1. ) ** 2 / x3 ) - P else : def to_solve ( T ) : P_calc = R * T / ( V - b ) - a * ( S1 * ( - sqrt ( T / Tc ) + 1 ) + S2 * ( - sqrt ( T / Tc ) + 1 ) / sqrt ( T / Tc ) + 1 ) ** 2 / ( V * ( V + b ) ) return P_calc - P return newton ( to_solve , Tc * 0.5 )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
8406	def zero_range ( x , tol = np . finfo ( float ) . eps * 100 ) : try : if len ( x ) == 1 : return True except TypeError : return True if len ( x ) != 2 : raise ValueError ( 'x must be length 1 or 2' ) x = tuple ( x ) if isinstance ( x [ 0 ] , ( pd . Timestamp , datetime . datetime ) ) : x = date2num ( x ) elif isinstance ( x [ 0 ] , np . datetime64 ) : return x [ 0 ] == x [ 1 ] elif isinstance ( x [ 0 ] , ( pd . Timedelta , datetime . timedelta ) ) : x = x [ 0 ] . total_seconds ( ) , x [ 1 ] . total_seconds ( ) elif isinstance ( x [ 0 ] , np . timedelta64 ) : return x [ 0 ] == x [ 1 ] elif not isinstance ( x [ 0 ] , ( float , int , np . number ) ) : raise TypeError ( "zero_range objects cannot work with objects " "of type '{}'" . format ( type ( x [ 0 ] ) ) ) if any ( np . isnan ( x ) ) : return np . nan if x [ 0 ] == x [ 1 ] : return True if all ( np . isinf ( x ) ) : return False m = np . abs ( x ) . min ( ) if m == 0 : return False return np . abs ( ( x [ 0 ] - x [ 1 ] ) / m ) < tol
2031	def EXTCODECOPY ( self , account , address , offset , size ) : extbytecode = self . world . get_code ( account ) self . _allocate ( address + size ) for i in range ( size ) : if offset + i < len ( extbytecode ) : self . _store ( address + i , extbytecode [ offset + i ] ) else : self . _store ( address + i , 0 )
2722	def _perform_action ( self , params , return_dict = True ) : action = self . get_data ( "droplets/%s/actions/" % self . id , type = POST , params = params ) if return_dict : return action else : action = action [ u'action' ] return_action = Action ( token = self . token ) for attr in action . keys ( ) : setattr ( return_action , attr , action [ attr ] ) return return_action
627	def _hashCoordinate ( coordinate ) : coordinateStr = "," . join ( str ( v ) for v in coordinate ) hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash
382	def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( "Unsupported shape {}" . format ( x . shape ) ) return x
5228	def load_info ( cat ) : res = _load_yaml_ ( f'{PKG_PATH}/markets/{cat}.yml' ) root = os . environ . get ( 'BBG_ROOT' , '' ) . replace ( '\\' , '/' ) if not root : return res for cat , ovrd in _load_yaml_ ( f'{root}/markets/{cat}.yml' ) . items ( ) : if isinstance ( ovrd , dict ) : if cat in res : res [ cat ] . update ( ovrd ) else : res [ cat ] = ovrd if isinstance ( ovrd , list ) and isinstance ( res [ cat ] , list ) : res [ cat ] += ovrd return res
12593	def execute_reliabledictionary ( client , application_name , service_name , input_file ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) with open ( input_file ) as json_file : json_data = json . load ( json_file ) service . execute ( json_data ) return
13399	def resourcePath ( self , relative_path ) : from os import path import sys try : base_path = sys . _MEIPASS except Exception : base_path = path . dirname ( path . abspath ( __file__ ) ) return path . join ( base_path , relative_path )
4794	def contains_value ( self , * values ) : self . _check_dict_like ( self . val , check_getitem = False ) if len ( values ) == 0 : raise ValueError ( 'one or more value args must be given' ) missing = [ ] for v in values : if v not in self . val . values ( ) : missing . append ( v ) if missing : self . _err ( 'Expected <%s> to contain values %s, but did not contain %s.' % ( self . val , self . _fmt_items ( values ) , self . _fmt_items ( missing ) ) ) return self
1876	def MOVQ ( cpu , dest , src ) : if dest . size == src . size and dest . size == 64 : dest . write ( src . read ( ) ) elif dest . size == src . size and dest . size == 128 : src_lo = Operators . EXTRACT ( src . read ( ) , 0 , 64 ) dest . write ( Operators . ZEXTEND ( src_lo , 128 ) ) elif dest . size == 128 and src . size == 64 : dest . write ( Operators . ZEXTEND ( src . read ( ) , dest . size ) ) elif dest . size == 64 and src . size == 128 : dest . write ( Operators . EXTRACT ( src . read ( ) , 0 , dest . size ) ) else : msg = 'Invalid size in MOVQ' logger . error ( msg ) raise Exception ( msg )
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
7406	def bottom ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Max ( 'order' ) ) . get ( 'order__max' ) self . to ( o )
3454	def add_SBO ( model ) : for r in model . reactions : if r . annotation . get ( "sbo" ) : continue if len ( r . metabolites ) != 1 : continue met_id = list ( r . _metabolites ) [ 0 ] . id if r . id . startswith ( "EX_" ) and r . id == "EX_" + met_id : r . annotation [ "sbo" ] = "SBO:0000627" elif r . id . startswith ( "DM_" ) and r . id == "DM_" + met_id : r . annotation [ "sbo" ] = "SBO:0000628"
5286	def post ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) if formset . is_valid ( ) : return self . formset_valid ( formset ) else : return self . formset_invalid ( formset )
776	def __getDBNameForVersion ( cls , dbVersion ) : prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) suffix = suffix . replace ( "-" , "_" ) suffix = suffix . replace ( "." , "_" ) dbName = '%s_%s' % ( prefix , suffix ) return dbName
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
5444	def parse_uri ( self , raw_uri , recursive ) : if recursive : raw_uri = directory_fmt ( raw_uri ) file_provider = self . parse_file_provider ( raw_uri ) self . _validate_paths_or_fail ( raw_uri , recursive ) uri , docker_uri = self . rewrite_uris ( raw_uri , file_provider ) uri_parts = job_model . UriParts ( directory_fmt ( os . path . dirname ( uri ) ) , os . path . basename ( uri ) ) return docker_uri , uri_parts , file_provider
7040	def list_recent_datasets ( lcc_server , nrecent = 25 ) : urlparams = { 'nsets' : nrecent } urlqs = urlencode ( urlparams ) url = '%s/api/datasets?%s' % ( lcc_server , urlqs ) try : LOGINFO ( 'getting list of recent publicly ' 'visible and owned datasets from %s' % ( lcc_server , ) ) have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( url , data = None , headers = headers ) resp = urlopen ( req ) recent_datasets = json . loads ( resp . read ( ) ) [ 'result' ] return recent_datasets except HTTPError as e : LOGERROR ( 'could not retrieve recent datasets list, ' 'URL used: %s, error code: %s, reason: %s' % ( url , e . code , e . reason ) ) return None
374	def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : if is_random : if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : raise AssertionError ( "if is_random = True, the arguments are (min, max)" ) illum_settings = np . random . randint ( 0 , 3 ) if illum_settings == 0 : gamma = np . random . uniform ( gamma [ 0 ] , 1.0 ) elif illum_settings == 1 : gamma = np . random . uniform ( 1.0 , gamma [ 1 ] ) else : gamma = 1 im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( np . random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) ) im_ = np . array ( image ) else : im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( contrast ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( saturation ) im_ = np . array ( image ) return np . asarray ( im_ )
13543	def from_server ( cls , server , slug , identifier ) : task = server . get ( 'task' , replacements = { 'slug' : slug , 'identifier' : identifier } ) return cls ( ** task )
12155	def list_move_to_front ( l , value = 'other' ) : l = list ( l ) if value in l : l . remove ( value ) l . insert ( 0 , value ) return l
4263	def filter_nomedia ( album , settings = None ) : nomediapath = os . path . join ( album . src_path , ".nomedia" ) if os . path . isfile ( nomediapath ) : if os . path . getsize ( nomediapath ) == 0 : logger . info ( "Ignoring album '%s' because of present 0-byte " ".nomedia file" , album . name ) _remove_albums_with_subdirs ( album . gallery . albums , [ album . path ] ) try : os . rmdir ( album . dst_path ) except OSError as e : pass album . subdirs = [ ] album . medias = [ ] else : with open ( nomediapath , "r" ) as nomediaFile : logger . info ( "Found a .nomedia file in %s, ignoring its " "entries" , album . name ) ignored = nomediaFile . read ( ) . split ( "\n" ) album . medias = [ media for media in album . medias if media . src_filename not in ignored ] album . subdirs = [ dirname for dirname in album . subdirs if dirname not in ignored ] _remove_albums_with_subdirs ( album . gallery . albums , ignored , album . path + os . path . sep )
6111	def trace_to_next_plane ( self ) : return list ( map ( lambda positions , deflections : np . subtract ( positions , deflections ) , self . positions , self . deflections ) )
7171	def train_subprocess ( self , * args , ** kwargs ) : ret = call ( [ sys . executable , '-m' , 'padatious' , 'train' , self . cache_dir , '-d' , json . dumps ( self . serialized_args ) , '-a' , json . dumps ( args ) , '-k' , json . dumps ( kwargs ) , ] ) if ret == 2 : raise TypeError ( 'Invalid train arguments: {} {}' . format ( args , kwargs ) ) data = self . serialized_args self . clear ( ) self . apply_training_args ( data ) self . padaos . compile ( ) if ret == 0 : self . must_train = False return True elif ret == 10 : return False else : raise ValueError ( 'Training failed and returned code: {}' . format ( ret ) )
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
3818	async def _base_request ( self , url , content_type , response_type , data ) : headers = { 'content-type' : content_type , 'X-Goog-Encode-Response-If-Executable' : 'base64' , } params = { 'alt' : response_type , 'key' : API_KEY , } res = await self . _session . fetch ( 'post' , url , headers = headers , params = params , data = data , ) return res
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
9813	def url ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) try : response = PolyaxonClient ( ) . project . get_project ( user , project_name ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . has_notebook : click . echo ( get_notebook_url ( user , project_name ) ) else : Printer . print_warning ( 'This project `{}` does not have a running notebook.' . format ( project_name ) ) click . echo ( 'You can start a notebook with this command: polyaxon notebook start --help' )
8935	def provider ( workdir , commit = True , ** kwargs ) : return SCM_PROVIDER [ auto_detect ( workdir ) ] ( workdir , commit = commit , ** kwargs )
3276	def handle_copy ( self , dest_path , depth_infinity ) : if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
5810	def _parse_hello_extensions ( data ) : if data == b'' : return extentions_length = int_from_bytes ( data [ 0 : 2 ] ) extensions_start = 2 extensions_end = 2 + extentions_length pointer = extensions_start while pointer < extensions_end : extension_type = int_from_bytes ( data [ pointer : pointer + 2 ] ) extension_length = int_from_bytes ( data [ pointer + 2 : pointer + 4 ] ) yield ( extension_type , data [ pointer + 4 : pointer + 4 + extension_length ] ) pointer += 4 + extension_length
12016	def model_uncert ( self ) : Y = self . photometry_array . T Y /= np . median ( Y , axis = 1 ) [ : , None ] C = np . median ( Y , axis = 0 ) nstars , nobs = np . shape ( Y ) Z = np . empty ( ( nstars , 4 ) ) qs = self . qs . astype ( int ) for s in range ( 4 ) : Z [ : , s ] = np . median ( ( Y / C ) [ : , qs == s ] , axis = 1 ) resid2 = ( Y - Z [ : , qs ] * C ) ** 2 z = Z [ : , qs ] trend = z * C [ None , : ] lnS = np . log ( np . nanmedian ( resid2 , axis = 0 ) ) jitter = np . log ( 0.1 * np . nanmedian ( np . abs ( np . diff ( Y , axis = 1 ) ) ) ) cal_ferr = np . sqrt ( np . exp ( 2 * ( jitter / trend ) ) + z ** 2 * np . exp ( lnS ) [ None , : ] ) self . modeled_uncert = cal_ferr self . target_uncert = cal_ferr [ 0 ]
5861	def _keys_to_camel_case ( self , obj ) : return dict ( ( to_camel_case ( key ) , value ) for ( key , value ) in obj . items ( ) )
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
5208	def format_intraday ( data : pd . DataFrame , ticker , ** kwargs ) -> pd . DataFrame : if data . empty : return pd . DataFrame ( ) data . columns = pd . MultiIndex . from_product ( [ [ ticker ] , data . rename ( columns = dict ( numEvents = 'num_trds' ) ) . columns ] , names = [ 'ticker' , 'field' ] ) data . index . name = None if kwargs . get ( 'price_only' , False ) : kw_xs = dict ( axis = 1 , level = 1 ) close = data . xs ( 'close' , ** kw_xs ) volume = data . xs ( 'volume' , ** kw_xs ) . iloc [ : , 0 ] return close . loc [ volume > 0 ] if volume . min ( ) > 0 else close else : return data
6974	def rfepd_magseries ( times , mags , errs , externalparam_arrs , magsarefluxes = False , epdsmooth = True , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None , rf_subsample = 1.0 , rf_ntrees = 300 , rf_extraparams = { 'criterion' : 'mse' , 'oob_score' : False , 'n_jobs' : - 1 } ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] finalparam_arrs = [ ] for ep in externalparam_arrs : finalparam_arrs . append ( ep [ : : ] [ finind ] ) stimes , smags , serrs , eparams = sigclip_magseries_with_extparams ( times , mags , errs , externalparam_arrs , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) if epdsmooth : if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) else : smoothedmags = smags if isinstance ( rf_extraparams , dict ) : RFR = RandomForestRegressor ( n_estimators = rf_ntrees , ** rf_extraparams ) else : RFR = RandomForestRegressor ( n_estimators = rf_ntrees ) features = np . column_stack ( eparams ) if rf_subsample < 1.0 : featureindices = np . arange ( smoothedmags . size ) training_indices = np . sort ( npr . choice ( featureindices , size = int ( rf_subsample * smoothedmags . size ) , replace = False ) ) else : training_indices = np . arange ( smoothedmags . size ) RFR . fit ( features [ training_indices , : ] , smoothedmags [ training_indices ] ) flux_corrections = RFR . predict ( np . column_stack ( finalparam_arrs ) ) corrected_fmags = npmedian ( fmags ) + fmags - flux_corrections retdict = { 'times' : ftimes , 'mags' : corrected_fmags , 'errs' : ferrs , 'feature_importances' : RFR . feature_importances_ , 'regressor' : RFR , 'mags_median' : npmedian ( corrected_fmags ) , 'mags_mad' : npmedian ( npabs ( corrected_fmags - npmedian ( corrected_fmags ) ) ) } return retdict
877	def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )
1860	def LODS ( cpu , dest , src ) : src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] base , _ , ty = cpu . get_descriptor ( cpu . DS ) src_addr = cpu . read_register ( src_reg ) + base size = dest . size arg0 = cpu . read_int ( src_addr , size ) dest . write ( arg0 ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment )
3385	def _reproject ( self , p ) : nulls = self . problem . nullspace equalities = self . problem . equalities if np . allclose ( equalities . dot ( p ) , self . problem . b , rtol = 0 , atol = self . feasibility_tol ) : new = p else : LOGGER . info ( "feasibility violated in sample" " %d, trying to reproject" % self . n_samples ) new = nulls . dot ( nulls . T . dot ( p ) ) if any ( new != p ) : LOGGER . info ( "reprojection failed in sample" " %d, using random point in space" % self . n_samples ) new = self . _random_point ( ) return new
9195	def get_publication ( request ) : publication_id = request . matchdict [ 'id' ] state , messages = check_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'state' : state , 'messages' : messages , } return response_data
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
12904	def toIndex ( self , value ) : if self . _isIrNull ( value ) : ret = IR_NULL_STR else : ret = self . _toIndex ( value ) if self . isIndexHashed is False : return ret return md5 ( tobytes ( ret ) ) . hexdigest ( )
815	def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] nalln = np . all ( seq1 == "N" , axis = 1 ) nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
13658	def _addRoute ( self , f , matcher ) : self . _routes . append ( ( f . func_name , f , matcher ) )
12812	def rawDataReceived ( self , data ) : if self . _len_expected is not None : data , extra = data [ : self . _len_expected ] , data [ self . _len_expected : ] self . _len_expected -= len ( data ) else : extra = "" self . _buffer += data if self . _len_expected == 0 : data = self . _buffer . strip ( ) if data : lines = data . split ( "\r" ) for line in lines : try : message = self . factory . get_stream ( ) . get_connection ( ) . parse ( line ) if message : self . factory . get_stream ( ) . received ( [ message ] ) except ValueError : pass self . _buffer = "" self . _len_expected = None self . setLineMode ( extra )
13449	def authed_post ( self , url , data , response_code = 200 , follow = False , headers = { } ) : if not self . authed : self . authorize ( ) response = self . client . post ( url , data , follow = follow , ** headers ) self . assertEqual ( response_code , response . status_code ) return response
8089	def text ( self , txt , x , y , width = None , height = 1000000 , outline = False , draw = True , ** kwargs ) : txt = self . Text ( txt , x , y , width , height , outline = outline , ctx = None , ** kwargs ) if outline : path = txt . path if draw : path . draw ( ) return path else : return txt
12746	def as_flat_array ( iterables ) : arr = [ ] for x in iterables : arr . extend ( x ) return np . array ( arr )
8934	def auto_detect ( workdir ) : if os . path . isdir ( os . path . join ( workdir , '.git' ) ) and os . path . isfile ( os . path . join ( workdir , '.git' , 'HEAD' ) ) : return 'git' return 'unknown'
4464	def load_jam_audio ( jam_in , audio_file , validate = True , strict = True , fmt = 'auto' , ** kwargs ) : if isinstance ( jam_in , jams . JAMS ) : jam = jam_in else : jam = jams . load ( jam_in , validate = validate , strict = strict , fmt = fmt ) y , sr = librosa . load ( audio_file , ** kwargs ) if jam . file_metadata . duration is None : jam . file_metadata . duration = librosa . get_duration ( y = y , sr = sr ) return jam_pack ( jam , _audio = dict ( y = y , sr = sr ) )
6051	def map_2d_array_to_masked_1d_array_from_array_2d_and_mask ( mask , array_2d ) : total_image_pixels = mask_util . total_regular_pixels_from_mask ( mask ) array_1d = np . zeros ( shape = total_image_pixels ) index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : array_1d [ index ] = array_2d [ y , x ] index += 1 return array_1d
5111	def _get_queues ( g , queues , edge , edge_type ) : INT = numbers . Integral if isinstance ( queues , INT ) : queues = [ queues ] elif queues is None : if edge is not None : if isinstance ( edge , tuple ) : if isinstance ( edge [ 0 ] , INT ) and isinstance ( edge [ 1 ] , INT ) : queues = [ g . edge_index [ edge ] ] elif isinstance ( edge [ 0 ] , collections . Iterable ) : if np . array ( [ len ( e ) == 2 for e in edge ] ) . all ( ) : queues = [ g . edge_index [ e ] for e in edge ] else : queues = [ g . edge_index [ edge ] ] elif edge_type is not None : if isinstance ( edge_type , collections . Iterable ) : edge_type = set ( edge_type ) else : edge_type = set ( [ edge_type ] ) tmp = [ ] for e in g . edges ( ) : if g . ep ( e , 'edge_type' ) in edge_type : tmp . append ( g . edge_index [ e ] ) queues = np . array ( tmp , int ) if queues is None : queues = range ( g . number_of_edges ( ) ) return queues
3225	def iter_project ( projects , key_file = None ) : def decorator ( func ) : @ wraps ( func ) def decorated_function ( * args , ** kwargs ) : item_list = [ ] exception_map = { } for project in projects : if isinstance ( project , string_types ) : kwargs [ 'project' ] = project if key_file : kwargs [ 'key_file' ] = key_file elif isinstance ( project , dict ) : kwargs [ 'project' ] = project [ 'project' ] kwargs [ 'key_file' ] = project [ 'key_file' ] itm , exc = func ( * args , ** kwargs ) item_list . extend ( itm ) exception_map . update ( exc ) return ( item_list , exception_map ) return decorated_function return decorator
10157	def get_transition_viewset_method ( transition_name , ** kwargs ) : @ detail_route ( methods = [ 'post' ] , ** kwargs ) def inner_func ( self , request , pk = None , ** kwargs ) : object = self . get_object ( ) transition_method = getattr ( object , transition_name ) transition_method ( by = self . request . user ) if self . save_after_transition : object . save ( ) serializer = self . get_serializer ( object ) return Response ( serializer . data ) return inner_func
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
2878	def serialize_value ( self , parent_elem , value ) : if isinstance ( value , ( str , int ) ) or type ( value ) . __name__ == 'str' : parent_elem . text = str ( value ) elif value is None : parent_elem . text = None else : parent_elem . append ( value . serialize ( self ) )
337	def plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = 50 , ax = None ) : score = compute_consistency_score ( returns_test , ppc ) ax = _plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = plot_train_len , ax = ax ) ax . text ( 0.40 , 0.90 , 'Consistency score: %.1f' % score , verticalalignment = 'bottom' , horizontalalignment = 'right' , transform = ax . transAxes , ) ax . set_ylabel ( 'Cumulative returns' ) return score
5064	def update_query_parameters ( url , query_parameters ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) url_params = parse_qs ( query_string ) url_params . update ( query_parameters ) return urlunsplit ( ( scheme , netloc , path , urlencode ( sorted ( url_params . items ( ) ) , doseq = True ) , fragment ) , )
1347	def clone ( git_uri ) : hash_digest = sha256_hash ( git_uri ) local_path = home_directory_path ( FOLDER , hash_digest ) exists_locally = path_exists ( local_path ) if not exists_locally : _clone_repo ( git_uri , local_path ) else : logging . info ( "Git repository already exists locally." ) return local_path
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" replnliks . append ( [ i . est_lnlik for i in reps ] ) if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) return tab
13475	def start ( self ) : assert not self . has_started ( ) , "called start() on an active GeventLoop" self . _stop_event = Event ( ) self . _greenlet = gevent . spawn ( self . _loop )
1944	def _hook_syscall ( self , uc , data ) : logger . debug ( f"Stopping emulation at {hex(uc.reg_read(self._to_unicorn_id('RIP')))} to perform syscall" ) self . sync_unicorn_to_manticore ( ) from . . native . cpu . abstractcpu import Syscall self . _to_raise = Syscall ( ) uc . emu_stop ( )
6795	def syncdb ( self , site = None , all = 0 , database = None , ignore_errors = 1 ) : r = self . local_renderer ignore_errors = int ( ignore_errors ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) use_run_syncdb = self . version_tuple >= ( 1 , 9 , 0 ) r . env . db_syncdb_all_flag = '--all' if int ( all ) else '' r . env . db_syncdb_database = '' if database : r . env . db_syncdb_database = ' --database=%s' % database if self . is_local : r . env . project_dir = r . env . local_project_dir site = site or self . genv . SITE for _site , site_data in r . iter_unique_databases ( site = site ) : r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : if post_south : if use_run_syncdb : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --run-syncdb --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} syncdb --noinput {db_syncdb_all_flag} {db_syncdb_database}' )
5335	def get_params ( ) : parser = get_params_parser ( ) args = parser . parse_args ( ) if not args . raw and not args . enrich and not args . identities and not args . panels : print ( "No tasks enabled" ) sys . exit ( 1 ) return args
12572	def put ( self , key , value , attrs = None , format = None , append = False , ** kwargs ) : if not isinstance ( value , np . ndarray ) : super ( NumpyHDFStore , self ) . put ( key , value , format , append , ** kwargs ) else : group = self . get_node ( key ) if group is not None and not append : self . _handle . removeNode ( group , recursive = True ) group = None if group is None : paths = key . split ( '/' ) path = '/' for p in paths : if not len ( p ) : continue new_path = path if not path . endswith ( '/' ) : new_path += '/' new_path += p group = self . get_node ( new_path ) if group is None : group = self . _handle . createGroup ( path , p ) path = new_path ds_name = kwargs . get ( 'ds_name' , self . _array_dsname ) ds = self . _handle . createArray ( group , ds_name , value ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) self . _handle . flush ( ) return ds
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
8960	def clean ( _dummy_ctx , docs = False , backups = False , bytecode = False , dist = False , all = False , venv = False , tox = False , extra = '' ) : cfg = config . load ( ) notify . banner ( "Cleaning up project files" ) venv_dirs = [ 'bin' , 'include' , 'lib' , 'share' , 'local' , '.venv' ] patterns = [ 'build/' , 'pip-selfcheck.json' ] excludes = [ '.git/' , '.hg/' , '.svn/' , 'debian/*/' ] if docs or all : patterns . extend ( [ 'docs/_build/' , 'doc/_build/' ] ) if dist or all : patterns . append ( 'dist/' ) if backups or all : patterns . extend ( [ '**/*~' ] ) if bytecode or all : patterns . extend ( [ '**/*.py[co]' , '**/__pycache__/' , '*.egg-info/' , cfg . srcjoin ( '*.egg-info/' ) [ len ( cfg . project_root ) + 1 : ] , ] ) if venv : patterns . extend ( [ i + '/' for i in venv_dirs ] ) if tox : patterns . append ( '.tox/' ) else : excludes . append ( '.tox/' ) if extra : patterns . extend ( shlex . split ( extra ) ) patterns = [ antglob . includes ( i ) for i in patterns ] + [ antglob . excludes ( i ) for i in excludes ] if not venv : patterns . extend ( [ antglob . excludes ( i + '/' ) for i in venv_dirs ] ) fileset = antglob . FileSet ( cfg . project_root , patterns ) for name in fileset : notify . info ( 'rm {0}' . format ( name ) ) if name . endswith ( '/' ) : shutil . rmtree ( os . path . join ( cfg . project_root , name ) ) else : os . unlink ( os . path . join ( cfg . project_root , name ) )
5585	def output_is_valid ( self , process_data ) : if self . METADATA [ "data_type" ] == "raster" : return ( is_numpy_or_masked_array ( process_data ) or is_numpy_or_masked_array_with_tags ( process_data ) ) elif self . METADATA [ "data_type" ] == "vector" : return is_feature_list ( process_data )
244	def days_to_liquidate_positions ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 ) : DV = market_data [ 'volume' ] * market_data [ 'price' ] roll_mean_dv = DV . rolling ( window = mean_volume_window , center = False ) . mean ( ) . shift ( ) roll_mean_dv = roll_mean_dv . replace ( 0 , np . nan ) positions_alloc = pos . get_percent_alloc ( positions ) positions_alloc = positions_alloc . drop ( 'cash' , axis = 1 ) days_to_liquidate = ( positions_alloc * capital_base ) / ( max_bar_consumption * roll_mean_dv ) return days_to_liquidate . iloc [ mean_volume_window : ]
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
6084	def blurred_image_of_planes_from_1d_images_and_convolver ( total_planes , image_plane_image_1d_of_planes , image_plane_blurring_image_1d_of_planes , convolver , map_to_scaled_array ) : blurred_image_of_planes = [ ] for plane_index in range ( total_planes ) : if np . count_nonzero ( image_plane_image_1d_of_planes [ plane_index ] ) > 0 : blurred_image_1d_of_plane = blurred_image_1d_from_1d_unblurred_and_blurring_images ( unblurred_image_1d = image_plane_image_1d_of_planes [ plane_index ] , blurring_image_1d = image_plane_blurring_image_1d_of_planes [ plane_index ] , convolver = convolver ) blurred_image_of_plane = map_to_scaled_array ( array_1d = blurred_image_1d_of_plane ) blurred_image_of_planes . append ( blurred_image_of_plane ) else : blurred_image_of_planes . append ( None ) return blurred_image_of_planes
4237	def get_attached_devices_2 ( self ) : _LOGGER . info ( "Get attached devices 2" ) success , response = self . _make_request ( SERVICE_DEVICE_INFO , "GetAttachDevice2" ) if not success : return None success , devices_node = _find_node ( response . text , ".//GetAttachDevice2Response/NewAttachDevice" ) if not success : return None xml_devices = devices_node . findall ( "Device" ) devices = [ ] for d in xml_devices : ip = _xml_get ( d , 'IP' ) name = _xml_get ( d , 'Name' ) mac = _xml_get ( d , 'MAC' ) signal = _convert ( _xml_get ( d , 'SignalStrength' ) , int ) link_type = _xml_get ( d , 'ConnectionType' ) link_rate = _xml_get ( d , 'Linkspeed' ) allow_or_block = _xml_get ( d , 'AllowOrBlock' ) device_type = _convert ( _xml_get ( d , 'DeviceType' ) , int ) device_model = _xml_get ( d , 'DeviceModel' ) ssid = _xml_get ( d , 'SSID' ) conn_ap_mac = _xml_get ( d , 'ConnAPMAC' ) devices . append ( Device ( name , ip , mac , link_type , signal , link_rate , allow_or_block , device_type , device_model , ssid , conn_ap_mac ) ) return devices
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
834	def run ( self ) : print "-" * 80 + "Computing the SDR" + "-" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )
5154	def get_copy ( dict_ , key , default = None ) : value = dict_ . get ( key , default ) if value : return deepcopy ( value ) return value
4184	def window_bohman ( N ) : r x = linspace ( - 1 , 1 , N ) w = ( 1. - abs ( x ) ) * cos ( pi * abs ( x ) ) + 1. / pi * sin ( pi * abs ( x ) ) return w
2005	def _serialize_uint ( value , size = 32 , padding = 0 ) : if size <= 0 or size > 32 : raise ValueError from . account import EVMAccount if not isinstance ( value , ( int , BitVec , EVMAccount ) ) : raise ValueError if issymbolic ( value ) : bytes = ArrayVariable ( index_bits = 256 , index_max = 32 , value_bits = 8 , name = 'temp{}' . format ( uuid . uuid1 ( ) ) ) if value . size <= size * 8 : value = Operators . ZEXTEND ( value , size * 8 ) else : value = Operators . EXTRACT ( value , 0 , size * 8 ) bytes = ArrayProxy ( bytes . write_BE ( padding , value , size ) ) else : value = int ( value ) bytes = bytearray ( ) for _ in range ( padding ) : bytes . append ( 0 ) for position in reversed ( range ( size ) ) : bytes . append ( Operators . EXTRACT ( value , position * 8 , 8 ) ) assert len ( bytes ) == size + padding return bytes
10170	def set_scheduled ( self ) : with self . _idle_lock : if self . _idle : self . _idle = False return True return False
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
11712	def flatten ( d ) : if not isinstance ( d , dict ) : return [ [ d ] ] returned = [ ] for key , value in d . items ( ) : nested = flatten ( value ) for nest in nested : current_row = [ key ] current_row . extend ( nest ) returned . append ( current_row ) return returned
4310	def _build_input_format_list ( input_filepath_list , input_volumes = None , input_format = None ) : n_inputs = len ( input_filepath_list ) input_format_list = [ ] for _ in range ( n_inputs ) : input_format_list . append ( [ ] ) if input_volumes is None : vols = [ 1 ] * n_inputs else : n_volumes = len ( input_volumes ) if n_volumes < n_inputs : logger . warning ( 'Volumes were only specified for %s out of %s files.' 'The last %s files will remain at their original volumes.' , n_volumes , n_inputs , n_inputs - n_volumes ) vols = input_volumes + [ 1 ] * ( n_inputs - n_volumes ) elif n_volumes > n_inputs : logger . warning ( '%s volumes were specified but only %s input files exist.' 'The last %s volumes will be ignored.' , n_volumes , n_inputs , n_volumes - n_inputs ) vols = input_volumes [ : n_inputs ] else : vols = [ v for v in input_volumes ] if input_format is None : fmts = [ [ ] for _ in range ( n_inputs ) ] else : n_fmts = len ( input_format ) if n_fmts < n_inputs : logger . warning ( 'Input formats were only specified for %s out of %s files.' 'The last %s files will remain unformatted.' , n_fmts , n_inputs , n_inputs - n_fmts ) fmts = [ f for f in input_format ] fmts . extend ( [ [ ] for _ in range ( n_inputs - n_fmts ) ] ) elif n_fmts > n_inputs : logger . warning ( '%s Input formats were specified but only %s input files exist' '. The last %s formats will be ignored.' , n_fmts , n_inputs , n_fmts - n_inputs ) fmts = input_format [ : n_inputs ] else : fmts = [ f for f in input_format ] for i , ( vol , fmt ) in enumerate ( zip ( vols , fmts ) ) : input_format_list [ i ] . extend ( [ '-v' , '{}' . format ( vol ) ] ) input_format_list [ i ] . extend ( fmt ) return input_format_list
10161	def py_hash ( key , num_buckets ) : b , j = - 1 , 0 if num_buckets < 1 : raise ValueError ( 'num_buckets must be a positive number' ) while j < num_buckets : b = int ( j ) key = ( ( key * long ( 2862933555777941757 ) ) + 1 ) & 0xffffffffffffffff j = float ( b + 1 ) * ( float ( 1 << 31 ) / float ( ( key >> 33 ) + 1 ) ) return int ( b )
6538	def compile_masks ( masks ) : if not masks : masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] return [ re . compile ( mask ) for mask in masks ]
4617	def formatTime ( t ) : if isinstance ( t , float ) : return datetime . utcfromtimestamp ( t ) . strftime ( timeFormat ) if isinstance ( t , datetime ) : return t . strftime ( timeFormat )
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
3457	def find_bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
10772	def filled_contour ( self , min = None , max = None ) : if min is None : min = np . finfo ( np . float64 ) . min if max is None : max = np . finfo ( np . float64 ) . max vertices , codes = ( self . _contour_generator . create_filled_contour ( min , max ) ) return self . formatter ( ( min , max ) , vertices , codes )
1471	def setup ( executor ) : def signal_handler ( signal_to_handle , frame ) : Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) os . killpg ( 0 , signal . SIGTERM ) shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
925	def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 if nonNone == 0 : return None sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
4240	def _make_request ( self , service , method , params = None , body = "" , need_auth = True ) : if need_auth and not self . cookie : if not self . login ( ) : return False , None headers = self . _get_headers ( service , method , need_auth ) if not body : if not params : params = "" if isinstance ( params , dict ) : _map = params params = "" for k in _map : params += "<" + k + ">" + _map [ k ] + "</" + k + ">\n" body = CALL_BODY . format ( service = SERVICE_PREFIX + service , method = method , params = params ) message = SOAP_REQUEST . format ( session_id = SESSION_ID , body = body ) try : response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) if need_auth and _is_unauthorized_response ( response ) : self . cookie = None _LOGGER . warning ( "Unauthorized response, let's login and retry..." ) if self . login ( ) : headers = self . _get_headers ( service , method , need_auth ) response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) success = _is_valid_response ( response ) if not success : _LOGGER . error ( "Invalid response" ) _LOGGER . debug ( "%s\n%s\n%s" , response . status_code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . RequestException : _LOGGER . exception ( "Error talking to API" ) return False , None
4404	def lock ( self , name , timeout = None , sleep = 0.1 ) : return Lock ( self , name , timeout = timeout , sleep = sleep )
4744	def dev_get_rprt ( dev_name , pugrp = None , punit = None ) : cmd = [ "nvm_cmd" , "rprt_all" , dev_name ] if not ( pugrp is None and punit is None ) : cmd = [ "nvm_cmd" , "rprt_lun" , dev_name , str ( pugrp ) , str ( punit ) ] _ , _ , _ , struct = cij . test . command_to_struct ( cmd ) if not struct : return None return struct [ "rprt_descr" ]
12192	def _instruction_list ( self , filters ) : return '\n\n' . join ( [ self . INSTRUCTIONS . strip ( ) , '*Supported methods:*' , 'If you send "@{}: help" to me I reply with these ' 'instructions.' . format ( self . user ) , 'If you send "@{}: version" to me I reply with my current ' 'version.' . format ( self . user ) , ] + [ filter . description ( ) for filter in filters ] )
10360	def is_edge_consistent ( graph , u , v ) : if not graph . has_edge ( u , v ) : raise ValueError ( '{} does not contain an edge ({}, {})' . format ( graph , u , v ) ) return 0 == len ( set ( d [ RELATION ] for d in graph . edge [ u ] [ v ] . values ( ) ) )
2008	def _deserialize_int ( data , nbytes = 32 , padding = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True ) value = Operators . SEXTEND ( value , nbytes * 8 , ( nbytes + padding ) * 8 ) if not issymbolic ( value ) : if value & ( 1 << ( nbytes * 8 - 1 ) ) : value = - ( ( ( ~ value ) + 1 ) & ( ( 1 << ( nbytes * 8 ) ) - 1 ) ) return value
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] inds = np . argsort ( mass ) [ : : - 1 ] return guess [ inds ] . copy ( ) , npart
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
9408	def write_file ( obj , path , oned_as = 'row' , convert_to_float = True ) : data = _encode ( obj , convert_to_float ) try : with _WRITE_LOCK : savemat ( path , data , appendmat = False , oned_as = oned_as , long_field_names = True ) except KeyError : raise Exception ( 'could not save mat file' )
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
3543	def python_source_files ( path , tests_dirs ) : if isdir ( path ) : for root , dirs , files in os . walk ( path ) : dirs [ : ] = [ d for d in dirs if os . path . join ( root , d ) not in tests_dirs ] for filename in files : if filename . endswith ( '.py' ) : yield os . path . join ( root , filename ) else : yield path
4981	def set_final_prices ( self , modes , request ) : result = [ ] for mode in modes : if mode [ 'premium' ] : mode [ 'final_price' ] = EcommerceApiClient ( request . user ) . get_course_final_price ( mode = mode , enterprise_catalog_uuid = request . GET . get ( 'catalog' ) if request . method == 'GET' else None , ) result . append ( mode ) return result
7322	def addattachments ( message , template_path ) : if 'attachment' not in message : return message , 0 message = make_message_multipart ( message ) attachment_filepaths = message . get_all ( 'attachment' , failobj = [ ] ) template_parent_dir = os . path . dirname ( template_path ) for attachment_filepath in attachment_filepaths : attachment_filepath = os . path . expanduser ( attachment_filepath . strip ( ) ) if not attachment_filepath : continue if not os . path . isabs ( attachment_filepath ) : attachment_filepath = os . path . join ( template_parent_dir , attachment_filepath ) normalized_path = os . path . abspath ( attachment_filepath ) if not os . path . exists ( normalized_path ) : print ( "Error: can't find attachment " + normalized_path ) sys . exit ( 1 ) filename = os . path . basename ( normalized_path ) with open ( normalized_path , "rb" ) as attachment : part = email . mime . application . MIMEApplication ( attachment . read ( ) , Name = filename ) part . add_header ( 'Content-Disposition' , 'attachment; filename="{}"' . format ( filename ) ) message . attach ( part ) print ( ">>> attached {}" . format ( normalized_path ) ) del message [ 'attachment' ] return message , len ( attachment_filepaths )
4677	def getMemoKeyForAccount ( self , name ) : account = self . rpc . get_account ( name ) key = self . getPrivateKeyForPublicKey ( account [ "options" ] [ "memo_key" ] ) if key : return key return False
709	def runWithPermutationsScript ( permutationsFilePath , options , outputLabel , permWorkDir ) : global g_currentVerbosityLevel if "verbosityCount" in options : g_currentVerbosityLevel = options [ "verbosityCount" ] del options [ "verbosityCount" ] else : g_currentVerbosityLevel = 1 _setupInterruptHandling ( ) options [ "permutationsScriptPath" ] = permutationsFilePath options [ "outputLabel" ] = outputLabel options [ "outDir" ] = permWorkDir options [ "permWorkDir" ] = permWorkDir runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )
3169	def resume ( self , campaign_id ) : self . campaign_id = campaign_id return self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'actions/resume' ) )
3585	def get_all ( self , cbobjects ) : try : with self . _lock : return [ self . _metadata [ x ] for x in cbobjects ] except KeyError : raise RuntimeError ( 'Failed to find expected metadata for CoreBluetooth object!' )
11552	def analog_read ( self , pin ) : with self . data_lock : data = self . _command_handler . analog_response_table [ pin ] [ self . _command_handler . RESPONSE_TABLE_PIN_DATA_VALUE ] return data
5520	def check_codes ( self , expected_codes , received_code , info ) : if not any ( map ( received_code . matches , expected_codes ) ) : raise errors . StatusCodeError ( expected_codes , received_code , info )
1295	def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )
12976	def compat_convertHashedIndexes ( self , objs , conn = None ) : if conn is None : conn = self . _get_connection ( ) fields = [ ] for indexedField in self . indexedFields : origField = self . fields [ indexedField ] if 'hashIndex' not in origField . __class__ . __new__ . __code__ . co_varnames : continue if indexedField . hashIndex is True : hashingField = origField regField = origField . copy ( ) regField . hashIndex = False else : regField = origField hashingField = origField . copy ( ) hashingField . hashIndex = True fields . append ( ( origField , regField , hashingField ) ) objDicts = [ obj . asDict ( True , forStorage = True ) for obj in objs ] for objDict in objDicts : pipeline = conn . pipeline ( ) pk = objDict [ '_id' ] for origField , regField , hashingField in fields : val = objDict [ indexedField ] self . _rem_id_from_index ( regField , pk , val , pipeline ) self . _rem_id_from_index ( hashingField , pk , val , pipeline ) self . _add_id_to_index ( origField , pk , val , pipeline ) pipeline . execute ( )
3720	def ionic_strength ( mis , zis ) : r return 0.5 * sum ( [ mi * zi * zi for mi , zi in zip ( mis , zis ) ] )
1205	def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments
4055	def everything ( self , query ) : try : items = [ ] items . extend ( query ) while self . links . get ( "next" ) : items . extend ( self . follow ( ) ) except TypeError : items = copy . deepcopy ( query ) while self . links . get ( "next" ) : items . entries . extend ( self . follow ( ) . entries ) return items
7768	def _stream_authorized ( self , event ) : with self . lock : if event . stream != self . stream : return self . me = event . stream . me self . peer = event . stream . peer presence = self . settings [ u"initial_presence" ] if presence : self . send ( presence )
10820	def _filter ( cls , query , state = MembershipState . ACTIVE , eager = None ) : query = query . filter_by ( state = state ) eager = eager or [ ] for field in eager : query = query . options ( joinedload ( field ) ) return query
1736	def parse_num ( source , start , charset ) : while start < len ( source ) and source [ start ] in charset : start += 1 return start
9734	def get_image ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . image_count ) : component_position , image_info = QRTPacket . _get_exact ( RTImage , data , component_position ) append_components ( ( image_info , data [ component_position : - 1 ] ) ) return components
6989	def parallel_varfeatures ( lclist , outdir , maxobjects = None , timecols = None , magcols = None , errcols = None , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : lclist = lclist [ : maxobjects ] tasks = [ ( x , outdir , timecols , magcols , errcols , mindet , lcformat , lcformatdir ) for x in lclist ] with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( varfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( lclist , results ) } return resdict
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
3268	def md_dynamic_default_values_info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None defaultValueExpression = n . find ( "defaultValueExpression" ) defaultValueExpression = defaultValueExpression . text if defaultValueExpression is not None else None configurations . append ( DynamicDefaultValuesConfiguration ( dimension , policy , defaultValueExpression ) ) return DynamicDefaultValues ( name , configurations )
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
8318	def connect_table ( self , table , chunk , markup ) : k = markup . find ( chunk ) i = markup . rfind ( "\n=" , 0 , k ) j = markup . find ( "\n" , i + 1 ) paragraph_title = markup [ i : j ] . strip ( ) . strip ( "= " ) for paragraph in self . paragraphs : if paragraph . title == paragraph_title : paragraph . tables . append ( table ) table . paragraph = paragraph
5049	def proxied_get ( self , * args , ** kwargs ) : original_kwargs = kwargs . copy ( ) if 'course_id' in kwargs : try : course_run_key = str ( CourseKey . from_string ( kwargs [ 'course_id' ] ) ) except InvalidKeyError : pass else : try : return self . get ( * args , ** kwargs ) except DataSharingConsent . DoesNotExist : kwargs [ 'course_id' ] = parse_course_key ( course_run_key ) try : return self . get ( * args , ** kwargs ) except DataSharingConsent . DoesNotExist : return ProxyDataSharingConsent ( ** original_kwargs )
11416	def record_add_subfield_into ( rec , tag , subfield_code , value , subfield_position = None , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) if subfield_position is None : subfields . append ( ( subfield_code , value ) ) else : subfields . insert ( subfield_position , ( subfield_code , value ) )
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
285	def plot_drawdown_underwater ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , ** kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax
8908	def fetch_by_name ( self , name ) : service = self . collection . find_one ( { 'name' : name } ) if not service : raise ServiceNotFound return Service ( service )
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , ** kwargs )
2224	def _convert_to_hashable ( data , types = True ) : r if data is None : hashable = b'NONE' prefix = b'NULL' elif isinstance ( data , six . binary_type ) : hashable = data prefix = b'TXT' elif isinstance ( data , six . text_type ) : hashable = data . encode ( 'utf-8' ) prefix = b'TXT' elif isinstance ( data , _intlike ) : hashable = _int_to_bytes ( data ) prefix = b'INT' elif isinstance ( data , float ) : a , b = float ( data ) . as_integer_ratio ( ) hashable = _int_to_bytes ( a ) + b'/' + _int_to_bytes ( b ) prefix = b'FLT' else : hash_func = _HASHABLE_EXTENSIONS . lookup ( data ) prefix , hashable = hash_func ( data ) if types : return prefix , hashable else : return b'' , hashable
11088	def wake ( self , channel ) : self . log . info ( 'Waking up in %s' , channel ) self . _bot . dispatcher . unignore ( channel ) self . send_message ( channel , 'Hello, how may I be of service?' )
4157	def arma_estimate ( X , P , Q , lag ) : R = CORRELATION ( X , maxlags = lag , norm = 'unbiased' ) R0 = R [ 0 ] MPQ = lag - Q + P N = len ( X ) Y = np . zeros ( N - P , dtype = complex ) for K in range ( 0 , MPQ ) : KPQ = K + Q - P + 1 if KPQ < 0 : Y [ K ] = R [ - KPQ ] . conjugate ( ) if KPQ == 0 : Y [ K ] = R0 if KPQ > 0 : Y [ K ] = R [ KPQ ] Y . resize ( lag ) if P <= 4 : res = arcovar_marple ( Y . copy ( ) , P ) ar_params = res [ 0 ] else : res = arcovar ( Y . copy ( ) , P ) ar_params = res [ 0 ] Y . resize ( N - P ) for k in range ( P , N ) : SUM = X [ k ] for j in range ( 0 , P ) : SUM = SUM + ar_params [ j ] * X [ k - j - 1 ] Y [ k - P ] = SUM ma_params , rho = ma ( Y , Q , 2 * Q ) return ar_params , ma_params , rho
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
12541	def group_dicom_files ( dicom_paths , hdr_field = 'PatientID' ) : dicom_groups = defaultdict ( list ) try : for dcm in dicom_paths : hdr = dicom . read_file ( dcm ) group_key = getattr ( hdr , hdr_field ) dicom_groups [ group_key ] . append ( dcm ) except KeyError as ke : raise KeyError ( 'Error reading field {} from file {}.' . format ( hdr_field , dcm ) ) from ke return dicom_groups
4814	def _word_ngrams ( self , tokens ) : if self . stop_words is not None : tokens = [ w for w in tokens if w not in self . stop_words ] min_n , max_n = self . ngram_range if max_n != 1 : original_tokens = tokens if min_n == 1 : tokens = list ( original_tokens ) min_n += 1 else : tokens = [ ] n_original_tokens = len ( original_tokens ) tokens_append = tokens . append space_join = " " . join for n in range ( min_n , min ( max_n + 1 , n_original_tokens + 1 ) ) : for i in range ( n_original_tokens - n + 1 ) : tokens_append ( space_join ( original_tokens [ i : i + n ] ) ) return tokens
11107	def walk_files_relative_path ( self , relativePath = "" ) : def walk_files ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) files = dict . __getitem__ ( directory , 'files' ) for f in sorted ( files ) : yield os . path . join ( relativePath , f ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = directories . __getitem__ ( k ) for e in walk_files ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_files ( dir , relativePath = '' )
353	def assign_params ( sess , params , network ) : ops = [ ] for idx , param in enumerate ( params ) : ops . append ( network . all_params [ idx ] . assign ( param ) ) if sess is not None : sess . run ( ops ) return ops
10209	def check_write_permissions ( file ) : try : open ( file , 'a' ) except IOError : print ( "Can't open file {}. " "Please grant write permissions or change the path in your config" . format ( file ) ) sys . exit ( 1 )
9257	def exclude_issues_by_labels ( self , issues ) : if not self . options . exclude_labels : return copy . deepcopy ( issues ) remove_issues = set ( ) exclude_labels = self . options . exclude_labels include_issues = [ ] for issue in issues : for label in issue [ "labels" ] : if label [ "name" ] in exclude_labels : remove_issues . add ( issue [ "number" ] ) break for issue in issues : if issue [ "number" ] not in remove_issues : include_issues . append ( issue ) return include_issues
2116	def convert ( self , value , param , ctx ) : if not isinstance ( value , str ) : return value if isinstance ( value , six . binary_type ) : value = value . decode ( 'UTF-8' ) if value . startswith ( '@' ) : filename = os . path . expanduser ( value [ 1 : ] ) file_obj = super ( Variables , self ) . convert ( filename , param , ctx ) if hasattr ( file_obj , 'read' ) : return file_obj . read ( ) return file_obj return value
2865	def get_i2c_device ( address , busnum = None , i2c_interface = None , ** kwargs ) : if busnum is None : busnum = get_default_bus ( ) return Device ( address , busnum , i2c_interface , ** kwargs )
13434	def _setup_index ( index ) : index = int ( index ) if index > 0 : index -= 1 elif index == 0 : raise ValueError return index
9396	def extract_metric_name ( self , metric_name ) : for metric_type in self . supported_sar_types : if metric_type in metric_name : return metric_type logger . error ( 'Section [%s] does not contain a valid metric type, using type: "SAR-generic". Naarad works better ' 'if it knows the metric type. Valid SAR metric names are: %s' , metric_name , self . supported_sar_types ) return 'SAR-generic'
3806	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : ks = [ i ( T , P ) for i in self . ThermalConductivityGases ] return mixing_simple ( zs , ks ) elif method == LINDSAY_BROMLEY : ks = [ i ( T , P ) for i in self . ThermalConductivityGases ] mus = [ i ( T , P ) for i in self . ViscosityGases ] return Lindsay_Bromley ( T = T , ys = zs , ks = ks , mus = mus , Tbs = self . Tbs , MWs = self . MWs ) else : raise Exception ( 'Method not valid' )
10584	def remove_account ( self , name ) : acc_to_remove = None for a in self . accounts : if a . name == name : acc_to_remove = a if acc_to_remove is not None : self . accounts . remove ( acc_to_remove )
4145	def speriodogram ( x , NFFT = None , detrend = True , sampling = 1. , scale_by_freq = True , window = 'hamming' , axis = 0 ) : x = np . array ( x ) if x . ndim == 1 : axis = 0 r = x . shape [ 0 ] w = Window ( r , window ) w = w . data elif x . ndim == 2 : logging . debug ( '2D array. each row is a 1D array' ) [ r , c ] = x . shape w = np . array ( [ Window ( r , window ) . data for this in range ( c ) ] ) . reshape ( r , c ) if NFFT is None : NFFT = len ( x ) isreal = np . isrealobj ( x ) if detrend == True : m = np . mean ( x , axis = axis ) else : m = 0 if isreal == True : if x . ndim == 2 : res = ( abs ( rfft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( rfft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r else : if x . ndim == 2 : res = ( abs ( fft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( fft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r if scale_by_freq is True : df = sampling / float ( NFFT ) res *= 2 * np . pi / df if x . ndim == 1 : return res . transpose ( ) else : return res
6408	def lmean ( nums ) : r if len ( nums ) != len ( set ( nums ) ) : raise AttributeError ( 'No two values in the nums list may be equal' ) rolling_sum = 0 for i in range ( len ( nums ) ) : rolling_prod = 1 for j in range ( len ( nums ) ) : if i != j : rolling_prod *= math . log ( nums [ i ] / nums [ j ] ) rolling_sum += nums [ i ] / rolling_prod return math . factorial ( len ( nums ) - 1 ) * rolling_sum
3021	def get_access_token ( self , http = None , additional_claims = None ) : if additional_claims is None : if self . access_token is None or self . access_token_expired : self . refresh ( None ) return client . AccessTokenInfo ( access_token = self . access_token , expires_in = self . _expires_in ( ) ) else : token , unused_expiry = self . _create_token ( additional_claims ) return client . AccessTokenInfo ( access_token = token , expires_in = self . _MAX_TOKEN_LIFETIME_SECS )
7349	def parse_token ( response ) : items = response . split ( "&" ) items = [ item . split ( "=" ) for item in items ] return { key : value for key , value in items }
10204	def _parse_time ( self , tokens ) : return self . time_parser . parse ( self . parse_keyword ( Keyword . WHERE , tokens ) )
6712	def tunnel ( self , local_port , remote_port ) : r = self . local_renderer r . env . tunnel_local_port = local_port r . env . tunnel_remote_port = remote_port r . local ( ' ssh -i {key_filename} -L {tunnel_local_port}:localhost:{tunnel_remote_port} {user}@{host_string} -N' )
8521	def plot_4 ( data , * args ) : params = nonconstant_parameters ( data ) scores = np . array ( [ d [ 'mean_test_score' ] for d in data ] ) order = np . argsort ( scores ) for key in params . keys ( ) : if params [ key ] . dtype == np . dtype ( 'bool' ) : params [ key ] = params [ key ] . astype ( np . int ) p_list = [ ] for key in params . keys ( ) : x = params [ key ] [ order ] y = scores [ order ] params = params . loc [ order ] try : radius = ( np . max ( x ) - np . min ( x ) ) / 100.0 except : print ( "error making plot4 for '%s'" % key ) continue p_list . append ( build_scatter_tooltip ( x = x , y = y , radius = radius , add_line = False , tt = params , xlabel = key , title = 'Score vs %s' % key ) ) return p_list
7212	def layers ( self ) : layers = [ self . _layer_def ( style ) for style in self . styles ] return layers
7502	def fill_boot ( seqarr , newboot , newmap , spans , loci ) : cidx = 0 for i in xrange ( loci . shape [ 0 ] ) : x1 = spans [ loci [ i ] ] [ 0 ] x2 = spans [ loci [ i ] ] [ 1 ] cols = seqarr [ : , x1 : x2 ] cord = np . random . choice ( cols . shape [ 1 ] , cols . shape [ 1 ] , replace = False ) rcols = cols [ : , cord ] newboot [ : , cidx : cidx + cols . shape [ 1 ] ] = rcols newmap [ cidx : cidx + cols . shape [ 1 ] , 0 ] = i + 1 cidx += cols . shape [ 1 ] return newboot , newmap
8060	def do_fullscreen ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( True ) print ( self . response_prompt , file = self . stdout )
541	def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( "Failed to delete model checkpoint %s. " "Assuming that another worker has already deleted it" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return
3760	def mass_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count return mass_fractions ( things )
12316	def init ( self , username , reponame , force , backend = None ) : key = self . key ( username , reponame ) server_repodir = self . server_rootdir ( username , reponame , create = False ) if os . path . exists ( server_repodir ) and not force : raise RepositoryExists ( ) if os . path . exists ( server_repodir ) : shutil . rmtree ( server_repodir ) os . makedirs ( server_repodir ) with cd ( server_repodir ) : git . init ( "." , "--bare" ) if backend is not None : backend . init_repo ( server_repodir ) repodir = self . rootdir ( username , reponame , create = False ) if os . path . exists ( repodir ) and not force : raise Exception ( "Local repo already exists" ) if os . path . exists ( repodir ) : shutil . rmtree ( repodir ) os . makedirs ( repodir ) with cd ( os . path . dirname ( repodir ) ) : git . clone ( server_repodir , '--no-hardlinks' ) url = server_repodir if backend is not None : url = backend . url ( username , reponame ) repo = Repo ( username , reponame ) repo . manager = self repo . remoteurl = url repo . rootdir = self . rootdir ( username , reponame ) self . add ( repo ) return repo
13169	def _match ( self , pred ) : if not pred : return True pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num_re . match ( pred ) : index = int ( pred ) if index < 0 : if self . parent : return self . index == ( len ( self . parent . _children ) + index ) else : return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . _children : if c . tagname == tag and c . data == value : return True else : for c in self . _children : if c . tagname == pred : return True return False
12807	def incoming ( self , messages ) : if self . _observers : campfire = self . _room . get_campfire ( ) for message in messages : for observer in self . _observers : observer ( Message ( campfire , message ) )
2766	def get_volume_snapshots ( self ) : data = self . get_data ( "snapshots?resource_type=volume" ) return [ Snapshot ( token = self . token , ** snapshot ) for snapshot in data [ 'snapshots' ] ]
12188	def message_is_to_me ( self , data ) : return ( data . get ( 'type' ) == 'message' and data . get ( 'text' , '' ) . startswith ( self . address_as ) )
8115	def distance ( x0 , y0 , x1 , y1 ) : return sqrt ( pow ( x1 - x0 , 2 ) + pow ( y1 - y0 , 2 ) )
12887	def create_session ( self ) : req_url = '%s/%s' % ( self . __webfsapi , 'CREATE_SESSION' ) sid = yield from self . __session . get ( req_url , params = dict ( pin = self . pin ) , timeout = self . timeout ) text = yield from sid . text ( encoding = 'utf-8' ) doc = objectify . fromstring ( text ) return doc . sessionId . text
13290	def _iter_filepaths_with_extension ( extname , root_dir = '.' ) : if not extname . startswith ( '.' ) : extname = '.' + extname root_dir = os . path . abspath ( root_dir ) for dirname , sub_dirnames , filenames in os . walk ( root_dir ) : for filename in filenames : if os . path . splitext ( filename ) [ - 1 ] == extname : full_filename = os . path . join ( dirname , filename ) rel_filepath = os . path . relpath ( full_filename , start = root_dir ) yield rel_filepath
2955	def update ( self , containers ) : self . _containers = deepcopy ( containers ) self . __write ( containers , initialize = False )
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) cpu . _calculate_logic_flags ( dest . size , res )
13054	def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] endpoint = reverse ( 'oembed_json' ) providers = oembed . site . get_providers ( ) for provider in providers : if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
12221	def dispatch ( self , func ) : self . callees . append ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
1693	def map ( self , map_function ) : from heronpy . streamlet . impl . mapbolt import MapStreamlet map_streamlet = MapStreamlet ( map_function , self ) self . _add_child ( map_streamlet ) return map_streamlet
11301	def populate ( self ) : self . _registry = { } for provider_class in self . _registered_providers : instance = provider_class ( ) self . _registry [ instance ] = instance . regex for stored_provider in StoredProvider . objects . active ( ) : self . _registry [ stored_provider ] = stored_provider . regex self . _populated = True
2315	def orient_undirected_graph ( self , data , graph , ** kwargs ) : self . arguments [ '{CITEST}' ] = self . dir_CI_test [ self . CI_test ] self . arguments [ '{METHOD_INDEP}' ] = self . dir_method_indep [ self . method_indep ] self . arguments [ '{DIRECTED}' ] = 'TRUE' self . arguments [ '{ALPHA}' ] = str ( self . alpha ) self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_pc ( data , fixedEdges = fe , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2438	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True if validations . validate_review_comment ( comment ) : doc . reviews [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ReviewComment::Comment' ) else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
2958	def _assure_dir ( self ) : try : os . makedirs ( self . _state_dir ) except OSError as err : if err . errno != errno . EEXIST : raise
59	def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
2102	def configure_model ( self , attrs , field_name ) : self . relationship = field_name self . _set_method_names ( relationship = field_name ) if self . res_name is None : self . res_name = grammar . singularize ( attrs . get ( 'endpoint' , 'unknown' ) . strip ( '/' ) )
3616	def _should_really_index ( self , instance ) : if self . _should_index_is_method : is_method = inspect . ismethod ( self . should_index ) try : count_args = len ( inspect . signature ( self . should_index ) . parameters ) except AttributeError : count_args = len ( inspect . getargspec ( self . should_index ) . args ) if is_method or count_args is 1 : return self . should_index ( instance ) else : return self . should_index ( ) else : attr_type = type ( self . should_index ) if attr_type is DeferredAttribute : attr_value = self . should_index . __get__ ( instance , None ) elif attr_type is str : attr_value = getattr ( instance , self . should_index ) elif attr_type is property : attr_value = self . should_index . __get__ ( instance ) else : raise AlgoliaIndexError ( '{} should be a boolean attribute or a method that returns a boolean.' . format ( self . should_index ) ) if type ( attr_value ) is not bool : raise AlgoliaIndexError ( "%s's should_index (%s) should be a boolean" % ( instance . __class__ . __name__ , self . should_index ) ) return attr_value
12436	def traverse ( cls , request , params = None ) : result = cls . parse ( request . path ) if result is None : return cls , { } elif not result : raise http . exceptions . NotFound ( ) resource , data , rest = result if params : data . update ( params ) if resource is None : return cls , data if data . get ( 'path' ) is not None : request . path = data . pop ( 'path' ) elif rest is not None : request . path = rest result = resource . traverse ( request , params = data ) return result
2606	def check_memo ( self , task_id , task ) : if not self . memoize or not task [ 'memoize' ] : task [ 'hashsum' ] = None return None , None hashsum = self . make_hash ( task ) present = False result = None if hashsum in self . memo_lookup_table : present = True result = self . memo_lookup_table [ hashsum ] logger . info ( "Task %s using result from cache" , task_id ) task [ 'hashsum' ] = hashsum return present , result
2158	def _echo_method ( self , method ) : @ functools . wraps ( method ) def func ( * args , ** kwargs ) : if getattr ( method , 'deprecated' , False ) : debug . log ( 'This method is deprecated in Tower 3.0.' , header = 'warning' ) result = method ( * args , ** kwargs ) color_info = { } if isinstance ( result , dict ) and 'changed' in result : if result [ 'changed' ] : color_info [ 'fg' ] = 'yellow' else : color_info [ 'fg' ] = 'green' format = getattr ( self , '_format_%s' % ( getattr ( method , 'format_freezer' , None ) or settings . format ) ) output = format ( result ) secho ( output , ** color_info ) return func
11995	def get_algorithms ( self ) : return { 'signature' : self . signature_algorithms , 'encryption' : self . encryption_algorithms , 'serialization' : self . serialization_algorithms , 'compression' : self . compression_algorithms , }
13022	def query ( self , sql_string , * args , ** kwargs ) : commit = None columns = None if kwargs . get ( 'commit' ) is not None : commit = kwargs . pop ( 'commit' ) if kwargs . get ( 'columns' ) is not None : columns = kwargs . pop ( 'columns' ) query = self . _assemble_simple ( sql_string , * args , ** kwargs ) return self . _execute ( query , commit = commit , working_columns = columns )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) y = librosa . resample ( y , soundf . samplerate , sr ) y = librosa . util . fix_length ( y , n_samples ) return y
11481	def _create_folder ( local_folder , parent_folder_id ) : new_folder = session . communicator . create_folder ( session . token , os . path . basename ( local_folder ) , parent_folder_id ) return new_folder [ 'folder_id' ]
10148	def from_schema_mapping ( self , schema_mapping ) : responses = { } for status , response_schema in schema_mapping . items ( ) : response = { } if response_schema . description : response [ 'description' ] = response_schema . description else : raise CorniceSwaggerException ( 'Responses must have a description.' ) for field_schema in response_schema . children : location = field_schema . name if location == 'body' : title = field_schema . __class__ . __name__ if title == 'body' : title = response_schema . __class__ . __name__ + 'Body' field_schema . title = title response [ 'schema' ] = self . definitions . from_schema ( field_schema ) elif location in ( 'header' , 'headers' ) : header_schema = self . type_converter ( field_schema ) headers = header_schema . get ( 'properties' ) if headers : for header in headers . values ( ) : header . pop ( 'title' ) response [ 'headers' ] = headers pointer = response_schema . __class__ . __name__ if self . ref : response = self . _ref ( response , pointer ) responses [ status ] = response return responses
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
5218	def hist_file ( ticker : str , dt , typ = 'TRADE' ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return '' asset = ticker . split ( ) [ - 1 ] proper_ticker = ticker . replace ( '/' , '_' ) cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'
10758	def writable_stream ( handle ) : if isinstance ( handle , io . IOBase ) and sys . version_info >= ( 3 , 5 ) : return handle . writable ( ) try : handle . write ( b'' ) except ( io . UnsupportedOperation , IOError ) : return False else : return True
7223	def save ( self , recipe ) : if 'id' in recipe and recipe [ 'id' ] is not None : self . logger . debug ( "Updating existing recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe [ 'id' ] } r = self . gbdx_connection . put ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise return recipe [ 'id' ] else : self . logger . debug ( "Creating new recipe: " + json . dumps ( recipe ) ) url = '%(base_url)s/recipe/json' % { 'base_url' : self . base_url } r = self . gbdx_connection . post ( url , json = recipe ) try : r . raise_for_status ( ) except : print ( r . text ) raise recipe_json = r . json ( ) return recipe_json [ 'id' ]
3860	def _wrap_event ( event_ ) : cls = conversation_event . ConversationEvent if event_ . HasField ( 'chat_message' ) : cls = conversation_event . ChatMessageEvent elif event_ . HasField ( 'otr_modification' ) : cls = conversation_event . OTREvent elif event_ . HasField ( 'conversation_rename' ) : cls = conversation_event . RenameEvent elif event_ . HasField ( 'membership_change' ) : cls = conversation_event . MembershipChangeEvent elif event_ . HasField ( 'hangout_event' ) : cls = conversation_event . HangoutEvent elif event_ . HasField ( 'group_link_sharing_modification' ) : cls = conversation_event . GroupLinkSharingModificationEvent return cls ( event_ )
3670	def Wilson ( xs , params ) : r gammas = [ ] cmps = range ( len ( xs ) ) for i in cmps : tot1 = log ( sum ( [ params [ i ] [ j ] * xs [ j ] for j in cmps ] ) ) tot2 = 0. for j in cmps : tot2 += params [ j ] [ i ] * xs [ j ] / sum ( [ params [ j ] [ k ] * xs [ k ] for k in cmps ] ) gamma = exp ( 1. - tot1 - tot2 ) gammas . append ( gamma ) return gammas
8503	def as_call ( self ) : default = self . _default ( ) default = ', ' + default if default else '' return "pyconfig.%s(%r%s)" % ( self . method , self . get_key ( ) , default )
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
6958	def list_trilegal_filtersystems ( ) : print ( '%-40s %s' % ( 'FILTER SYSTEM NAME' , 'DESCRIPTION' ) ) print ( '%-40s %s' % ( '------------------' , '-----------' ) ) for key in sorted ( TRILEGAL_FILTER_SYSTEMS . keys ( ) ) : print ( '%-40s %s' % ( key , TRILEGAL_FILTER_SYSTEMS [ key ] [ 'desc' ] ) )
6123	def instance_for_arguments ( self , arguments ) : profiles = { ** { key : value . instance_for_arguments ( arguments ) for key , value in self . profile_prior_model_dict . items ( ) } , ** self . constant_profile_dict } try : redshift = self . redshift . instance_for_arguments ( arguments ) except AttributeError : redshift = self . redshift pixelization = self . pixelization . instance_for_arguments ( arguments ) if isinstance ( self . pixelization , pm . PriorModel ) else self . pixelization regularization = self . regularization . instance_for_arguments ( arguments ) if isinstance ( self . regularization , pm . PriorModel ) else self . regularization hyper_galaxy = self . hyper_galaxy . instance_for_arguments ( arguments ) if isinstance ( self . hyper_galaxy , pm . PriorModel ) else self . hyper_galaxy return galaxy . Galaxy ( redshift = redshift , pixelization = pixelization , regularization = regularization , hyper_galaxy = hyper_galaxy , ** profiles )
9935	def find ( self , path , all = False ) : matches = [ ] for prefix , root in self . locations : if root not in searched_locations : searched_locations . append ( root ) matched_path = self . find_location ( root , path , prefix ) if matched_path : if not all : return matched_path matches . append ( matched_path ) return matches
8829	def security_group_rule_update ( context , rule , ** kwargs ) : rule . update ( kwargs ) context . session . add ( rule ) return rule
4616	def refresh ( self ) : asset = self . blockchain . rpc . get_asset ( self . identifier ) if not asset : raise AssetDoesNotExistsException ( self . identifier ) super ( Asset , self ) . __init__ ( asset , blockchain_instance = self . blockchain ) if self . full : if "bitasset_data_id" in asset : self [ "bitasset_data" ] = self . blockchain . rpc . get_object ( asset [ "bitasset_data_id" ] ) self [ "dynamic_asset_data" ] = self . blockchain . rpc . get_object ( asset [ "dynamic_asset_data_id" ] )
11337	def connect ( self ) : for tried_connection_count in range ( CFG_FTP_CONNECTION_ATTEMPTS ) : try : self . ftp = FtpHandler ( self . config . OXFORD . URL , self . config . OXFORD . LOGIN , self . config . OXFORD . PASSWORD ) self . logger . debug ( ( "Successful connection to the " "Oxford University Press server" ) ) return except socket_timeout_exception as err : self . logger . error ( ( 'Failed to connect %d of %d times. ' 'Will sleep for %d seconds and try again.' ) % ( tried_connection_count + 1 , CFG_FTP_CONNECTION_ATTEMPTS , CFG_FTP_TIMEOUT_SLEEP_DURATION ) ) time . sleep ( CFG_FTP_TIMEOUT_SLEEP_DURATION ) except Exception as err : self . logger . error ( ( 'Failed to connect to the Oxford ' 'University Press server. %s' ) % ( err , ) ) break raise LoginException ( err )
9806	def deploy ( file , manager_path , check , dry_run ) : config = read_deployment_config ( file ) manager = DeployManager ( config = config , filepath = file , manager_path = manager_path , dry_run = dry_run ) exception = None if check : manager . check ( ) Printer . print_success ( 'Polyaxon deployment file is valid.' ) else : try : manager . install ( ) except Exception as e : Printer . print_error ( 'Polyaxon could not be installed.' ) exception = e if exception : Printer . print_error ( 'Error message `{}`.' . format ( exception ) )
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s
11532	def setup ( self , address , rack = 0 , slot = 1 , port = 102 ) : rack = int ( rack ) slot = int ( slot ) port = int ( port ) address = str ( address ) self . _client = snap7 . client . Client ( ) self . _client . connect ( address , rack , slot , port )
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
7464	def _parse_00 ( ofile ) : with open ( ofile ) as infile : arr = np . array ( [ " " ] + infile . read ( ) . split ( "Summary of MCMC results\n\n\n" ) [ 1 : ] [ 0 ] . strip ( ) . split ( ) ) rows = 12 cols = ( arr . shape [ 0 ] + 1 ) / rows arr = arr . reshape ( rows , cols ) df = pd . DataFrame ( data = arr [ 1 : , 1 : ] , columns = arr [ 0 , 1 : ] , index = arr [ 1 : , 0 ] , ) . T return df
12957	def _get_key_for_index ( self , indexedField , val ) : if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
10675	def list_compounds ( ) : print ( 'Compounds currently loaded:' ) for compound in sorted ( compounds . keys ( ) ) : phases = compounds [ compound ] . get_phase_list ( ) print ( '%s: %s' % ( compound , ', ' . join ( phases ) ) )
9559	def _apply_each_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'each' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
5562	def effective_bounds ( self ) : return snap_bounds ( bounds = clip_bounds ( bounds = self . init_bounds , clip = self . process_pyramid . bounds ) , pyramid = self . process_pyramid , zoom = min ( self . baselevels [ "zooms" ] ) if self . baselevels else min ( self . init_zoom_levels ) )
10233	def _reaction_cartesion_expansion_unqualified_helper ( graph : BELGraph , u : BaseEntity , v : BaseEntity , d : dict , ) -> None : if isinstance ( u , Reaction ) and isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) | _get_catalysts_in_reaction ( v ) for reactant , product in chain ( itt . product ( u . reactants , u . products ) , itt . product ( v . reactants , v . products ) ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( reactant , product , INCREASES ) for product , reactant in itt . product ( u . products , u . reactants ) : if reactant in enzymes or product in enzymes : continue graph . add_unqualified_edge ( product , reactant , d [ RELATION ] , ) elif isinstance ( u , Reaction ) : enzymes = _get_catalysts_in_reaction ( u ) for product in u . products : if product in enzymes : continue if v not in u . products and v not in u . reactants : graph . add_unqualified_edge ( product , v , INCREASES ) for reactant in u . reactants : graph . add_unqualified_edge ( reactant , product , INCREASES ) elif isinstance ( v , Reaction ) : enzymes = _get_catalysts_in_reaction ( v ) for reactant in v . reactants : if reactant in enzymes : continue if u not in v . products and u not in v . reactants : graph . add_unqualified_edge ( u , reactant , INCREASES ) for product in v . products : graph . add_unqualified_edge ( reactant , product , INCREASES )
7533	def concat_multiple_edits ( data , sample ) : if len ( sample . files . edits ) > 1 : cmd1 = [ "cat" ] + [ i [ 0 ] for i in sample . files . edits ] conc1 = os . path . join ( data . dirs . edits , sample . name + "_R1_concatedit.fq.gz" ) with open ( conc1 , 'w' ) as cout1 : proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = cout1 , close_fds = True ) res1 = proc1 . communicate ( ) [ 0 ] if proc1 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd1 , res1 ) conc2 = 0 if os . path . exists ( str ( sample . files . edits [ 0 ] [ 1 ] ) ) : cmd2 = [ "cat" ] + [ i [ 1 ] for i in sample . files . edits ] conc2 = os . path . join ( data . dirs . edits , sample . name + "_R2_concatedit.fq.gz" ) with gzip . open ( conc2 , 'w' ) as cout2 : proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = cout2 , close_fds = True ) res2 = proc2 . communicate ( ) [ 0 ] if proc2 . returncode : raise IPyradWarningExit ( "error in: %s, %s" , cmd2 , res2 ) sample . files . edits = [ ( conc1 , conc2 ) ] return sample . files . edits
3775	def solve_prop ( self , goal , reset_method = True ) : r if self . Tmin is None or self . Tmax is None : raise Exception ( 'Both a minimum and a maximum value are not present indicating there is not enough data for temperature dependency.' ) if not self . test_property_validity ( goal ) : raise Exception ( 'Input property is not considered plausible; no method would calculate it.' ) def error ( T ) : if reset_method : self . method = None return self . T_dependent_property ( T ) - goal try : return brenth ( error , self . Tmin , self . Tmax ) except ValueError : raise Exception ( 'To within the implemented temperature range, it is not possible to calculate the desired value.' )
9875	def aggregate_tree ( l_tree ) : def _aggregate_phase1 ( tree ) : n_tree = radix . Radix ( ) for prefix in tree . prefixes ( ) : if tree . search_worst ( prefix ) . prefix == prefix : n_tree . add ( prefix ) return n_tree def _aggregate_phase2 ( tree ) : n_tree = radix . Radix ( ) for rnode in tree : p = text ( ip_network ( text ( rnode . prefix ) ) . supernet ( ) ) r = tree . search_covered ( p ) if len ( r ) == 2 : if r [ 0 ] . prefixlen == r [ 1 ] . prefixlen == rnode . prefixlen : n_tree . add ( p ) else : n_tree . add ( rnode . prefix ) else : n_tree . add ( rnode . prefix ) return n_tree l_tree = _aggregate_phase1 ( l_tree ) if len ( l_tree . prefixes ( ) ) == 1 : return l_tree while True : r_tree = _aggregate_phase2 ( l_tree ) if l_tree . prefixes ( ) == r_tree . prefixes ( ) : break else : l_tree = r_tree del r_tree return l_tree
122	def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
13665	def update_item ( filename , item , uuid ) : with atomic_write ( os . fsencode ( str ( filename ) ) ) as temp_file : with open ( os . fsencode ( str ( filename ) ) ) as products_file : products_data = json . load ( products_file ) if 'products' in products_data [ - 1 ] : [ products_data [ i ] [ "products" ] [ 0 ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] else : [ products_data [ i ] . update ( item ) for ( i , j ) in enumerate ( products_data ) if j [ "uuid" ] == str ( uuid ) ] json . dump ( products_data , temp_file ) return True
4922	def retrieve ( self , request , pk = None ) : catalog_api = CourseCatalogApiClient ( request . user ) catalog = catalog_api . get_catalog ( pk ) self . ensure_data_exists ( request , catalog , error_message = ( "Unable to fetch API response for given catalog from endpoint '/catalog/{pk}/'. " "The resource you are looking for does not exist." . format ( pk = pk ) ) ) serializer = self . serializer_class ( catalog ) return Response ( serializer . data )
2007	def _deserialize_uint ( data , nbytes = 32 , padding = 0 , offset = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True , offset = offset ) value = Operators . ZEXTEND ( value , ( nbytes + padding ) * 8 ) return value
6086	def unmasked_blurred_image_of_planes_and_galaxies_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : return [ plane . unmasked_blurred_image_of_galaxies_from_psf ( padded_grid_stack , psf ) for plane in planes ]
10275	def generate_mechanism ( graph : BELGraph , node : BaseEntity , key : Optional [ str ] = None ) -> BELGraph : subgraph = get_upstream_causal_subgraph ( graph , node ) expand_upstream_causal ( graph , subgraph ) remove_inconsistent_edges ( subgraph ) collapse_consistent_edges ( subgraph ) if key is not None : prune_mechanism_by_data ( subgraph , key ) return subgraph
11616	def _roman ( data , scheme_map , ** kw ) : vowels = scheme_map . vowels marks = scheme_map . marks virama = scheme_map . virama consonants = scheme_map . consonants non_marks_viraama = scheme_map . non_marks_viraama max_key_length_from_scheme = scheme_map . max_key_length_from_scheme to_roman = scheme_map . to_scheme . is_roman togglers = kw . pop ( 'togglers' , set ( ) ) suspend_on = kw . pop ( 'suspend_on' , set ( ) ) suspend_off = kw . pop ( 'suspend_off' , set ( ) ) if kw : raise TypeError ( 'Unexpected keyword argument %s' % list ( kw . keys ( ) ) [ 0 ] ) buf = [ ] i = 0 had_consonant = found = False len_data = len ( data ) append = buf . append toggled = False suspended = False while i <= len_data : token = data [ i : i + max_key_length_from_scheme ] while token : if token in togglers : toggled = not toggled i += 2 found = True break if token in suspend_on : suspended = True elif token in suspend_off : suspended = False if toggled or suspended : token = token [ : - 1 ] continue if had_consonant and token in vowels : mark = marks . get ( token , '' ) if mark : append ( mark ) elif to_roman : append ( vowels [ token ] ) found = True elif token in non_marks_viraama : if had_consonant : append ( virama [ '' ] ) append ( non_marks_viraama [ token ] ) found = True if found : had_consonant = token in consonants i += len ( token ) break else : token = token [ : - 1 ] if not found : if had_consonant : append ( virama [ '' ] ) if i < len_data : append ( data [ i ] ) had_consonant = False i += 1 found = False return '' . join ( buf )
9285	def close ( self ) : self . _connected = False self . buf = b'' if self . sock is not None : self . sock . close ( )
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
5518	def clone ( self ) : return StreamThrottle ( read = self . read . clone ( ) , write = self . write . clone ( ) )
10738	def grid_evaluation ( X , Y , f , vectorized = True ) : XX = np . reshape ( np . concatenate ( [ X [ ... , None ] , Y [ ... , None ] ] , axis = 2 ) , ( X . size , 2 ) , order = 'C' ) if vectorized : ZZ = f ( XX ) else : ZZ = np . array ( [ f ( x ) for x in XX ] ) return np . reshape ( ZZ , X . shape , order = 'C' )
13	def copy_obs_dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }
8287	def adjacency ( graph , directed = False , reversed = False , stochastic = False , heuristic = None ) : v = { } for n in graph . nodes : v [ n . id ] = { } for e in graph . edges : id1 = e . node1 . id id2 = e . node2 . id if reversed : id1 , id2 = id2 , id1 v [ id1 ] [ id2 ] = 1.0 - e . weight * 0.5 if heuristic : v [ id1 ] [ id2 ] += heuristic ( id1 , id2 ) if not directed : v [ id2 ] [ id1 ] = v [ id1 ] [ id2 ] if stochastic : for id1 in v : d = sum ( v [ id1 ] . values ( ) ) for id2 in v [ id1 ] : v [ id1 ] [ id2 ] /= d return v
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid return logList
13362	def prompt ( text , default = None , hide_input = False , confirmation_prompt = False , type = None , value_proc = None , prompt_suffix = ': ' , show_default = True , err = False ) : result = None def prompt_func ( text ) : f = hide_input and hidden_prompt_func or visible_prompt_func try : echo ( text , nl = False , err = err ) return f ( '' ) except ( KeyboardInterrupt , EOFError ) : if hide_input : echo ( None , err = err ) raise Abort ( ) if value_proc is None : value_proc = convert_type ( type , default ) prompt = _build_prompt ( text , prompt_suffix , show_default , default ) while 1 : while 1 : value = prompt_func ( prompt ) if value : break elif default is not None : return default try : result = value_proc ( value ) except UsageError as e : echo ( 'Error: %s' % e . message , err = err ) continue if not confirmation_prompt : return result while 1 : value2 = prompt_func ( 'Repeat for confirmation: ' ) if value2 : break if value == value2 : return result echo ( 'Error: the two entered values do not match' , err = err )
13129	def initialize_indices ( ) : Host . init ( ) Range . init ( ) Service . init ( ) User . init ( ) Credential . init ( ) Log . init ( )
5747	def date_asn_block ( self , ip , announce_date = None ) : assignations , announce_date , keys = self . run ( ip , announce_date ) pos = next ( ( i for i , j in enumerate ( assignations ) if j is not None ) , None ) if pos is not None : block = keys [ pos ] if block != '0.0.0.0/0' : return announce_date , assignations [ pos ] , block return None
11906	def to_permutation_matrix ( matches ) : n = len ( matches ) P = np . zeros ( ( n , n ) ) P [ list ( zip ( * ( matches . items ( ) ) ) ) ] = 1 return P
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
9934	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFinder ) : raise ImproperlyConfigured ( 'Finder "%s" is not a subclass of "%s"' % ( Finder , BaseFinder ) ) return Finder ( )
4384	def allow ( self , roles , methods , with_children = True ) : def decorator ( view_func ) : _methods = [ m . upper ( ) for m in methods ] for r , m , v in itertools . product ( roles , _methods , [ view_func . __name__ ] ) : self . before_acl [ 'allow' ] . append ( ( r , m , v , with_children ) ) return view_func return decorator
2888	def disconnect ( self , callback ) : if self . weak_subscribers is not None : with self . lock : index = self . _weakly_connected_index ( callback ) if index is not None : self . weak_subscribers . pop ( index ) [ 0 ] if self . hard_subscribers is not None : try : index = self . _hard_callbacks ( ) . index ( callback ) except ValueError : pass else : self . hard_subscribers . pop ( index )
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : continue targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
5296	def get_end_date ( self , obj ) : obj_date = getattr ( obj , self . get_end_date_field ( ) ) try : obj_date = obj_date . date ( ) except AttributeError : pass return obj_date
5458	def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : job = yaml . load ( yaml_string ) dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
13866	def fromtsms ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts / 1000 ) . replace ( microsecond = ts % 1000 * 1000 ) when = when . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
12870	def migrate ( migrator , database , ** kwargs ) : @ migrator . create_table class DataItem ( pw . Model ) : created = pw . DateTimeField ( default = dt . datetime . utcnow ) content = pw . CharField ( )
2217	def _list_itemstrs ( list_ , ** kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , ** kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
11776	def WeightedMajority ( predictors , weights ) : "Return a predictor that takes a weighted vote." def predict ( example ) : return weighted_mode ( ( predictor ( example ) for predictor in predictors ) , weights ) return predict
8678	def put ( self , name , value = None , modify = False , metadata = None , description = '' , encrypt = True , lock = False , key_type = 'secret' , add = False ) : def assert_key_is_unlocked ( existing_key ) : if existing_key and existing_key . get ( 'lock' ) : raise GhostError ( 'Key `{0}` is locked and therefore cannot be modified. ' 'Unlock the key and try again' . format ( name ) ) def assert_value_provided_for_new_key ( value , existing_key ) : if not value and not existing_key . get ( 'value' ) : raise GhostError ( 'You must provide a value for new keys' ) self . _assert_valid_stash ( ) self . _validate_key_schema ( value , key_type ) if value and encrypt and not isinstance ( value , dict ) : raise GhostError ( 'Value must be of type dict' ) key = self . _handle_existing_key ( name , modify or add ) assert_key_is_unlocked ( key ) assert_value_provided_for_new_key ( value , key ) new_key = dict ( name = name , lock = lock ) if value : if add : value = self . _update_existing_key ( key , value ) new_key [ 'value' ] = self . _encrypt ( value ) if encrypt else value else : new_key [ 'value' ] = key . get ( 'value' ) new_key [ 'description' ] = description or key . get ( 'description' ) new_key [ 'created_at' ] = key . get ( 'created_at' ) or _get_current_time ( ) new_key [ 'modified_at' ] = _get_current_time ( ) new_key [ 'metadata' ] = metadata or key . get ( 'metadata' ) new_key [ 'uid' ] = key . get ( 'uid' ) or str ( uuid . uuid4 ( ) ) new_key [ 'type' ] = key . get ( 'type' ) or key_type key_id = self . _storage . put ( new_key ) audit ( storage = self . _storage . db_path , action = 'MODIFY' if ( modify or add ) else 'PUT' , message = json . dumps ( dict ( key_name = new_key [ 'name' ] , value = 'HIDDEN' , description = new_key [ 'description' ] , uid = new_key [ 'uid' ] , metadata = json . dumps ( new_key [ 'metadata' ] ) , lock = new_key [ 'lock' ] , type = new_key [ 'type' ] ) ) ) return key_id
8271	def _save ( self ) : if not os . path . exists ( self . cache ) : os . makedirs ( self . cache ) path = os . path . join ( self . cache , self . name + ".xml" ) f = open ( path , "w" ) f . write ( self . xml ) f . close ( )
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
12299	def register ( self , what , obj ) : name = obj . name version = obj . version enable = obj . enable if enable == 'n' : return key = Key ( name , version ) self . plugins [ what ] [ key ] = obj
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
8537	def push ( self , ip_packet ) : data_len = len ( ip_packet . data . data ) seq_id = ip_packet . data . seq if data_len == 0 : self . _next_seq_id = seq_id return False if self . _next_seq_id != - 1 and seq_id != self . _next_seq_id : return False self . _next_seq_id = seq_id + data_len with self . _lock_packets : self . _length += len ( ip_packet . data . data ) self . _remaining += len ( ip_packet . data . data ) self . _packets . append ( ip_packet ) return True
8533	def is_isomorphic_to ( self , other ) : return ( isinstance ( other , self . __class__ ) and len ( self . fields ) == len ( other . fields ) and all ( a . is_isomorphic_to ( b ) for a , b in zip ( self . fields , other . fields ) ) )
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
3921	def get_menu_widget ( self , close_callback ) : return ConversationMenu ( self . _coroutine_queue , self . _conversation , close_callback , self . _keys )
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
7609	def get_all_locations ( self , timeout : int = None ) : url = self . api . LOCATIONS return self . _get_model ( url , timeout = timeout )
7692	def _handle_auth_success ( self , stream , success ) : if not self . _check_authorization ( success . properties , stream ) : element = ElementTree . Element ( FAILURE_TAG ) ElementTree . SubElement ( element , SASL_QNP + "invalid-authzid" ) return True authzid = success . properties . get ( "authzid" ) if authzid : peer = JID ( success . authzid ) elif "username" in success . properties : peer = JID ( success . properties [ "username" ] , stream . me . domain ) else : peer = None stream . set_peer_authenticated ( peer , True )
2565	def async_process ( fn ) : def run ( * args , ** kwargs ) : proc = mp . Process ( target = fn , args = args , kwargs = kwargs ) proc . start ( ) return proc return run
4006	def pty_fork ( * args ) : updated_env = copy ( os . environ ) updated_env . update ( get_docker_env ( ) ) args += ( updated_env , ) executable = args [ 0 ] demote_fn = demote_to_user ( get_config_value ( constants . CONFIG_MAC_USERNAME_KEY ) ) child_pid , pty_fd = pty . fork ( ) if child_pid == 0 : demote_fn ( ) os . execle ( _executable_path ( executable ) , * args ) else : child_process = psutil . Process ( child_pid ) terminal = os . fdopen ( pty_fd , 'r' , 0 ) with streaming_to_client ( ) : while child_process . status ( ) == 'running' : output = terminal . read ( 1 ) log_to_client ( output ) _ , exit_code = os . waitpid ( child_pid , 0 ) if exit_code != 0 : raise subprocess . CalledProcessError ( exit_code , ' ' . join ( args [ : - 1 ] ) )
11394	def mock_request ( ) : current_site = Site . objects . get_current ( ) request = HttpRequest ( ) request . META [ 'SERVER_NAME' ] = current_site . domain return request
13151	def log_update ( entity , update ) : p = { 'on' : entity , 'update' : update } _log ( TYPE_CODES . UPDATE , p )
1019	def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p
6649	def inheritsFrom ( self , target_name ) : for t in self . hierarchy : if t and t . getName ( ) == target_name or target_name in t . description . get ( 'inherits' , { } ) : return True return False
13493	def read ( args ) : if args . config_file is None or not isfile ( args . config_file ) : return logging . info ( "Reading configure file: %s" % args . config_file ) config = cparser . ConfigParser ( ) config . read ( args . config_file ) if not config . has_section ( 'lrcloud' ) : raise RuntimeError ( "Configure file has no [lrcloud] section!" ) for ( name , value ) in config . items ( 'lrcloud' ) : if value == "True" : value = True elif value == "False" : value = False if getattr ( args , name ) is None : setattr ( args , name , value )
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
10616	def clear ( self ) : self . _compound_masses = self . _compound_masses * 0.0 self . _P = 1.0 self . _T = 25.0 self . _H = 0.0
7719	def xpath_eval ( self , expr ) : ctxt = common_doc . xpathNewContext ( ) ctxt . setContextNode ( self . xmlnode ) ctxt . xpathRegisterNs ( "muc" , self . ns . getContent ( ) ) ret = ctxt . xpathEval ( to_utf8 ( expr ) ) ctxt . xpathFreeContext ( ) return ret
2234	def _proc_async_iter_stream ( proc , stream , buffersize = 1 ) : from six . moves import queue from threading import Thread def enqueue_output ( proc , stream , stream_queue ) : while proc . poll ( ) is None : line = stream . readline ( ) stream_queue . put ( line ) for line in _textio_iterlines ( stream ) : stream_queue . put ( line ) stream_queue . put ( None ) stream_queue = queue . Queue ( maxsize = buffersize ) _thread = Thread ( target = enqueue_output , args = ( proc , stream , stream_queue ) ) _thread . daemon = True _thread . start ( ) return stream_queue
4210	def csvd ( A ) : U , S , V = numpy . linalg . svd ( A ) return U , S , V
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : alphas = a . gradient ( image , target ) * mask if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
3846	def parse_typing_status_message ( p ) : return TypingStatusMessage ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , timestamp = from_timestamp ( p . timestamp ) , status = p . type , )
8748	def update_scalingip ( context , id , content ) : LOG . info ( 'update_scalingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) requested_ports = content . get ( 'ports' , [ ] ) flip = _update_flip ( context , id , ip_types . SCALING , requested_ports ) return v . _make_scaling_ip_dict ( flip )
6793	def load_django_settings ( self ) : r = self . local_renderer _env = { } save_vars = [ 'ALLOW_CELERY' , 'DJANGO_SETTINGS_MODULE' ] for var_name in save_vars : _env [ var_name ] = os . environ . get ( var_name ) try : if r . env . local_project_dir : sys . path . insert ( 0 , r . env . local_project_dir ) os . environ [ 'ALLOW_CELERY' ] = '0' os . environ [ 'DJANGO_SETTINGS_MODULE' ] = r . format ( r . env . settings_module ) try : import django django . setup ( ) except AttributeError : pass settings = self . get_settings ( ) try : from django . contrib import staticfiles from django . conf import settings as _settings if settings is not None : for k , v in settings . __dict__ . items ( ) : setattr ( _settings , k , v ) else : raise ImportError except ( ImportError , RuntimeError ) : print ( 'Unable to load settings.' ) traceback . print_exc ( ) finally : for var_name , var_value in _env . items ( ) : if var_value is None : del os . environ [ var_name ] else : os . environ [ var_name ] = var_value return settings
4126	def spectrum_data ( filename ) : import os import pkg_resources info = pkg_resources . get_distribution ( 'spectrum' ) location = info . location share = os . sep . join ( [ location , "spectrum" , 'data' ] ) filename2 = os . sep . join ( [ share , filename ] ) if os . path . exists ( filename2 ) : return filename2 else : raise Exception ( 'unknown file %s' % filename2 )
79	def Dropout ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( "Expected p to be float or int or StochasticParameter, got %s." % ( type ( p ) , ) ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p2 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
10036	def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
9733	def get_6d_euler ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , euler = QRTPacket . _get_exact ( RT6DBodyEuler , data , component_position ) append_components ( ( position , euler ) ) return components
10836	def filter ( self , ** kwargs ) : if not len ( self ) : self . all ( ) new_list = filter ( lambda item : [ True for arg in kwargs if item [ arg ] == kwargs [ arg ] ] != [ ] , self ) return Profiles ( self . api , new_list )
10017	def upload_archive ( self , filename , key , auto_create_bucket = True ) : try : bucket = self . s3 . get_bucket ( self . aws . bucket ) if ( ( self . aws . region != 'us-east-1' and self . aws . region != 'eu-west-1' ) and bucket . get_location ( ) != self . aws . region ) or ( self . aws . region == 'us-east-1' and bucket . get_location ( ) != '' ) or ( self . aws . region == 'eu-west-1' and bucket . get_location ( ) != 'eu-west-1' ) : raise Exception ( "Existing bucket doesn't match region" ) except S3ResponseError : bucket = self . s3 . create_bucket ( self . aws . bucket , location = self . aws . region ) def __report_upload_progress ( sent , total ) : if not sent : sent = 0 if not total : total = 0 out ( "Uploaded " + str ( sent ) + " bytes of " + str ( total ) + " (" + str ( int ( float ( max ( 1 , sent ) ) / float ( total ) * 100 ) ) + "%)" ) k = Key ( bucket ) k . key = self . aws . bucket_path + key k . set_metadata ( 'time' , str ( time ( ) ) ) k . set_contents_from_filename ( filename , cb = __report_upload_progress , num_cb = 10 )
3789	def property_derivative_T ( self , T , P , zs , ws , order = 1 ) : r sorted_valid_methods = self . select_valid_methods ( T , P , zs , ws ) for method in sorted_valid_methods : try : return self . calculate_derivative_T ( T , P , zs , ws , method , order ) except : pass return None
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
5634	def make_toc ( sections , maxdepth = 0 ) : if not sections : return [ ] outer = min ( n for n , t in sections ) refs = [ ] for ind , sec in sections : if maxdepth and ind - outer + 1 > maxdepth : continue ref = sec . lower ( ) ref = ref . replace ( '`' , '' ) ref = ref . replace ( ' ' , '-' ) ref = ref . replace ( '?' , '' ) refs . append ( " " * ( ind - outer ) + "- [%s](#%s)" % ( sec , ref ) ) return refs
13047	def f_i18n_citation_type ( string , lang = "eng" ) : s = " " . join ( string . strip ( "%" ) . split ( "|" ) ) return s . capitalize ( )
5113	def clear ( self ) : self . _t = 0 self . num_events = 0 self . num_agents = np . zeros ( self . nE , int ) self . _fancy_heap = PriorityQueue ( ) self . _prev_edge = None self . _initialized = False self . reset_colors ( ) for q in self . edge2queue : q . clear ( )
3331	def release ( self ) : me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount -= 1 if not self . __writercount : self . __writer = None self . __condition . notifyAll ( ) elif me in self . __readers : self . __readers [ me ] -= 1 if not self . __readers [ me ] : del self . __readers [ me ] if not self . __readers : self . __condition . notifyAll ( ) else : raise ValueError ( "Trying to release unheld lock" ) finally : self . __condition . release ( )
37	def share_file ( comm , path ) : localrank , _ = get_local_rank_size ( comm ) if comm . Get_rank ( ) == 0 : with open ( path , 'rb' ) as fh : data = fh . read ( ) comm . bcast ( data ) else : data = comm . bcast ( None ) if localrank == 0 : os . makedirs ( os . path . dirname ( path ) , exist_ok = True ) with open ( path , 'wb' ) as fh : fh . write ( data ) comm . Barrier ( )
770	def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )
2397	def histogram ( ratings , min_rating = None , max_rating = None ) : ratings = [ int ( r ) for r in ratings ] if min_rating is None : min_rating = min ( ratings ) if max_rating is None : max_rating = max ( ratings ) num_ratings = int ( max_rating - min_rating + 1 ) hist_ratings = [ 0 for x in range ( num_ratings ) ] for r in ratings : hist_ratings [ r - min_rating ] += 1 return hist_ratings
9383	def parse ( self ) : file_status = True for infile in self . infile_list : file_status = file_status and naarad . utils . is_valid_file ( infile ) if not file_status : return False status = self . parse_xml_jtl ( self . aggregation_granularity ) gc . collect ( ) return status
4569	def dump ( data , file = sys . stdout , use_yaml = None , ** kwds ) : if use_yaml is None : use_yaml = ALWAYS_DUMP_YAML def dump ( fp ) : if use_yaml : yaml . safe_dump ( data , stream = fp , ** kwds ) else : json . dump ( data , fp , indent = 4 , sort_keys = True , ** kwds ) if not isinstance ( file , str ) : return dump ( file ) if os . path . isabs ( file ) : parent = os . path . dirname ( file ) if not os . path . exists ( parent ) : os . makedirs ( parent , exist_ok = True ) with open ( file , 'w' ) as fp : return dump ( fp )
492	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = self . _pool . connection ( shareable = False ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
8529	def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
11024	def _get_networking_mode ( app ) : networks = app . get ( 'networks' ) if networks : return networks [ - 1 ] . get ( 'mode' , 'container' ) container = app . get ( 'container' ) if container is not None and 'docker' in container : docker_network = container [ 'docker' ] . get ( 'network' ) if docker_network == 'USER' : return 'container' elif docker_network == 'BRIDGE' : return 'container/bridge' return 'container' if _is_legacy_ip_per_task ( app ) else 'host'
12333	def wait ( self , cmd , raise_on_error = True ) : _ , stdout , stderr = self . exec_command ( cmd ) stdout . channel . recv_exit_status ( ) output = stdout . read ( ) if self . interactive : print ( output ) errors = stderr . read ( ) if self . interactive : print ( errors ) if errors and raise_on_error : raise ValueError ( errors ) return output
5801	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : all_purposes = '2.5.29.37.0' ca_path = system_path ( ) output = [ ] with open ( ca_path , 'rb' ) as f : for armor_type , _ , cert_bytes in unarmor ( f . read ( ) , multiple = True ) : if armor_type == 'CERTIFICATE' : if cert_callback : cert_callback ( Certificate . load ( cert_bytes ) , None ) output . append ( ( cert_bytes , set ( ) , set ( ) ) ) elif armor_type == 'TRUSTED CERTIFICATE' : cert , aux = TrustedCertificate . load ( cert_bytes ) reject_all = False trust_oids = set ( ) reject_oids = set ( ) for purpose in aux [ 'trust' ] : if purpose . dotted == all_purposes : trust_oids = set ( [ purpose . dotted ] ) break trust_oids . add ( purpose . dotted ) for purpose in aux [ 'reject' ] : if purpose . dotted == all_purposes : reject_all = True break reject_oids . add ( purpose . dotted ) if reject_all : if cert_callback : cert_callback ( cert , 'explicitly distrusted' ) continue if cert_callback and not callback_only_on_failure : cert_callback ( cert , None ) output . append ( ( cert . dump ( ) , trust_oids , reject_oids ) ) return output
3020	def create_with_claims ( self , claims ) : new_kwargs = dict ( self . _kwargs ) new_kwargs . update ( claims ) result = self . __class__ ( self . _service_account_email , self . _signer , scopes = self . _scopes , private_key_id = self . _private_key_id , client_id = self . client_id , user_agent = self . _user_agent , ** new_kwargs ) result . token_uri = self . token_uri result . revoke_uri = self . revoke_uri result . _private_key_pkcs8_pem = self . _private_key_pkcs8_pem result . _private_key_pkcs12 = self . _private_key_pkcs12 result . _private_key_password = self . _private_key_password return result
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
10226	def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
12565	def get_3D_from_4D ( image , vol_idx = 0 ) : img = check_img ( image ) hdr , aff = get_img_info ( img ) if len ( img . shape ) != 4 : raise AttributeError ( 'Volume in {} does not have 4 dimensions.' . format ( repr_imgs ( img ) ) ) if not 0 <= vol_idx < img . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, ' 'not {}.' . format ( repr_imgs ( img ) , img . shape [ 3 ] , vol_idx ) ) img_data = img . get_data ( ) new_vol = img_data [ : , : , : , vol_idx ] . copy ( ) hdr . set_data_shape ( hdr . get_data_shape ( ) [ : 3 ] ) return new_vol , hdr , aff
890	def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
3660	def _coeff_ind_from_T ( self , T ) : if self . n == 1 : return 0 for i in range ( self . n ) : if T <= self . Ts [ i + 1 ] : return i return self . n - 1
11590	def _rc_smove ( self , src , dst , value ) : if self . type ( src ) != b ( "set" ) : return self . smove ( src + "{" + src + "}" , dst , value ) if self . type ( dst ) != b ( "set" ) : return self . smove ( dst + "{" + dst + "}" , src , value ) if self . srem ( src , value ) : return 1 if self . sadd ( dst , value ) else 0 return 0
7990	def transport_connected ( self ) : with self . lock : if self . initiator : if self . _output_state is None : self . _initiate ( )
5555	def _element_at_zoom ( name , element , zoom ) : if isinstance ( element , dict ) : if "format" in element : return element out_elements = { } for sub_name , sub_element in element . items ( ) : out_element = _element_at_zoom ( sub_name , sub_element , zoom ) if name == "input" : out_elements [ sub_name ] = out_element elif out_element is not None : out_elements [ sub_name ] = out_element if len ( out_elements ) == 1 and name != "input" : return next ( iter ( out_elements . values ( ) ) ) if len ( out_elements ) == 0 : return None return out_elements elif isinstance ( name , str ) : if name . startswith ( "zoom" ) : return _filter_by_zoom ( conf_string = name . strip ( "zoom" ) . strip ( ) , zoom = zoom , element = element ) else : return element else : return element
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
4672	def newWallet ( self , pwd ) : if self . created ( ) : raise WalletExists ( "You already have created a wallet!" ) self . store . unlock ( pwd )
13056	def _plugin_endpoint_rename ( fn_name , instance ) : if instance and instance . namespaced : fn_name = "r_{0}_{1}" . format ( instance . name , fn_name [ 2 : ] ) return fn_name
9823	def delete ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not click . confirm ( "Are sure you want to delete project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without deleting project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . delete_project ( user , project_name ) local_project = ProjectManager . get_config ( ) if local_project and ( user , project_name ) == ( local_project . user , local_project . name ) : ProjectManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete project `{}/{}`.' . format ( user , project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Project `{}/{}` was delete successfully" . format ( user , project_name ) )
8724	def from_timestamp ( ts ) : return datetime . datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = pytz . utc )
11687	def changeset_info ( changeset ) : keys = [ tag . attrib . get ( 'k' ) for tag in changeset . getchildren ( ) ] keys += [ 'id' , 'user' , 'uid' , 'bbox' , 'created_at' ] values = [ tag . attrib . get ( 'v' ) for tag in changeset . getchildren ( ) ] values += [ changeset . get ( 'id' ) , changeset . get ( 'user' ) , changeset . get ( 'uid' ) , get_bounds ( changeset ) , changeset . get ( 'created_at' ) ] return dict ( zip ( keys , values ) )
9231	def fetch_events_async ( self , issues , tag_name ) : if not issues : return issues max_simultaneous_requests = self . options . max_simultaneous_requests verbose = self . options . verbose gh = self . github user = self . options . user repo = self . options . project self . events_cnt = 0 if verbose : print ( "fetching events for {} {}... " . format ( len ( issues ) , tag_name ) ) def worker ( issue ) : page = 1 issue [ 'events' ] = [ ] while page > 0 : rc , data = gh . repos [ user ] [ repo ] . issues [ issue [ 'number' ] ] . events . get ( page = page , per_page = PER_PAGE_NUMBER ) if rc == 200 : issue [ 'events' ] . extend ( data ) self . events_cnt += len ( data ) else : self . raise_GitHubError ( rc , data , gh . getheaders ( ) ) page = NextPage ( gh ) threads = [ ] cnt = len ( issues ) for i in range ( 0 , ( cnt // max_simultaneous_requests ) + 1 ) : for j in range ( max_simultaneous_requests ) : idx = i * max_simultaneous_requests + j if idx == cnt : break t = threading . Thread ( target = worker , args = ( issues [ idx ] , ) ) threads . append ( t ) t . start ( ) if verbose > 2 : print ( "." , end = "" ) if not idx % PER_PAGE_NUMBER : print ( "" ) for t in threads : t . join ( ) if verbose > 2 : print ( "." )
1138	def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''
8928	def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) ctx . run ( 'python setup.py -q develop -U' ) version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
9729	def get_analog_single ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDeviceSingle , data , component_position ) RTAnalogDeviceSamples . format = struct . Struct ( RTAnalogDeviceSamples . format_str % device . channel_count ) component_position , sample = QRTPacket . _get_tuple ( RTAnalogDeviceSamples , data , component_position ) append_components ( ( device , sample ) ) return components
6012	def load_exposure_time_map ( exposure_time_map_path , exposure_time_map_hdu , pixel_scale , shape , exposure_time , exposure_time_map_from_inverse_noise_map , inverse_noise_map ) : exposure_time_map_options = sum ( [ exposure_time_map_from_inverse_noise_map ] ) if exposure_time is not None and exposure_time_map_path is not None : raise exc . DataException ( 'You have supplied both a exposure_time_map_path to an exposure time map and an exposure time. Only' 'one quantity should be supplied.' ) if exposure_time_map_options == 0 : if exposure_time is not None and exposure_time_map_path is None : return ExposureTimeMap . single_value ( value = exposure_time , pixel_scale = pixel_scale , shape = shape ) elif exposure_time is None and exposure_time_map_path is not None : return ExposureTimeMap . from_fits_with_pixel_scale ( file_path = exposure_time_map_path , hdu = exposure_time_map_hdu , pixel_scale = pixel_scale ) else : if exposure_time_map_from_inverse_noise_map : return ExposureTimeMap . from_exposure_time_and_inverse_noise_map ( pixel_scale = pixel_scale , exposure_time = exposure_time , inverse_noise_map = inverse_noise_map )
5147	def generate ( self ) : tar_bytes = BytesIO ( ) tar = tarfile . open ( fileobj = tar_bytes , mode = 'w' ) self . _generate_contents ( tar ) self . _process_files ( tar ) tar . close ( ) tar_bytes . seek ( 0 ) gzip_bytes = BytesIO ( ) gz = gzip . GzipFile ( fileobj = gzip_bytes , mode = 'wb' , mtime = 0 ) gz . write ( tar_bytes . getvalue ( ) ) gz . close ( ) gzip_bytes . seek ( 0 ) return gzip_bytes
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
10069	def index ( method = None , delete = False ) : if method is None : return partial ( index , delete = delete ) @ wraps ( method ) def wrapper ( self_or_cls , * args , ** kwargs ) : result = method ( self_or_cls , * args , ** kwargs ) try : if delete : self_or_cls . indexer . delete ( result ) else : self_or_cls . indexer . index ( result ) except RequestError : current_app . logger . exception ( 'Could not index {0}.' . format ( result ) ) return result return wrapper
8774	def _load_worker_plugin_with_module ( self , module , version ) : classes = inspect . getmembers ( module , inspect . isclass ) loaded = 0 for cls_name , cls in classes : if hasattr ( cls , 'versions' ) : if version not in cls . versions : continue else : continue if issubclass ( cls , base_worker . QuarkAsyncPluginBase ) : LOG . debug ( "Loading plugin %s" % cls_name ) plugin = cls ( ) self . plugins . append ( plugin ) loaded += 1 LOG . debug ( "Found %d possible plugins and loaded %d" % ( len ( classes ) , loaded ) )
9480	def _arg_parser ( ) : description = "Converts a completezip to a litezip" parser = argparse . ArgumentParser ( description = description ) verbose_group = parser . add_mutually_exclusive_group ( ) verbose_group . add_argument ( '-v' , '--verbose' , action = 'store_true' , dest = 'verbose' , default = None , help = "increase verbosity" ) verbose_group . add_argument ( '-q' , '--quiet' , action = 'store_false' , dest = 'verbose' , default = None , help = "print nothing to stdout or stderr" ) parser . add_argument ( 'location' , help = "Location of the unpacked litezip" ) return parser
10504	def removecallback ( window_name ) : if window_name in _pollEvents . _callback : del _pollEvents . _callback [ window_name ] return _remote_removecallback ( window_name )
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
4895	def get_enterprise_user_id ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . id
2912	def _ready ( self ) : if self . _has_state ( self . COMPLETED ) or self . _has_state ( self . CANCELLED ) : return self . _set_state ( self . READY ) self . task_spec . _on_ready ( self )
4169	def zpk2tf ( z , p , k ) : r import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
1372	def get_heron_libs ( local_jars ) : heron_lib_dir = get_heron_lib_dir ( ) heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] return heron_libs
22	def get_wrapper_by_name ( env , classname ) : currentenv = env while True : if classname == currentenv . class_name ( ) : return currentenv elif isinstance ( currentenv , gym . Wrapper ) : currentenv = currentenv . env else : raise ValueError ( "Couldn't find wrapper named %s" % classname )
5583	def get_path ( self , tile ) : return os . path . join ( * [ self . path , str ( tile . zoom ) , str ( tile . row ) , str ( tile . col ) + self . file_extension ] )
10650	def add_activity ( self , activity ) : self . gl . structure . validate_account_names ( activity . get_referenced_accounts ( ) ) self . activities . append ( activity ) activity . set_parent_path ( self . path )
9475	def add_node ( self , label ) : try : n = self . _nodes [ label ] except KeyError : n = Node ( ) n [ 'label' ] = label self . _nodes [ label ] = n return n
6762	def write_pgpass ( self , name = None , site = None , use_sudo = 0 , root = 0 ) : r = self . database_renderer ( name = name , site = site ) root = int ( root ) use_sudo = int ( use_sudo ) r . run ( 'touch {pgpass_path}' ) if '~' in r . env . pgpass_path : r . run ( 'chmod {pgpass_chmod} {pgpass_path}' ) else : r . sudo ( 'chmod {pgpass_chmod} {pgpass_path}' ) if root : r . env . shell_username = r . env . get ( 'db_root_username' , 'postgres' ) r . env . shell_password = r . env . get ( 'db_root_password' , 'password' ) else : r . env . shell_username = r . env . db_user r . env . shell_password = r . env . db_password r . append ( '{db_host}:{port}:*:{shell_username}:{shell_password}' , r . env . pgpass_path , use_sudo = use_sudo )
9323	def refresh_information ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response ) self . _loaded_information = True
6583	def _post_start ( self ) : flags = fcntl . fcntl ( self . _process . stdout , fcntl . F_GETFL ) fcntl . fcntl ( self . _process . stdout , fcntl . F_SETFL , flags | os . O_NONBLOCK )
4752	def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
9356	def words ( quantity = 10 , as_list = False ) : global _words if not _words : _words = ' ' . join ( get_dictionary ( 'lorem_ipsum' ) ) . lower ( ) . replace ( '\n' , '' ) _words = re . sub ( r'\.|,|;/' , '' , _words ) _words = _words . split ( ' ' ) result = random . sample ( _words , quantity ) if as_list : return result else : return ' ' . join ( result )
12181	def api_subclass_factory ( name , docstring , remove_methods , base = SlackApi ) : methods = deepcopy ( base . API_METHODS ) for parent , to_remove in remove_methods . items ( ) : if to_remove is ALL : del methods [ parent ] else : for method in to_remove : del methods [ parent ] [ method ] return type ( name , ( base , ) , dict ( API_METHODS = methods , __doc__ = docstring ) )
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
10486	def _generateFindR ( self , ** kwargs ) : for needle in self . _generateChildrenR ( ) : if needle . _match ( ** kwargs ) : yield needle
7426	def refmap_stats ( data , sample ) : mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
6623	def _getTarball ( url , into_directory , cache_key , origin_info = None ) : try : access_common . unpackFromCache ( cache_key , into_directory ) except KeyError as e : tok = settings . getProperty ( 'github' , 'authtoken' ) headers = { } if tok is not None : headers [ 'Authorization' ] = 'token ' + str ( tok ) logger . debug ( 'GET %s' , url ) response = requests . get ( url , allow_redirects = True , stream = True , headers = headers ) response . raise_for_status ( ) logger . debug ( 'getting file: %s' , url ) logger . debug ( 'headers: %s' , response . headers ) response . raise_for_status ( ) access_common . unpackTarballStream ( stream = response , into_directory = into_directory , hash = { } , cache_key = cache_key , origin_info = origin_info )
7042	def stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = False ) : ndet = len ( fmags ) if ndet > 9 : medmag = npmedian ( fmags ) delta_prefactor = ( ndet / ( ndet - 1 ) ) sigma_i = delta_prefactor * ( fmags - medmag ) / ferrs sigma_j = nproll ( sigma_i , 1 ) if weightbytimediff : difft = npdiff ( ftimes ) deltat = npmedian ( difft ) weights_i = npexp ( - difft / deltat ) products = ( weights_i * sigma_i [ 1 : ] * sigma_j [ 1 : ] ) else : products = ( sigma_i * sigma_j ) [ 1 : ] stetsonj = ( npsum ( npsign ( products ) * npsqrt ( npabs ( products ) ) ) ) / ndet return stetsonj else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate stetson J index' ) return npnan
12610	def _concat_queries ( queries , operators = '__and__' ) : if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
3960	def update_local_repo_async ( self , task_queue , force = False ) : self . ensure_local_repo ( ) task_queue . enqueue_task ( self . update_local_repo , force = force )
3511	def sample ( model , n , method = "optgp" , thinning = 100 , processes = 1 , seed = None ) : if method == "optgp" : sampler = OptGPSampler ( model , processes , thinning = thinning , seed = seed ) elif method == "achr" : sampler = ACHRSampler ( model , thinning = thinning , seed = seed ) else : raise ValueError ( "method must be 'optgp' or 'achr'!" ) return pandas . DataFrame ( columns = [ rxn . id for rxn in model . reactions ] , data = sampler . sample ( n ) )
9597	def save_screenshot ( self , filename , quietly = False ) : imgData = self . take_screenshot ( ) try : with open ( filename , "wb" ) as f : f . write ( b64decode ( imgData . encode ( 'ascii' ) ) ) except IOError as err : if not quietly : raise err
6629	def read ( self , filenames ) : for fn in filenames : try : self . configs [ fn ] = ordered_json . load ( fn ) except IOError : self . configs [ fn ] = OrderedDict ( ) except Exception as e : self . configs [ fn ] = OrderedDict ( ) logging . warning ( "Failed to read settings file %s, it will be ignored. The error was: %s" , fn , e )
12040	def checkOut ( thing , html = True ) : msg = "" for name in sorted ( dir ( thing ) ) : if not "__" in name : msg += "<b>%s</b>\n" % name try : msg += " ^-VALUE: %s\n" % getattr ( thing , name ) ( ) except : pass if html : html = '<html><body><code>' + msg + '</code></body></html>' html = html . replace ( " " , "&nbsp;" ) . replace ( "\n" , "<br>" ) fname = tempfile . gettempdir ( ) + "/swhlab/checkout.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname ) print ( msg . replace ( '<b>' , '' ) . replace ( '</b>' , '' ) )
8819	def get_networks_count ( context , filters = None ) : LOG . info ( "get_networks_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . network_count_all ( context )
12635	def levenshtein_analysis ( self , field_weights = None ) : if field_weights is None : if not isinstance ( self . field_weights , dict ) : raise ValueError ( 'Expected a dict for `field_weights` parameter, ' 'got {}' . format ( type ( self . field_weights ) ) ) key_dicoms = list ( self . dicom_groups . keys ( ) ) file_dists = calculate_file_distances ( key_dicoms , field_weights , self . _dist_method_cls ) return file_dists
2476	def set_lic_comment ( self , doc , comment ) : if self . has_extr_lic ( doc ) : if not self . extr_lic_comment_set : self . extr_lic_comment_set = True if validations . validate_is_free_form_text ( comment ) : self . extr_lic ( doc ) . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'ExtractedLicense::comment' ) else : raise CardinalityError ( 'ExtractedLicense::comment' ) else : raise OrderError ( 'ExtractedLicense::comment' )
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
12229	def autodiscover_siteprefs ( admin_site = None ) : if admin_site is None : admin_site = admin . site if 'manage' not in sys . argv [ 0 ] or ( len ( sys . argv ) > 1 and sys . argv [ 1 ] in MANAGE_SAFE_COMMANDS ) : import_prefs ( ) Preference . read_prefs ( get_prefs ( ) ) register_admin_models ( admin_site )
2756	def get_all_tags ( self ) : data = self . get_data ( "tags" ) return [ Tag ( token = self . token , ** tag ) for tag in data [ 'tags' ] ]
11052	def sync ( self ) : self . log . info ( 'Starting a sync...' ) def log_success ( result ) : self . log . info ( 'Sync completed successfully' ) return result def log_failure ( failure ) : self . log . failure ( 'Sync failed' , failure , LogLevel . error ) return failure return ( self . marathon_client . get_apps ( ) . addCallback ( self . _apps_acme_domains ) . addCallback ( self . _filter_new_domains ) . addCallback ( self . _issue_certs ) . addCallbacks ( log_success , log_failure ) )
2176	def request ( self , method , url , data = None , headers = None , withhold_token = False , client_id = None , client_secret = None , ** kwargs ) : if not is_secure_transport ( url ) : raise InsecureTransportError ( ) if self . token and not withhold_token : log . debug ( "Invoking %d protected resource request hooks." , len ( self . compliance_hook [ "protected_request" ] ) , ) for hook in self . compliance_hook [ "protected_request" ] : log . debug ( "Invoking hook %s." , hook ) url , headers , data = hook ( url , headers , data ) log . debug ( "Adding token %s to request." , self . token ) try : url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) except TokenExpiredError : if self . auto_refresh_url : log . debug ( "Auto refresh is set, attempting to refresh at %s." , self . auto_refresh_url , ) auth = kwargs . pop ( "auth" , None ) if client_id and client_secret and ( auth is None ) : log . debug ( 'Encoding client_id "%s" with client_secret as Basic auth credentials.' , client_id , ) auth = requests . auth . HTTPBasicAuth ( client_id , client_secret ) token = self . refresh_token ( self . auto_refresh_url , auth = auth , ** kwargs ) if self . token_updater : log . debug ( "Updating token to %s using %s." , token , self . token_updater ) self . token_updater ( token ) url , headers , data = self . _client . add_token ( url , http_method = method , body = data , headers = headers ) else : raise TokenUpdated ( token ) else : raise log . debug ( "Requesting url %s using method %s." , url , method ) log . debug ( "Supplying headers %s and data %s" , headers , data ) log . debug ( "Passing through key word arguments %s." , kwargs ) return super ( OAuth2Session , self ) . request ( method , url , headers = headers , data = data , ** kwargs )
9458	def sound_touch_stop ( self , call_params ) : path = '/' + self . api_version + '/SoundTouchStop/' method = 'POST' return self . request ( path , method , call_params )
2507	def get_extr_lics_xref ( self , extr_lic ) : xrefs = list ( self . graph . triples ( ( extr_lic , RDFS . seeAlso , None ) ) ) return map ( lambda xref_triple : xref_triple [ 2 ] , xrefs )
3535	def clickmap ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return ClickmapNode ( )
6011	def load_psf ( psf_path , psf_hdu , pixel_scale , renormalize = False ) : if renormalize : return PSF . from_fits_renormalized ( file_path = psf_path , hdu = psf_hdu , pixel_scale = pixel_scale ) if not renormalize : return PSF . from_fits_with_scale ( file_path = psf_path , hdu = psf_hdu , pixel_scale = pixel_scale )
11096	def select_by_pattern_in_fname ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . fname else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . fname . lower ( ) return self . select_file ( filters , recursive )
5486	def jsonify_status_code ( status_code , * args , ** kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , ** kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , ** kw ) response . status_code = status_code return response
693	def loadExperiment ( path ) : if not os . path . isdir ( path ) : path = os . path . dirname ( path ) descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) return expIface . getModelDescription ( ) , expIface . getModelControl ( )
4008	def _increase_file_handle_limit ( ) : logging . info ( 'Increasing file handle limit to {}' . format ( constants . FILE_HANDLE_LIMIT ) ) resource . setrlimit ( resource . RLIMIT_NOFILE , ( constants . FILE_HANDLE_LIMIT , resource . RLIM_INFINITY ) )
2797	def transfer ( self , new_region_slug ) : return self . get_data ( "images/%s/actions/" % self . id , type = POST , params = { "type" : "transfer" , "region" : new_region_slug } )
7817	def remove_handler ( self , handler ) : with self . lock : if handler in self . handlers : self . handlers . remove ( handler ) self . _update_handlers ( )
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
13873	def CopyFile ( source_filename , target_filename , override = True , md5_check = False , copy_symlink = True ) : from . _exceptions import FileNotFoundError if not override and Exists ( target_filename ) : from . _exceptions import FileAlreadyExistsError raise FileAlreadyExistsError ( target_filename ) md5_check = md5_check and not target_filename . endswith ( '.md5' ) if md5_check : source_md5_filename = source_filename + '.md5' target_md5_filename = target_filename + '.md5' try : source_md5_contents = GetFileContents ( source_md5_filename ) except FileNotFoundError : source_md5_contents = None try : target_md5_contents = GetFileContents ( target_md5_filename ) except FileNotFoundError : target_md5_contents = None if source_md5_contents is not None and source_md5_contents == target_md5_contents and Exists ( target_filename ) : return MD5_SKIP _DoCopyFile ( source_filename , target_filename , copy_symlink = copy_symlink ) if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents : CreateFile ( target_md5_filename , source_md5_contents )
1010	def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , minNumSyns ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold nSegsRemoved , nSynsRemoved = 0 , 0 segsToDel = [ ] for segment in segList : synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] if len ( synsToDel ) == len ( segment . syns ) : segsToDel . append ( segment ) else : if len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) nSynsRemoved += 1 if len ( segment . syns ) < minNumSyns : segsToDel . append ( segment ) nSegsRemoved += len ( segsToDel ) for seg in segsToDel : self . _cleanUpdatesList ( colIdx , cellIdx , seg ) self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) nSynsRemoved += len ( seg . syns ) return nSegsRemoved , nSynsRemoved
7703	def get_items_by_group ( self , group , case_sensitive = True ) : result = [ ] if not group : for item in self . _items : if not item . groups : result . append ( item ) return result if not case_sensitive : group = group . lower ( ) for item in self . _items : if group in item . groups : result . append ( item ) elif not case_sensitive and group in [ g . lower ( ) for g in item . groups ] : result . append ( item ) return result
1078	def isoformat ( self ) : return "%s-%s-%s" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )
4068	def update_items ( self , payload ) : to_send = [ self . check_items ( [ p ] ) [ 0 ] for p in payload ] headers = { } headers . update ( self . default_headers ( ) ) for chunk in chunks ( to_send , 50 ) : req = requests . post ( url = self . endpoint + "/{t}/{u}/items/" . format ( t = self . library_type , u = self . library_id ) , headers = headers , data = json . dumps ( chunk ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
6038	def yticks ( self ) : return np . linspace ( np . min ( self [ : , 0 ] ) , np . max ( self [ : , 0 ] ) , 4 )
9676	def _calculate_period ( self , vals ) : if len ( vals ) < 4 : return None if self . firmware [ 'major' ] < 16 : return ( ( vals [ 3 ] << 24 ) | ( vals [ 2 ] << 16 ) | ( vals [ 1 ] << 8 ) | vals [ 0 ] ) / 12e6 else : return self . _calculate_float ( vals )
9805	def delete ( username ) : try : PolyaxonClient ( ) . user . delete_user ( username ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete user `{}`.' . format ( username ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "User `{}` was deleted successfully." . format ( username ) )
5115	def copy ( self ) : net = QueueNetwork ( None ) net . g = self . g . copy ( ) net . max_agents = copy . deepcopy ( self . max_agents ) net . nV = copy . deepcopy ( self . nV ) net . nE = copy . deepcopy ( self . nE ) net . num_agents = copy . deepcopy ( self . num_agents ) net . num_events = copy . deepcopy ( self . num_events ) net . _t = copy . deepcopy ( self . _t ) net . _initialized = copy . deepcopy ( self . _initialized ) net . _prev_edge = copy . deepcopy ( self . _prev_edge ) net . _blocking = copy . deepcopy ( self . _blocking ) net . colors = copy . deepcopy ( self . colors ) net . out_edges = copy . deepcopy ( self . out_edges ) net . in_edges = copy . deepcopy ( self . in_edges ) net . edge2queue = copy . deepcopy ( self . edge2queue ) net . _route_probs = copy . deepcopy ( self . _route_probs ) if net . _initialized : keys = [ q . _key ( ) for q in net . edge2queue if q . _time < np . infty ] net . _fancy_heap = PriorityQueue ( keys , net . nE ) return net
7499	def resolve_ambigs ( tmpseq ) : for ambig in np . uint8 ( [ 82 , 83 , 75 , 87 , 89 , 77 ] ) : idx , idy = np . where ( tmpseq == ambig ) res1 , res2 = AMBIGS [ ambig . view ( "S1" ) ] halfmask = np . random . choice ( [ True , False ] , idx . shape [ 0 ] ) for i in xrange ( halfmask . shape [ 0 ] ) : if halfmask [ i ] : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res1 ) . view ( np . uint8 ) else : tmpseq [ idx [ i ] , idy [ i ] ] = np . array ( res2 ) . view ( np . uint8 ) return tmpseq
2831	def set_training ( model , mode ) : if mode is None : yield return old_mode = model . training if old_mode != mode : model . train ( mode ) try : yield finally : if old_mode != mode : model . train ( old_mode )
9738	def get_3d_markers_no_label_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabelResidual , component_info , data , component_position )
9073	def _get_connection ( cls , connection : Optional [ str ] = None ) -> str : return get_connection ( cls . module_name , connection = connection )
13031	def poll_once ( self , timeout = 0.0 ) : if self . _map : self . _poll_func ( timeout , self . _map )
9575	def read_elements ( fd , endian , mtps , is_name = False ) : mtpn , num_bytes , data = read_element_tag ( fd , endian ) if mtps and mtpn not in [ etypes [ mtp ] [ 'n' ] for mtp in mtps ] : raise ParseError ( 'Got type {}, expected {}' . format ( mtpn , ' / ' . join ( '{} ({})' . format ( etypes [ mtp ] [ 'n' ] , mtp ) for mtp in mtps ) ) ) if not data : data = fd . read ( num_bytes ) mod8 = num_bytes % 8 if mod8 : fd . seek ( 8 - mod8 , 1 ) if is_name : fmt = 's' val = [ unpack ( endian , fmt , s ) for s in data . split ( b'\0' ) if s ] if len ( val ) == 0 : val = '' elif len ( val ) == 1 : val = asstr ( val [ 0 ] ) else : val = [ asstr ( s ) for s in val ] else : fmt = etypes [ inv_etypes [ mtpn ] ] [ 'fmt' ] val = unpack ( endian , fmt , data ) return val
13180	def _get_column_nums_from_args ( columns ) : nums = [ ] for c in columns : for p in c . split ( ',' ) : p = p . strip ( ) try : c = int ( p ) nums . append ( c ) except ( TypeError , ValueError ) : start , ignore , end = p . partition ( '-' ) try : start = int ( start ) end = int ( end ) except ( TypeError , ValueError ) : raise ValueError ( 'Did not understand %r, expected digit-digit' % c ) inc = 1 if start < end else - 1 nums . extend ( range ( start , end + inc , inc ) ) return [ n - 1 for n in nums ]
2062	def declarations ( self ) : declarations = GetDeclarations ( ) for a in self . constraints : try : declarations . visit ( a ) except RuntimeError : if sys . getrecursionlimit ( ) >= PickleSerializer . MAX_RECURSION : raise Exception ( f'declarations recursion limit surpassed {PickleSerializer.MAX_RECURSION}, aborting' ) new_limit = sys . getrecursionlimit ( ) + PickleSerializer . DEFAULT_RECURSION if new_limit <= PickleSerializer . DEFAULT_RECURSION : sys . setrecursionlimit ( new_limit ) return self . declarations return declarations . result
5165	def __intermediate_proto ( self , interface , address ) : address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : return interface . pop ( 'proto' )
7646	def note_hz_to_midi ( annotation ) : annotation . namespace = 'note_midi' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = 12 * ( np . log2 ( obs . value ) - np . log2 ( 440.0 ) ) + 69 ) return annotation
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
13573	def skip ( course , num = 1 ) : sel = None try : sel = Exercise . get_selected ( ) if sel . course . tid != course . tid : sel = None except NoExerciseSelected : pass if sel is None : sel = course . exercises . first ( ) else : try : sel = Exercise . get ( Exercise . id == sel . id + num ) except peewee . DoesNotExist : print ( "There are no more exercises in this course." ) return False sel . set_select ( ) list_all ( single = sel )
331	def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace
11039	def read ( self , path , ** params ) : d = self . request ( 'GET' , '/v1/' + path , params = params ) return d . addCallback ( self . _handle_response )
2402	def gen_bag_feats ( self , e_set ) : if ( hasattr ( self , '_stem_dict' ) ) : sfeats = self . _stem_dict . transform ( e_set . _clean_stem_text ) nfeats = self . _normal_dict . transform ( e_set . _text ) bag_feats = numpy . concatenate ( ( sfeats . toarray ( ) , nfeats . toarray ( ) ) , axis = 1 ) else : raise util_functions . InputError ( self , "Dictionaries must be initialized prior to generating bag features." ) return bag_feats . copy ( )
7838	def set_node ( self , node ) : if node is None : if self . xmlnode . hasProp ( "node" ) : self . xmlnode . unsetProp ( "node" ) return node = unicode ( node ) self . xmlnode . setProp ( "node" , node . encode ( "utf-8" ) )
4389	def adsGetLocalAddressEx ( port ) : get_local_address_ex = _adsDLL . AdsGetLocalAddressEx ams_address_struct = SAmsAddr ( ) error_code = get_local_address_ex ( port , ctypes . pointer ( ams_address_struct ) ) if error_code : raise ADSError ( error_code ) local_ams_address = AmsAddr ( ) local_ams_address . _ams_addr = ams_address_struct return local_ams_address
4042	def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment parsed = list ( urlparse ( self . self_link ) ) stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : return None
3754	def Skin ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : _Skin = ( _OntarioExposureLimits [ CASRN ] [ "Skin" ] ) elif Method == NONE : _Skin = None else : raise Exception ( 'Failure in in function' ) return _Skin
2134	def _get_schema ( self , wfjt_id ) : node_res = get_resource ( 'node' ) node_results = node_res . list ( workflow_job_template = wfjt_id , all_pages = True ) [ 'results' ] return self . _workflow_node_structure ( node_results )
12177	def show_variances ( Y , variances , varianceX , logScale = False ) : plt . figure ( 1 , figsize = ( 10 , 7 ) ) plt . figure ( 2 , figsize = ( 10 , 7 ) ) varSorted = sorted ( variances ) plt . figure ( 1 ) plt . subplot ( 211 ) plt . grid ( ) plt . title ( "chronological variance" ) plt . ylabel ( "original data" ) plot_shaded_data ( X , Y , variances , varianceX ) plt . margins ( 0 , .1 ) plt . subplot ( 212 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "time in sweep (sec)" ) plt . plot ( varianceX , variances , 'k-' , lw = 2 ) plt . figure ( 2 ) plt . ylabel ( "variance (pA) (log%s)" % str ( logScale ) ) plt . xlabel ( "chunk number" ) plt . title ( "sorted variance" ) plt . plot ( varSorted , 'k-' , lw = 2 ) for i in range ( 0 , 100 , PERCENT_STEP ) : varLimitLow = np . percentile ( variances , i ) varLimitHigh = np . percentile ( variances , i + PERCENT_STEP ) label = "%2d-%d percentile" % ( i , i + + PERCENT_STEP ) color = COLORMAP ( i / 100 ) print ( "%s: variance = %.02f - %.02f" % ( label , varLimitLow , varLimitHigh ) ) plt . figure ( 1 ) plt . axhspan ( varLimitLow , varLimitHigh , alpha = .5 , lw = 0 , color = color , label = label ) plt . figure ( 2 ) chunkLow = np . where ( varSorted >= varLimitLow ) [ 0 ] [ 0 ] chunkHigh = np . where ( varSorted >= varLimitHigh ) [ 0 ] [ 0 ] plt . axvspan ( chunkLow , chunkHigh , alpha = .5 , lw = 0 , color = color , label = label ) for fignum in [ 1 , 2 ] : plt . figure ( fignum ) if logScale : plt . semilogy ( ) plt . margins ( 0 , 0 ) plt . grid ( ) if fignum is 2 : plt . legend ( fontsize = 10 , loc = 'upper left' , shadow = True ) plt . tight_layout ( ) plt . savefig ( '2016-12-15-variance-%d-log%s.png' % ( fignum , str ( logScale ) ) ) plt . show ( )
12278	def run_executable ( repo , args , includes ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) tmpdir = tempfile . mkdtemp ( ) print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) files = extract_files ( strace_filename , includes ) execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
12424	def loads ( s , separator = DEFAULT , index_separator = DEFAULT , cls = dict , list_cls = list ) : if isinstance ( s , six . text_type ) : io = StringIO ( s ) else : io = BytesIO ( s ) return load ( fp = io , separator = separator , index_separator = index_separator , cls = cls , list_cls = list_cls , )
6912	def generate_flare_lightcurve ( times , mags = None , errs = None , paramdists = { 'amplitude' : sps . uniform ( loc = 0.01 , scale = 0.99 ) , 'nflares' : [ 1 , 5 ] , 'risestdev' : sps . uniform ( loc = 0.007 , scale = 0.04 ) , 'decayconst' : sps . uniform ( loc = 0.04 , scale = 0.163 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) nflares = npr . randint ( paramdists [ 'nflares' ] [ 0 ] , high = paramdists [ 'nflares' ] [ 1 ] ) flarepeaktimes = ( npr . random ( size = nflares ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) ) params = { 'nflares' : nflares } for flareind , peaktime in zip ( range ( nflares ) , flarepeaktimes ) : amp = paramdists [ 'amplitude' ] . rvs ( size = 1 ) risestdev = paramdists [ 'risestdev' ] . rvs ( size = 1 ) decayconst = paramdists [ 'decayconst' ] . rvs ( size = 1 ) if magsarefluxes and amp < 0.0 : amp = - amp elif not magsarefluxes and amp > 0.0 : amp = - amp modelmags , ptimes , pmags , perrs = ( flares . flare_model ( [ amp , peaktime , risestdev , decayconst ] , times , mags , errs ) ) mags = modelmags params [ flareind ] = { 'peaktime' : peaktime , 'amplitude' : amp , 'risestdev' : risestdev , 'decayconst' : decayconst } modeldict = { 'vartype' : 'flare' , 'params' : params , 'times' : times , 'mags' : mags , 'errs' : errs , 'varperiod' : None , 'varamplitude' : [ params [ x ] [ 'amplitude' ] for x in range ( params [ 'nflares' ] ) ] , } return modeldict
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
9772	def restart ( ctx , copy , file , u ) : config = None update_code = None if file : config = rhea . read ( file ) if u : ctx . invoke ( upload , sync = False ) update_code = True user , project_name , _job = get_job_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'job' ) ) try : if copy : response = PolyaxonClient ( ) . job . copy ( user , project_name , _job , config = config , update_code = update_code ) else : response = PolyaxonClient ( ) . job . restart ( user , project_name , _job , config = config , update_code = update_code ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not restart job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_job_details ( response )
9221	def mix ( self , color1 , color2 , weight = 50 , * args ) : if color1 and color2 : if isinstance ( weight , string_types ) : weight = float ( weight . strip ( '%' ) ) weight = ( ( weight / 100.0 ) * 2 ) - 1 rgb1 = self . _hextorgb ( color1 ) rgb2 = self . _hextorgb ( color2 ) alpha = 0 w1 = ( ( ( weight if weight * alpha == - 1 else weight + alpha ) / ( 1 + weight * alpha ) ) + 1 ) w1 = w1 / 2.0 w2 = 1 - w1 rgb = [ rgb1 [ 0 ] * w1 + rgb2 [ 0 ] * w2 , rgb1 [ 1 ] * w1 + rgb2 [ 1 ] * w2 , rgb1 [ 2 ] * w1 + rgb2 [ 2 ] * w2 , ] return self . _rgbatohex ( rgb ) raise ValueError ( 'Illegal color values' )
13605	def url_correct ( self , point , auth = None , export = None ) : newUrl = self . __url + point + '.json' if auth or export : newUrl += "?" if auth : newUrl += ( "auth=" + auth ) if export : if not newUrl . endswith ( '?' ) : newUrl += "&" newUrl += "format=export" return newUrl
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
11273	def get_dict ( self ) : return dict ( current_page = self . current_page , total_page_count = self . total_page_count , items = self . items , total_item_count = self . total_item_count , page_size = self . page_size )
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
10856	def _tile ( self , n ) : pos = self . _trans ( self . pos [ n ] ) return Tile ( pos , pos ) . pad ( self . support_pad )
6970	def _old_epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , epdsmooth_windowsize = 21 , epdsmooth_sigclip = 3.0 , epdsmooth_func = smooth_magseries_signal_medfilt , epdsmooth_extraparams = None ) : finiteind = np . isfinite ( mags ) mags_median = np . median ( mags [ finiteind ] ) mags_stdev = np . nanstd ( mags ) if epdsmooth_sigclip : excludeind = abs ( mags - mags_median ) < epdsmooth_sigclip * mags_stdev finalind = finiteind & excludeind else : finalind = finiteind final_mags = mags [ finalind ] final_len = len ( final_mags ) if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( final_mags , epdsmooth_windowsize ) epdmatrix = np . c_ [ fsv [ finalind ] ** 2.0 , fsv [ finalind ] , fdv [ finalind ] ** 2.0 , fdv [ finalind ] , fkv [ finalind ] ** 2.0 , fkv [ finalind ] , np . ones ( final_len ) , fsv [ finalind ] * fdv [ finalind ] , fsv [ finalind ] * fkv [ finalind ] , fdv [ finalind ] * fkv [ finalind ] , np . sin ( 2 * np . pi * xcc [ finalind ] ) , np . cos ( 2 * np . pi * xcc [ finalind ] ) , np . sin ( 2 * np . pi * ycc [ finalind ] ) , np . cos ( 2 * np . pi * ycc [ finalind ] ) , np . sin ( 4 * np . pi * xcc [ finalind ] ) , np . cos ( 4 * np . pi * xcc [ finalind ] ) , np . sin ( 4 * np . pi * ycc [ finalind ] ) , np . cos ( 4 * np . pi * ycc [ finalind ] ) , bgv [ finalind ] , bge [ finalind ] ] try : coeffs , residuals , rank , singulars = lstsq ( epdmatrix , smoothedmags , rcond = None ) if DEBUG : print ( 'coeffs = %s, residuals = %s' % ( coeffs , residuals ) ) retdict = { 'times' : times , 'mags' : ( mags_median + _old_epd_diffmags ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , mags ) ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict except Exception as e : LOGEXCEPTION ( 'EPD solution did not converge' ) retdict = { 'times' : times , 'mags' : np . full_like ( mags , np . nan ) , 'errs' : errs , 'fitcoeffs' : coeffs , 'residuals' : residuals } return retdict
5617	def clean_geometry_type ( geometry , target_type , allow_multipart = True ) : multipart_geoms = { "Point" : MultiPoint , "LineString" : MultiLineString , "Polygon" : MultiPolygon , "MultiPoint" : MultiPoint , "MultiLineString" : MultiLineString , "MultiPolygon" : MultiPolygon } if target_type not in multipart_geoms . keys ( ) : raise TypeError ( "target type is not supported: %s" % target_type ) if geometry . geom_type == target_type : return geometry elif allow_multipart : target_multipart_type = multipart_geoms [ target_type ] if geometry . geom_type == "GeometryCollection" : return target_multipart_type ( [ clean_geometry_type ( g , target_type , allow_multipart ) for g in geometry ] ) elif any ( [ isinstance ( geometry , target_multipart_type ) , multipart_geoms [ geometry . geom_type ] == target_multipart_type ] ) : return geometry raise GeometryTypeError ( "geometry type does not match: %s, %s" % ( geometry . geom_type , target_type ) )
7574	def get_threaded_view ( ipyclient , split = True ) : eids = ipyclient . ids dview = ipyclient . direct_view ( ) hosts = dview . apply_sync ( socket . gethostname ) hostdict = defaultdict ( list ) for host , eid in zip ( hosts , eids ) : hostdict [ host ] . append ( eid ) hostdictkeys = hostdict . keys ( ) for key in hostdictkeys : gids = hostdict [ key ] maxt = len ( gids ) if len ( gids ) >= 4 : maxt = 2 if ( len ( gids ) == 4 ) and ( len ( hosts ) >= 4 ) : maxt = 4 if len ( gids ) >= 6 : maxt = 3 if len ( gids ) >= 8 : maxt = 4 if len ( gids ) >= 16 : maxt = 4 threaded = [ gids [ i : i + maxt ] for i in xrange ( 0 , len ( gids ) , maxt ) ] lth = len ( threaded ) if lth > 1 : hostdict . pop ( key ) for hostid in range ( lth ) : hostdict [ str ( key ) + "_" + str ( hostid ) ] = threaded [ hostid ] LOGGER . info ( "threaded_view: %s" , dict ( hostdict ) ) return hostdict
12744	def get_internal_urls ( self ) : internal_urls = self . get_subfields ( "856" , "u" , i1 = "4" , i2 = "0" ) internal_urls . extend ( self . get_subfields ( "998" , "a" ) ) internal_urls . extend ( self . get_subfields ( "URL" , "u" ) ) return map ( lambda x : x . replace ( "&amp;" , "&" ) , internal_urls )
4509	def set_device_id ( self , dev , id ) : if id < 0 or id > 255 : raise ValueError ( "ID must be an unsigned byte!" ) com , code , ok = io . send_packet ( CMDTYPE . SETID , 1 , dev , self . baudrate , 5 , id ) if not ok : raise_error ( code )
4528	def _receive ( self , msg ) : msg = self . _convert ( msg ) if msg is None : return str_msg = self . verbose and self . _msg_to_str ( msg ) if self . verbose and log . is_debug ( ) : log . debug ( 'Message %s' , str_msg ) if self . pre_routing : self . pre_routing . receive ( msg ) receiver , msg = self . routing . receive ( msg ) if receiver : receiver . receive ( msg ) if self . verbose : log . info ( 'Routed message %s (%s) to %s' , str_msg [ : 128 ] , msg , repr ( receiver ) )
11956	def is_bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( TypeError , ValueError ) : return False if dec > 4294967295 or dec < 0 : return False return True
13310	def fullStats ( a , b ) : stats = [ [ 'bias' , 'Bias' , bias ( a , b ) ] , [ 'stderr' , 'Standard Deviation Error' , stderr ( a , b ) ] , [ 'mae' , 'Mean Absolute Error' , mae ( a , b ) ] , [ 'rmse' , 'Root Mean Square Error' , rmse ( a , b ) ] , [ 'nmse' , 'Normalized Mean Square Error' , nmse ( a , b ) ] , [ 'mfbe' , 'Mean Fractionalized bias Error' , mfbe ( a , b ) ] , [ 'fa2' , 'Factor of Two' , fa ( a , b , 2 ) ] , [ 'foex' , 'Factor of Exceedance' , foex ( a , b ) ] , [ 'correlation' , 'Correlation R' , correlation ( a , b ) ] , [ 'determination' , 'Coefficient of Determination r2' , determination ( a , b ) ] , [ 'gmb' , 'Geometric Mean Bias' , gmb ( a , b ) ] , [ 'gmv' , 'Geometric Mean Variance' , gmv ( a , b ) ] , [ 'fmt' , 'Figure of Merit in Time' , fmt ( a , b ) ] ] rec = np . rec . fromrecords ( stats , names = ( 'stat' , 'description' , 'result' ) ) df = pd . DataFrame . from_records ( rec , index = 'stat' ) return df
12392	def read ( self , deserialize = False , format = None ) : if deserialize : data , _ = self . deserialize ( format = format ) return data content = self . _read ( ) if not content : return '' if type ( content ) is six . binary_type : content = content . decode ( self . encoding ) return content
2094	def stdout ( self , pk , start_line = None , end_line = None , outfile = sys . stdout , ** kwargs ) : if self . unified_job_type != self . endpoint : unified_job = self . last_job_data ( pk , ** kwargs ) pk = unified_job [ 'id' ] elif not pk : unified_job = self . get ( ** kwargs ) pk = unified_job [ 'id' ] content = self . lookup_stdout ( pk , start_line , end_line ) opened = False if isinstance ( outfile , six . string_types ) : outfile = open ( outfile , 'w' ) opened = True if len ( content ) > 0 : click . echo ( content , nl = 1 , file = outfile ) if opened : outfile . close ( ) return { "changed" : False }
11989	def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
11434	def _tag_matches_pattern ( tag , pattern ) : for char1 , char2 in zip ( tag , pattern ) : if char2 not in ( '%' , char1 ) : return False return True
9235	def parse ( data ) : sections = re . compile ( "^## .+$" , re . MULTILINE ) . split ( data ) headings = re . findall ( "^## .+?$" , data , re . MULTILINE ) sections . pop ( 0 ) parsed = [ ] def func ( h , s ) : p = parse_heading ( h ) p [ "content" ] = s parsed . append ( p ) list ( map ( func , headings , sections ) ) return parsed
1451	def incr ( self , key , to_add = 1 ) : if key not in self . value : self . value [ key ] = CountMetric ( ) self . value [ key ] . incr ( to_add )
11538	def pin_direction ( self , pin ) : if type ( pin ) is list : return [ self . pin_direction ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_direction ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
7094	def init_options ( self ) : self . options = GoogleMapOptions ( ) d = self . declaration self . set_map_type ( d . map_type ) if d . ambient_mode : self . set_ambient_mode ( d . ambient_mode ) if ( d . camera_position or d . camera_zoom or d . camera_tilt or d . camera_bearing ) : self . update_camera ( ) if d . map_bounds : self . set_map_bounds ( d . map_bounds ) if not d . show_compass : self . set_show_compass ( d . show_compass ) if not d . show_zoom_controls : self . set_show_zoom_controls ( d . show_zoom_controls ) if not d . show_toolbar : self . set_show_toolbar ( d . show_toolbar ) if d . lite_mode : self . set_lite_mode ( d . lite_mode ) if not d . rotate_gestures : self . set_rotate_gestures ( d . rotate_gestures ) if not d . scroll_gestures : self . set_scroll_gestures ( d . scroll_gestures ) if not d . tilt_gestures : self . set_tilt_gestures ( d . tilt_gestures ) if not d . zoom_gestures : self . set_zoom_gestures ( d . zoom_gestures ) if d . min_zoom : self . set_min_zoom ( d . min_zoom ) if d . max_zoom : self . set_max_zoom ( d . max_zoom )
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
13845	def get_unique_pathname ( path , root = '' ) : path = os . path . join ( root , path ) potentialPaths = itertools . chain ( ( path , ) , __get_numbered_paths ( path ) ) potentialPaths = six . moves . filterfalse ( os . path . exists , potentialPaths ) return next ( potentialPaths )
10966	def set_shape ( self , shape , inner ) : for c in self . comps : c . set_shape ( shape , inner )
13227	def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , ** kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , ** kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , ** kwargs ) return _decorator_helper return _decorator
5726	def get_gdb_response ( self , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if USING_WINDOWS : retval = self . _get_responses_windows ( timeout_sec ) else : retval = self . _get_responses_unix ( timeout_sec ) if not retval and raise_error_on_timeout : raise GdbTimeoutError ( "Did not get response from gdb after %s seconds" % timeout_sec ) else : return retval
3255	def list_granules ( self , coverage , store , workspace = None , filter = None , limit = None , offset = None ) : params = dict ( ) if filter is not None : params [ 'filter' ] = filter if limit is not None : params [ 'limit' ] = limit if offset is not None : params [ 'offset' ] = offset workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to list granules in mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
9234	def run ( self ) : if not self . options . project or not self . options . user : print ( "Project and/or user missing. " "For help run:\n pygcgen --help" ) return if not self . options . quiet : print ( "Generating changelog..." ) log = None try : log = self . generator . compound_changelog ( ) except ChangelogGeneratorError as err : print ( "\n\033[91m\033[1m{}\x1b[0m" . format ( err . args [ 0 ] ) ) exit ( 1 ) if not log : if not self . options . quiet : print ( "Empty changelog generated. {} not written." . format ( self . options . output ) ) return if self . options . no_overwrite : out = checkname ( self . options . output ) else : out = self . options . output with codecs . open ( out , "w" , "utf-8" ) as fh : fh . write ( log ) if not self . options . quiet : print ( "Done!" ) print ( "Generated changelog written to {}" . format ( out ) )
13264	def get_configuration ( self , key , default = None ) : if key in self . config : return self . config . get ( key ) else : return default
2065	def inverse_transform ( self , X_in ) : X = X_in . copy ( deep = True ) X = util . convert_input ( X ) if self . _dim is None : raise ValueError ( 'Must train encoder before it can be used to inverse_transform data' ) if X . shape [ 1 ] != self . _dim : if self . drop_invariant : raise ValueError ( "Unexpected input dimension %d, the attribute drop_invariant should " "set as False when transform data" % ( X . shape [ 1 ] , ) ) else : raise ValueError ( 'Unexpected input dimension %d, expected %d' % ( X . shape [ 1 ] , self . _dim , ) ) if not self . cols : return X if self . return_df else X . values if self . handle_unknown == 'value' : for col in self . cols : if any ( X [ col ] == - 1 ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category -1 when encode %s" % ( col , ) ) if self . handle_unknown == 'return_nan' and self . handle_missing == 'return_nan' : for col in self . cols : if X [ col ] . isnull ( ) . any ( ) : warnings . warn ( "inverse_transform is not supported because transform impute " "the unknown category nan when encode %s" % ( col , ) ) for switch in self . mapping : column_mapping = switch . get ( 'mapping' ) inverse = pd . Series ( data = column_mapping . index , index = column_mapping . get_values ( ) ) X [ switch . get ( 'col' ) ] = X [ switch . get ( 'col' ) ] . map ( inverse ) . astype ( switch . get ( 'data_type' ) ) return X if self . return_df else X . values
7535	def muscle_chunker ( data , sample ) : LOGGER . info ( "inside muscle_chunker" ) if data . paramsdict [ "assembly_method" ] != "reference" : clustfile = os . path . join ( data . dirs . clusts , sample . name + ".clust.gz" ) with iter ( gzip . open ( clustfile , 'rb' ) ) as clustio : nloci = sum ( 1 for i in clustio if "//" in i ) // 2 optim = ( nloci // 20 ) + ( nloci % 20 ) LOGGER . info ( "optim for align chunks: %s" , optim ) clustio = gzip . open ( clustfile , 'rb' ) inclusts = iter ( clustio . read ( ) . strip ( ) . split ( "//\n//\n" ) ) inc = optim // 10 for idx in range ( 10 ) : this = optim + ( idx * inc ) left = nloci - this if idx == 9 : grabchunk = list ( itertools . islice ( inclusts , int ( 1e9 ) ) ) else : grabchunk = list ( itertools . islice ( inclusts , this ) ) nloci = left tmpfile = os . path . join ( data . tmpdir , sample . name + "_chunk_{}.ali" . format ( idx ) ) with open ( tmpfile , 'wb' ) as out : out . write ( "//\n//\n" . join ( grabchunk ) ) clustio . close ( )
799	def modelsInfo ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "wrong modelIDs type: %s" ) % ( type ( modelIDs ) , ) assert modelIDs , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( model_id = modelIDs ) , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . modelInfoNamedTuple . _fields ] ) results = [ self . _models . modelInfoNamedTuple . _make ( r ) for r in rows ] assert len ( results ) == len ( modelIDs ) , "modelIDs not found: %s" % ( set ( modelIDs ) - set ( r . modelId for r in results ) ) return results
1593	def prepare ( self , context ) : for stream_id , targets in self . targets . items ( ) : for target in targets : target . prepare ( context , stream_id )
12748	def load_skel ( self , source , ** kwargs ) : logging . info ( '%s: parsing skeleton configuration' , source ) if hasattr ( source , 'read' ) : p = parser . parse ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
9703	def monitorTUN ( self ) : packet = self . checkTUN ( ) if packet : try : ret = self . _faraday . send ( packet ) return ret except AttributeError as error : print ( "AttributeError" )
8779	def _try_allocate ( self , context , segment_id , network_id ) : LOG . info ( "Attempting to allocate segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) ) filter_dict = { "segment_id" : segment_id , "segment_type" : self . segment_type , "do_not_use" : False } available_ranges = db_api . segment_allocation_range_find ( context , scope = db_api . ALL , ** filter_dict ) available_range_ids = [ r [ "id" ] for r in available_ranges ] try : with context . session . begin ( subtransactions = True ) : filter_dict = { "deallocated" : True , "segment_id" : segment_id , "segment_type" : self . segment_type , "segment_allocation_range_ids" : available_range_ids } allocations = db_api . segment_allocation_find ( context , lock_mode = True , ** filter_dict ) . limit ( 100 ) . all ( ) if allocations : allocation = random . choice ( allocations ) update_dict = { "deallocated" : False , "deallocated_at" : None , "network_id" : network_id } allocation = db_api . segment_allocation_update ( context , allocation , ** update_dict ) LOG . info ( "Allocated segment %s for network %s " "segment_id %s segment_type %s" % ( allocation [ "id" ] , network_id , segment_id , self . segment_type ) ) return allocation except Exception : LOG . exception ( "Error in segment reallocation." ) LOG . info ( "Cannot find reallocatable segment for network %s " "segment_id %s segment_type %s" % ( network_id , segment_id , self . segment_type ) )
7358	def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
11492	def login_with_api_key ( self , email , api_key , application = 'Default' ) : parameters = dict ( ) parameters [ 'email' ] = BaseDriver . email = email parameters [ 'apikey' ] = BaseDriver . apikey = api_key parameters [ 'appname' ] = application response = self . request ( 'midas.login' , parameters ) if 'token' in response : return response [ 'token' ] if 'mfa_token_id' : return response [ 'mfa_token_id' ]
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
7080	def tic_objectsearch ( objectid , idcol_to_use = "ID" , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 90.0 , refresh = 5.0 , maxtimeout = 180.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'columns' : '*' , 'filters' : [ { "paramName" : idcol_to_use , "values" : [ str ( objectid ) ] } ] } service = 'Mast.Catalogs.Filtered.Tic' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
7592	def Async ( cls , token , session = None , ** options ) : return cls ( token , session = session , is_async = True , ** options )
6826	def clone ( self , remote_url , path = None , use_sudo = False , user = None ) : cmd = 'git clone --quiet %s' % remote_url if path is not None : cmd = cmd + ' %s' % path if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
3819	async def add_user ( self , add_user_request ) : response = hangouts_pb2 . AddUserResponse ( ) await self . _pb_request ( 'conversations/adduser' , add_user_request , response ) return response
11642	def yaml_write_data ( yaml_data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml_data , fd , default_flow_style = False ) return True return False
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
13348	def prompt ( prefix = None , colored = True ) : if platform == 'win' : return '[{0}] $P$G' . format ( prefix ) else : if colored : return ( '[{0}] ' '\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\] ' '\\[\\033[01;34m\\]\\w $ \\[\\033[00m\\]' ) . format ( prefix ) return '[{0}] \\u@\\h \\w $ ' . format ( prefix )
3369	def set_objective ( model , value , additive = False ) : interface = model . problem reverse_value = model . solver . objective . expression reverse_value = interface . Objective ( reverse_value , direction = model . solver . objective . direction , sloppy = True ) if isinstance ( value , dict ) : if not model . objective . is_Linear : raise ValueError ( 'can only update non-linear objectives ' 'additively using object of class ' 'model.problem.Objective, not %s' % type ( value ) ) if not additive : model . solver . objective = interface . Objective ( Zero , direction = model . solver . objective . direction ) for reaction , coef in value . items ( ) : model . solver . objective . set_linear_coefficients ( { reaction . forward_variable : coef , reaction . reverse_variable : - coef } ) elif isinstance ( value , ( Basic , optlang . interface . Objective ) ) : if isinstance ( value , Basic ) : value = interface . Objective ( value , direction = model . solver . objective . direction , sloppy = False ) if not _valid_atoms ( model , value . expression ) : value = interface . Objective . clone ( value , model = model . solver ) if not additive : model . solver . objective = value else : model . solver . objective += value . expression else : raise TypeError ( '%r is not a valid objective for %r.' % ( value , model . solver ) ) context = get_context ( model ) if context : def reset ( ) : model . solver . objective = reverse_value model . solver . objective . direction = reverse_value . direction context ( reset )
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero for what in ( "minimum" , "maximum" ) : if processes > 1 : chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
10530	def get_project ( project_id ) : try : res = _pybossa_req ( 'get' , 'project' , project_id ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
13403	def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
13645	def parse ( parser , argv = None , settings_key = 'settings' , no_args_func = None ) : argv = argv or sys . argv commands = command_list ( ) if type ( argv ) not in [ list , tuple ] : raise TypeError ( "argv only can be list or tuple" ) if len ( argv ) >= 2 and argv [ 1 ] in commands : sub_parsers = parser . add_subparsers ( ) class_name = argv [ 1 ] . capitalize ( ) + 'Component' from cliez . conf import ( COMPONENT_ROOT , LOGGING_CONFIG , EPILOG , GENERAL_ARGUMENTS ) sys . path . insert ( 0 , os . path . dirname ( COMPONENT_ROOT ) ) mod = importlib . import_module ( '{}.components.{}' . format ( os . path . basename ( COMPONENT_ROOT ) , argv [ 1 ] ) ) klass = getattr ( mod , class_name ) sub_parser = append_arguments ( klass , sub_parsers , EPILOG , GENERAL_ARGUMENTS ) options = parser . parse_args ( argv [ 1 : ] ) settings = Settings . bind ( getattr ( options , settings_key ) ) if settings_key and hasattr ( options , settings_key ) else None obj = klass ( parser , sub_parser , options , settings ) logger_level = logging . CRITICAL if hasattr ( options , 'verbose' ) : if options . verbose == 1 : logger_level = logging . ERROR elif options . verbose == 2 : logger_level = logging . WARNING elif options . verbose == 3 : logger_level = logging . INFO obj . logger . setLevel ( logging . INFO ) pass if hasattr ( options , 'debug' ) and options . debug : logger_level = logging . DEBUG try : import http . client as http_client http_client . HTTPConnection . debuglevel = 1 except Exception : pass pass loggers = LOGGING_CONFIG [ 'loggers' ] for k , v in loggers . items ( ) : v . setdefault ( 'level' , logger_level ) if logger_level in [ logging . INFO , logging . DEBUG ] : v [ 'handlers' ] = [ 'stdout' ] pass logging_config . dictConfig ( LOGGING_CONFIG ) obj . run ( options ) return obj if not parser . description and len ( commands ) : sub_parsers = parser . add_subparsers ( ) [ sub_parsers . add_parser ( v ) for v in commands ] pass pass options = parser . parse_args ( argv [ 1 : ] ) if no_args_func and callable ( no_args_func ) : return no_args_func ( options ) else : parser . _print_message ( "nothing to do...\n" ) pass
12115	def fileModifiedTimestamp ( fname ) : modifiedTime = os . path . getmtime ( fname ) stamp = time . strftime ( '%Y-%m-%d' , time . localtime ( modifiedTime ) ) return stamp
10466	def getFrontmostApp ( cls ) : apps = cls . _getRunningApps ( ) for app in apps : pid = app . processIdentifier ( ) ref = cls . getAppRefByPid ( pid ) try : if ref . AXFrontmost : return ref except ( _a11y . ErrorUnsupported , _a11y . ErrorCannotComplete , _a11y . ErrorAPIDisabled , _a11y . ErrorNotImplemented ) : pass raise ValueError ( 'No GUI application found.' )
5678	def get_stop_count_data ( self , start_ut , end_ut ) : trips_df = self . get_tripIs_active_in_range ( start_ut , end_ut ) stop_counts = Counter ( ) for row in trips_df . itertuples ( ) : stops_seq = self . get_trip_stop_time_data ( row . trip_I , row . day_start_ut ) for stop_time_row in stops_seq . itertuples ( index = False ) : if ( stop_time_row . dep_time_ut >= start_ut ) and ( stop_time_row . dep_time_ut <= end_ut ) : stop_counts [ stop_time_row . stop_I ] += 1 all_stop_data = self . stops ( ) counts = [ stop_counts [ stop_I ] for stop_I in all_stop_data [ "stop_I" ] . values ] all_stop_data . loc [ : , "count" ] = pd . Series ( counts , index = all_stop_data . index ) return all_stop_data
507	def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . _recordsCache ) == 0 : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'. Model has no saved records." ) try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID startID = self . _recordsCache [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . _recordsCache ) if end is None else max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for " "'removeLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . _recordsCache [ len ( self . _recordsCache ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . _recordsCache ) } ) recordsToDelete = [ ] for state in self . _recordsCache [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . _recordsCache [ clippedEnd : ] : self . _classifyState ( state )
5318	def format ( self , string , * args , ** kwargs ) : return string . format ( c = self , * args , ** kwargs )
5624	def absolute_path ( path = None , base_dir = None ) : if path_is_remote ( path ) : return path else : if os . path . isabs ( path ) : return path else : if base_dir is None or not os . path . isabs ( base_dir ) : raise TypeError ( "base_dir must be an absolute path." ) return os . path . abspath ( os . path . join ( base_dir , path ) )
4979	def post ( self , request ) : enterprise_uuid = request . POST . get ( 'enterprise_customer_uuid' ) success_url = request . POST . get ( 'redirect_url' ) failure_url = request . POST . get ( 'failure_url' ) course_id = request . POST . get ( 'course_id' , '' ) program_uuid = request . POST . get ( 'program_uuid' , '' ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) if not ( enterprise_uuid and success_url and failure_url ) : error_code = 'ENTGDS005' log_message = ( 'Error: one or more of the following values was falsy: ' 'enterprise_uuid: {enterprise_uuid}, ' 'success_url: {success_url}, ' 'failure_url: {failure_url} for course_id {course_id}. ' 'The following error code was reported to the user {userid}: {error_code}' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , success_url = success_url , failure_url = failure_url , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) if not self . course_or_program_exist ( course_id , program_uuid ) : error_code = 'ENTGDS006' log_message = ( 'Neither the course with course_id: {course_id} ' 'or program with {program_uuid} exist for ' 'enterprise customer {enterprise_uuid}' 'Error code {error_code} presented to user {userid}' . format ( course_id = course_id , program_uuid = program_uuid , error_code = error_code , userid = request . user . id , enterprise_uuid = enterprise_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) consent_record = get_data_sharing_consent ( request . user . username , enterprise_uuid , program_uuid = program_uuid , course_id = course_id ) if consent_record is None : error_code = 'ENTGDS007' log_message = ( 'The was a problem with the consent record of user {userid} with ' 'enterprise_uuid {enterprise_uuid}. consent_record has a value ' 'of {consent_record} and a ' 'value for course_id {course_id}. ' 'Error code {error_code} presented to user' . format ( userid = request . user . id , enterprise_uuid = enterprise_uuid , consent_record = consent_record , error_code = error_code , course_id = course_id , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) defer_creation = request . POST . get ( 'defer_creation' ) consent_provided = bool ( request . POST . get ( 'data_sharing_consent' , False ) ) if defer_creation is None and consent_record . consent_required ( ) : if course_id : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = consent_record . enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id , ) if created : track_enrollment ( 'data-consent-page-enrollment' , request . user . id , course_id , request . path ) consent_record . granted = consent_provided consent_record . save ( ) return redirect ( success_url if consent_provided else failure_url )
10448	def click ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) size = self . _getobjectsize ( object_handle ) self . _grabfocus ( object_handle ) self . wait ( 0.5 ) self . generatemouseevent ( size [ 0 ] + size [ 2 ] / 2 , size [ 1 ] + size [ 3 ] / 2 , "b1c" ) return 1
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
12085	def folderScan ( self , abfFolder = None ) : if abfFolder is None and 'abfFolder' in dir ( self ) : abfFolder = self . abfFolder else : self . abfFolder = abfFolder self . abfFolder = os . path . abspath ( self . abfFolder ) self . log . info ( "scanning [%s]" , self . abfFolder ) if not os . path . exists ( self . abfFolder ) : self . log . error ( "path doesn't exist: [%s]" , abfFolder ) return self . abfFolder2 = os . path . abspath ( self . abfFolder + "/swhlab/" ) if not os . path . exists ( self . abfFolder2 ) : self . log . error ( "./swhlab/ doesn't exist. creating it..." ) os . mkdir ( self . abfFolder2 ) self . fnames = os . listdir ( self . abfFolder ) self . fnames2 = os . listdir ( self . abfFolder2 ) self . log . debug ( "./ has %d files" , len ( self . fnames ) ) self . log . debug ( "./swhlab/ has %d files" , len ( self . fnames2 ) ) self . fnamesByExt = filesByExtension ( self . fnames ) if not "abf" in self . fnamesByExt . keys ( ) : self . log . error ( "no ABF files found" ) self . log . debug ( "found %d ABFs" , len ( self . fnamesByExt [ "abf" ] ) ) self . cells = findCells ( self . fnames ) self . log . debug ( "found %d cells" % len ( self . cells ) ) self . fnamesByCell = filesByCell ( self . fnames , self . cells ) self . log . debug ( "grouped cells by number of source files: %s" % str ( [ len ( self . fnamesByCell [ elem ] ) for elem in self . fnamesByCell ] ) )
13315	def command ( self ) : cmd = self . config . get ( 'command' , None ) if cmd is None : return cmd = cmd [ platform ] return cmd [ 'path' ] , cmd [ 'args' ]
10528	def _pybossa_req ( method , domain , id = None , payload = None , params = { } , headers = { 'content-type' : 'application/json' } , files = None ) : url = _opts [ 'endpoint' ] + '/api/' + domain if id is not None : url += '/' + str ( id ) if 'api_key' in _opts : params [ 'api_key' ] = _opts [ 'api_key' ] if method == 'get' : r = requests . get ( url , params = params ) elif method == 'post' : if files is None and headers [ 'content-type' ] == 'application/json' : r = requests . post ( url , params = params , headers = headers , data = json . dumps ( payload ) ) else : r = requests . post ( url , params = params , files = files , data = payload ) elif method == 'put' : r = requests . put ( url , params = params , headers = headers , data = json . dumps ( payload ) ) elif method == 'delete' : r = requests . delete ( url , params = params , headers = headers , data = json . dumps ( payload ) ) if r . status_code // 100 == 2 : if r . text and r . text != '""' : return json . loads ( r . text ) else : return True else : return json . loads ( r . text )
4531	def construct ( cls , project , ** desc ) : return cls ( project . drivers , maker = project . maker , ** desc )
6619	def poll ( self ) : finished_procs = [ p for p in self . running_procs if p . poll ( ) is not None ] self . running_procs = collections . deque ( [ p for p in self . running_procs if p not in finished_procs ] ) for proc in finished_procs : stdout , stderr = proc . communicate ( ) finished_pids = [ p . pid for p in finished_procs ] self . finished_pids . extend ( finished_pids ) logger = logging . getLogger ( __name__ ) messages = 'Running: {}, Finished: {}' . format ( len ( self . running_procs ) , len ( self . finished_pids ) ) logger . info ( messages ) return finished_pids
5884	def get_title ( self ) : title = '' if "title" in list ( self . article . opengraph . keys ( ) ) : return self . clean_title ( self . article . opengraph [ 'title' ] ) elif self . article . schema and "headline" in self . article . schema : return self . clean_title ( self . article . schema [ 'headline' ] ) meta_headline = self . parser . getElementsByTag ( self . article . doc , tag = "meta" , attr = "name" , value = "headline" ) if meta_headline is not None and len ( meta_headline ) > 0 : title = self . parser . getAttribute ( meta_headline [ 0 ] , 'content' ) return self . clean_title ( title ) title_element = self . parser . getElementsByTag ( self . article . doc , tag = 'title' ) if title_element is not None and len ( title_element ) > 0 : title = self . parser . getText ( title_element [ 0 ] ) return self . clean_title ( title ) return title
7684	def mkclick ( freq , sr = 22050 , duration = 0.1 ) : times = np . arange ( int ( sr * duration ) ) click = np . sin ( 2 * np . pi * times * freq / float ( sr ) ) click *= np . exp ( - times / ( 1e-2 * sr ) ) return click
7470	def fill_dups_arr ( data ) : duplefiles = glob . glob ( os . path . join ( data . tmpdir , "duples_*.tmp.npy" ) ) duplefiles . sort ( key = lambda x : int ( x . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) ) io5 = h5py . File ( data . clust_database , 'r+' ) dfilter = io5 [ "duplicates" ] init = 0 for dupf in duplefiles : end = int ( dupf . rsplit ( "_" , 1 ) [ - 1 ] [ : - 8 ] ) inarr = np . load ( dupf ) dfilter [ init : end ] = inarr init += end - init LOGGER . info ( "all duplicates: %s" , dfilter [ : ] . sum ( ) ) io5 . close ( )
7497	def nworker ( data , smpchunk , tests ) : with h5py . File ( data . database . input , 'r' ) as io5 : seqview = io5 [ "bootsarr" ] [ : ] maparr = io5 [ "bootsmap" ] [ : ] nall_mask = seqview [ : ] == 78 rquartets = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint16 ) rweights = None rdstats = np . zeros ( ( smpchunk . shape [ 0 ] , 4 ) , dtype = np . uint32 ) for idx in xrange ( smpchunk . shape [ 0 ] ) : sidx = smpchunk [ idx ] seqchunk = seqview [ sidx ] nmask = np . any ( nall_mask [ sidx ] , axis = 0 ) nmask += np . all ( seqchunk == seqchunk [ 0 ] , axis = 0 ) bidx , qstats = calculate ( seqchunk , maparr [ : , 0 ] , nmask , tests ) rdstats [ idx ] = qstats rquartets [ idx ] = smpchunk [ idx ] [ bidx ] return rquartets , rweights , rdstats
10308	def barh ( d , plt , title = None ) : labels = sorted ( d , key = d . get ) index = range ( len ( labels ) ) plt . yticks ( index , labels ) plt . barh ( index , [ d [ v ] for v in labels ] ) if title is not None : plt . title ( title )
5159	def _add_tc_script ( self ) : context = dict ( tc_options = self . config . get ( 'tc_options' , [ ] ) ) contents = self . _render_template ( 'tc_script.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/tc_script.sh" , "contents" : contents , "mode" : "755" } )
2092	def last_job_data ( self , pk = None , ** kwargs ) : ujt = self . get ( pk , include_debug_header = True , ** kwargs ) if 'current_update' in ujt [ 'related' ] : debug . log ( 'A current job; retrieving it.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'current_update' ] [ 7 : ] ) . json ( ) elif ujt [ 'related' ] . get ( 'last_update' , None ) : debug . log ( 'No current job or update exists; retrieving the most recent.' , header = 'details' ) return client . get ( ujt [ 'related' ] [ 'last_update' ] [ 7 : ] ) . json ( ) else : raise exc . NotFound ( 'No related jobs or updates exist.' )
10374	def get_cutoff ( value : float , cutoff : Optional [ float ] = None ) -> int : cutoff = cutoff if cutoff is not None else 0 if value > cutoff : return 1 if value < ( - 1 * cutoff ) : return - 1 return 0
310	def var_cov_var_normal ( P , c , mu = 0 , sigma = 1 ) : alpha = sp . stats . norm . ppf ( 1 - c , mu , sigma ) return P - P * ( alpha + 1 )
283	def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
6953	def bootstrap_falsealarmprob ( lspinfo , times , mags , errs , nbootstrap = 250 , magsarefluxes = False , sigclip = 10.0 , npeaks = None ) : if ( npeaks and ( 0 < npeaks < len ( lspinfo [ 'nbestperiods' ] ) ) ) : nperiods = npeaks else : LOGWARNING ( 'npeaks not specified or invalid, ' 'getting FAP for all %s periodogram peaks' % len ( lspinfo [ 'nbestperiods' ] ) ) nperiods = len ( lspinfo [ 'nbestperiods' ] ) nbestperiods = lspinfo [ 'nbestperiods' ] [ : nperiods ] nbestpeaks = lspinfo [ 'nbestlspvals' ] [ : nperiods ] stimes , smags , serrs = sigclip_magseries ( times , mags , errs , magsarefluxes = magsarefluxes , sigclip = sigclip ) allpeaks = [ ] allperiods = [ ] allfaps = [ ] alltrialbestpeaks = [ ] if len ( stimes ) > 9 and len ( smags ) > 9 and len ( serrs ) > 9 : for ind , period , peak in zip ( range ( len ( nbestperiods ) ) , nbestperiods , nbestpeaks ) : LOGINFO ( 'peak %s: running %s trials...' % ( ind + 1 , nbootstrap ) ) trialbestpeaks = [ ] for _trial in range ( nbootstrap ) : tindex = np . random . randint ( 0 , high = mags . size , size = mags . size ) if 'kwargs' in lspinfo : kwargs = lspinfo [ 'kwargs' ] kwargs . update ( { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } ) else : kwargs = { 'magsarefluxes' : magsarefluxes , 'sigclip' : sigclip , 'verbose' : False } lspres = LSPMETHODS [ lspinfo [ 'method' ] ] ( times , mags [ tindex ] , errs [ tindex ] , ** kwargs ) trialbestpeaks . append ( lspres [ 'bestlspval' ] ) trialbestpeaks = np . array ( trialbestpeaks ) alltrialbestpeaks . append ( trialbestpeaks ) if lspinfo [ 'method' ] != 'pdm' : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks > peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) else : falsealarmprob = ( ( 1.0 + trialbestpeaks [ trialbestpeaks < peak ] . size ) / ( trialbestpeaks . size + 1.0 ) ) LOGINFO ( 'FAP for peak %s, period: %.6f = %.3g' % ( ind + 1 , period , falsealarmprob ) ) allpeaks . append ( peak ) allperiods . append ( period ) allfaps . append ( falsealarmprob ) return { 'peaks' : allpeaks , 'periods' : allperiods , 'probabilities' : allfaps , 'alltrialbestpeaks' : alltrialbestpeaks } else : LOGERROR ( 'not enough mag series points to calculate periodogram' ) return None
12206	def raise_for_status ( response ) : for err_name in web_exceptions . __all__ : err = getattr ( web_exceptions , err_name ) if err . status_code == response . status : payload = dict ( headers = response . headers , reason = response . reason , ) if issubclass ( err , web_exceptions . _HTTPMove ) : raise err ( response . headers [ 'Location' ] , ** payload ) raise err ( ** payload )
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) return self . current_time_format
5258	def parse_operand ( self , buf ) : buf = iter ( buf ) try : operand = 0 for _ in range ( self . operand_size ) : operand <<= 8 operand |= next ( buf ) self . _operand = operand except StopIteration : raise ParseError ( "Not enough data for decoding" )
99	def angle_between_vectors ( v1 , v2 ) : l1 = np . linalg . norm ( v1 ) l2 = np . linalg . norm ( v2 ) v1_u = ( v1 / l1 ) if l1 > 0 else np . float32 ( v1 ) * 0 v2_u = ( v2 / l2 ) if l2 > 0 else np . float32 ( v2 ) * 0 return np . arccos ( np . clip ( np . dot ( v1_u , v2_u ) , - 1.0 , 1.0 ) )
10630	def clone ( self ) : result = copy . copy ( self ) result . _compound_mfrs = copy . deepcopy ( self . _compound_mfrs ) return result
9653	def write_shas_to_shastore ( sha_dict ) : if sys . version_info [ 0 ] < 3 : fn_open = open else : fn_open = io . open with fn_open ( ".shastore" , "w" ) as fh : fh . write ( "---\n" ) fh . write ( 'sake version: {}\n' . format ( constants . VERSION ) ) if sha_dict : fh . write ( yaml . dump ( sha_dict ) ) fh . write ( "..." )
2706	def mh_digest ( data ) : num_perm = 512 m = MinHash ( num_perm ) for d in data : m . update ( d . encode ( 'utf8' ) ) return m
2611	def serialize_object ( obj , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : buffers = [ ] if istype ( obj , sequence_types ) and len ( obj ) < item_threshold : cobj = can_sequence ( obj ) for c in cobj : buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) elif istype ( obj , dict ) and len ( obj ) < item_threshold : cobj = { } for k in sorted ( obj ) : c = can ( obj [ k ] ) buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) cobj [ k ] = c else : cobj = can ( obj ) buffers . extend ( _extract_buffers ( cobj , buffer_threshold ) ) buffers . insert ( 0 , pickle . dumps ( cobj , PICKLE_PROTOCOL ) ) return buffers
5034	def get_enterprise_customer_user_queryset ( self , request , search_keyword , customer_uuid , page_size = PAGE_SIZE ) : page = request . GET . get ( 'page' , 1 ) learners = EnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) user_ids = learners . values_list ( 'user_id' , flat = True ) matching_users = User . objects . filter ( pk__in = user_ids ) if search_keyword is not None : matching_users = matching_users . filter ( Q ( email__icontains = search_keyword ) | Q ( username__icontains = search_keyword ) ) matching_user_ids = matching_users . values_list ( 'pk' , flat = True ) learners = learners . filter ( user_id__in = matching_user_ids ) return paginated_list ( learners , page , page_size )
10294	def get_namespaces_with_incorrect_names ( graph : BELGraph ) -> Set [ str ] : return { exc . namespace for _ , exc , _ in graph . warnings if isinstance ( exc , ( MissingNamespaceNameWarning , MissingNamespaceRegexWarning ) ) }
9405	def _get_user_class ( self , name ) : self . _user_classes . setdefault ( name , _make_user_class ( self , name ) ) return self . _user_classes [ name ]
5167	def __intermediate_dns_search ( self , uci , address ) : if 'dns_search' in uci : return uci [ 'dns_search' ] if address [ 'proto' ] == 'none' : return None dns_search = self . netjson . get ( 'dns_search' , None ) if dns_search : return ' ' . join ( dns_search )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
12813	def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
3620	def unregister ( self , model ) : if not self . is_registered ( model ) : raise RegistrationError ( '{} is not registered with Algolia engine' . format ( model ) ) del self . __registered_models [ model ] post_save . disconnect ( self . __post_save_receiver , model ) pre_delete . disconnect ( self . __pre_delete_receiver , model ) logger . info ( 'UNREGISTER %s' , model )
1424	def validate_state_locations ( self ) : names = map ( lambda loc : loc [ "name" ] , self . locations ) assert len ( names ) == len ( set ( names ) ) , "Names of state locations must be unique"
11728	def _assert_contains ( haystack , needle , invert , escape = False ) : myneedle = re . escape ( needle ) if escape else needle matched = re . search ( myneedle , haystack , re . M ) if ( invert and matched ) or ( not invert and not matched ) : raise AssertionError ( "'%s' %sfound in '%s'" % ( needle , "" if invert else "not " , haystack ) )
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) peripheral . discoverCharacteristics_forService_ ( None , service )
6881	def _parse_csv_header_lcc_csv_v1 ( headerlines ) : commentchar = headerlines [ 1 ] separator = headerlines [ 2 ] headerlines = [ x . lstrip ( '%s ' % commentchar ) for x in headerlines [ 3 : ] ] metadatastart = headerlines . index ( 'OBJECT METADATA' ) columnstart = headerlines . index ( 'COLUMN DEFINITIONS' ) lcstart = headerlines . index ( 'LIGHTCURVE' ) metadata = ' ' . join ( headerlines [ metadatastart + 1 : columnstart - 1 ] ) columns = ' ' . join ( headerlines [ columnstart + 1 : lcstart - 1 ] ) metadata = json . loads ( metadata ) columns = json . loads ( columns ) return metadata , columns , separator
11692	def set_fields ( self , changeset ) : self . id = int ( changeset . get ( 'id' ) ) self . user = changeset . get ( 'user' ) self . uid = changeset . get ( 'uid' ) self . editor = changeset . get ( 'created_by' , None ) self . review_requested = changeset . get ( 'review_requested' , False ) self . host = changeset . get ( 'host' , 'Not reported' ) self . bbox = changeset . get ( 'bbox' ) . wkt self . comment = changeset . get ( 'comment' , 'Not reported' ) self . source = changeset . get ( 'source' , 'Not reported' ) self . imagery_used = changeset . get ( 'imagery_used' , 'Not reported' ) self . date = datetime . strptime ( changeset . get ( 'created_at' ) , '%Y-%m-%dT%H:%M:%SZ' ) self . suspicion_reasons = [ ] self . is_suspect = False self . powerfull_editor = False
8677	def ssh ( key_name , no_tunnel , stash , passphrase , backend ) : def execute ( command ) : try : click . echo ( 'Executing: {0}' . format ( ' ' . join ( command ) ) ) subprocess . check_call ( ' ' . join ( command ) , shell = True ) except subprocess . CalledProcessError : sys . exit ( 1 ) stash = _get_stash ( backend , stash , passphrase ) key = stash . get ( key_name ) if key : _assert_is_ssh_type_key ( key ) else : sys . exit ( 'Key `{0}` not found' . format ( key_name ) ) conn_info = key [ 'value' ] ssh_key_path = conn_info . get ( 'ssh_key_path' ) ssh_key = conn_info . get ( 'ssh_key' ) proxy_key_path = conn_info . get ( 'proxy_key_path' ) proxy_key = conn_info . get ( 'proxy_key' ) id_file = _write_tmp ( ssh_key ) if ssh_key else ssh_key_path conn_info [ 'ssh_key_path' ] = id_file if conn_info . get ( 'proxy' ) : proxy_id_file = _write_tmp ( proxy_key ) if proxy_key else proxy_key_path conn_info [ 'proxy_key_path' ] = proxy_id_file ssh_command = _build_ssh_command ( conn_info , no_tunnel ) try : execute ( ssh_command ) finally : if id_file != ssh_key_path : click . echo ( 'Removing temp ssh key file: {0}...' . format ( id_file ) ) os . remove ( id_file ) if conn_info . get ( 'proxy' ) and proxy_id_file != proxy_key_path : click . echo ( 'Removing temp proxy key file: {0}...' . format ( proxy_id_file ) ) os . remove ( proxy_id_file )
6658	def _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained = False , memory_limit = None , test_mode = False ) : if not memory_constrained : return np . sum ( ( np . dot ( inbag - 1 , pred_centered . T ) / n_trees ) ** 2 , 0 ) if not memory_limit : raise ValueError ( 'If memory_constrained=True, must provide' , 'memory_limit.' ) chunk_size = int ( ( memory_limit * 1e6 ) / ( 8.0 * X_train . shape [ 0 ] ) ) if chunk_size == 0 : min_limit = 8.0 * X_train . shape [ 0 ] / 1e6 raise ValueError ( 'memory_limit provided is too small.' + 'For these dimensions, memory_limit must ' + 'be greater than or equal to %.3e' % min_limit ) chunk_edges = np . arange ( 0 , X_test . shape [ 0 ] + chunk_size , chunk_size ) inds = range ( X_test . shape [ 0 ] ) chunks = [ inds [ chunk_edges [ i ] : chunk_edges [ i + 1 ] ] for i in range ( len ( chunk_edges ) - 1 ) ] if test_mode : print ( 'Number of chunks: %d' % ( len ( chunks ) , ) ) V_IJ = np . concatenate ( [ np . sum ( ( np . dot ( inbag - 1 , pred_centered [ chunk ] . T ) / n_trees ) ** 2 , 0 ) for chunk in chunks ] ) return V_IJ
9739	def get_2d_markers ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
3538	def yandex_metrica ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return YandexMetricaNode ( )
6806	def init_raspbian_vm ( self ) : r = self . local_renderer r . comment ( 'Installing system packages.' ) r . sudo ( 'add-apt-repository ppa:linaro-maintainers/tools' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install libsdl-dev qemu-system' ) r . comment ( 'Download image.' ) r . local ( 'wget https://downloads.raspberrypi.org/raspbian_lite_latest' ) r . local ( 'unzip raspbian_lite_latest.zip' ) r . comment ( 'Find start of the Linux ext4 partition.' ) r . local ( "parted -s 2016-03-18-raspbian-jessie-lite.img unit B print | " "awk '/^Number/{{p=1;next}}; p{{gsub(/[^[:digit:]]/, " ", $2); print $2}}' | sed -n 2p" , assign_to = 'START' ) r . local ( 'mkdir -p {raspbian_mount_point}' ) r . sudo ( 'mount -v -o offset=$START -t ext4 {raspbian_image} $MNT' ) r . comment ( 'Comment out everything in ld.so.preload' ) r . local ( "sed -i 's/^/#/g' {raspbian_mount_point}/etc/ld.so.preload" ) r . comment ( 'Comment out entries containing /dev/mmcblk in fstab.' ) r . local ( "sed -i '/mmcblk/ s?^?#?' /etc/fstab" ) r . sudo ( 'umount {raspbian_mount_point}' ) r . comment ( 'Download kernel.' ) r . local ( 'wget https://github.com/dhruvvyas90/qemu-rpi-kernel/blob/master/{raspbian_kernel}?raw=true' ) r . local ( 'mv {raspbian_kernel} {libvirt_images_dir}' ) r . comment ( 'Creating libvirt machine.' ) r . local ( 'virsh define libvirt-raspbian.xml' ) r . comment ( 'You should now be able to boot the VM by running:' ) r . comment ( '' ) r . comment ( ' qemu-system-arm -kernel {libvirt_boot_dir}/{raspbian_kernel} ' '-cpu arm1176 -m 256 -M versatilepb -serial stdio -append "root=/dev/sda2 rootfstype=ext4 rw" ' '-hda {libvirt_images_dir}/{raspbian_image}' ) r . comment ( '' ) r . comment ( 'Or by running virt-manager.' )
12533	def from_set ( self , fileset , check_if_dicoms = True ) : if check_if_dicoms : self . items = [ ] for f in fileset : if is_dicom_file ( f ) : self . items . append ( f ) else : self . items = fileset
12675	def modify ( * args ) : if args and isinstance ( args [ 0 ] , dataframe . DataFrame ) : return args [ 0 ] . modify ( args [ 1 ] , args [ 2 ] , * args [ 3 : ] ) elif not args : raise ValueError ( "No arguments provided" ) else : return pipeable . Pipeable ( pipeable . PipingMethod . MODIFY , * args )
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
7307	def set_embedded_doc ( self , document , form_key , current_key , remaining_key ) : embedded_doc = getattr ( document , current_key , False ) if not embedded_doc : embedded_doc = document . _fields [ current_key ] . document_type_obj ( ) new_key , new_remaining_key_array = trim_field_key ( embedded_doc , remaining_key ) self . process_document ( embedded_doc , form_key , make_key ( new_key , new_remaining_key_array ) ) setattr ( document , current_key , embedded_doc )
5017	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : sys_msg = 'Not available' LOGGER . error ( ( 'Failed to send completion status call for enterprise enrollment %s' 'with payload %s' '\nError message: %s' '\nSystem message: %s' ) , learner_data . enterprise_course_enrollment_id , learner_data , str ( request_exception ) , sys_msg )
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
10673	def load_data_factsage ( path = '' ) : compounds . clear ( ) if path == '' : path = default_data_path if not os . path . exists ( path ) : warnings . warn ( 'The specified data file path does not exist. (%s)' % path ) return files = glob . glob ( os . path . join ( path , 'Compound_*.txt' ) ) for file in files : compound = Compound ( _read_compound_from_factsage_file_ ( file ) ) compounds [ compound . formula ] = compound
10347	def run_rcr ( graph , tag = 'dgxp' ) : hypotheses = defaultdict ( set ) increases = defaultdict ( set ) decreases = defaultdict ( set ) for u , v , d in graph . edges ( data = True ) : hypotheses [ u ] . add ( v ) if d [ RELATION ] in CAUSAL_INCREASE_RELATIONS : increases [ u ] . add ( v ) elif d [ RELATION ] in CAUSAL_DECREASE_RELATIONS : decreases [ u ] . add ( v ) correct = defaultdict ( int ) contra = defaultdict ( int ) ambiguous = defaultdict ( int ) missing = defaultdict ( int ) for controller , downstream_nodes in hypotheses . items ( ) : if len ( downstream_nodes ) < 4 : continue for node in downstream_nodes : if node in increases [ controller ] and node in decreases [ controller ] : ambiguous [ controller ] += 1 elif node in increases [ controller ] : if graph . node [ node ] [ tag ] == 1 : correct [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : contra [ controller ] += 1 elif node in decreases [ controller ] : if graph . node [ node ] [ tag ] == 1 : contra [ controller ] += 1 elif graph . node [ node ] [ tag ] == - 1 : correct [ controller ] += 1 else : missing [ controller ] += 1 controllers = { controller for controller , downstream_nodes in hypotheses . items ( ) if 4 <= len ( downstream_nodes ) } concordance_scores = { controller : scipy . stats . beta ( 0.5 , correct [ controller ] , contra [ controller ] ) for controller in controllers } population = { node for controller in controllers for node in hypotheses [ controller ] } population_size = len ( population ) return pandas . DataFrame ( { 'contra' : contra , 'correct' : correct , 'concordance' : concordance_scores } )
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
2776	def remove_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = DELETE , params = { "droplet_ids" : droplet_ids } )
6312	def from_separate ( cls , meta : ProgramDescription , vertex_source , geometry_source = None , fragment_source = None , tess_control_source = None , tess_evaluation_source = None ) : instance = cls ( meta ) instance . vertex_source = ShaderSource ( VERTEX_SHADER , meta . path or meta . vertex_shader , vertex_source , ) if geometry_source : instance . geometry_source = ShaderSource ( GEOMETRY_SHADER , meta . path or meta . geometry_shader , geometry_source , ) if fragment_source : instance . fragment_source = ShaderSource ( FRAGMENT_SHADER , meta . path or meta . fragment_shader , fragment_source , ) if tess_control_source : instance . tess_control_source = ShaderSource ( TESS_CONTROL_SHADER , meta . path or meta . tess_control_shader , tess_control_source , ) if tess_evaluation_source : instance . tess_evaluation_source = ShaderSource ( TESS_EVALUATION_SHADER , meta . path or meta . tess_control_shader , tess_evaluation_source , ) return instance
6426	def sim_sift4 ( src , tar , max_offset = 5 , max_distance = 0 ) : return Sift4 ( ) . sim ( src , tar , max_offset , max_distance )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
10540	def delete_category ( category_id ) : try : res = _pybossa_req ( 'delete' , 'category' , category_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
3640	def tradeStatus ( self , trade_id ) : method = 'GET' url = 'trade/status' if not isinstance ( trade_id , ( list , tuple ) ) : trade_id = ( trade_id , ) trade_id = ( str ( i ) for i in trade_id ) params = { 'tradeIds' : ',' . join ( trade_id ) } rc = self . __request__ ( method , url , params = params ) return [ itemParse ( i , full = False ) for i in rc [ 'auctionInfo' ] ]
4596	def pid_context ( pid_filename = None ) : pid_filename = pid_filename or DEFAULT_PID_FILENAME if os . path . exists ( pid_filename ) : contents = open ( pid_filename ) . read ( 16 ) log . warning ( 'pid_filename %s already exists with contents %s' , pid_filename , contents ) with open ( pid_filename , 'w' ) as fp : fp . write ( str ( os . getpid ( ) ) ) fp . write ( '\n' ) try : yield finally : try : os . remove ( pid_filename ) except Exception as e : log . error ( 'Got an exception %s deleting the pid_filename %s' , e , pid_filename )
10627	def Hfr ( self , Hfr ) : self . _Hfr = Hfr self . _T = self . _calculate_T ( Hfr )
2280	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_ccdr ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
9180	def _validate_roles ( model ) : required_roles = ( ATTRIBUTED_ROLE_KEYS [ 0 ] , ATTRIBUTED_ROLE_KEYS [ 4 ] , ) for role_key in ATTRIBUTED_ROLE_KEYS : try : roles = model . metadata [ role_key ] except KeyError : if role_key in required_roles : raise exceptions . MissingRequiredMetadata ( role_key ) else : if role_key in required_roles and len ( roles ) == 0 : raise exceptions . MissingRequiredMetadata ( role_key ) for role in roles : if role . get ( 'type' ) != 'cnx-id' : raise exceptions . InvalidRole ( role_key , role )
2559	def create_reg_message ( self ) : msg = { 'parsl_v' : PARSL_VERSION , 'python_v' : "{}.{}.{}" . format ( sys . version_info . major , sys . version_info . minor , sys . version_info . micro ) , 'os' : platform . system ( ) , 'hname' : platform . node ( ) , 'dir' : os . getcwd ( ) , } b_msg = json . dumps ( msg ) . encode ( 'utf-8' ) return b_msg
8444	def update ( check , enter_parameters , version ) : if check : if temple . update . up_to_date ( version = version ) : print ( 'Temple package is up to date' ) else : msg = ( 'This temple package is out of date with the latest template.' ' Update your package by running "temple update" and commiting changes.' ) raise temple . exceptions . NotUpToDateWithTemplateError ( msg ) else : temple . update . update ( new_version = version , enter_parameters = enter_parameters )
9368	def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
9250	def filter_issues_for_tags ( self , newer_tag , older_tag ) : filtered_pull_requests = self . delete_by_time ( self . pull_requests , older_tag , newer_tag ) filtered_issues = self . delete_by_time ( self . issues , older_tag , newer_tag ) newer_tag_name = newer_tag [ "name" ] if newer_tag else None if self . options . filter_issues_by_milestone : filtered_issues = self . filter_by_milestone ( filtered_issues , newer_tag_name , self . issues ) filtered_pull_requests = self . filter_by_milestone ( filtered_pull_requests , newer_tag_name , self . pull_requests ) return filtered_issues , filtered_pull_requests
5888	def __crawl ( self , crawl_candidate ) : def crawler_wrapper ( parser , parsers_lst , crawl_candidate ) : try : crawler = Crawler ( self . config , self . fetcher ) article = crawler . crawl ( crawl_candidate ) except ( UnicodeDecodeError , ValueError ) as ex : if parsers_lst : parser = parsers_lst . pop ( 0 ) return crawler_wrapper ( parser , parsers_lst , crawl_candidate ) else : raise ex return article parsers = list ( self . config . available_parsers ) parsers . remove ( self . config . parser_class ) return crawler_wrapper ( self . config . parser_class , parsers , crawl_candidate )
6283	def set_default_viewport ( self ) : expected_height = int ( self . buffer_width / self . aspect_ratio ) blank_space = self . buffer_height - expected_height self . fbo . viewport = ( 0 , blank_space // 2 , self . buffer_width , expected_height )
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : self . _assert_version ( VER_3_0 )
7017	def parallel_concat_worker ( task ) : lcbasedir , objectid , kwargs = task try : return concat_write_pklc ( lcbasedir , objectid , ** kwargs ) except Exception as e : LOGEXCEPTION ( 'failed LC concatenation for %s in %s' % ( objectid , lcbasedir ) ) return None
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
6340	def sim ( self , src , tar ) : r if src == tar : return 1.0 elif not src or not tar : return 0.0 return len ( self . lcsseq ( src , tar ) ) / max ( len ( src ) , len ( tar ) )
12688	def send_now ( users , label , extra_context = None , sender = None ) : sent = False if extra_context is None : extra_context = { } notice_type = NoticeType . objects . get ( label = label ) current_language = get_language ( ) for user in users : try : language = get_notification_language ( user ) except LanguageStoreNotAvailable : language = None if language is not None : activate ( language ) for backend in NOTIFICATION_BACKENDS . values ( ) : if backend . can_send ( user , notice_type ) : backend . deliver ( user , sender , notice_type , extra_context ) sent = True activate ( current_language ) return sent
11727	def ppdict ( dict_to_print , br = '\n' , html = False , key_align = 'l' , sort_keys = True , key_preffix = '' , key_suffix = '' , value_prefix = '' , value_suffix = '' , left_margin = 3 , indent = 2 ) : if dict_to_print : if sort_keys : dic = dict_to_print . copy ( ) keys = list ( dic . keys ( ) ) keys . sort ( ) dict_to_print = OrderedDict ( ) for k in keys : dict_to_print [ k ] = dic [ k ] tmp = [ '{' ] ks = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . keys ( ) ] vs = [ type ( x ) == str and "'%s'" % x or x for x in dict_to_print . values ( ) ] max_key_len = max ( [ len ( str ( x ) ) for x in ks ] ) for i in range ( len ( ks ) ) : k = { 1 : str ( ks [ i ] ) . ljust ( max_key_len ) , key_align == 'r' : str ( ks [ i ] ) . rjust ( max_key_len ) } [ 1 ] v = vs [ i ] tmp . append ( ' ' * indent + '{}{}{}:{}{}{},' . format ( key_preffix , k , key_suffix , value_prefix , v , value_suffix ) ) tmp [ - 1 ] = tmp [ - 1 ] [ : - 1 ] tmp . append ( '}' ) if left_margin : tmp = [ ' ' * left_margin + x for x in tmp ] if html : return '<code>{}</code>' . format ( br . join ( tmp ) . replace ( ' ' , '&nbsp;' ) ) else : return br . join ( tmp ) else : return '{}'
2070	def basen_to_integer ( self , X , cols , base ) : out_cols = X . columns . values . tolist ( ) for col in cols : col_list = [ col0 for col0 in out_cols if str ( col0 ) . startswith ( str ( col ) ) ] insert_at = out_cols . index ( col_list [ 0 ] ) if base == 1 : value_array = np . array ( [ int ( col0 . split ( '_' ) [ - 1 ] ) for col0 in col_list ] ) else : len0 = len ( col_list ) value_array = np . array ( [ base ** ( len0 - 1 - i ) for i in range ( len0 ) ] ) X . insert ( insert_at , col , np . dot ( X [ col_list ] . values , value_array . T ) ) X . drop ( col_list , axis = 1 , inplace = True ) out_cols = X . columns . values . tolist ( ) return X
1500	def fail ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in fail()" ) return if self . acking_enabled : fail_tuple = tuple_pb2 . AckTuple ( ) fail_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = fail_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( fail_tuple , tuple_size_in_bytes , False ) fail_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_fail ( tup , fail_latency_ns ) self . bolt_metrics . failed_tuple ( tup . stream , tup . component , fail_latency_ns )
784	def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
10413	def node_inclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def inclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node in node_set return inclusion_filter
9971	def _get_range ( book , range_ , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load_workbook ( book , data_only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise TypeError if _is_range_address ( range_ ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range_ ] else : data = _get_namedrange ( book , range_ , sheet ) if data is None : raise ValueError ( "Named range '%s' not found in %s" % ( range_ , filename or book ) ) return data
13025	def create_payload ( self , x86_file , x64_file , payload_file ) : sc_x86 = open ( os . path . join ( self . datadir , x86_file ) , 'rb' ) . read ( ) sc_x64 = open ( os . path . join ( self . datadir , x64_file ) , 'rb' ) . read ( ) fp = open ( os . path . join ( self . datadir , payload_file ) , 'wb' ) fp . write ( b'\x31\xc0\x40\x0f\x84' + pack ( '<I' , len ( sc_x86 ) ) ) fp . write ( sc_x86 ) fp . write ( sc_x64 ) fp . close ( )
6924	def open ( self , database , user , password , host ) : try : self . connection = pg . connect ( user = user , password = password , database = database , host = host ) LOGINFO ( 'postgres connection successfully ' 'created, using DB %s, user %s' % ( database , user ) ) self . database = database self . user = user except Exception as e : LOGEXCEPTION ( 'postgres connection failed, ' 'using DB %s, user %s' % ( database , user ) ) self . database = None self . user = None
12253	def _delete_key_internal ( self , * args , ** kwargs ) : mimicdb . backend . srem ( tpl . bucket % self . name , args [ 0 ] ) mimicdb . backend . delete ( tpl . key % ( self . name , args [ 0 ] ) ) return super ( Bucket , self ) . _delete_key_internal ( * args , ** kwargs )
8824	def context ( self ) : if not self . _context : self . _context = context . get_admin_context ( ) return self . _context
5786	def _raw_write ( self ) : data_available = libssl . BIO_ctrl_pending ( self . _wbio ) if data_available == 0 : return b'' to_read = min ( self . _buffer_size , data_available ) read = libssl . BIO_read ( self . _wbio , self . _bio_write_buffer , to_read ) to_write = bytes_from_buffer ( self . _bio_write_buffer , read ) output = to_write while len ( to_write ) : raise_disconnect = False try : sent = self . _socket . send ( to_write ) except ( socket_ . error ) as e : if e . errno == 104 or e . errno == 32 : raise_disconnect = True else : raise if raise_disconnect : raise_disconnection ( ) to_write = to_write [ sent : ] if len ( to_write ) : self . select_write ( ) return output
6090	def transform_grid ( func ) : @ wraps ( func ) def wrapper ( profile , grid , * args , ** kwargs ) : if not isinstance ( grid , TransformedGrid ) : return func ( profile , profile . transform_grid_to_reference_frame ( grid ) , * args , ** kwargs ) else : return func ( profile , grid , * args , ** kwargs ) return wrapper
4636	def derive_from_seed ( self , offset ) : seed = int ( hexlify ( bytes ( self ) ) . decode ( "ascii" ) , 16 ) z = int ( hexlify ( offset ) . decode ( "ascii" ) , 16 ) order = ecdsa . SECP256k1 . order secexp = ( seed + z ) % order secret = "%0x" % secexp if len ( secret ) < 64 : secret = ( "0" * ( 64 - len ( secret ) ) ) + secret return PrivateKey ( secret , prefix = self . pubkey . prefix )
7175	def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) sys . exit ( min ( returncode , 125 ) )
6033	def from_shape_pixel_scale_and_sub_grid_size ( cls , shape , pixel_scale , sub_grid_size = 2 ) : regular_grid = RegularGrid . from_shape_and_pixel_scale ( shape = shape , pixel_scale = pixel_scale ) sub_grid = SubGrid . from_shape_pixel_scale_and_sub_grid_size ( shape = shape , pixel_scale = pixel_scale , sub_grid_size = sub_grid_size ) blurring_grid = np . array ( [ [ 0.0 , 0.0 ] ] ) return GridStack ( regular_grid , sub_grid , blurring_grid )
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )
3609	def post_async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_post_request , args = ( endpoint , data , params , headers ) , callback = callback )
2726	def create ( self , * args , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) if not self . size_slug and self . size : self . size_slug = self . size ssh_keys_id = Droplet . __get_ssh_keys_id_or_fingerprint ( self . ssh_keys , self . token , self . name ) data = { "name" : self . name , "size" : self . size_slug , "image" : self . image , "region" : self . region , "ssh_keys" : ssh_keys_id , "backups" : bool ( self . backups ) , "ipv6" : bool ( self . ipv6 ) , "private_networking" : bool ( self . private_networking ) , "volumes" : self . volumes , "tags" : self . tags , "monitoring" : bool ( self . monitoring ) , } if self . user_data : data [ "user_data" ] = self . user_data data = self . get_data ( "droplets/" , type = POST , params = data ) if data : self . id = data [ 'droplet' ] [ 'id' ] action_id = data [ 'links' ] [ 'actions' ] [ 0 ] [ 'id' ] self . action_ids = [ ] self . action_ids . append ( action_id )
12552	def write_mhd_file ( filename , data , shape = None , meta_dict = None ) : ext = get_extension ( filename ) fname = op . basename ( filename ) if ext != '.mhd' or ext != '.raw' : mhd_filename = fname + '.mhd' raw_filename = fname + '.raw' elif ext == '.mhd' : mhd_filename = fname raw_filename = remove_ext ( fname ) + '.raw' elif ext == '.raw' : mhd_filename = remove_ext ( fname ) + '.mhd' raw_filename = fname else : raise ValueError ( '`filename` extension {} from {} is not recognised. ' 'Expected .mhd or .raw.' . format ( ext , filename ) ) if meta_dict is None : meta_dict = { } if shape is None : shape = data . shape meta_dict [ 'ObjectType' ] = meta_dict . get ( 'ObjectType' , 'Image' ) meta_dict [ 'BinaryData' ] = meta_dict . get ( 'BinaryData' , 'True' ) meta_dict [ 'BinaryDataByteOrderMSB' ] = meta_dict . get ( 'BinaryDataByteOrderMSB' , 'False' ) meta_dict [ 'ElementType' ] = meta_dict . get ( 'ElementType' , NUMPY_TO_MHD_TYPE [ data . dtype . type ] ) meta_dict [ 'NDims' ] = meta_dict . get ( 'NDims' , str ( len ( shape ) ) ) meta_dict [ 'DimSize' ] = meta_dict . get ( 'DimSize' , ' ' . join ( [ str ( i ) for i in shape ] ) ) meta_dict [ 'ElementDataFile' ] = meta_dict . get ( 'ElementDataFile' , raw_filename ) mhd_filename = op . join ( op . dirname ( filename ) , mhd_filename ) raw_filename = op . join ( op . dirname ( filename ) , raw_filename ) write_meta_header ( mhd_filename , meta_dict ) dump_raw_data ( raw_filename , data ) return mhd_filename , raw_filename
10557	def login ( self , oauth_filename = "oauth" , uploader_id = None ) : cls_name = type ( self ) . __name__ oauth_cred = os . path . join ( os . path . dirname ( OAUTH_FILEPATH ) , oauth_filename + '.cred' ) try : if not self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) : try : self . api . perform_oauth ( storage_filepath = oauth_cred ) except OSError : logger . exception ( "\nUnable to login with specified oauth code." ) self . api . login ( oauth_credentials = oauth_cred , uploader_id = uploader_id ) except ( OSError , ValueError ) : logger . exception ( "{} authentication failed." . format ( cls_name ) ) return False if not self . is_authenticated : logger . warning ( "{} authentication failed." . format ( cls_name ) ) return False logger . info ( "{} authentication succeeded.\n" . format ( cls_name ) ) return True
6666	def update_merge ( d , u ) : import collections for k , v in u . items ( ) : if isinstance ( v , collections . Mapping ) : r = update_merge ( d . get ( k , dict ( ) ) , v ) d [ k ] = r else : d [ k ] = u [ k ] return d
12241	def camel ( theta ) : x , y = theta obj = 2 * x ** 2 - 1.05 * x ** 4 + x ** 6 / 6 + x * y + y ** 2 grad = np . array ( [ 4 * x - 4.2 * x ** 3 + x ** 5 + y , x + 2 * y ] ) return obj , grad
5680	def get_trip_counts_per_day ( self ) : query = "SELECT date, count(*) AS number_of_trips FROM day_trips GROUP BY date" trip_counts_per_day = pd . read_sql_query ( query , self . conn , index_col = "date" ) max_day = trip_counts_per_day . index . max ( ) min_day = trip_counts_per_day . index . min ( ) min_date = datetime . datetime . strptime ( min_day , '%Y-%m-%d' ) max_date = datetime . datetime . strptime ( max_day , '%Y-%m-%d' ) num_days = ( max_date - min_date ) . days dates = [ min_date + datetime . timedelta ( days = x ) for x in range ( num_days + 1 ) ] trip_counts = [ ] date_strings = [ ] for date in dates : date_string = date . strftime ( "%Y-%m-%d" ) date_strings . append ( date_string ) try : value = trip_counts_per_day . loc [ date_string , 'number_of_trips' ] except KeyError : value = 0 trip_counts . append ( value ) for date_string in trip_counts_per_day . index : assert date_string in date_strings data = { "date" : dates , "date_str" : date_strings , "trip_counts" : trip_counts } return pd . DataFrame ( data )
9224	def convergent_round ( value , ndigits = 0 ) : if sys . version_info [ 0 ] < 3 : if value < 0.0 : return - convergent_round ( - value ) epsilon = 0.0000001 integral_part , _ = divmod ( value , 1 ) if abs ( value - ( integral_part + 0.5 ) ) < epsilon : if integral_part % 2.0 < epsilon : return integral_part else : nearest_even = integral_part + 0.5 return math . ceil ( nearest_even ) return round ( value , ndigits )
7815	def run ( self ) : if self . args . roster_cache and os . path . exists ( self . args . roster_cache ) : logging . info ( u"Loading roster from {0!r}" . format ( self . args . roster_cache ) ) try : self . client . roster_client . load_roster ( self . args . roster_cache ) except ( IOError , ValueError ) , err : logging . error ( u"Could not load the roster: {0!r}" . format ( err ) ) self . client . connect ( ) self . client . run ( )
4381	def exempt ( self , resource ) : if resource not in self . _exempt : self . _exempt . append ( resource )
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
6705	def create ( self , username , groups = None , uid = None , create_home = None , system = False , password = None , home_dir = None ) : r = self . local_renderer r . env . username = username args = [ ] if uid : args . append ( '-u %s' % uid ) if create_home is None : create_home = not system if create_home is True : if home_dir : args . append ( '--home %s' % home_dir ) elif create_home is False : args . append ( '--no-create-home' ) if password is None : pass elif password : crypted_password = _crypt_password ( password ) args . append ( '-p %s' % quote ( crypted_password ) ) else : args . append ( '--disabled-password' ) args . append ( '--gecos ""' ) if system : args . append ( '--system' ) r . env . args = ' ' . join ( args ) r . env . groups = ( groups or '' ) . strip ( ) r . sudo ( 'adduser {args} {username} || true' ) if groups : for group in groups . split ( ' ' ) : group = group . strip ( ) if not group : continue r . sudo ( 'adduser %s %s || true' % ( username , group ) )
1417	def _get_execution_state_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_execution_state_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_execution_state ( data , stats ) : if data : executionState = ExecutionState ( ) executionState . ParseFromString ( data ) callback ( executionState ) else : callback ( None ) return isWatching
1200	def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
1447	def offer ( self , item ) : try : self . _buffer . put ( item , block = False ) if self . _consumer_callback is not None : self . _consumer_callback ( ) return True except Queue . Full : Log . debug ( "%s: Full in offer()" % str ( self ) ) raise Queue . Full
13126	def get_domains ( self ) : search = User . search ( ) search . aggs . bucket ( 'domains' , 'terms' , field = 'domain' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) return [ entry . key for entry in response . aggregations . domains . buckets ]
407	def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
4897	def get_course_duration ( self , obj ) : duration = obj . end - obj . start if obj . start and obj . end else None if duration : return strfdelta ( duration , '{W} weeks {D} days.' ) return ''
13467	def set_Courant_Snyder ( self , beta , alpha , emit = None , emit_n = None ) : self . _store_emit ( emit = emit , emit_n = emit_n ) self . _sx = _np . sqrt ( beta * self . emit ) self . _sxp = _np . sqrt ( ( 1 + alpha ** 2 ) / beta * self . emit ) self . _sxxp = - alpha * self . emit
5055	def get_idp_choices ( ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None first = [ ( "" , "-" * 7 ) ] if Registry : return first + [ ( idp . provider_id , idp . name ) for idp in Registry . enabled ( ) ] return None
10494	def clickMouseButtonRight ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _postQueuedEvents ( )
12003	def _add_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( ** header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( ** options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
9109	def _create_encrypted_zip ( self , source = 'dirty' , fs_target_dir = None ) : backup_recipients = [ r for r in self . editors if checkRecipient ( self . gpg_context , r ) ] if not backup_recipients : self . status = u'500 no valid keys at all' return self . status fs_backup = join ( self . fs_path , '%s.zip' % source ) if fs_target_dir is None : fs_backup_pgp = join ( self . fs_path , '%s.zip.pgp' % source ) else : fs_backup_pgp = join ( fs_target_dir , '%s.zip.pgp' % self . drop_id ) fs_source = dict ( dirty = self . fs_dirty_attachments , clean = self . fs_cleansed_attachments ) with ZipFile ( fs_backup , 'w' , ZIP_STORED ) as backup : if exists ( join ( self . fs_path , 'message' ) ) : backup . write ( join ( self . fs_path , 'message' ) , arcname = 'message' ) for fs_attachment in fs_source [ source ] : backup . write ( fs_attachment , arcname = split ( fs_attachment ) [ - 1 ] ) with open ( fs_backup , "rb" ) as backup : self . gpg_context . encrypt_file ( backup , backup_recipients , always_trust = True , output = fs_backup_pgp ) remove ( fs_backup ) return fs_backup_pgp
9967	def shareable_parameters ( cells ) : result = [ ] for c in cells . values ( ) : params = c . formula . parameters for i in range ( min ( len ( result ) , len ( params ) ) ) : if params [ i ] != result [ i ] : return None for i in range ( len ( result ) , len ( params ) ) : result . append ( params [ i ] ) return result
8988	def last_consumed_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . consumes_meshes ( ) : return instruction . last_consumed_mesh raise IndexError ( "{} consumes no meshes" . format ( self ) )
2866	def write8 ( self , register , value ) : value = value & 0xFF self . _bus . write_byte_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
1242	def _sample_with_priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . _memory ) : return parent left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) if p <= left_p : parent = left else : if left + 1 >= len ( self . _memory ) : raise RuntimeError ( 'Right child is expected to exist.' ) p -= left_p parent = left + 1
513	def _updateDutyCycles ( self , overlaps , activeColumns ) : overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) overlapArray [ overlaps > 0 ] = 1 activeArray [ activeColumns ] = 1 period = self . _dutyCyclePeriod if ( period > self . _iterationNum ) : period = self . _iterationNum self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( self . _overlapDutyCycles , overlapArray , period ) self . _activeDutyCycles = self . _updateDutyCyclesHelper ( self . _activeDutyCycles , activeArray , period )
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
1875	def PSUBB ( cpu , dest , src ) : result = [ ] value_a = dest . read ( ) value_b = src . read ( ) for i in reversed ( range ( 0 , dest . size , 8 ) ) : a = Operators . EXTRACT ( value_a , i , 8 ) b = Operators . EXTRACT ( value_b , i , 8 ) result . append ( ( a - b ) & 0xff ) dest . write ( Operators . CONCAT ( 8 * len ( result ) , * result ) )
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp if rstart_buff == rend_buff : rend_buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust
2016	def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
12398	def gen_methods ( self , * args , ** kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , ** kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) yield from self . gen_generic ( )
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
12794	def get_headers ( self ) : headers = { "User-Agent" : "kFlame 1.0" } password_url = self . _get_password_url ( ) if password_url and password_url in self . _settings [ "authorizations" ] : headers [ "Authorization" ] = self . _settings [ "authorizations" ] [ password_url ] return headers
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
590	def _allocateSpatialFDR ( self , rfInput ) : if self . _sfdr : return autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _spatialArgNames ) if ( ( self . SpatialClass == CPPSpatialPooler ) or ( self . SpatialClass == PYSpatialPooler ) ) : autoArgs [ 'columnDimensions' ] = [ self . columnCount ] autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] autoArgs [ 'potentialRadius' ] = self . inputWidth self . _sfdr = self . SpatialClass ( ** autoArgs )
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
7126	def add_download_total ( rows ) : total_row = [ "" ] * len ( rows [ 0 ] ) total_row [ 0 ] = "Total" total_downloads , downloads_column = get_download_total ( rows ) total_row [ downloads_column ] = str ( total_downloads ) rows . append ( total_row ) return rows
7530	def build_dag ( data , samples ) : snames = [ i . name for i in samples ] dag = nx . DiGraph ( ) joborder = JOBORDER [ data . paramsdict [ "assembly_method" ] ] for sname in snames : for func in joborder : dag . add_node ( "{}-{}-{}" . format ( func , 0 , sname ) ) for chunk in xrange ( 10 ) : dag . add_node ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_node ( "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) for sname in snames : for sname2 in snames : dag . add_edge ( "{}-{}-{}" . format ( joborder [ 0 ] , 0 , sname2 ) , "{}-{}-{}" . format ( joborder [ 1 ] , 0 , sname ) ) for idx in xrange ( 2 , len ( joborder ) ) : dag . add_edge ( "{}-{}-{}" . format ( joborder [ idx - 1 ] , 0 , sname ) , "{}-{}-{}" . format ( joborder [ idx ] , 0 , sname ) ) for sname2 in snames : for chunk in range ( 10 ) : dag . add_edge ( "{}-{}-{}" . format ( "muscle_chunker" , 0 , sname2 ) , "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) ) dag . add_edge ( "{}-{}-{}" . format ( "muscle_align" , chunk , sname ) , "{}-{}-{}" . format ( "reconcat" , 0 , sname ) ) return dag , joborder
6805	def init_ubuntu_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) if not self . dryrun : device_question = 'SD card present at %s? ' % self . env . sd_device inp = raw_input ( device_question ) . strip ( ) print ( 'inp:' , inp ) if not yes and inp and not inp . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local ( 'ls {sd_device}' ) r . env . ubuntu_image_fn = os . path . abspath ( os . path . split ( self . env . ubuntu_download_url ) [ - 1 ] ) r . local ( '[ ! -f {ubuntu_image_fn} ] && wget {ubuntu_download_url} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2}' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'xzcat {ubuntu_image_fn} | dd bs=4M of={sd_device}' ) r . run ( 'sync' )
12267	def check_grad ( f_df , xref , stepsize = 1e-6 , tol = 1e-6 , width = 15 , style = 'round' , out = sys . stdout ) : CORRECT = u'\x1b[32m\N{CHECK MARK}\x1b[0m' INCORRECT = u'\x1b[31m\N{BALLOT X}\x1b[0m' obj , grad = wrap ( f_df , xref , size = 0 ) x0 = destruct ( xref ) df = grad ( x0 ) out . write ( tp . header ( [ "Numerical" , "Analytic" , "Error" ] , width = width , style = style ) + "\n" ) out . flush ( ) def parse_error ( number ) : failure = "\033[91m" passing = "\033[92m" warning = "\033[93m" end = "\033[0m" base = "{}{:0.3e}{}" if error < 0.1 * tol : return base . format ( passing , error , end ) elif error < tol : return base . format ( warning , error , end ) else : return base . format ( failure , error , end ) num_errors = 0 for j in range ( x0 . size ) : dx = np . zeros ( x0 . size ) dx [ j ] = stepsize df_approx = ( obj ( x0 + dx ) - obj ( x0 - dx ) ) / ( 2 * stepsize ) df_analytic = df [ j ] abs_error = np . linalg . norm ( df_approx - df_analytic ) error = abs_error if np . allclose ( abs_error , 0 ) else abs_error / ( np . linalg . norm ( df_analytic ) + np . linalg . norm ( df_approx ) ) num_errors += error >= tol errstr = CORRECT if error < tol else INCORRECT out . write ( tp . row ( [ df_approx , df_analytic , parse_error ( error ) + ' ' + errstr ] , width = width , style = style ) + "\n" ) out . flush ( ) out . write ( tp . bottom ( 3 , width = width , style = style ) + "\n" ) return num_errors
932	def run ( self , inputRecord ) : predictionNumber = self . _numPredictions self . _numPredictions += 1 result = opf_utils . ModelResult ( predictionNumber = predictionNumber , rawInput = inputRecord ) return result
4266	def set_meta ( target , keys , overwrite = False ) : if not os . path . exists ( target ) : sys . stderr . write ( "The target {} does not exist.\n" . format ( target ) ) sys . exit ( 1 ) if len ( keys ) < 2 or len ( keys ) % 2 > 0 : sys . stderr . write ( "Need an even number of arguments.\n" ) sys . exit ( 1 ) if os . path . isdir ( target ) : descfile = os . path . join ( target , 'index.md' ) else : descfile = os . path . splitext ( target ) [ 0 ] + '.md' if os . path . exists ( descfile ) and not overwrite : sys . stderr . write ( "Description file '{}' already exists. " "Use --overwrite to overwrite it.\n" . format ( descfile ) ) sys . exit ( 2 ) with open ( descfile , "w" ) as fp : for i in range ( len ( keys ) // 2 ) : k , v = keys [ i * 2 : ( i + 1 ) * 2 ] fp . write ( "{}: {}\n" . format ( k . capitalize ( ) , v ) ) print ( "{} metadata key(s) written to {}" . format ( len ( keys ) // 2 , descfile ) )
13097	def terminate_processes ( self ) : if self . relay : self . relay . terminate ( ) if self . responder : self . responder . terminate ( )
3747	def calculate ( self , T , P , zs , ws , method ) : r if method == MIXING_LOG_MOLAR : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( zs , mus ) elif method == MIXING_LOG_MASS : mus = [ i ( T , P ) for i in self . ViscosityLiquids ] return mixing_logarithmic ( ws , mus ) elif method == LALIBERTE_MU : ws = list ( ws ) ws . pop ( self . index_w ) return Laliberte_viscosity ( T , ws , self . wCASs ) else : raise Exception ( 'Method not valid' )
6704	def togroups ( self , user , groups ) : r = self . local_renderer if isinstance ( groups , six . string_types ) : groups = [ _ . strip ( ) for _ in groups . split ( ',' ) if _ . strip ( ) ] for group in groups : r . env . username = user r . env . group = group r . sudo ( 'groupadd --force {group}' ) r . sudo ( 'adduser {username} {group}' )
11900	def _get_thumbnail_image_from_file ( dir_path , image_file ) : img = _get_image_from_file ( dir_path , image_file ) if img is None : return None if img . format . lower ( ) == 'gif' : return None img_width , img_height = img . size scale_ratio = THUMBNAIL_WIDTH / float ( img_width ) target_height = int ( scale_ratio * img_height ) try : img . thumbnail ( ( THUMBNAIL_WIDTH , target_height ) , resample = RESAMPLE ) except IOError as exptn : print ( 'WARNING: IOError when thumbnailing %s/%s: %s' % ( dir_path , image_file , exptn ) ) return None return img
6289	def update ( self , ** kwargs ) : for name , value in kwargs . items ( ) : setattr ( self , name , value )
542	def __getOptimizedMetricLabel ( self ) : matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , self . _getMetricLabels ( ) ) if len ( matchingKeys ) == 0 : raise Exception ( "None of the generated metrics match the specified " "optimization pattern: %s. Available metrics are %s" % ( self . _optimizeKeyPattern , self . _getMetricLabels ( ) ) ) elif len ( matchingKeys ) > 1 : raise Exception ( "The specified optimization pattern '%s' matches more " "than one metric: %s" % ( self . _optimizeKeyPattern , matchingKeys ) ) return matchingKeys [ 0 ]
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
11540	def pin_type ( self , pin ) : if type ( pin ) is list : return [ self . pin_type ( p ) for p in pin ] pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : return self . _pin_type ( pin_id ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
2920	def _send_call ( self , my_task ) : args , kwargs = None , None if self . args : args = _eval_args ( self . args , my_task ) if self . kwargs : kwargs = _eval_kwargs ( self . kwargs , my_task ) LOG . debug ( "%s (task id %s) calling %s" % ( self . name , my_task . id , self . call ) , extra = dict ( data = dict ( args = args , kwargs = kwargs ) ) ) async_call = default_app . send_task ( self . call , args = args , kwargs = kwargs ) my_task . _set_internal_data ( task_id = async_call . task_id ) my_task . async_call = async_call LOG . debug ( "'%s' called: %s" % ( self . call , my_task . async_call . task_id ) )
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
8703	def download_file ( self , filename ) : res = self . __exchange ( 'send("{filename}")' . format ( filename = filename ) ) if ( 'unexpected' in res ) or ( 'stdin' in res ) : log . error ( 'Unexpected error downloading file: %s' , res ) raise Exception ( 'Unexpected error downloading file' ) self . __write ( 'C' ) sent_filename = self . __expect ( NUL ) . strip ( ) log . info ( 'receiveing ' + sent_filename ) self . __write ( ACK , True ) buf = '' data = '' chunk , buf = self . __read_chunk ( buf ) while chunk != '' : self . __write ( ACK , True ) data = data + chunk chunk , buf = self . __read_chunk ( buf ) return data
12152	def html_single_plot ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_plot.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDFF;">' html += '<span class="title">intrinsic properties for: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' for fname in filesByType [ 'plot' ] : html += self . htmlFor ( fname ) print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
13121	def get_pipe ( self , object_type ) : for line in sys . stdin : try : data = json . loads ( line . strip ( ) ) obj = object_type ( ** data ) yield obj except ValueError : yield self . id_to_object ( line . strip ( ) )
1863	def STOS ( cpu , dest , src ) : size = src . size dest . write ( src . read ( ) ) dest_reg = dest . mem . base increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
13744	def create_table ( self ) : table = self . conn . create_table ( name = self . get_table_name ( ) , schema = self . get_schema ( ) , read_units = self . get_read_units ( ) , write_units = self . get_write_units ( ) , ) if table . status != 'ACTIVE' : table . refresh ( wait_for_active = True , retry_seconds = 1 ) return table
9573	def unpack ( endian , fmt , data ) : if fmt == 's' : val = struct . unpack ( '' . join ( [ endian , str ( len ( data ) ) , 's' ] ) , data ) [ 0 ] else : num = len ( data ) // struct . calcsize ( fmt ) val = struct . unpack ( '' . join ( [ endian , str ( num ) , fmt ] ) , data ) if len ( val ) == 1 : val = val [ 0 ] return val
3932	def _auth_with_code ( session , authorization_code ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'code' : authorization_code , 'grant_type' : 'authorization_code' , 'redirect_uri' : 'urn:ietf:wg:oauth:2.0:oob' , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ] , res [ 'refresh_token' ]
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
1441	def serialize_data_tuple ( self , stream_id , latency_in_ns ) : self . update_count ( self . TUPLE_SERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id )
1873	def MOVLPD ( cpu , dest , src ) : value = src . read ( ) if src . size == 64 and dest . size == 128 : value = ( dest . read ( ) & 0xffffffffffffffff0000000000000000 ) | Operators . ZEXTEND ( value , 128 ) dest . write ( value )
532	def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( "getParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) return getter ( paramName )
10284	def count_targets ( edge_iter : EdgeIterator ) -> Counter : return Counter ( v for _ , v , _ in edge_iter )
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
12731	def create_body ( self , shape , name = None , ** kwargs ) : shape = shape . lower ( ) if name is None : for i in range ( 1 + len ( self . _bodies ) ) : name = '{}{}' . format ( shape , i ) if name not in self . _bodies : break self . _bodies [ name ] = Body . build ( shape , name , self , ** kwargs ) return self . _bodies [ name ]
5251	def _init_services ( self ) : logger = _get_logger ( self . debug ) opened = self . _session . openService ( '//blp/refdata' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise ConnectionError ( 'Could not open a //blp/refdata service' ) self . refDataService = self . _session . getService ( '//blp/refdata' ) opened = self . _session . openService ( '//blp/exrsvc' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise ConnectionError ( 'Could not open a //blp/exrsvc service' ) self . exrService = self . _session . getService ( '//blp/exrsvc' ) return self
422	def delete_validation_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . ValidLog . delete_many ( kwargs ) logging . info ( "[Database] Delete ValidLog SUCCESS" )
4045	def num_collectionitems ( self , collection ) : query = "/{t}/{u}/collections/{c}/items" . format ( u = self . library_id , t = self . library_type , c = collection . upper ( ) ) return self . _totals ( query )
4424	def get ( self , guild_id ) : if guild_id not in self . _players : p = self . _player ( lavalink = self . lavalink , guild_id = guild_id ) self . _players [ guild_id ] = p return self . _players [ guild_id ]
229	def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure
3998	def copy_from_local ( local_path , remote_name , remote_path , demote = True ) : if not os . path . exists ( local_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist' . format ( local_path ) ) temp_identifier = str ( uuid . uuid1 ( ) ) if os . path . isdir ( local_path ) : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_dir_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path ) else : sync_local_path_to_vm ( local_path , os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) , demote = demote ) move_file_inside_container ( remote_name , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) , remote_path )
5869	def _inactivate_organization_course_relationship ( relationship ) : relationship = internal . OrganizationCourse . objects . get ( id = relationship . id , active = True ) _inactivate_record ( relationship )
3706	def COSTALD ( T , Tc , Vc , omega ) : r Tr = T / Tc V_delta = ( - 0.296123 + 0.386914 * Tr - 0.0427258 * Tr ** 2 - 0.0480645 * Tr ** 3 ) / ( Tr - 1.00001 ) V_0 = 1 - 1.52816 * ( 1 - Tr ) ** ( 1 / 3. ) + 1.43907 * ( 1 - Tr ) ** ( 2 / 3. ) - 0.81446 * ( 1 - Tr ) + 0.190454 * ( 1 - Tr ) ** ( 4 / 3. ) return Vc * V_0 * ( 1 - omega * V_delta )
1978	def sched ( self ) : if len ( self . procs ) > 1 : logger . info ( "SCHED:" ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) logger . info ( "\tCurrent clock: %d" , self . clocks ) logger . info ( "\tCurrent cpu: %d" , self . _current ) if len ( self . running ) == 0 : logger . info ( "None running checking if there is some process waiting for a timeout" ) if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . clocks = min ( [ x for x in self . timers if x is not None ] ) + 1 self . check_timers ( ) assert len ( self . running ) != 0 , "DEADLOCK!" self . _current = self . running [ 0 ] return next_index = ( self . running . index ( self . _current ) + 1 ) % len ( self . running ) next = self . running [ next_index ] if len ( self . procs ) > 1 : logger . info ( "\tTransfer control from process %d to %d" , self . _current , next ) self . _current = next
11148	def create_package ( self , path = None , name = None , mode = None ) : assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:' if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) for dpath in sorted ( list ( self . walk_directories_path ( recursive = True ) ) ) : t = tarfile . TarInfo ( dpath ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) tarHandler . add ( os . path . join ( self . __path , dpath , self . __dirInfo ) , arcname = self . __dirInfo ) for fpath in self . walk_files_path ( recursive = True ) : relaPath , fname = os . path . split ( fpath ) tarHandler . add ( os . path . join ( self . __path , fpath ) , arcname = fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) , arcname = self . __fileInfo % fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileClass % fname ) , arcname = self . __fileClass % fname ) tarHandler . add ( os . path . join ( self . __path , self . __repoFile ) , arcname = ".pyrepinfo" ) tarHandler . close ( )
11152	def sha256file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha256 , nbytes = nbytes , chunk_size = chunk_size )
6659	def _bias_correction ( V_IJ , inbag , pred_centered , n_trees ) : n_train_samples = inbag . shape [ 0 ] n_var = np . mean ( np . square ( inbag [ 0 : n_trees ] ) . mean ( axis = 1 ) . T . view ( ) - np . square ( inbag [ 0 : n_trees ] . mean ( axis = 1 ) ) . T . view ( ) ) boot_var = np . square ( pred_centered ) . sum ( axis = 1 ) / n_trees bias_correction = n_train_samples * n_var * boot_var / n_trees V_IJ_unbiased = V_IJ - bias_correction return V_IJ_unbiased
10581	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) mm_average = 0.0 for compound , molefraction in state [ "x" ] . items ( ) : mm_average += molefraction * mm ( compound ) mm_average /= 1000.0 return mm_average * state [ "P" ] / R / state [ "T" ]
6525	def get_grouped_issues ( self , keyfunc = None , sortby = None ) : if not keyfunc : keyfunc = default_group if not sortby : sortby = self . DEFAULT_SORT self . _ensure_cleaned_issues ( ) return self . _group_issues ( self . _cleaned_issues , keyfunc , sortby )
4147	def DaniellPeriodogram ( data , P , NFFT = None , detrend = 'mean' , sampling = 1. , scale_by_freq = True , window = 'hamming' ) : r psd = speriodogram ( data , NFFT = NFFT , detrend = detrend , sampling = sampling , scale_by_freq = scale_by_freq , window = window ) if len ( psd ) % 2 == 1 : datatype = 'real' else : datatype = 'complex' N = len ( psd ) _slice = 2 * P + 1 if datatype == 'real' : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 0 : newN = psd . size / _slice else : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 1 : newN = psd . size / _slice newpsd = np . zeros ( int ( newN ) ) for i in range ( 0 , newpsd . size ) : count = 0 for n in range ( i * _slice - P , i * _slice + P + 1 ) : if n > 0 and n < N : count += 1 newpsd [ i ] += psd [ n ] newpsd [ i ] /= float ( count ) if datatype == 'complex' : freq = np . linspace ( 0 , sampling , len ( newpsd ) ) else : df = 1. / sampling freq = np . linspace ( 0 , sampling / 2. , len ( newpsd ) ) return newpsd , freq
11952	def execute ( varsfile , templatefile , outputfile = None , configfile = None , dryrun = False , build = False , push = False , verbose = False ) : if dryrun and ( build or push ) : jocker_lgr . error ( 'dryrun requested, cannot build.' ) sys . exit ( 100 ) _set_global_verbosity_level ( verbose ) j = Jocker ( varsfile , templatefile , outputfile , configfile , dryrun , build , push ) formatted_text = j . generate ( ) if dryrun : g = j . dryrun ( formatted_text ) if build or push : j . build_image ( ) if push : j . push_image ( ) if dryrun : return g
3022	def _detect_gce_environment ( ) : http = transport . get_http_object ( timeout = GCE_METADATA_TIMEOUT ) try : response , _ = transport . request ( http , _GCE_METADATA_URI , headers = _GCE_HEADERS ) return ( response . status == http_client . OK and response . get ( _METADATA_FLAVOR_HEADER ) == _DESIRED_METADATA_FLAVOR ) except socket . error : logger . info ( 'Timeout attempting to reach GCE metadata service.' ) return False
13178	def cache_func ( prefix , method = False ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : cache_args = args if method : cache_args = args [ 1 : ] cache_key = get_cache_key ( prefix , * cache_args , ** kwargs ) cached_value = cache . get ( cache_key ) if cached_value is None : cached_value = func ( * args , ** kwargs ) cache . set ( cache_key , cached_value ) return cached_value return wrapper return decorator
2800	def convert_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Sum ...' ) def target_layer ( x ) : import keras . backend as K return K . sum ( x ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
8396	def trans_new ( name , transform , inverse , breaks = None , minor_breaks = None , _format = None , domain = ( - np . inf , np . inf ) , doc = '' , ** kwargs ) : def _get ( func ) : if isinstance ( func , ( classmethod , staticmethod , MethodType ) ) : return func else : return staticmethod ( func ) klass_name = '{}_trans' . format ( name ) d = { 'transform' : _get ( transform ) , 'inverse' : _get ( inverse ) , 'domain' : domain , '__doc__' : doc , ** kwargs } if breaks : d [ 'breaks_' ] = _get ( breaks ) if minor_breaks : d [ 'minor_breaks' ] = _get ( minor_breaks ) if _format : d [ 'format' ] = _get ( _format ) return type ( klass_name , ( trans , ) , d )
6791	def loaddata ( self , path , site = None ) : site = site or self . genv . SITE r = self . local_renderer r . env . _loaddata_path = path for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : try : self . set_db ( site = _site ) r . env . SITE = _site r . sudo ( 'export SITE={SITE}; export ROLE={ROLE}; ' 'cd {project_dir}; ' '{manage_cmd} loaddata {_loaddata_path}' ) except KeyError : pass
13175	def prev ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index - 1 , - 1 , - 1 ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
3025	def _save_private_file ( filename , json_contents ) : temp_filename = tempfile . mktemp ( ) file_desc = os . open ( temp_filename , os . O_WRONLY | os . O_CREAT , 0o600 ) with os . fdopen ( file_desc , 'w' ) as file_handle : json . dump ( json_contents , file_handle , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) shutil . move ( temp_filename , filename )
9329	def post ( self , url , headers = None , params = None , ** kwargs ) : if len ( kwargs ) > 1 : raise InvalidArgumentsError ( "Too many extra args ({} > 1)" . format ( len ( kwargs ) ) ) if kwargs : kwarg = next ( iter ( kwargs ) ) if kwarg not in ( "json" , "data" ) : raise InvalidArgumentsError ( "Invalid kwarg: " + kwarg ) resp = self . session . post ( url , headers = headers , params = params , ** kwargs ) resp . raise_for_status ( ) return _to_json ( resp )
10245	def create_timeline ( year_counter : typing . Counter [ int ] ) -> List [ Tuple [ int , int ] ] : if not year_counter : return [ ] from_year = min ( year_counter ) - 1 until_year = datetime . now ( ) . year + 1 return [ ( year , year_counter . get ( year , 0 ) ) for year in range ( from_year , until_year ) ]
6082	def deflections_of_galaxies_from_grid ( grid , galaxies ) : if len ( galaxies ) > 0 : deflections = sum ( map ( lambda galaxy : galaxy . deflections_from_grid ( grid ) , galaxies ) ) else : deflections = np . full ( ( grid . shape [ 0 ] , 2 ) , 0.0 ) if isinstance ( grid , grids . SubGrid ) : return np . asarray ( [ grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 0 ] ) , grid . regular_data_1d_from_sub_data_1d ( deflections [ : , 1 ] ) ] ) . T return deflections
5782	def select_read ( self , timeout = None ) : if len ( self . _decrypted_bytes ) > 0 : return True read_ready , _ , _ = select . select ( [ self . _socket ] , [ ] , [ ] , timeout ) return len ( read_ready ) > 0
54	def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )
299	def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + " bps" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax
2792	def load ( self ) : data = self . get_data ( "certificates/%s" % self . id ) certificate = data [ "certificate" ] for attr in certificate . keys ( ) : setattr ( self , attr , certificate [ attr ] ) return self
2522	def p_file_chk_sum ( self , f_term , predicate ) : try : for _s , _p , checksum in self . graph . triples ( ( f_term , predicate , None ) ) : for _ , _ , value in self . graph . triples ( ( checksum , self . spdx_namespace [ 'checksumValue' ] , None ) ) : self . builder . set_file_chksum ( self . doc , six . text_type ( value ) ) except CardinalityError : self . more_than_one_error ( 'File checksum' )
9072	def build_engine_session ( connection , echo = False , autoflush = None , autocommit = None , expire_on_commit = None , scopefunc = None ) : if connection is None : raise ValueError ( 'can not build engine when connection is None' ) engine = create_engine ( connection , echo = echo ) autoflush = autoflush if autoflush is not None else False autocommit = autocommit if autocommit is not None else False expire_on_commit = expire_on_commit if expire_on_commit is not None else True log . debug ( 'auto flush: %s, auto commit: %s, expire on commmit: %s' , autoflush , autocommit , expire_on_commit ) session_maker = sessionmaker ( bind = engine , autoflush = autoflush , autocommit = autocommit , expire_on_commit = expire_on_commit , ) session = scoped_session ( session_maker , scopefunc = scopefunc ) return engine , session
13592	def main ( target , label ) : check_environment ( target , label ) click . secho ( 'Fetching tags from the upstream ...' ) handler = TagHandler ( git . list_tags ( ) ) print_information ( handler , label ) tag = handler . yield_tag ( target , label ) confirm ( tag )
2333	def predict_dataset ( self , df ) : if len ( list ( df . columns ) ) == 2 : df . columns = [ "A" , "B" ] if self . model is None : raise AssertionError ( "Model has not been trained before predictions" ) df2 = DataFrame ( ) for idx , row in df . iterrows ( ) : df2 = df2 . append ( row , ignore_index = True ) df2 = df2 . append ( { 'A' : row [ "B" ] , 'B' : row [ "A" ] } , ignore_index = True ) return predict . predict ( deepcopy ( df2 ) , deepcopy ( self . model ) ) [ : : 2 ]
3975	def _env_vars_from_file ( filename ) : def split_env ( env ) : if '=' in env : return env . split ( '=' , 1 ) else : return env , None env = { } for line in open ( filename , 'r' ) : line = line . strip ( ) if line and not line . startswith ( '#' ) : k , v = split_env ( line ) env [ k ] = v return env
4254	def time_zone_by_country_and_region ( country_code , region_code = None ) : timezone = country_dict . get ( country_code ) if not timezone : return None if isinstance ( timezone , str ) : return timezone return timezone . get ( region_code )
11454	def from_source ( cls , source ) : bibrecs = BibRecordPackage ( source ) bibrecs . parse ( ) for bibrec in bibrecs . get_records ( ) : yield cls ( bibrec )
8626	def delete_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_delete_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
5406	def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : module , attribute = full_name . rsplit ( '.' , 1 ) module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
9331	def cpu_count ( ) : num = os . getenv ( "OMP_NUM_THREADS" ) if num is None : num = os . getenv ( "PBS_NUM_PPN" ) try : return int ( num ) except : return multiprocessing . cpu_count ( )
10183	def _aggregations_delete ( aggregation_types = None , start_date = None , end_date = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) aggregator . delete ( start_date , end_date )
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
4018	def _get_app_libs_volume_mounts ( app_name , assembled_specs ) : volumes = [ ] for lib_name in assembled_specs [ 'apps' ] [ app_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( "{}:{}" . format ( Repo ( lib_spec [ 'repo' ] ) . vm_path , container_code_path ( lib_spec ) ) ) return volumes
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
115	def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( "Expected to get a list as 'batches', got type %s. " + "Call imap_batches() if you use generators." ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )
3556	def power_off ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_off . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 0 ) if not self . _powered_off . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power off!' )
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
847	def remapCategories ( self , mapping ) : categoryArray = numpy . array ( self . _categoryList ) newCategoryArray = numpy . zeros ( categoryArray . shape [ 0 ] ) newCategoryArray . fill ( - 1 ) for i in xrange ( len ( mapping ) ) : newCategoryArray [ categoryArray == i ] = mapping [ i ] self . _categoryList = list ( newCategoryArray )
7965	def start ( self , tag , attrs ) : if self . _level == 0 : self . _root = ElementTree . Element ( tag , attrs ) self . _handler . stream_start ( self . _root ) if self . _level < 2 : self . _builder = ElementTree . TreeBuilder ( ) self . _level += 1 return self . _builder . start ( tag , attrs )
4561	def simpixel ( new = 0 , autoraise = True ) : simpixel_driver . open_browser ( new = new , autoraise = autoraise )
9110	def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
4109	def mexican ( lb , ub , n ) : r if n <= 0 : raise ValueError ( "n must be strictly positive" ) x = numpy . linspace ( lb , ub , n ) psi = ( 1. - x ** 2. ) * ( 2. / ( numpy . sqrt ( 3. ) * pi ** 0.25 ) ) * numpy . exp ( - x ** 2 / 2. ) return psi
8780	def delete_locks ( context , network_ids , addresses ) : addresses_no_longer_null_routed = _find_addresses_to_be_unlocked ( context , network_ids , addresses ) LOG . info ( "Deleting %s lock holders on IPAddress with ids: %s" , len ( addresses_no_longer_null_routed ) , [ addr . id for addr in addresses_no_longer_null_routed ] ) for address in addresses_no_longer_null_routed : lock_holder = None try : lock_holder = db_api . lock_holder_find ( context , lock_id = address . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if lock_holder : db_api . lock_holder_delete ( context , address , lock_holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock_holder ) continue context . session . flush ( )
7068	def read_fakelc ( fakelcfile ) : try : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( fakelcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
1939	def get_func_return_types ( self , hsh : bytes ) -> str : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) abi = self . get_abi ( hsh ) outputs = abi . get ( 'outputs' ) return '()' if outputs is None else SolidityMetadata . tuple_signature_for_components ( outputs )
3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
9060	def beta ( self ) : from numpy_sugar . linalg import rsolve return rsolve ( self . _X [ "VT" ] , rsolve ( self . _X [ "tX" ] , self . mean ( ) ) )
2696	def get_tiles ( graf , size = 3 ) : keeps = list ( filter ( lambda w : w . word_id > 0 , graf ) ) keeps_len = len ( keeps ) for i in iter ( range ( 0 , keeps_len - 1 ) ) : w0 = keeps [ i ] for j in iter ( range ( i + 1 , min ( keeps_len , i + 1 + size ) ) ) : w1 = keeps [ j ] if ( w1 . idx - w0 . idx ) <= size : yield ( w0 . root , w1 . root , )
13495	def new ( self , mode ) : dw = DigitWord ( wordtype = mode . digit_type ) dw . random ( mode . digits ) self . _key = str ( uuid . uuid4 ( ) ) self . _status = "" self . _ttl = 3600 self . _answer = dw self . _mode = mode self . _guesses_remaining = mode . guesses_allowed self . _guesses_made = 0
8059	def do_vars ( self , line ) : if self . bot . _vars : max_name_len = max ( [ len ( name ) for name in self . bot . _vars ] ) for i , ( name , v ) in enumerate ( self . bot . _vars . items ( ) ) : keep = i < len ( self . bot . _vars ) - 1 self . print_response ( "%s = %s" % ( name . ljust ( max_name_len ) , v . value ) , keep = keep ) else : self . print_response ( "No vars" )
2077	def secho ( message , ** kwargs ) : if not settings . color : for key in ( 'fg' , 'bg' , 'bold' , 'blink' ) : kwargs . pop ( key , None ) return click . secho ( message , ** kwargs )
11526	def create_small_thumbnail ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'itemId' ] = item_id response = self . request ( 'midas.thumbnailcreator.create.small.thumbnail' , parameters ) return response
9253	def get_string_for_issue ( self , issue ) : encapsulated_title = self . encapsulate_string ( issue [ 'title' ] ) try : title_with_number = u"{0} [\\#{1}]({2})" . format ( encapsulated_title , issue [ "number" ] , issue [ "html_url" ] ) except UnicodeEncodeError : title_with_number = "ERROR ERROR ERROR: #{0} {1}" . format ( issue [ "number" ] , issue [ 'title' ] ) print ( title_with_number , '\n' , issue [ "html_url" ] ) return self . issue_line_with_user ( title_with_number , issue )
13522	def _make_url ( self , slug ) : if slug . startswith ( "http" ) : return slug return "{0}{1}" . format ( self . server_url , slug )
4140	def execute_script ( code_block , example_globals , image_path , fig_count , src_file , gallery_conf ) : time_elapsed = 0 stdout = '' print ( 'plotting code blocks in %s' % src_file ) plt . close ( 'all' ) cwd = os . getcwd ( ) orig_stdout = sys . stdout try : os . chdir ( os . path . dirname ( src_file ) ) my_buffer = StringIO ( ) my_stdout = Tee ( sys . stdout , my_buffer ) sys . stdout = my_stdout t_start = time ( ) exec ( code_block , example_globals ) time_elapsed = time ( ) - t_start sys . stdout = orig_stdout my_stdout = my_buffer . getvalue ( ) . strip ( ) . expandtabs ( ) if my_stdout : stdout = CODE_OUTPUT . format ( indent ( my_stdout , ' ' * 4 ) ) os . chdir ( cwd ) figure_list = save_figures ( image_path , fig_count , gallery_conf ) image_list = "" if len ( figure_list ) == 1 : figure_name = figure_list [ 0 ] image_list = SINGLE_IMAGE % figure_name . lstrip ( '/' ) elif len ( figure_list ) > 1 : image_list = HLIST_HEADER for figure_name in figure_list : image_list += HLIST_IMAGE_TEMPLATE % figure_name . lstrip ( '/' ) except Exception : formatted_exception = traceback . format_exc ( ) print ( 80 * '_' ) print ( '%s is not compiling:' % src_file ) print ( formatted_exception ) print ( 80 * '_' ) figure_list = [ ] image_list = codestr2rst ( formatted_exception , lang = 'pytb' ) broken_img = os . path . join ( glr_path_static ( ) , 'broken_example.png' ) shutil . copyfile ( broken_img , os . path . join ( cwd , image_path . format ( 1 ) ) ) fig_count += 1 if gallery_conf [ 'abort_on_example_error' ] : raise finally : os . chdir ( cwd ) sys . stdout = orig_stdout print ( " - time elapsed : %.2g sec" % time_elapsed ) code_output = "\n{0}\n\n{1}\n\n" . format ( image_list , stdout ) return code_output , time_elapsed , fig_count + len ( figure_list )
10957	def _calc_loglikelihood ( self , model = None , tile = None ) : if model is None : res = self . residuals else : res = model - self . _data [ tile . slicer ] sig , isig = self . sigma , 1.0 / self . sigma nlogs = - np . log ( np . sqrt ( 2 * np . pi ) * sig ) * res . size return - 0.5 * isig * isig * np . dot ( res . flat , res . flat ) + nlogs
5289	def forms_invalid ( self , form , inlines ) : return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines ) )
5169	def __netjson_protocol ( self , radio ) : htmode = radio . get ( 'htmode' ) hwmode = radio . get ( 'hwmode' , None ) if htmode . startswith ( 'HT' ) : return '802.11n' elif htmode . startswith ( 'VHT' ) : return '802.11ac' return '802.{0}' . format ( hwmode )
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
11229	def after ( self , dt , inc = False ) : if self . _cache_complete : gen = self . _cache else : gen = self if inc : for i in gen : if i >= dt : return i else : for i in gen : if i > dt : return i return None
4832	def course_discovery_api_client ( user , catalog_url ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( _ ( "To get a Catalog API client, this package must be " "installed in an Open edX environment." ) ) jwt = JwtBuilder . create_jwt_for_user ( user ) return EdxRestApiClient ( catalog_url , jwt = jwt )
6475	def line ( self , p1 , p2 , resolution = 1 ) : xdiff = max ( p1 . x , p2 . x ) - min ( p1 . x , p2 . x ) ydiff = max ( p1 . y , p2 . y ) - min ( p1 . y , p2 . y ) xdir = [ - 1 , 1 ] [ int ( p1 . x <= p2 . x ) ] ydir = [ - 1 , 1 ] [ int ( p1 . y <= p2 . y ) ] r = int ( round ( max ( xdiff , ydiff ) ) ) if r == 0 : return for i in range ( ( r + 1 ) * resolution ) : x = p1 . x y = p1 . y if xdiff : x += ( float ( i ) * xdiff ) / r * xdir / resolution if ydiff : y += ( float ( i ) * ydiff ) / r * ydir / resolution yield Point ( ( x , y ) )
8457	def _needs_new_cc_config_for_update ( old_template , old_version , new_template , new_version ) : if old_template != new_template : return True else : return _cookiecutter_configs_have_changed ( new_template , old_version , new_version )
9631	def send ( self , extra_context = None , ** kwargs ) : message = self . render_to_message ( extra_context = extra_context , ** kwargs ) return message . send ( )
13114	def resolve_domains ( domains , disable_zone = False ) : dnsresolver = dns . resolver . Resolver ( ) ips = [ ] for domain in domains : print_notification ( "Resolving {}" . format ( domain ) ) try : result = dnsresolver . query ( domain , 'A' ) for a in result . response . answer [ 0 ] : ips . append ( str ( a ) ) if not disable_zone : ips . extend ( zone_transfer ( str ( a ) , domain ) ) except dns . resolver . NXDOMAIN as e : print_error ( e ) return ips
11356	def escape_for_xml ( data , tags_to_keep = None ) : data = re . sub ( "&" , "&amp;" , data ) if tags_to_keep : data = re . sub ( r"(<)(?![\/]?({0})\b)" . format ( "|" . join ( tags_to_keep ) ) , '&lt;' , data ) else : data = re . sub ( "<" , "&lt;" , data ) return data
256	def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats
5988	def scaled_deflection_stack_from_plane_and_scaling_factor ( plane , scaling_factor ) : def scale ( grid ) : return np . multiply ( scaling_factor , grid ) if plane . deflection_stack is not None : return plane . deflection_stack . apply_function ( scale ) else : return None
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
8119	def transform_path ( self , path ) : p = path . __class__ ( ) for pt in path : if pt . cmd == "close" : p . closepath ( ) elif pt . cmd == "moveto" : p . moveto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "lineto" : p . lineto ( * self . apply ( pt . x , pt . y ) ) elif pt . cmd == "curveto" : vx1 , vy1 = self . apply ( pt . ctrl1 . x , pt . ctrl1 . y ) vx2 , vy2 = self . apply ( pt . ctrl2 . x , pt . ctrl2 . y ) x , y = self . apply ( pt . x , pt . y ) p . curveto ( vx1 , vy1 , vx2 , vy2 , x , y ) return p
2477	def add_lic_xref ( self , doc , ref ) : if self . has_extr_lic ( doc ) : self . extr_lic ( doc ) . add_xref ( ref ) return True else : raise OrderError ( 'ExtractedLicense::CrossRef' )
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : "Gets within-bag distances for each bag." logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
8785	def update_port ( self , context , port_id , ** kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
9183	def lookup_document_pointer ( ident_hash , cursor ) : id , version = split_ident_hash ( ident_hash , split_version = True ) stmt = "SELECT name FROM modules WHERE uuid = %s" args = [ id ] if version and version [ 0 ] is not None : operator = version [ 1 ] is None and 'is' or '=' stmt += " AND (major_version = %s AND minor_version {} %s)" . format ( operator ) args . extend ( version ) cursor . execute ( stmt , args ) try : title = cursor . fetchone ( ) [ 0 ] except TypeError : raise DocumentLookupError ( ) else : metadata = { 'title' : title } return cnxepub . DocumentPointer ( ident_hash , metadata )
8291	def _keywords ( self ) : meta = self . find ( "meta" , { "name" : "keywords" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : keywords = [ k . strip ( ) for k in meta [ "content" ] . split ( "," ) ] else : keywords = [ ] return keywords
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
13724	def log_post ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if credentials is None : credentials = self . credentials if do_verify_certificate is None : do_verify_certificate = self . do_verify_certificate if credentials and "base64" in credentials : headers = { "Content-Type" : "application/json" , 'Authorization' : 'Basic %s' % credentials [ "base64" ] } else : headers = { "Content-Type" : "application/json" } try : request = requests . post ( url , headers = headers , data = self . store . get_json ( ) , verify = do_verify_certificate ) except httplib . IncompleteRead as e : request = e . partial
9701	def send ( self , msg ) : slipDriver = sliplib . Driver ( ) slipData = slipDriver . send ( msg ) res = self . _serialPort . write ( slipData ) return res
6557	def assert_penaltymodel_factory_available ( ) : from pkg_resources import iter_entry_points from penaltymodel . core import FACTORY_ENTRYPOINT from itertools import chain supported = ( 'maxgap' , 'mip' ) factories = chain ( * ( iter_entry_points ( FACTORY_ENTRYPOINT , name ) for name in supported ) ) try : next ( factories ) except StopIteration : raise AssertionError ( "To use 'dwavebinarycsp', at least one penaltymodel factory must be installed. " "Try {}." . format ( " or " . join ( "'pip install dwavebinarycsp[{}]'" . format ( name ) for name in supported ) ) )
13888	def DeleteDirectory ( directory , skip_on_error = False ) : _AssertIsLocal ( directory ) import shutil def OnError ( fn , path , excinfo ) : if IsLink ( path ) : return if fn is os . remove and os . access ( path , os . W_OK ) : raise import stat os . chmod ( path , stat . S_IWRITE ) fn ( path ) try : if not os . path . isdir ( directory ) : if skip_on_error : return from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( directory ) shutil . rmtree ( directory , onerror = OnError ) except : if not skip_on_error : raise
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
2150	def delete ( self , pk = None , fail_on_missing = False , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . delete ( pk = pk , fail_on_missing = fail_on_missing , ** kwargs )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
7981	def auth_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise LegacyAuthenticationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
5991	def weighted_regularization_matrix_from_pixel_neighbors ( regularization_weights , pixel_neighbors , pixel_neighbors_size ) : pixels = len ( regularization_weights ) regularization_matrix = np . zeros ( shape = ( pixels , pixels ) ) regularization_weight = regularization_weights ** 2.0 for i in range ( pixels ) : for j in range ( pixel_neighbors_size [ i ] ) : neighbor_index = pixel_neighbors [ i , j ] regularization_matrix [ i , i ] += regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , neighbor_index ] += regularization_weight [ neighbor_index ] regularization_matrix [ i , neighbor_index ] -= regularization_weight [ neighbor_index ] regularization_matrix [ neighbor_index , i ] -= regularization_weight [ neighbor_index ] return regularization_matrix
5457	def _from_yaml_v0 ( cls , job ) : job_metadata = { } for key in [ 'job-id' , 'job-name' , 'create-time' ] : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( datetime . datetime . strptime ( job [ 'create-time' ] , '%Y-%m-%d %H:%M:%S.%f' ) , tzlocal ( ) ) job_resources = Resources ( ) params = { } labels = job . get ( 'labels' , { } ) if 'dsub-version' in labels : job_metadata [ 'dsub-version' ] = labels [ 'dsub-version' ] del labels [ 'dsub-version' ] params [ 'labels' ] = cls . _label_params_from_dict ( labels ) params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) if job . get ( 'task-id' ) is None : job_params = params task_metadata = { 'task-id' : None } task_params = { } else : job_params = { } task_metadata = { 'task-id' : str ( job . get ( 'task-id' ) ) } task_params = params task_resources = Resources ( logging_path = job . get ( 'logging' ) ) task_descriptors = [ TaskDescriptor . get_complete_descriptor ( task_metadata , task_params , task_resources ) ] return JobDescriptor . get_complete_descriptor ( job_metadata , job_params , job_resources , task_descriptors )
11967	def _BYTES_TO_BITS ( ) : the_table = 256 * [ None ] bits_per_byte = list ( range ( 7 , - 1 , - 1 ) ) for n in range ( 256 ) : l = n bits = 8 * [ None ] for i in bits_per_byte : bits [ i ] = '01' [ n & 1 ] n >>= 1 the_table [ l ] = '' . join ( bits ) return the_table
2399	def initialize_dictionaries ( self , e_set , max_feats2 = 200 ) : if ( hasattr ( e_set , '_type' ) ) : if ( e_set . _type == "train" ) : nvocab = util_functions . get_vocab ( e_set . _text , e_set . _score , max_feats2 = max_feats2 ) svocab = util_functions . get_vocab ( e_set . _clean_stem_text , e_set . _score , max_feats2 = max_feats2 ) self . _normal_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = nvocab ) self . _stem_dict = CountVectorizer ( ngram_range = ( 1 , 2 ) , vocabulary = svocab ) self . dict_initialized = True self . _mean_spelling_errors = sum ( e_set . _spelling_errors ) / float ( len ( e_set . _spelling_errors ) ) self . _spell_errors_per_character = sum ( e_set . _spelling_errors ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) self . _grammar_errors_per_character = ( sum ( good_pos_tags ) / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ) bag_feats = self . gen_bag_feats ( e_set ) f_row_sum = numpy . sum ( bag_feats [ : , : ] ) self . _mean_f_prop = f_row_sum / float ( sum ( [ len ( t ) for t in e_set . _text ] ) ) ret = "ok" else : raise util_functions . InputError ( e_set , "needs to be an essay set of the train type." ) else : raise util_functions . InputError ( e_set , "wrong input. need an essay set object" ) return ret
12586	def get_nifti1hdr_from_h5attrs ( h5attrs ) : hdr = nib . Nifti1Header ( ) for k in list ( h5attrs . keys ( ) ) : hdr [ str ( k ) ] = np . array ( h5attrs [ k ] ) return hdr
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
9283	def set_login ( self , callsign , passwd = "-1" , skip_login = False ) : self . __dict__ . update ( locals ( ) )
3828	async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
2128	def set_display_columns ( self , set_true = [ ] , set_false = [ ] ) : for i in range ( len ( self . fields ) ) : if self . fields [ i ] . name in set_true : self . fields [ i ] . display = True elif self . fields [ i ] . name in set_false : self . fields [ i ] . display = False
8455	def _apply_template ( template , target , * , checkout , extra_context ) : with tempfile . TemporaryDirectory ( ) as tempdir : repo_dir = cc_main . cookiecutter ( template , checkout = checkout , no_input = True , output_dir = tempdir , extra_context = extra_context ) for item in os . listdir ( repo_dir ) : src = os . path . join ( repo_dir , item ) dst = os . path . join ( target , item ) if os . path . isdir ( src ) : if os . path . exists ( dst ) : shutil . rmtree ( dst ) shutil . copytree ( src , dst ) else : if os . path . exists ( dst ) : os . remove ( dst ) shutil . copy2 ( src , dst )
5925	def check_setup ( ) : if "GROMACSWRAPPER_SUPPRESS_SETUP_CHECK" in os . environ : return True missing = [ d for d in config_directories if not os . path . exists ( d ) ] if len ( missing ) > 0 : print ( "NOTE: Some configuration directories are not set up yet: " ) print ( "\t{0!s}" . format ( '\n\t' . join ( missing ) ) ) print ( "NOTE: You can create the configuration file and directories with:" ) print ( "\t>>> import gromacs" ) print ( "\t>>> gromacs.config.setup()" ) return False return True
2278	def parse ( config ) : if not isinstance ( config , basestring ) : raise TypeError ( "Contains input must be a simple string" ) validator = ContainsValidator ( ) validator . contains_string = config return validator
13415	def addlabel ( ax = None , toplabel = None , xlabel = None , ylabel = None , zlabel = None , clabel = None , cb = None , windowlabel = None , fig = None , axes = None ) : if ( axes is None ) and ( ax is not None ) : axes = ax if ( windowlabel is not None ) and ( fig is not None ) : fig . canvas . set_window_title ( windowlabel ) if fig is None : fig = _plt . gcf ( ) if fig is not None and axes is None : axes = fig . get_axes ( ) if axes == [ ] : logger . error ( 'No axes found!' ) if axes is not None : if toplabel is not None : axes . set_title ( toplabel ) if xlabel is not None : axes . set_xlabel ( xlabel ) if ylabel is not None : axes . set_ylabel ( ylabel ) if zlabel is not None : axes . set_zlabel ( zlabel ) if ( clabel is not None ) or ( cb is not None ) : if ( clabel is not None ) and ( cb is not None ) : cb . set_label ( clabel ) else : if clabel is None : logger . error ( 'Missing colorbar label' ) else : logger . error ( 'Missing colorbar instance' )
7742	def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
4151	def power ( self ) : r if self . scale_by_freq == False : return sum ( self . psd ) * len ( self . psd ) else : return sum ( self . psd ) * self . df / ( 2. * numpy . pi )
13199	def format_content ( self , format = 'plain' , mathjax = False , smart = True , extra_args = None ) : output_text = convert_lsstdoc_tex ( self . _tex , format , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
12697	def _parse_string ( self , xml ) : if not isinstance ( xml , HTMLElement ) : xml = dhtmlparser . parseString ( str ( xml ) ) record = xml . find ( "record" ) if not record : raise ValueError ( "There is no <record> in your MARC XML document!" ) record = record [ 0 ] self . oai_marc = len ( record . find ( "oai_marc" ) ) > 0 if not self . oai_marc : leader = record . find ( "leader" ) if len ( leader ) >= 1 : self . leader = leader [ 0 ] . getContent ( ) if self . oai_marc : self . _parse_control_fields ( record . find ( "fixfield" ) , "id" ) self . _parse_data_fields ( record . find ( "varfield" ) , "id" , "label" ) else : self . _parse_control_fields ( record . find ( "controlfield" ) , "tag" ) self . _parse_data_fields ( record . find ( "datafield" ) , "tag" , "code" ) if self . oai_marc and "LDR" in self . controlfields : self . leader = self . controlfields [ "LDR" ]
9523	def scaffolds_to_contigs ( infile , outfile , number_contigs = False ) : seq_reader = sequences . file_reader ( infile ) fout = utils . open_file_write ( outfile ) for seq in seq_reader : contigs = seq . contig_coords ( ) counter = 1 for contig in contigs : if number_contigs : name = seq . id + '.' + str ( counter ) counter += 1 else : name = '.' . join ( [ seq . id , str ( contig . start + 1 ) , str ( contig . end + 1 ) ] ) print ( sequences . Fasta ( name , seq [ contig . start : contig . end + 1 ] ) , file = fout ) utils . close ( fout )
10812	def query_by_user ( cls , user , with_pending = False , eager = False ) : q1 = Group . query . join ( Membership ) . filter_by ( user_id = user . get_id ( ) ) if not with_pending : q1 = q1 . filter_by ( state = MembershipState . ACTIVE ) if eager : q1 = q1 . options ( joinedload ( Group . members ) ) q2 = Group . query . join ( GroupAdmin ) . filter_by ( admin_id = user . get_id ( ) , admin_type = resolve_admin_type ( user ) ) if eager : q2 = q2 . options ( joinedload ( Group . members ) ) query = q1 . union ( q2 ) . with_entities ( Group . id ) return Group . query . filter ( Group . id . in_ ( query ) )
3867	async def set_notification_level ( self , level ) : await self . _client . set_conversation_notification_level ( hangouts_pb2 . SetConversationNotificationLevelRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , level = level , ) )
3826	async def get_self_info ( self , get_self_info_request ) : response = hangouts_pb2 . GetSelfInfoResponse ( ) await self . _pb_request ( 'contacts/getselfinfo' , get_self_info_request , response ) return response
674	def _loadDummyModelParameters ( self , params ) : for key , value in params . iteritems ( ) : if type ( value ) == list : index = self . modelIndex % len ( params [ key ] ) self . _params [ key ] = params [ key ] [ index ] else : self . _params [ key ] = params [ key ]
846	def _getDistances ( self , inputPattern , partitionId = None ) : if not self . _finishedLearning : self . finishLearning ( ) self . _finishedLearning = True if self . _vt is not None and len ( self . _vt ) > 0 : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) sparseInput = self . _sparsifyVector ( inputPattern ) dist = self . _calcDistance ( sparseInput ) if self . _specificIndexTraining : dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf if partitionId is not None : dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf return dist
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
7315	def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
13775	def includeme ( config ) : settings = config . get_settings ( ) should_create = asbool ( settings . get ( 'baka_model.should_create_all' , False ) ) should_drop = asbool ( settings . get ( 'baka_model.should_drop_all' , False ) ) config . add_settings ( { "retry.attempts" : 3 , "tm.activate_hook" : tm_activate_hook , "tm.annotate_user" : False , } ) config . include ( 'pyramid_retry' ) config . include ( 'pyramid_tm' ) engine = get_engine ( settings ) session_factory = get_session_factory ( engine ) config . registry [ 'db_session_factory' ] = session_factory config . add_request_method ( lambda r : get_tm_session ( session_factory , r . tm ) , 'db' , reify = True ) config . include ( '.service' ) config . action ( None , bind_engine , ( engine , ) , { 'should_create' : should_create , 'should_drop' : should_drop } , order = 10 )
10003	def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
12922	def render ( self , * args , ** kwargs ) : render_to = StringIO ( ) self . output ( render_to , * args , ** kwargs ) return render_to . getvalue ( )
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
10974	def new ( ) : form = GroupForm ( request . form ) if form . validate_on_submit ( ) : try : group = Group . create ( admins = [ current_user ] , ** form . data ) flash ( _ ( 'Group "%(name)s" created' , name = group . name ) , 'success' ) return redirect ( url_for ( ".index" ) ) except IntegrityError : flash ( _ ( 'Group creation failure' ) , 'error' ) return render_template ( "invenio_groups/new.html" , form = form , )
5442	def parse_file_provider ( uri ) : providers = { 'gs' : job_model . P_GCS , 'file' : job_model . P_LOCAL } provider_found = re . match ( r'^([A-Za-z][A-Za-z0-9+.-]{0,29})://' , uri ) if provider_found : prefix = provider_found . group ( 1 ) . lower ( ) else : prefix = 'file' if prefix in providers : return providers [ prefix ] else : raise ValueError ( 'File prefix not supported: %s://' % prefix )
10562	def _normalize_metadata ( metadata ) : metadata = str ( metadata ) metadata = metadata . lower ( ) metadata = re . sub ( r'\/\s*\d+' , '' , metadata ) metadata = re . sub ( r'^0+([0-9]+)' , r'\1' , metadata ) metadata = re . sub ( r'^\d+\.+' , '' , metadata ) metadata = re . sub ( r'[^\w\s]' , '' , metadata ) metadata = re . sub ( r'\s+' , ' ' , metadata ) metadata = re . sub ( r'^\s+' , '' , metadata ) metadata = re . sub ( r'\s+$' , '' , metadata ) metadata = re . sub ( r'^the\s+' , '' , metadata , re . I ) return metadata
7118	def merge_dicts ( d1 , d2 , _path = None ) : if _path is None : _path = ( ) if isinstance ( d1 , dict ) and isinstance ( d2 , dict ) : for k , v in d2 . items ( ) : if isinstance ( v , MissingValue ) and v . name is None : v . name = '.' . join ( _path + ( k , ) ) if isinstance ( v , DeletedValue ) : d1 . pop ( k , None ) elif k not in d1 : if isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( { } , v , _path + ( k , ) ) else : d1 [ k ] = v else : if isinstance ( d1 [ k ] , dict ) and isinstance ( v , dict ) : d1 [ k ] = merge_dicts ( d1 [ k ] , v , _path + ( k , ) ) elif isinstance ( d1 [ k ] , list ) and isinstance ( v , list ) : d1 [ k ] += v elif isinstance ( d1 [ k ] , MissingValue ) : d1 [ k ] = v elif d1 [ k ] is None : d1 [ k ] = v elif type ( d1 [ k ] ) == type ( v ) : d1 [ k ] = v else : raise TypeError ( 'Refusing to replace a %s with a %s' % ( type ( d1 [ k ] ) , type ( v ) ) ) else : raise TypeError ( 'Cannot merge a %s with a %s' % ( type ( d1 ) , type ( d2 ) ) ) return d1
4503	def SPI ( ledtype = None , num = 0 , ** kwargs ) : from . . . project . types . ledtype import make if ledtype is None : raise ValueError ( 'Must provide ledtype value!' ) ledtype = make ( ledtype ) if num == 0 : raise ValueError ( 'Must provide num value >0!' ) if ledtype not in SPI_DRIVERS . keys ( ) : raise ValueError ( '{} is not a valid LED type.' . format ( ledtype ) ) return SPI_DRIVERS [ ledtype ] ( num , ** kwargs )
7084	def _make_magseries_plot ( axes , stimes , smags , serrs , magsarefluxes = False , ms = 2.0 ) : scaledplottime = stimes - npmin ( stimes ) axes . plot ( scaledplottime , smags , marker = 'o' , ms = ms , ls = 'None' , mew = 0 , color = 'green' , rasterized = True ) if not magsarefluxes : plot_ylim = axes . get_ylim ( ) axes . set_ylim ( ( plot_ylim [ 1 ] , plot_ylim [ 0 ] ) ) axes . set_xlim ( ( npmin ( scaledplottime ) - 1.0 , npmax ( scaledplottime ) + 1.0 ) ) axes . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) plot_xlabel = 'JD - %.3f' % npmin ( stimes ) if magsarefluxes : plot_ylabel = 'flux' else : plot_ylabel = 'magnitude' axes . set_xlabel ( plot_xlabel ) axes . set_ylabel ( plot_ylabel ) axes . get_yaxis ( ) . get_major_formatter ( ) . set_useOffset ( False ) axes . get_xaxis ( ) . get_major_formatter ( ) . set_useOffset ( False )
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) for row in reader : task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
7926	def shuffle_srv ( records ) : if not records : return [ ] ret = [ ] while len ( records ) > 1 : weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 thres = random . random ( ) * weight_sum weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 if thres < weight_sum : records . remove ( rrecord ) ret . append ( rrecord ) break ret . append ( records [ 0 ] ) return ret
11250	def variance ( numbers , type = 'population' ) : mean = average ( numbers ) variance = 0 for number in numbers : variance += ( mean - number ) ** 2 if type == 'population' : return variance / len ( numbers ) else : return variance / ( len ( numbers ) - 1 )
7011	def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : if isinstance ( lspinfo , str ) and os . path . exists ( lspinfo ) : LOGINFO ( 'loading LSP info from pickle %s' % lspinfo ) with open ( lspinfo , 'rb' ) as infd : lspinfo = pickle . load ( infd ) try : periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] lspmethod = lspinfo [ 'method' ] plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( PLOTYLABELS [ lspmethod ] ) plottitle = '%s best period: %.6f d' % ( METHODSHORTLABELS [ lspmethod ] , bestperiod ) plt . title ( plottitle ) for bestperiod , bestpeak in zip ( lspinfo [ 'nbestperiods' ] , lspinfo [ 'nbestlspvals' ] ) : plt . annotate ( '%.6f' % bestperiod , xy = ( bestperiod , bestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = 'x-small' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) if outfile and isinstance ( outfile , str ) : if outfile . endswith ( '.png' ) : plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) else : plt . savefig ( outfile , bbox_inches = 'tight' ) plt . close ( ) return os . path . abspath ( outfile ) elif dispok : plt . show ( ) plt . close ( ) return else : LOGWARNING ( 'no output file specified and no $DISPLAY set, ' 'saving to lsp-plot.png in current directory' ) outfile = 'lsp-plot.png' plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) plt . close ( ) return os . path . abspath ( outfile ) except Exception as e : LOGEXCEPTION ( 'could not plot this LSP, appears to be empty' ) return
12295	def annotate_metadata_dependencies ( repo ) : options = repo . options if 'dependencies' not in options : print ( "No dependencies" ) return [ ] repos = [ ] dependent_repos = options [ 'dependencies' ] for d in dependent_repos : if "/" not in d : print ( "Invalid dependency specification" ) ( username , reponame ) = d . split ( "/" ) try : repos . append ( repo . manager . lookup ( username , reponame ) ) except : print ( "Repository does not exist. Please create one" , d ) package = repo . package package [ 'dependencies' ] = [ ] for r in repos : package [ 'dependencies' ] . append ( { 'username' : r . username , 'reponame' : r . reponame , } )
10532	def create_project ( name , short_name , description ) : try : project = dict ( name = name , short_name = short_name , description = description ) res = _pybossa_req ( 'post' , 'project' , payload = project ) if res . get ( 'id' ) : return Project ( res ) else : return res except : raise
7374	async def throw ( response , loads = None , encoding = None , ** kwargs ) : if loads is None : loads = data_processing . loads data = await data_processing . read ( response , loads = loads , encoding = encoding ) error = get_error ( data ) if error is not None : exception = errors [ error [ 'code' ] ] raise exception ( response = response , error = error , data = data , ** kwargs ) if response . status in statuses : exception = statuses [ response . status ] raise exception ( response = response , data = data , ** kwargs ) raise PeonyException ( response = response , data = data , ** kwargs )
6393	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) if lzma is not None : src_comp = lzma . compress ( src ) [ 14 : ] tar_comp = lzma . compress ( tar ) [ 14 : ] concat_comp = lzma . compress ( src + tar ) [ 14 : ] concat_comp2 = lzma . compress ( tar + src ) [ 14 : ] else : raise ValueError ( 'Install the PylibLZMA module in order to use LZMA' ) return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
9872	def start_response ( self , status , response_headers , exc_info = None ) : if exc_info : try : if self . headers_sent : raise finally : exc_info = None elif self . header_set : raise AssertionError ( "Headers already set!" ) if PY3K and not isinstance ( status , str ) : self . status = str ( status , 'ISO-8859-1' ) else : self . status = status try : self . header_set = Headers ( response_headers ) except UnicodeDecodeError : self . error = ( '500 Internal Server Error' , 'HTTP Headers should be bytes' ) self . err_log . error ( 'Received HTTP Headers from client that contain' ' invalid characters for Latin-1 encoding.' ) return self . write_warning
8720	def operation_upload ( uploader , sources , verify , do_compile , do_file , do_restart ) : sources , destinations = destination_from_source ( sources ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : if do_compile : uploader . file_remove ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) uploader . write_file ( filename , dst , verify ) if do_compile and dst != 'init.lua' : uploader . file_compile ( dst ) uploader . file_remove ( dst ) if do_file : uploader . file_do ( os . path . splitext ( dst ) [ 0 ] + '.lc' ) elif do_file : uploader . file_do ( dst ) else : raise Exception ( 'Error preparing nodemcu for reception' ) else : raise Exception ( 'You must specify a destination filename for each file you want to upload.' ) if do_restart : uploader . node_restart ( ) log . info ( 'All done!' )
11579	def system_reset ( self ) : data = chr ( self . SYSTEM_RESET ) self . pymata . transport . write ( data ) with self . pymata . data_lock : for _ in range ( len ( self . digital_response_table ) ) : self . digital_response_table . pop ( ) for _ in range ( len ( self . analog_response_table ) ) : self . analog_response_table . pop ( ) for pin in range ( 0 , self . total_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . digital_response_table . append ( response_entry ) for pin in range ( 0 , self . number_of_analog_pins_discovered ) : response_entry = [ self . pymata . INPUT , 0 , None ] self . analog_response_table . append ( response_entry )
12467	def run_hook ( hook , config , quiet = False ) : if not hook : return True if not quiet : print_message ( '== Step 3. Run post-bootstrap hook ==' ) result = not run_cmd ( prepare_args ( hook , config ) , echo = not quiet , fail_silently = True , shell = True ) if not quiet : print_message ( ) return result
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
706	def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : if not self . _createCheckpoints : modelCheckpointGUID = None self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( "Running Model. \nmodelParams: %s, \nmodelID=%s, " % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) cpuTimeStart = time . clock ( ) logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( "%s" , e )
1216	def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer
7454	def estimate_optim ( data , testfile , ipyclient ) : insize = os . path . getsize ( testfile ) tmp_file_name = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-step1-count.fq" ) if testfile . endswith ( ".gz" ) : infile = gzip . open ( testfile ) outfile = gzip . open ( tmp_file_name , 'wb' , compresslevel = 5 ) else : infile = open ( testfile ) outfile = open ( tmp_file_name , 'w' ) outfile . write ( "" . join ( itertools . islice ( infile , 40000 ) ) ) outfile . close ( ) infile . close ( ) tmp_size = os . path . getsize ( tmp_file_name ) inputreads = int ( insize / tmp_size ) * 10000 os . remove ( tmp_file_name ) return inputreads
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return None
43	def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : if replay_strategy == 'future' : future_p = 1 - ( 1. / ( 1 + replay_k ) ) else : future_p = 0 def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : T = episode_batch [ 'u' ] . shape [ 1 ] rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] batch_size = batch_size_in_transitions episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) t_samples = np . random . randint ( T , size = batch_size ) transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) for key in episode_batch . keys ( ) } her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) future_offset = future_offset . astype ( int ) future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] transitions [ 'g' ] [ her_indexes ] = future_ag info = { } for key , value in transitions . items ( ) : if key . startswith ( 'info_' ) : info [ key . replace ( 'info_' , '' ) ] = value reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } reward_params [ 'info' ] = info transitions [ 'r' ] = reward_fun ( ** reward_params ) transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) for k in transitions . keys ( ) } assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) return transitions return _sample_her_transitions
9833	def initialize ( self ) : return self . DXclasses [ self . type ] ( self . id , ** self . args )
7007	def _fourier_func ( fourierparams , phase , mags ) : order = int ( len ( fourierparams ) / 2 ) f_amp = fourierparams [ : order ] f_pha = fourierparams [ order : ] f_orders = [ f_amp [ x ] * npcos ( 2.0 * pi_value * x * phase + f_pha [ x ] ) for x in range ( order ) ] total_f = npmedian ( mags ) for fo in f_orders : total_f += fo return total_f
6331	def encode ( self , word , terminator = '\0' ) : r if word : if terminator in word : raise ValueError ( 'Specified terminator, {}, already in word.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : word += terminator wordlist = sorted ( word [ i : ] + word [ : i ] for i in range ( len ( word ) ) ) return '' . join ( [ w [ - 1 ] for w in wordlist ] ) else : return terminator
929	def _getFuncPtrAndParams ( self , funcName ) : params = None if isinstance ( funcName , basestring ) : if funcName == 'sum' : fp = _aggr_sum elif funcName == 'first' : fp = _aggr_first elif funcName == 'last' : fp = _aggr_last elif funcName == 'mean' : fp = _aggr_mean elif funcName == 'max' : fp = max elif funcName == 'min' : fp = min elif funcName == 'mode' : fp = _aggr_mode elif funcName . startswith ( 'wmean:' ) : fp = _aggr_weighted_mean paramsName = funcName [ 6 : ] params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) else : fp = funcName return ( fp , params )
9139	def label ( labels = [ ] , language = 'any' , sortLabel = False ) : if not labels : return None if not language : language = 'und' labels = [ dict_to_label ( l ) for l in labels ] l = False if sortLabel : l = find_best_label_for_type ( labels , language , 'sortLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'prefLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'altLabel' ) if l : return l else : return label ( labels , 'any' , sortLabel ) if language != 'any' else None
8784	def create_port ( self , context , network_id , port_id , ** kwargs ) : LOG . info ( "create_port %s %s %s" % ( context . tenant_id , network_id , port_id ) ) if not kwargs . get ( 'base_net_driver' ) : raise IronicException ( msg = 'base_net_driver required.' ) base_net_driver = kwargs [ 'base_net_driver' ] if not kwargs . get ( 'device_id' ) : raise IronicException ( msg = 'device_id required.' ) device_id = kwargs [ 'device_id' ] if not kwargs . get ( 'instance_node_id' ) : raise IronicException ( msg = 'instance_node_id required.' ) instance_node_id = kwargs [ 'instance_node_id' ] if not kwargs . get ( 'mac_address' ) : raise IronicException ( msg = 'mac_address is required.' ) mac_address = str ( netaddr . EUI ( kwargs [ "mac_address" ] [ "address" ] ) ) mac_address = mac_address . replace ( '-' , ':' ) if kwargs . get ( 'security_groups' ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) fixed_ips = [ ] addresses = kwargs . get ( 'addresses' ) if not isinstance ( addresses , list ) : addresses = [ addresses ] for address in addresses : fixed_ips . append ( self . _make_fixed_ip_dict ( context , address ) ) body = { "id" : port_id , "network_id" : network_id , "device_id" : device_id , "device_owner" : kwargs . get ( 'device_owner' , '' ) , "tenant_id" : context . tenant_id or "quark" , "roles" : context . roles , "mac_address" : mac_address , "fixed_ips" : fixed_ips , "switch:hardware_id" : instance_node_id , "dynamic_network" : not STRATEGY . is_provider_network ( network_id ) } net_info = self . _get_base_network_info ( context , network_id , base_net_driver ) body . update ( net_info ) try : LOG . info ( "creating downstream port: %s" % ( body ) ) port = self . _create_port ( context , body ) LOG . info ( "created downstream port: %s" % ( port ) ) return { "uuid" : port [ 'port' ] [ 'id' ] , "vlan_id" : port [ 'port' ] [ 'vlan_id' ] } except Exception as e : msg = "failed to create downstream port. Exception: %s" % ( e ) raise IronicException ( msg = msg )
6396	def sim_minkowski ( src , tar , qval = 2 , pval = 1 , alphabet = None ) : return Minkowski ( ) . sim ( src , tar , qval , pval , alphabet )
187	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image
13600	def increment ( cls , name ) : with transaction . atomic ( ) : counter = Counter . objects . select_for_update ( ) . get ( name = name ) counter . value += 1 counter . save ( ) return counter . value
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
8970	def step ( self , other_pub ) : if self . triggersStep ( other_pub ) : self . __wrapOtherPub ( other_pub ) self . __newRootKey ( "receiving" ) self . __newRatchetKey ( ) self . __newRootKey ( "sending" )
13050	def main ( ) : search = ServiceSearch ( ) services = search . get_services ( up = True , tags = [ '!header_scan' ] ) print_notification ( "Scanning {} services" . format ( len ( services ) ) ) urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) pool = Pool ( 100 ) count = 0 for service in services : count += 1 if count % 50 == 0 : print_notification ( "Checking {}/{} services" . format ( count , len ( services ) ) ) pool . spawn ( check_service , service ) pool . join ( ) print_notification ( "Completed, 'http' tag added to services that respond to http, 'https' tag added to services that respond to https." )
3516	def woopra ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return WoopraNode ( )
12403	def requirements_for_changes ( self , changes ) : requirements = [ ] reqs_set = set ( ) if isinstance ( changes , str ) : changes = changes . split ( '\n' ) if not changes or changes [ 0 ] . startswith ( '-' ) : return requirements for line in changes : line = line . strip ( ' -+*' ) if not line : continue match = IS_REQUIREMENTS_RE2 . search ( line ) if match : for match in REQUIREMENTS_RE . findall ( match . group ( 1 ) ) : if match [ 1 ] : version = '==' + match [ 2 ] if match [ 1 ] . startswith ( ' to ' ) else match [ 1 ] req_str = match [ 0 ] + version else : req_str = match [ 0 ] if req_str not in reqs_set : reqs_set . add ( req_str ) try : requirements . append ( pkg_resources . Requirement . parse ( req_str ) ) except Exception as e : log . warn ( 'Could not parse requirement "%s" from changes: %s' , req_str , e ) return requirements
4620	def unlock ( self , password ) : self . password = password if self . config_key in self . config and self . config [ self . config_key ] : self . _decrypt_masterpassword ( ) else : self . _new_masterpassword ( password ) self . _save_encrypted_masterpassword ( )
5268	def _check_input ( self , input ) : if isinstance ( input , str ) : return 'st' elif isinstance ( input , list ) : if all ( isinstance ( item , str ) for item in input ) : return 'gst' raise ValueError ( "String argument should be of type String or" " a list of strings" )
1591	def _setup_custom_grouping ( self , topology ) : for i in range ( len ( topology . bolts ) ) : for in_stream in topology . bolts [ i ] . inputs : if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) if isinstance ( custom_grouping_obj , str ) : pex_loader . load_pex ( self . topology_pex_abs_path ) grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) custom_grouping_obj = grouping_cls ( ) assert isinstance ( custom_grouping_obj , ICustomGrouping ) self . custom_grouper . add ( in_stream . stream . id , self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , custom_grouping_obj , self . my_component_name ) elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : raise NotImplementedError ( "Java-serialized custom grouping is not yet supported " "for python topology" ) else : raise ValueError ( "Unrecognized custom grouping type found: %s" % str ( in_stream . type ) )
8362	def encode ( self , o ) : if isinstance ( o , basestring ) : if isinstance ( o , str ) : _encoding = self . encoding if ( _encoding is not None and not ( _encoding == 'utf-8' ) ) : o = o . decode ( _encoding ) if self . ensure_ascii : return encode_basestring_ascii ( o ) else : return encode_basestring ( o ) chunks = list ( self . iterencode ( o ) ) return '' . join ( chunks )
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_topologies_watch ( state_manager , topologies ) : Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
4361	def _watcher ( self ) : while True : gevent . sleep ( 1.0 ) if not self . connected : for ns_name , ns in list ( six . iteritems ( self . active_ns ) ) : ns . recv_disconnect ( ) gevent . killall ( self . jobs ) break
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
13145	def remove_near_duplicate_relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) _assert_threshold ( threshold ) duplicate_rel_counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate_rel_counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num_triples = len ( triples ) removal_relation_set = set ( ) for rel , values in duplicate_rel_counter . items ( ) : duplicate_rel_counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal_relation_set or j in removal_relation_set : continue close_relations = [ i ] if _set_close_to ( duplicate_rel_counter [ i ] , duplicate_rel_counter [ j ] , threshold ) : close_relations . append ( j ) if len ( close_relations ) > 1 : close_relations . pop ( np . random . randint ( len ( close_relations ) ) ) removal_relation_set |= set ( close_relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal_relation_set ) , str ( removal_relation_set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal_relation_set , triples ) )
3140	def get ( self , folder_id , ** queryparams ) : self . folder_id = folder_id return self . _mc_client . _get ( url = self . _build_path ( folder_id ) , ** queryparams )
7108	def print_cm ( cm , labels , hide_zeroes = False , hide_diagonal = False , hide_threshold = None ) : columnwidth = max ( [ len ( x ) for x in labels ] + [ 5 ] ) empty_cell = " " * columnwidth print ( " " + empty_cell , end = " " ) for label in labels : print ( "%{0}s" . format ( columnwidth ) % label , end = " " ) print ( ) for i , label1 in enumerate ( labels ) : print ( " %{0}s" . format ( columnwidth ) % label1 , end = " " ) for j in range ( len ( labels ) ) : cell = "%{0}.1f" . format ( columnwidth ) % cm [ i , j ] if hide_zeroes : cell = cell if float ( cm [ i , j ] ) != 0 else empty_cell if hide_diagonal : cell = cell if i != j else empty_cell if hide_threshold : cell = cell if cm [ i , j ] > hide_threshold else empty_cell print ( cell , end = " " ) print ( )
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , ** kwargs ) : completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
3422	def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
3018	def _generate_assertion ( self ) : now = int ( time . time ( ) ) payload = { 'aud' : self . token_uri , 'scope' : self . _scopes , 'iat' : now , 'exp' : now + self . MAX_TOKEN_LIFETIME_SECS , 'iss' : self . _service_account_email , } payload . update ( self . _kwargs ) return crypt . make_signed_jwt ( self . _signer , payload , key_id = self . _private_key_id )
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
5826	def _validate_search_query ( self , returning_query ) : start_index = returning_query . from_index or 0 size = returning_query . size or 0 if start_index < 0 : raise CitrinationClientError ( "start_index cannot be negative. Please enter a value greater than or equal to zero" ) if size < 0 : raise CitrinationClientError ( "Size cannot be negative. Please enter a value greater than or equal to zero" ) if start_index + size > MAX_QUERY_DEPTH : raise CitrinationClientError ( "Citrination does not support pagination past the {0}th result. Please reduce either the from_index and/or size such that their sum is below {0}" . format ( MAX_QUERY_DEPTH ) )
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
3734	def load_included_indentifiers ( self , file_name ) : self . restrict_identifiers = True included_identifiers = set ( ) with open ( file_name ) as f : [ included_identifiers . add ( int ( line ) ) for line in f ] self . included_identifiers = included_identifiers
7909	def __presence_available ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_available_presence ( MucPresence ( stanza ) ) return True
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
4115	def lar2rc ( g ) : assert numpy . isrealobj ( g ) , 'Log area ratios not defined for complex reflection coefficients.' return - numpy . tanh ( - numpy . array ( g ) / 2 )
4934	def parse_datetime_to_epoch ( datestamp , magnitude = 1.0 ) : parsed_datetime = parse_lms_api_datetime ( datestamp ) time_since_epoch = parsed_datetime - UNIX_EPOCH return int ( time_since_epoch . total_seconds ( ) * magnitude )
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
1291	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . demo_memory = Replay ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , include_next_states = True , capacity = self . demo_memory_capacity , scope = 'demo-replay' , summary_labels = self . summary_labels ) self . fn_import_demo_experience = tf . make_template ( name_ = 'import-demo-experience' , func_ = self . tf_import_demo_experience , custom_getter_ = custom_getter ) self . fn_demo_loss = tf . make_template ( name_ = 'demo-loss' , func_ = self . tf_demo_loss , custom_getter_ = custom_getter ) self . fn_combined_loss = tf . make_template ( name_ = 'combined-loss' , func_ = self . tf_combined_loss , custom_getter_ = custom_getter ) self . fn_demo_optimization = tf . make_template ( name_ = 'demo-optimization' , func_ = self . tf_demo_optimization , custom_getter_ = custom_getter ) return custom_getter
12445	def options ( self , request , response ) : response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) response . status = http . client . OK
3727	def Vc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Vc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Vc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) : methods . append ( PSRK ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Vc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Vc = float ( _crit_IUPAC . at [ CASRN , 'Vc' ] ) elif Method == PSRK : _Vc = float ( _crit_PSRKR4 . at [ CASRN , 'Vc' ] ) elif Method == MATTHEWS : _Vc = float ( _crit_Matthews . at [ CASRN , 'Vc' ] ) elif Method == CRC : _Vc = float ( _crit_CRC . at [ CASRN , 'Vc' ] ) elif Method == YAWS : _Vc = float ( _crit_Yaws . at [ CASRN , 'Vc' ] ) elif Method == SURF : _Vc = third_property ( CASRN = CASRN , V = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Vc
13743	def get_schema ( self ) : if not self . schema : raise NotImplementedError ( 'You must provide a schema value or override the get_schema method' ) return self . conn . create_schema ( ** self . schema )
9863	def get_homes ( self , only_active = True ) : return [ self . get_home ( home_id ) for home_id in self . get_home_ids ( only_active ) ]
9140	def find_best_label_for_type ( labels , language , labeltype ) : typelabels = [ l for l in labels if l . type == labeltype ] if not typelabels : return False if language == 'any' : return typelabels [ 0 ] exact = filter_labels_by_language ( typelabels , language ) if exact : return exact [ 0 ] inexact = filter_labels_by_language ( typelabels , language , True ) if inexact : return inexact [ 0 ] return False
6817	def create_local_renderer ( self ) : r = super ( ApacheSatchel , self ) . create_local_renderer ( ) os_version = self . os_version apache_specifics = r . env . specifics [ os_version . type ] [ os_version . distro ] r . env . update ( apache_specifics ) return r
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
9345	def call ( self , args , axis = 0 , out = None , chunksize = 1024 * 1024 , ** kwargs ) : if self . altreduce is not None : ret = [ None ] else : if out is None : if self . outdtype is not None : dtype = self . outdtype else : try : dtype = numpy . result_type ( * [ args [ i ] for i in self . ins ] * 2 ) except : dtype = None out = sharedmem . empty ( numpy . broadcast ( * [ args [ i ] for i in self . ins ] * 2 ) . shape , dtype = dtype ) if axis != 0 : for i in self . ins : args [ i ] = numpy . rollaxis ( args [ i ] , axis ) out = numpy . rollaxis ( out , axis ) size = numpy . max ( [ len ( args [ i ] ) for i in self . ins ] ) with sharedmem . MapReduce ( ) as pool : def work ( i ) : sl = slice ( i , i + chunksize ) myargs = args [ : ] for j in self . ins : try : tmp = myargs [ j ] [ sl ] a , b , c = sl . indices ( len ( args [ j ] ) ) myargs [ j ] = tmp except Exception as e : print tmp print j , e pass if b == a : return None rt = self . ufunc ( * myargs , ** kwargs ) if self . altreduce is not None : return rt else : out [ sl ] = rt def reduce ( rt ) : if self . altreduce is None : return if ret [ 0 ] is None : ret [ 0 ] = rt elif rt is not None : ret [ 0 ] = self . altreduce ( ret [ 0 ] , rt ) pool . map ( work , range ( 0 , size , chunksize ) , reduce = reduce ) if self . altreduce is None : if axis != 0 : out = numpy . rollaxis ( out , 0 , axis + 1 ) return out else : return ret [ 0 ]
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
3415	def model_from_dict ( obj ) : if 'reactions' not in obj : raise ValueError ( 'Object has no reactions attribute. Cannot load.' ) model = Model ( ) model . add_metabolites ( [ metabolite_from_dict ( metabolite ) for metabolite in obj [ 'metabolites' ] ] ) model . genes . extend ( [ gene_from_dict ( gene ) for gene in obj [ 'genes' ] ] ) model . add_reactions ( [ reaction_from_dict ( reaction , model ) for reaction in obj [ 'reactions' ] ] ) objective_reactions = [ rxn for rxn in obj [ 'reactions' ] if rxn . get ( 'objective_coefficient' , 0 ) != 0 ] coefficients = { model . reactions . get_by_id ( rxn [ 'id' ] ) : rxn [ 'objective_coefficient' ] for rxn in objective_reactions } set_objective ( model , coefficients ) for k , v in iteritems ( obj ) : if k in { 'id' , 'name' , 'notes' , 'compartments' , 'annotation' } : setattr ( model , k , v ) return model
11809	def index_collection ( self , filenames ) : "Index a whole collection of files." for filename in filenames : self . index_document ( open ( filename ) . read ( ) , filename )
4156	def arma2psd ( A = None , B = None , rho = 1. , T = 1. , NFFT = 4096 , sides = 'default' , norm = False ) : r if NFFT is None : NFFT = 4096 if A is None and B is None : raise ValueError ( "Either AR or MA model must be provided" ) psd = np . zeros ( NFFT , dtype = complex ) if A is not None : ip = len ( A ) den = np . zeros ( NFFT , dtype = complex ) den [ 0 ] = 1. + 0j for k in range ( 0 , ip ) : den [ k + 1 ] = A [ k ] denf = fft ( den , NFFT ) if B is not None : iq = len ( B ) num = np . zeros ( NFFT , dtype = complex ) num [ 0 ] = 1. + 0j for k in range ( 0 , iq ) : num [ k + 1 ] = B [ k ] numf = fft ( num , NFFT ) if A is not None and B is not None : psd = rho / T * abs ( numf ) ** 2. / abs ( denf ) ** 2. elif A is not None : psd = rho / T / abs ( denf ) ** 2. elif B is not None : psd = rho / T * abs ( numf ) ** 2. psd = np . real ( psd ) if sides != 'default' : from . import tools assert sides in [ 'centerdc' ] if sides == 'centerdc' : psd = tools . twosided_2_centerdc ( psd ) if norm == True : psd /= max ( psd ) return psd
13675	def add_path_object ( self , * args ) : for obj in args : obj . bundle = self self . files . append ( obj )
10580	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return self . mm * self . P / R / state [ "T" ]
3991	def _nginx_stream_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_proxy_string ( port_spec , bridge_ip ) ) server_string_spec += "\t }\n" return server_string_spec
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
9464	def conference_undeaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUndeaf/' method = 'POST' return self . request ( path , method , call_params )
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
12753	def indices_for_body ( self , name , step = 3 ) : for j , body in enumerate ( self . bodies ) : if body . name == name : return list ( range ( j * step , ( j + 1 ) * step ) ) return [ ]
1909	def get_profiling_stats ( self ) : profile_file_path = os . path . join ( self . workspace , 'profiling.bin' ) try : return pstats . Stats ( profile_file_path ) except Exception as e : logger . debug ( f'Failed to get profiling stats: {e}' ) return None
7008	def _fourier_chisq ( fourierparams , phase , mags , errs ) : f = _fourier_func ( fourierparams , phase , mags ) chisq = npsum ( ( ( mags - f ) * ( mags - f ) ) / ( errs * errs ) ) return chisq
12726	def stop_cfms ( self , stop_cfms ) : _set_params ( self . ode_obj , 'StopCFM' , stop_cfms , self . ADOF + self . LDOF )
12463	def print_error ( message , wrap = True ) : if wrap : message = 'ERROR: {0}. Exit...' . format ( message . rstrip ( '.' ) ) colorizer = ( _color_wrap ( colorama . Fore . RED ) if colorama else lambda message : message ) return print ( colorizer ( message ) , file = sys . stderr )
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
6549	def fill_field ( self , ypos , xpos , tosend , length ) : if length < len ( tosend ) : raise FieldTruncateError ( 'length limit %d, but got "%s"' % ( length , tosend ) ) if xpos is not None and ypos is not None : self . move_to ( ypos , xpos ) self . delete_field ( ) self . send_string ( tosend )
2886	def is_connected ( self , callback ) : index = self . _weakly_connected_index ( callback ) if index is not None : return True if self . hard_subscribers is None : return False return callback in self . _hard_callbacks ( )
741	def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )
9818	def upgrade ( self ) : if not self . is_valid : raise PolyaxonDeploymentConfigError ( 'Deployment type `{}` not supported' . format ( self . deployment_type ) ) if self . is_kubernetes : self . upgrade_on_kubernetes ( ) elif self . is_docker_compose : self . upgrade_on_docker_compose ( ) elif self . is_docker : self . upgrade_on_docker ( ) elif self . is_heroku : self . upgrade_on_heroku ( )
11189	def edit ( dataset_uri ) : try : dataset = dtoolcore . ProtoDataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) except dtoolcore . DtoolCoreTypeError : dataset = dtoolcore . DataSet . from_uri ( uri = dataset_uri , config_path = CONFIG_PATH ) readme_content = dataset . get_readme_content ( ) try : readme_content = unicode ( readme_content , "utf-8" ) except NameError : pass edited_content = click . edit ( readme_content ) if edited_content is not None : _validate_and_put_readme ( dataset , edited_content ) click . secho ( "Updated readme " , nl = False , fg = "green" ) else : click . secho ( "Did not update readme " , nl = False , fg = "red" ) click . secho ( dataset_uri )
2914	def _inherit_data ( self ) : LOG . debug ( "'%s' inheriting data from '%s'" % ( self . get_name ( ) , self . parent . get_name ( ) ) , extra = dict ( data = self . parent . data ) ) self . set_data ( ** self . parent . data )
2691	def iter_cython ( path ) : for dir_path , dir_names , file_names in os . walk ( path ) : for file_name in file_names : if file_name . startswith ( '.' ) : continue if os . path . splitext ( file_name ) [ 1 ] not in ( '.pyx' , '.pxd' ) : continue yield os . path . join ( dir_path , file_name )
6746	def topological_sort ( source ) : if isinstance ( source , dict ) : source = source . items ( ) pending = sorted ( [ ( name , set ( deps ) ) for name , deps in source ] ) emitted = [ ] while pending : next_pending = [ ] next_emitted = [ ] for entry in pending : name , deps = entry deps . difference_update ( emitted ) if deps : next_pending . append ( entry ) else : yield name emitted . append ( name ) next_emitted . append ( name ) if not next_emitted : raise ValueError ( "cyclic or missing dependancy detected: %r" % ( next_pending , ) ) pending = next_pending emitted = next_emitted
12515	def new_img_like ( ref_niimg , data , affine = None , copy_header = False ) : if not ( hasattr ( ref_niimg , 'get_data' ) and hasattr ( ref_niimg , 'get_affine' ) ) : if isinstance ( ref_niimg , _basestring ) : ref_niimg = nib . load ( ref_niimg ) elif operator . isSequenceType ( ref_niimg ) : ref_niimg = nib . load ( ref_niimg [ 0 ] ) else : raise TypeError ( ( 'The reference image should be a niimg, %r ' 'was passed' ) % ref_niimg ) if affine is None : affine = ref_niimg . get_affine ( ) if data . dtype == bool : default_dtype = np . int8 if ( LooseVersion ( nib . __version__ ) >= LooseVersion ( '1.2.0' ) and isinstance ( ref_niimg , nib . freesurfer . mghformat . MGHImage ) ) : default_dtype = np . uint8 data = as_ndarray ( data , dtype = default_dtype ) header = None if copy_header : header = copy . copy ( ref_niimg . get_header ( ) ) header [ 'scl_slope' ] = 0. header [ 'scl_inter' ] = 0. header [ 'glmax' ] = 0. header [ 'cal_max' ] = np . max ( data ) if data . size > 0 else 0. header [ 'cal_max' ] = np . min ( data ) if data . size > 0 else 0. return ref_niimg . __class__ ( data , affine , header = header )
4560	def stop ( self = None ) : if not self : instance = getattr ( Runner . instance ( ) , 'builder' , None ) self = instance and instance ( ) if not self : return self . _runner . stop ( ) if self . project : self . project . stop ( ) self . project = None
924	def _aggr_mean ( inList ) : aggrSum = 0 nonNone = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem nonNone += 1 if nonNone != 0 : return aggrSum / nonNone else : return None
208	def draw_on_image ( self , image , alpha = 0.75 , cmap = "jet" , resize = "heatmaps" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "heatmaps" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = "cubic" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == "heatmaps" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix
10131	def _unescape ( s , uri = False ) : out = '' while len ( s ) > 0 : c = s [ 0 ] if c == '\\' : esc_c = s [ 1 ] if esc_c in ( 'u' , 'U' ) : out += six . unichr ( int ( s [ 2 : 6 ] , base = 16 ) ) s = s [ 6 : ] continue else : if esc_c == 'b' : out += '\b' elif esc_c == 'f' : out += '\f' elif esc_c == 'n' : out += '\n' elif esc_c == 'r' : out += '\r' elif esc_c == 't' : out += '\t' else : if uri and ( esc_c == '#' ) : out += '\\' out += esc_c s = s [ 2 : ] continue else : out += c s = s [ 1 : ] return out
3240	def get_group_policy_document ( group_name , policy_name , client = None , ** kwargs ) : return client . get_group_policy ( GroupName = group_name , PolicyName = policy_name , ** kwargs ) [ 'PolicyDocument' ]
1554	def _add_in_streams ( self , bolt ) : if self . inputs is None : return input_dict = self . _sanitize_inputs ( ) for global_streamid , gtype in input_dict . items ( ) : in_stream = bolt . inputs . add ( ) in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , global_streamid . stream_id ) ) if isinstance ( gtype , Grouping . FIELDS ) : in_stream . gtype = gtype . gtype in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : in_stream . gtype = gtype . gtype in_stream . custom_grouping_object = gtype . python_serialized in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) else : in_stream . gtype = gtype
1763	def pop_bytes ( self , nbytes , force = False ) : data = self . read_bytes ( self . STACK , nbytes , force = force ) self . STACK += nbytes return data
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
10760	def from_rectilinear ( cls , x , y , z , formatter = numpy_formatter ) : x = np . asarray ( x , dtype = np . float64 ) y = np . asarray ( y , dtype = np . float64 ) z = np . ma . asarray ( z , dtype = np . float64 ) if x . ndim != 1 : raise TypeError ( "'x' must be a 1D array but is a {:d}D array" . format ( x . ndim ) ) if y . ndim != 1 : raise TypeError ( "'y' must be a 1D array but is a {:d}D array" . format ( y . ndim ) ) if z . ndim != 2 : raise TypeError ( "'z' must be a 2D array but it a {:d}D array" . format ( z . ndim ) ) if x . size != z . shape [ 1 ] : raise TypeError ( ( "the length of 'x' must be equal to the number of columns in " "'z' but the length of 'x' is {:d} and 'z' has {:d} " "columns" ) . format ( x . size , z . shape [ 1 ] ) ) if y . size != z . shape [ 0 ] : raise TypeError ( ( "the length of 'y' must be equal to the number of rows in " "'z' but the length of 'y' is {:d} and 'z' has {:d} " "rows" ) . format ( y . size , z . shape [ 0 ] ) ) y , x = np . meshgrid ( y , x , indexing = 'ij' ) return cls ( x , y , z , formatter )
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
2803	def convert_slice ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting slice ...' ) if len ( params [ 'axes' ] ) > 1 : raise AssertionError ( 'Cannot convert slice by multiple dimensions' ) if params [ 'axes' ] [ 0 ] not in [ 0 , 1 , 2 , 3 ] : raise AssertionError ( 'Slice by dimension more than 3 or less than 0 is not supported' ) def target_layer ( x , axis = int ( params [ 'axes' ] [ 0 ] ) , start = int ( params [ 'starts' ] [ 0 ] ) , end = int ( params [ 'ends' ] [ 0 ] ) ) : if axis == 0 : return x [ start : end ] elif axis == 1 : return x [ : , start : end ] elif axis == 2 : return x [ : , : , start : end ] elif axis == 3 : return x [ : , : , : , start : end ] lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
1789	def DIV ( cpu , src ) : size = src . size reg_name_h = { 8 : 'DL' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] dividend = Operators . CONCAT ( size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = Operators . ZEXTEND ( src . read ( ) , size * 2 ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) quotient = Operators . UDIV ( dividend , divisor ) MASK = ( 1 << size ) - 1 if isinstance ( quotient , int ) and quotient > MASK : raise DivideByZeroError ( ) remainder = Operators . UREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , size ) )
8491	def _parse_hosts ( self , hosts ) : if hosts is None : return if isinstance ( hosts , six . string_types ) : hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] hosts = [ host . split ( ':' ) for host in hosts ] hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] return tuple ( hosts )
1251	def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
659	def populationStability ( vectors , numSamples = None ) : numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples
9201	def extract_cycles ( series , left = False , right = False ) : points = deque ( ) for x in reversals ( series , left = left , right = right ) : points . append ( x ) while len ( points ) >= 3 : X = abs ( points [ - 2 ] - points [ - 1 ] ) Y = abs ( points [ - 3 ] - points [ - 2 ] ) if X < Y : break elif len ( points ) == 3 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( ) else : yield points [ - 3 ] , points [ - 2 ] , 1.0 last = points . pop ( ) points . pop ( ) points . pop ( ) points . append ( last ) else : while len ( points ) > 1 : yield points [ 0 ] , points [ 1 ] , 0.5 points . popleft ( )
2858	def bulkread ( self , data = [ ] , lengthR = 'None' , readmode = 1 ) : if ( 1 > lengthR > 65536 ) | ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) if ( lengthR == 'None' ) & ( readmode == 1 ) : lengthR = len ( data ) commandW = 0x10 | ( self . lsbfirst << 3 ) | self . write_clock_ve lengthW = len ( data ) - 1 len_lowW = ( lengthW ) & 0xFF len_highW = ( ( lengthW ) >> 8 ) & 0xFF commandR = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) length = lengthR if lengthR % 2 == 1 : length += 1 length = length / 2 lenremain = lengthR - length len_lowR = ( length - 1 ) & 0xFF len_highR = ( ( length - 1 ) >> 8 ) & 0xFF logger . debug ( 'SPI bulkread with write command {0:2X}.' . format ( commandW ) ) logger . debug ( 'and read command {0:2X}.' . format ( commandR ) ) self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( commandW , len_lowW , len_highW ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data ) ) ) self . _ft232h . _write ( str ( bytearray ( ( commandR , len_lowR , len_highR ) ) ) ) payload1 = self . _ft232h . _poll_read ( length ) self . _ft232h . _write ( str ( bytearray ( ( commandR , len_lowR , len_highR ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
2555	def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
13800	def guid ( * args ) : t = float ( time . time ( ) * 1000 ) r = float ( random . random ( ) * 10000000000000 ) a = random . random ( ) * 10000000000000 data = str ( t ) + ' ' + str ( r ) + ' ' + str ( a ) + ' ' + str ( args ) data = hashlib . md5 ( data . encode ( ) ) . hexdigest ( ) [ : 10 ] return data
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : return EnterpriseCustomer . objects . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : return None
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
9835	def __general ( self ) : while 1 : try : tok = self . __peek ( ) except DXParserNoTokens : if self . currentobject and self . currentobject not in self . objects : self . objects . append ( self . currentobject ) return if tok . iscode ( 'COMMENT' ) : self . set_parser ( 'comment' ) elif tok . iscode ( 'WORD' ) and tok . equals ( 'object' ) : self . set_parser ( 'object' ) elif self . __parser is self . __general : raise DXParseError ( 'Unknown level-1 construct at ' + str ( tok ) ) self . apply_parser ( )
5331	def get_enrich ( config , backend_section ) : TaskProjects ( config ) . execute ( ) task = TaskEnrich ( config , backend_section = backend_section ) try : task . execute ( ) logging . info ( "Loading enriched data finished!" ) except Exception as e : logging . error ( str ( e ) ) sys . exit ( - 1 )
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
11206	def gettz ( name ) : warnings . warn ( "zoneinfo.gettz() will be removed in future versions, " "to use the dateutil-provided zoneinfo files, instantiate a " "ZoneInfoFile object and use ZoneInfoFile.zones.get() " "instead. See the documentation for details." , DeprecationWarning ) if len ( _CLASS_ZONE_INSTANCE ) == 0 : _CLASS_ZONE_INSTANCE . append ( ZoneInfoFile ( getzoneinfofile_stream ( ) ) ) return _CLASS_ZONE_INSTANCE [ 0 ] . zones . get ( name )
3685	def set_from_PT ( self , Vs ) : good_roots = [ ] bad_roots = [ ] for i in Vs : j = i . real if abs ( i . imag ) > 1E-9 or j < 0 : bad_roots . append ( i ) else : good_roots . append ( j ) if len ( bad_roots ) == 2 : V = good_roots [ 0 ] self . phase = self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) if self . phase == 'l' : self . V_l = V else : self . V_g = V else : self . V_l , self . V_g = min ( good_roots ) , max ( good_roots ) [ self . set_properties_from_solution ( self . T , self . P , V , self . b , self . delta , self . epsilon , self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 ) for V in [ self . V_l , self . V_g ] ] self . phase = 'l/g'
2870	def setup ( self , pin , mode ) : self . mraa_gpio . Gpio . dir ( self . mraa_gpio . Gpio ( pin ) , self . _dir_mapping [ mode ] )
9367	def bik ( ) : return '04' + '' . join ( [ str ( random . randint ( 1 , 9 ) ) for _ in range ( 5 ) ] ) + str ( random . randint ( 0 , 49 ) + 50 )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
13701	def _before ( self ) : if request . path in self . excluded_routes : request . _tracy_exclude = True return request . _tracy_start_time = monotonic ( ) client = request . headers . get ( trace_header_client , None ) require_client = current_app . config . get ( "TRACY_REQUIRE_CLIENT" , False ) if client is None and require_client : abort ( 400 , "Missing %s header" % trace_header_client ) request . _tracy_client = client request . _tracy_id = request . headers . get ( trace_header_id , new_id ( ) )
8106	def copytree ( src , dst , symlinks = False , ignore = None ) : if not os . path . exists ( dst ) : os . makedirs ( dst ) shutil . copystat ( src , dst ) lst = os . listdir ( src ) if ignore : excl = ignore ( src , lst ) lst = [ x for x in lst if x not in excl ] for item in lst : s = os . path . join ( src , item ) d = os . path . join ( dst , item ) if symlinks and os . path . islink ( s ) : if os . path . lexists ( d ) : os . remove ( d ) os . symlink ( os . readlink ( s ) , d ) try : st = os . lstat ( s ) mode = stat . S_IMODE ( st . st_mode ) os . lchmod ( d , mode ) except : pass elif os . path . isdir ( s ) : copytree ( s , d , symlinks , ignore ) else : shutil . copy2 ( s , d )
734	def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error
10962	def get_values ( self , params ) : return util . delistify ( [ self . param_dict [ p ] for p in util . listify ( params ) ] , params )
6069	def intensity_at_radius ( self , radius ) : return self . intensity * np . exp ( - self . sersic_constant * ( ( ( radius / self . effective_radius ) ** ( 1. / self . sersic_index ) ) - 1 ) )
6531	def get_user_config ( project_path , use_cache = True ) : if sys . platform == 'win32' : user_config = os . path . expanduser ( r'~\\tidypy' ) else : user_config = os . path . join ( os . getenv ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) , 'tidypy' ) if os . path . exists ( user_config ) : with open ( user_config , 'r' ) as config_file : config = pytoml . load ( config_file ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
7197	def plot ( self , spec = "rgb" , ** kwargs ) : if self . shape [ 0 ] == 1 or ( "bands" in kwargs and len ( kwargs [ "bands" ] ) == 1 ) : if "cmap" in kwargs : cmap = kwargs [ "cmap" ] del kwargs [ "cmap" ] else : cmap = "Greys_r" self . _plot ( tfm = self . _single_band , cmap = cmap , ** kwargs ) else : if spec == "rgb" and self . _has_token ( ** kwargs ) : self . _plot ( tfm = self . rgb , ** kwargs ) else : self . _plot ( tfm = getattr ( self , spec ) , ** kwargs )
3348	def remove_members ( self , to_remove ) : if isinstance ( to_remove , string_types ) or hasattr ( to_remove , "id" ) : warn ( "need to pass in a list" ) to_remove = [ to_remove ] self . _members . difference_update ( to_remove )
12729	def axes ( self , axes ) : self . amotor . axes = [ axes [ 0 ] ] self . ode_obj . setAxis ( tuple ( axes [ 0 ] ) )
11247	def triangle_area ( point1 , point2 , point3 ) : a = point_distance ( point1 , point2 ) b = point_distance ( point1 , point3 ) c = point_distance ( point2 , point3 ) s = ( a + b + c ) / 2.0 return math . sqrt ( s * ( s - a ) * ( s - b ) * ( s - c ) )
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
8776	def start_api_and_rpc_workers ( self ) : pool = eventlet . GreenPool ( ) quark_rpc = self . serve_rpc ( ) pool . spawn ( quark_rpc . wait ) pool . waitall ( )
13545	def get_user ( self , user_id ) : url = "/2/users/%s" % user_id return self . user_from_json ( self . _get_resource ( url ) [ "user" ] )
6287	def get ( self , name ) -> Track : name = name . lower ( ) track = self . track_map . get ( name ) if not track : track = Track ( name ) self . tacks . append ( track ) self . track_map [ name ] = track return track
3329	def acquire_read ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return while True : if self . __writer is None : if self . __upgradewritercount or self . __pendingwriters : if me in self . __readers : self . __readers [ me ] += 1 return else : self . __readers [ me ] = self . __readers . get ( me , 0 ) + 1 return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : raise RuntimeError ( "Acquiring read lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
5393	def _make_environment ( self , inputs , outputs , mounts ) : env = { } env . update ( providers_util . get_file_environment_variables ( inputs ) ) env . update ( providers_util . get_file_environment_variables ( outputs ) ) env . update ( providers_util . get_file_environment_variables ( mounts ) ) return env
4766	def is_same_as ( self , other ) : if self . val is not other : self . _err ( 'Expected <%s> to be identical to <%s>, but was not.' % ( self . val , other ) ) return self
6389	def _sb_short_word ( self , term , r1_prefixes = None ) : if self . _sb_r1 ( term , r1_prefixes ) == len ( term ) and self . _sb_ends_in_short_syllable ( term ) : return True return False
12288	def datapackage_exists ( repo ) : datapath = os . path . join ( repo . rootdir , "datapackage.json" ) return os . path . exists ( datapath )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
1440	def register_metrics ( self , context ) : sys_config = system_config . get_sys_config ( ) interval = float ( sys_config [ constants . HERON_METRICS_EXPORT_INTERVAL_SEC ] ) collector = context . get_metrics_collector ( ) super ( ComponentMetrics , self ) . register_metrics ( collector , interval )
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
13751	def handle_data ( self , data ) : if data . strip ( ) : data = djeffify_string ( data ) self . djhtml += data
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
236	def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax
1481	def start_process_monitor ( self ) : Log . info ( "Start process monitor" ) while True : if len ( self . processes_to_monitor ) > 0 : ( pid , status ) = os . wait ( ) with self . process_lock : if pid in self . processes_to_monitor . keys ( ) : old_process_info = self . processes_to_monitor [ pid ] name = old_process_info . name command = old_process_info . command Log . info ( "%s (pid=%s) exited with status %d. command=%s" % ( name , pid , status , command ) ) self . _wait_process_std_out_err ( name , old_process_info . process ) if os . path . isfile ( "core.%d" % pid ) : os . system ( "chmod a+r core.%d" % pid ) if old_process_info . attempts >= self . max_runs : Log . info ( "%s exited too many times" % name ) sys . exit ( 1 ) time . sleep ( self . interval_between_runs ) p = self . _run_process ( name , command ) del self . processes_to_monitor [ pid ] self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) log_pid_for_process ( name , p . pid )
2856	def write ( self , data ) : if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x10 | ( self . lsbfirst << 3 ) | self . write_clock_ve logger . debug ( 'SPI write with command {0:2X}.' . format ( command ) ) data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) self . _deassert_cs ( )
1511	def start_heron_tools ( masters , cl_args ) : single_master = list ( masters ) [ 0 ] wait_for_master_to_start ( single_master ) cmd = "%s run %s >> /tmp/heron_tools_start.log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_heron_tools_job_file ( cl_args ) ) Log . info ( "Starting Heron Tools on %s" % single_master ) if not is_self ( single_master ) : cmd = ssh_remote_execute ( cmd , single_master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : Log . error ( "Failed to start Heron Tools on %s with error:\n%s" % ( single_master , output [ 1 ] ) ) sys . exit ( - 1 ) wait_for_job_to_start ( single_master , "heron-tools" ) Log . info ( "Done starting Heron Tools" )
13756	def copy_file ( src , dest ) : dir_path = os . path . dirname ( dest ) if not os . path . exists ( dir_path ) : os . makedirs ( dir_path ) shutil . copy2 ( src , dest )
279	def plot_monthly_returns_heatmap ( returns , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { "size" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , ** kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( "Monthly returns (%)" ) return ax
9037	def walk_connections ( self , mapping = identity ) : for start in self . walk_instructions ( ) : for stop_instruction in start . instruction . consuming_instructions : if stop_instruction is None : continue stop = self . _walk . instruction_in_grid ( stop_instruction ) connection = Connection ( start , stop ) if connection . is_visible ( ) : yield mapping ( connection )
6015	def output_positions ( positions , positions_path ) : with open ( positions_path , 'w' ) as f : for position in positions : f . write ( "%s\n" % position )
6858	def database_exists ( name , ** kwargs ) : with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = query ( "SHOW DATABASES LIKE '%(name)s';" % { 'name' : name } , ** kwargs ) return res . succeeded and ( res == name )
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
11578	def send_command ( self , command ) : send_message = "" for i in command : send_message += chr ( i ) for data in send_message : self . pymata . transport . write ( data )
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
3034	def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
7279	def play ( self ) : if not self . is_playing ( ) : self . play_pause ( ) self . _is_playing = True self . playEvent ( self )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
3963	def start_local_env ( recreate_containers ) : assembled_spec = spec_assembler . get_assembled_specs ( ) required_absent_assets = virtualbox . required_absent_assets ( assembled_spec ) if required_absent_assets : raise RuntimeError ( 'Assets {} are specified as required but are not set. Set them with `dusty assets set`' . format ( required_absent_assets ) ) docker_ip = virtualbox . get_docker_vm_ip ( ) if os . path . exists ( constants . COMPOSEFILE_PATH ) : try : stop_apps_or_services ( rm_containers = recreate_containers ) except CalledProcessError as e : log_to_client ( "WARNING: docker-compose stop failed" ) log_to_client ( str ( e ) ) daemon_warnings . clear_namespace ( 'disk' ) df_info = virtualbox . get_docker_vm_disk_info ( as_dict = True ) if 'M' in df_info [ 'free' ] or 'K' in df_info [ 'free' ] : warning_msg = 'VM is low on disk. Available disk: {}' . format ( df_info [ 'free' ] ) daemon_warnings . warn ( 'disk' , warning_msg ) log_to_client ( warning_msg ) log_to_client ( "Compiling together the assembled specs" ) active_repos = spec_assembler . get_all_repos ( active_only = True , include_specs_repo = False ) log_to_client ( "Compiling the port specs" ) port_spec = port_spec_compiler . get_port_spec_document ( assembled_spec , docker_ip ) log_to_client ( "Compiling the nginx config" ) docker_bridge_ip = virtualbox . get_docker_bridge_ip ( ) nginx_config = nginx_compiler . get_nginx_configuration_spec ( port_spec , docker_bridge_ip ) log_to_client ( "Creating setup and script bash files" ) make_up_command_files ( assembled_spec , port_spec ) log_to_client ( "Compiling docker-compose config" ) compose_config = compose_compiler . get_compose_dict ( assembled_spec , port_spec ) log_to_client ( "Saving port forwarding to hosts file" ) hosts . update_hosts_file_from_port_spec ( port_spec ) log_to_client ( "Configuring NFS" ) nfs . configure_nfs ( ) log_to_client ( "Saving updated nginx config to the VM" ) nginx . update_nginx_from_config ( nginx_config ) log_to_client ( "Saving Docker Compose config and starting all containers" ) compose . update_running_containers_from_spec ( compose_config , recreate_containers = recreate_containers ) log_to_client ( "Your local environment is now started!" )
3149	def create ( self , list_id , data ) : self . list_id = list_id if 'url' not in data : raise KeyError ( 'The list webhook must have a url' ) check_url ( data [ 'url' ] ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'webhooks' ) , data = data ) if response is not None : self . webhook_id = response [ 'id' ] else : self . webhook_id = None return response
11812	def present ( self , results ) : "Present the results as a list." for ( score , d ) in results : doc = self . documents [ d ] print ( "%5.2f|%25s | %s" % ( 100 * score , doc . url , doc . title [ : 45 ] . expandtabs ( ) ) )
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
9463	def conference_deaf ( self , call_params ) : path = '/' + self . api_version + '/ConferenceDeaf/' method = 'POST' return self . request ( path , method , call_params )
12307	def auto_get_repo ( autooptions , debug = False ) : pluginmgr = plugins_get_mgr ( ) repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
11945	def _store ( self , messages , response , * args , ** kwargs ) : contrib_messages = [ ] if self . user . is_authenticated ( ) : if not messages : self . backend . inbox_purge ( self . user ) else : for m in messages : try : self . backend . inbox_store ( [ self . user ] , m ) except MessageTypeNotSupported : contrib_messages . append ( m ) super ( StorageMixin , self ) . _store ( contrib_messages , response , * args , ** kwargs )
8620	def getServerStates ( pbclient = None , dc_id = None , serverid = None , servername = None ) : if pbclient is None : raise ValueError ( "argument 'pbclient' must not be None" ) if dc_id is None : raise ValueError ( "argument 'dc_id' must not be None" ) server = None if serverid is None : if servername is None : raise ValueError ( "one of 'serverid' or 'servername' must be specified" ) server_info = select_where ( getServerInfo ( pbclient , dc_id ) , [ 'id' , 'name' , 'state' , 'vmstate' ] , name = servername ) if len ( server_info ) > 1 : raise NameError ( "ambiguous server name '{}'" . format ( servername ) ) if len ( server_info ) == 1 : server = server_info [ 0 ] else : try : server_info = pbclient . get_server ( dc_id , serverid , 1 ) server = dict ( id = server_info [ 'id' ] , name = server_info [ 'properties' ] [ 'name' ] , state = server_info [ 'metadata' ] [ 'state' ] , vmstate = server_info [ 'properties' ] [ 'vmState' ] ) except Exception : ex = sys . exc_info ( ) [ 1 ] if ex . args [ 0 ] is not None and ex . args [ 0 ] == 404 : print ( "Server w/ ID {} not found" . format ( serverid ) ) server = None else : raise ex return server
4994	def unlink_inactive_learners ( channel_code , channel_pk ) : start = time . time ( ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Processing learners to unlink inactive users using configuration: [%s]' , integrated_channel ) integrated_channel . unlink_inactive_learners ( ) duration = time . time ( ) - start LOGGER . info ( 'Unlink inactive learners task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
1431	def getInstancePid ( topology_info , instance_id ) : try : http_client = tornado . httpclient . AsyncHTTPClient ( ) endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) url = "%s/pid/%s" % ( endpoint , instance_id ) Log . debug ( "HTTP call for url: %s" , url ) response = yield http_client . fetch ( url ) raise tornado . gen . Return ( response . body ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) )
3247	def get_users ( group , ** conn ) : group_details = get_group_api ( group [ 'GroupName' ] , ** conn ) user_list = [ ] for user in group_details . get ( 'Users' , [ ] ) : user_list . append ( user [ 'UserName' ] ) return user_list
9266	def sort_tags_by_date ( self , tags ) : if self . options . verbose : print ( "Sorting tags..." ) tags . sort ( key = lambda x : self . get_time_of_tag ( x ) ) tags . reverse ( ) return tags
8449	def not_has_branch ( branch ) : if _has_branch ( branch ) : msg = 'Cannot proceed while {} branch exists; remove and try again.' . format ( branch ) raise temple . exceptions . ExistingBranchError ( msg )
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
13516	def dimension ( self , length , draught , beam , speed , slenderness_coefficient , prismatic_coefficient ) : self . length = length self . draught = draught self . beam = beam self . speed = speed self . slenderness_coefficient = slenderness_coefficient self . prismatic_coefficient = prismatic_coefficient self . displacement = ( self . length / self . slenderness_coefficient ) ** 3 self . surface_area = 1.025 * ( 1.7 * self . length * self . draught + self . displacement / self . draught )
7363	def with_prefix ( self , prefix , strict = False ) : def decorated ( func ) : return EventHandler ( func = func , event = self . event , prefix = prefix , strict = strict ) return decorated
113	def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )
1576	def make_shell_endpoint ( topologyInfo , instance_id ) : pplan = topologyInfo [ "physical_plan" ] stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] return "http://%s:%d" % ( host , shell_port )
2206	def compressuser ( path , home = '~' ) : path = normpath ( path ) userhome_dpath = userhome ( ) if path . startswith ( userhome_dpath ) : if len ( path ) == len ( userhome_dpath ) : path = home elif path [ len ( userhome_dpath ) ] == os . path . sep : path = home + path [ len ( userhome_dpath ) : ] return path
4168	def ss2zpk ( a , b , c , d , input = 0 ) : import scipy . signal z , p , k = scipy . signal . ss2zpk ( a , b , c , d , input = input ) return z , p , k
9844	def __refill_tokenbuffer ( self ) : if len ( self . tokens ) == 0 : self . __tokenize ( self . dxfile . readline ( ) )
9644	def _flatten ( iterable ) : for i in iterable : if isinstance ( i , Iterable ) and not isinstance ( i , string_types ) : for sub_i in _flatten ( i ) : yield sub_i else : yield i
13879	def AppendToFile ( filename , contents , eol_style = EOL_STYLE_NATIVE , encoding = None , binary = False ) : _AssertIsLocal ( filename ) assert isinstance ( contents , six . text_type ) ^ binary , 'Must always receive unicode contents, unless binary=True' if not binary : contents = _HandleContentsEol ( contents , eol_style ) contents = contents . encode ( encoding or sys . getfilesystemencoding ( ) ) oss = open ( filename , 'ab' ) try : oss . write ( contents ) finally : oss . close ( )
7612	def get_clan_image ( self , obj : BaseAttrDict ) : try : badge_id = obj . clan . badge_id except AttributeError : try : badge_id = obj . badge_id except AttributeError : return 'https://i.imgur.com/Y3uXsgj.png' if badge_id is None : return 'https://i.imgur.com/Y3uXsgj.png' for i in self . constants . alliance_badges : if i . id == badge_id : return 'https://royaleapi.github.io/cr-api-assets/badges/' + i . name + '.png'
3593	def getHeaders ( self , upload_fields = False ) : if upload_fields : headers = self . deviceBuilder . getDeviceUploadHeaders ( ) else : headers = self . deviceBuilder . getBaseHeaders ( ) if self . gsfId is not None : headers [ "X-DFE-Device-Id" ] = "{0:x}" . format ( self . gsfId ) if self . authSubToken is not None : headers [ "Authorization" ] = "GoogleLogin auth=%s" % self . authSubToken if self . device_config_token is not None : headers [ "X-DFE-Device-Config-Token" ] = self . device_config_token if self . deviceCheckinConsistencyToken is not None : headers [ "X-DFE-Device-Checkin-Consistency-Token" ] = self . deviceCheckinConsistencyToken if self . dfeCookie is not None : headers [ "X-DFE-Cookie" ] = self . dfeCookie return headers
1000	def printParameters ( self ) : print "numberOfCols=" , self . numberOfCols print "cellsPerColumn=" , self . cellsPerColumn print "minThreshold=" , self . minThreshold print "newSynapseCount=" , self . newSynapseCount print "activationThreshold=" , self . activationThreshold print print "initialPerm=" , self . initialPerm print "connectedPerm=" , self . connectedPerm print "permanenceInc=" , self . permanenceInc print "permanenceDec=" , self . permanenceDec print "permanenceMax=" , self . permanenceMax print "globalDecay=" , self . globalDecay print print "doPooling=" , self . doPooling print "segUpdateValidDuration=" , self . segUpdateValidDuration print "pamLength=" , self . pamLength
961	def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( "." . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger
7872	def add_payload ( self , payload ) : if self . _payload is None : self . decode_payload ( ) if isinstance ( payload , ElementClass ) : self . _payload . append ( XMLPayload ( payload ) ) elif isinstance ( payload , StanzaPayload ) : self . _payload . append ( payload ) else : raise TypeError ( "Bad payload type" ) self . _dirty = True
5253	def assemble_one ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : try : instruction_table = instruction_tables [ fork ] asmcode = asmcode . strip ( ) . split ( ' ' ) instr = instruction_table [ asmcode [ 0 ] . upper ( ) ] if pc : instr . pc = pc if instr . operand_size > 0 : assert len ( asmcode ) == 2 instr . operand = int ( asmcode [ 1 ] , 0 ) return instr except : raise AssembleError ( "Something wrong at pc %d" % pc )
9930	def post ( self , request ) : serializer = self . get_serializer ( data = request . data ) if serializer . is_valid ( ) : serializer . save ( ) return Response ( serializer . data ) return Response ( serializer . errors , status = status . HTTP_400_BAD_REQUEST )
9826	def experiments ( ctx , metrics , declarations , independent , group , query , sort , page ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) page = page or 1 try : response = PolyaxonClient ( ) . project . list_experiments ( username = user , project_name = project_name , independent = independent , group = group , metrics = metrics , declarations = declarations , query = query , sort = sort , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get experiments for project `{}`.' . format ( project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Experiments for project `{}/{}`.' . format ( user , project_name ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No experiments found for project `{}/{}`.' . format ( user , project_name ) ) if metrics : objects = get_experiments_with_metrics ( response ) elif declarations : objects = get_experiments_with_declarations ( response ) else : objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Experiments:" ) objects . pop ( 'project_name' , None ) dict_tabulate ( objects , is_list_dict = True )
5319	def readattr ( path , name ) : try : f = open ( USB_SYS_PREFIX + path + "/" + name ) return f . readline ( ) . rstrip ( "\n" ) except IOError : return None
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
12448	def from_cookie_string ( self , cookie_string ) : for key_value in cookie_string . split ( ';' ) : if '=' in key_value : key , value = key_value . split ( '=' , 1 ) else : key = key_value strip_key = key . strip ( ) if strip_key and strip_key . lower ( ) not in COOKIE_ATTRIBUTE_NAMES : self [ strip_key ] = value . strip ( )
1882	def new_symbolic_buffer ( self , nbytes , ** options ) : label = options . get ( 'label' ) avoid_collisions = False if label is None : label = 'buffer' avoid_collisions = True taint = options . get ( 'taint' , frozenset ( ) ) expr = self . _constraints . new_array ( name = label , index_max = nbytes , value_bits = 8 , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) if options . get ( 'cstring' , False ) : for i in range ( nbytes - 1 ) : self . _constraints . add ( expr [ i ] != 0 ) return expr
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
1642	def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] match = Match ( r'^(.*[^ ({>]){' , line ) if match : leading_text = match . group ( 1 ) ( endline , endlinenum , endpos ) = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) trailing_text = '' if endpos > - 1 : trailing_text = endline [ endpos : ] for offset in xrange ( endlinenum + 1 , min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : trailing_text += clean_lines . elided [ offset ] if ( not Match ( r'^[\s}]*[{.;,)<>\]:]' , trailing_text ) and not _IsType ( clean_lines , nesting_state , leading_text ) ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before {' ) if Search ( r'}else' , line ) : error ( filename , linenum , 'whitespace/braces' , 5 , 'Missing space before else' ) if Search ( r':\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Semicolon defining empty statement. Use {} instead.' ) elif Search ( r'^\s*;\s*$' , line ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Line contains only semicolon. If this should be an empty statement, ' 'use {} instead.' ) elif ( Search ( r'\s+;\s*$' , line ) and not Search ( r'\bfor\b' , line ) ) : error ( filename , linenum , 'whitespace/semicolon' , 5 , 'Extra space before last semicolon. If this should be an empty ' 'statement, use {} instead.' )
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
12421	def dump ( obj , fp , startindex = 1 , separator = DEFAULT , index_separator = DEFAULT ) : if startindex < 0 : raise ValueError ( 'startindex must be non-negative, but was {}' . format ( startindex ) ) try : firstkey = next ( iter ( obj . keys ( ) ) ) except StopIteration : return if isinstance ( firstkey , six . text_type ) : converter = six . u else : converter = six . b default_separator = converter ( '|' ) default_index_separator = converter ( '_' ) newline = converter ( '\n' ) if separator is DEFAULT : separator = default_separator if index_separator is DEFAULT : index_separator = default_index_separator for key , value in six . iteritems ( obj ) : if isinstance ( value , ( list , tuple , set ) ) : for index , item in enumerate ( value , start = startindex ) : fp . write ( key ) fp . write ( index_separator ) fp . write ( converter ( str ( index ) ) ) fp . write ( separator ) fp . write ( item ) fp . write ( newline ) else : fp . write ( key ) fp . write ( separator ) fp . write ( value ) fp . write ( newline )
3296	def ref_url_to_path ( self , ref_url ) : return "/" + compat . unquote ( util . lstripstr ( ref_url , self . share_path ) ) . lstrip ( "/" )
12959	def _peekNextID ( self , conn = None ) : if conn is None : conn = self . _get_connection ( ) return to_unicode ( conn . get ( self . _get_next_id_key ( ) ) or 0 )
1960	def sys_rename ( self , oldnamep , newnamep ) : oldname = self . current . read_string ( oldnamep ) newname = self . current . read_string ( newnamep ) ret = 0 try : os . rename ( oldname , newname ) except OSError as e : ret = - e . errno return ret
6428	def encode ( self , word , lang = 'en' ) : if lang == 'es' : return self . _phonetic_spanish . encode ( self . _spanish_metaphone . encode ( word ) ) word = self . _soundex . encode ( self . _metaphone . encode ( word ) ) word = word [ 0 ] . translate ( self . _trans ) + word [ 1 : ] return word
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
7642	def _conversion ( target , source ) : def register ( func ) : __CONVERSION__ [ target ] [ source ] = func return func return register
13595	def confirm ( tag ) : click . echo ( ) if click . confirm ( 'Do you want to create the tag {tag}?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True , abort = True ) : git . create_tag ( tag ) if click . confirm ( 'Do you want to push the tag {tag} into the upstream?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True ) : git . push_tag ( tag ) click . echo ( 'Done!' ) else : git . delete_tag ( tag ) click . echo ( 'Aborted!' )
11276	def disown ( debug ) : pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False for line in infile : if line . find ( "ardexa.service" ) == - 1 : continue line = line . replace ( "name=" , "" ) items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" if not accounts : continue full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : return False return True
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
3300	def make_sub_element ( parent , tag , nsmap = None ) : if use_lxml : return etree . SubElement ( parent , tag , nsmap = nsmap ) return etree . SubElement ( parent , tag )
5342	def __get_dash_menu ( self , kibiter_major ) : omenu = [ ] omenu . append ( self . menu_panels_common [ 'Overview' ] ) ds_menu = self . __get_menu_entries ( kibiter_major ) kafka_menu = None community_menu = None found_kafka = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == KAFKA_NAME ] if found_kafka : kafka_menu = ds_menu . pop ( found_kafka [ 0 ] ) found_community = [ pos for pos , menu in enumerate ( ds_menu ) if menu [ 'name' ] == COMMUNITY_NAME ] if found_community : community_menu = ds_menu . pop ( found_community [ 0 ] ) ds_menu . sort ( key = operator . itemgetter ( 'name' ) ) omenu += ds_menu if kafka_menu : omenu . append ( kafka_menu ) if community_menu : omenu . append ( community_menu ) omenu . append ( self . menu_panels_common [ 'Data Status' ] ) omenu . append ( self . menu_panels_common [ 'About' ] ) logger . debug ( "Menu for panels: %s" , json . dumps ( ds_menu , indent = 4 ) ) return omenu
6269	def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
10830	def create ( cls , group , admin ) : with db . session . begin_nested ( ) : obj = cls ( group = group , admin = admin , ) db . session . add ( obj ) return obj
13233	def load ( directory_name , module_name ) : directory_name = os . path . expanduser ( directory_name ) if os . path . isdir ( directory_name ) and directory_name not in sys . path : sys . path . append ( directory_name ) try : return importlib . import_module ( module_name ) except ImportError : pass
11801	def restore ( self , removals ) : "Undo a supposition and all inferences from it." for B , b in removals : self . curr_domains [ B ] . append ( b )
10149	def _ref ( self , resp , base_name = None ) : name = base_name or resp . get ( 'title' , '' ) or resp . get ( 'name' , '' ) pointer = self . json_pointer + name self . response_registry [ name ] = resp return { '$ref' : pointer }
1360	def get_argument_starttime ( self ) : try : starttime = self . get_argument ( constants . PARAM_STARTTIME ) return starttime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
9246	def compound_changelog ( self ) : self . fetch_and_filter_tags ( ) tags_sorted = self . sort_tags_by_date ( self . filtered_tags ) self . filtered_tags = tags_sorted self . fetch_and_filter_issues_and_pr ( ) log = str ( self . options . frontmatter ) if self . options . frontmatter else u"" log += u"{0}\n\n" . format ( self . options . header ) if self . options . unreleased_only : log += self . generate_unreleased_section ( ) else : log += self . generate_log_for_all_tags ( ) try : with open ( self . options . base ) as fh : log += fh . read ( ) except ( TypeError , IOError ) : pass return log
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
4640	def find_next ( self ) : if int ( self . num_retries ) < 0 : self . _cnt_retries += 1 sleeptime = ( self . _cnt_retries - 1 ) * 2 if self . _cnt_retries < 10 else 10 if sleeptime : log . warning ( "Lost connection to node during rpcexec(): %s (%d/%d) " % ( self . url , self . _cnt_retries , self . num_retries ) + "Retrying in %d seconds" % sleeptime ) sleep ( sleeptime ) return next ( self . urls ) urls = [ k for k , v in self . _url_counter . items ( ) if ( int ( self . num_retries ) >= 0 and v <= self . num_retries and ( k != self . url or len ( self . _url_counter ) == 1 ) ) ] if not len ( urls ) : raise NumRetriesReached url = urls [ 0 ] return url
11037	def maybe_key ( pem_path ) : acme_key_file = pem_path . child ( u'client.key' ) if acme_key_file . exists ( ) : key = _load_pem_private_key_bytes ( acme_key_file . getContent ( ) ) else : key = generate_private_key ( u'rsa' ) acme_key_file . setContent ( _dump_pem_private_key_bytes ( key ) ) return succeed ( JWKRSA ( key = key ) )
685	def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w
1734	def remove_objects ( code , count = 1 ) : replacements = { } br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) if is_object ( n , last ) : res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e else : n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : res += e last = e return res , replacements , count
1133	def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]
12958	def _compat_rem_str_id_from_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _compat_get_str_key_for_index ( indexedField , val ) , pk )
7611	def get_top_clans ( self , location_id = 'global' , ** params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , ** params )
5241	def market_normal ( self , session , after_open , before_close ) -> Session : logger = logs . get_logger ( self . market_normal ) if session not in self . exch : return SessNA ss = self . exch [ session ] s_time = shift_time ( ss [ 0 ] , int ( after_open ) + 1 ) e_time = shift_time ( ss [ - 1 ] , - int ( before_close ) ) request_cross = pd . Timestamp ( s_time ) >= pd . Timestamp ( e_time ) session_cross = pd . Timestamp ( ss [ 0 ] ) >= pd . Timestamp ( ss [ 1 ] ) if request_cross and ( not session_cross ) : logger . warning ( f'end time {e_time} is earlier than {s_time} ...' ) return SessNA return Session ( s_time , e_time )
3256	def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
4552	def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
13736	def get_param_values ( request , model = None ) : if type ( request ) == dict : return request params = get_payload ( request ) try : del params [ 'pk' ] params [ params . pop ( 'name' ) ] = params . pop ( 'value' ) except KeyError : pass return { k . rstrip ( '[]' ) : safe_eval ( v ) if not type ( v ) == list else [ safe_eval ( sv ) for sv in v ] for k , v in params . items ( ) }
11436	def _record_sort_by_indicators ( record ) : for tag , fields in record . items ( ) : record [ tag ] = _fields_sort_by_indicators ( fields )
1967	def sched ( self ) : if len ( self . procs ) > 1 : logger . debug ( "SCHED:" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) logger . debug ( f"\tCurrent clock: {self.clocks}" ) logger . debug ( f"\tCurrent cpu: {self._current}" ) if len ( self . running ) == 0 : logger . debug ( "None running checking if there is some process waiting for a timeout" ) if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . clocks = min ( x for x in self . timers if x is not None ) + 1 self . check_timers ( ) assert len ( self . running ) != 0 , "DEADLOCK!" self . _current = self . running [ 0 ] return next_index = ( self . running . index ( self . _current ) + 1 ) % len ( self . running ) next_running_idx = self . running [ next_index ] if len ( self . procs ) > 1 : logger . debug ( f"\tTransfer control from process {self._current} to {next_running_idx}" ) self . _current = next_running_idx
1125	def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule
7023	def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes = base64 . b64decode ( b64str ) if writetostrio : outobj = StrIO ( filebytes ) return outobj else : with open ( outfpath , 'wb' ) as outfd : outfd . write ( filebytes ) if os . path . exists ( outfpath ) : return outfpath else : LOGERROR ( 'could not write output file: %s' % outfpath ) return None except Exception as e : LOGEXCEPTION ( 'failed while trying to convert ' 'b64 string to file %s' % outfpath ) return None
2838	def setup ( self , pin , value ) : self . _validate_pin ( pin ) if value == GPIO . IN : self . iodir [ int ( pin / 8 ) ] |= 1 << ( int ( pin % 8 ) ) elif value == GPIO . OUT : self . iodir [ int ( pin / 8 ) ] &= ~ ( 1 << ( int ( pin % 8 ) ) ) else : raise ValueError ( 'Unexpected value. Must be GPIO.IN or GPIO.OUT.' ) self . write_iodir ( )
11927	def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
3931	def _auth_with_refresh_token ( session , refresh_token ) : token_request_data = { 'client_id' : OAUTH2_CLIENT_ID , 'client_secret' : OAUTH2_CLIENT_SECRET , 'grant_type' : 'refresh_token' , 'refresh_token' : refresh_token , } res = _make_token_request ( session , token_request_data ) return res [ 'access_token' ]
6569	def signature_matches ( func , args = ( ) , kwargs = { } ) : try : sig = inspect . signature ( func ) sig . bind ( * args , ** kwargs ) except TypeError : return False else : return True
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
8298	def readLong ( data ) : high , low = struct . unpack ( ">ll" , data [ 0 : 8 ] ) big = ( long ( high ) << 32 ) + low rest = data [ 8 : ] return ( big , rest )
6026	def geometry_from_grid ( self , grid , pixel_centres , pixel_neighbors , pixel_neighbors_size , buffer = 1e-8 ) : y_min = np . min ( grid [ : , 0 ] ) - buffer y_max = np . max ( grid [ : , 0 ] ) + buffer x_min = np . min ( grid [ : , 1 ] ) - buffer x_max = np . max ( grid [ : , 1 ] ) + buffer shape_arcsec = ( y_max - y_min , x_max - x_min ) origin = ( ( y_max + y_min ) / 2.0 , ( x_max + x_min ) / 2.0 ) return self . Geometry ( shape_arcsec = shape_arcsec , pixel_centres = pixel_centres , origin = origin , pixel_neighbors = pixel_neighbors , pixel_neighbors_size = pixel_neighbors_size )
4024	def _get_host_only_mac_address ( ) : vm_config = _get_vm_config ( ) for line in vm_config : if line . startswith ( 'hostonlyadapter' ) : adapter_number = int ( line [ 15 : 16 ] ) break else : raise ValueError ( 'No host-only adapter is defined for the Dusty VM' ) for line in vm_config : if line . startswith ( 'macaddress{}' . format ( adapter_number ) ) : return line . split ( '=' ) [ 1 ] . strip ( '"' ) . lower ( ) raise ValueError ( 'Could not find MAC address for adapter number {}' . format ( adapter_number ) )
7932	def connect ( self , server = None , port = None ) : self . lock . acquire ( ) try : self . _connect ( server , port ) finally : self . lock . release ( )
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) s = self . styles . default s . graph_background ( s ) _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) _ctx . pop ( )
6964	def get ( self ) : project_checkplots = self . currentproject [ 'checkplots' ] project_checkplotbasenames = [ os . path . basename ( x ) for x in project_checkplots ] project_checkplotindices = range ( len ( project_checkplots ) ) project_cpsortkey = self . currentproject [ 'sortkey' ] if self . currentproject [ 'sortorder' ] == 'asc' : project_cpsortorder = 'ascending' elif self . currentproject [ 'sortorder' ] == 'desc' : project_cpsortorder = 'descending' project_cpfilterstatements = self . currentproject [ 'filterstatements' ] self . render ( 'cpindex.html' , project_checkplots = project_checkplots , project_cpsortorder = project_cpsortorder , project_cpsortkey = project_cpsortkey , project_cpfilterstatements = project_cpfilterstatements , project_checkplotbasenames = project_checkplotbasenames , project_checkplotindices = project_checkplotindices , project_checkplotfile = self . cplistfile , readonly = self . readonly , baseurl = self . baseurl )
10186	def _aggregations_config ( self ) : result = { } for ep in iter_entry_points ( group = self . entry_point_group_aggs ) : for cfg in ep . load ( ) ( ) : if cfg [ 'aggregation_name' ] not in self . enabled_aggregations : continue elif cfg [ 'aggregation_name' ] in result : raise DuplicateAggregationError ( 'Duplicate aggregation {0} in entry point ' '{1}' . format ( cfg [ 'event_type' ] , ep . name ) ) cfg . update ( self . enabled_aggregations [ cfg [ 'aggregation_name' ] ] or { } ) result [ cfg [ 'aggregation_name' ] ] = cfg return result
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
13812	def FindMethodByName ( self , name ) : for method in self . methods : if name == method . name : return method return None
9199	def reversals ( series , left = False , right = False ) : series = iter ( series ) x_last , x = next ( series ) , next ( series ) d_last = ( x - x_last ) if left : yield x_last for x_next in series : if x_next == x : continue d_next = x_next - x if d_last * d_next < 0 : yield x x_last , x = x , x_next d_last = d_next if right : yield x_next
11077	def send_message ( self , channel , text ) : if isinstance ( channel , SlackIM ) or isinstance ( channel , SlackUser ) : self . _bot . send_im ( channel , text ) elif isinstance ( channel , SlackRoom ) : self . _bot . send_message ( channel , text ) elif isinstance ( channel , basestring ) : if channel [ 0 ] == '@' : self . _bot . send_im ( channel [ 1 : ] , text ) elif channel [ 0 ] == '#' : self . _bot . send_message ( channel [ 1 : ] , text ) else : self . _bot . send_message ( channel , text ) else : self . _bot . send_message ( channel , text )
3052	def step1_get_authorize_url ( self , redirect_uri = None , state = None ) : if redirect_uri is not None : logger . warning ( ( 'The redirect_uri parameter for ' 'OAuth2WebServerFlow.step1_get_authorize_url is deprecated. ' 'Please move to passing the redirect_uri in via the ' 'constructor.' ) ) self . redirect_uri = redirect_uri if self . redirect_uri is None : raise ValueError ( 'The value of redirect_uri must not be None.' ) query_params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , 'scope' : self . scope , } if state is not None : query_params [ 'state' ] = state if self . login_hint is not None : query_params [ 'login_hint' ] = self . login_hint if self . _pkce : if not self . code_verifier : self . code_verifier = _pkce . code_verifier ( ) challenge = _pkce . code_challenge ( self . code_verifier ) query_params [ 'code_challenge' ] = challenge query_params [ 'code_challenge_method' ] = 'S256' query_params . update ( self . params ) return _helpers . update_query_params ( self . auth_uri , query_params )
4986	def get_path_variables ( ** kwargs ) : enterprise_customer_uuid = kwargs . get ( 'enterprise_uuid' , '' ) course_run_id = kwargs . get ( 'course_id' , '' ) course_key = kwargs . get ( 'course_key' , '' ) program_uuid = kwargs . get ( 'program_uuid' , '' ) return enterprise_customer_uuid , course_run_id , course_key , program_uuid
198	def Snowflakes ( density = ( 0.005 , 0.075 ) , density_uniformity = ( 0.3 , 0.9 ) , flake_size = ( 0.2 , 0.7 ) , flake_size_uniformity = ( 0.4 , 0.8 ) , angle = ( - 30 , 30 ) , speed = ( 0.007 , 0.03 ) , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) layer = SnowflakesLayer ( density = density , density_uniformity = density_uniformity , flake_size = flake_size , flake_size_uniformity = flake_size_uniformity , angle = angle , speed = speed , blur_sigma_fraction = ( 0.0001 , 0.001 ) ) return meta . SomeOf ( ( 1 , 3 ) , children = [ layer . deepcopy ( ) for _ in range ( 3 ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
12022	def check_parent_boundary ( self ) : for line in self . lines : for parent_feature in line [ 'parents' ] : ok = False for parent_line in parent_feature : if parent_line [ 'start' ] <= line [ 'start' ] and line [ 'end' ] <= parent_line [ 'end' ] : ok = True break if not ok : self . add_line_error ( line , { 'message' : 'This feature is not contained within the feature boundaries of parent: {0:s}: {1:s}' . format ( parent_feature [ 0 ] [ 'attributes' ] [ 'ID' ] , ',' . join ( [ '({0:s}, {1:d}, {2:d})' . format ( line [ 'seqid' ] , line [ 'start' ] , line [ 'end' ] ) for line in parent_feature ] ) ) , 'error_type' : 'BOUNDS' , 'location' : 'parent_boundary' } )
13427	def delete_messages ( self , messages ) : url = "/2/messages/?%s" % urlencode ( [ ( 'ids' , "," . join ( messages ) ) ] ) data = self . _delete_resource ( url ) return data
12916	def prune ( self , regex = r".*" ) : return filetree ( self . root , ignore = self . ignore , regex = regex )
10397	def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
11820	def as_dict ( self , default = None ) : settings = SettingDict ( queryset = self , default = default ) return settings
7035	def submit_post_searchquery ( url , data , apikey ) : postdata = { } for key in data : if key == 'columns' : postdata [ 'columns[]' ] = data [ key ] elif key == 'collections' : postdata [ 'collections[]' ] = data [ key ] else : postdata [ key ] = data [ key ] encoded_postdata = urlencode ( postdata , doseq = True ) . encode ( ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } LOGINFO ( 'submitting search query to LCC-Server API URL: %s' % url ) try : req = Request ( url , data = encoded_postdata , headers = headers ) resp = urlopen ( req ) if resp . code == 200 : for line in resp : data = json . loads ( line ) msg = data [ 'message' ] status = data [ 'status' ] if status != 'failed' : LOGINFO ( 'status: %s, %s' % ( status , msg ) ) else : LOGERROR ( 'status: %s, %s' % ( status , msg ) ) if status in ( 'ok' , 'background' ) : setid = data [ 'result' ] [ 'setid' ] outpickle = os . path . join ( os . path . expanduser ( '~' ) , '.astrobase' , 'lccs' , 'query-%s.pkl' % setid ) if not os . path . exists ( os . path . dirname ( outpickle ) ) : os . makedirs ( os . path . dirname ( outpickle ) ) with open ( outpickle , 'wb' ) as outfd : pickle . dump ( data , outfd , pickle . HIGHEST_PROTOCOL ) LOGINFO ( 'saved query info to %s, use this to ' 'download results later with ' 'retrieve_dataset_files' % outpickle ) return status , data , data [ 'result' ] [ 'setid' ] elif status == 'failed' : return status , data , None else : try : data = json . load ( resp ) msg = data [ 'message' ] LOGERROR ( msg ) return 'failed' , None , None except Exception as e : LOGEXCEPTION ( 'failed to submit query to %s' % url ) return 'failed' , None , None except HTTPError as e : LOGERROR ( 'could not submit query to LCC API at: %s' % url ) LOGERROR ( 'HTTP status code was %s, reason: %s' % ( e . code , e . reason ) ) return 'failed' , None , None
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
6967	def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaussian1DKernel ( windowfwhm , x_size = windowsize ) smoothed = convolve ( mags , convkernel , boundary = 'extend' ) return smoothed
11751	def get_blueprint_routes ( app , base_path ) : routes = [ ] for child in app . url_map . iter_rules ( ) : if child . rule . startswith ( base_path ) : relative_path = child . rule [ len ( base_path ) : ] routes . append ( { 'path' : relative_path , 'endpoint' : child . endpoint , 'methods' : list ( child . methods ) } ) return routes
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
12702	def _get_params ( target , param , dof ) : return [ target . getParam ( getattr ( ode , 'Param{}{}' . format ( param , s ) ) ) for s in [ '' , '2' , '3' ] [ : dof ] ]
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
1751	def scan_mem ( self , data_to_find ) : if isinstance ( data_to_find , bytes ) : data_to_find = [ bytes ( [ c ] ) for c in data_to_find ] for mapping in sorted ( self . maps ) : for ptr in mapping : if ptr + len ( data_to_find ) >= mapping . end : break candidate = mapping [ ptr : ptr + len ( data_to_find ) ] if issymbolic ( candidate [ 0 ] ) : break if candidate == data_to_find : yield ptr
11674	def copy ( self , stack = False , copy_meta = False , memo = None ) : if self . stacked : fs = deepcopy ( self . stacked_features , memo ) n_pts = self . n_pts . copy ( ) elif stack : fs = np . vstack ( self . features ) n_pts = self . n_pts . copy ( ) else : fs = deepcopy ( self . features , memo ) n_pts = None meta = deepcopy ( self . meta , memo ) if copy_meta else self . meta return Features ( fs , n_pts , copy = False , ** meta )
9413	def _setup_log ( ) : try : handler = logging . StreamHandler ( stream = sys . stdout ) except TypeError : handler = logging . StreamHandler ( strm = sys . stdout ) log = get_log ( ) log . addHandler ( handler ) log . setLevel ( logging . INFO ) log . propagate = False
11917	def render ( template , ** data ) : try : return renderer . render ( template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
5773	def dsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'dsa' : raise ValueError ( 'The key specified is not a DSA private key' ) return _sign ( private_key , data , hash_algorithm )
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
10661	def convert_compound ( mass , source , target , element ) : target_mass_fraction = element_mass_fraction ( target , element ) if target_mass_fraction == 0.0 : return 0.0 else : source_mass_fraction = element_mass_fraction ( source , element ) return mass * source_mass_fraction / target_mass_fraction
12877	def many_until ( these , term ) : results = [ ] while True : stop , result = choice ( _tag ( True , term ) , _tag ( False , these ) ) if stop : return results , result else : results . append ( result )
3865	async def leave ( self ) : is_group_conversation = ( self . _conversation . type == hangouts_pb2 . CONVERSATION_TYPE_GROUP ) try : if is_group_conversation : await self . _client . remove_user ( hangouts_pb2 . RemoveUserRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , ) ) else : await self . _client . delete_conversation ( hangouts_pb2 . DeleteConversationRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , delete_upper_bound_timestamp = parsers . to_timestamp ( datetime . datetime . now ( tz = datetime . timezone . utc ) ) ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to leave conversation: {}' . format ( e ) ) raise
6603	def package_fullpath ( self , package_index ) : ret = os . path . join ( self . path , self . package_relpath ( package_index ) ) return ret
397	def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( "Please give a unique name to tl.cost.cross_entropy for TF1.0+" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
10981	def accept ( group_id ) : membership = Membership . query . get_or_404 ( ( current_user . get_id ( ) , group_id ) ) try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) ) flash ( _ ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) )
7306	def process_document ( self , document , form_key , passed_key ) : if passed_key is not None : current_key , remaining_key_array = trim_field_key ( document , passed_key ) else : current_key , remaining_key_array = trim_field_key ( document , form_key ) key_array_digit = remaining_key_array [ - 1 ] if remaining_key_array and has_digit ( remaining_key_array ) else None remaining_key = make_key ( remaining_key_array ) if current_key . lower ( ) == 'id' : raise KeyError ( u"Mongonaut does not work with models which have fields beginning with id_" ) is_embedded_doc = ( isinstance ( document . _fields . get ( current_key , None ) , EmbeddedDocumentField ) if hasattr ( document , '_fields' ) else False ) is_list = not key_array_digit is None key_in_fields = current_key in document . _fields . keys ( ) if hasattr ( document , '_fields' ) else False if key_in_fields : if is_embedded_doc : self . set_embedded_doc ( document , form_key , current_key , remaining_key ) elif is_list : self . set_list_field ( document , form_key , current_key , remaining_key , key_array_digit ) else : value = translate_value ( document . _fields [ current_key ] , self . form . cleaned_data [ form_key ] ) setattr ( document , current_key , value )
4993	def transmit_learner_data ( username , channel_code , channel_pk ) : start = time . time ( ) api_user = User . objects . get ( username = username ) integrated_channel = INTEGRATED_CHANNEL_CHOICES [ channel_code ] . objects . get ( pk = channel_pk ) LOGGER . info ( 'Processing learners for integrated channel using configuration: [%s]' , integrated_channel ) integrated_channel . transmit_learner_data ( api_user ) duration = time . time ( ) - start LOGGER . info ( 'Learner data transmission task for integrated channel configuration [%s] took [%s] seconds' , integrated_channel , duration )
9877	def _ratio_metric ( v1 , v2 , ** _kwargs ) : return ( ( ( v1 - v2 ) / ( v1 + v2 ) ) ** 2 ) if v1 + v2 != 0 else 0
6459	def _has_vowel ( self , term ) : for letter in term : if letter in self . _vowels : return True return False
3258	def get_resources ( self , names = None , stores = None , workspaces = None ) : stores = self . get_stores ( names = stores , workspaces = workspaces ) resources = [ ] for s in stores : try : resources . extend ( s . get_resources ( ) ) except FailedRequestError : continue if names is None : names = [ ] elif isinstance ( names , basestring ) : names = [ s . strip ( ) for s in names . split ( ',' ) if s . strip ( ) ] if resources and names : return ( [ resource for resource in resources if resource . name in names ] ) return resources
8981	def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
7368	async def read ( response , loads = loads , encoding = None ) : ctype = response . headers . get ( 'Content-Type' , "" ) . lower ( ) try : if "application/json" in ctype : logger . info ( "decoding data as json" ) return await response . json ( encoding = encoding , loads = loads ) if "text" in ctype : logger . info ( "decoding data as text" ) return await response . text ( encoding = encoding ) except ( UnicodeDecodeError , json . JSONDecodeError ) as exc : data = await response . read ( ) raise exceptions . PeonyDecodeError ( response = response , data = data , exception = exc ) return await response . read ( )
8956	def compile_glob ( spec ) : parsed = "" . join ( parse_glob ( spec ) ) regex = "^{0}$" . format ( parsed ) return re . compile ( regex )
12613	def is_unique ( self , table_name , sample , unique_fields = None ) : try : eid = find_unique ( self . table ( table_name ) , sample = sample , unique_fields = unique_fields ) except : return False else : return eid is not None
11277	def run_program ( prog_list , debug , shell ) : try : if not shell : process = Popen ( prog_list , stdout = PIPE , stderr = PIPE ) stdout , stderr = process . communicate ( ) retcode = process . returncode if debug >= 1 : print ( "Program : " , " " . join ( prog_list ) ) print ( "Return Code: " , retcode ) print ( "Stdout: " , stdout ) print ( "Stderr: " , stderr ) return bool ( retcode ) else : command = " " . join ( prog_list ) os . system ( command ) return True except : return False
4589	def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
6095	def voronoi_regular_to_pix_from_grids_and_geometry ( regular_grid , regular_to_nearest_pix , pixel_centres , pixel_neighbors , pixel_neighbors_size ) : regular_to_pix = np . zeros ( ( regular_grid . shape [ 0 ] ) ) for regular_index in range ( regular_grid . shape [ 0 ] ) : nearest_pix_pixel_index = regular_to_nearest_pix [ regular_index ] while True : nearest_pix_pixel_center = pixel_centres [ nearest_pix_pixel_index ] sub_to_nearest_pix_distance = ( regular_grid [ regular_index , 0 ] - nearest_pix_pixel_center [ 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - nearest_pix_pixel_center [ 1 ] ) ** 2 closest_separation_from_pix_neighbor = 1.0e8 for neighbor_index in range ( pixel_neighbors_size [ nearest_pix_pixel_index ] ) : neighbor = pixel_neighbors [ nearest_pix_pixel_index , neighbor_index ] separation_from_neighbor = ( regular_grid [ regular_index , 0 ] - pixel_centres [ neighbor , 0 ] ) ** 2 + ( regular_grid [ regular_index , 1 ] - pixel_centres [ neighbor , 1 ] ) ** 2 if separation_from_neighbor < closest_separation_from_pix_neighbor : closest_separation_from_pix_neighbor = separation_from_neighbor closest_neighbor_index = neighbor_index neighboring_pix_pixel_index = pixel_neighbors [ nearest_pix_pixel_index , closest_neighbor_index ] sub_to_neighboring_pix_distance = closest_separation_from_pix_neighbor if sub_to_nearest_pix_distance <= sub_to_neighboring_pix_distance : regular_to_pix [ regular_index ] = nearest_pix_pixel_index break else : nearest_pix_pixel_index = neighboring_pix_pixel_index return regular_to_pix
2833	def stop ( self , pin ) : if pin not in self . pwm : raise ValueError ( 'Pin {0} is not configured as a PWM. Make sure to first call start for the pin.' . format ( pin ) ) self . pwm [ pin ] . stop ( ) del self . pwm [ pin ]
1467	def process ( self , tup ) : curtime = int ( time . time ( ) ) self . current_tuples . append ( ( tup , curtime ) ) self . _expire ( curtime )
9668	def _make_package ( args ) : from lingpy . sequence . sound_classes import token2class from lingpy . data import Model columns = [ 'LATEX' , 'FEATURES' , 'SOUND' , 'IMAGE' , 'COUNT' , 'NOTE' ] bipa = TranscriptionSystem ( 'bipa' ) for src , rows in args . repos . iter_sources ( type = 'td' ) : args . log . info ( 'TranscriptionData {0} ...' . format ( src [ 'NAME' ] ) ) uritemplate = URITemplate ( src [ 'URITEMPLATE' ] ) if src [ 'URITEMPLATE' ] else None out = [ [ 'BIPA_GRAPHEME' , 'CLTS_NAME' , 'GENERATED' , 'EXPLICIT' , 'GRAPHEME' , 'URL' ] + columns ] graphemes = set ( ) for row in rows : if row [ 'GRAPHEME' ] in graphemes : args . log . warn ( 'skipping duplicate grapheme: {0}' . format ( row [ 'GRAPHEME' ] ) ) continue graphemes . add ( row [ 'GRAPHEME' ] ) if not row [ 'BIPA' ] : bipa_sound = bipa [ row [ 'GRAPHEME' ] ] explicit = '' else : bipa_sound = bipa [ row [ 'BIPA' ] ] explicit = '+' generated = '+' if bipa_sound . generated else '' if is_valid_sound ( bipa_sound , bipa ) : bipa_grapheme = bipa_sound . s bipa_name = bipa_sound . name else : bipa_grapheme , bipa_name = '<NA>' , '<NA>' url = uritemplate . expand ( ** row ) if uritemplate else row . get ( 'URL' , '' ) out . append ( [ bipa_grapheme , bipa_name , generated , explicit , row [ 'GRAPHEME' ] , url ] + [ row . get ( c , '' ) for c in columns ] ) found = len ( [ o for o in out if o [ 0 ] != '<NA>' ] ) args . log . info ( '... {0} of {1} graphemes found ({2:.0f}%)' . format ( found , len ( out ) , found / len ( out ) * 100 ) ) with UnicodeWriter ( pkg_path ( 'transcriptiondata' , '{0}.tsv' . format ( src [ 'NAME' ] ) ) , delimiter = '\t' ) as writer : writer . writerows ( out ) count = 0 with UnicodeWriter ( pkg_path ( 'soundclasses' , 'lingpy.tsv' ) , delimiter = '\t' ) as writer : writer . writerow ( [ 'CLTS_NAME' , 'BIPA_GRAPHEME' ] + SOUNDCLASS_SYSTEMS ) for grapheme , sound in sorted ( bipa . sounds . items ( ) ) : if not sound . alias : writer . writerow ( [ sound . name , grapheme ] + [ token2class ( grapheme , Model ( cls ) ) for cls in SOUNDCLASS_SYSTEMS ] ) count += 1 args . log . info ( 'SoundClasses: {0} written to file.' . format ( count ) )
9447	def hangup_all_calls ( self ) : path = '/' + self . api_version + '/HangupAllCalls/' method = 'POST' return self . request ( path , method )
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
12644	def cert_info ( ) : sec_type = security_type ( ) if sec_type == 'pem' : return get_config_value ( 'pem_path' , fallback = None ) if sec_type == 'cert' : cert_path = get_config_value ( 'cert_path' , fallback = None ) key_path = get_config_value ( 'key_path' , fallback = None ) return cert_path , key_path return None
7357	def _check_peptide_lengths ( self , peptide_lengths = None ) : if not peptide_lengths : peptide_lengths = self . default_peptide_lengths if not peptide_lengths : raise ValueError ( ( "Must either provide 'peptide_lengths' argument " "or set 'default_peptide_lengths" ) ) if isinstance ( peptide_lengths , int ) : peptide_lengths = [ peptide_lengths ] require_iterable_of ( peptide_lengths , int ) for peptide_length in peptide_lengths : if ( self . min_peptide_length is not None and peptide_length < self . min_peptide_length ) : raise ValueError ( "Invalid peptide length %d, shorter than min %d" % ( peptide_length , self . min_peptide_length ) ) elif ( self . max_peptide_length is not None and peptide_length > self . max_peptide_length ) : raise ValueError ( "Invalid peptide length %d, longer than max %d" % ( peptide_length , self . max_peptide_length ) ) return peptide_lengths
7155	def raw ( prompt , * args , ** kwargs ) : go_back = kwargs . get ( 'go_back' , '<' ) type_ = kwargs . get ( 'type' , str ) default = kwargs . get ( 'default' , '' ) with stdout_redirected ( sys . stderr ) : while True : try : if kwargs . get ( 'secret' , False ) : answer = getpass . getpass ( prompt ) elif sys . version_info < ( 3 , 0 ) : answer = raw_input ( prompt ) else : answer = input ( prompt ) if not answer : answer = default if answer == go_back : raise QuestionnaireGoBack return type_ ( answer ) except ValueError : eprint ( '\n`{}` is not a valid `{}`\n' . format ( answer , type_ ) )
2019	def SMOD ( self , a , b ) : s0 , s1 = to_signed ( a ) , to_signed ( b ) sign = Operators . ITEBV ( 256 , s0 < 0 , - 1 , 1 ) try : result = ( Operators . ABS ( s0 ) % Operators . ABS ( s1 ) ) * sign except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , s1 == 0 , 0 , result )
7643	def convert ( annotation , target_namespace ) : annotation . validate ( strict = True ) if annotation . namespace == target_namespace : return annotation if target_namespace in __CONVERSION__ : annotation = deepcopy ( annotation ) for source in __CONVERSION__ [ target_namespace ] : if annotation . search ( namespace = source ) : return __CONVERSION__ [ target_namespace ] [ source ] ( annotation ) raise NamespaceError ( 'Unable to convert annotation from namespace=' '"{0}" to "{1}"' . format ( annotation . namespace , target_namespace ) )
6639	def runScript ( self , scriptname , additional_environment = None ) : import subprocess import shlex command = self . getScript ( scriptname ) if command is None : logger . debug ( '%s has no script %s' , self , scriptname ) return 0 if not len ( command ) : logger . error ( "script %s of %s is empty" , scriptname , self . getName ( ) ) return 1 env = os . environ . copy ( ) if additional_environment is not None : env . update ( additional_environment ) errcode = 0 child = None try : logger . debug ( 'running script: %s' , command ) child = subprocess . Popen ( command , cwd = self . path , env = env ) child . wait ( ) if child . returncode : logger . error ( "script %s (from %s) exited with non-zero status %s" , scriptname , self . getName ( ) , child . returncode ) errcode = child . returncode child = None finally : if child is not None : tryTerminate ( child ) return errcode
10320	def _microcanonical_average_moments ( moments , alpha ) : ret = dict ( ) runs = moments . shape [ 0 ] sqrt_n = np . sqrt ( runs ) moments_sample_mean = moments . mean ( axis = 0 ) ret [ 'moments' ] = moments_sample_mean moments_sample_std = moments . std ( axis = 0 , ddof = 1 ) ret [ 'moments_ci' ] = np . empty ( ( 5 , 2 ) ) for k in range ( 5 ) : if moments_sample_std [ k ] : old_settings = np . seterr ( all = 'raise' ) ret [ 'moments_ci' ] [ k ] = scipy . stats . t . interval ( 1 - alpha , df = runs - 1 , loc = moments_sample_mean [ k ] , scale = moments_sample_std [ k ] / sqrt_n ) np . seterr ( ** old_settings ) else : ret [ 'moments_ci' ] [ k ] = ( moments_sample_mean [ k ] * np . ones ( 2 ) ) return ret
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
9667	def itertable ( table ) : for item in table : res = { k . lower ( ) : nfd ( v ) if isinstance ( v , text_type ) else v for k , v in item . items ( ) } for extra in res . pop ( 'extra' , [ ] ) : k , _ , v = extra . partition ( ':' ) res [ k . strip ( ) ] = v . strip ( ) yield res
4797	def is_before ( self , other ) : if type ( self . val ) is not datetime . datetime : raise TypeError ( 'val must be datetime, but was type <%s>' % type ( self . val ) . __name__ ) if type ( other ) is not datetime . datetime : raise TypeError ( 'given arg must be datetime, but was type <%s>' % type ( other ) . __name__ ) if self . val >= other : self . _err ( 'Expected <%s> to be before <%s>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) ) ) return self
11791	def mrv ( assignment , csp ) : "Minimum-remaining-values heuristic." return argmin_random_tie ( [ v for v in csp . vars if v not in assignment ] , lambda var : num_legal_values ( csp , var , assignment ) )
2934	def merge_option_and_config_str ( cls , option_name , config , options ) : opt = getattr ( options , option_name , None ) if opt : config . set ( CONFIG_SECTION_NAME , option_name , opt ) elif config . has_option ( CONFIG_SECTION_NAME , option_name ) : setattr ( options , option_name , config . get ( CONFIG_SECTION_NAME , option_name ) )
4066	def item_fields ( self ) : if self . templates . get ( "item_fields" ) and not self . _updated ( "/itemFields" , self . templates [ "item_fields" ] , "item_fields" ) : return self . templates [ "item_fields" ] [ "tmplt" ] query_string = "/itemFields" retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , "item_fields" )
1933	def function_signature_for_name_and_inputs ( name : str , inputs : Sequence [ Mapping [ str , Any ] ] ) -> str : return name + SolidityMetadata . tuple_signature_for_components ( inputs )
5872	def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
11499	def get_community_by_id ( self , community_id , token = None ) : parameters = dict ( ) parameters [ 'id' ] = community_id if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
7841	def get_category ( self ) : var = self . xmlnode . prop ( "category" ) if not var : var = "?" return var . decode ( "utf-8" )
13354	def _pipepager ( text , cmd , color ) : import subprocess env = dict ( os . environ ) cmd_detail = cmd . rsplit ( '/' , 1 ) [ - 1 ] . split ( ) if color is None and cmd_detail [ 0 ] == 'less' : less_flags = os . environ . get ( 'LESS' , '' ) + ' ' . join ( cmd_detail [ 1 : ] ) if not less_flags : env [ 'LESS' ] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags : color = True if not color : text = strip_ansi ( text ) c = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , env = env ) encoding = get_best_encoding ( c . stdin ) try : c . stdin . write ( text . encode ( encoding , 'replace' ) ) c . stdin . close ( ) except ( IOError , KeyboardInterrupt ) : pass while True : try : c . wait ( ) except KeyboardInterrupt : pass else : break
11700	def spawn ( self , generations ) : egg_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XX' ] sperm_donors = [ god for god in self . gods . values ( ) if god . chromosomes == 'XY' ] for i in range ( generations ) : print ( "\nGENERATION %d\n" % ( i + 1 ) ) gen_xx = [ ] gen_xy = [ ] for egg_donor in egg_donors : sperm_donor = random . choice ( sperm_donors ) brood = self . breed ( egg_donor , sperm_donor ) for child in brood : if child . divinity > human : self . add_god ( child ) if child . chromosomes == 'XX' : gen_xx . append ( child ) else : gen_xy . append ( child ) egg_donors = [ ed for ed in egg_donors if ed . generation > ( i - 2 ) ] sperm_donors = [ sd for sd in sperm_donors if sd . generation > ( i - 3 ) ] egg_donors += gen_xx sperm_donors += gen_xy
2875	def add_bpmn_files ( self , filenames ) : for filename in filenames : f = open ( filename , 'r' ) try : self . add_bpmn_xml ( ET . parse ( f ) , filename = filename ) finally : f . close ( )
7069	def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] varfeaturedir = os . path . join ( simbasedir , 'varfeatures' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) varinfo = lcvfeatures . parallel_varfeatures ( lcfpaths , varfeaturedir , lcformat = fakelc_formatkey , mindet = mindet , nworkers = nworkers ) with open ( os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' ) , 'wb' ) as outfd : pickle . dump ( varinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' )
1888	def must_be_true ( self , constraints , expression ) -> bool : solutions = self . get_all_values ( constraints , expression , maxcnt = 2 , silent = True ) return solutions == [ True ]
8762	def _perform_async_update_rule ( context , id , db_sg_group , rule_id , action ) : rpc_reply = None sg_rpc = sg_rpc_api . QuarkSGAsyncProcessClient ( ) ports = db_api . sg_gather_associated_ports ( context , db_sg_group ) if len ( ports ) > 0 : rpc_reply = sg_rpc . start_update ( context , id , rule_id , action ) if rpc_reply : job_id = rpc_reply [ 'job_id' ] job_api . add_job_to_context ( context , job_id ) else : LOG . error ( "Async update failed. Is the worker running?" )
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
3788	def set_user_method ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this mixture" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . TP_zs_ws_cached = ( None , None , None , None )
105	def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ "bool" , "uint8" , "uint16" , "uint32" , "int8" , "int16" , "int32" , "float16" , "float32" , "float64" , "float128" ] , disallowed = [ "uint64" , "uint128" , "uint256" , "int64" , "int128" , "int256" , "float256" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced
13287	def load ( cls , query_name ) : template_path = os . path . join ( os . path . dirname ( __file__ ) , '../data/githubv4' , query_name + '.graphql' ) with open ( template_path ) as f : query_data = f . read ( ) return cls ( query_data , name = query_name )
11836	def h ( self , node ) : "h function is straight-line distance from a node's state to goal." locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity
13613	def combine_filenames ( filenames , max_length = 40 ) : path = None names = [ ] extension = None timestamps = [ ] shas = [ ] filenames . sort ( ) concat_names = "_" . join ( filenames ) if concat_names in COMBINED_FILENAMES_GENERATED : return COMBINED_FILENAMES_GENERATED [ concat_names ] for filename in filenames : name = os . path . basename ( filename ) if not extension : extension = os . path . splitext ( name ) [ 1 ] elif os . path . splitext ( name ) [ 1 ] != extension : raise ValueError ( "Can't combine multiple file extensions" ) for base in MEDIA_ROOTS : try : shas . append ( md5 ( os . path . join ( base , filename ) ) ) break except IOError : pass if path is None : path = os . path . dirname ( filename ) else : if len ( os . path . dirname ( filename ) ) < len ( path ) : path = os . path . dirname ( filename ) m = hashlib . md5 ( ) m . update ( "," . join ( shas ) ) new_filename = "%s-inkmd" % m . hexdigest ( ) new_filename = new_filename [ : max_length ] new_filename += extension COMBINED_FILENAMES_GENERATED [ concat_names ] = new_filename return os . path . join ( path , new_filename )
7685	def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , ** kwargs )
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
5822	def vcl ( self , name , content ) : vcl = VCL ( ) vcl . conn = self . conn vcl . attrs = { 'service_id' : self . attrs [ 'service_id' ] , 'version' : self . attrs [ 'number' ] , 'name' : name , 'content' : content , } vcl . save ( ) return vcl
1014	def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]
8726	def _localize ( dt ) : try : tz = dt . tzinfo return tz . localize ( dt . replace ( tzinfo = None ) ) except AttributeError : return dt
4341	def repeat ( self , count = 1 ) : if not isinstance ( count , int ) or count < 1 : raise ValueError ( "count must be a postive integer." ) effect_args = [ 'repeat' , '{}' . format ( count ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'repeat' )
7370	def permission_check ( data , command_permissions , command = None , permissions = None ) : if permissions : pass elif command : if hasattr ( command , 'permissions' ) : permissions = command . permissions else : return True else : msg = "{name} must be called with command or permissions argument" raise RuntimeError ( msg . format ( name = "_permission_check" ) ) return any ( data [ 'sender' ] [ 'id' ] in command_permissions [ permission ] for permission in permissions if permission in command_permissions )
10022	def update_environment ( self , environment_name , description = None , option_settings = [ ] , tier_type = None , tier_name = None , tier_version = '1.0' ) : out ( "Updating environment: " + str ( environment_name ) ) messages = self . ebs . validate_configuration_settings ( self . app_name , option_settings , environment_name = environment_name ) messages = messages [ 'ValidateConfigurationSettingsResponse' ] [ 'ValidateConfigurationSettingsResult' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment_name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'OptionName' ] + "': " + message [ 'Message' ] ) self . ebs . update_environment ( environment_name = environment_name , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
