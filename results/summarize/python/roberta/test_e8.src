6118	def circular_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
9878	def _coincidences ( value_counts , value_domain , dtype = np . float64 ) : value_counts_matrices = value_counts . reshape ( value_counts . shape + ( 1 , ) ) pairable = np . maximum ( np . sum ( value_counts , axis = 1 ) , 2 ) diagonals = np . tile ( np . eye ( len ( value_domain ) ) , ( len ( value_counts ) , 1 , 1 ) ) * value_counts . reshape ( ( value_counts . shape [ 0 ] , 1 , value_counts . shape [ 1 ] ) ) unnormalized_coincidences = value_counts_matrices * value_counts_matrices . transpose ( ( 0 , 2 , 1 ) ) - diagonals return np . sum ( np . divide ( unnormalized_coincidences , ( pairable - 1 ) . reshape ( ( - 1 , 1 , 1 ) ) , dtype = dtype ) , axis = 0 )
2035	def SLOAD ( self , offset ) : storage_address = self . address self . _publish ( 'will_evm_read_storage' , storage_address , offset ) value = self . world . get_storage_data ( storage_address , offset ) self . _publish ( 'did_evm_read_storage' , storage_address , offset , value ) return value
5695	def import_ ( self , conn ) : if self . print_progress : print ( 'Beginning' , self . __class__ . __name__ ) self . _conn = conn self . create_table ( conn ) if self . mode in ( 'all' , 'import' ) and self . fname and self . exists ( ) and self . table not in ignore_tables : self . insert_data ( conn ) if self . mode in ( 'all' , 'index' ) and hasattr ( self , 'index' ) : self . create_index ( conn ) if self . mode in ( 'all' , 'import' ) and hasattr ( self , 'post_import' ) : self . run_post_import ( conn ) conn . commit ( )
251	def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( "Unexpected value for denominator '{}'. The " "denominator parameter must be either 'AGB'" " or 'portfolio_value'." . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover
4094	def AIC ( N , rho , k ) : r from numpy import log , array res = N * log ( array ( rho ) ) + 2. * ( array ( k ) + 1 ) return res
9493	def _simulate_stack ( code : list ) -> int : max_stack = 0 curr_stack = 0 def _check_stack ( ins ) : if curr_stack < 0 : raise CompileError ( "Stack turned negative on instruction: {}" . format ( ins ) ) if curr_stack > max_stack : return curr_stack for instruction in code : assert isinstance ( instruction , dis . Instruction ) if instruction . arg is not None : try : effect = dis . stack_effect ( instruction . opcode , instruction . arg ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e else : try : effect = dis . stack_effect ( instruction . opcode ) except ValueError as e : raise CompileError ( "Invalid opcode `{}` when compiling" . format ( instruction . opcode ) ) from e curr_stack += effect _should_new_stack = _check_stack ( instruction ) if _should_new_stack : max_stack = _should_new_stack return max_stack
8233	def complement ( clr ) : clr = color ( clr ) colors = colorlist ( clr ) colors . append ( clr . complement ) return colors
12229	def autodiscover_siteprefs ( admin_site = None ) : if admin_site is None : admin_site = admin . site if 'manage' not in sys . argv [ 0 ] or ( len ( sys . argv ) > 1 and sys . argv [ 1 ] in MANAGE_SAFE_COMMANDS ) : import_prefs ( ) Preference . read_prefs ( get_prefs ( ) ) register_admin_models ( admin_site )
11	def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs
6297	def release ( self , buffer = True ) : for key , vao in self . vaos : vao . release ( ) if buffer : for buff in self . buffers : buff . buffer . release ( ) if self . _index_buffer : self . _index_buffer . release ( )
7747	def _process_handler_result ( self , response ) : if response is None or response is False : return False if isinstance ( response , Stanza ) : self . send ( response ) return True try : response = iter ( response ) except TypeError : return bool ( response ) for stanza in response : if isinstance ( stanza , Stanza ) : self . send ( stanza ) else : logger . warning ( u"Unexpected object in stanza handler result:" u" {0!r}" . format ( stanza ) ) return True
11833	def make_undirected ( self ) : "Make a digraph into an undirected graph by adding symmetric edges." for a in self . dict . keys ( ) : for ( b , distance ) in self . dict [ a ] . items ( ) : self . connect1 ( b , a , distance )
2529	def parse ( self , fil ) : self . error = False self . graph = Graph ( ) self . graph . parse ( file = fil , format = 'xml' ) self . doc = document . Document ( ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'SpdxDocument' ] ) ) : self . parse_doc_fields ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'ExternalDocumentRef' ] ) ) : self . parse_ext_doc_ref ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'CreationInfo' ] ) ) : self . parse_creation_info ( s ) for s , _p , o in self . graph . triples ( ( None , RDF . type , self . spdx_namespace [ 'Package' ] ) ) : self . parse_package ( s ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'referencesFile' ] , None ) ) : self . parse_file ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'reviewed' ] , None ) ) : self . parse_review ( o ) for s , _p , o in self . graph . triples ( ( None , self . spdx_namespace [ 'annotation' ] , None ) ) : self . parse_annotation ( o ) validation_messages = [ ] validation_messages = self . doc . validate ( validation_messages ) if not self . error : if validation_messages : for msg in validation_messages : self . logger . log ( msg ) self . error = True return self . doc , self . error
11829	def expand ( self , problem ) : "List the nodes reachable in one step from this node." return [ self . child_node ( problem , action ) for action in problem . actions ( self . state ) ]
1506	def template_heron_tools_hcl ( cl_args , masters , zookeepers ) : heron_tools_hcl_template = "%s/standalone/templates/heron_tools.template.hcl" % cl_args [ "config_path" ] heron_tools_hcl_actual = "%s/standalone/resources/heron_tools.hcl" % cl_args [ "config_path" ] single_master = masters [ 0 ] template_file ( heron_tools_hcl_template , heron_tools_hcl_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<heron_tracker_executable>" : '"%s/heron-tracker"' % config . get_heron_bin_dir ( ) , "<heron_tools_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_ui_executable>" : '"%s/heron-ui"' % config . get_heron_bin_dir ( ) } )
8061	def do_windowed ( self , line ) : self . bot . canvas . sink . trigger_fullscreen_action ( False ) print ( self . response_prompt , file = self . stdout )
12312	def create_ingest_point ( self , privateStreamName , publicStreamName ) : return self . protocol . execute ( 'createIngestPoint' , privateStreamName = privateStreamName , publicStreamName = publicStreamName )
11525	def create_big_thumbnail ( self , token , bitstream_id , item_id , width = 575 ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'bitstreamId' ] = bitstream_id parameters [ 'itemId' ] = item_id parameters [ 'width' ] = width response = self . request ( 'midas.thumbnailcreator.create.big.thumbnail' , parameters ) return response
4068	def update_items ( self , payload ) : to_send = [ self . check_items ( [ p ] ) [ 0 ] for p in payload ] headers = { } headers . update ( self . default_headers ( ) ) for chunk in chunks ( to_send , 50 ) : req = requests . post ( url = self . endpoint + "/{t}/{u}/items/" . format ( t = self . library_type , u = self . library_id ) , headers = headers , data = json . dumps ( chunk ) , ) self . request = req try : req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( req ) return True
9392	def calc_key_stats ( self , metric_store ) : stats_to_calculate = [ 'mean' , 'std' , 'min' , 'max' ] percentiles_to_calculate = range ( 0 , 100 , 1 ) for column , groups_store in metric_store . items ( ) : for group , time_store in groups_store . items ( ) : data = metric_store [ column ] [ group ] . values ( ) if self . groupby : column_name = group + '.' + column else : column_name = column if column . startswith ( 'qps' ) : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( data , stats_to_calculate , percentiles_to_calculate ) else : self . calculated_stats [ column_name ] , self . calculated_percentiles [ column_name ] = naarad . utils . calculate_stats ( list ( heapq . merge ( * data ) ) , stats_to_calculate , percentiles_to_calculate ) self . update_summary_stats ( column_name )
1806	def SETB ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
13742	def get_conn ( self , aws_access_key = None , aws_secret_key = None ) : return boto . connect_dynamodb ( aws_access_key_id = aws_access_key , aws_secret_access_key = aws_secret_key , )
519	def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p
2107	def config ( key = None , value = None , scope = 'user' , global_ = False , unset = False ) : if global_ : scope = 'global' warnings . warn ( 'The `--global` option is deprecated and will be ' 'removed. Use `--scope=global` to get the same effect.' , DeprecationWarning ) if not key : seen = set ( ) parser_desc = { 'runtime' : 'Runtime options.' , 'environment' : 'Options from environment variables.' , 'local' : 'Local options (set with `tower-cli config ' '--scope=local`; stored in .tower_cli.cfg of this ' 'directory or a parent)' , 'user' : 'User options (set with `tower-cli config`; stored in ' '~/.tower_cli.cfg).' , 'global' : 'Global options (set with `tower-cli config ' '--scope=global`, stored in /etc/tower/tower_cli.cfg).' , 'defaults' : 'Defaults.' , } click . echo ( '' ) for name , parser in zip ( settings . _parser_names , settings . _parsers ) : will_echo = False for option in parser . options ( 'general' ) : if option in seen : continue will_echo = True if will_echo : secho ( '# %s' % parser_desc [ name ] , fg = 'green' , bold = True ) for option in parser . options ( 'general' ) : if option in seen : continue _echo_setting ( option ) seen . add ( option ) if will_echo : click . echo ( '' ) return if not hasattr ( settings , key ) : raise exc . TowerCLIError ( 'Invalid configuration option "%s".' % key ) if value and unset : raise exc . UsageError ( 'Cannot provide both a value and --unset.' ) if key and not value and not unset : _echo_setting ( key ) return filename = os . path . expanduser ( '~/.tower_cli.cfg' ) if scope == 'global' : if not os . path . isdir ( '/etc/tower/' ) : raise exc . TowerCLIError ( '/etc/tower/ does not exist, and this ' 'command cowardly declines to create it.' ) filename = '/etc/tower/tower_cli.cfg' elif scope == 'local' : filename = '.tower_cli.cfg' parser = Parser ( ) parser . add_section ( 'general' ) parser . read ( filename ) if unset : parser . remove_option ( 'general' , key ) else : parser . set ( 'general' , key , value ) with open ( filename , 'w' ) as config_file : parser . write ( config_file ) try : os . chmod ( filename , stat . S_IRUSR | stat . S_IWUSR ) except Exception as e : warnings . warn ( 'Unable to set permissions on {0} - {1} ' . format ( filename , e ) , UserWarning ) click . echo ( 'Configuration updated successfully.' )
13550	def _get_resource ( self , url , data_key = None ) : headers = { "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . getURL ( url , headers ) if response . status != 200 : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
5661	def find_segments ( stops , shape ) : if not shape : return [ ] , 0 break_points = [ ] last_i = 0 cumul_d = 0 badness = 0 d_last_stop = float ( 'inf' ) lstlat , lstlon = None , None break_shape_points = [ ] for stop in stops : stlat , stlon = stop [ 'lat' ] , stop [ 'lon' ] best_d = float ( 'inf' ) if badness > 500 and badness > 30 * len ( break_points ) : return [ ] , badness for i in range ( last_i , len ( shape ) ) : d = wgs84_distance ( stlat , stlon , shape [ i ] [ 'lat' ] , shape [ i ] [ 'lon' ] ) if lstlat : d_last_stop = wgs84_distance ( lstlat , lstlon , shape [ i ] [ 'lat' ] , shape [ i ] [ 'lon' ] ) if d < best_d : best_d = d best_i = i cumul_d += d if ( d_last_stop < d ) or ( d > 500 ) or ( i < best_i + 100 ) : continue else : badness += best_d break_points . append ( best_i ) last_i = best_i lstlat , lstlon = stlat , stlon break_shape_points . append ( shape [ best_i ] ) break else : badness += best_d break_points . append ( best_i ) last_i = best_i lstlat , lstlon = stlat , stlon break_shape_points . append ( shape [ best_i ] ) pass return break_points , badness
7358	def _check_peptide_inputs ( self , peptides ) : require_iterable_of ( peptides , string_types ) check_X = not self . allow_X_in_peptides check_lower = not self . allow_lowercase_in_peptides check_min_length = self . min_peptide_length is not None min_length = self . min_peptide_length check_max_length = self . max_peptide_length is not None max_length = self . max_peptide_length for p in peptides : if not p . isalpha ( ) : raise ValueError ( "Invalid characters in peptide '%s'" % p ) elif check_X and "X" in p : raise ValueError ( "Invalid character 'X' in peptide '%s'" % p ) elif check_lower and not p . isupper ( ) : raise ValueError ( "Invalid lowercase letters in peptide '%s'" % p ) elif check_min_length and len ( p ) < min_length : raise ValueError ( "Peptide '%s' too short (%d chars), must be at least %d" % ( p , len ( p ) , min_length ) ) elif check_max_length and len ( p ) > max_length : raise ValueError ( "Peptide '%s' too long (%d chars), must be at least %d" % ( p , len ( p ) , max_length ) )
3334	def string_repr ( s ) : if compat . is_bytes ( s ) : res = "{!r}: " . format ( s ) for b in s : if type ( b ) is str : b = ord ( b ) res += "%02x " % b return res return "{}" . format ( s )
12710	def relative_offset_to_world ( self , offset ) : return np . array ( self . body_to_world ( offset * self . dimensions / 2 ) )
2807	def convert_gemm ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting Linear ...' ) if names == 'short' : tf_name = 'FC' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] has_bias = False if bias_name in weights : bias = weights [ bias_name ] . numpy ( ) keras_weights = [ W , bias ] has_bias = True dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = has_bias , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] )
7742	def _prepare_pending ( self ) : if not self . _unprepared_pending : return for handler in list ( self . _unprepared_pending ) : self . _configure_io_handler ( handler ) self . check_events ( )
3957	def update_running_containers_from_spec ( compose_config , recreate_containers = True ) : write_composefile ( compose_config , constants . COMPOSEFILE_PATH ) compose_up ( constants . COMPOSEFILE_PATH , 'dusty' , recreate_containers = recreate_containers )
6514	def get_gender ( self , name , country = None ) : if not self . case_sensitive : name = name . lower ( ) if name not in self . names : return self . unknown_value elif not country : def counter ( country_values ) : country_values = map ( ord , country_values . replace ( " " , "" ) ) return ( len ( country_values ) , sum ( map ( lambda c : c > 64 and c - 55 or c - 48 , country_values ) ) ) return self . _most_popular_gender ( name , counter ) elif country in self . __class__ . COUNTRIES : index = self . __class__ . COUNTRIES . index ( country ) counter = lambda e : ( ord ( e [ index ] ) - 32 , 0 ) return self . _most_popular_gender ( name , counter ) else : raise NoCountryError ( "No such country: %s" % country )
11926	def parse_filename ( self , filepath ) : name = os . path . basename ( filepath ) [ : - src_ext_len ] try : dt = datetime . strptime ( name , "%Y-%m-%d-%H-%M" ) except ValueError : raise PostNameInvalid return { 'name' : name , 'datetime' : dt , 'filepath' : filepath }
20	def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return "{} {}{}" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'
10675	def list_compounds ( ) : print ( 'Compounds currently loaded:' ) for compound in sorted ( compounds . keys ( ) ) : phases = compounds [ compound ] . get_phase_list ( ) print ( '%s: %s' % ( compound , ', ' . join ( phases ) ) )
1690	def UpdatePreprocessor ( self , line ) : if Match ( r'^\s*#\s*(if|ifdef|ifndef)\b' , line ) : self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) elif Match ( r'^\s*#\s*(else|elif)\b' , line ) : if self . pp_stack : if not self . pp_stack [ - 1 ] . seen_else : self . pp_stack [ - 1 ] . seen_else = True self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) else : pass elif Match ( r'^\s*#\s*endif\b' , line ) : if self . pp_stack : if self . pp_stack [ - 1 ] . seen_else : self . stack = self . pp_stack [ - 1 ] . stack_before_else self . pp_stack . pop ( ) else : pass
871	def edit ( cls , properties ) : copyOfProperties = copy ( properties ) configFilePath = cls . getPath ( ) try : with open ( configFilePath , 'r' ) as fp : contents = fp . read ( ) except IOError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( "Error %s reading custom configuration store " "from %s, while editing properties %s." , e . errno , configFilePath , properties ) raise contents = '<configuration/>' try : elements = ElementTree . XML ( contents ) ElementTree . tostring ( elements ) except Exception , e : msg = "File contents of custom configuration is corrupt. File " "location: %s; Contents: '%s'. Original Error (%s): %s." % ( configFilePath , contents , type ( e ) , e ) _getLogger ( ) . exception ( msg ) raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] if elements . tag != 'configuration' : e = "Expected top-level element to be 'configuration' but got '%s'" % ( elements . tag ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyItem in elements . findall ( './property' ) : propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) name = propInfo [ 'name' ] if name in copyOfProperties : foundValues = propertyItem . findall ( './value' ) if len ( foundValues ) > 0 : foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) if not copyOfProperties : break else : e = "Property %s missing value tag." % ( name , ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyName , value in copyOfProperties . iteritems ( ) : newProp = ElementTree . Element ( 'property' ) nameTag = ElementTree . Element ( 'name' ) nameTag . text = propertyName newProp . append ( nameTag ) valueTag = ElementTree . Element ( 'value' ) valueTag . text = str ( value ) newProp . append ( valueTag ) elements . append ( newProp ) try : makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) with open ( configFilePath , 'w' ) as fp : fp . write ( ElementTree . tostring ( elements ) ) except Exception , e : _getLogger ( ) . exception ( "Error while saving custom configuration " "properties %s in %s." , properties , configFilePath ) raise
10132	def parse_grid ( grid_data ) : try : grid_parts = NEWLINE_RE . split ( grid_data ) if len ( grid_parts ) < 2 : raise ZincParseException ( 'Malformed grid received' , grid_data , 1 , 1 ) grid_meta_str = grid_parts . pop ( 0 ) col_meta_str = grid_parts . pop ( 0 ) ver_match = VERSION_RE . match ( grid_meta_str ) if ver_match is None : raise ZincParseException ( 'Could not determine version from %r' % grid_meta_str , grid_data , 1 , 1 ) version = Version ( ver_match . group ( 1 ) ) try : grid_meta = hs_gridMeta [ version ] . parseString ( grid_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse grid metadata: %s' % pe , grid_data , 1 , pe . col ) except : LOG . debug ( 'Failed to parse grid meta: %r' , grid_meta_str ) raise try : col_meta = hs_cols [ version ] . parseString ( col_meta_str , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse column metadata: %s' % reformat_exception ( pe , 2 ) , grid_data , 2 , pe . col ) except : LOG . debug ( 'Failed to parse column meta: %r' , col_meta_str ) raise row_grammar = hs_row [ version ] def _parse_row ( row_num_and_data ) : ( row_num , row ) = row_num_and_data line_num = row_num + 3 try : return dict ( zip ( col_meta . keys ( ) , row_grammar . parseString ( row , parseAll = True ) [ 0 ] . asList ( ) ) ) except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse row: %s' % reformat_exception ( pe , line_num ) , grid_data , line_num , pe . col ) except : LOG . debug ( 'Failed to parse row: %r' , row ) raise g = Grid ( version = grid_meta . pop ( 'ver' ) , metadata = grid_meta , columns = list ( col_meta . items ( ) ) ) g . extend ( map ( _parse_row , filter ( lambda gp : bool ( gp [ 1 ] ) , enumerate ( grid_parts ) ) ) ) return g except : LOG . debug ( 'Failing grid: %r' , grid_data ) raise
4650	def appendSigner ( self , accounts , permission ) : assert permission in self . permission_types , "Invalid permission" if self . blockchain . wallet . locked ( ) : raise WalletLocked ( ) if not isinstance ( accounts , ( list , tuple , set ) ) : accounts = [ accounts ] for account in accounts : if account not in self . signing_accounts : if isinstance ( account , self . publickey_class ) : self . appendWif ( self . blockchain . wallet . getPrivateKeyForPublicKey ( str ( account ) ) ) else : accountObj = self . account_class ( account , blockchain_instance = self . blockchain ) required_treshold = accountObj [ permission ] [ "weight_threshold" ] keys = self . _fetchkeys ( accountObj , permission , required_treshold = required_treshold ) if not keys and permission != "owner" : keys . extend ( self . _fetchkeys ( accountObj , "owner" , required_treshold = required_treshold ) ) for x in keys : self . appendWif ( x [ 0 ] ) self . signing_accounts . append ( account )
356	def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( "session is None." ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( "[*] save %s n_params: %d" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( " param {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )
13053	def nmap_scan ( ) : hs = HostSearch ( ) config = Config ( ) nmap_types = [ 'top10' , 'top100' , 'custom' , 'top1000' , 'all' ] options = { 'top10' : '--top-ports 10' , 'top100' : '--top-ports 100' , 'custom' : config . get ( 'nmap' , 'options' ) , 'top1000' : '--top-ports 1000' , 'all' : '-p-' } hs_parser = hs . argparser argparser = argparse . ArgumentParser ( parents = [ hs_parser ] , conflict_handler = 'resolve' , description = "Scans hosts from the database using nmap, any arguments that are not in the help are passed to nmap" ) argparser . add_argument ( 'type' , metavar = 'type' , help = 'The number of ports to scan: top10, top100, custom, top1000 (default) or all' , type = str , choices = nmap_types , default = 'top1000' , const = 'top1000' , nargs = '?' ) arguments , extra_nmap_args = argparser . parse_known_args ( ) tags = nmap_types [ nmap_types . index ( arguments . type ) : ] tags = [ "!nmap_" + tag for tag in tags ] hosts = hs . get_hosts ( tags = tags ) hosts = [ host for host in hosts ] nmap_args = [ ] nmap_args . extend ( extra_nmap_args ) nmap_args . extend ( options [ arguments . type ] . split ( ' ' ) ) print_notification ( "Running nmap with args: {} on {} hosts(s)" . format ( nmap_args , len ( hosts ) ) ) if len ( hosts ) : result = nmap ( nmap_args , [ str ( h . address ) for h in hosts ] ) for host in hosts : host . add_tag ( "nmap_{}" . format ( arguments . type ) ) host . save ( ) print_notification ( "Nmap done, importing results" ) stats = import_nmap ( result , "nmap_{}" . format ( arguments . type ) , check_function = all_hosts , import_services = True ) stats [ 'scanned_hosts' ] = len ( hosts ) stats [ 'type' ] = arguments . type Logger ( ) . log ( 'nmap_scan' , "Performed nmap {} scan on {} hosts" . format ( arguments . type , len ( hosts ) ) , stats ) else : print_notification ( "No hosts found" )
13575	def select ( course = False , tid = None , auto = False ) : if course : update ( course = True ) course = None try : course = Course . get_selected ( ) except NoCourseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select a course" , Course . select ( ) . execute ( ) , course ) else : ret [ "item" ] = Course . get ( Course . tid == tid ) if "item" in ret : ret [ "item" ] . set_select ( ) update ( ) if ret [ "item" ] . path == "" : select_a_path ( auto = auto ) skip ( ) return else : print ( "You can select the course with `tmc select --course`" ) return else : selected = None try : selected = Exercise . get_selected ( ) except NoExerciseSelected : pass ret = { } if not tid : ret = Menu . launch ( "Select an exercise" , Course . get_selected ( ) . exercises , selected ) else : ret [ "item" ] = Exercise . byid ( tid ) if "item" in ret : ret [ "item" ] . set_select ( ) print ( "Selected {}" . format ( ret [ "item" ] ) )
412	def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( "[Database] Save model: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save model: FAIL" ) return False
1364	def get_argument_length ( self ) : try : length = self . get_argument ( constants . PARAM_LENGTH ) return length except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
3923	def _set_title ( self ) : self . title = get_conv_name ( self . _conversation , show_unread = True , truncate = True ) self . _set_title_cb ( self , self . title )
9157	def stroke_linejoin ( self , linejoin ) : linejoin = getattr ( pgmagick . LineJoin , "%sJoin" % linejoin . title ( ) ) linejoin = pgmagick . DrawableStrokeLineJoin ( linejoin ) self . drawer . append ( linejoin )
7011	def plot_periodbase_lsp ( lspinfo , outfile = None , plotdpi = 100 ) : if isinstance ( lspinfo , str ) and os . path . exists ( lspinfo ) : LOGINFO ( 'loading LSP info from pickle %s' % lspinfo ) with open ( lspinfo , 'rb' ) as infd : lspinfo = pickle . load ( infd ) try : periods = lspinfo [ 'periods' ] lspvals = lspinfo [ 'lspvals' ] bestperiod = lspinfo [ 'bestperiod' ] lspmethod = lspinfo [ 'method' ] plt . plot ( periods , lspvals ) plt . xscale ( 'log' , basex = 10 ) plt . xlabel ( 'Period [days]' ) plt . ylabel ( PLOTYLABELS [ lspmethod ] ) plottitle = '%s best period: %.6f d' % ( METHODSHORTLABELS [ lspmethod ] , bestperiod ) plt . title ( plottitle ) for bestperiod , bestpeak in zip ( lspinfo [ 'nbestperiods' ] , lspinfo [ 'nbestlspvals' ] ) : plt . annotate ( '%.6f' % bestperiod , xy = ( bestperiod , bestpeak ) , xycoords = 'data' , xytext = ( 0.0 , 25.0 ) , textcoords = 'offset points' , arrowprops = dict ( arrowstyle = "->" ) , fontsize = 'x-small' ) plt . grid ( color = '#a9a9a9' , alpha = 0.9 , zorder = 0 , linewidth = 1.0 , linestyle = ':' ) if outfile and isinstance ( outfile , str ) : if outfile . endswith ( '.png' ) : plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) else : plt . savefig ( outfile , bbox_inches = 'tight' ) plt . close ( ) return os . path . abspath ( outfile ) elif dispok : plt . show ( ) plt . close ( ) return else : LOGWARNING ( 'no output file specified and no $DISPLAY set, ' 'saving to lsp-plot.png in current directory' ) outfile = 'lsp-plot.png' plt . savefig ( outfile , bbox_inches = 'tight' , dpi = plotdpi ) plt . close ( ) return os . path . abspath ( outfile ) except Exception as e : LOGEXCEPTION ( 'could not plot this LSP, appears to be empty' ) return
11711	def add_logged_in_session ( self , response = None ) : if not response : response = self . get ( 'go/api/pipelines.xml' ) self . _set_session_cookie ( response ) if not self . _session_id : raise AuthenticationFailed ( 'No session id extracted from request.' ) response = self . get ( 'go/pipelines' ) match = re . search ( r'name="authenticity_token".+?value="([^"]+)' , response . read ( ) . decode ( 'utf-8' ) ) if match : self . _authenticity_token = match . group ( 1 ) else : raise AuthenticationFailed ( 'Authenticity token not found on page' )
1247	def recv ( self , socket_ , encoding = None ) : unpacker = msgpack . Unpacker ( encoding = encoding ) response = socket_ . recv ( 8 ) if response == b"" : raise TensorForceError ( "No data received by socket.recv in call to method `recv` " + "(listener possibly closed)!" ) orig_len = int ( response ) received_len = 0 while True : data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) if not data : raise TensorForceError ( "No data of len {} received by socket.recv in call to method `recv`!" . format ( orig_len - received_len ) ) data_len = len ( data ) received_len += data_len unpacker . feed ( data ) if received_len == orig_len : break for message in unpacker : sts = message . get ( "status" , message . get ( b"status" ) ) if sts : if sts == "ok" or sts == b"ok" : return message else : raise TensorForceError ( "RemoteEnvironment server error: {}" . format ( message . get ( "message" , "not specified" ) ) ) else : raise TensorForceError ( "Message without field 'status' received!" ) raise TensorForceError ( "No message encoded in data stream (data stream had len={})" . format ( orig_len ) )
7005	def apply_rf_classifier ( classifier , varfeaturesdir , outpickle , maxobjects = None ) : if isinstance ( classifier , str ) and os . path . exists ( classifier ) : with open ( classifier , 'rb' ) as infd : clfdict = pickle . load ( infd ) elif isinstance ( classifier , dict ) : clfdict = classifier else : LOGERROR ( "can't figure out the input classifier arg" ) return None if 'feature_names' not in clfdict : LOGERROR ( "feature_names not present in classifier input, " "can't figure out which ones to extract from " "varfeature pickles in %s" % varfeaturesdir ) return None featurestouse = clfdict [ 'feature_names' ] pklglob = clfdict [ 'collect_kwargs' ] [ 'pklglob' ] magcol = clfdict [ 'magcol' ] featfile = os . path . join ( os . path . dirname ( outpickle ) , 'actual-collected-features.pkl' ) features = collect_nonperiodic_features ( varfeaturesdir , magcol , featfile , pklglob = pklglob , featurestouse = featurestouse , maxobjects = maxobjects ) bestclf = clfdict [ 'best_classifier' ] predicted_labels = bestclf . predict ( features [ 'features_array' ] ) predicted_label_probs = bestclf . predict_proba ( features [ 'features_array' ] ) outdict = { 'features' : features , 'featfile' : featfile , 'classifier' : clfdict , 'predicted_labels' : predicted_labels , 'predicted_label_probs' : predicted_label_probs , } with open ( outpickle , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict
11805	def record_conflict ( self , assignment , var , val , delta ) : "Record conflicts caused by addition or deletion of a Queen." n = len ( self . vars ) self . rows [ val ] += delta self . downs [ var + val ] += delta self . ups [ var - val + n - 1 ] += delta
4761	def assert_that ( val , description = '' ) : global _soft_ctx if _soft_ctx : return AssertionBuilder ( val , description , 'soft' ) return AssertionBuilder ( val , description )
9092	def _get_old_entry_identifiers ( namespace : Namespace ) -> Set [ NamespaceEntry ] : return { term . identifier for term in namespace . entries }
9543	def add_record_length_check ( self , code = RECORD_LENGTH_CHECK_FAILED , message = MESSAGES [ RECORD_LENGTH_CHECK_FAILED ] , modulus = 1 ) : t = code , message , modulus self . _record_length_checks . append ( t )
10750	def download ( self , bands , download_dir = None , metadata = False ) : super ( GoogleDownloader , self ) . validate_bands ( bands ) pattern = re . compile ( '^[^\s]+_(.+)\.tiff?' , re . I ) image_list = [ ] band_list = [ 'B%i' % ( i , ) if isinstance ( i , int ) else i for i in bands ] if download_dir is None : download_dir = DOWNLOAD_DIR check_create_folder ( join ( download_dir , self . sceneInfo . name ) ) filename = "%s%s" % ( self . sceneInfo . name , self . __remote_file_ext ) downloaded = self . fetch ( self . remote_file_url , download_dir , filename ) try : tar = tarfile . open ( downloaded [ 0 ] , 'r' ) folder_path = join ( download_dir , self . sceneInfo . name ) logger . debug ( 'Starting data extraction in directory ' , folder_path ) tar . extractall ( folder_path ) remove ( downloaded [ 0 ] ) images_path = listdir ( folder_path ) for image_path in images_path : matched = pattern . match ( image_path ) file_path = join ( folder_path , image_path ) if matched and matched . group ( 1 ) in band_list : image_list . append ( [ file_path , getsize ( file_path ) ] ) elif matched : remove ( file_path ) except tarfile . ReadError as error : logger . error ( 'Error when extracting files: ' , error ) print ( 'Error when extracting files.' ) return image_list
6148	def unique_cpx_roots ( rlist , tol = 0.001 ) : uniq = [ rlist [ 0 ] ] mult = [ 1 ] for k in range ( 1 , len ( rlist ) ) : N_uniq = len ( uniq ) for m in range ( N_uniq ) : if abs ( rlist [ k ] - uniq [ m ] ) <= tol : mult [ m ] += 1 uniq [ m ] = ( uniq [ m ] * ( mult [ m ] - 1 ) + rlist [ k ] ) / float ( mult [ m ] ) break uniq = np . hstack ( ( uniq , rlist [ k ] ) ) mult = np . hstack ( ( mult , [ 1 ] ) ) return np . array ( uniq ) , np . array ( mult )
3175	def create ( self , campaign_id , data , ** queryparams ) : self . campaign_id = campaign_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) response = self . _mc_client . _post ( url = self . _build_path ( campaign_id , 'feedback' ) , data = data , ** queryparams ) if response is not None : self . feedback_id = response [ 'feedback_id' ] else : self . feedback_id = None return response
778	def _getMatchingRowsNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = None ) : assert fieldsToMatch , repr ( fieldsToMatch ) assert all ( k in tableInfo . dbFieldNames for k in fieldsToMatch . iterkeys ( ) ) , repr ( fieldsToMatch ) assert selectFieldNames , repr ( selectFieldNames ) assert all ( f in tableInfo . dbFieldNames for f in selectFieldNames ) , repr ( selectFieldNames ) matchPairs = fieldsToMatch . items ( ) matchExpressionGen = ( p [ 0 ] + ( ' IS ' + { True : 'TRUE' , False : 'FALSE' } [ p [ 1 ] ] if isinstance ( p [ 1 ] , bool ) else ' IS NULL' if p [ 1 ] is None else ' IN %s' if isinstance ( p [ 1 ] , self . _SEQUENCE_TYPES ) else '=%s' ) for p in matchPairs ) matchFieldValues = [ p [ 1 ] for p in matchPairs if ( not isinstance ( p [ 1 ] , ( bool ) ) and p [ 1 ] is not None ) ] query = 'SELECT %s FROM %s WHERE (%s)' % ( ',' . join ( selectFieldNames ) , tableInfo . tableName , ' AND ' . join ( matchExpressionGen ) ) sqlParams = matchFieldValues if maxRows is not None : query += ' LIMIT %s' sqlParams . append ( maxRows ) conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows : assert maxRows is None or len ( rows ) <= maxRows , "%d !<= %d" % ( len ( rows ) , maxRows ) assert len ( rows [ 0 ] ) == len ( selectFieldNames ) , "%d != %d" % ( len ( rows [ 0 ] ) , len ( selectFieldNames ) ) else : rows = tuple ( ) return rows
4170	def zpk2ss ( z , p , k ) : import scipy . signal return scipy . signal . zpk2ss ( z , p , k )
5391	def _delocalize_logging_command ( self , logging_path , user_project ) : logging_prefix = os . path . splitext ( logging_path . uri ) [ 0 ] if logging_path . file_provider == job_model . P_LOCAL : mkdir_cmd = 'mkdir -p "%s"\n' % os . path . dirname ( logging_prefix ) cp_cmd = 'cp' elif logging_path . file_provider == job_model . P_GCS : mkdir_cmd = '' if user_project : cp_cmd = 'gsutil -u {} -mq cp' . format ( user_project ) else : cp_cmd = 'gsutil -mq cp' else : assert False copy_logs_cmd = textwrap . dedent ( ) . format ( cp_cmd = cp_cmd , prefix = logging_prefix ) body = textwrap . dedent ( ) . format ( mkdir_cmd = mkdir_cmd , copy_logs_cmd = copy_logs_cmd ) return body
13529	def parse ( self , data : RawMessage ) -> Message : try : return self . receiver . parse ( data ) except KeyError as err : raise UnknownCommandError from err except DecodeError as err : raise UnknownCommandError ( f"{err}" ) from err
11280	def get_item_creator ( item_type ) : if item_type not in Pipe . pipe_item_types : for registered_type in Pipe . pipe_item_types : if issubclass ( item_type , registered_type ) : return Pipe . pipe_item_types [ registered_type ] return None else : return Pipe . pipe_item_types [ item_type ]
1345	def predictions ( self , image ) : return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 )
11616	def _roman ( data , scheme_map , ** kw ) : vowels = scheme_map . vowels marks = scheme_map . marks virama = scheme_map . virama consonants = scheme_map . consonants non_marks_viraama = scheme_map . non_marks_viraama max_key_length_from_scheme = scheme_map . max_key_length_from_scheme to_roman = scheme_map . to_scheme . is_roman togglers = kw . pop ( 'togglers' , set ( ) ) suspend_on = kw . pop ( 'suspend_on' , set ( ) ) suspend_off = kw . pop ( 'suspend_off' , set ( ) ) if kw : raise TypeError ( 'Unexpected keyword argument %s' % list ( kw . keys ( ) ) [ 0 ] ) buf = [ ] i = 0 had_consonant = found = False len_data = len ( data ) append = buf . append toggled = False suspended = False while i <= len_data : token = data [ i : i + max_key_length_from_scheme ] while token : if token in togglers : toggled = not toggled i += 2 found = True break if token in suspend_on : suspended = True elif token in suspend_off : suspended = False if toggled or suspended : token = token [ : - 1 ] continue if had_consonant and token in vowels : mark = marks . get ( token , '' ) if mark : append ( mark ) elif to_roman : append ( vowels [ token ] ) found = True elif token in non_marks_viraama : if had_consonant : append ( virama [ '' ] ) append ( non_marks_viraama [ token ] ) found = True if found : had_consonant = token in consonants i += len ( token ) break else : token = token [ : - 1 ] if not found : if had_consonant : append ( virama [ '' ] ) if i < len_data : append ( data [ i ] ) had_consonant = False i += 1 found = False return '' . join ( buf )
10646	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return np . polyval ( self . _coeffs , state [ 'T' ] )
11705	def reproduce_asexually ( self , egg_word , sperm_word ) : egg = self . generate_gamete ( egg_word ) sperm = self . generate_gamete ( sperm_word ) self . genome = list ( set ( egg + sperm ) ) self . generation = 1 self . divinity = god
401	def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights loss = tf . divide ( tf . reduce_sum ( losses ) , tf . reduce_sum ( weights ) , name = "seq_loss_with_mask" ) if return_details : return loss , losses , weights , targets else : return loss
2039	def CALLCODE ( self , gas , _ignored_ , value , in_offset , in_size , out_offset , out_size ) : self . world . start_transaction ( 'CALLCODE' , address = self . address , data = self . read_buffer ( in_offset , in_size ) , caller = self . address , value = value , gas = gas ) raise StartTx ( )
2952	def connect ( self , task_spec ) : assert self . default_task_spec is None self . outputs . append ( task_spec ) self . default_task_spec = task_spec . name task_spec . _connect_notify ( self )
7284	def has_edit_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_staff
8836	def minus ( * args ) : if len ( args ) == 1 : return - to_numeric ( args [ 0 ] ) return to_numeric ( args [ 0 ] ) - to_numeric ( args [ 1 ] )
12804	def get_user ( self , id = None ) : if not id : id = self . _user . id if id not in self . _users : self . _users [ id ] = self . _user if id == self . _user . id else User ( self , id ) return self . _users [ id ]
4200	def _thumbnail_div ( full_dir , fname , snippet , is_backref = False ) : thumb = os . path . join ( full_dir , 'images' , 'thumb' , 'sphx_glr_%s_thumb.png' % fname [ : - 3 ] ) ref_name = os . path . join ( full_dir , fname ) . replace ( os . path . sep , '_' ) template = BACKREF_THUMBNAIL_TEMPLATE if is_backref else THUMBNAIL_TEMPLATE return template . format ( snippet = snippet , thumbnail = thumb , ref_name = ref_name )
370	def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x
3440	def escape_ID ( cobra_model ) : for x in chain ( [ cobra_model ] , cobra_model . metabolites , cobra_model . reactions , cobra_model . genes ) : x . id = _escape_str_id ( x . id ) cobra_model . repair ( ) gene_renamer = _GeneEscaper ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) )
4555	def genVector ( width , height , x_mult = 1 , y_mult = 1 ) : center_x = ( width - 1 ) / 2 center_y = ( height - 1 ) / 2 def length ( x , y ) : dx = math . pow ( x - center_x , 2 * x_mult ) dy = math . pow ( y - center_y , 2 * y_mult ) return int ( math . sqrt ( dx + dy ) ) return [ [ length ( x , y ) for x in range ( width ) ] for y in range ( height ) ]
5136	def get_class_traits ( klass ) : source = inspect . getsource ( klass ) cb = CommentBlocker ( ) cb . process_file ( StringIO ( source ) ) mod_ast = compiler . parse ( source ) class_ast = mod_ast . node . nodes [ 0 ] for node in class_ast . code . nodes : if isinstance ( node , compiler . ast . Assign ) : name = node . nodes [ 0 ] . name rhs = unparse ( node . expr ) . strip ( ) doc = strip_comment_marker ( cb . search_for_comment ( node . lineno , default = '' ) ) yield name , rhs , doc
9496	def parse_module ( path , excludes = None ) : file = path / MODULE_FILENAME if not file . exists ( ) : raise MissingFile ( file ) id = _parse_document_id ( etree . parse ( file . open ( ) ) ) excludes = excludes or [ ] excludes . extend ( [ lambda filepath : filepath . name == MODULE_FILENAME , ] ) resources_paths = _find_resources ( path , excludes = excludes ) resources = tuple ( _resource_from_path ( res ) for res in resources_paths ) return Module ( id , file , resources )
6151	def fir_remez_hpf ( f_stop , f_pass , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : f_pass_eq = fs / 2. - f_pass f_stop_eq = fs / 2. - f_stop n , ff , aa , wts = lowpass_order ( f_pass_eq , f_stop_eq , d_pass , d_stop , fsamp = fs ) N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 ) n = np . arange ( len ( b ) ) b *= ( - 1 ) ** n print ( 'Remez filter taps = %d.' % N_taps ) return b
9	def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError
3766	def Z_from_virial_pressure_form ( P , * args ) : r return 1 + P * sum ( [ coeff * P ** i for i , coeff in enumerate ( args ) ] )
7273	def set_rate ( self , rate ) : self . _rate = self . _player_interface_property ( 'Rate' , dbus . Double ( rate ) ) return self . _rate
10702	def get_modes ( _id ) : url = MODES_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
2930	def pre_parse_and_validate ( self , bpmn , filename ) : bpmn = self . _call_editor_hook ( 'pre_parse_and_validate' , bpmn , filename ) or bpmn return bpmn
8646	def create_milestone_request ( session , project_id , bid_id , description , amount ) : milestone_request_data = { 'project_id' : project_id , 'bid_id' : bid_id , 'description' : description , 'amount' : amount , } response = make_post_request ( session , 'milestone_requests' , json_data = milestone_request_data ) json_data = response . json ( ) if response . status_code == 200 : milestone_request_data = json_data [ 'result' ] return MilestoneRequest ( milestone_request_data ) else : raise MilestoneRequestNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
10743	def declaration ( function ) : function , name = _strip_function ( function ) if not function . __code__ . co_code in [ empty_function . __code__ . co_code , doc_string_only_function . __code__ . co_code ] : raise ValueError ( 'Declaration requires empty function definition' ) def not_implemented_function ( * args , ** kwargs ) : raise ValueError ( 'Argument \'{}\' did not specify how \'{}\' should act on it' . format ( args [ 0 ] , name ) ) not_implemented_function . __qualname__ = not_implemented_function . __name__ return default ( not_implemented_function , name = name )
11824	def genetic_search ( problem , fitness_fn , ngen = 1000 , pmut = 0.1 , n = 20 ) : s = problem . initial_state states = [ problem . result ( s , a ) for a in problem . actions ( s ) ] random . shuffle ( states ) return genetic_algorithm ( states [ : n ] , problem . value , ngen , pmut )
10983	def get_initial_featuring ( statemaker , feature_rad , actual_rad = None , im_name = None , tile = None , invert = True , desc = '' , use_full_path = False , featuring_params = { } , statemaker_kwargs = { } , ** kwargs ) : if actual_rad is None : actual_rad = feature_rad _ , im_name = _pick_state_im_name ( '' , im_name , use_full_path = use_full_path ) im = util . RawImage ( im_name , tile = tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) if np . size ( pos ) == 0 : msg = 'No particles found. Try using a smaller `feature_rad`.' raise ValueError ( msg ) rad = np . ones ( pos . shape [ 0 ] , dtype = 'float' ) * actual_rad s = statemaker ( im , pos , rad , ** statemaker_kwargs ) RLOG . info ( 'State Created.' ) if desc is not None : states . save ( s , desc = desc + 'initial' ) optimize_from_initial ( s , invert = invert , desc = desc , ** kwargs ) return s
11137	def path_required ( func ) : @ wraps ( func ) def wrapper ( self , * args , ** kwargs ) : if self . path is None : warnings . warn ( 'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !' ) return return func ( self , * args , ** kwargs ) return wrapper
553	def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] if self . _priorStateJSON is None : swarms = dict ( ) if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , "The field '%s' " " specified in the fixedFields list is not present in this " " model." % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( "Unsupported search type: %s" % ( self . _hsObj . _searchType ) ) self . _state = dict ( lastUpdateTime = time . time ( ) , lastGoodSprint = None , searchOver = False , activeSwarms = swarms . keys ( ) , swarms = swarms , sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , blackListedEncoders = [ ] , ) self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False
10029	def add_arguments ( parser ) : parser . add_argument ( '-o' , '--old-environment' , help = 'Old environment name' , required = True ) parser . add_argument ( '-n' , '--new-environment' , help = 'New environment name' , required = True )
2454	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True if validations . validate_pkg_src_info ( text ) : doc . package . source_info = str_from_text ( text ) return True else : raise SPDXValueError ( 'Pacckage::SourceInfo' ) else : raise CardinalityError ( 'Package::SourceInfo' )
4162	def _parse_dict_recursive ( dict_str ) : dict_out = dict ( ) pos_last = 0 pos = dict_str . find ( ':' ) while pos >= 0 : key = dict_str [ pos_last : pos ] if dict_str [ pos + 1 ] == '[' : pos_tmp = dict_str . find ( ']' , pos + 1 ) if pos_tmp < 0 : raise RuntimeError ( 'error when parsing dict' ) value = dict_str [ pos + 2 : pos_tmp ] . split ( ',' ) for i in range ( len ( value ) ) : try : value [ i ] = int ( value [ i ] ) except ValueError : pass elif dict_str [ pos + 1 ] == '{' : subdict_str = _select_block ( dict_str [ pos : ] , '{' , '}' ) value = _parse_dict_recursive ( subdict_str ) pos_tmp = pos + len ( subdict_str ) else : raise ValueError ( 'error when parsing dict: unknown elem' ) key = key . strip ( '"' ) if len ( key ) > 0 : dict_out [ key ] = value pos_last = dict_str . find ( ',' , pos_tmp ) if pos_last < 0 : break pos_last += 1 pos = dict_str . find ( ':' , pos_last ) return dict_out
10807	def validate ( cls , state ) : return state in [ cls . ACTIVE , cls . PENDING_ADMIN , cls . PENDING_USER ]
13037	def overview ( ) : doc = Host ( ) search = doc . search ( ) search . aggs . bucket ( 'tag_count' , 'terms' , field = 'tags' , order = { '_count' : 'desc' } , size = 100 ) response = search . execute ( ) print_line ( "{0:<25} {1}" . format ( 'Tag' , 'Count' ) ) print_line ( "-" * 30 ) for entry in response . aggregations . tag_count . buckets : print_line ( "{0:<25} {1}" . format ( entry . key , entry . doc_count ) )
6864	def normalized_flux_to_mag ( lcdict , columns = ( 'sap.sap_flux' , 'sap.sap_flux_err' , 'sap.sap_bkg' , 'sap.sap_bkg_err' , 'pdc.pdcsap_flux' , 'pdc.pdcsap_flux_err' ) ) : tess_mag = lcdict [ 'objectinfo' ] [ 'tessmag' ] for key in columns : k1 , k2 = key . split ( '.' ) if 'err' not in k2 : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( tess_mag - 2.5 * np . log10 ( lcdict [ k1 ] [ k2 ] ) ) else : lcdict [ k1 ] [ k2 . replace ( 'flux' , 'mag' ) ] = ( - 2.5 * np . log10 ( 1.0 - lcdict [ k1 ] [ k2 ] ) ) return lcdict
10292	def expand_internal ( universe : BELGraph , graph : BELGraph , edge_predicates : EdgePredicates = None ) -> None : edge_filter = and_edge_predicates ( edge_predicates ) for u , v in itt . product ( graph , repeat = 2 ) : if graph . has_edge ( u , v ) or not universe . has_edge ( u , v ) : continue rs = defaultdict ( list ) for key , data in universe [ u ] [ v ] . items ( ) : if not edge_filter ( universe , u , v , key ) : continue rs [ data [ RELATION ] ] . append ( ( key , data ) ) if 1 == len ( rs ) : relation = list ( rs ) [ 0 ] for key , data in rs [ relation ] : graph . add_edge ( u , v , key = key , ** data ) else : log . debug ( 'Multiple relationship types found between %s and %s' , u , v )
3013	def locked_delete ( self ) : filters = { self . key_name : self . key_value } self . session . query ( self . model_class ) . filter_by ( ** filters ) . delete ( )
13791	def get_app_name ( ) : fn = getattr ( sys . modules [ '__main__' ] , '__file__' , None ) if fn is None : return '__main__' return os . path . splitext ( os . path . basename ( fn ) ) [ 0 ]
8734	def divide_timedelta ( td1 , td2 ) : try : return td1 / td2 except TypeError : return td1 . total_seconds ( ) / td2 . total_seconds ( )
13133	def parse_domain_users ( domain_users_file , domain_groups_file ) : with open ( domain_users_file ) as f : users = json . loads ( f . read ( ) ) domain_groups = { } if domain_groups_file : with open ( domain_groups_file ) as f : groups = json . loads ( f . read ( ) ) for group in groups : sid = get_field ( group , 'objectSid' ) domain_groups [ int ( sid . split ( '-' ) [ - 1 ] ) ] = get_field ( group , 'cn' ) user_search = UserSearch ( ) count = 0 total = len ( users ) print_notification ( "Importing {} users" . format ( total ) ) for entry in users : result = parse_user ( entry , domain_groups ) user = user_search . id_to_object ( result [ 'username' ] ) user . name = result [ 'name' ] user . domain . append ( result [ 'domain' ] ) user . description = result [ 'description' ] user . groups . extend ( result [ 'groups' ] ) user . flags . extend ( result [ 'flags' ] ) user . sid = result [ 'sid' ] user . add_tag ( "domaindump" ) user . save ( ) count += 1 sys . stdout . write ( '\r' ) sys . stdout . write ( "[{}/{}]" . format ( count , total ) ) sys . stdout . flush ( ) sys . stdout . write ( '\r' ) return count
9879	def _random_coincidences ( value_domain , n , n_v ) : n_v_column = n_v . reshape ( - 1 , 1 ) return ( n_v_column . dot ( n_v_column . T ) - np . eye ( len ( value_domain ) ) * n_v_column ) / ( n - 1 )
5354	def get_repos_by_backend_section ( cls , backend_section , raw = True ) : repos = [ ] projects = TaskProjects . get_projects ( ) for pro in projects : if backend_section in projects [ pro ] : if cls . GLOBAL_PROJECT not in projects : repos += projects [ pro ] [ backend_section ] else : if raw : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : not_in_unknown = [ projects [ pro ] for pro in projects if pro != cls . GLOBAL_PROJECT ] [ 0 ] if backend_section not in not_in_unknown : repos += projects [ cls . GLOBAL_PROJECT ] [ backend_section ] else : if pro != cls . GLOBAL_PROJECT : if backend_section not in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] elif backend_section in projects [ pro ] and backend_section in projects [ cls . GLOBAL_PROJECT ] : repos += projects [ pro ] [ backend_section ] else : not_in_unknown_prj = [ projects [ prj ] for prj in projects if prj != cls . GLOBAL_PROJECT ] not_in_unknown_sections = list ( set ( [ section for prj in not_in_unknown_prj for section in list ( prj . keys ( ) ) ] ) ) if backend_section not in not_in_unknown_sections : repos += projects [ pro ] [ backend_section ] logger . debug ( "List of repos for %s: %s (raw=%s)" , backend_section , repos , raw ) repos = list ( set ( repos ) ) return repos
1641	def _IsType ( clean_lines , nesting_state , expr ) : last_word = Match ( r'^.*(\b\S+)$' , expr ) if last_word : token = last_word . group ( 1 ) else : token = expr if _TYPES . match ( token ) : return True typename_pattern = ( r'\b(?:typename|class|struct)\s+' + re . escape ( token ) + r'\b' ) block_index = len ( nesting_state . stack ) - 1 while block_index >= 0 : if isinstance ( nesting_state . stack [ block_index ] , _NamespaceInfo ) : return False last_line = nesting_state . stack [ block_index ] . starting_linenum next_block_start = 0 if block_index > 0 : next_block_start = nesting_state . stack [ block_index - 1 ] . starting_linenum first_line = last_line while first_line >= next_block_start : if clean_lines . elided [ first_line ] . find ( 'template' ) >= 0 : break first_line -= 1 if first_line < next_block_start : block_index -= 1 continue for i in xrange ( first_line , last_line + 1 , 1 ) : if Search ( typename_pattern , clean_lines . elided [ i ] ) : return True block_index -= 1 return False
3219	def get_network_acls ( vpc , ** conn ) : route_tables = describe_network_acls ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) nacl_ids = [ ] for r in route_tables : nacl_ids . append ( r [ "NetworkAclId" ] ) return nacl_ids
5007	def get_user_from_social_auth ( tpa_provider , tpa_username ) : user_social_auth = UserSocialAuth . objects . select_related ( 'user' ) . filter ( user__username = tpa_username , provider = tpa_provider . backend_name ) . first ( ) return user_social_auth . user if user_social_auth else None
759	def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break
12727	def stop_erps ( self , stop_erps ) : _set_params ( self . ode_obj , 'StopERP' , stop_erps , self . ADOF + self . LDOF )
10441	def getmemorystat ( self , process_name ) : _stat_inst = ProcessStats ( process_name ) _stat_list = [ ] for p in _stat_inst . get_cpu_memory_stat ( ) : try : _stat_list . append ( round ( p . get_memory_percent ( ) , 2 ) ) except psutil . AccessDenied : pass return _stat_list
9051	def poisson_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) link = LogLink ( ) lik = PoissonProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
8628	def create_project ( session , title , description , currency , budget , jobs ) : project_data = { 'title' : title , 'description' : description , 'currency' : currency , 'budget' : budget , 'jobs' : jobs } response = make_post_request ( session , 'projects' , json_data = project_data ) json_data = response . json ( ) if response . status_code == 200 : project_data = json_data [ 'result' ] p = Project ( project_data ) p . url = urljoin ( session . url , 'projects/%s' % p . seo_url ) return p else : raise ProjectNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] , )
4476	def slice_clip ( filename , start , stop , n_samples , sr , mono = True ) : with psf . SoundFile ( str ( filename ) , mode = 'r' ) as soundf : n_target = stop - start soundf . seek ( start ) y = soundf . read ( n_target ) . T if mono : y = librosa . to_mono ( y ) y = librosa . resample ( y , soundf . samplerate , sr ) y = librosa . util . fix_length ( y , n_samples ) return y
8364	def _output_file ( self , frame ) : if self . buff : return self . buff elif self . multifile : return self . file_root + "_%03d" % frame + self . file_ext else : return self . filename
7974	def stop ( self , join = False , timeout = None ) : logger . debug ( "Closing the io handlers..." ) for handler in self . io_handlers : handler . close ( ) if self . event_thread and self . event_thread . is_alive ( ) : logger . debug ( "Sending the QUIT signal" ) self . event_queue . put ( QUIT ) logger . debug ( " sent" ) threads = self . io_threads + self . timeout_threads for thread in threads : logger . debug ( "Stopping thread: {0!r}" . format ( thread ) ) thread . stop ( ) if not join : return if self . event_thread : threads . append ( self . event_thread ) if timeout is None : for thread in threads : thread . join ( ) else : timeout1 = ( timeout * 0.01 ) / len ( threads ) threads_left = [ ] for thread in threads : logger . debug ( "Quick-joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout1 ) if thread . is_alive ( ) : logger . debug ( " thread still alive" . format ( thread ) ) threads_left . append ( thread ) if threads_left : timeout2 = ( timeout * 0.99 ) / len ( threads_left ) for thread in threads_left : logger . debug ( "Joining thread {0!r}..." . format ( thread ) ) thread . join ( timeout2 ) self . io_threads = [ ] self . event_thread = None
11770	def name ( object ) : "Try to find some reasonable name for the object." return ( getattr ( object , 'name' , 0 ) or getattr ( object , '__name__' , 0 ) or getattr ( getattr ( object , '__class__' , 0 ) , '__name__' , 0 ) or str ( object ) )
8535	def pop ( self , nbytes ) : size = 0 popped = [ ] with self . _lock_packets : while size < nbytes : try : packet = self . _packets . pop ( 0 ) size += len ( packet . data . data ) self . _remaining -= len ( packet . data . data ) popped . append ( packet ) except IndexError : break return popped
3630	def add_dividers ( row , divider , padding ) : div = '' . join ( [ padding * ' ' , divider , padding * ' ' ] ) return div . join ( row )
8484	def _update ( self , conf_dict , base_name = None ) : for name in conf_dict : if name . startswith ( '_' ) : continue value = conf_dict [ name ] if value is Namespace : continue if base_name : name = base_name + '.' + name if isinstance ( value , Namespace ) : for name , value in value . iteritems ( name ) : self . set ( name , value ) elif callable ( value ) : value = value ( ) if value is not None : self . set ( name , value ) else : self . set ( name , value )
10490	def popUpItem ( self , * args ) : self . Press ( ) time . sleep ( .5 ) return self . _menuItem ( self , * args )
5671	def plot_temporal_distance_cdf ( self ) : xvalues , cdf = self . profile_block_analyzer . _temporal_distance_cdf ( ) fig = plt . figure ( ) ax = fig . add_subplot ( 111 ) xvalues = numpy . array ( xvalues ) / 60.0 ax . plot ( xvalues , cdf , "-k" ) ax . fill_between ( xvalues , cdf , color = "red" , alpha = 0.2 ) ax . set_ylabel ( "CDF(t)" ) ax . set_xlabel ( "Temporal distance t (min)" ) return fig
7891	def set_stream ( self , stream ) : _unused = stream if self . joined and self . handler : self . handler . user_left ( self . me , None ) self . joined = False
474	def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , "w" ) as f : for i in xrange ( vocabulary_size ) : f . write ( "%s %d\n" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( "%d vocab saved to %s in %s" % ( vocabulary_size , name , pwd ) )
11757	def pl_true ( exp , model = { } ) : op , args = exp . op , exp . args if exp == TRUE : return True elif exp == FALSE : return False elif is_prop_symbol ( op ) : return model . get ( exp ) elif op == '~' : p = pl_true ( args [ 0 ] , model ) if p is None : return None else : return not p elif op == '|' : result = False for arg in args : p = pl_true ( arg , model ) if p is True : return True if p is None : result = None return result elif op == '&' : result = True for arg in args : p = pl_true ( arg , model ) if p is False : return False if p is None : result = None return result p , q = args if op == '>>' : return pl_true ( ~ p | q , model ) elif op == '<<' : return pl_true ( p | ~ q , model ) pt = pl_true ( p , model ) if pt is None : return None qt = pl_true ( q , model ) if qt is None : return None if op == '<=>' : return pt == qt elif op == '^' : return pt != qt else : raise ValueError , "illegal operator in logic expression" + str ( exp )
4112	def rc2ac ( k , R0 ) : [ a , efinal ] = rc2poly ( k , R0 ) R , u , kr , e = rlevinson ( a , efinal ) return R
13428	def get_site ( self , site_id ) : url = "/2/sites/%s" % site_id return self . site_from_json ( self . _get_resource ( url ) [ "site" ] )
10909	def missing_particle ( separation = 0.0 , radius = RADIUS , SNR = 20 ) : s = init . create_two_particle_state ( imsize = 6 * radius + 4 , axis = 'x' , sigma = 1.0 / SNR , delta = separation , radius = radius , stateargs = { 'varyn' : True } , psfargs = { 'error' : 1e-6 } ) s . obj . typ [ 1 ] = 0. s . reset ( ) return s , s . obj . pos . copy ( )
9190	def admin_content_status_single ( request ) : uuid = request . matchdict [ 'uuid' ] try : UUID ( uuid ) except ValueError : raise httpexceptions . HTTPBadRequest ( '{} is not a valid uuid' . format ( uuid ) ) statement , sql_args = get_baking_statuses_sql ( { 'uuid' : uuid } ) with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( statement , sql_args ) modules = cursor . fetchall ( ) if len ( modules ) == 0 : raise httpexceptions . HTTPBadRequest ( '{} is not a book' . format ( uuid ) ) states = [ ] collection_info = modules [ 0 ] for row in modules : message = '' state = row [ 'state' ] or 'PENDING' if state == 'FAILURE' : if row [ 'traceback' ] is not None : message = row [ 'traceback' ] latest_recipe = row [ 'latest_recipe_id' ] current_recipe = row [ 'recipe_id' ] if ( latest_recipe is not None and current_recipe != latest_recipe ) : state += ' stale_recipe' states . append ( { 'version' : row [ 'current_version' ] , 'recipe' : row [ 'recipe' ] , 'created' : str ( row [ 'created' ] ) , 'state' : state , 'state_message' : message , } ) return { 'uuid' : str ( collection_info [ 'uuid' ] ) , 'title' : collection_info [ 'name' ] . decode ( 'utf-8' ) , 'authors' : format_authors ( collection_info [ 'authors' ] ) , 'print_style' : collection_info [ 'print_style' ] , 'current_recipe' : collection_info [ 'recipe_id' ] , 'current_ident' : collection_info [ 'module_ident' ] , 'current_state' : states [ 0 ] [ 'state' ] , 'states' : states }
4217	def delete_password ( self , service , username ) : if not self . connected ( service ) : raise PasswordDeleteError ( "Cancelled by user" ) if not self . iface . hasEntry ( self . handle , service , username , self . appid ) : raise PasswordDeleteError ( "Password not found" ) self . iface . removeEntry ( self . handle , service , username , self . appid )
689	def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )
12914	def write_json ( self , fh , pretty = True ) : sjson = json . JSONEncoder ( ) . encode ( self . json ( ) ) if pretty : json . dump ( json . loads ( sjson ) , fh , sort_keys = True , indent = 4 ) else : json . dump ( json . loads ( sjson ) , fh ) return
1022	def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , ** kwargs ) : connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print "Creating BacktrackingTMCPP instance" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print "Creating PY TM instance" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms
4258	def url_from_path ( path ) : if os . sep != '/' : path = '/' . join ( path . split ( os . sep ) ) return quote ( path )
12939	def clearRedisPools ( ) : global RedisPools global _redisManagedConnectionParams for pool in RedisPools . values ( ) : try : pool . disconnect ( ) except : pass for paramsList in _redisManagedConnectionParams . values ( ) : for params in paramsList : if 'connection_pool' in params : del params [ 'connection_pool' ] RedisPools . clear ( ) _redisManagedConnectionParams . clear ( )
7259	def search_point ( self , lat , lng , filters = None , startDate = None , endDate = None , types = None , type = None ) : searchAreaWkt = "POLYGON ((%s %s, %s %s, %s %s, %s %s, %s %s))" % ( lng , lat , lng , lat , lng , lat , lng , lat , lng , lat ) return self . search ( searchAreaWkt = searchAreaWkt , filters = filters , startDate = startDate , endDate = endDate , types = types )
938	def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , "model.pkl" ) path = os . path . abspath ( path ) return path
5566	def params_at_zoom ( self , zoom ) : if zoom not in self . init_zoom_levels : raise ValueError ( "zoom level not available with current configuration" ) out = dict ( self . _params_at_zoom [ zoom ] , input = { } , output = self . output ) if "input" in self . _params_at_zoom [ zoom ] : flat_inputs = { } for k , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) : if v is None : flat_inputs [ k ] = None else : flat_inputs [ k ] = self . input [ get_hash ( v ) ] out [ "input" ] = _unflatten_tree ( flat_inputs ) else : out [ "input" ] = { } return out
13580	def dmap ( fn , record ) : values = ( fn ( v ) for k , v in record . items ( ) ) return dict ( itertools . izip ( record , values ) )
7136	def redirect_stdout ( new_stdout ) : old_stdout , sys . stdout = sys . stdout , new_stdout try : yield None finally : sys . stdout = old_stdout
9370	def person_inn ( ) : mask11 = [ 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] mask12 = [ 3 , 7 , 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] weighted11 = [ v * mask11 [ i ] for i , v in enumerate ( inn [ : - 2 ] ) ] inn [ 10 ] = sum ( weighted11 ) % 11 % 10 weighted12 = [ v * mask12 [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 11 ] = sum ( weighted12 ) % 11 % 10 return "" . join ( map ( str , inn ) )
758	def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs
3869	async def update_read_timestamp ( self , read_timestamp = None ) : if read_timestamp is None : read_timestamp = ( self . events [ - 1 ] . timestamp if self . events else datetime . datetime . now ( datetime . timezone . utc ) ) if read_timestamp > self . latest_read_timestamp : logger . info ( 'Setting {} latest_read_timestamp from {} to {}' . format ( self . id_ , self . latest_read_timestamp , read_timestamp ) ) state = self . _conversation . self_conversation_state state . self_read_state . latest_read_timestamp = ( parsers . to_timestamp ( read_timestamp ) ) try : await self . _client . update_watermark ( hangouts_pb2 . UpdateWatermarkRequest ( request_header = self . _client . get_request_header ( ) , conversation_id = hangouts_pb2 . ConversationId ( id = self . id_ ) , last_read_timestamp = parsers . to_timestamp ( read_timestamp ) , ) ) except exceptions . NetworkError as e : logger . warning ( 'Failed to update read timestamp: {}' . format ( e ) ) raise
6913	def generate_sinusoidal_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.04 , scale = 500.0 ) , 'fourierorder' : [ 2 , 10 ] , 'amplitude' : sps . uniform ( loc = 0.1 , scale = 0.9 ) , 'phioffset' : 0.0 , } , magsarefluxes = False ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'period' ] . rvs ( size = 1 ) fourierorder = npr . randint ( paramdists [ 'fourierorder' ] [ 0 ] , high = paramdists [ 'fourierorder' ] [ 1 ] ) amplitude = paramdists [ 'amplitude' ] . rvs ( size = 1 ) if magsarefluxes and amplitude < 0.0 : amplitude = - amplitude elif not magsarefluxes and amplitude > 0.0 : amplitude = - amplitude ampcomps = [ abs ( amplitude / 2.0 ) / float ( x ) for x in range ( 1 , fourierorder + 1 ) ] phacomps = [ paramdists [ 'phioffset' ] * float ( x ) for x in range ( 1 , fourierorder + 1 ) ] modelmags , phase , ptimes , pmags , perrs = sinusoidal . sine_series_sum ( [ period , epoch , ampcomps , phacomps ] , times , mags , errs ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] mphase = phase [ timeind ] modeldict = { 'vartype' : 'sinusoidal' , 'params' : { x : y for x , y in zip ( [ 'period' , 'epoch' , 'amplitude' , 'fourierorder' , 'fourieramps' , 'fourierphases' ] , [ period , epoch , amplitude , fourierorder , ampcomps , phacomps ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'phase' : mphase , 'varperiod' : period , 'varamplitude' : amplitude } return modeldict
11274	def check_pidfile ( pidfile , debug ) : if os . path . isfile ( pidfile ) : pidfile_handle = open ( pidfile , 'r' ) try : pid = int ( pidfile_handle . read ( ) ) pidfile_handle . close ( ) if check_pid ( pid , debug ) : return True except : pass os . unlink ( pidfile ) pid = str ( os . getpid ( ) ) open ( pidfile , 'w' ) . write ( pid ) return False
4633	def from_privkey ( cls , privkey , prefix = None ) : privkey = PrivateKey ( privkey , prefix = prefix or Prefix . prefix ) secret = unhexlify ( repr ( privkey ) ) order = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . curve . generator . order ( ) p = ecdsa . SigningKey . from_string ( secret , curve = ecdsa . SECP256k1 ) . verifying_key . pubkey . point x_str = ecdsa . util . number_to_string ( p . x ( ) , order ) compressed = hexlify ( chr ( 2 + ( p . y ( ) & 1 ) ) . encode ( "ascii" ) + x_str ) . decode ( "ascii" ) return cls ( compressed , prefix = prefix or Prefix . prefix )
8530	def of_structs ( cls , a , b ) : t_diff = ThriftDiff ( a , b ) t_diff . _do_diff ( ) return t_diff
9673	def resolve ( self , context , quiet = True ) : try : obj = context for level in self . levels : if isinstance ( obj , dict ) : obj = obj [ level ] elif isinstance ( obj , list ) or isinstance ( obj , tuple ) : obj = obj [ int ( level ) ] else : if callable ( getattr ( obj , level ) ) : try : obj = getattr ( obj , level ) ( ) except KeyError : obj = getattr ( obj , level ) else : display = 'get_%s_display' % level obj = getattr ( obj , display ) ( ) if hasattr ( obj , display ) else getattr ( obj , level ) if not obj : break return obj except Exception as e : if quiet : return '' else : raise e
10545	def delete_task ( task_id ) : try : res = _pybossa_req ( 'delete' , 'task' , task_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
12576	def set_mask ( self , mask_img ) : mask = load_mask ( mask_img , allow_empty = True ) check_img_compatibility ( self . img , mask , only_check_3d = True ) self . mask = mask
11687	def changeset_info ( changeset ) : keys = [ tag . attrib . get ( 'k' ) for tag in changeset . getchildren ( ) ] keys += [ 'id' , 'user' , 'uid' , 'bbox' , 'created_at' ] values = [ tag . attrib . get ( 'v' ) for tag in changeset . getchildren ( ) ] values += [ changeset . get ( 'id' ) , changeset . get ( 'user' ) , changeset . get ( 'uid' ) , get_bounds ( changeset ) , changeset . get ( 'created_at' ) ] return dict ( zip ( keys , values ) )
3257	def publish_featuretype ( self , name , store , native_crs , srs = None , jdbc_virtual_table = None , native_name = None ) : if native_crs is None : raise ValueError ( "must specify native_crs" ) srs = srs or native_crs feature_type = FeatureType ( self , store . workspace , store , name ) feature_type . dirty [ 'name' ] = name feature_type . dirty [ 'srs' ] = srs feature_type . dirty [ 'nativeCRS' ] = native_crs feature_type . enabled = True feature_type . advertised = True feature_type . title = name if native_name is not None : feature_type . native_name = native_name headers = { "Content-type" : "application/xml" , "Accept" : "application/xml" } resource_url = store . resource_url if jdbc_virtual_table is not None : feature_type . metadata = ( { 'JDBC_VIRTUAL_TABLE' : jdbc_virtual_table } ) params = dict ( ) resource_url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "datastores" , store . name , "featuretypes.xml" ] , params ) resp = self . http_request ( resource_url , method = 'post' , data = feature_type . message ( ) , headers = headers ) if resp . status_code not in ( 200 , 201 , 202 ) : FailedRequestError ( 'Failed to publish feature type {} : {}, {}' . format ( name , resp . status_code , resp . text ) ) self . _cache . clear ( ) feature_type . fetch ( ) return feature_type
12648	def filter_objlist ( olist , fieldname , fieldval ) : return [ x for x in olist if getattr ( x , fieldname ) == fieldval ]
5776	def _bcrypt_sign ( private_key , data , hash_algorithm , rsa_pss_padding = False ) : if hash_algorithm == 'raw' : digest = data else : hash_constant = { 'md5' : BcryptConst . BCRYPT_MD5_ALGORITHM , 'sha1' : BcryptConst . BCRYPT_SHA1_ALGORITHM , 'sha256' : BcryptConst . BCRYPT_SHA256_ALGORITHM , 'sha384' : BcryptConst . BCRYPT_SHA384_ALGORITHM , 'sha512' : BcryptConst . BCRYPT_SHA512_ALGORITHM } [ hash_algorithm ] digest = getattr ( hashlib , hash_algorithm ) ( data ) . digest ( ) padding_info = null ( ) flags = 0 if private_key . algorithm == 'rsa' : if rsa_pss_padding : hash_length = { 'md5' : 16 , 'sha1' : 20 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } [ hash_algorithm ] flags = BcryptConst . BCRYPT_PAD_PSS padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PSS_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . cbSalt = hash_length else : flags = BcryptConst . BCRYPT_PAD_PKCS1 padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_PKCS1_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) if hash_algorithm == 'raw' : padding_info_struct . pszAlgId = null ( ) else : hash_buffer = buffer_from_unicode ( hash_constant ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) if private_key . algorithm == 'dsa' and private_key . bit_size > 1024 and hash_algorithm in set ( [ 'md5' , 'sha1' ] ) : raise ValueError ( pretty_message ( ) ) out_len = new ( bcrypt , 'DWORD *' ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) if private_key . algorithm == 'rsa' : padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) res = bcrypt . BCryptSignHash ( private_key . key_handle , padding_info , digest , len ( digest ) , buffer , buffer_len , out_len , flags ) handle_error ( res ) signature = bytes_from_buffer ( buffer , deref ( out_len ) ) if private_key . algorithm != 'rsa' : signature = algos . DSASignature . from_p1363 ( signature ) . dump ( ) return signature
7874	def get_payload ( self , payload_class , payload_key = None , specialize = False ) : if self . _payload is None : self . decode_payload ( ) if payload_class is None : if self . _payload : payload = self . _payload [ 0 ] if specialize and isinstance ( payload , XMLPayload ) : klass = payload_class_for_element_name ( payload . element . tag ) if klass is not XMLPayload : payload = klass . from_xml ( payload . element ) self . _payload [ 0 ] = payload return payload else : return None elements = payload_class . _pyxmpp_payload_element_name for i , payload in enumerate ( self . _payload ) : if isinstance ( payload , XMLPayload ) : if payload_class is not XMLPayload : if payload . xml_element_name not in elements : continue payload = payload_class . from_xml ( payload . element ) elif not isinstance ( payload , payload_class ) : continue if payload_key is not None and payload_key != payload . handler_key ( ) : continue self . _payload [ i ] = payload return payload return None
12566	def get_dataset ( self , ds_name , mode = 'r' ) : if ds_name in self . _datasets : return self . _datasets [ ds_name ] else : return self . create_empty_dataset ( ds_name )
2156	def launch ( self , monitor = False , wait = False , timeout = None , ** kwargs ) : r = client . get ( '/' ) if 'ad_hoc_commands' not in r . json ( ) : raise exc . TowerCLIError ( 'Your host is running an outdated version' 'of Ansible Tower that can not run ' 'ad-hoc commands (2.2 or earlier)' ) self . _pop_none ( kwargs ) debug . log ( 'Launching the ad-hoc command.' , header = 'details' ) result = client . post ( self . endpoint , data = kwargs ) command = result . json ( ) command_id = command [ 'id' ] if monitor : return self . monitor ( command_id , timeout = timeout ) elif wait : return self . wait ( command_id , timeout = timeout ) answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , command_id ) , ) ) answer . update ( result . json ( ) ) return answer
3581	def _get_objects ( self , interface , parent_path = '/org/bluez' ) : parent_path = parent_path . lower ( ) objects = [ ] for opath , interfaces in iteritems ( self . _bluez . GetManagedObjects ( ) ) : if interface in interfaces . keys ( ) and opath . lower ( ) . startswith ( parent_path ) : objects . append ( self . _bus . get_object ( 'org.bluez' , opath ) ) return objects
9417	def to_pointer ( cls , instance ) : return OctavePtr ( instance . _ref , instance . _name , instance . _address )
6815	def enable_mods ( self ) : r = self . local_renderer for mod_name in r . env . mods_enabled : with self . settings ( warn_only = True ) : self . enable_mod ( mod_name )
6886	def mdwarf_subtype_from_sdsscolor ( ri_color , iz_color ) : if np . isfinite ( ri_color ) and np . isfinite ( iz_color ) : obj_sti = 0.875274 * ri_color + 0.483628 * ( iz_color + 0.00438 ) obj_sts = - 0.483628 * ri_color + 0.875274 * ( iz_color + 0.00438 ) else : obj_sti = np . nan obj_sts = np . nan if ( np . isfinite ( obj_sti ) and np . isfinite ( obj_sts ) and ( obj_sti > 0.666 ) and ( obj_sti < 3.4559 ) ) : if ( ( obj_sti > 0.6660 ) and ( obj_sti < 0.8592 ) ) : m_class = 'M0' if ( ( obj_sti > 0.8592 ) and ( obj_sti < 1.0822 ) ) : m_class = 'M1' if ( ( obj_sti > 1.0822 ) and ( obj_sti < 1.2998 ) ) : m_class = 'M2' if ( ( obj_sti > 1.2998 ) and ( obj_sti < 1.6378 ) ) : m_class = 'M3' if ( ( obj_sti > 1.6378 ) and ( obj_sti < 2.0363 ) ) : m_class = 'M4' if ( ( obj_sti > 2.0363 ) and ( obj_sti < 2.2411 ) ) : m_class = 'M5' if ( ( obj_sti > 2.2411 ) and ( obj_sti < 2.4126 ) ) : m_class = 'M6' if ( ( obj_sti > 2.4126 ) and ( obj_sti < 2.9213 ) ) : m_class = 'M7' if ( ( obj_sti > 2.9213 ) and ( obj_sti < 3.2418 ) ) : m_class = 'M8' if ( ( obj_sti > 3.2418 ) and ( obj_sti < 3.4559 ) ) : m_class = 'M9' else : m_class = None return m_class , obj_sti , obj_sts
9791	def find_matching ( cls , path , patterns ) : for pattern in patterns : if pattern . match ( path ) : yield pattern
5095	def get_map_image ( url , dest_path = None ) : image = requests . get ( url , stream = True , timeout = 10 ) if dest_path : image_url = url . rsplit ( '/' , 2 ) [ 1 ] + '-' + url . rsplit ( '/' , 1 ) [ 1 ] image_filename = image_url . split ( '?' ) [ 0 ] dest = os . path . join ( dest_path , image_filename ) image . raise_for_status ( ) with open ( dest , 'wb' ) as data : image . raw . decode_content = True shutil . copyfileobj ( image . raw , data ) return image . raw
3356	def extend ( self , iterable ) : if not hasattr ( self , "_dict" ) or self . _dict is None : self . _dict = { } _dict = self . _dict current_length = len ( self ) list . extend ( self , iterable ) for i , obj in enumerate ( islice ( self , current_length , None ) , current_length ) : the_id = obj . id if the_id not in _dict : _dict [ the_id ] = i else : self = self [ : current_length ] self . _check ( the_id ) raise ValueError ( "id '%s' at index %d is non-unique. " "Is it present twice?" % ( str ( the_id ) , i ) )
6663	def get_expiration_date ( self , fn ) : r = self . local_renderer r . env . crt_fn = fn with hide ( 'running' ) : ret = r . local ( 'openssl x509 -noout -in {ssl_crt_fn} -dates' , capture = True ) matches = re . findall ( 'notAfter=(.*?)$' , ret , flags = re . IGNORECASE ) if matches : return dateutil . parser . parse ( matches [ 0 ] )
7362	async def connect ( self ) : with async_timeout . timeout ( self . timeout ) : self . response = await self . _connect ( ) if self . response . status in range ( 200 , 300 ) : self . _error_timeout = 0 self . state = NORMAL elif self . response . status == 500 : self . state = DISCONNECTION elif self . response . status in range ( 501 , 600 ) : self . state = RECONNECTION elif self . response . status in ( 420 , 429 ) : self . state = ENHANCE_YOUR_CALM else : logger . debug ( "raising error during stream connection" ) raise await exceptions . throw ( self . response , loads = self . client . _loads , url = self . kwargs [ 'url' ] ) logger . debug ( "stream state: %d" % self . state )
5039	def is_user_enrolled ( cls , user , course_id , course_mode ) : enrollment_client = EnrollmentApiClient ( ) try : enrollments = enrollment_client . get_course_enrollment ( user . username , course_id ) if enrollments and course_mode == enrollments . get ( 'mode' ) : return True except HttpClientError as exc : logging . error ( 'Error while checking enrollment status of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) except KeyError as exc : logging . warning ( 'Error while parsing enrollment data of user %(user)s: %(message)s' , dict ( user = user . username , message = str ( exc ) ) ) return False
1070	def getaddrlist ( self ) : result = [ ] ad = self . getaddress ( ) while ad : result += ad ad = self . getaddress ( ) return result
3908	def put ( self , coro ) : assert asyncio . iscoroutine ( coro ) self . _queue . put_nowait ( coro )
10480	def _performAction ( self , action ) : try : _a11y . AXUIElement . _performAction ( self , 'AX%s' % action ) except _a11y . ErrorUnsupported as e : sierra_ver = '10.12' if mac_ver ( ) [ 0 ] < sierra_ver : raise e else : pass
5478	def get_operation_full_job_id ( op ) : job_id = op . get_field ( 'job-id' ) task_id = op . get_field ( 'task-id' ) if task_id : return '%s.%s' % ( job_id , task_id ) else : return job_id
5401	def _get_logging_env ( self , logging_uri , user_project ) : if not logging_uri . endswith ( '.log' ) : raise ValueError ( 'Logging URI must end in ".log": {}' . format ( logging_uri ) ) logging_prefix = logging_uri [ : - len ( '.log' ) ] return { 'LOGGING_PATH' : '{}.log' . format ( logging_prefix ) , 'STDOUT_PATH' : '{}-stdout.log' . format ( logging_prefix ) , 'STDERR_PATH' : '{}-stderr.log' . format ( logging_prefix ) , 'USER_PROJECT' : user_project , }
4495	def remove ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To remove a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . target ) store = project . storage ( storage ) for f in store . files : if norm_remote_path ( f . path ) == remote_path : f . remove ( )
4191	def window_cauchy ( N , alpha = 3 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = 1. / ( 1. + ( alpha * n / ( N / 2. ) ) ** 2 ) return w
8404	def squish ( x , range = ( 0 , 1 ) , only_finite = True ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) finite = np . isfinite ( x ) if only_finite else True x [ np . logical_and ( x < range [ 0 ] , finite ) ] = range [ 0 ] x [ np . logical_and ( x > range [ 1 ] , finite ) ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
529	def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a
2987	def get_app_kwarg_dict ( appInstance = None ) : app = ( appInstance or current_app ) app_config = getattr ( app , 'config' , { } ) return { k . lower ( ) . replace ( 'cors_' , '' ) : app_config . get ( k ) for k in CONFIG_OPTIONS if app_config . get ( k ) is not None }
11281	def clone ( self ) : new_object = copy . copy ( self ) if new_object . next : new_object . next = new_object . next . clone ( ) return new_object
7845	def get_items ( self ) : ret = [ ] l = self . xpath_ctxt . xpathEval ( "d:item" ) if l is not None : for i in l : ret . append ( DiscoItem ( self , i ) ) return ret
13590	def sigma_prime ( self ) : return _np . sqrt ( self . emit / self . beta ( self . E ) )
935	def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model
11834	def connect ( self , A , B , distance = 1 ) : self . connect1 ( A , B , distance ) if not self . directed : self . connect1 ( B , A , distance )
3730	def third_property ( CASRN = None , T = False , P = False , V = False ) : r Third = None if V : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Pc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Pc = _Pc , Vc = None ) elif P : Tc_methods = Tc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Tc_methods and Vc_methods : _Tc = Tc ( CASRN = CASRN , Method = Tc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Tc = _Tc , Vc = _Vc , Pc = None ) elif T : Pc_methods = Pc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] Vc_methods = Vc ( CASRN , AvailableMethods = True ) [ 0 : - 2 ] if Pc_methods and Vc_methods : _Pc = Pc ( CASRN = CASRN , Method = Pc_methods [ 0 ] ) _Vc = Vc ( CASRN = CASRN , Method = Vc_methods [ 0 ] ) Third = critical_surface ( Pc = _Pc , Vc = _Vc , Tc = None ) else : raise Exception ( 'Error in function' ) if not Third : return None return Third
5805	def get_dh_params_length ( server_handshake_bytes ) : output = None dh_params_bytes = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0c' : dh_params_bytes = message_data break if dh_params_bytes : break if dh_params_bytes : output = int_from_bytes ( dh_params_bytes [ 0 : 2 ] ) * 8 return output
4580	def toggle ( s ) : is_numeric = ',' in s or s . startswith ( '0x' ) or s . startswith ( '#' ) c = name_to_color ( s ) return color_to_name ( c ) if is_numeric else str ( c )
4952	def get_no_record_response ( self , request ) : username , course_id , program_uuid , enterprise_customer_uuid = self . get_required_query_params ( request ) data = { self . REQUIRED_PARAM_USERNAME : username , self . REQUIRED_PARAM_ENTERPRISE_CUSTOMER : enterprise_customer_uuid , self . CONSENT_EXISTS : False , self . CONSENT_GRANTED : False , self . CONSENT_REQUIRED : False , } if course_id : data [ self . REQUIRED_PARAM_COURSE_ID ] = course_id if program_uuid : data [ self . REQUIRED_PARAM_PROGRAM_UUID ] = program_uuid return Response ( data , status = HTTP_200_OK )
11894	def readtxt ( filepath ) : with open ( filepath , 'rt' ) as f : lines = f . readlines ( ) return '' . join ( lines )
3100	def loadfile ( filename , cache = None ) : _SECRET_NAMESPACE = 'oauth2client:secrets#ns' if not cache : return _loadfile ( filename ) obj = cache . get ( filename , namespace = _SECRET_NAMESPACE ) if obj is None : client_type , client_info = _loadfile ( filename ) obj = { client_type : client_info } cache . set ( filename , obj , namespace = _SECRET_NAMESPACE ) return next ( six . iteritems ( obj ) )
4128	def readwav ( filename ) : from scipy . io . wavfile import read as readwav samplerate , signal = readwav ( filename ) return signal , samplerate
2395	def quadratic_weighted_kappa ( rater_a , rater_b , min_rating = None , max_rating = None ) : assert ( len ( rater_a ) == len ( rater_b ) ) rater_a = [ int ( a ) for a in rater_a ] rater_b = [ int ( b ) for b in rater_b ] if min_rating is None : min_rating = min ( rater_a + rater_b ) if max_rating is None : max_rating = max ( rater_a + rater_b ) conf_mat = confusion_matrix ( rater_a , rater_b , min_rating , max_rating ) num_ratings = len ( conf_mat ) num_scored_items = float ( len ( rater_a ) ) hist_rater_a = histogram ( rater_a , min_rating , max_rating ) hist_rater_b = histogram ( rater_b , min_rating , max_rating ) numerator = 0.0 denominator = 0.0 if ( num_ratings > 1 ) : for i in range ( num_ratings ) : for j in range ( num_ratings ) : expected_count = ( hist_rater_a [ i ] * hist_rater_b [ j ] / num_scored_items ) d = pow ( i - j , 2.0 ) / pow ( num_ratings - 1 , 2.0 ) numerator += d * conf_mat [ i ] [ j ] / num_scored_items denominator += d * expected_count / num_scored_items return 1.0 - numerator / denominator else : return 1.0
12105	def _qsub_block ( self , output_dir , error_dir , tid_specs ) : processes = [ ] job_names = [ ] for ( tid , spec ) in tid_specs : job_name = "%s_%s_tid_%d" % ( self . batch_name , self . job_timestamp , tid ) job_names . append ( job_name ) cmd_args = self . command ( self . command . _formatter ( spec ) , tid , self . _launchinfo ) popen_args = self . _qsub_args ( [ ( "-e" , error_dir ) , ( '-N' , job_name ) , ( "-o" , output_dir ) ] , cmd_args ) p = subprocess . Popen ( popen_args , stdout = subprocess . PIPE ) ( stdout , stderr ) = p . communicate ( ) self . debug ( stdout ) if p . poll ( ) != 0 : raise EnvironmentError ( "qsub command exit with code: %d" % p . poll ( ) ) processes . append ( p ) self . message ( "Invoked qsub for %d commands" % len ( processes ) ) if ( self . reduction_fn is not None ) or self . dynamic : self . _qsub_collate_and_launch ( output_dir , error_dir , job_names )
1400	def extract_logical_plan ( self , topology ) : logicalPlan = { "spouts" : { } , "bolts" : { } , } for spout in topology . spouts ( ) : spoutName = spout . comp . name spoutType = "default" spoutSource = "NA" spoutVersion = "NA" spoutConfigs = spout . comp . config . kvs for kvs in spoutConfigs : if kvs . key == "spout.type" : spoutType = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.source" : spoutSource = javaobj . loads ( kvs . serialized_value ) elif kvs . key == "spout.version" : spoutVersion = javaobj . loads ( kvs . serialized_value ) spoutPlan = { "config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , "type" : spoutType , "source" : spoutSource , "version" : spoutVersion , "outputs" : [ ] } for outputStream in list ( spout . outputs ) : spoutPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan for bolt in topology . bolts ( ) : boltName = bolt . comp . name boltPlan = { "config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , "outputs" : [ ] , "inputs" : [ ] } for outputStream in list ( bolt . outputs ) : boltPlan [ "outputs" ] . append ( { "stream_name" : outputStream . stream . id } ) for inputStream in list ( bolt . inputs ) : boltPlan [ "inputs" ] . append ( { "stream_name" : inputStream . stream . id , "component_name" : inputStream . stream . component_name , "grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) } ) logicalPlan [ "bolts" ] [ boltName ] = boltPlan return logicalPlan
8044	def leapfrog ( self , kind , value = None ) : while self . current is not None : if self . current . kind == kind and ( value is None or self . current . value == value ) : self . consume ( kind ) return self . stream . move ( )
1307	def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked
3446	def add_mip_obj ( model ) : if len ( model . variables ) > 1e4 : LOGGER . warning ( "the MIP version of minimal media is extremely slow for" " models that large :(" ) exchange_rxns = find_boundary_types ( model , "exchange" ) big_m = max ( abs ( b ) for r in exchange_rxns for b in r . bounds ) prob = model . problem coefs = { } to_add = [ ] for rxn in exchange_rxns : export = len ( rxn . reactants ) == 1 indicator = prob . Variable ( "ind_" + rxn . id , lb = 0 , ub = 1 , type = "binary" ) if export : vrv = rxn . reverse_variable indicator_const = prob . Constraint ( vrv - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) else : vfw = rxn . forward_variable indicator_const = prob . Constraint ( vfw - indicator * big_m , ub = 0 , name = "ind_constraint_" + rxn . id ) to_add . extend ( [ indicator , indicator_const ] ) coefs [ indicator ] = 1 model . add_cons_vars ( to_add ) model . solver . update ( ) model . objective . set_linear_coefficients ( coefs ) model . objective . direction = "min"
10202	def register_aggregations ( ) : return [ dict ( aggregation_name = 'file-download-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_file_download' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'file-download' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( file_key = 'file_key' , bucket_id = 'bucket_id' , file_id = 'file_id' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , 'volume' : ( 'sum' , 'size' , { } ) , } , ) ) , dict ( aggregation_name = 'record-view-agg' , templates = 'invenio_stats.contrib.aggregations.aggr_record_view' , aggregator_class = StatAggregator , aggregator_config = dict ( client = current_search_client , event = 'record-view' , aggregation_field = 'unique_id' , aggregation_interval = 'day' , copy_fields = dict ( record_id = 'record_id' , pid_type = 'pid_type' , pid_value = 'pid_value' , ) , metric_aggregation_fields = { 'unique_count' : ( 'cardinality' , 'unique_session_id' , { 'precision_threshold' : 1000 } ) , } , ) ) ]
9589	def init ( self ) : resp = self . _execute ( Command . NEW_SESSION , { 'desiredCapabilities' : self . desired_capabilities } , False ) resp . raise_for_status ( ) self . session_id = str ( resp . session_id ) self . capabilities = resp . value
8251	def swatch ( self , x , y , w = 35 , h = 35 , roundness = 0 ) : _ctx . fill ( self ) _ctx . rect ( x , y , w , h , roundness )
1630	def CheckForHeaderGuard ( filename , clean_lines , error ) : raw_lines = clean_lines . lines_without_raw_strings for i in raw_lines : if Search ( r'//\s*NOLINT\(build/header_guard\)' , i ) : return for i in raw_lines : if Search ( r'^\s*#pragma\s+once' , i ) : return cppvar = GetHeaderGuardCPPVariable ( filename ) ifndef = '' ifndef_linenum = 0 define = '' endif = '' endif_linenum = 0 for linenum , line in enumerate ( raw_lines ) : linesplit = line . split ( ) if len ( linesplit ) >= 2 : if not ifndef and linesplit [ 0 ] == '#ifndef' : ifndef = linesplit [ 1 ] ifndef_linenum = linenum if not define and linesplit [ 0 ] == '#define' : define = linesplit [ 1 ] if line . startswith ( '#endif' ) : endif = line endif_linenum = linenum if not ifndef or not define or ifndef != define : error ( filename , 0 , 'build/header_guard' , 5 , 'No #ifndef header guard found, suggested CPP variable is: %s' % cppvar ) return if ifndef != cppvar : error_level = 0 if ifndef != cppvar + '_' : error_level = 5 ParseNolintSuppressions ( filename , raw_lines [ ifndef_linenum ] , ifndef_linenum , error ) error ( filename , ifndef_linenum , 'build/header_guard' , error_level , '#ifndef header guard has wrong style, please use: %s' % cppvar ) ParseNolintSuppressions ( filename , raw_lines [ endif_linenum ] , endif_linenum , error ) match = Match ( r'#endif\s*//\s*' + cppvar + r'(_)?\b' , endif ) if match : if match . group ( 1 ) == '_' : error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif // %s"' % cppvar ) return no_single_line_comments = True for i in xrange ( 1 , len ( raw_lines ) - 1 ) : line = raw_lines [ i ] if Match ( r'^(?:(?:\'(?:\.|[^\'])*\')|(?:"(?:\.|[^"])*")|[^\'"])*//' , line ) : no_single_line_comments = False break if no_single_line_comments : match = Match ( r'#endif\s*/\*\s*' + cppvar + r'(_)?\s*\*/' , endif ) if match : if match . group ( 1 ) == '_' : error ( filename , endif_linenum , 'build/header_guard' , 0 , '#endif line should be "#endif /* %s */"' % cppvar ) return error ( filename , endif_linenum , 'build/header_guard' , 5 , '#endif line should be "#endif // %s"' % cppvar )
7113	def predict ( self , X ) : x = X if not isinstance ( X , list ) : x = [ X ] y = self . estimator . predict ( x ) y = [ item [ 0 ] for item in y ] y = [ self . _remove_prefix ( label ) for label in y ] if not isinstance ( X , list ) : y = y [ 0 ] return y
10776	def finalize ( self , result = None ) : if not self . settings_path : return from django . test . utils import teardown_test_environment from django . db import connection from django . conf import settings self . call_plugins_method ( 'beforeDestroyTestDb' , settings , connection ) try : connection . creation . destroy_test_db ( self . old_db , verbosity = self . verbosity , ) except Exception : pass self . call_plugins_method ( 'afterDestroyTestDb' , settings , connection ) self . call_plugins_method ( 'beforeTeardownTestEnv' , settings , teardown_test_environment ) teardown_test_environment ( ) self . call_plugins_method ( 'afterTeardownTestEnv' , settings )
12951	def _get_new_connection ( self ) : pool = getRedisPool ( self . mdl . REDIS_CONNECTION_PARAMS ) return redis . Redis ( connection_pool = pool )
4166	def tf2zp ( b , a ) : from numpy import roots assert len ( b ) == len ( a ) , "length of the vectors a and b must be identical. fill with zeros if needed." g = b [ 0 ] / a [ 0 ] z = roots ( b ) p = roots ( a ) return z , p , g
713	def __startSearch ( self ) : params = _ClientJobUtils . makeSearchJobParamsDict ( options = self . _options , forRunning = True ) if self . _options [ "action" ] == "dryRun" : args = [ sys . argv [ 0 ] , "--params=%s" % ( json . dumps ( params ) ) ] print print "==================================================================" print "RUNNING PERMUTATIONS INLINE as \"DRY RUN\"..." print "==================================================================" jobID = hypersearch_worker . main ( args ) else : cmdLine = _setUpExports ( self . _options [ "exports" ] ) cmdLine += "$HYPERSEARCH" maxWorkers = self . _options [ "maxWorkers" ] jobID = self . __cjDAO . jobInsert ( client = "GRP" , cmdLine = cmdLine , params = json . dumps ( params ) , minimumWorkers = 1 , maximumWorkers = maxWorkers , jobType = self . __cjDAO . JOB_TYPE_HS ) cmdLine = "python -m nupic.swarming.hypersearch_worker" " --jobID=%d" % ( jobID ) self . _launchWorkers ( cmdLine , maxWorkers ) searchJob = _HyperSearchJob ( jobID ) self . __saveHyperSearchJobID ( permWorkDir = self . _options [ "permWorkDir" ] , outputLabel = self . _options [ "outputLabel" ] , hyperSearchJob = searchJob ) if self . _options [ "action" ] == "dryRun" : print "Successfully executed \"dry-run\" hypersearch, jobID=%d" % ( jobID ) else : print "Successfully submitted new HyperSearch job, jobID=%d" % ( jobID ) _emit ( Verbosity . DEBUG , "Each worker executing the command line: %s" % ( cmdLine , ) ) return searchJob
6021	def from_fits_renormalized ( cls , file_path , hdu , pixel_scale ) : psf = PSF . from_fits_with_scale ( file_path , hdu , pixel_scale ) psf [ : , : ] = np . divide ( psf , np . sum ( psf ) ) return psf
11323	def check_pkgs_integrity ( filelist , logger , ftp_connector , timeout = 120 , sleep_time = 10 ) : ref_1 = [ ] ref_2 = [ ] i = 1 print >> sys . stdout , "\nChecking packages integrity." for filename in filelist : get_remote_file_size ( ftp_connector , filename , ref_1 ) print >> sys . stdout , "\nGoing to sleep for %i sec." % ( sleep_time , ) time . sleep ( sleep_time ) while sleep_time * i < timeout : for filename in filelist : get_remote_file_size ( ftp_connector , filename , ref_2 ) if ref_1 == ref_2 : print >> sys . stdout , "\nIntegrity OK:)" logger . info ( "Packages integrity OK." ) break else : print >> sys . stdout , "\nWaiting %d time for itegrity..." % ( i , ) logger . info ( "\nWaiting %d time for itegrity..." % ( i , ) ) i += 1 ref_1 , ref_2 = ref_2 , [ ] time . sleep ( sleep_time ) else : not_finished_files = [ ] for count , val1 in enumerate ( ref_1 ) : if val1 != ref_2 [ count ] : not_finished_files . append ( filelist [ count ] ) print >> sys . stdout , "\nOMG, OMG something wrong with integrity." logger . error ( "Integrity check faild for files %s" % ( not_finished_files , ) )
5026	def get_result ( self , course_grade ) : return Result ( score = Score ( scaled = course_grade . percent , raw = course_grade . percent * 100 , min = MIN_SCORE , max = MAX_SCORE , ) , success = course_grade . passed , completion = course_grade . passed )
4738	def warn ( txt ) : print ( "%s# %s%s%s" % ( PR_WARN_CC , get_time_stamp ( ) , txt , PR_NC ) ) sys . stdout . flush ( )
3047	def _do_retrieve_scopes ( self , http , token ) : logger . info ( 'Refreshing scopes' ) query_params = { 'access_token' : token , 'fields' : 'scope' } token_info_uri = _helpers . update_query_params ( self . token_info_uri , query_params ) resp , content = transport . request ( http , token_info_uri ) content = _helpers . _from_bytes ( content ) if resp . status == http_client . OK : d = json . loads ( content ) self . scopes = set ( _helpers . string_to_scopes ( d . get ( 'scope' , '' ) ) ) else : error_msg = 'Invalid response {0}.' . format ( resp . status ) try : d = json . loads ( content ) if 'error_description' in d : error_msg = d [ 'error_description' ] except ( TypeError , ValueError ) : pass raise Error ( error_msg )
8421	def _format ( formatter , x ) : formatter . create_dummy_axis ( ) formatter . set_locs ( [ val for val in x if ~ np . isnan ( val ) ] ) try : oom = int ( formatter . orderOfMagnitude ) except AttributeError : oom = 0 labels = [ formatter ( tick ) for tick in x ] pattern = re . compile ( r'\.0+$' ) for i , label in enumerate ( labels ) : match = pattern . search ( label ) if match : labels [ i ] = pattern . sub ( '' , label ) if oom : labels = [ '{}e{}' . format ( s , oom ) if s != '0' else s for s in labels ] return labels
11608	def add ( self , addend_mat , axis = 1 ) : if self . finalized : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] + addend_mat elif axis == 2 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
11057	def _remove_by_pk ( self , key , flush = True ) : try : del self . store [ key ] except Exception as error : pass if flush : self . flush ( )
10811	def query_by_names ( cls , names ) : assert isinstance ( names , list ) return cls . query . filter ( cls . name . in_ ( names ) )
885	def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )
9934	def get_finder ( import_path ) : Finder = import_string ( import_path ) if not issubclass ( Finder , BaseFinder ) : raise ImproperlyConfigured ( 'Finder "%s" is not a subclass of "%s"' % ( Finder , BaseFinder ) ) return Finder ( )
12474	def join_path_to_filelist ( path , filelist ) : return [ op . join ( path , str ( item ) ) for item in filelist ]
8295	def cliques ( graph , threshold = 3 ) : cliques = [ ] for n in graph . nodes : c = clique ( graph , n . id ) if len ( c ) >= threshold : c . sort ( ) if c not in cliques : cliques . append ( c ) return cliques
13849	def get_time ( filename ) : ts = os . stat ( filename ) . st_mtime return datetime . datetime . utcfromtimestamp ( ts )
6799	def database_renderer ( self , name = None , site = None , role = None ) : name = name or self . env . default_db_name site = site or self . genv . SITE role = role or self . genv . ROLE key = ( name , site , role ) self . vprint ( 'checking key:' , key ) if key not in self . _database_renderers : self . vprint ( 'No cached db renderer, generating...' ) if self . verbose : print ( 'db.name:' , name ) print ( 'db.databases:' , self . env . databases ) print ( 'db.databases[%s]:' % name , self . env . databases . get ( name ) ) d = type ( self . genv ) ( self . lenv ) d . update ( self . get_database_defaults ( ) ) d . update ( self . env . databases . get ( name , { } ) ) d [ 'db_name' ] = name if self . verbose : print ( 'db.d:' ) pprint ( d , indent = 4 ) print ( 'db.connection_handler:' , d . connection_handler ) if d . connection_handler == CONNECTION_HANDLER_DJANGO : self . vprint ( 'Using django handler...' ) dj = self . get_satchel ( 'dj' ) if self . verbose : print ( 'Loading Django DB settings for site {} and role {}.' . format ( site , role ) , file = sys . stderr ) dj . set_db ( name = name , site = site , role = role ) _d = dj . local_renderer . collect_genv ( include_local = True , include_global = False ) for k , v in _d . items ( ) : if k . startswith ( 'dj_db_' ) : _d [ k [ 3 : ] ] = v del _d [ k ] if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) elif d . connection_handler and d . connection_handler . startswith ( CONNECTION_HANDLER_CUSTOM + ':' ) : _callable_str = d . connection_handler [ len ( CONNECTION_HANDLER_CUSTOM + ':' ) : ] self . vprint ( 'Using custom handler %s...' % _callable_str ) _d = str_to_callable ( _callable_str ) ( role = self . genv . ROLE ) if self . verbose : print ( 'Loaded:' ) pprint ( _d ) d . update ( _d ) r = LocalRenderer ( self , lenv = d ) self . set_root_login ( r ) self . _database_renderers [ key ] = r else : self . vprint ( 'Cached db renderer found.' ) return self . _database_renderers [ key ]
4662	def proposal ( self , proposer = None , proposal_expiration = None , proposal_review = None ) : if not self . _propbuffer : return self . new_proposal ( self . tx ( ) , proposer , proposal_expiration , proposal_review ) if proposer : self . _propbuffer [ 0 ] . set_proposer ( proposer ) if proposal_expiration : self . _propbuffer [ 0 ] . set_expiration ( proposal_expiration ) if proposal_review : self . _propbuffer [ 0 ] . set_review ( proposal_review ) return self . _propbuffer [ 0 ]
13336	def module_resolver ( resolver , path ) : if resolver . resolved : if isinstance ( resolver . resolved [ 0 ] , VirtualEnvironment ) : env = resolver . resolved [ 0 ] mod = env . get_module ( path ) if mod : return mod raise ResolveError
10325	def _binomial_pmf ( n , p ) : n = int ( n ) ret = np . empty ( n + 1 ) nmax = int ( np . round ( p * n ) ) ret [ nmax ] = 1.0 old_settings = np . seterr ( under = 'ignore' ) for i in range ( nmax + 1 , n + 1 ) : ret [ i ] = ret [ i - 1 ] * ( n - i + 1.0 ) / i * p / ( 1.0 - p ) for i in range ( nmax - 1 , - 1 , - 1 ) : ret [ i ] = ret [ i + 1 ] * ( i + 1.0 ) / ( n - i ) * ( 1.0 - p ) / p np . seterr ( ** old_settings ) return ret / ret . sum ( )
2100	def read ( self , * args , ** kwargs ) : if 'actor' in kwargs : kwargs [ 'actor' ] = kwargs . pop ( 'actor' ) r = super ( Resource , self ) . read ( * args , ** kwargs ) if 'results' in r : for d in r [ 'results' ] : self . _promote_actor ( d ) else : self . _promote_actor ( d ) return r
10935	def check_update_J ( self ) : self . _J_update_counter += 1 update = self . _J_update_counter >= self . update_J_frequency return update & ( not self . _fresh_JTJ )
5397	def _delocalize_outputs_commands ( self , task_dir , outputs , user_project ) : commands = [ ] for o in outputs : if o . recursive or not o . value : continue dest_path = o . uri . path local_path = task_dir + '/' + _DATA_SUBDIR + '/' + o . docker_path if o . file_provider == job_model . P_LOCAL : commands . append ( 'mkdir -p "%s"' % dest_path ) if o . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , local_path , dest_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( local_path , dest_path ) commands . append ( command ) return '\n' . join ( commands )
4184	def window_bohman ( N ) : r x = linspace ( - 1 , 1 , N ) w = ( 1. - abs ( x ) ) * cos ( pi * abs ( x ) ) + 1. / pi * sin ( pi * abs ( x ) ) return w
10163	def load ( self ) : ret = { } with open ( self . get_path ( ) , 'r' ) as f : lines = f . readlines ( ) ret [ 'personalities' ] = self . get_personalities ( lines [ 0 ] ) ret [ 'arrays' ] = self . get_arrays ( lines [ 1 : - 1 ] , ret [ 'personalities' ] ) self . content = reduce ( lambda x , y : x + y , lines ) return ret
12056	def ftp_folder_match ( ftp , localFolder , deleteStuff = True ) : for fname in glob . glob ( localFolder + "/*.*" ) : ftp_upload ( ftp , fname ) return
10759	def from_curvilinear ( cls , x , y , z , formatter = numpy_formatter ) : return cls ( x , y , z , formatter )
569	def _getReportItem ( itemName , results ) : subKeys = itemName . split ( ':' ) subResults = results for subKey in subKeys : subResults = subResults [ subKey ] return subResults
6646	def _mirrorStructure ( dictionary , value ) : result = type ( dictionary ) ( ) for k in dictionary . keys ( ) : if isinstance ( dictionary [ k ] , dict ) : result [ k ] = _mirrorStructure ( dictionary [ k ] , value ) else : result [ k ] = value return result
10240	def count_author_publications ( graph : BELGraph ) -> typing . Counter [ str ] : authors = group_as_dict ( _iter_author_publiations ( graph ) ) return Counter ( count_dict_values ( count_defaultdict ( authors ) ) )
9759	def statuses ( ctx , job , page ) : def get_experiment_statuses ( ) : try : response = PolyaxonClient ( ) . experiment . get_statuses ( user , project_name , _experiment , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could get status for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for experiment `{}`.' . format ( _experiment ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for experiment `{}`.' . format ( _experiment ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'experiment' , None ) dict_tabulate ( objects , is_list_dict = True ) def get_experiment_job_statuses ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_statuses ( user , project_name , _experiment , _job , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get status for job `{}`.' . format ( job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Statuses for Job `{}`.' . format ( _job ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No statuses found for job `{}`.' . format ( _job ) ) objects = list_dicts_to_tabulate ( [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) , status_key = 'status' ) for o in response [ 'results' ] ] ) if objects : Printer . print_header ( "Statuses:" ) objects . pop ( 'job' , None ) dict_tabulate ( objects , is_list_dict = True ) page = page or 1 user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_statuses ( ) else : get_experiment_statuses ( )
8866	def make_python_patterns ( additional_keywords = [ ] , additional_builtins = [ ] ) : kw = r"\b" + any ( "keyword" , kwlist + additional_keywords ) + r"\b" kw_namespace = r"\b" + any ( "namespace" , kw_namespace_list ) + r"\b" word_operators = r"\b" + any ( "operator_word" , wordop_list ) + r"\b" builtinlist = [ str ( name ) for name in dir ( builtins ) if not name . startswith ( '_' ) ] + additional_builtins for v in [ 'None' , 'True' , 'False' ] : builtinlist . remove ( v ) builtin = r"([^.'\"\\#]\b|^)" + any ( "builtin" , builtinlist ) + r"\b" builtin_fct = any ( "builtin_fct" , [ r'_{2}[a-zA-Z_]*_{2}' ] ) comment = any ( "comment" , [ r"#[^\n]*" ] ) instance = any ( "instance" , [ r"\bself\b" , r"\bcls\b" ] ) decorator = any ( 'decorator' , [ r'@\w*' , r'.setter' ] ) number = any ( "number" , [ r"\b[+-]?[0-9]+[lLjJ]?\b" , r"\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\b" , r"\b[+-]?0[oO][0-7]+[lL]?\b" , r"\b[+-]?0[bB][01]+[lL]?\b" , r"\b[+-]?[0-9]+(?:\.[0-9]+)?(?:[eE][+-]?[0-9]+)?[jJ]?\b" ] ) sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*'?" dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*"?' uf_sqstring = r"(\b[rRuU])?'[^'\\\n]*(\\.[^'\\\n]*)*(\\)$(?!')$" uf_dqstring = r'(\b[rRuU])?"[^"\\\n]*(\\.[^"\\\n]*)*(\\)$(?!")$' sq3string = r"(\b[rRuU])?)?" dq3string = r'(\b[rRuU])?)?' uf_sq3string = r"(\b[rRuU])?)$" uf_dq3string = r'(\b[rRuU])?)$' string = any ( "string" , [ sq3string , dq3string , sqstring , dqstring ] ) ufstring1 = any ( "uf_sqstring" , [ uf_sqstring ] ) ufstring2 = any ( "uf_dqstring" , [ uf_dqstring ] ) ufstring3 = any ( "uf_sq3string" , [ uf_sq3string ] ) ufstring4 = any ( "uf_dq3string" , [ uf_dq3string ] ) return "|" . join ( [ instance , decorator , kw , kw_namespace , builtin , word_operators , builtin_fct , comment , ufstring1 , ufstring2 , ufstring3 , ufstring4 , string , number , any ( "SYNC" , [ r"\n" ] ) ] )
7964	def event ( self , event ) : logger . debug ( u"TCP transport event: {0}" . format ( event ) ) if self . _stream : event . stream = self . _stream self . _event_queue . put ( event )
1890	def minmax ( self , constraints , x , iters = 10000 ) : if issymbolic ( x ) : m = self . min ( constraints , x , iters ) M = self . max ( constraints , x , iters ) return m , M else : return x , x
7953	def handle_write ( self ) : with self . lock : logger . debug ( "handle_write: queue: {0!r}" . format ( self . _write_queue ) ) try : job = self . _write_queue . popleft ( ) except IndexError : return if isinstance ( job , WriteData ) : self . _do_write ( job . data ) elif isinstance ( job , ContinueConnect ) : self . _continue_connect ( ) elif isinstance ( job , StartTLS ) : self . _initiate_starttls ( ** job . kwargs ) elif isinstance ( job , TLSHandshake ) : self . _continue_tls_handshake ( ) else : raise ValueError ( "Unrecognized job in the write queue: " "{0!r}" . format ( job ) )
12660	def copy ( configfile = '' , destpath = '' , overwrite = False , sub_node = '' ) : log . info ( 'Running {0} {1} {2}' . format ( os . path . basename ( __file__ ) , whoami ( ) , locals ( ) ) ) assert ( os . path . isfile ( configfile ) ) if os . path . exists ( destpath ) : if os . listdir ( destpath ) : raise FolderAlreadyExists ( 'Folder {0} already exists. Please clean ' 'it or change destpath.' . format ( destpath ) ) else : log . info ( 'Creating folder {0}' . format ( destpath ) ) path ( destpath ) . makedirs_p ( ) from boyle . files . file_tree_map import FileTreeMap file_map = FileTreeMap ( ) try : file_map . from_config_file ( configfile ) except Exception as e : raise FileTreeMapError ( str ( e ) ) if sub_node : sub_map = file_map . get_node ( sub_node ) if not sub_map : raise FileTreeMapError ( 'Could not find sub node ' '{0}' . format ( sub_node ) ) file_map . _filetree = { } file_map . _filetree [ sub_node ] = sub_map try : file_map . copy_to ( destpath , overwrite = overwrite ) except Exception as e : raise FileTreeMapError ( str ( e ) )
7548	def cluster_info ( ipyclient , spacer = "" ) : hosts = [ ] for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : hosts . append ( engine . apply ( _socket . gethostname ) ) hosts = [ i . get ( ) for i in hosts ] result = [ ] for hostname in set ( hosts ) : result . append ( "{}host compute node: [{} cores] on {}" . format ( spacer , hosts . count ( hostname ) , hostname ) ) print "\n" . join ( result )
7226	def paint ( self ) : snippet = { 'line-opacity' : VectorStyle . get_style_value ( self . opacity ) , 'line-color' : VectorStyle . get_style_value ( self . color ) , 'line-width' : VectorStyle . get_style_value ( self . width ) , } if self . translate : snippet [ 'line-translate' ] = self . translate if self . dasharray : snippet [ 'line-dasharray' ] = VectorStyle . get_style_value ( self . dasharray ) return snippet
9022	def add_row ( self , id_ ) : row = self . _parser . new_row ( id_ ) self . _rows . append ( row ) return row
10979	def approve ( group_id , user_id ) : membership = Membership . query . get_or_404 ( ( user_id , group_id ) ) group = membership . group if group . can_edit ( current_user ) : try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( '%(user)s accepted to %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.requests' , group_id = membership . group . id ) ) flash ( _ ( 'You cannot approve memberships for the group %(group_name)s' , group_name = group . name ) , 'error' ) return redirect ( url_for ( '.index' ) )
12456	def install ( env , requirements , args , ignore_activated = False , install_dev_requirements = False , quiet = False ) : if os . path . isfile ( requirements ) : args += ( '-r' , requirements ) label = 'project' else : args += ( '-U' , '-e' , '.' ) label = 'library' if install_dev_requirements : dev_requirements = None dirname = os . path . dirname ( requirements ) basename , ext = os . path . splitext ( os . path . basename ( requirements ) ) for delimiter in ( '-' , '_' , '' ) : filename = os . path . join ( dirname , '' . join ( ( basename , delimiter , 'dev' , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break filename = os . path . join ( dirname , '' . join ( ( 'dev' , delimiter , basename , ext ) ) ) if os . path . isfile ( filename ) : dev_requirements = filename break if dev_requirements : args += ( '-r' , dev_requirements ) if not quiet : print_message ( '== Step 2. Install {0} ==' . format ( label ) ) result = not pip_cmd ( env , ( 'install' , ) + args , ignore_activated , echo = not quiet ) if not quiet : print_message ( ) return result
407	def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )
3718	def estimate ( self ) : self . mul ( 300 ) self . Cpig ( 300 ) estimates = { 'Tb' : self . Tb ( self . counts ) , 'Tm' : self . Tm ( self . counts ) , 'Tc' : self . Tc ( self . counts , self . Tb_estimated ) , 'Pc' : self . Pc ( self . counts , self . atom_count ) , 'Vc' : self . Vc ( self . counts ) , 'Hf' : self . Hf ( self . counts ) , 'Gf' : self . Gf ( self . counts ) , 'Hfus' : self . Hfus ( self . counts ) , 'Hvap' : self . Hvap ( self . counts ) , 'mul' : self . mul , 'mul_coeffs' : self . calculated_mul_coeffs , 'Cpig' : self . Cpig , 'Cpig_coeffs' : self . calculated_Cpig_coeffs } return estimates
12682	def row ( self , idx ) : return DataFrameRow ( idx , [ x [ idx ] for x in self ] , self . colnames )
212	def from_uint8 ( arr_uint8 , shape , min_value = 0.0 , max_value = 1.0 ) : arr_0to1 = arr_uint8 . astype ( np . float32 ) / 255.0 return HeatmapsOnImage . from_0to1 ( arr_0to1 , shape , min_value = min_value , max_value = max_value )
10657	def amounts ( masses ) : return { compound : amount ( compound , masses [ compound ] ) for compound in masses . keys ( ) }
10599	def create_template ( material , path , show = False ) : file_name = 'dataset-%s.csv' % material . lower ( ) file_path = os . path . join ( path , file_name ) with open ( file_path , 'w' , newline = '' ) as csvfile : writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE_MINIMAL ) writer . writerow ( [ 'Name' , material ] ) writer . writerow ( [ 'Description' , '<Add a data set description ' 'here.>' ] ) writer . writerow ( [ 'Reference' , '<Add a reference to the source of ' 'the data set here.>' ] ) writer . writerow ( [ 'Temperature' , '<parameter 1 name>' , '<parameter 2 name>' , '<parameter 3 name>' ] ) writer . writerow ( [ 'T' , '<parameter 1 display symbol>' , '<parameter 2 display symbol>' , '<parameter 3 display symbol>' ] ) writer . writerow ( [ 'K' , '<parameter 1 units>' , '<parameter 2 units>' , '<parameter 3 units>' ] ) writer . writerow ( [ 'T' , '<parameter 1 symbol>' , '<parameter 2 symbol>' , '<parameter 3 symbol>' ] ) for i in range ( 10 ) : writer . writerow ( [ 100.0 + i * 50 , float ( i ) , 10.0 + i , 100.0 + i ] ) if show is True : webbrowser . open_new ( file_path )
2655	def isdir ( self , path ) : result = True try : self . sftp_client . lstat ( path ) except FileNotFoundError : result = False return result
6676	def upload_template ( self , filename , destination , context = None , use_jinja = False , template_dir = None , use_sudo = False , backup = True , mirror_local_mode = False , mode = None , mkdir = False , chown = False , user = None ) : if mkdir : remote_dir = os . path . dirname ( destination ) if use_sudo : self . sudo ( 'mkdir -p %s' % quote ( remote_dir ) , user = user ) else : self . run ( 'mkdir -p %s' % quote ( remote_dir ) ) if not self . dryrun : _upload_template ( filename = filename , destination = destination , context = context , use_jinja = use_jinja , template_dir = template_dir , use_sudo = use_sudo , backup = backup , mirror_local_mode = mirror_local_mode , mode = mode , ) if chown : if user is None : user = self . genv . user run_as_root ( 'chown %s: %s' % ( user , quote ( destination ) ) )
11485	def upload ( file_pattern , destination = 'Private' , leaf_folders_as_items = False , reuse_existing = False ) : session . token = verify_credentials ( ) parent_folder_id = None user_folders = session . communicator . list_user_folders ( session . token ) if destination . startswith ( '/' ) : parent_folder_id = _find_resource_id_from_path ( destination ) else : for cur_folder in user_folders : if cur_folder [ 'name' ] == destination : parent_folder_id = cur_folder [ 'folder_id' ] if parent_folder_id is None : print ( 'Unable to locate specified destination. Defaulting to {0}.' . format ( user_folders [ 0 ] [ 'name' ] ) ) parent_folder_id = user_folders [ 0 ] [ 'folder_id' ] for current_file in glob . iglob ( file_pattern ) : current_file = os . path . normpath ( current_file ) if os . path . isfile ( current_file ) : print ( 'Uploading item from {0}' . format ( current_file ) ) _upload_as_item ( os . path . basename ( current_file ) , parent_folder_id , current_file , reuse_existing ) else : _upload_folder_recursive ( current_file , parent_folder_id , leaf_folders_as_items , reuse_existing )
1952	def perm ( lst , func ) : for i in range ( 1 , 2 ** len ( lst ) ) : yield [ func ( item ) if ( 1 << j ) & i else item for ( j , item ) in enumerate ( lst ) ]
1637	def CheckComment ( line , filename , linenum , next_line_start , error ) : commentpos = line . find ( '//' ) if commentpos != - 1 : if re . sub ( r'\\.' , '' , line [ 0 : commentpos ] ) . count ( '"' ) % 2 == 0 : if ( not ( Match ( r'^.*{ *//' , line ) and next_line_start == commentpos ) and ( ( commentpos >= 1 and line [ commentpos - 1 ] not in string . whitespace ) or ( commentpos >= 2 and line [ commentpos - 2 ] not in string . whitespace ) ) ) : error ( filename , linenum , 'whitespace/comments' , 2 , 'At least two spaces is best between code and comments' ) comment = line [ commentpos : ] match = _RE_PATTERN_TODO . match ( comment ) if match : leading_whitespace = match . group ( 1 ) if len ( leading_whitespace ) > 1 : error ( filename , linenum , 'whitespace/todo' , 2 , 'Too many spaces before TODO' ) username = match . group ( 2 ) if not username : error ( filename , linenum , 'readability/todo' , 2 , 'Missing username in TODO; it should look like ' '"// TODO(my_username): Stuff."' ) middle_whitespace = match . group ( 3 ) if middle_whitespace != ' ' and middle_whitespace != '' : error ( filename , linenum , 'whitespace/todo' , 2 , 'TODO(my_username) should be followed by a space' ) if ( Match ( r'//[^ ]*\w' , comment ) and not Match ( r'(///|//\!)(\s+|$)' , comment ) ) : error ( filename , linenum , 'whitespace/comments' , 4 , 'Should have a space between // and comment' )
13531	def ancestors ( self ) : ancestors = set ( [ ] ) self . _depth_ascend ( self , ancestors ) try : ancestors . remove ( self ) except KeyError : pass return list ( ancestors )
10817	def can_see_members ( self , user ) : if self . privacy_policy == PrivacyPolicy . PUBLIC : return True elif self . privacy_policy == PrivacyPolicy . MEMBERS : return self . is_member ( user ) or self . is_admin ( user ) elif self . privacy_policy == PrivacyPolicy . ADMINS : return self . is_admin ( user )
10427	def boilerplate ( name , contact , description , pmids , version , copyright , authors , licenses , disclaimer , output ) : from . document_utils import write_boilerplate write_boilerplate ( name = name , version = version , description = description , authors = authors , contact = contact , copyright = copyright , licenses = licenses , disclaimer = disclaimer , pmids = pmids , file = output , )
1499	def ack ( self , tup ) : if not isinstance ( tup , HeronTuple ) : Log . error ( "Only HeronTuple type is supported in ack()" ) return if self . acking_enabled : ack_tuple = tuple_pb2 . AckTuple ( ) ack_tuple . ackedtuple = int ( tup . id ) tuple_size_in_bytes = 0 for rt in tup . roots : to_add = ack_tuple . roots . add ( ) to_add . CopyFrom ( rt ) tuple_size_in_bytes += rt . ByteSize ( ) super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns )
11334	def table ( * columns , ** kwargs ) : ret = [ ] prefix = kwargs . get ( 'prefix' , '' ) buf_count = kwargs . get ( 'buf_count' , 2 ) if len ( columns ) == 1 : columns = list ( columns [ 0 ] ) else : columns = list ( zip ( * columns ) ) headers = kwargs . get ( "headers" , [ ] ) if headers : columns . insert ( 0 , headers ) widths = kwargs . get ( "widths" , [ ] ) row_counts = Counter ( ) for i in range ( len ( widths ) ) : row_counts [ i ] = int ( widths [ i ] ) width = int ( kwargs . get ( "width" , 0 ) ) for row in columns : for i , c in enumerate ( row ) : if isinstance ( c , basestring ) : cl = len ( c ) else : cl = len ( str ( c ) ) if cl > row_counts [ i ] : row_counts [ i ] = cl width = int ( kwargs . get ( "width" , 0 ) ) if width : for i in row_counts : if row_counts [ i ] < width : row_counts [ i ] = width def colstr ( c ) : if isinstance ( c , basestring ) : return c return str ( c ) def rowstr ( row , prefix , row_counts ) : row_format = prefix cols = list ( map ( colstr , row ) ) for i in range ( len ( row_counts ) ) : c = cols [ i ] if re . match ( r"^\d+(?:\.\d+)?$" , c ) : if i == 0 : row_format += "{:>" + str ( row_counts [ i ] ) + "}" else : row_format += "{:>" + str ( row_counts [ i ] + buf_count ) + "}" else : row_format += "{:<" + str ( row_counts [ i ] + buf_count ) + "}" return row_format . format ( * cols ) for row in columns : ret . append ( rowstr ( row , prefix , row_counts ) ) out ( os . linesep . join ( ret ) )
4459	def between ( a , b , inclusive_min = True , inclusive_max = True ) : return RangeValue ( a , b , inclusive_min = inclusive_min , inclusive_max = inclusive_max )
10055	def get ( self , pid , record , key , version_id , ** kwargs ) : try : obj = record . files [ str ( key ) ] . get_version ( version_id = version_id ) return self . make_response ( obj = obj or abort ( 404 ) , pid = pid , record = record ) except KeyError : abort ( 404 )
11541	def write ( self , pin , value , pwm = False ) : if type ( pin ) is list : for p in pin : self . write ( p , value , pwm ) return if pwm and type ( value ) is not int and type ( value ) is not float : raise TypeError ( 'pwm is set, but value is not a float or int' ) pin_id = self . _pin_mapping . get ( pin , None ) if pin_id : lpin = self . _pin_lin . get ( pin , None ) if lpin and type ( lpin [ 'write' ] ) is tuple : write_range = lpin [ 'write' ] value = self . _linear_interpolation ( value , * write_range ) self . _write ( pin_id , value , pwm ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
10764	def _random_token ( self , bits = 128 ) : alphabet = string . ascii_letters + string . digits + '-_' num_letters = int ( math . ceil ( bits / 6.0 ) ) return '' . join ( random . choice ( alphabet ) for i in range ( num_letters ) )
12498	def xfm_atlas_to_functional ( atlas_filepath , anatbrain_filepath , meanfunc_filepath , atlas2anat_nonlin_xfm_filepath , is_atlas2anat_inverted , anat2func_lin_xfm_filepath , atlasinanat_out_filepath , atlasinfunc_out_filepath , interp = 'nn' , rewrite = True , parallel = False ) : if is_atlas2anat_inverted : anat_to_mni_nl_inv = atlas2anat_nonlin_xfm_filepath else : output_dir = op . abspath ( op . dirname ( atlasinanat_out_filepath ) ) ext = get_extension ( atlas2anat_nonlin_xfm_filepath ) anat_to_mni_nl_inv = op . join ( output_dir , remove_ext ( op . basename ( atlas2anat_nonlin_xfm_filepath ) ) + '_inv' + ext ) invwarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'invwarp' ) applywarp_cmd = op . join ( '${FSLDIR}' , 'bin' , 'applywarp' ) fslsub_cmd = op . join ( '${FSLDIR}' , 'bin' , 'fsl_sub' ) if parallel : invwarp_cmd = fslsub_cmd + ' ' + invwarp_cmd applywarp_cmd = fslsub_cmd + ' ' + applywarp_cmd if rewrite or ( not is_atlas2anat_inverted and not op . exists ( anat_to_mni_nl_inv ) ) : log . debug ( 'Creating {}.\n' . format ( anat_to_mni_nl_inv ) ) cmd = invwarp_cmd + ' ' cmd += '-w {} ' . format ( atlas2anat_nonlin_xfm_filepath ) cmd += '-o {} ' . format ( anat_to_mni_nl_inv ) cmd += '-r {} ' . format ( anatbrain_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinanat_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinanat_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlas_filepath ) cmd += '--ref={} ' . format ( anatbrain_filepath ) cmd += '--warp={} ' . format ( anat_to_mni_nl_inv ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinanat_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd ) if rewrite or not op . exists ( atlasinfunc_out_filepath ) : log . debug ( 'Creating {}.\n' . format ( atlasinfunc_out_filepath ) ) cmd = applywarp_cmd + ' ' cmd += '--in={} ' . format ( atlasinanat_out_filepath ) cmd += '--ref={} ' . format ( meanfunc_filepath ) cmd += '--premat={} ' . format ( anat2func_lin_xfm_filepath ) cmd += '--interp={} ' . format ( interp ) cmd += '--out={} ' . format ( atlasinfunc_out_filepath ) log . debug ( 'Running {}' . format ( cmd ) ) check_call ( cmd )
1710	def Js ( val , Clamped = False ) : if isinstance ( val , PyJs ) : return val elif val is None : return undefined elif isinstance ( val , basestring ) : return PyJsString ( val , StringPrototype ) elif isinstance ( val , bool ) : return true if val else false elif isinstance ( val , float ) or isinstance ( val , int ) or isinstance ( val , long ) or ( NUMPY_AVAILABLE and isinstance ( val , ( numpy . int8 , numpy . uint8 , numpy . int16 , numpy . uint16 , numpy . int32 , numpy . uint32 , numpy . float32 , numpy . float64 ) ) ) : if val in NUM_BANK : return NUM_BANK [ val ] return PyJsNumber ( float ( val ) , NumberPrototype ) elif isinstance ( val , FunctionType ) : return PyJsFunction ( val , FunctionPrototype ) elif isinstance ( val , dict ) : temp = PyJsObject ( { } , ObjectPrototype ) for k , v in six . iteritems ( val ) : temp . put ( Js ( k ) , Js ( v ) ) return temp elif isinstance ( val , ( list , tuple ) ) : return PyJsArray ( val , ArrayPrototype ) elif isinstance ( val , JsObjectWrapper ) : return val . __dict__ [ '_obj' ] elif NUMPY_AVAILABLE and isinstance ( val , numpy . ndarray ) : if val . dtype == numpy . int8 : return PyJsInt8Array ( val , Int8ArrayPrototype ) elif val . dtype == numpy . uint8 and not Clamped : return PyJsUint8Array ( val , Uint8ArrayPrototype ) elif val . dtype == numpy . uint8 and Clamped : return PyJsUint8ClampedArray ( val , Uint8ClampedArrayPrototype ) elif val . dtype == numpy . int16 : return PyJsInt16Array ( val , Int16ArrayPrototype ) elif val . dtype == numpy . uint16 : return PyJsUint16Array ( val , Uint16ArrayPrototype ) elif val . dtype == numpy . int32 : return PyJsInt32Array ( val , Int32ArrayPrototype ) elif val . dtype == numpy . uint32 : return PyJsUint16Array ( val , Uint32ArrayPrototype ) elif val . dtype == numpy . float32 : return PyJsFloat32Array ( val , Float32ArrayPrototype ) elif val . dtype == numpy . float64 : return PyJsFloat64Array ( val , Float64ArrayPrototype ) else : return py_wrap ( val )
13515	def residual_resistance_coef ( slenderness , prismatic_coef , froude_number ) : Cr = cr ( slenderness , prismatic_coef , froude_number ) if math . isnan ( Cr ) : Cr = cr_nearest ( slenderness , prismatic_coef , froude_number ) return Cr
8095	def edge ( s , path , edge , alpha = 1.0 ) : path . moveto ( edge . node1 . x , edge . node1 . y ) if edge . node2 . style == BACK : path . curveto ( edge . node1 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , edge . node2 . x , edge . node2 . y , ) else : path . lineto ( edge . node2 . x , edge . node2 . y )
8020	async def websocket_accept ( self , message , stream_name ) : is_first = not self . applications_accepting_frames self . applications_accepting_frames . add ( stream_name ) if is_first : await self . accept ( )
3655	def stop_image_acquisition ( self ) : if self . is_acquiring_images : self . _is_acquiring_images = False if self . thread_image_acquisition . is_running : self . thread_image_acquisition . stop ( ) with MutexLocker ( self . thread_image_acquisition ) : self . device . node_map . AcquisitionStop . execute ( ) try : self . device . node_map . TLParamsLocked . value = 0 except LogicalErrorException : pass for data_stream in self . _data_streams : try : data_stream . stop_acquisition ( ACQ_STOP_FLAGS_LIST . ACQ_STOP_FLAGS_KILL ) except ( ResourceInUseException , TimeoutException ) as e : self . _logger . error ( e , exc_info = True ) data_stream . flush_buffer_queue ( ACQ_QUEUE_TYPE_LIST . ACQ_QUEUE_ALL_DISCARD ) for event_manager in self . _event_new_buffer_managers : event_manager . flush_event_queue ( ) if self . _create_ds_at_connection : self . _release_buffers ( ) else : self . _release_data_streams ( ) self . _has_acquired_1st_image = False self . _chunk_adapter . detach_buffer ( ) self . _logger . info ( '{0} stopped image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
12645	def set_aad_cache ( token , cache ) : set_config_value ( 'aad_token' , jsonpickle . encode ( token ) ) set_config_value ( 'aad_cache' , jsonpickle . encode ( cache ) )
9414	def _make_user_class ( session , name ) : attrs = session . eval ( 'fieldnames(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) methods = session . eval ( 'methods(%s);' % name , nout = 1 ) . ravel ( ) . tolist ( ) ref = weakref . ref ( session ) doc = _DocDescriptor ( ref , name ) values = dict ( __doc__ = doc , _name = name , _ref = ref , _attrs = attrs , __module__ = 'oct2py.dynamic' ) for method in methods : doc = _MethodDocDescriptor ( ref , name , method ) cls_name = '%s_%s' % ( name , method ) method_values = dict ( __doc__ = doc ) method_cls = type ( str ( cls_name ) , ( OctaveUserClassMethod , ) , method_values ) values [ method ] = method_cls ( ref , method , name ) for attr in attrs : values [ attr ] = OctaveUserClassAttr ( ref , attr , attr ) return type ( str ( name ) , ( OctaveUserClass , ) , values )
2307	def reset_parameters ( self ) : stdv = 1. / math . sqrt ( self . weight . size ( 1 ) ) self . weight . data . uniform_ ( - stdv , stdv ) if self . bias is not None : self . bias . data . uniform_ ( - stdv , stdv )
11846	def add_thing ( self , thing , location = None ) : if not isinstance ( thing , Thing ) : thing = Agent ( thing ) assert thing not in self . things , "Don't add the same thing twice" thing . location = location or self . default_location ( thing ) self . things . append ( thing ) if isinstance ( thing , Agent ) : thing . performance = 0 self . agents . append ( thing )
7484	def run2 ( data , samples , force , ipyclient ) : data . dirs . edits = os . path . join ( os . path . realpath ( data . paramsdict [ "project_dir" ] ) , data . name + "_edits" ) if not os . path . exists ( data . dirs . edits ) : os . makedirs ( data . dirs . edits ) subsamples = choose_samples ( samples , force ) if int ( data . paramsdict [ "filter_adapters" ] ) == 3 : if not data . _hackersonly [ "p3_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p3_adapters_extra" ] . append ( poly ) if not data . _hackersonly [ "p5_adapters_extra" ] : for poly in [ "A" * 8 , "T" * 8 , "C" * 8 , "G" * 8 ] : data . _hackersonly [ "p5_adapters_extra" ] . append ( poly ) else : data . _hackersonly [ "p5_adapters_extra" ] = [ ] data . _hackersonly [ "p3_adapters_extra" ] = [ ] subsamples = concat_reads ( data , subsamples , ipyclient ) lbview = ipyclient . load_balanced_view ( targets = ipyclient . ids [ : : 2 ] ) run_cutadapt ( data , subsamples , lbview ) assembly_cleanup ( data )
8522	def add_int ( self , name , min , max , warp = None ) : min , max = map ( int , ( min , max ) ) if max < min : raise ValueError ( 'variable %s: max < min error' % name ) if warp not in ( None , 'log' ) : raise ValueError ( 'variable %s: warp=%s is not supported. use ' 'None or "log",' % ( name , warp ) ) if min <= 0 and warp == 'log' : raise ValueError ( 'variable %s: log-warping requires min > 0' ) self . variables [ name ] = IntVariable ( name , min , max , warp )
2961	def __write ( self , containers , initialize = True ) : path = self . _state_file self . _assure_dir ( ) try : flags = os . O_WRONLY | os . O_CREAT if initialize : flags |= os . O_EXCL with os . fdopen ( os . open ( path , flags ) , "w" ) as f : yaml . safe_dump ( self . __base_state ( containers ) , f ) except OSError as err : if err . errno == errno . EEXIST : raise AlreadyInitializedError ( "Path %s exists. " "You may need to destroy a previous blockade." % path ) raise except Exception : self . _state_delete ( ) raise
10232	def list_abundance_cartesian_expansion ( graph : BELGraph ) -> None : for u , v , k , d in list ( graph . edges ( keys = True , data = True ) ) : if CITATION not in d : continue if isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for u_member , v_member in itt . product ( u . members , v . members ) : graph . add_qualified_edge ( u_member , v_member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( u , ListAbundance ) : for member in u . members : graph . add_qualified_edge ( member , v , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) elif isinstance ( v , ListAbundance ) : for member in v . members : graph . add_qualified_edge ( u , member , relation = d [ RELATION ] , citation = d . get ( CITATION ) , evidence = d . get ( EVIDENCE ) , annotations = d . get ( ANNOTATIONS ) , ) _remove_list_abundance_nodes ( graph )
6857	def create_user ( name , password , host = 'localhost' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE USER '%(name)s'@'%(host)s' IDENTIFIED BY '%(password)s';" % { 'name' : name , 'password' : password , 'host' : host } , ** kwargs ) puts ( "Created MySQL user '%s'." % name )
4413	def add ( self , requester : int , track : dict ) : self . queue . append ( AudioTrack ( ) . build ( track , requester ) )
584	def _deleteRecordsFromKNN ( self , recordsToDelete ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) idsToDelete = [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete ) assert knn . _numPatterns == nProtos - len ( idsToDelete )
3560	def list_services ( self ) : return map ( BluezGattService , get_provider ( ) . _get_objects ( _SERVICE_INTERFACE , self . _device . object_path ) )
8092	def node ( s , node , alpha = 1.0 ) : if s . depth : try : colors . shadow ( dx = 5 , dy = 5 , blur = 10 , alpha = 0.5 * alpha ) except : pass s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if s . fill : s . _ctx . fill ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * alpha ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * alpha * 3 ) r = node . r s . _ctx . oval ( node . x - r , node . y - r , r * 2 , r * 2 )
9112	def replies ( self ) : fs_reply_path = join ( self . fs_replies_path , 'message_001.txt' ) if exists ( fs_reply_path ) : return [ load ( open ( fs_reply_path , 'r' ) ) ] else : return [ ]
10659	def masses ( amounts ) : return { compound : mass ( compound , amounts [ compound ] ) for compound in amounts . keys ( ) }
6555	def copy ( self ) : return self . __class__ ( self . func , self . configurations , self . variables , self . vartype , name = self . name )
4544	def _add_redundant_arguments ( parser ) : parser . add_argument ( '-a' , '--animation' , default = None , help = 'Default animation type if no animation is specified' ) if deprecated . allowed ( ) : parser . add_argument ( '--dimensions' , '--dim' , default = None , help = 'DEPRECATED: x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '--shape' , default = None , help = 'x, (x, y) or (x, y, z) dimensions for project' ) parser . add_argument ( '-l' , '--layout' , default = None , help = 'Default layout class if no layout is specified' ) parser . add_argument ( '--numbers' , '-n' , default = 'python' , choices = NUMBER_TYPES , help = NUMBERS_HELP ) parser . add_argument ( '-p' , '--path' , default = None , help = PATH_HELP )
338	def _GetNextLogCountPerToken ( token ) : global _log_counter_per_token _log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) return _log_counter_per_token [ token ]
13150	def log_state ( entity , state ) : p = { 'on' : entity , 'state' : state } _log ( TYPE_CODES . STATE , p )
10414	def node_exclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def exclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node not in node_set return exclusion_filter
5696	def copy ( cls , conn , ** where ) : cur = conn . cursor ( ) if where and cls . copy_where : copy_where = cls . copy_where . format ( ** where ) else : copy_where = '' cur . execute ( 'INSERT INTO %s ' 'SELECT * FROM source.%s %s' % ( cls . table , cls . table , copy_where ) )
1795	def SBB ( cpu , dest , src ) : cpu . _SUB ( dest , src , carry = True )
2262	def find_duplicates ( items , k = 2 , key = None ) : duplicates = defaultdict ( list ) if key is None : for count , item in enumerate ( items ) : duplicates [ item ] . append ( count ) else : for count , item in enumerate ( items ) : duplicates [ key ( item ) ] . append ( count ) for key in list ( duplicates . keys ( ) ) : if len ( duplicates [ key ] ) < k : del duplicates [ key ] duplicates = dict ( duplicates ) return duplicates
117	def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( "Expected to get a generator as 'batches', got type %s. " + "Call map_batches() if you use lists." ) % ( type ( batches ) , ) gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch
10910	def name_globals ( s , remove_params = None ) : all_params = s . params for p in s . param_particle ( np . arange ( s . obj_get_positions ( ) . shape [ 0 ] ) ) : all_params . remove ( p ) if remove_params is not None : for p in set ( remove_params ) : all_params . remove ( p ) return all_params
11387	def call_path ( self , basepath ) : rel_filepath = self . path if basepath : rel_filepath = os . path . relpath ( self . path , basepath ) basename = self . name if basename in set ( [ '__init__.py' , '__main__.py' ] ) : rel_filepath = os . path . dirname ( rel_filepath ) return rel_filepath
2271	def _win32_symlink ( path , link , verbose = 0 ) : from ubelt import util_cmd if os . path . isdir ( path ) : if verbose : print ( '... as directory symlink' ) command = 'mklink /D "{}" "{}"' . format ( link , path ) else : if verbose : print ( '... as file symlink' ) command = 'mklink "{}" "{}"' . format ( link , path ) if command is not None : info = util_cmd . cmd ( command , shell = True ) if info [ 'ret' ] != 0 : from ubelt import util_format permission_msg = 'You do not have sufficient privledges' if permission_msg not in info [ 'err' ] : print ( 'Failed command:' ) print ( info [ 'command' ] ) print ( util_format . repr2 ( info , nl = 1 ) ) raise OSError ( str ( info ) ) return link
4328	def echo ( self , gain_in = 0.8 , gain_out = 0.9 , n_echos = 1 , delays = [ 60 ] , decays = [ 0.4 ] ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not isinstance ( n_echos , int ) or n_echos <= 0 : raise ValueError ( "n_echos must be a positive integer." ) if not isinstance ( delays , list ) : raise ValueError ( "delays must be a list" ) if len ( delays ) != n_echos : raise ValueError ( "the length of delays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 ) for p in delays ) : raise ValueError ( "the elements of delays must be numbers > 0" ) if not isinstance ( decays , list ) : raise ValueError ( "decays must be a list" ) if len ( decays ) != n_echos : raise ValueError ( "the length of decays must equal n_echos" ) if any ( ( not is_number ( p ) or p <= 0 or p > 1 ) for p in decays ) : raise ValueError ( "the elements of decays must be between 0 and 1" ) effect_args = [ 'echo' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) ] for i in range ( n_echos ) : effect_args . extend ( [ '{}' . format ( delays [ i ] ) , '{}' . format ( decays [ i ] ) ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'echo' ) return self
12574	def smooth_fwhm ( self , fwhm ) : if fwhm != self . _smooth_fwhm : self . _is_data_smooth = False self . _smooth_fwhm = fwhm
2344	def create_graph_from_data ( self , data , ** kwargs ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{CUTOFF}' ] = str ( self . cutoff ) self . arguments [ '{VARSEL}' ] = str ( self . variablesel ) . upper ( ) self . arguments [ '{SELMETHOD}' ] = self . var_selection [ self . selmethod ] self . arguments [ '{PRUNING}' ] = str ( self . pruning ) . upper ( ) self . arguments [ '{PRUNMETHOD}' ] = self . var_selection [ self . prunmethod ] self . arguments [ '{NJOBS}' ] = str ( self . nb_jobs ) self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_cam ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
2251	def color_text ( text , color ) : r if color is None : return text try : import pygments import pygments . console if sys . platform . startswith ( 'win32' ) : import colorama colorama . init ( ) ansi_text = pygments . console . colorize ( color , text ) return ansi_text except ImportError : import warnings warnings . warn ( 'pygments is not installed, text will not be colored' ) return text
5705	def timeit ( method ) : def timed ( * args , ** kw ) : time_start = time . time ( ) result = method ( * args , ** kw ) time_end = time . time ( ) print ( 'timeit: %r %2.2f sec (%r, %r) ' % ( method . __name__ , time_end - time_start , str ( args ) [ : 20 ] , kw ) ) return result return timed
3045	def _refresh ( self , http ) : if not self . store : self . _do_refresh_request ( http ) else : self . store . acquire_lock ( ) try : new_cred = self . store . locked_get ( ) if ( new_cred and not new_cred . invalid and new_cred . access_token != self . access_token and not new_cred . access_token_expired ) : logger . info ( 'Updated access_token read from Storage' ) self . _updateFromCredential ( new_cred ) else : self . _do_refresh_request ( http ) finally : self . store . release_lock ( )
12278	def run_executable ( repo , args , includes ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) platform_metadata = repomgr . get_metadata ( ) print ( "Obtaining Commit Information" ) ( executable , commiturl ) = find_executable_commitpath ( repo , args ) tmpdir = tempfile . mkdtemp ( ) print ( "Running the command" ) strace_filename = os . path . join ( tmpdir , 'strace.out.txt' ) cmd = [ "strace.py" , "-f" , "-o" , strace_filename , "-s" , "1024" , "-q" , "--" ] + args p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = p . communicate ( ) stdout = os . path . join ( tmpdir , 'stdout.log.txt' ) with open ( stdout , 'w' ) as fd : fd . write ( out . decode ( 'utf-8' ) ) stderr = os . path . join ( tmpdir , 'stderr.log.txt' ) with open ( stderr , 'w' ) as fd : fd . write ( err . decode ( 'utf-8' ) ) files = extract_files ( strace_filename , includes ) execution_metadata = { 'likelyexecutable' : executable , 'commitpath' : commiturl , 'args' : args , } execution_metadata . update ( platform_metadata ) for i in range ( len ( files ) ) : files [ i ] [ 'execution_metadata' ] = execution_metadata return files
10435	def gettablerowindex ( self , window_name , object_name , row_text ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) index = 0 for cell in object_handle . AXRows : if re . match ( row_text , cell . AXChildren [ 0 ] . AXValue ) : return index index += 1 raise LdtpServerException ( u"Unable to find row: %s" % row_text )
10719	def x10_command ( self , house_code , unit_number , state ) : house_code = normalize_housecode ( house_code ) if unit_number is not None : unit_number = normalize_unitnumber ( unit_number ) return self . _x10_command ( house_code , unit_number , state )
1847	def JZ ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC )
2223	def _hashable_sequence ( data , types = True ) : r hasher = _HashTracer ( ) _update_hasher ( hasher , data , types = types ) return hasher . sequence
11581	def run ( self ) : self . command_dispatch . update ( { self . REPORT_VERSION : [ self . report_version , 2 ] } ) self . command_dispatch . update ( { self . REPORT_FIRMWARE : [ self . report_firmware , 1 ] } ) self . command_dispatch . update ( { self . ANALOG_MESSAGE : [ self . analog_message , 2 ] } ) self . command_dispatch . update ( { self . DIGITAL_MESSAGE : [ self . digital_message , 2 ] } ) self . command_dispatch . update ( { self . ENCODER_DATA : [ self . encoder_data , 3 ] } ) self . command_dispatch . update ( { self . SONAR_DATA : [ self . sonar_data , 3 ] } ) self . command_dispatch . update ( { self . STRING_DATA : [ self . _string_data , 2 ] } ) self . command_dispatch . update ( { self . I2C_REPLY : [ self . i2c_reply , 2 ] } ) self . command_dispatch . update ( { self . CAPABILITY_RESPONSE : [ self . capability_response , 2 ] } ) self . command_dispatch . update ( { self . PIN_STATE_RESPONSE : [ self . pin_state_response , 2 ] } ) self . command_dispatch . update ( { self . ANALOG_MAPPING_RESPONSE : [ self . analog_mapping_response , 2 ] } ) self . command_dispatch . update ( { self . STEPPER_DATA : [ self . stepper_version_response , 2 ] } ) while not self . is_stopped ( ) : if len ( self . pymata . command_deque ) : data = self . pymata . command_deque . popleft ( ) command_data = [ ] if data == self . START_SYSEX : while len ( self . pymata . command_deque ) == 0 : pass sysex_command = self . pymata . command_deque . popleft ( ) dispatch_entry = self . command_dispatch . get ( sysex_command ) method = dispatch_entry [ 0 ] end_of_sysex = False while not end_of_sysex : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) if data != self . END_SYSEX : command_data . append ( data ) else : end_of_sysex = True method ( command_data ) continue elif 0x80 <= data <= 0xff : if 0x90 <= data <= 0x9f : port = data & 0xf command_data . append ( port ) data = 0x90 elif 0xe0 <= data <= 0xef : pin = data & 0xf command_data . append ( pin ) data = 0xe0 else : pass dispatch_entry = self . command_dispatch . get ( data ) method = dispatch_entry [ 0 ] num_args = dispatch_entry [ 1 ] for i in range ( num_args ) : while len ( self . pymata . command_deque ) == 0 : pass data = self . pymata . command_deque . popleft ( ) command_data . append ( data ) method ( command_data ) continue else : time . sleep ( .1 )
2334	def predict_proba ( self , a , b , idx = 0 , ** kwargs ) : return self . predict_dataset ( DataFrame ( [ [ a , b ] ] , columns = [ 'A' , 'B' ] ) )
1208	def output_image_link ( self , m ) : return self . renderer . image_link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )
12039	def html_temp_launch ( html ) : fname = tempfile . gettempdir ( ) + "/swhlab/temp.html" with open ( fname , 'w' ) as f : f . write ( html ) webbrowser . open ( fname )
3926	def keypress ( self , size , key ) : key = super ( ) . keypress ( size , key ) num_tabs = len ( self . _widgets ) if key == self . _keys [ 'prev_tab' ] : self . _tab_index = ( self . _tab_index - 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'next_tab' ] : self . _tab_index = ( self . _tab_index + 1 ) % num_tabs self . _update_tabs ( ) elif key == self . _keys [ 'close_tab' ] : if self . _tab_index > 0 : curr_tab = self . _widgets [ self . _tab_index ] self . _widgets . remove ( curr_tab ) del self . _widget_title [ curr_tab ] self . _tab_index -= 1 self . _update_tabs ( ) else : return key
11610	def update_probability_at_read_level ( self , model = 3 ) : self . probability . reset ( ) if model == 1 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . HAPLOGROUP , grouping_mat = self . t2t_mat ) haplogroup_sum_mat = self . allelic_expression * self . t2t_mat self . probability . multiply ( haplogroup_sum_mat , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( haplogroup_sum_mat . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 2 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . LOCUS ) self . probability . multiply ( self . allelic_expression . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 3 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . GROUP , grouping_mat = self . t2t_mat ) self . probability . multiply ( ( self . allelic_expression * self . t2t_mat ) . sum ( axis = 0 ) , axis = APM . Axis . HAPLOTYPE ) self . probability . normalize_reads ( axis = APM . Axis . READ ) elif model == 4 : self . probability . multiply ( self . allelic_expression , axis = APM . Axis . READ ) self . probability . normalize_reads ( axis = APM . Axis . READ ) else : raise RuntimeError ( 'The read normalization model should be 1, 2, 3, or 4.' )
2127	def populate_resource_columns ( item_dict ) : item_dict [ 'type' ] = item_dict [ 'name' ] if len ( item_dict [ 'summary_fields' ] ) == 0 : item_dict [ 'resource_name' ] = None item_dict [ 'resource_type' ] = None else : sf = item_dict [ 'summary_fields' ] item_dict [ 'resource_name' ] = sf . get ( 'resource_name' , '[unknown]' ) item_dict [ 'resource_type' ] = sf . get ( 'resource_type' , '[unknown]' )
4074	def get_cfg_value ( config , section , option ) : try : value = config [ section ] [ option ] except KeyError : if ( section , option ) in MULTI_OPTIONS : return [ ] else : return '' if ( section , option ) in MULTI_OPTIONS : value = split_multiline ( value ) if ( section , option ) in ENVIRON_OPTIONS : value = eval_environ ( value ) return value
433	def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( "number of coordinates and classes are equal" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( "number of scores and classes are equal" ) image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( " %.2f" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else " " ) , ( int ( x ) , int ( y ) ) , 0 , 1.5e-3 * imh , [ 0 , 0 , 256 ] , int ( thick / 2 ) + 1 ) if save_name is not None : save_image ( image , save_name ) return image
6040	def regular_data_1d_from_sub_data_1d ( self , sub_array_1d ) : return np . multiply ( self . sub_grid_fraction , sub_array_1d . reshape ( - 1 , self . sub_grid_length ) . sum ( axis = 1 ) )
1892	def _start_proc ( self ) : assert '_proc' not in dir ( self ) or self . _proc is None try : self . _proc = Popen ( shlex . split ( self . _command ) , stdin = PIPE , stdout = PIPE , bufsize = 0 , universal_newlines = True ) except OSError as e : print ( e , "Probably too many cached expressions? visitors._cache..." ) raise Z3NotFoundError for cfg in self . _init : self . _send ( cfg )
4773	def contains_only ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : extra = [ ] for i in self . val : if i not in items : extra . append ( i ) if extra : self . _err ( 'Expected <%s> to contain only %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( extra ) ) ) missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : self . _err ( 'Expected <%s> to contain only %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
4887	def get_course_final_price ( self , mode , currency = '$' , enterprise_catalog_uuid = None ) : try : price_details = self . client . baskets . calculate . get ( sku = [ mode [ 'sku' ] ] , username = self . user . username , catalog = enterprise_catalog_uuid , ) except ( SlumberBaseException , ConnectionError , Timeout ) as exc : LOGGER . exception ( 'Failed to get price details for sku %s due to: %s' , mode [ 'sku' ] , str ( exc ) ) price_details = { } price = price_details . get ( 'total_incl_tax' , mode [ 'min_price' ] ) if price != mode [ 'min_price' ] : return format_price ( price , currency ) return mode [ 'original_price' ]
10746	def get_default_fields ( self ) : field_names = self . _meta . get_all_field_names ( ) if 'id' in field_names : field_names . remove ( 'id' ) return field_names
644	def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : if doForeground and doBackground : return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) else : if doForeground : return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) if doBackground : return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) return input
13280	def child_end_handler ( self , scache ) : desc = self . desc desc_level = scache . desc_level breadth = desc_level . __len__ ( ) desc [ 'breadth' ] = breadth desc [ 'breadth_path' ] . append ( breadth ) desc_level . append ( desc )
8145	def sharpen ( self , value = 1.0 ) : s = ImageEnhance . Sharpness ( self . img ) self . img = s . enhance ( value )
10140	def check_max_filesize ( chosen_file , max_size ) : if os . path . getsize ( chosen_file ) > max_size : return False else : return True
1453	def add_key ( self , key ) : if key not in self . value : self . value [ key ] = ReducedMetric ( self . reducer )
11514	def search_item_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.item.searchbyname' , parameters ) return response [ 'items' ]
13283	def _parse_whitespace_argument ( source , name ) : r command_pattern = r'\\(' + name + r')(?:[\s{[%])' command_match = re . search ( command_pattern , source ) if command_match is not None : source = source [ command_match . end ( 1 ) : ] pattern = r'(?P<content>\S+)(?:[ %\t\n]+)' match = re . search ( pattern , source ) if match is None : message = ( 'When parsing {}, did not find whitespace-delimited command ' 'argument' ) raise CommandParserError ( message . format ( name ) ) content = match . group ( 'content' ) content . strip ( ) return content
5817	def _write_callback ( connection_id , data_buffer , data_length_pointer ) : try : self = _connection_refs . get ( connection_id ) if not self : socket = _socket_refs . get ( connection_id ) else : socket = self . _socket if not self and not socket : return 0 data_length = deref ( data_length_pointer ) data = bytes_from_buffer ( data_buffer , data_length ) if self and not self . _done_handshake : self . _client_hello += data error = None try : sent = socket . send ( data ) except ( socket_ . error ) as e : error = e . errno if error is not None and error != errno . EAGAIN : if error == errno . ECONNRESET or error == errno . EPIPE : return SecurityConst . errSSLClosedNoNotify return SecurityConst . errSSLClosedAbort if sent != data_length : pointer_set ( data_length_pointer , sent ) return SecurityConst . errSSLWouldBlock return 0 except ( KeyboardInterrupt ) as e : self . _exception = e return SecurityConst . errSSLPeerUserCancelled
9397	def run ( self ) : print ( 'Oct2Py speed test' ) print ( '*' * 20 ) time . sleep ( 1 ) print ( 'Raw speed: ' ) avg = timeit . timeit ( self . raw_speed , number = 10 ) / 10 print ( ' {0:0.01f} usec per loop' . format ( avg * 1e6 ) ) sides = [ 1 , 10 , 100 , 1000 ] runs = [ 10 , 10 , 10 , 5 ] for ( side , nruns ) in zip ( sides , runs ) : self . array = np . reshape ( np . arange ( side ** 2 ) , ( - 1 ) ) print ( 'Put {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_put , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) print ( 'Get {0}x{1}: ' . format ( side , side ) ) avg = timeit . timeit ( self . large_array_get , number = nruns ) / nruns print ( ' {0:0.01f} msec' . format ( avg * 1e3 ) ) self . octave . exit ( ) print ( '*' * 20 ) print ( 'Test complete!' )
988	def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ "inputFilePath" ] scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] dateEncoderArgs = recordParams [ "dateEncoderArgs" ] scalarEncoder = ScalarEncoder ( ** scalarEncoderArgs ) dateEncoder = DateEncoder ( ** dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) network = Network ( ) network . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( { "verbosity" : verbosity } ) ) sensor = network . regions [ "sensor" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( spatialParams ) ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "resetOut" , destInput = "resetIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , json . dumps ( temporalParams ) ) network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "topDownIn" ) spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] spatialPoolerRegion . setParameter ( "learningMode" , True ) spatialPoolerRegion . setParameter ( "anomalyMode" , False ) temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] temporalPoolerRegion . setParameter ( "topDownMode" , True ) temporalPoolerRegion . setParameter ( "learningMode" , True ) temporalPoolerRegion . setParameter ( "inferenceMode" , True ) temporalPoolerRegion . setParameter ( "anomalyMode" , True ) return network
11839	def set_board ( self , board = None ) : "Set the board, and find all the words in it." if board is None : board = random_boggle ( ) self . board = board self . neighbors = boggle_neighbors ( len ( board ) ) self . found = { } for i in range ( len ( board ) ) : lo , hi = self . wordlist . bounds [ board [ i ] ] self . find ( lo , hi , i , [ ] , '' ) return self
393	def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : if rewards is None : raise Exception ( "rewards should be a list" ) discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) running_add = 0 for t in reversed ( xrange ( 0 , rewards . size ) ) : if mode == 0 : if rewards [ t ] != 0 : running_add = 0 running_add = running_add * gamma + rewards [ t ] discounted_r [ t ] = running_add return discounted_r
4846	def get_content_metadata ( self , enterprise_customer ) : content_metadata = OrderedDict ( ) if enterprise_customer . catalog : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_ENDPOINT , detail_resource = 'courses' , resource_id = str ( enterprise_customer . uuid ) , traverse_pagination = True , ) for course in response [ 'results' ] : for course_run in course [ 'course_runs' ] : course_run [ 'content_type' ] = 'courserun' content_metadata [ course_run [ 'key' ] ] = course_run for enterprise_customer_catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : response = self . _load_data ( self . ENTERPRISE_CUSTOMER_CATALOGS_ENDPOINT , resource_id = str ( enterprise_customer_catalog . uuid ) , traverse_pagination = True , querystring = { 'page_size' : 1000 } , ) for item in response [ 'results' ] : content_id = utils . get_content_metadata_item_id ( item ) content_metadata [ content_id ] = item return content_metadata . values ( )
6802	def loadable ( self , src , dst ) : from fabric import state from fabric . task_utils import crawl src_task = crawl ( src , state . commands ) assert src_task , 'Unknown source role: %s' % src dst_task = crawl ( dst , state . commands ) assert dst_task , 'Unknown destination role: %s' % src src_task ( ) env . host_string = env . hosts [ 0 ] src_size_bytes = self . get_size ( ) dst_task ( ) env . host_string = env . hosts [ 0 ] try : dst_size_bytes = self . get_size ( ) except ( ValueError , TypeError ) : dst_size_bytes = 0 free_space_bytes = self . get_free_space ( ) balance_bytes = free_space_bytes + dst_size_bytes - src_size_bytes balance_bytes_scaled , units = pretty_bytes ( balance_bytes ) viable = balance_bytes >= 0 if self . verbose : print ( 'src_db_size:' , pretty_bytes ( src_size_bytes ) ) print ( 'dst_db_size:' , pretty_bytes ( dst_size_bytes ) ) print ( 'dst_free_space:' , pretty_bytes ( free_space_bytes ) ) print if viable : print ( 'Viable! There will be %.02f %s of disk space left.' % ( balance_bytes_scaled , units ) ) else : print ( 'Not viable! We would be %.02f %s short.' % ( balance_bytes_scaled , units ) ) return viable
1903	def colored_level_name ( self , levelname ) : if self . colors_disabled : return self . plain_levelname_format . format ( levelname ) else : return self . colored_levelname_format . format ( self . color_map [ levelname ] , levelname )
7920	def __prepare_local ( data ) : if not data : return None data = unicode ( data ) try : local = NODEPREP . prepare ( data ) except StringprepError , err : raise JIDError ( u"Local part invalid: {0}" . format ( err ) ) if len ( local . encode ( "utf-8" ) ) > 1023 : raise JIDError ( u"Local part too long" ) return local
11038	def maybe_key_vault ( client , mount_path ) : d = client . read_kv2 ( 'client_key' , mount_path = mount_path ) def get_or_create_key ( client_key ) : if client_key is not None : key_data = client_key [ 'data' ] [ 'data' ] key = _load_pem_private_key_bytes ( key_data [ 'key' ] . encode ( 'utf-8' ) ) return JWKRSA ( key = key ) else : key = generate_private_key ( u'rsa' ) key_data = { 'key' : _dump_pem_private_key_bytes ( key ) . decode ( 'utf-8' ) } d = client . create_or_update_kv2 ( 'client_key' , key_data , mount_path = mount_path ) return d . addCallback ( lambda _result : JWKRSA ( key = key ) ) return d . addCallback ( get_or_create_key )
11141	def load_repository ( self , path , verbose = True , ntrials = 3 ) : assert isinstance ( ntrials , int ) , "ntrials must be integer" assert ntrials > 0 , "ntrials must be >0" repo = None for _trial in range ( ntrials ) : try : self . __load_repository ( path = path , verbose = True ) except Exception as err1 : try : from . OldRepository import Repository REP = Repository ( path ) except Exception as err2 : error = "Unable to load repository using neiher new style (%s) nor old style (%s)" % ( err1 , err2 ) if self . DEBUG_PRINT_FAILED_TRIALS : print ( "Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute" % ( _trial , inspect . stack ( ) [ 1 ] [ 3 ] , str ( error ) ) ) else : error = None repo = REP break else : error = None repo = self break assert error is None , error return repo
8643	def post_track ( session , user_id , project_id , latitude , longitude ) : tracking_data = { 'user_id' : user_id , 'project_id' : project_id , 'track_point' : { 'latitude' : latitude , 'longitude' : longitude } } response = make_post_request ( session , 'tracks' , json_data = tracking_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise TrackNotCreatedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
3532	def mixpanel ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MixpanelNode ( )
4359	def spawn ( self , fn , * args , ** kwargs ) : log . debug ( "Spawning sub-Socket Greenlet: %s" % fn . __name__ ) job = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( job ) return job
6574	def formatter ( self , api_client , data , newval ) : if newval is None : return None user_param = data [ '_paramAdditionalUrls' ] urls = { } if isinstance ( newval , str ) : urls [ user_param [ 0 ] ] = newval else : for key , url in zip ( user_param , newval ) : urls [ key ] = url return urls
9718	async def stream_frames_stop ( self ) : self . _protocol . set_on_packet ( None ) cmd = "streamframes stop" await self . _protocol . send_command ( cmd , callback = False )
7610	def get_location ( self , location_id : int , timeout : int = None ) : url = self . api . LOCATIONS + '/' + str ( location_id ) return self . _get_model ( url , timeout = timeout )
7365	def run_command ( args , ** kwargs ) : assert len ( args ) > 0 start_time = time . time ( ) process = AsyncProcess ( args , ** kwargs ) process . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "%s took %0.4f seconds" , args [ 0 ] , elapsed_time )
9027	def _width ( self ) : layout = self . _instruction . get ( GRID_LAYOUT ) if layout is not None : width = layout . get ( WIDTH ) if width is not None : return width return self . _instruction . number_of_consumed_meshes
11755	def tt_check_all ( kb , alpha , symbols , model ) : "Auxiliary routine to implement tt_entails." if not symbols : if pl_true ( kb , model ) : result = pl_true ( alpha , model ) assert result in ( True , False ) return result else : return True else : P , rest = symbols [ 0 ] , symbols [ 1 : ] return ( tt_check_all ( kb , alpha , rest , extend ( model , P , True ) ) and tt_check_all ( kb , alpha , rest , extend ( model , P , False ) ) )
3871	def next_event ( self , event_id , prev = False ) : i = self . events . index ( self . _events_dict [ event_id ] ) if prev and i > 0 : return self . events [ i - 1 ] elif not prev and i + 1 < len ( self . events ) : return self . events [ i + 1 ] else : return None
10963	def set_shape ( self , shape , inner ) : if self . shape != shape or self . inner != inner : self . shape = shape self . inner = inner self . initialize ( )
6859	def create_database ( name , owner = None , owner_host = 'localhost' , charset = 'utf8' , collate = 'utf8_general_ci' , ** kwargs ) : with settings ( hide ( 'running' ) ) : query ( "CREATE DATABASE %(name)s CHARACTER SET %(charset)s COLLATE %(collate)s;" % { 'name' : name , 'charset' : charset , 'collate' : collate } , ** kwargs ) if owner : query ( "GRANT ALL PRIVILEGES ON %(name)s.* TO '%(owner)s'@'%(owner_host)s' WITH GRANT OPTION;" % { 'name' : name , 'owner' : owner , 'owner_host' : owner_host } , ** kwargs ) puts ( "Created MySQL database '%s'." % name )
2340	def GNN_instance ( x , idx = 0 , device = None , nh = 20 , ** kwargs ) : device = SETTINGS . get_default ( device = device ) xy = scale ( x ) . astype ( 'float32' ) inputx = th . FloatTensor ( xy [ : , [ 0 ] ] ) . to ( device ) target = th . FloatTensor ( xy [ : , [ 1 ] ] ) . to ( device ) GNNXY = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNYX = GNN_model ( x . shape [ 0 ] , device = device , nh = nh ) . to ( device ) GNNXY . reset_parameters ( ) GNNYX . reset_parameters ( ) XY = GNNXY . run ( inputx , target , ** kwargs ) YX = GNNYX . run ( target , inputx , ** kwargs ) return [ XY , YX ]
12381	def delete ( self , request , response ) : if self . slug is None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'destroy' ) self . destroy ( ) self . response . status = http . client . NO_CONTENT self . make_response ( )
13896	def ExpandUser ( path ) : if six . PY2 : encoding = sys . getfilesystemencoding ( ) path = path . encode ( encoding ) result = os . path . expanduser ( path ) if six . PY2 : result = result . decode ( encoding ) return result
5145	def _merge_config ( self , config , templates ) : if not templates : return config if not isinstance ( templates , list ) : raise TypeError ( 'templates argument must be an instance of list' ) result = { } config_list = templates + [ config ] for merging in config_list : result = merge_config ( result , self . _load ( merging ) , self . list_identifiers ) return result
11431	def _compare_fields ( field1 , field2 , strict = True ) : if strict : return field1 [ : 4 ] == field2 [ : 4 ] else : if field1 [ 1 : 4 ] != field2 [ 1 : 4 ] : return False else : return set ( field1 [ 0 ] ) == set ( field2 [ 0 ] )
9404	def _get_function_ptr ( self , name ) : func = _make_function_ptr_instance self . _function_ptrs . setdefault ( name , func ( self , name ) ) return self . _function_ptrs [ name ]
2451	def set_pkg_down_location ( self , doc , location ) : self . assert_package_exists ( ) if not self . package_down_location_set : self . package_down_location_set = True doc . package . download_location = location return True else : raise CardinalityError ( 'Package::DownloadLocation' )
8617	def _underscore_to_camelcase ( value ) : def camelcase ( ) : yield str . lower while True : yield str . capitalize c = camelcase ( ) return "" . join ( next ( c ) ( x ) if x else '_' for x in value . split ( "_" ) )
6772	def install_required ( self , type = None , service = None , list_only = 0 , ** kwargs ) : r = self . local_renderer list_only = int ( list_only ) type = ( type or '' ) . lower ( ) . strip ( ) assert not type or type in PACKAGE_TYPES , 'Unknown package type: %s' % ( type , ) lst = [ ] if type : types = [ type ] else : types = PACKAGE_TYPES for _type in types : if _type == SYSTEM : content = '\n' . join ( self . list_required ( type = _type , service = service ) ) if list_only : lst . extend ( _ for _ in content . split ( '\n' ) if _ . strip ( ) ) if self . verbose : print ( 'content:' , content ) break fd , fn = tempfile . mkstemp ( ) fout = open ( fn , 'w' ) fout . write ( content ) fout . close ( ) self . install_custom ( fn = fn ) else : raise NotImplementedError return lst
13258	def _file_path ( self , uid ) : file_name = '%s.doentry' % ( uid ) return os . path . join ( self . dayone_journal_path , file_name )
3596	def bulkDetails ( self , packageNames ) : params = { 'au' : '1' } req = googleplay_pb2 . BulkDetailsRequest ( ) req . docid . extend ( packageNames ) data = req . SerializeToString ( ) message = self . executeRequestApi2 ( BULK_URL , post_data = data . decode ( "utf-8" ) , content_type = CONTENT_TYPE_PROTO , params = params ) response = message . payload . bulkDetailsResponse return [ None if not utils . hasDoc ( entry ) else utils . parseProtobufObj ( entry . doc ) for entry in response . entry ]
8058	def do_play ( self , line ) : if self . pause_speed is None : self . bot . _speed = self . pause_speed self . pause_speed = None self . print_response ( "Play" )
6761	def configure ( self ) : lm = self . last_manifest for tracker in self . get_trackers ( ) : self . vprint ( 'Checking tracker:' , tracker ) last_thumbprint = lm [ '_tracker_%s' % tracker . get_natural_key_hash ( ) ] self . vprint ( 'last thumbprint:' , last_thumbprint ) has_changed = tracker . is_changed ( last_thumbprint ) self . vprint ( 'Tracker changed:' , has_changed ) if has_changed : self . vprint ( 'Change detected!' ) tracker . act ( )
3784	def TP_dependent_property ( self , T , P ) : r if self . method_P : if self . test_method_validity_P ( T , P , self . method_P ) : try : prop = self . calculate_P ( T , P , self . method_P ) if self . test_property_validity ( prop ) : return prop except : pass self . sorted_valid_methods_P = self . select_valid_methods_P ( T , P ) for method_P in self . sorted_valid_methods_P : try : prop = self . calculate_P ( T , P , method_P ) if self . test_property_validity ( prop ) : self . method_P = method_P return prop except : pass return None
1874	def MOVHPD ( cpu , dest , src ) : if src . size == 128 : assert dest . size == 64 dest . write ( Operators . EXTRACT ( src . read ( ) , 64 , 64 ) ) else : assert src . size == 64 and dest . size == 128 value = Operators . EXTRACT ( dest . read ( ) , 0 , 64 ) dest . write ( Operators . CONCAT ( 128 , src . read ( ) , value ) )
5487	def send_payload ( self , params ) : data = json . dumps ( { 'jsonrpc' : self . version , 'method' : self . service_name , 'params' : params , 'id' : text_type ( uuid . uuid4 ( ) ) } ) data_binary = data . encode ( 'utf-8' ) url_request = Request ( self . service_url , data_binary , headers = self . headers ) return urlopen ( url_request ) . read ( )
4146	def WelchPeriodogram ( data , NFFT = None , sampling = 1. , ** kargs ) : r from pylab import psd spectrum = Spectrum ( data , sampling = 1. ) P = psd ( data , NFFT , Fs = sampling , ** kargs ) spectrum . psd = P [ 0 ] return P , spectrum
10726	def _handle_variant ( self ) : def the_func ( a_tuple , variant = 0 ) : ( signature , an_obj ) = a_tuple ( func , sig ) = self . COMPLETE . parseString ( signature ) [ 0 ] assert sig == signature ( xformed , _ ) = func ( an_obj , variant = variant + 1 ) return ( xformed , xformed . variant_level ) return ( the_func , 'v' )
9628	def detail_view ( self , request ) : context = { 'preview' : self , } kwargs = { } if self . form_class : if request . GET : form = self . form_class ( data = request . GET ) else : form = self . form_class ( ) context [ 'form' ] = form if not form . is_bound or not form . is_valid ( ) : return render ( request , 'mailviews/previews/detail.html' , context ) kwargs . update ( form . get_message_view_kwargs ( ) ) message_view = self . get_message_view ( request , ** kwargs ) message = message_view . render_to_message ( ) raw = message . message ( ) headers = OrderedDict ( ( header , maybe_decode_header ( raw [ header ] ) ) for header in self . headers ) context . update ( { 'message' : message , 'subject' : message . subject , 'body' : message . body , 'headers' : headers , 'raw' : raw . as_string ( ) , } ) alternatives = getattr ( message , 'alternatives' , [ ] ) try : html = next ( alternative [ 0 ] for alternative in alternatives if alternative [ 1 ] == 'text/html' ) context . update ( { 'html' : html , 'escaped_html' : b64encode ( html . encode ( 'utf-8' ) ) , } ) except StopIteration : pass return render ( request , self . template_name , context )
1882	def new_symbolic_buffer ( self , nbytes , ** options ) : label = options . get ( 'label' ) avoid_collisions = False if label is None : label = 'buffer' avoid_collisions = True taint = options . get ( 'taint' , frozenset ( ) ) expr = self . _constraints . new_array ( name = label , index_max = nbytes , value_bits = 8 , taint = taint , avoid_collisions = avoid_collisions ) self . _input_symbols . append ( expr ) if options . get ( 'cstring' , False ) : for i in range ( nbytes - 1 ) : self . _constraints . add ( expr [ i ] != 0 ) return expr
10197	def _handle_request ( self , scheme , netloc , path , headers , body = None , method = "GET" ) : backend_url = "{}://{}{}" . format ( scheme , netloc , path ) try : response = self . http_request . request ( backend_url , method = method , body = body , headers = dict ( headers ) ) self . _return_response ( response ) except Exception as e : body = "Invalid response from backend: '{}' Server might be busy" . format ( e . message ) logging . debug ( body ) self . send_error ( httplib . SERVICE_UNAVAILABLE , body )
4486	def remove ( self ) : response = self . _delete ( self . _delete_url ) if response . status_code != 204 : raise RuntimeError ( 'Could not delete {}.' . format ( self . path ) )
11293	def oembed_schema ( request ) : current_domain = Site . objects . get_current ( ) . domain url_schemes = [ ] endpoint = reverse ( 'oembed_json' ) providers = oembed . site . get_providers ( ) for provider in providers : if not provider . provides : continue match = None if isinstance ( provider , DjangoProvider ) : url_pattern = resolver . reverse_dict . get ( provider . _meta . named_view ) if url_pattern : regex = re . sub ( r'%\(.+?\)s' , '*' , url_pattern [ 0 ] [ 0 ] [ 0 ] ) match = 'http://%s/%s' % ( current_domain , regex ) elif isinstance ( provider , HTTPProvider ) : match = provider . url_scheme else : match = provider . regex if match : url_schemes . append ( { 'type' : provider . resource_type , 'matches' : match , 'endpoint' : endpoint } ) url_schemes . sort ( key = lambda item : item [ 'matches' ] ) response = HttpResponse ( mimetype = 'application/json' ) response . write ( simplejson . dumps ( url_schemes ) ) return response
10220	def preprocessing_excel ( path ) : if not os . path . exists ( path ) : raise ValueError ( "Error: %s file not found" % path ) df = pd . read_excel ( path , sheetname = 0 , header = 0 ) df . iloc [ : , 0 ] = pd . Series ( df . iloc [ : , 0 ] ) . fillna ( method = 'ffill' ) df = df [ df . ix [ : , 1 ] . notnull ( ) ] df = df . reset_index ( drop = True ) df . ix [ : , 2 ] . fillna ( 0 , inplace = True ) if ( df . ix [ : , 1 ] . isnull ( ) . sum ( ) ) != 0 : raise ValueError ( "Error: Empty cells in the gene column" ) return df
5273	def _generalized_word_starts ( self , xs ) : self . word_starts = [ ] i = 0 for n in range ( len ( xs ) ) : self . word_starts . append ( i ) i += len ( xs [ n ] ) + 1
102	def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]
12770	def step ( self , substeps = 2 ) : self . frame_no += 1 try : next ( self . follower ) except ( AttributeError , StopIteration ) as err : self . reset ( )
8434	def apply ( cls , x , palette , na_value = None , trans = None ) : if trans is not None : x = trans . transform ( x ) limits = cls . train ( x ) return cls . map ( x , palette , limits , na_value )
5976	def total_regular_pixels_from_mask ( mask ) : total_regular_pixels = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : total_regular_pixels += 1 return total_regular_pixels
10243	def count_citation_years ( graph : BELGraph ) -> typing . Counter [ int ] : result = defaultdict ( set ) for _ , _ , data in graph . edges ( data = True ) : if CITATION not in data or CITATION_DATE not in data [ CITATION ] : continue try : dt = _ensure_datetime ( data [ CITATION ] [ CITATION_DATE ] ) result [ dt . year ] . add ( ( data [ CITATION ] [ CITATION_TYPE ] , data [ CITATION ] [ CITATION_REFERENCE ] ) ) except Exception : continue return count_dict_values ( result )
9206	def get_prefix ( multicodec ) : try : prefix = varint . encode ( NAME_TABLE [ multicodec ] ) except KeyError : raise ValueError ( '{} multicodec is not supported.' . format ( multicodec ) ) return prefix
13536	def prune_list ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : pass targets . append ( self ) return targets
5408	def _operation_status_message ( self ) : msg = None action = None if not google_v2_operations . is_done ( self . _op ) : last_event = google_v2_operations . get_last_event ( self . _op ) if last_event : msg = last_event [ 'description' ] action_id = last_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) else : msg = 'Pending' else : failed_events = google_v2_operations . get_failed_events ( self . _op ) if failed_events : failed_event = failed_events [ - 1 ] msg = failed_event . get ( 'details' , { } ) . get ( 'stderr' ) action_id = failed_event . get ( 'details' , { } ) . get ( 'actionId' ) if action_id : action = google_v2_operations . get_action_by_id ( self . _op , action_id ) if not msg : error = google_v2_operations . get_error ( self . _op ) if error : msg = error [ 'message' ] else : msg = 'Success' return msg , action
930	def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record
3508	def nullspace ( A , atol = 1e-13 , rtol = 0 ) : A = np . atleast_2d ( A ) u , s , vh = np . linalg . svd ( A ) tol = max ( atol , rtol * s [ 0 ] ) nnz = ( s >= tol ) . sum ( ) ns = vh [ nnz : ] . conj ( ) . T return ns
9921	def validate_key ( self , key ) : if not models . PasswordResetToken . valid_tokens . filter ( key = key ) . exists ( ) : raise serializers . ValidationError ( _ ( "The provided reset token does not exist, or is expired." ) ) return key
8082	def relcurveto ( self , h1x , h1y , h2x , h2y , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . relcurveto ( h1x , h1y , h2x , h2y , x , y )
8175	def update ( self , shuffled = True , cohesion = 100 , separation = 10 , alignment = 5 , goal = 20 , limit = 30 ) : from random import shuffle if shuffled : shuffle ( self ) m1 = 1.0 m2 = 1.0 m3 = 1.0 m4 = 1.0 if not self . scattered and _ctx . random ( ) < self . _scatter : self . scattered = True if self . scattered : m1 = - m1 m3 *= 0.25 self . _scatter_i += 1 if self . _scatter_i >= self . _scatter_t : self . scattered = False self . _scatter_i = 0 if not self . has_goal : m4 = 0 if self . flee : m4 = - m4 for b in self : if b . is_perching : if b . _perch_t > 0 : b . _perch_t -= 1 continue else : b . is_perching = False vx1 , vy1 , vz1 = b . cohesion ( cohesion ) vx2 , vy2 , vz2 = b . separation ( separation ) vx3 , vy3 , vz3 = b . alignment ( alignment ) vx4 , vy4 , vz4 = b . goal ( self . _gx , self . _gy , self . _gz , goal ) b . vx += m1 * vx1 + m2 * vx2 + m3 * vx3 + m4 * vx4 b . vy += m1 * vy1 + m2 * vy2 + m3 * vy3 + m4 * vy4 b . vz += m1 * vz1 + m2 * vz2 + m3 * vz3 + m4 * vz4 b . limit ( limit ) b . x += b . vx b . y += b . vy b . z += b . vz self . constrain ( )
7778	def __from_xml ( self , data ) : ns = get_node_ns ( data ) if ns and ns . getContent ( ) != VCARD_NS : raise ValueError ( "Not in the %r namespace" % ( VCARD_NS , ) ) if data . name != "vCard" : raise ValueError ( "Bad root element name: %r" % ( data . name , ) ) n = data . children dns = get_node_ns ( data ) while n : if n . type != 'element' : n = n . next continue ns = get_node_ns ( n ) if ( ns and dns and ns . getContent ( ) != dns . getContent ( ) ) : n = n . next continue if not self . components . has_key ( n . name ) : n = n . next continue cl , tp = self . components [ n . name ] if tp in ( "required" , "optional" ) : if self . content . has_key ( n . name ) : raise ValueError ( "Duplicate %s" % ( n . name , ) ) try : self . content [ n . name ] = cl ( n . name , n ) except Empty : pass elif tp == "multi" : if not self . content . has_key ( n . name ) : self . content [ n . name ] = [ ] try : self . content [ n . name ] . append ( cl ( n . name , n ) ) except Empty : pass n = n . next
4340	def remix ( self , remix_dictionary = None , num_output_channels = None ) : if not ( isinstance ( remix_dictionary , dict ) or remix_dictionary is None ) : raise ValueError ( "remix_dictionary must be a dictionary or None." ) if remix_dictionary is not None : if not all ( [ isinstance ( i , int ) and i > 0 for i in remix_dictionary . keys ( ) ] ) : raise ValueError ( "remix dictionary must have positive integer keys." ) if not all ( [ isinstance ( v , list ) for v in remix_dictionary . values ( ) ] ) : raise ValueError ( "remix dictionary values must be lists." ) for v_list in remix_dictionary . values ( ) : if not all ( [ isinstance ( v , int ) and v > 0 for v in v_list ] ) : raise ValueError ( "elements of remix dictionary values must " "be positive integers" ) if not ( ( isinstance ( num_output_channels , int ) and num_output_channels > 0 ) or num_output_channels is None ) : raise ValueError ( "num_output_channels must be a positive integer or None." ) effect_args = [ 'remix' ] if remix_dictionary is None : effect_args . append ( '-' ) else : if num_output_channels is None : num_output_channels = max ( remix_dictionary . keys ( ) ) for channel in range ( 1 , num_output_channels + 1 ) : if channel in remix_dictionary . keys ( ) : out_channel = ',' . join ( [ str ( i ) for i in remix_dictionary [ channel ] ] ) else : out_channel = '0' effect_args . append ( out_channel ) self . effects . extend ( effect_args ) self . effects_log . append ( 'remix' ) return self
12320	def add_files ( self , repo , files ) : rootdir = repo . rootdir for f in files : relativepath = f [ 'relativepath' ] sourcepath = f [ 'localfullpath' ] if sourcepath is None : continue targetpath = os . path . join ( rootdir , relativepath ) try : os . makedirs ( os . path . dirname ( targetpath ) ) except : pass print ( "Updating: {}" . format ( relativepath ) ) shutil . copyfile ( sourcepath , targetpath ) with cd ( repo . rootdir ) : self . _run ( [ 'add' , relativepath ] )
3074	def authorize_view ( self ) : args = request . args . to_dict ( ) args [ 'scopes' ] = request . args . getlist ( 'scopes' ) return_url = args . pop ( 'return_url' , None ) if return_url is None : return_url = request . referrer or '/' flow = self . _make_flow ( return_url = return_url , ** args ) auth_url = flow . step1_get_authorize_url ( ) return redirect ( auth_url )
5424	def _wait_and_retry ( provider , job_id , poll_interval , retries , job_descriptor ) : while True : tasks = provider . lookup_job_tasks ( { '*' } , job_ids = [ job_id ] ) running_tasks = set ( ) completed_tasks = set ( ) canceled_tasks = set ( ) fully_failed_tasks = set ( ) task_fail_count = dict ( ) message_task = None task_dict = dict ( ) for t in tasks : task_id = job_model . numeric_task_id ( t . get_field ( 'task-id' ) ) task_dict [ task_id ] = t status = t . get_field ( 'task-status' ) if status == 'FAILURE' : task_fail_count [ task_id ] = task_fail_count . get ( task_id , 0 ) + 1 if task_fail_count [ task_id ] > retries : fully_failed_tasks . add ( task_id ) message_task = t elif status == 'CANCELED' : canceled_tasks . add ( task_id ) if not message_task : message_task = t elif status == 'SUCCESS' : completed_tasks . add ( task_id ) elif status == 'RUNNING' : running_tasks . add ( task_id ) retry_tasks = ( set ( task_fail_count ) . difference ( fully_failed_tasks ) . difference ( running_tasks ) . difference ( completed_tasks ) . difference ( canceled_tasks ) ) if not retry_tasks and not running_tasks : if message_task : return [ provider . get_tasks_completion_messages ( [ message_task ] ) ] return [ ] for task_id in retry_tasks : identifier = '{}.{}' . format ( job_id , task_id ) if task_id else job_id print ( ' {} (attempt {}) failed. Retrying.' . format ( identifier , task_fail_count [ task_id ] ) ) msg = task_dict [ task_id ] . get_field ( 'status-message' ) print ( ' Failure message: {}' . format ( msg ) ) _retry_task ( provider , job_descriptor , task_id , task_fail_count [ task_id ] + 1 ) SLEEP_FUNCTION ( poll_interval )
5738	def enqueue ( self , f , * args , ** kwargs ) : task = Task ( uuid4 ( ) . hex , f , args , kwargs ) self . storage . put_task ( task ) return self . enqueue_task ( task )
845	def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == "rawOverlap" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfInput" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == "pctOverlapOfProto" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "pctOverlapOfLarger" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == "norm" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( "Unimplemented distance method %s" % self . distanceMethod ) else : if self . distanceMethod == "norm" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( "Not implemented yet for dense storage...." ) return dist
2683	def get_function_config ( cfg ) : function_name = cfg . get ( 'function_name' ) profile_name = cfg . get ( 'profile' ) aws_access_key_id = cfg . get ( 'aws_access_key_id' ) aws_secret_access_key = cfg . get ( 'aws_secret_access_key' ) client = get_client ( 'lambda' , profile_name , aws_access_key_id , aws_secret_access_key , cfg . get ( 'region' ) , ) try : return client . get_function ( FunctionName = function_name ) except client . exceptions . ResourceNotFoundException as e : if 'Function not found' in str ( e ) : return False
13842	def arkt_to_unixt ( ark_timestamp ) : res = datetime . datetime ( 2017 , 3 , 21 , 15 , 55 , 44 ) + datetime . timedelta ( seconds = ark_timestamp ) return res . timestamp ( )
2322	def read_causal_pairs ( filename , scale = True , ** kwargs ) : def convert_row ( row , scale ) : a = row [ "A" ] . split ( " " ) b = row [ "B" ] . split ( " " ) if a [ 0 ] == "" : a . pop ( 0 ) b . pop ( 0 ) if a [ - 1 ] == "" : a . pop ( - 1 ) b . pop ( - 1 ) a = array ( [ float ( i ) for i in a ] ) b = array ( [ float ( i ) for i in b ] ) if scale : a = scaler ( a ) b = scaler ( b ) return row [ 'SampleID' ] , a , b if isinstance ( filename , str ) : data = read_csv ( filename , ** kwargs ) elif isinstance ( filename , DataFrame ) : data = filename else : raise TypeError ( "Type not supported." ) conv_data = [ ] for idx , row in data . iterrows ( ) : conv_data . append ( convert_row ( row , scale ) ) df = DataFrame ( conv_data , columns = [ 'SampleID' , 'A' , 'B' ] ) df = df . set_index ( "SampleID" ) return df
3782	def set_user_methods_P ( self , user_methods_P , forced_P = False ) : r if isinstance ( user_methods_P , str ) : user_methods_P = [ user_methods_P ] self . user_methods_P = user_methods_P self . forced_P = forced_P if set ( self . user_methods_P ) . difference ( self . all_methods_P ) : raise Exception ( "One of the given methods is not available for this chemical" ) if not self . user_methods_P and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method_P = None self . sorted_valid_methods_P = [ ] self . TP_cached = None
1648	def CheckAltTokens ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] if Match ( r'^\s*#' , line ) : return if line . find ( '/*' ) >= 0 or line . find ( '*/' ) >= 0 : return for match in _ALT_TOKEN_REPLACEMENT_PATTERN . finditer ( line ) : error ( filename , linenum , 'readability/alt_tokens' , 2 , 'Use operator %s instead of %s' % ( _ALT_TOKEN_REPLACEMENT [ match . group ( 1 ) ] , match . group ( 1 ) ) )
2376	def run ( self , args ) : self . args = self . parse_and_process_args ( args ) if self . args . version : print ( __version__ ) return 0 if self . args . rulefile : for filename in self . args . rulefile : self . _load_rule_file ( filename ) if self . args . list : self . list_rules ( ) return 0 if self . args . describe : self . _describe_rules ( self . args . args ) return 0 self . counts = { ERROR : 0 , WARNING : 0 , "other" : 0 } for filename in self . args . args : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) continue if os . path . isdir ( filename ) : self . _process_folder ( filename ) else : self . _process_file ( filename ) if self . counts [ ERROR ] > 0 : return self . counts [ ERROR ] if self . counts [ ERROR ] < 254 else 255 return 0
8470	def execute ( self , shell = True ) : process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = shell ) self . output , self . errors = process . communicate ( )
8151	def _addvar ( self , v ) : oldvar = self . _oldvars . get ( v . name ) if oldvar is not None : if isinstance ( oldvar , Variable ) : if oldvar . compliesTo ( v ) : v . value = oldvar . value else : v . value = v . sanitize ( oldvar ) else : for listener in VarListener . listeners : listener . var_added ( v ) self . _vars [ v . name ] = v self . _namespace [ v . name ] = v . value self . _oldvars [ v . name ] = v return v
5643	def get_min_visit_time ( self ) : if not self . visit_events : return float ( 'inf' ) else : return min ( self . visit_events , key = lambda event : event . arr_time_ut ) . arr_time_ut
4921	def list ( self , request ) : catalog_api = CourseCatalogApiClient ( request . user ) catalogs = catalog_api . get_paginated_catalogs ( request . GET ) self . ensure_data_exists ( request , catalogs ) serializer = serializers . ResponsePaginationSerializer ( catalogs ) return get_paginated_response ( serializer . data , request )
40	def add ( self , * args , ** kwargs ) : idx = self . _next_idx super ( ) . add ( * args , ** kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha
11290	def load_dict ( self , source , namespace = '' ) : for key , value in source . items ( ) : if isinstance ( key , str ) : nskey = ( namespace + '.' + key ) . strip ( '.' ) if isinstance ( value , dict ) : self . load_dict ( value , namespace = nskey ) else : self [ nskey ] = value else : raise TypeError ( 'Key has type %r (not a string)' % type ( key ) ) return self
8257	def _average ( self ) : r , g , b , a = 0 , 0 , 0 , 0 for clr in self : r += clr . r g += clr . g b += clr . b a += clr . alpha r /= len ( self ) g /= len ( self ) b /= len ( self ) a /= len ( self ) return color ( r , g , b , a , mode = "rgb" )
1373	def defaults_cluster_role_env ( cluster_role_env ) : if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] )
11099	def select_by_mtime ( self , min_time = 0 , max_time = ts_2100 , recursive = True ) : def filters ( p ) : return min_time <= p . mtime <= max_time return self . select_file ( filters , recursive )
4147	def DaniellPeriodogram ( data , P , NFFT = None , detrend = 'mean' , sampling = 1. , scale_by_freq = True , window = 'hamming' ) : r psd = speriodogram ( data , NFFT = NFFT , detrend = detrend , sampling = sampling , scale_by_freq = scale_by_freq , window = window ) if len ( psd ) % 2 == 1 : datatype = 'real' else : datatype = 'complex' N = len ( psd ) _slice = 2 * P + 1 if datatype == 'real' : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 0 : newN = psd . size / _slice else : newN = np . ceil ( psd . size / float ( _slice ) ) if newN % 2 == 1 : newN = psd . size / _slice newpsd = np . zeros ( int ( newN ) ) for i in range ( 0 , newpsd . size ) : count = 0 for n in range ( i * _slice - P , i * _slice + P + 1 ) : if n > 0 and n < N : count += 1 newpsd [ i ] += psd [ n ] newpsd [ i ] /= float ( count ) if datatype == 'complex' : freq = np . linspace ( 0 , sampling , len ( newpsd ) ) else : df = 1. / sampling freq = np . linspace ( 0 , sampling / 2. , len ( newpsd ) ) return newpsd , freq
5491	def create_config ( cls , cfgfile , nick , twtfile , twturl , disclose_identity , add_news ) : cfgfile_dir = os . path . dirname ( cfgfile ) if not os . path . exists ( cfgfile_dir ) : os . makedirs ( cfgfile_dir ) cfg = configparser . ConfigParser ( ) cfg . add_section ( "twtxt" ) cfg . set ( "twtxt" , "nick" , nick ) cfg . set ( "twtxt" , "twtfile" , twtfile ) cfg . set ( "twtxt" , "twturl" , twturl ) cfg . set ( "twtxt" , "disclose_identity" , str ( disclose_identity ) ) cfg . set ( "twtxt" , "character_limit" , "140" ) cfg . set ( "twtxt" , "character_warning" , "140" ) cfg . add_section ( "following" ) if add_news : cfg . set ( "following" , "twtxt" , "https://buckket.org/twtxt_news.txt" ) conf = cls ( cfgfile , cfg ) conf . write_config ( ) return conf
2710	def make_sentence ( sent_text ) : lex = [ ] idx = 0 for word in sent_text : if len ( word ) > 0 : if ( idx > 0 ) and not ( word [ 0 ] in ",.:;!?-\"'" ) : lex . append ( " " ) lex . append ( word ) idx += 1 return "" . join ( lex )
6803	def assume_localhost ( self ) : if not self . genv . host_string : self . genv . host_string = 'localhost' self . genv . hosts = [ 'localhost' ] self . genv . user = getpass . getuser ( )
4667	def refresh ( self ) : dict . __init__ ( self , self . blockchain . rpc . get_object ( self . identifier ) , blockchain_instance = self . blockchain , )
10559	def convert_cygwin_path ( path ) : try : win_path = subprocess . check_output ( [ "cygpath" , "-aw" , path ] , universal_newlines = True ) . strip ( ) except ( FileNotFoundError , subprocess . CalledProcessError ) : logger . exception ( "Call to cygpath failed." ) raise return win_path
3990	def _nginx_http_spec ( port_spec , bridge_ip ) : server_string_spec = "\t server {\n" server_string_spec += "\t \t {}\n" . format ( _nginx_max_file_size_string ( ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_listen_string ( port_spec ) ) server_string_spec += "\t \t {}\n" . format ( _nginx_server_name_string ( port_spec ) ) server_string_spec += _nginx_location_spec ( port_spec , bridge_ip ) server_string_spec += _custom_502_page ( ) server_string_spec += "\t }\n" return server_string_spec
10582	def set_parent_path ( self , value ) : self . _parent_path = value self . path = value + r'/' + self . name self . _update_childrens_parent_path ( )
9335	def copy ( a ) : shared = anonymousmemmap ( a . shape , dtype = a . dtype ) shared [ : ] = a [ : ] return shared
3804	def calculate ( self , T , method ) : r if method == GHARAGHEIZI_G : kg = Gharagheizi_gas ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == DIPPR_9B : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = DIPPR9B ( T , self . MW , Cvgm , mug , self . Tc ) elif method == CHUNG : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Chung ( T , self . MW , self . Tc , self . omega , Cvgm , mug ) elif method == ELI_HANLEY : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm kg = eli_hanley ( T , self . MW , self . Tc , self . Vc , self . Zc , self . omega , Cvgm ) elif method == EUCKEN_MOD : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken_modified ( self . MW , Cvgm , mug ) elif method == EUCKEN : Cvgm = self . Cvgm ( T ) if hasattr ( self . Cvgm , '__call__' ) else self . Cvgm mug = self . mug ( T ) if hasattr ( self . mug , '__call__' ) else self . mug kg = Eucken ( self . MW , Cvgm , mug ) elif method == DIPPR_PERRY_8E : kg = EQ102 ( T , * self . Perrys2_314_coeffs ) elif method == VDI_PPDS : kg = horner ( self . VDI_PPDS_coeffs , T ) elif method == BAHADORI_G : kg = Bahadori_gas ( T , self . MW ) elif method == COOLPROP : kg = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'g' ) elif method in self . tabular_data : kg = self . interpolate ( T , method ) return kg
3682	def logP ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in CRClogPDict . index : methods . append ( CRC ) if CASRN in SyrresDict2 . index : methods . append ( SYRRES ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CRC : return float ( CRClogPDict . at [ CASRN , 'logP' ] ) elif Method == SYRRES : return float ( SyrresDict2 . at [ CASRN , 'logP' ] ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
66	def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( "Usage of argument 'thickness' in BoundingBox.draw_on_image() " "is deprecated. The argument was renamed to 'size'." ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( "Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s." % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result
9105	def dropbox_factory ( request ) : try : return request . registry . settings [ 'dropbox_container' ] . get_dropbox ( request . matchdict [ 'drop_id' ] ) except KeyError : raise HTTPNotFound ( 'no such dropbox' )
6216	def buffers_exist ( self ) : for buff in self . buffers : if not buff . is_separate_file : continue path = self . path . parent / buff . uri if not os . path . exists ( path ) : raise FileNotFoundError ( "Buffer {} referenced in {} not found" . format ( path , self . path ) )
13631	def _adaptToResource ( self , result ) : if result is None : return NotFound ( ) spinneretResource = ISpinneretResource ( result , None ) if spinneretResource is not None : return SpinneretResource ( spinneretResource ) renderable = IRenderable ( result , None ) if renderable is not None : return _RenderableResource ( renderable ) resource = IResource ( result , None ) if resource is not None : return resource if isinstance ( result , URLPath ) : return Redirect ( str ( result ) ) return result
1606	def run_containers ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] container_id = cl_args [ 'id' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False containers = result [ 'physical_plan' ] [ 'stmgrs' ] all_bolts , all_spouts = set ( ) , set ( ) for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : all_bolts = all_bolts | set ( bolts ) for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : all_spouts = all_spouts | set ( spouts ) stmgrs = containers . keys ( ) stmgrs . sort ( ) if container_id is not None : try : normalized_cid = container_id - 1 if normalized_cid < 0 : raise stmgrs = [ stmgrs [ normalized_cid ] ] except : Log . error ( 'Invalid container id: %d' % container_id ) return False table = [ ] for sid , name in enumerate ( stmgrs ) : cid = sid + 1 host = containers [ name ] [ "host" ] port = containers [ name ] [ "port" ] pid = containers [ name ] [ "pid" ] instances = containers [ name ] [ "instance_ids" ] bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] sys . stdout . flush ( ) print ( tabulate ( table , headers = headers ) ) return True
12839	def init_async ( self , loop ) : super ( PooledAIODatabase , self ) . init_async ( loop ) self . _waiters = collections . deque ( )
6401	def _undouble ( self , word ) : if ( len ( word ) > 1 and word [ - 1 ] == word [ - 2 ] and word [ - 1 ] in { 'd' , 'k' , 't' } ) : return word [ : - 1 ] return word
3433	def remove_groups ( self , group_list ) : if isinstance ( group_list , string_types ) or hasattr ( group_list , "id" ) : warn ( "need to pass in a list" ) group_list = [ group_list ] for group in group_list : if group . id not in self . groups : LOGGER . warning ( "%r not in %r. Ignored." , group , self ) else : self . groups . remove ( group ) group . _model = None
148	def remove_out_of_image ( self , fully = True , partly = False ) : polys_clean = [ poly for poly in self . polygons if not poly . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return PolygonsOnImage ( polys_clean , shape = self . shape )
11172	def posarghelp ( self , indent = 0 , maxindent = 25 , width = 79 ) : docs = [ ] makelabel = lambda posarg : ' ' * indent + posarg . displayname + ': ' helpindent = _autoindent ( [ makelabel ( p ) for p in self . positional_args ] , indent , maxindent ) for posarg in self . positional_args : label = makelabel ( posarg ) text = posarg . formatname + '. ' + posarg . docs wrapped = self . _wrap_labelled ( label , text , helpindent , width ) docs . extend ( wrapped ) return '\n' . join ( docs )
9227	def NextPage ( gh ) : header = dict ( gh . getheaders ( ) ) if 'Link' in header : parts = header [ 'Link' ] . split ( ',' ) for part in parts : subparts = part . split ( ';' ) sub = subparts [ 1 ] . split ( '=' ) if sub [ 0 ] . strip ( ) == 'rel' : if sub [ 1 ] == '"next"' : page = int ( re . match ( r'.*page=(\d+).*' , subparts [ 0 ] , re . IGNORECASE | re . DOTALL | re . UNICODE ) . groups ( ) [ 0 ] ) return page return 0
1251	def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward
2225	def _update_hasher ( hasher , data , types = True ) : if isinstance ( data , ( tuple , list , zip ) ) : needs_iteration = True else : needs_iteration = any ( check ( data ) for check in _HASHABLE_EXTENSIONS . iterable_checks ) if needs_iteration : SEP = b'_,_' ITER_PREFIX = b'_[_' ITER_SUFFIX = b'_]_' iter_ = iter ( data ) hasher . update ( ITER_PREFIX ) try : for item in iter_ : prefix , hashable = _convert_to_hashable ( item , types ) binary_data = prefix + hashable + SEP hasher . update ( binary_data ) except TypeError : _update_hasher ( hasher , item , types ) for item in iter_ : _update_hasher ( hasher , item , types ) hasher . update ( SEP ) hasher . update ( ITER_SUFFIX ) else : prefix , hashable = _convert_to_hashable ( data , types ) binary_data = prefix + hashable hasher . update ( binary_data )
3044	def _generate_refresh_request_body ( self ) : body = urllib . parse . urlencode ( { 'grant_type' : 'refresh_token' , 'client_id' : self . client_id , 'client_secret' : self . client_secret , 'refresh_token' : self . refresh_token , } ) return body
410	def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] inputs = self . _to_bc_h_w ( inputs , input_shape ) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) coords = tf . expand_dims ( grid_offset , 0 ) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals
8636	def get_milestone_by_id ( session , milestone_id , user_details = None ) : endpoint = 'milestones/{}' . format ( milestone_id ) response = make_get_request ( session , endpoint , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MilestonesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
2350	def wait_for_page_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_page_to_load ( page = self ) return self
614	def _getPredictedField ( options ) : if not options [ 'inferenceArgs' ] or not options [ 'inferenceArgs' ] [ 'predictedField' ] : return None , None predictedField = options [ 'inferenceArgs' ] [ 'predictedField' ] predictedFieldInfo = None includedFields = options [ 'includedFields' ] for info in includedFields : if info [ 'fieldName' ] == predictedField : predictedFieldInfo = info break if predictedFieldInfo is None : raise ValueError ( "Predicted field '%s' does not exist in included fields." % predictedField ) predictedFieldType = predictedFieldInfo [ 'fieldType' ] return predictedField , predictedFieldType
4731	def terminate ( self ) : if self . __thread : cmd = [ "who am i" ] status , output , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: who am i failed" ) return 1 tty = output . split ( ) [ 1 ] cmd = [ "pkill -f '{}' -t '{}'" . format ( " " . join ( self . __prefix ) , tty ) ] status , _ , _ = cij . util . execute ( cmd , shell = True , echo = True ) if status : cij . warn ( "cij.dmesg.terminate: pkill failed" ) return 1 self . __thread . join ( ) self . __thread = None return 0
12129	def _build_specs ( self , specs , kwargs , fp_precision ) : if specs is None : overrides = param . ParamOverrides ( self , kwargs , allow_extra_keywords = True ) extra_kwargs = overrides . extra_keywords ( ) kwargs = dict ( [ ( k , v ) for ( k , v ) in kwargs . items ( ) if k not in extra_kwargs ] ) rounded_specs = list ( self . round_floats ( [ extra_kwargs ] , fp_precision ) ) if extra_kwargs == { } : return [ ] , kwargs , True else : return rounded_specs , kwargs , False return list ( self . round_floats ( specs , fp_precision ) ) , kwargs , True
10998	def moment ( p , v , order = 1 ) : if order == 1 : return ( v * p ) . sum ( ) elif order == 2 : return np . sqrt ( ( ( v ** 2 ) * p ) . sum ( ) - ( v * p ) . sum ( ) ** 2 )
13405	def prepareImages ( self , fileName , logType ) : import subprocess if self . imageType == "png" : self . imagePixmap . save ( fileName + ".png" , "PNG" , - 1 ) if logType == "Physics" : makePostScript = "convert " + fileName + ".png " + fileName + ".ps" process = subprocess . Popen ( makePostScript , shell = True ) process . wait ( ) thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 ) else : renameImage = "cp " + self . image + " " + fileName + ".gif" process = subprocess . Popen ( renameImage , shell = True ) process . wait ( ) if logType == "Physics" : thumbnailPixmap = self . imagePixmap . scaled ( 500 , 450 , Qt . KeepAspectRatio ) thumbnailPixmap . save ( fileName + ".png" , "PNG" , - 1 )
1233	def from_spec ( spec , kwargs = None ) : distribution = util . get_object ( obj = spec , predefined_objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution
12954	def _rem_id_from_keys ( self , pk , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . srem ( self . _get_ids_key ( ) , pk )
11865	def pointwise_product ( self , other , bn ) : "Multiply two factors, combining their variables." vars = list ( set ( self . vars ) | set ( other . vars ) ) cpt = dict ( ( event_values ( e , vars ) , self . p ( e ) * other . p ( e ) ) for e in all_events ( vars , bn , { } ) ) return Factor ( vars , cpt )
9159	def delete_license_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted_uids = [ x [ 'uid' ] for x in request . json . get ( 'licensors' , [ ] ) ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_license_requests ( cursor , uuid_ , posted_uids ) resp = request . response resp . status_int = 200 return resp
8290	def _description ( self ) : meta = self . find ( "meta" , { "name" : "description" } ) if isinstance ( meta , dict ) and meta . has_key ( "content" ) : return meta [ "content" ] else : return u""
6106	def masses_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_mass = 'angular' , critical_surface_density = None ) : return list ( map ( lambda galaxy : galaxy . mass_within_ellipse_in_units ( major_axis = major_axis , unit_mass = unit_mass , kpc_per_arcsec = self . kpc_per_arcsec , critical_surface_density = critical_surface_density ) , self . galaxies ) )
4371	def spawn ( self , fn , * args , ** kwargs ) : if hasattr ( self , 'exception_handler_decorator' ) : fn = self . exception_handler_decorator ( fn ) new = gevent . spawn ( fn , * args , ** kwargs ) self . jobs . append ( new ) return new
1459	def load_pex ( path_to_pex , include_deps = True ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) if abs_path_to_pex not in sys . path : sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) if include_deps : for dep in _get_deps_list ( abs_path_to_pex ) : to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) if to_join not in sys . path : Log . debug ( "Add a new dependency to the path: %s" % dep ) sys . path . insert ( 0 , to_join ) Log . debug ( "Python path: %s" % str ( sys . path ) )
13429	def get_sites ( self ) : url = "/2/sites" data = self . _get_resource ( url ) sites = [ ] for entry in data [ 'sites' ] : sites . append ( self . site_from_json ( entry ) ) return sites
9916	def validate_is_primary ( self , is_primary ) : if is_primary and not ( self . instance and self . instance . is_verified ) : raise serializers . ValidationError ( _ ( "Unverified email addresses may not be used as the " "primary address." ) ) return is_primary
4713	def hooks_setup ( trun , parent , hnames = None ) : hooks = { "enter" : [ ] , "exit" : [ ] } if hnames is None : return hooks for hname in hnames : for med in HOOK_PATTERNS : for ptn in HOOK_PATTERNS [ med ] : fpath = os . sep . join ( [ trun [ "conf" ] [ "HOOKS" ] , ptn % hname ] ) if not os . path . exists ( fpath ) : continue hook = hook_setup ( parent , fpath ) if not hook : continue hooks [ med ] . append ( hook ) if not hooks [ "enter" ] + hooks [ "exit" ] : cij . err ( "rnr:hooks_setup:FAIL { hname: %r has no files }" % hname ) return None return hooks
2738	def assign ( self , droplet_id ) : return self . get_data ( "floating_ips/%s/actions/" % self . ip , type = POST , params = { "type" : "assign" , "droplet_id" : droplet_id } )
9072	def build_engine_session ( connection , echo = False , autoflush = None , autocommit = None , expire_on_commit = None , scopefunc = None ) : if connection is None : raise ValueError ( 'can not build engine when connection is None' ) engine = create_engine ( connection , echo = echo ) autoflush = autoflush if autoflush is not None else False autocommit = autocommit if autocommit is not None else False expire_on_commit = expire_on_commit if expire_on_commit is not None else True log . debug ( 'auto flush: %s, auto commit: %s, expire on commmit: %s' , autoflush , autocommit , expire_on_commit ) session_maker = sessionmaker ( bind = engine , autoflush = autoflush , autocommit = autocommit , expire_on_commit = expire_on_commit , ) session = scoped_session ( session_maker , scopefunc = scopefunc ) return engine , session
9149	def count_relations ( self ) -> int : if self . edge_model is ... : raise Bio2BELMissingEdgeModelError ( 'edge_edge model is undefined/count_bel_relations is not overridden' ) elif isinstance ( self . edge_model , list ) : return sum ( self . _count_model ( m ) for m in self . edge_model ) else : return self . _count_model ( self . edge_model )
3711	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeLiquids ] return Amgat ( zs , Vms ) elif method == COSTALD_MIXTURE : return COSTALD_mixture ( zs , T , self . Tcs , self . Vcs , self . omegas ) elif method == COSTALD_MIXTURE_FIT : return COSTALD_mixture ( zs , T , self . Tcs , self . COSTALD_Vchars , self . COSTALD_omegas ) elif method == RACKETT : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Zcs ) elif method == RACKETT_PARAMETERS : return Rackett_mixture ( T , zs , self . MWs , self . Tcs , self . Pcs , self . Z_RAs ) elif method == LALIBERTE : ws = list ( ws ) ws . pop ( self . index_w ) rho = Laliberte_density ( T , ws , self . wCASs ) MW = mixing_simple ( zs , self . MWs ) return rho_to_Vm ( rho , MW ) else : raise Exception ( 'Method not valid' )
13406	def submitEntry ( self ) : mcclogs , physlogs = self . selectedLogs ( ) success = True if mcclogs != [ ] : if not self . acceptedUser ( "MCC" ) : QMessageBox ( ) . warning ( self , "Invalid User" , "Please enter a valid user name!" ) return fileName = self . xmlSetup ( "MCC" , mcclogs ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "MCC" ) success = self . sendToLogbook ( fileName , "MCC" ) if physlogs != [ ] : for i in range ( len ( physlogs ) ) : fileName = self . xmlSetup ( "Physics" , physlogs [ i ] ) if fileName is None : return if not self . imagePixmap . isNull ( ) : self . prepareImages ( fileName , "Physics" ) success_phys = self . sendToLogbook ( fileName , "Physics" , physlogs [ i ] ) success = success and success_phys self . done ( success )
12487	def create_folder ( dirpath , overwrite = False ) : if not overwrite : while op . exists ( dirpath ) : dirpath += '+' os . makedirs ( dirpath , exist_ok = overwrite ) return dirpath
1332	def gradient ( self , image = None , label = None , strict = True ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class assert not strict or self . in_bounds ( image ) self . _total_gradient_calls += 1 gradient = self . __model . gradient ( image , label ) assert gradient . shape == image . shape return gradient
7777	def __make_fn ( self ) : s = [ ] if self . n . prefix : s . append ( self . n . prefix ) if self . n . given : s . append ( self . n . given ) if self . n . middle : s . append ( self . n . middle ) if self . n . family : s . append ( self . n . family ) if self . n . suffix : s . append ( self . n . suffix ) s = u" " . join ( s ) self . content [ "FN" ] = VCardString ( "FN" , s , empty_ok = True )
11257	def values ( prev , * keys , ** kw ) : d = next ( prev ) if isinstance ( d , dict ) : yield [ d [ k ] for k in keys if k in d ] for d in prev : yield [ d [ k ] for k in keys if k in d ] else : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ] for d in prev : yield [ d [ i ] for i in keys if 0 <= i < len ( d ) ]
9087	async def update ( self ) -> None : _LOGGER . debug ( "Requesting state update from server (S00, S14)" ) await asyncio . gather ( self . send_command ( 'S00' ) , self . send_command ( 'S14' ) , )
242	def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( "## Performance Relative to Common Risk Factors" ) ) perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig
4284	def generate_thumbnail ( source , outname , box , delay , fit = True , options = None , converter = 'ffmpeg' ) : logger = logging . getLogger ( __name__ ) tmpfile = outname + ".tmp.jpg" cmd = [ converter , '-i' , source , '-an' , '-r' , '1' , '-ss' , delay , '-vframes' , '1' , '-y' , tmpfile ] logger . debug ( 'Create thumbnail for video: %s' , ' ' . join ( cmd ) ) check_subprocess ( cmd , source , outname ) image . generate_thumbnail ( tmpfile , outname , box , fit = fit , options = options ) os . unlink ( tmpfile )
6049	def set_defaults ( key ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( phase , new_value ) : new_value = new_value or [ ] for item in new_value : galaxy = new_value [ item ] if isinstance ( item , str ) else item galaxy . redshift = galaxy . redshift or conf . instance . general . get ( "redshift" , key , float ) return func ( phase , new_value ) return wrapper return decorator
5713	def is_safe_path ( path ) : contains_windows_var = lambda val : re . match ( r'%.+%' , val ) contains_posix_var = lambda val : re . match ( r'\$.+' , val ) unsafeness_conditions = [ os . path . isabs ( path ) , ( '..%s' % os . path . sep ) in path , path . startswith ( '~' ) , os . path . expandvars ( path ) != path , contains_windows_var ( path ) , contains_posix_var ( path ) , ] return not any ( unsafeness_conditions )
4890	def update_course_runs ( self , course_runs , enterprise_customer , enterprise_context ) : updated_course_runs = [ ] for course_run in course_runs : track_selection_url = utils . get_course_track_selection_url ( course_run = course_run , query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) , ) enrollment_url = enterprise_customer . get_course_run_enrollment_url ( course_run . get ( 'key' ) ) course_run . update ( { 'enrollment_url' : enrollment_url , 'track_selection_url' : track_selection_url , } ) marketing_url = course_run . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) course_run . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) updated_course_runs . append ( course_run ) return updated_course_runs
11368	def punctuate_authorname ( an ) : name = an . strip ( ) parts = [ x for x in name . split ( ',' ) if x != '' ] ret_str = '' for idx , part in enumerate ( parts ) : subparts = part . strip ( ) . split ( ' ' ) for sidx , substr in enumerate ( subparts ) : ret_str += substr if len ( substr ) == 1 : ret_str += '.' if sidx < ( len ( subparts ) - 1 ) : ret_str += ' ' if idx < ( len ( parts ) - 1 ) : ret_str += ', ' return ret_str . strip ( )
4945	def get_program_data_sharing_consent ( username , program_uuid , enterprise_customer_uuid ) : enterprise_customer = get_enterprise_customer ( enterprise_customer_uuid ) discovery_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) course_ids = discovery_client . get_program_course_keys ( program_uuid ) child_consents = ( get_data_sharing_consent ( username , enterprise_customer_uuid , course_id = individual_course_id ) for individual_course_id in course_ids ) return ProxyDataSharingConsent . from_children ( program_uuid , * child_consents )
11524	def mfa_otp_login ( self , temp_token , one_time_pass ) : parameters = dict ( ) parameters [ 'mfaTokenId' ] = temp_token parameters [ 'otp' ] = one_time_pass response = self . request ( 'midas.mfa.otp.login' , parameters ) return response [ 'token' ]
11145	def get_repository_state ( self , relaPath = None ) : state = [ ] def _walk_dir ( relaPath , dirList ) : dirDict = { 'type' : 'dir' , 'exists' : os . path . isdir ( os . path . join ( self . __path , relaPath ) ) , 'pyrepdirinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) , } state . append ( { relaPath : dirDict } ) for fname in sorted ( [ f for f in dirList if isinstance ( f , basestring ) ] ) : relaFilePath = os . path . join ( relaPath , fname ) realFilePath = os . path . join ( self . __path , relaFilePath ) fileDict = { 'type' : 'file' , 'exists' : os . path . isfile ( realFilePath ) , 'pyrepfileinfo' : os . path . isfile ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) ) , } state . append ( { relaFilePath : fileDict } ) for ddict in sorted ( [ d for d in dirList if isinstance ( d , dict ) ] , key = lambda k : list ( k ) [ 0 ] ) : dirname = list ( ddict ) [ 0 ] _walk_dir ( relaPath = os . path . join ( relaPath , dirname ) , dirList = ddict [ dirname ] ) if relaPath is None : _walk_dir ( relaPath = '' , dirList = self . __repo [ 'walk_repo' ] ) else : assert isinstance ( relaPath , basestring ) , "relaPath must be None or a str" relaPath = self . to_repo_relative_path ( path = relaPath , split = False ) spath = relaPath . split ( os . sep ) dirList = self . __repo [ 'walk_repo' ] while len ( spath ) : dirname = spath . pop ( 0 ) dList = [ d for d in dirList if isinstance ( d , dict ) ] if not len ( dList ) : dirList = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : dirList = None break dirList = cDict [ 0 ] [ dirname ] if dirList is not None : _walk_dir ( relaPath = relaPath , dirList = dirList ) return state
6094	def mapping_matrix_from_sub_to_pix ( sub_to_pix , pixels , regular_pixels , sub_to_regular , sub_grid_fraction ) : mapping_matrix = np . zeros ( ( regular_pixels , pixels ) ) for sub_index in range ( sub_to_regular . shape [ 0 ] ) : mapping_matrix [ sub_to_regular [ sub_index ] , sub_to_pix [ sub_index ] ] += sub_grid_fraction return mapping_matrix
4024	def _get_host_only_mac_address ( ) : vm_config = _get_vm_config ( ) for line in vm_config : if line . startswith ( 'hostonlyadapter' ) : adapter_number = int ( line [ 15 : 16 ] ) break else : raise ValueError ( 'No host-only adapter is defined for the Dusty VM' ) for line in vm_config : if line . startswith ( 'macaddress{}' . format ( adapter_number ) ) : return line . split ( '=' ) [ 1 ] . strip ( '"' ) . lower ( ) raise ValueError ( 'Could not find MAC address for adapter number {}' . format ( adapter_number ) )
1657	def IsInitializerList ( clean_lines , linenum ) : for i in xrange ( linenum , 1 , - 1 ) : line = clean_lines . elided [ i ] if i == linenum : remove_function_body = Match ( r'^(.*)\{\s*$' , line ) if remove_function_body : line = remove_function_body . group ( 1 ) if Search ( r'\s:\s*\w+[({]' , line ) : return True if Search ( r'\}\s*,\s*$' , line ) : return True if Search ( r'[{};]\s*$' , line ) : return False return False
597	def _compute ( self , inputs , outputs ) : if self . _tfdr is None : raise RuntimeError ( "TM has not been initialized" ) self . _conditionalBreak ( ) self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = " " . join ( [ "%d" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ "activeCells" ] . fill ( 0 ) outputs [ "activeCells" ] [ activeIndices ] = 1 outputs [ "predictedActiveCells" ] . fill ( 0 ) outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1
5109	def set_num_servers ( self , n ) : if not isinstance ( n , numbers . Integral ) and n is not infty : the_str = "n must be an integer or infinity.\n{0}" raise TypeError ( the_str . format ( str ( self ) ) ) elif n <= 0 : the_str = "n must be a positive integer or infinity.\n{0}" raise ValueError ( the_str . format ( str ( self ) ) ) else : self . num_servers = n
1538	def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : spout_spec = spout_cls . spec ( name = name , par = par , config = config , optional_outputs = optional_outputs ) self . add_spec ( spout_spec ) return spout_spec
7905	def forget ( self , rs ) : try : del self . rooms [ rs . room_jid . bare ( ) . as_unicode ( ) ] except KeyError : pass
13153	def dict_cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( _CursorType . DICT ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
12341	def _set_path ( self , path ) : "Set self.path, self.dirname and self.basename." import os . path self . path = os . path . abspath ( path ) self . dirname = os . path . dirname ( path ) self . basename = os . path . basename ( path )
6226	def rot_state ( self , x , y ) : if self . last_x is None : self . last_x = x if self . last_y is None : self . last_y = y x_offset = self . last_x - x y_offset = self . last_y - y self . last_x = x self . last_y = y x_offset *= self . mouse_sensitivity y_offset *= self . mouse_sensitivity self . yaw -= x_offset self . pitch += y_offset if self . pitch > 85.0 : self . pitch = 85.0 if self . pitch < - 85.0 : self . pitch = - 85.0 self . _update_yaw_and_pitch ( )
3625	def encode ( latitude , longitude , precision = 12 ) : lat_interval , lon_interval = ( - 90.0 , 90.0 ) , ( - 180.0 , 180.0 ) geohash = [ ] bits = [ 16 , 8 , 4 , 2 , 1 ] bit = 0 ch = 0 even = True while len ( geohash ) < precision : if even : mid = ( lon_interval [ 0 ] + lon_interval [ 1 ] ) / 2 if longitude > mid : ch |= bits [ bit ] lon_interval = ( mid , lon_interval [ 1 ] ) else : lon_interval = ( lon_interval [ 0 ] , mid ) else : mid = ( lat_interval [ 0 ] + lat_interval [ 1 ] ) / 2 if latitude > mid : ch |= bits [ bit ] lat_interval = ( mid , lat_interval [ 1 ] ) else : lat_interval = ( lat_interval [ 0 ] , mid ) even = not even if bit < 4 : bit += 1 else : geohash += __base32 [ ch ] bit = 0 ch = 0 return '' . join ( geohash )
991	def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc
13702	def _after ( self , response ) : if getattr ( request , '_tracy_exclude' , False ) : return response duration = None if getattr ( request , '_tracy_start_time' , None ) : duration = monotonic ( ) - request . _tracy_start_time trace_id = None if getattr ( request , '_tracy_id' , None ) : trace_id = request . _tracy_id response . headers [ trace_header_id ] = trace_id trace_client = None if getattr ( request , '_tracy_client' , None ) : trace_client = request . _tracy_client d = { 'status_code' : response . status_code , 'url' : request . base_url , 'client_ip' : request . remote_addr , 'trace_name' : trace_client , 'trace_id' : trace_id , 'trace_duration' : duration } logger . info ( None , extra = d ) return response
13010	def print_line ( text ) : try : signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) except ValueError : pass try : sys . stdout . write ( text ) if not text . endswith ( '\n' ) : sys . stdout . write ( '\n' ) sys . stdout . flush ( ) except IOError : sys . exit ( 0 )
11636	def refresh_access_token ( self , ) : logger . debug ( "REFRESHING TOKEN" ) self . token_time = time . time ( ) credentials = { 'token_time' : self . token_time } if self . oauth_version == 'oauth1' : self . access_token , self . access_token_secret = self . oauth . get_access_token ( self . access_token , self . access_token_secret , params = { "oauth_session_handle" : self . session_handle } ) credentials . update ( { 'access_token' : self . access_token , 'access_token_secret' : self . access_token_secret , 'session_handle' : self . session_handle , 'token_time' : self . token_time } ) else : headers = self . generate_oauth2_headers ( ) raw_access = self . oauth . get_raw_access_token ( data = { "refresh_token" : self . refresh_token , 'redirect_uri' : self . callback_uri , 'grant_type' : 'refresh_token' } , headers = headers ) credentials . update ( self . oauth2_access_parser ( raw_access ) ) return credentials
11959	def _check_nm ( nm , notation ) : _NM_CHECK_FUNCT = { NM_DOT : _dot_to_dec , NM_HEX : _hex_to_dec , NM_BIN : _bin_to_dec , NM_OCT : _oct_to_dec , NM_DEC : _dec_to_dec_long } try : dec = _NM_CHECK_FUNCT [ notation ] ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
11570	def set_brightness ( self , brightness ) : if brightness > 15 : brightness = 15 brightness |= 0xE0 self . brightness = brightness self . firmata . i2c_write ( 0x70 , brightness )
13642	def check_confirmations_or_resend ( self , use_open_peers = False , ** kw ) : if self . confirmations ( ) == 0 : self . send ( use_open_peers , ** kw )
12042	def XMLtoPython ( xmlStr = r"C:\Apps\pythonModules\GSTemp.xml" ) : if os . path . exists ( xmlStr ) : with open ( xmlStr ) as f : xmlStr = f . read ( ) print ( xmlStr ) print ( "DONE" ) return
6084	def blurred_image_of_planes_from_1d_images_and_convolver ( total_planes , image_plane_image_1d_of_planes , image_plane_blurring_image_1d_of_planes , convolver , map_to_scaled_array ) : blurred_image_of_planes = [ ] for plane_index in range ( total_planes ) : if np . count_nonzero ( image_plane_image_1d_of_planes [ plane_index ] ) > 0 : blurred_image_1d_of_plane = blurred_image_1d_from_1d_unblurred_and_blurring_images ( unblurred_image_1d = image_plane_image_1d_of_planes [ plane_index ] , blurring_image_1d = image_plane_blurring_image_1d_of_planes [ plane_index ] , convolver = convolver ) blurred_image_of_plane = map_to_scaled_array ( array_1d = blurred_image_1d_of_plane ) blurred_image_of_planes . append ( blurred_image_of_plane ) else : blurred_image_of_planes . append ( None ) return blurred_image_of_planes
5899	def get_double_or_single_prec_mdrun ( ) : try : gromacs . mdrun_d ( h = True , stdout = False , stderr = False ) logger . debug ( "using double precision gromacs.mdrun_d" ) return gromacs . mdrun_d except ( AttributeError , GromacsError , OSError ) : wmsg = "No 'mdrun_d' binary found so trying 'mdrun' instead.\n" "(Note that energy minimization runs better with mdrun_d.)" logger . warn ( wmsg ) warnings . warn ( wmsg , category = AutoCorrectionWarning ) return gromacs . mdrun
8932	def capture ( cmd , ** kw ) : kw = kw . copy ( ) kw [ 'hide' ] = 'out' if not kw . get ( 'echo' , False ) : kw [ 'echo' ] = False ignore_failures = kw . pop ( 'ignore_failures' , False ) try : return invoke_run ( cmd , ** kw ) . stdout . strip ( ) except exceptions . Failure as exc : if not ignore_failures : notify . error ( "Command `{}` failed with RC={}!" . format ( cmd , exc . result . return_code , ) ) raise
1647	def CheckCheck ( filename , clean_lines , linenum , error ) : lines = clean_lines . elided ( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) if not check_macro : return ( last_line , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , start_pos ) if end_pos < 0 : return if not Match ( r'\s*;' , last_line [ end_pos : ] ) : return if linenum == end_line : expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] else : expression = lines [ linenum ] [ start_pos + 1 : ] for i in xrange ( linenum + 1 , end_line ) : expression += lines [ i ] expression += last_line [ 0 : end_pos - 1 ] lhs = '' rhs = '' operator = None while expression : matched = Match ( r'^\s*(<<|<<=|>>|>>=|->\*|->|&&|\|\||' r'==|!=|>=|>|<=|<|\()(.*)$' , expression ) if matched : token = matched . group ( 1 ) if token == '(' : expression = matched . group ( 2 ) ( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) if end < 0 : return lhs += '(' + expression [ 0 : end ] expression = expression [ end : ] elif token in ( '&&' , '||' ) : return elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : lhs += token expression = matched . group ( 2 ) else : operator = token rhs = matched . group ( 2 ) break else : matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) if not matched : matched = Match ( r'^(\s*\S)(.*)$' , expression ) if not matched : break lhs += matched . group ( 1 ) expression = matched . group ( 2 ) if not ( lhs and operator and rhs ) : return if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : return lhs = lhs . strip ( ) rhs = rhs . strip ( ) match_constant = r'^([-+]?(\d+|0[xX][0-9a-fA-F]+)[lLuU]{0,3}|".*"|\'.*\')$' if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : error ( filename , linenum , 'readability/check' , 2 , 'Consider using %s instead of %s(a %s b)' % ( _CHECK_REPLACEMENT [ check_macro ] [ operator ] , check_macro , operator ) )
12488	def _import_config ( filepath ) : if not op . isfile ( filepath ) : raise IOError ( 'Data config file not found. ' 'Got: {0}' . format ( filepath ) ) cfg = import_pyfile ( filepath ) if not hasattr ( cfg , 'root_path' ) : raise KeyError ( 'Config file root_path key not found.' ) if not hasattr ( cfg , 'filetree' ) : raise KeyError ( 'Config file filetree key not found.' ) return cfg . root_path , cfg . filetree
10666	def stoichiometry_coefficients ( compound , elements ) : stoichiometry = parse_compound ( compound . strip ( ) ) . count ( ) return [ stoichiometry [ element ] for element in elements ]
7707	def save_roster ( self , dest , pretty = True ) : if self . roster is None : raise ValueError ( "No roster" ) element = self . roster . as_xml ( ) if pretty : if len ( element ) : element . text = u'\n ' p_child = None for child in element : if p_child is not None : p_child . tail = u'\n ' if len ( child ) : child . text = u'\n ' p_grand = None for grand in child : if p_grand is not None : p_grand . tail = u'\n ' p_grand = grand if p_grand is not None : p_grand . tail = u'\n ' p_child = child if p_child is not None : p_child . tail = u"\n" tree = ElementTree . ElementTree ( element ) tree . write ( dest , "utf-8" )
3400	def fill ( self , iterations = 1 ) : used_reactions = list ( ) for i in range ( iterations ) : self . model . slim_optimize ( error_value = None , message = 'gapfilling optimization failed' ) solution = [ self . model . reactions . get_by_id ( ind . rxn_id ) for ind in self . indicators if ind . _get_primal ( ) > self . integer_threshold ] if not self . validate ( solution ) : raise RuntimeError ( 'failed to validate gapfilled model, ' 'try lowering the integer_threshold' ) used_reactions . append ( solution ) self . update_costs ( ) return used_reactions
5404	def _get_delocalization_env ( self , outputs , user_project ) : non_empty_outputs = [ var for var in outputs if var . value ] env = { 'OUTPUT_COUNT' : str ( len ( non_empty_outputs ) ) } for idx , var in enumerate ( non_empty_outputs ) : env [ 'OUTPUT_{}' . format ( idx ) ] = var . name env [ 'OUTPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'OUTPUT_SRC_{}' . format ( idx ) ] = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) if '*' in var . uri . basename : dst = var . uri . path else : dst = var . uri env [ 'OUTPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
326	def simulate_paths ( is_returns , num_days , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = np . empty ( ( num_samples , num_days ) ) seed = np . random . RandomState ( seed = random_seed ) for i in range ( num_samples ) : samples [ i , : ] = is_returns . sample ( num_days , replace = True , random_state = seed ) return samples
12766	def distances ( self ) : distances = [ ] for label in self . labels : joint = self . joints . get ( label ) distances . append ( [ np . nan , np . nan , np . nan ] if joint is None else np . array ( joint . getAnchor ( ) ) - joint . getAnchor2 ( ) ) return np . array ( distances )
13675	def add_path_object ( self , * args ) : for obj in args : obj . bundle = self self . files . append ( obj )
12726	def stop_cfms ( self , stop_cfms ) : _set_params ( self . ode_obj , 'StopCFM' , stop_cfms , self . ADOF + self . LDOF )
5333	def config_logging ( debug ) : if debug : logging . basicConfig ( level = logging . DEBUG , format = '%(asctime)s %(message)s' ) logging . debug ( "Debug mode activated" ) else : logging . basicConfig ( level = logging . INFO , format = '%(asctime)s %(message)s' )
10541	def get_tasks ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'task' , params = params ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : raise
10383	def main ( ) : logging . basicConfig ( level = logging . INFO ) log . setLevel ( logging . INFO ) bms_base = get_bms_base ( ) neurommsig_base = get_neurommsig_base ( ) neurommsig_excel_dir = os . path . join ( neurommsig_base , 'resources' , 'excels' , 'neurommsig' ) nift_values = get_nift_values ( ) log . info ( 'Starting Alzheimers' ) ad_path = os . path . join ( neurommsig_excel_dir , 'alzheimers' , 'alzheimers.xlsx' ) ad_df = preprocess ( ad_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'alzheimers' , 'neurommsigdb_ad.bel' ) , 'w' ) as ad_file : write_neurommsig_bel ( ad_file , ad_df , mesh_alzheimer , nift_values ) log . info ( 'Starting Parkinsons' ) pd_path = os . path . join ( neurommsig_excel_dir , 'parkinsons' , 'parkinsons.xlsx' ) pd_df = preprocess ( pd_path ) with open ( os . path . join ( bms_base , 'aetionomy' , 'parkinsons' , 'neurommsigdb_pd.bel' ) , 'w' ) as pd_file : write_neurommsig_bel ( pd_file , pd_df , mesh_parkinson , nift_values )
317	def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , ** kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values
10656	def count_with_multiplier ( groups , multiplier ) : counts = collections . defaultdict ( float ) for group in groups : for element , count in group . count ( ) . items ( ) : counts [ element ] += count * multiplier return counts
3124	def _verify_time_range ( payload_dict ) : now = int ( time . time ( ) ) issued_at = payload_dict . get ( 'iat' ) if issued_at is None : raise AppIdentityError ( 'No iat field in token: {0}' . format ( payload_dict ) ) expiration = payload_dict . get ( 'exp' ) if expiration is None : raise AppIdentityError ( 'No exp field in token: {0}' . format ( payload_dict ) ) if expiration >= now + MAX_TOKEN_LIFETIME_SECS : raise AppIdentityError ( 'exp field too far in future: {0}' . format ( payload_dict ) ) earliest = issued_at - CLOCK_SKEW_SECS if now < earliest : raise AppIdentityError ( 'Token used too early, {0} < {1}: {2}' . format ( now , earliest , payload_dict ) ) latest = expiration + CLOCK_SKEW_SECS if now > latest : raise AppIdentityError ( 'Token used too late, {0} > {1}: {2}' . format ( now , latest , payload_dict ) )
8219	def do_unfullscreen ( self , widget ) : self . unfullscreen ( ) self . is_fullscreen = False self . bot . _screen_ratio = None
43	def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : if replay_strategy == 'future' : future_p = 1 - ( 1. / ( 1 + replay_k ) ) else : future_p = 0 def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : T = episode_batch [ 'u' ] . shape [ 1 ] rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] batch_size = batch_size_in_transitions episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) t_samples = np . random . randint ( T , size = batch_size ) transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) for key in episode_batch . keys ( ) } her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) future_offset = future_offset . astype ( int ) future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] transitions [ 'g' ] [ her_indexes ] = future_ag info = { } for key , value in transitions . items ( ) : if key . startswith ( 'info_' ) : info [ key . replace ( 'info_' , '' ) ] = value reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } reward_params [ 'info' ] = info transitions [ 'r' ] = reward_fun ( ** reward_params ) transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) for k in transitions . keys ( ) } assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) return transitions return _sample_her_transitions
6177	def reduce_chunk ( func , array ) : res = [ ] for slice in iter_chunk_slice ( array . shape [ - 1 ] , array . chunkshape [ - 1 ] ) : res . append ( func ( array [ ... , slice ] ) ) return func ( res )
8186	def draw ( self , dx = 0 , dy = 0 , weighted = False , directed = False , highlight = [ ] , traffic = None ) : self . update ( ) s = self . styles . default s . graph_background ( s ) _ctx . push ( ) _ctx . translate ( self . x + dx , self . y + dy ) if traffic : if isinstance ( traffic , bool ) : traffic = 5 for n in self . nodes_by_betweenness ( ) [ : traffic ] : try : s = self . styles [ n . style ] except : s = self . styles . default if s . graph_traffic : s . graph_traffic ( s , n , self . alpha ) s = self . styles . default if s . edges : s . edges ( s , self . edges , self . alpha , weighted , directed ) for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node : s . node ( s , n , self . alpha ) try : s = self . styles . highlight except : s = self . styles . default if s . path : s . path ( s , self , highlight ) for n in self . nodes : try : s = self . styles [ n . style ] except : s = self . styles . default if s . node_label : s . node_label ( s , n , self . alpha ) _ctx . pop ( )
3072	def init_app ( self , app , scopes = None , client_secrets_file = None , client_id = None , client_secret = None , authorize_callback = None , storage = None , ** kwargs ) : self . app = app self . authorize_callback = authorize_callback self . flow_kwargs = kwargs if storage is None : storage = dictionary_storage . DictionaryStorage ( session , key = _CREDENTIALS_KEY ) self . storage = storage if scopes is None : scopes = app . config . get ( 'GOOGLE_OAUTH2_SCOPES' , _DEFAULT_SCOPES ) self . scopes = scopes self . _load_config ( client_secrets_file , client_id , client_secret ) app . register_blueprint ( self . _create_blueprint ( ) )
633	def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse
13874	def _CopyFileLocal ( source_filename , target_filename , copy_symlink = True ) : import shutil try : dir_name = os . path . dirname ( target_filename ) if dir_name and not os . path . isdir ( dir_name ) : os . makedirs ( dir_name ) if copy_symlink and IsLink ( source_filename ) : if os . path . isfile ( target_filename ) or IsLink ( target_filename ) : DeleteFile ( target_filename ) source_filename = ReadLink ( source_filename ) CreateLink ( source_filename , target_filename ) else : if sys . platform == 'win32' : while IsLink ( source_filename ) : link = ReadLink ( source_filename ) if os . path . isabs ( link ) : source_filename = link else : source_filename = os . path . join ( os . path . dirname ( source_filename ) , link ) shutil . copyfile ( source_filename , target_filename ) shutil . copymode ( source_filename , target_filename ) except Exception as e : reraise ( e , 'While executiong _filesystem._CopyFileLocal(%s, %s)' % ( source_filename , target_filename ) )
6183	def git_path_valid ( git_path = None ) : if git_path is None and GIT_PATH is None : return False if git_path is None : git_path = GIT_PATH try : call ( [ git_path , '--version' ] ) return True except OSError : return False
9094	def add_namespace_to_graph ( self , graph : BELGraph ) -> Namespace : namespace = self . upload_bel_namespace ( ) graph . namespace_url [ namespace . keyword ] = namespace . url self . _add_annotation_to_graph ( graph ) return namespace
8995	def relative_file ( self , module , file ) : path = self . _relative_to_absolute ( module , file ) return self . path ( path )
6635	def ignores ( self , path ) : test_path = PurePath ( '/' , path ) test_paths = tuple ( [ test_path ] + list ( test_path . parents ) ) for exp in self . ignore_patterns : for tp in test_paths : if tp . match ( exp ) : logger . debug ( '"%s" ignored ("%s" matched "%s")' , path , tp , exp ) return True return False
8730	def get_nearest_year_for_day ( day ) : now = time . gmtime ( ) result = now . tm_year if day - now . tm_yday > 365 // 2 : result -= 1 if now . tm_yday - day > 365 // 2 : result += 1 return result
8272	def _load ( self , top = 5 , blue = "blue" , archive = None , member = None ) : if archive is None : path = os . path . join ( self . cache , self . name + ".xml" ) xml = open ( path ) . read ( ) else : assert member is not None xml = archive . read ( member ) dom = parseString ( xml ) . documentElement attr = lambda e , a : e . attributes [ a ] . value for e in dom . getElementsByTagName ( "color" ) [ : top ] : w = float ( attr ( e , "weight" ) ) try : rgb = e . getElementsByTagName ( "rgb" ) [ 0 ] clr = color ( float ( attr ( rgb , "r" ) ) , float ( attr ( rgb , "g" ) ) , float ( attr ( rgb , "b" ) ) , float ( attr ( rgb , "a" ) ) , mode = "rgb" ) try : clr . name = attr ( e , "name" ) if clr . name == "blue" : clr = color ( blue ) except : pass except : name = attr ( e , "name" ) if name == "blue" : name = blue clr = color ( name ) for s in e . getElementsByTagName ( "shade" ) : self . ranges . append ( ( clr , shade ( attr ( s , "name" ) ) , w * float ( attr ( s , "weight" ) ) ) )
12267	def check_grad ( f_df , xref , stepsize = 1e-6 , tol = 1e-6 , width = 15 , style = 'round' , out = sys . stdout ) : CORRECT = u'\x1b[32m\N{CHECK MARK}\x1b[0m' INCORRECT = u'\x1b[31m\N{BALLOT X}\x1b[0m' obj , grad = wrap ( f_df , xref , size = 0 ) x0 = destruct ( xref ) df = grad ( x0 ) out . write ( tp . header ( [ "Numerical" , "Analytic" , "Error" ] , width = width , style = style ) + "\n" ) out . flush ( ) def parse_error ( number ) : failure = "\033[91m" passing = "\033[92m" warning = "\033[93m" end = "\033[0m" base = "{}{:0.3e}{}" if error < 0.1 * tol : return base . format ( passing , error , end ) elif error < tol : return base . format ( warning , error , end ) else : return base . format ( failure , error , end ) num_errors = 0 for j in range ( x0 . size ) : dx = np . zeros ( x0 . size ) dx [ j ] = stepsize df_approx = ( obj ( x0 + dx ) - obj ( x0 - dx ) ) / ( 2 * stepsize ) df_analytic = df [ j ] abs_error = np . linalg . norm ( df_approx - df_analytic ) error = abs_error if np . allclose ( abs_error , 0 ) else abs_error / ( np . linalg . norm ( df_analytic ) + np . linalg . norm ( df_approx ) ) num_errors += error >= tol errstr = CORRECT if error < tol else INCORRECT out . write ( tp . row ( [ df_approx , df_analytic , parse_error ( error ) + ' ' + errstr ] , width = width , style = style ) + "\n" ) out . flush ( ) out . write ( tp . bottom ( 3 , width = width , style = style ) + "\n" ) return num_errors
6838	def distrib_id ( ) : with settings ( hide ( 'running' , 'stdout' ) ) : kernel = ( run ( 'uname -s' ) or '' ) . strip ( ) . lower ( ) if kernel == LINUX : if is_file ( '/usr/bin/lsb_release' ) : id_ = run ( 'lsb_release --id --short' ) . strip ( ) . lower ( ) if id in [ 'arch' , 'archlinux' ] : id_ = ARCH return id_ else : if is_file ( '/etc/debian_version' ) : return DEBIAN elif is_file ( '/etc/fedora-release' ) : return FEDORA elif is_file ( '/etc/arch-release' ) : return ARCH elif is_file ( '/etc/redhat-release' ) : release = run ( 'cat /etc/redhat-release' ) if release . startswith ( 'Red Hat Enterprise Linux' ) : return REDHAT elif release . startswith ( 'CentOS' ) : return CENTOS elif release . startswith ( 'Scientific Linux' ) : return SLES elif is_file ( '/etc/gentoo-release' ) : return GENTOO elif kernel == SUNOS : return SUNOS
6914	def generate_rrab_lightcurve ( times , mags = None , errs = None , paramdists = { 'period' : sps . uniform ( loc = 0.45 , scale = 0.35 ) , 'fourierorder' : [ 8 , 11 ] , 'amplitude' : sps . uniform ( loc = 0.4 , scale = 0.5 ) , 'phioffset' : np . pi , } , magsarefluxes = False ) : modeldict = generate_sinusoidal_lightcurve ( times , mags = mags , errs = errs , paramdists = paramdists , magsarefluxes = magsarefluxes ) modeldict [ 'vartype' ] = 'RRab' return modeldict
13022	def query ( self , sql_string , * args , ** kwargs ) : commit = None columns = None if kwargs . get ( 'commit' ) is not None : commit = kwargs . pop ( 'commit' ) if kwargs . get ( 'columns' ) is not None : columns = kwargs . pop ( 'columns' ) query = self . _assemble_simple ( sql_string , * args , ** kwargs ) return self . _execute ( query , commit = commit , working_columns = columns )
5180	def base_url ( self ) : return '{proto}://{host}:{port}{url_path}' . format ( proto = self . protocol , host = self . host , port = self . port , url_path = self . url_path , )
1220	def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches
10554	def get_helping_materials ( project_id , limit = 100 , offset = 0 , last_id = None ) : if last_id is not None : params = dict ( limit = limit , last_id = last_id ) else : params = dict ( limit = limit , offset = offset ) print ( OFFSET_WARNING ) params [ 'project_id' ] = project_id try : res = _pybossa_req ( 'get' , 'helpingmaterial' , params = params ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
10751	def validate_sceneInfo ( self ) : if self . sceneInfo . prefix not in self . __prefixesValid : raise WrongSceneNameError ( 'AWS: Prefix of %s (%s) is invalid' % ( self . sceneInfo . name , self . sceneInfo . prefix ) )
10879	def wrap_and_calc_psf ( xpts , ypts , zpts , func , ** kwargs ) : for t in [ xpts , ypts , zpts ] : if len ( t . shape ) != 1 : raise ValueError ( 'xpts,ypts,zpts must be 1D.' ) dx = 1 if xpts [ 0 ] == 0 else 0 dy = 1 if ypts [ 0 ] == 0 else 0 xg , yg , zg = np . meshgrid ( xpts , ypts , zpts , indexing = 'ij' ) xs , ys , zs = [ pts . size for pts in [ xpts , ypts , zpts ] ] to_return = np . zeros ( [ 2 * xs - dx , 2 * ys - dy , zs ] ) up_corner_psf = func ( xg , yg , zg , ** kwargs ) to_return [ xs - dx : , ys - dy : , : ] = up_corner_psf . copy ( ) if dx == 0 : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ : : - 1 , : , : ] . copy ( ) else : to_return [ : xs - dx , ys - dy : , : ] = up_corner_psf [ - 1 : 0 : - 1 , : , : ] . copy ( ) if dy == 0 : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , : : - 1 , : ] . copy ( ) else : to_return [ xs - dx : , : ys - dy , : ] = up_corner_psf [ : , - 1 : 0 : - 1 , : ] . copy ( ) if ( dx == 0 ) and ( dy == 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , : : - 1 , : ] . copy ( ) elif ( dx == 0 ) and ( dy != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ : : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) elif ( dy == 0 ) and ( dx != 0 ) : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , : : - 1 , : ] . copy ( ) else : to_return [ : xs - dx , : ys - dy , : ] = up_corner_psf [ - 1 : 0 : - 1 , - 1 : 0 : - 1 , : ] . copy ( ) return to_return
7023	def _base64_to_file ( b64str , outfpath , writetostrio = False ) : try : filebytes = base64 . b64decode ( b64str ) if writetostrio : outobj = StrIO ( filebytes ) return outobj else : with open ( outfpath , 'wb' ) as outfd : outfd . write ( filebytes ) if os . path . exists ( outfpath ) : return outfpath else : LOGERROR ( 'could not write output file: %s' % outfpath ) return None except Exception as e : LOGEXCEPTION ( 'failed while trying to convert ' 'b64 string to file %s' % outfpath ) return None
10394	def calculate_average_score_by_annotation ( graph : BELGraph , annotation : str , key : Optional [ str ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) -> Mapping [ str , float ] : candidate_mechanisms = generate_bioprocess_mechanisms ( graph , key = key ) scores : Mapping [ BaseEntity , Tuple ] = calculate_average_scores_on_subgraphs ( subgraphs = candidate_mechanisms , key = key , runs = runs , use_tqdm = use_tqdm , ) subgraph_bp : Mapping [ str , List [ BaseEntity ] ] = defaultdict ( list ) subgraphs : Mapping [ str , BELGraph ] = get_subgraphs_by_annotation ( graph , annotation ) for annotation_value , subgraph in subgraphs . items ( ) : subgraph_bp [ annotation_value ] . extend ( get_nodes_by_function ( subgraph , BIOPROCESS ) ) return { annotation_value : np . average ( scores [ bp ] [ 0 ] for bp in bps ) for annotation_value , bps in subgraph_bp . items ( ) }
993	def _getFirstOnBit ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : if input < self . minval : if self . clipInput and not self . periodic : if self . verbosity > 0 : print "Clipped input %s=%.2f to minval %.2f" % ( self . name , input , self . minval ) input = self . minval else : raise Exception ( 'input (%s) less than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : if input >= self . maxval : raise Exception ( 'input (%s) greater than periodic range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) else : if input > self . maxval : if self . clipInput : if self . verbosity > 0 : print "Clipped input %s=%.2f to maxval %.2f" % ( self . name , input , self . maxval ) input = self . maxval else : raise Exception ( 'input (%s) greater than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding else : centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding minbin = centerbin - self . halfwidth return [ minbin ]
5907	def edit_txt ( filename , substitutions , newname = None ) : if newname is None : newname = filename _substitutions = [ { 'lRE' : re . compile ( str ( lRE ) ) , 'sRE' : re . compile ( str ( sRE ) ) , 'repl' : repl } for lRE , sRE , repl in substitutions if repl is not None ] with tempfile . TemporaryFile ( ) as target : with open ( filename , 'rb' ) as src : logger . info ( "editing txt = {0!r} ({1:d} substitutions)" . format ( filename , len ( substitutions ) ) ) for line in src : line = line . decode ( "utf-8" ) keep_line = True for subst in _substitutions : m = subst [ 'lRE' ] . match ( line ) if m : logger . debug ( 'match: ' + line . rstrip ( ) ) if subst [ 'repl' ] is False : keep_line = False else : line = subst [ 'sRE' ] . sub ( str ( subst [ 'repl' ] ) , line ) logger . debug ( 'replaced: ' + line . rstrip ( ) ) if keep_line : target . write ( line . encode ( 'utf-8' ) ) else : logger . debug ( "Deleting line %r" , line ) target . seek ( 0 ) with open ( newname , 'wb' ) as final : shutil . copyfileobj ( target , final ) logger . info ( "edited txt = {newname!r}" . format ( ** vars ( ) ) )
11474	def login ( email = None , password = None , api_key = None , application = 'Default' , url = None , verify_ssl_certificate = True ) : try : input_ = raw_input except NameError : input_ = input if url is None : url = input_ ( 'Server URL: ' ) url = url . rstrip ( '/' ) if session . communicator is None : session . communicator = Communicator ( url ) else : session . communicator . url = url session . communicator . verify_ssl_certificate = verify_ssl_certificate if email is None : email = input_ ( 'Email: ' ) session . email = email if api_key is None : if password is None : password = getpass . getpass ( ) session . api_key = session . communicator . get_default_api_key ( session . email , password ) session . application = 'Default' else : session . api_key = api_key session . application = application return renew_token ( )
10081	def publish ( self , pid = None , id_ = None ) : pid = pid or self . pid if not pid . is_registered ( ) : raise PIDInvalidAction ( ) self [ '_deposit' ] [ 'status' ] = 'published' if self [ '_deposit' ] . get ( 'pid' ) is None : self . _publish_new ( id_ = id_ ) else : record = self . _publish_edited ( ) record . commit ( ) self . commit ( ) return self
1102	def restore ( delta , which ) : r try : tag = { 1 : "- " , 2 : "+ " } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( " " , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]
38	def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , "only %i out of %i MPI workers have sent '%s'" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result
10247	def enrich_pubmed_citations ( graph : BELGraph , manager : Manager ) -> Set [ str ] : pmids = get_pubmed_identifiers ( graph ) pmid_data , errors = get_citations_by_pmids ( manager = manager , pmids = pmids ) for u , v , k in filter_edges ( graph , has_pubmed ) : pmid = graph [ u ] [ v ] [ k ] [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) if pmid not in pmid_data : log . warning ( 'Missing data for PubMed identifier: %s' , pmid ) errors . add ( pmid ) continue graph [ u ] [ v ] [ k ] [ CITATION ] . update ( pmid_data [ pmid ] ) return errors
2153	def config_from_environment ( ) : kwargs = { } for k in CONFIG_OPTIONS : env = 'TOWER_' + k . upper ( ) v = os . getenv ( env , None ) if v is not None : kwargs [ k ] = v return kwargs
12426	def _expand_targets ( self , targets , base_dir = None ) : all_targets = [ ] for target in targets : target_dirs = [ p for p in [ base_dir , os . path . dirname ( target ) ] if p ] target_dir = target_dirs and os . path . join ( * target_dirs ) or '' target = os . path . basename ( target ) target_path = os . path . join ( target_dir , target ) if os . path . exists ( target_path ) : all_targets . append ( target_path ) with open ( target_path ) as fp : for line in fp : if line . startswith ( '-r ' ) : _ , new_target = line . split ( ' ' , 1 ) all_targets . extend ( self . _expand_targets ( [ new_target . strip ( ) ] , base_dir = target_dir ) ) return all_targets
1179	def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist
11389	def can_run_from_cli ( self ) : ret = False ast_tree = ast . parse ( self . body , self . path ) calls = self . _find_calls ( ast_tree , __name__ , "exit" ) for call in calls : if re . search ( "{}\(" . format ( re . escape ( call ) ) , self . body ) : ret = True break return ret
2723	def take_snapshot ( self , snapshot_name , return_dict = True , power_off = False ) : if power_off is True and self . status != "off" : action = self . power_off ( return_dict = False ) action . wait ( ) self . load ( ) return self . _perform_action ( { "type" : "snapshot" , "name" : snapshot_name } , return_dict )
6417	def stem ( self , word ) : word = normalize ( 'NFKD' , text_type ( word . lower ( ) ) ) word = '' . join ( c for c in word if c in { 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' , } ) word = word . replace ( 'j' , 'i' ) . replace ( 'v' , 'u' ) if word [ - 3 : ] == 'que' : if word [ : - 3 ] in self . _keep_que or word == 'que' : return { 'n' : word , 'v' : word } else : word = word [ : - 3 ] noun = word verb = word for endlen in range ( 4 , 0 , - 1 ) : if word [ - endlen : ] in self . _n_endings [ endlen ] : if len ( word ) - 2 >= endlen : noun = word [ : - endlen ] else : noun = word break for endlen in range ( 6 , 0 , - 1 ) : if word [ - endlen : ] in self . _v_endings_strip [ endlen ] : if len ( word ) - 2 >= endlen : verb = word [ : - endlen ] else : verb = word break if word [ - endlen : ] in self . _v_endings_alter [ endlen ] : if word [ - endlen : ] in { 'iuntur' , 'erunt' , 'untur' , 'iunt' , 'unt' , } : new_word = word [ : - endlen ] + 'i' addlen = 1 elif word [ - endlen : ] in { 'beris' , 'bor' , 'bo' } : new_word = word [ : - endlen ] + 'bi' addlen = 2 else : new_word = word [ : - endlen ] + 'eri' addlen = 3 if len ( new_word ) >= 2 + addlen : verb = new_word else : verb = word break return { 'n' : noun , 'v' : verb }
7386	def find_node_group_membership ( self , node ) : for group , nodelist in self . nodes . items ( ) : if node in nodelist : return group
4566	def euclidean ( c1 , c2 ) : diffs = ( ( i - j ) for i , j in zip ( c1 , c2 ) ) return sum ( x * x for x in diffs )
10718	def normalize_unitnumber ( unit_number ) : try : try : unit_number = int ( unit_number ) except ValueError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) except TypeError : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) if not ( 1 <= unit_number <= 16 ) : raise X10InvalidUnitNumber ( '%r not a valid unit number' % unit_number ) return unit_number
8747	def create_scalingip ( context , content ) : LOG . info ( 'create_scalingip for tenant %s and body %s' , context . tenant_id , content ) network_id = content . get ( 'scaling_network_id' ) ip_address = content . get ( 'scaling_ip_address' ) requested_ports = content . get ( 'ports' , [ ] ) network = _get_network ( context , network_id ) port_fixed_ips = { } for req_port in requested_ports : port = _get_port ( context , req_port [ 'port_id' ] ) fixed_ip = _get_fixed_ip ( context , req_port . get ( 'fixed_ip_address' ) , port ) port_fixed_ips [ port . id ] = { "port" : port , "fixed_ip" : fixed_ip } scip = _allocate_ip ( context , network , None , ip_address , ip_types . SCALING ) _create_flip ( context , scip , port_fixed_ips ) return v . _make_scaling_ip_dict ( scip )
2860	def _idle ( self ) : self . _ft232h . setup_pins ( { 0 : GPIO . OUT , 1 : GPIO . OUT , 2 : GPIO . IN } , { 0 : GPIO . HIGH , 1 : GPIO . HIGH } )
13877	def CopyDirectory ( source_dir , target_dir , override = False ) : _AssertIsLocal ( source_dir ) _AssertIsLocal ( target_dir ) if override and IsDir ( target_dir ) : DeleteDirectory ( target_dir , skip_on_error = False ) import shutil shutil . copytree ( source_dir , target_dir )
10640	def Gr ( L : float , Ts : float , Tf : float , beta : float , nu : float , g : float ) : return g * beta * ( Ts - Tf ) * L ** 3.0 / nu ** 2.0
6995	def runcp_producer_loop_savedstate ( use_saved_state = None , lightcurve_list = None , input_queue = None , input_bucket = None , result_queue = None , result_bucket = None , pfresult_list = None , runcp_kwargs = None , process_list_slice = None , download_when_done = True , purge_queues_when_done = True , save_state_when_done = True , delete_queues_when_done = False , s3_client = None , sqs_client = None ) : if use_saved_state is not None and os . path . exists ( use_saved_state ) : with open ( use_saved_state , 'rb' ) as infd : saved_state = pickle . load ( infd ) return runcp_producer_loop ( saved_state [ 'in_progress' ] , saved_state [ 'args' ] [ 1 ] , saved_state [ 'args' ] [ 2 ] , saved_state [ 'args' ] [ 3 ] , saved_state [ 'args' ] [ 4 ] , ** saved_state [ 'kwargs' ] ) else : return runcp_producer_loop ( lightcurve_list , input_queue , input_bucket , result_queue , result_bucket , pfresult_list = pfresult_list , runcp_kwargs = runcp_kwargs , process_list_slice = process_list_slice , download_when_done = download_when_done , purge_queues_when_done = purge_queues_when_done , save_state_when_done = save_state_when_done , delete_queues_when_done = delete_queues_when_done , s3_client = s3_client , sqs_client = sqs_client )
3614	def _get_available_choices ( self , queryset , value ) : item = queryset . filter ( pk = value ) . first ( ) if item : try : pk = getattr ( item , self . chained_model_field + "_id" ) filter = { self . chained_model_field : pk } except AttributeError : try : pks = getattr ( item , self . chained_model_field ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : try : pks = getattr ( item , self . chained_model_field + "_set" ) . all ( ) . values_list ( 'pk' , flat = True ) filter = { self . chained_model_field + "__in" : pks } except AttributeError : filter = { } filtered = list ( get_model ( self . to_app_name , self . to_model_name ) . objects . filter ( ** filter ) . distinct ( ) ) if self . sort : sort_results ( filtered ) else : filtered = [ ] return filtered
4953	def ready ( self ) : from enterprise . signals import handle_user_post_save from django . db . models . signals import pre_migrate , post_save post_save . connect ( handle_user_post_save , sender = self . auth_user_model , dispatch_uid = USER_POST_SAVE_DISPATCH_UID ) pre_migrate . connect ( self . _disconnect_user_post_save_for_migrations )
1479	def _wait_process_std_out_err ( self , name , process ) : proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) process . wait ( )
13645	def parse ( parser , argv = None , settings_key = 'settings' , no_args_func = None ) : argv = argv or sys . argv commands = command_list ( ) if type ( argv ) not in [ list , tuple ] : raise TypeError ( "argv only can be list or tuple" ) if len ( argv ) >= 2 and argv [ 1 ] in commands : sub_parsers = parser . add_subparsers ( ) class_name = argv [ 1 ] . capitalize ( ) + 'Component' from cliez . conf import ( COMPONENT_ROOT , LOGGING_CONFIG , EPILOG , GENERAL_ARGUMENTS ) sys . path . insert ( 0 , os . path . dirname ( COMPONENT_ROOT ) ) mod = importlib . import_module ( '{}.components.{}' . format ( os . path . basename ( COMPONENT_ROOT ) , argv [ 1 ] ) ) klass = getattr ( mod , class_name ) sub_parser = append_arguments ( klass , sub_parsers , EPILOG , GENERAL_ARGUMENTS ) options = parser . parse_args ( argv [ 1 : ] ) settings = Settings . bind ( getattr ( options , settings_key ) ) if settings_key and hasattr ( options , settings_key ) else None obj = klass ( parser , sub_parser , options , settings ) logger_level = logging . CRITICAL if hasattr ( options , 'verbose' ) : if options . verbose == 1 : logger_level = logging . ERROR elif options . verbose == 2 : logger_level = logging . WARNING elif options . verbose == 3 : logger_level = logging . INFO obj . logger . setLevel ( logging . INFO ) pass if hasattr ( options , 'debug' ) and options . debug : logger_level = logging . DEBUG try : import http . client as http_client http_client . HTTPConnection . debuglevel = 1 except Exception : pass pass loggers = LOGGING_CONFIG [ 'loggers' ] for k , v in loggers . items ( ) : v . setdefault ( 'level' , logger_level ) if logger_level in [ logging . INFO , logging . DEBUG ] : v [ 'handlers' ] = [ 'stdout' ] pass logging_config . dictConfig ( LOGGING_CONFIG ) obj . run ( options ) return obj if not parser . description and len ( commands ) : sub_parsers = parser . add_subparsers ( ) [ sub_parsers . add_parser ( v ) for v in commands ] pass pass options = parser . parse_args ( argv [ 1 : ] ) if no_args_func and callable ( no_args_func ) : return no_args_func ( options ) else : parser . _print_message ( "nothing to do...\n" ) pass
8157	def close ( self ) : self . _con . commit ( ) self . _cur . close ( ) self . _con . close ( )
204	def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )
10944	def reset ( self , new_region_size = None , do_calc_size = True , new_damping = None , new_max_mem = None ) : if new_region_size is not None : self . region_size = new_region_size if new_max_mem != None : self . max_mem = new_max_mem if do_calc_size : self . region_size = calc_particle_group_region_size ( self . state , region_size = self . region_size , max_mem = self . max_mem ) self . stats = [ ] self . particle_groups = separate_particles_into_groups ( self . state , self . region_size , doshift = 'rand' ) if new_damping is not None : self . _kwargs . update ( { 'damping' : new_damping } ) if self . save_J : if len ( self . particle_groups ) > 90 : CLOG . warn ( 'Attempting to create many open files. Consider increasing max_mem and/or region_size to avoid crashes.' ) self . _tempfiles = [ ] self . _has_saved_J = [ ] for a in range ( len ( self . particle_groups ) ) : for _ in [ 'j' , 'tile' ] : self . _tempfiles . append ( tempfile . TemporaryFile ( dir = os . getcwd ( ) ) ) self . _has_saved_J . append ( False )
12742	def get_ISBNs ( self ) : invalid_isbns = set ( self . get_invalid_ISBNs ( ) ) valid_isbns = [ self . _clean_isbn ( isbn ) for isbn in self [ "020a" ] if self . _clean_isbn ( isbn ) not in invalid_isbns ] if valid_isbns : return valid_isbns return [ self . _clean_isbn ( isbn ) for isbn in self [ "901i" ] ]
2528	def get_annotation_date ( self , r_term ) : annotation_date_list = list ( self . graph . triples ( ( r_term , self . spdx_namespace [ 'annotationDate' ] , None ) ) ) if len ( annotation_date_list ) != 1 : self . error = True msg = 'Annotation must have exactly one annotation date.' self . logger . log ( msg ) return return six . text_type ( annotation_date_list [ 0 ] [ 2 ] )
8966	def which ( command , path = None , verbose = 0 , exts = None ) : matched = whichgen ( command , path , verbose , exts ) try : match = next ( matched ) except StopIteration : raise WhichError ( "Could not find '%s' on the path." % command ) else : return match
1550	def _get_spout ( self ) : spout = topology_pb2 . Spout ( ) spout . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_out_streams ( spout ) return spout
4281	def check_subprocess ( cmd , source , outname ) : logger = logging . getLogger ( __name__ ) try : res = subprocess . run ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) except KeyboardInterrupt : logger . debug ( 'Process terminated, removing file %s' , outname ) if os . path . isfile ( outname ) : os . remove ( outname ) raise if res . returncode : logger . debug ( 'STDOUT:\n %s' , res . stdout . decode ( 'utf8' ) ) logger . debug ( 'STDERR:\n %s' , res . stderr . decode ( 'utf8' ) ) if os . path . isfile ( outname ) : logger . debug ( 'Removing file %s' , outname ) os . remove ( outname ) raise SubprocessException ( 'Failed to process ' + source )
13549	def pip_install ( * args ) : download_cache = ( '--download-cache=%s ' % options . paved . pip . download_cache ) if options . paved . pip . download_cache else '' shv ( 'pip install %s%s' % ( download_cache , ' ' . join ( args ) ) )
3690	def solve_T ( self , P , V , quick = True ) : r a , b = self . a , self . b if quick : x1 = - 1.j * 1.7320508075688772 + 1. x2 = V - b x3 = x2 / R x4 = V + b x5 = ( 1.7320508075688772 * ( x2 * x2 * ( - 4. * P * P * P * x3 + 27. * a * a / ( V * V * x4 * x4 ) ) / ( R * R ) ) ** 0.5 - 9. * a * x3 / ( V * x4 ) + 0j ) ** ( 1. / 3. ) return ( 3.3019272488946263 * ( 11.537996562459266 * P * x3 / ( x1 * x5 ) + 1.2599210498948732 * x1 * x5 ) ** 2 / 144.0 ) . real else : return ( ( - ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) / 3 + ( - P * V + P * b ) / ( R * ( - 1 / 2 + sqrt ( 3 ) * 1j / 2 ) * ( sqrt ( 729 * ( - V * a + a * b ) ** 2 / ( R * V ** 2 + R * V * b ) ** 2 + 108 * ( - P * V + P * b ) ** 3 / R ** 3 ) / 2 + 27 * ( - V * a + a * b ) / ( 2 * ( R * V ** 2 + R * V * b ) ) + 0j ) ** ( 1 / 3 ) ) ) ** 2 ) . real
1158	def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( "cannot release un-acquired lock" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( "%s.release(): final release" , self ) else : if __debug__ : self . _note ( "%s.release(): non-final release" , self )
2166	def list_commands ( self , ctx ) : commands = set ( self . list_resource_commands ( ) ) commands . union ( set ( self . list_misc_commands ( ) ) ) return sorted ( commands )
3982	def get_repo_of_app_or_library ( app_or_library_name ) : specs = get_specs ( ) repo_name = specs . get_app_or_lib ( app_or_library_name ) [ 'repo' ] if not repo_name : return None return Repo ( repo_name )
8589	def stop_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/stop' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
4300	def _install_aldryn ( config_data ) : import requests media_project = os . path . join ( config_data . project_directory , 'dist' , 'media' ) static_main = False static_project = os . path . join ( config_data . project_directory , 'dist' , 'static' ) template_target = os . path . join ( config_data . project_directory , 'templates' ) tmpdir = tempfile . mkdtemp ( ) aldrynzip = requests . get ( data . ALDRYN_BOILERPLATE ) zip_open = zipfile . ZipFile ( BytesIO ( aldrynzip . content ) ) zip_open . extractall ( path = tmpdir ) for component in os . listdir ( os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' ) ) : src = os . path . join ( tmpdir , 'aldryn-boilerplate-standard-master' , component ) dst = os . path . join ( config_data . project_directory , component ) if os . path . isfile ( src ) : shutil . copy ( src , dst ) else : shutil . copytree ( src , dst ) shutil . rmtree ( tmpdir ) return media_project , static_main , static_project , template_target
3129	def get_subscriber_hash ( member_email ) : check_email ( member_email ) member_email = member_email . lower ( ) . encode ( ) m = hashlib . md5 ( member_email ) return m . hexdigest ( )
2314	def anm_score ( self , x , y ) : gp = GaussianProcessRegressor ( ) . fit ( x , y ) y_predict = gp . predict ( x ) indepscore = normalized_hsic ( y_predict - y , x ) return indepscore
10551	def update_result ( result ) : try : result_id = result . id result = _forbidden_attributes ( result ) res = _pybossa_req ( 'put' , 'result' , result_id , payload = result . data ) if res . get ( 'id' ) : return Result ( res ) else : return res except : raise
7931	def send_message ( source_jid , password , target_jid , body , subject = None , message_type = "chat" , message_thread = None , settings = None ) : if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) if isinstance ( source_jid , str ) : source_jid = source_jid . decode ( encoding ) if isinstance ( password , str ) : password = password . decode ( encoding ) if isinstance ( target_jid , str ) : target_jid = target_jid . decode ( encoding ) if isinstance ( body , str ) : body = body . decode ( encoding ) if isinstance ( message_type , str ) : message_type = message_type . decode ( encoding ) if isinstance ( message_thread , str ) : message_thread = message_thread . decode ( encoding ) if not isinstance ( source_jid , JID ) : source_jid = JID ( source_jid ) if not isinstance ( target_jid , JID ) : target_jid = JID ( target_jid ) msg = Message ( to_jid = target_jid , body = body , subject = subject , stanza_type = message_type ) def action ( client ) : client . stream . send ( msg ) if settings is None : settings = XMPPSettings ( { "starttls" : True , "tls_verify_peer" : False } ) if password is not None : settings [ "password" ] = password handler = FireAndForget ( source_jid , action , settings ) try : handler . run ( ) except KeyboardInterrupt : handler . disconnect ( ) raise
6976	def keplermag_to_sdssr ( keplermag , kic_sdssg , kic_sdssr ) : kic_sdssgr = kic_sdssg - kic_sdssr if kic_sdssgr < 0.8 : kepsdssr = ( keplermag - 0.2 * kic_sdssg ) / 0.8 else : kepsdssr = ( keplermag - 0.1 * kic_sdssg ) / 0.9 return kepsdssr
6313	def print ( self ) : print ( "---[ START {} ]---" . format ( self . name ) ) for i , line in enumerate ( self . lines ) : print ( "{}: {}" . format ( str ( i ) . zfill ( 3 ) , line ) ) print ( "---[ END {} ]---" . format ( self . name ) )
4963	def clean_course ( self ) : course_id = self . cleaned_data [ self . Fields . COURSE ] . strip ( ) if not course_id : return None try : client = EnrollmentApiClient ( ) return client . get_course_details ( course_id ) except ( HttpClientError , HttpServerError ) : raise ValidationError ( ValidationMessages . INVALID_COURSE_ID . format ( course_id = course_id ) )
12078	def save ( self , callit = "misc" , closeToo = True , fullpath = False ) : if fullpath is False : fname = self . abf . outPre + "plot_" + callit + ".jpg" else : fname = callit if not os . path . exists ( os . path . dirname ( fname ) ) : os . mkdir ( os . path . dirname ( fname ) ) plt . savefig ( fname ) self . log . info ( "saved [%s]" , os . path . basename ( fname ) ) if closeToo : plt . close ( )
13506	def get_position ( self , position_id ) : url = "/2/positions/%s" % position_id return self . position_from_json ( self . _get_resource ( url ) [ "position" ] )
12761	def process_data ( self ) : self . visibility = self . data [ : , : , 3 ] self . positions = self . data [ : , : , : 3 ] self . velocities = np . zeros_like ( self . positions ) + 1000 for frame_no in range ( 1 , len ( self . data ) - 1 ) : prev = self . data [ frame_no - 1 ] next = self . data [ frame_no + 1 ] for c in range ( self . num_markers ) : if - 1 < prev [ c , 3 ] < 100 and - 1 < next [ c , 3 ] < 100 : self . velocities [ frame_no , c ] = ( next [ c , : 3 ] - prev [ c , : 3 ] ) / ( 2 * self . world . dt ) self . cfms = np . zeros_like ( self . visibility ) + self . DEFAULT_CFM
2592	def get_all_checkpoints ( rundir = "runinfo" ) : if ( not os . path . isdir ( rundir ) ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) checkpoints = [ ] for runid in dirs : checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , runid ) ) if os . path . isdir ( checkpoint ) : checkpoints . append ( checkpoint ) return checkpoints
2822	def convert_relu ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting relu ...' ) if names == 'short' : tf_name = 'RELU' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) relu = keras . layers . Activation ( 'relu' , name = tf_name ) layers [ scope_name ] = relu ( layers [ inputs [ 0 ] ] )
9654	def take_shas_of_all_files ( G , settings ) : global ERROR_FN sprint = settings [ "sprint" ] error = settings [ "error" ] ERROR_FN = error sha_dict = { } all_files = [ ] for target in G . nodes ( data = True ) : sprint ( "About to take shas of files in target '{}'" . format ( target [ 0 ] ) , level = "verbose" ) if 'dependencies' in target [ 1 ] : sprint ( "It has dependencies" , level = "verbose" ) deplist = [ ] for dep in target [ 1 ] [ 'dependencies' ] : glist = glob . glob ( dep ) if glist : for oneglob in glist : deplist . append ( oneglob ) else : deplist . append ( dep ) target [ 1 ] [ 'dependencies' ] = list ( deplist ) for dep in target [ 1 ] [ 'dependencies' ] : sprint ( " - {}" . format ( dep ) , level = "verbose" ) all_files . append ( dep ) if 'output' in target [ 1 ] : sprint ( "It has outputs" , level = "verbose" ) for out in acts . get_all_outputs ( target [ 1 ] ) : sprint ( " - {}" . format ( out ) , level = "verbose" ) all_files . append ( out ) if len ( all_files ) : sha_dict [ 'files' ] = { } extant_files = [ ] for item in all_files : if item not in extant_files and os . path . isfile ( item ) : extant_files . append ( item ) pool = Pool ( ) results = pool . map ( get_sha , extant_files ) pool . close ( ) pool . join ( ) for fn , sha in zip ( extant_files , results ) : sha_dict [ 'files' ] [ fn ] = { 'sha' : sha } return sha_dict sprint ( "No dependencies" , level = "verbose" )
11119	def get_file_info ( self , relativePath , name = None ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' can't be a file name." if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) errorMessage = "" dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) if dirInfoDict is None : return None , errorMessage fileInfo = dict . __getitem__ ( dirInfoDict , "files" ) . get ( name , None ) if fileInfo is None : errorMessage = "file %s does not exist in relative path '%s'" % ( name , relativePath ) return fileInfo , errorMessage
8615	def wait_for_completion ( self , response , timeout = 3600 , initial_wait = 5 , scaleup = 10 ) : if not response : return logger = logging . getLogger ( __name__ ) wait_period = initial_wait next_increase = time . time ( ) + wait_period * scaleup if timeout : timeout = time . time ( ) + timeout while True : request = self . get_request ( request_id = response [ 'requestId' ] , status = True ) if request [ 'metadata' ] [ 'status' ] == 'DONE' : break elif request [ 'metadata' ] [ 'status' ] == 'FAILED' : raise PBFailedRequest ( 'Request {0} failed to complete: {1}' . format ( response [ 'requestId' ] , request [ 'metadata' ] [ 'message' ] ) , response [ 'requestId' ] ) current_time = time . time ( ) if timeout and current_time > timeout : raise PBTimeoutError ( 'Timed out waiting for request {0}.' . format ( response [ 'requestId' ] ) , response [ 'requestId' ] ) if current_time > next_increase : wait_period *= 2 next_increase = time . time ( ) + wait_period * scaleup scaleup *= 2 logger . info ( "Request %s is in state '%s'. Sleeping for %i seconds..." , response [ 'requestId' ] , request [ 'metadata' ] [ 'status' ] , wait_period ) time . sleep ( wait_period )
12966	def allOnlyIndexedFields ( self ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyIndexedFields ( matchedKeys ) return IRQueryableList ( [ ] , mdl = self . mdl )
5416	def get_dstat_provider_args ( provider , project ) : provider_name = get_provider_name ( provider ) args = [ ] if provider_name == 'google' : args . append ( '--project %s' % project ) elif provider_name == 'google-v2' : args . append ( '--project %s' % project ) elif provider_name == 'local' : pass elif provider_name == 'test-fails' : pass else : assert False , 'Provider %s needs get_dstat_provider_args support' % provider args . insert ( 0 , '--provider %s' % provider_name ) return ' ' . join ( args )
849	def getOutputElementCount ( self , name ) : if name == "resetOut" : print ( "WARNING: getOutputElementCount should not have been called with " "resetOut" ) return 1 elif name == "sequenceIdOut" : print ( "WARNING: getOutputElementCount should not have been called with " "sequenceIdOut" ) return 1 elif name == "dataOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'dataOut' " "on a RecordSensor node, but the encoder has not " "been set" ) return self . encoder . getWidth ( ) elif name == "sourceOut" : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) elif name == "bucketIdxOut" : return 1 elif name == "actValueOut" : return 1 elif name == "categoryOut" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( "NuPIC requested output element count for 'sourceOut' " "on a RecordSensor node, " "but the encoder has not been set" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( "Unknown output %s" % name )
12435	def parse ( cls , path ) : for resource , pattern in cls . meta . patterns : match = re . match ( pattern , path ) if match is not None : return resource , match . groupdict ( ) , match . string [ match . end ( ) : ] return None if not cls . meta . patterns else False
3083	def _parse_state_value ( state , user ) : uri , token = state . rsplit ( ':' , 1 ) if xsrfutil . validate_token ( xsrf_secret_key ( ) , token , user . user_id ( ) , action_id = uri ) : return uri else : return None
4303	def sox ( args ) : if args [ 0 ] . lower ( ) != "sox" : args . insert ( 0 , "sox" ) else : args [ 0 ] = "sox" try : logger . info ( "Executing: %s" , ' ' . join ( args ) ) process_handle = subprocess . Popen ( args , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) out , err = process_handle . communicate ( ) out = out . decode ( "utf-8" ) err = err . decode ( "utf-8" ) status = process_handle . returncode return status , out , err except OSError as error_msg : logger . error ( "OSError: SoX failed! %s" , error_msg ) except TypeError as error_msg : logger . error ( "TypeError: %s" , error_msg ) return 1 , None , None
13663	def get_item ( filename , uuid ) : with open ( os . fsencode ( str ( filename ) ) , "r" ) as f : data = json . load ( f ) results = [ i for i in data if i [ "uuid" ] == str ( uuid ) ] if results : return results return None
10380	def calculate_concordance_probability_by_annotation ( graph , annotation , key , cutoff = None , permutations = None , percentage = None , use_ambiguous = False ) : result = [ ( value , calculate_concordance_probability ( subgraph , key , cutoff = cutoff , permutations = permutations , percentage = percentage , use_ambiguous = use_ambiguous , ) ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) ] return dict ( result )
10999	def psf_slice ( self , zint , size = 11 , zoffset = 0. , getextent = False ) : zint = max ( self . _p2k ( self . _tz ( zint ) ) , 0 ) offset = np . array ( [ zoffset * ( zint > 0 ) , 0 , 0 ] ) scale = [ self . param_dict [ self . zscale ] , 1.0 , 1.0 ] tile = util . Tile ( left = 0 , size = size , centered = True ) vecs = tile . coords ( form = 'flat' ) vecs = [ self . _p2k ( s * i + o ) for i , s , o in zip ( vecs , scale , offset ) ] psf = self . psffunc ( * vecs [ : : - 1 ] , zint = zint , ** self . pack_args ( ) ) . T vec = tile . coords ( form = 'meshed' ) if self . cutoffval is not None and not self . cutbyval : edge = psf > psf . max ( ) * self . cutoffval dd = nd . morphology . distance_transform_edt ( ~ edge ) psf = psf * np . exp ( - dd ** 4 ) psf /= psf . sum ( ) if getextent : size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec if self . cutoffval is not None and self . cutbyval : cutval = self . cutoffval * psf . max ( ) dd = ( psf - cutval ) / cutval dd [ dd > 0 ] = 0. psf = psf * np . exp ( - ( dd / self . cutfallrate ) ** 4 ) psf /= psf . sum ( ) edge = psf > cutval * self . cutedgeval if getextent : size = np . array ( [ ( vec * edge ) . min ( axis = ( 1 , 2 , 3 ) ) - 2 , ( vec * edge ) . max ( axis = ( 1 , 2 , 3 ) ) + 2 , ] ) . T return psf , vec , size return psf , vec return psf , vec
9937	def list ( self , ignore_patterns ) : for storage in six . itervalues ( self . storages ) : if storage . exists ( '' ) : for path in utils . get_files ( storage , ignore_patterns ) : yield path , storage
9061	def beta_covariance ( self ) : from numpy_sugar . linalg import ddot tX = self . _X [ "tX" ] Q = concatenate ( self . _QS [ 0 ] , axis = 1 ) S0 = self . _QS [ 1 ] D = self . v0 * S0 + self . v1 D = D . tolist ( ) + [ self . v1 ] * ( len ( self . _y ) - len ( D ) ) D = asarray ( D ) A = inv ( tX . T @ ( Q @ ddot ( 1 / D , Q . T @ tX ) ) ) VT = self . _X [ "VT" ] H = lstsq ( VT , A , rcond = None ) [ 0 ] return lstsq ( VT , H . T , rcond = None ) [ 0 ]
12679	def can_send ( self , user , notice_type ) : from notification . models import NoticeSetting return NoticeSetting . for_user ( user , notice_type , self . medium_id ) . send
4089	def with_logger ( cls ) : attr_name = '_logger' cls_name = cls . __qualname__ module = cls . __module__ if module is not None : cls_name = module + '.' + cls_name else : raise AssertionError setattr ( cls , attr_name , logging . getLogger ( cls_name ) ) return cls
4837	def get_catalog_courses ( self , catalog_id ) : return self . _load_data ( self . CATALOGS_COURSES_ENDPOINT . format ( catalog_id ) , default = [ ] )
2611	def serialize_object ( obj , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : buffers = [ ] if istype ( obj , sequence_types ) and len ( obj ) < item_threshold : cobj = can_sequence ( obj ) for c in cobj : buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) elif istype ( obj , dict ) and len ( obj ) < item_threshold : cobj = { } for k in sorted ( obj ) : c = can ( obj [ k ] ) buffers . extend ( _extract_buffers ( c , buffer_threshold ) ) cobj [ k ] = c else : cobj = can ( obj ) buffers . extend ( _extract_buffers ( cobj , buffer_threshold ) ) buffers . insert ( 0 , pickle . dumps ( cobj , PICKLE_PROTOCOL ) ) return buffers
3005	def _get_storage_model ( ) : storage_model_settings = getattr ( django . conf . settings , 'GOOGLE_OAUTH2_STORAGE_MODEL' , None ) if storage_model_settings is not None : return ( storage_model_settings [ 'model' ] , storage_model_settings [ 'user_property' ] , storage_model_settings [ 'credentials_property' ] ) else : return None , None , None
5800	def system_path ( ) : ca_path = None paths = [ '/usr/lib/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-certificates.crt' , '/etc/ssl/certs/ca-bundle.crt' , '/etc/pki/tls/certs/ca-bundle.crt' , '/etc/ssl/ca-bundle.pem' , '/usr/local/share/certs/ca-root-nss.crt' , '/etc/ssl/cert.pem' ] if 'SSL_CERT_FILE' in os . environ : paths . insert ( 0 , os . environ [ 'SSL_CERT_FILE' ] ) for path in paths : if os . path . exists ( path ) and os . path . getsize ( path ) > 0 : ca_path = path break if not ca_path : raise OSError ( pretty_message ( ) ) return ca_path
10586	def _create_account_ ( self , name , number , account_type ) : new_acc = GeneralLedgerAccount ( name , None , number , account_type ) self . accounts . append ( new_acc ) return new_acc
8370	def create_bot ( src = None , grammar = NODEBOX , format = None , outputfile = None , iterations = 1 , buff = None , window = False , title = None , fullscreen = None , server = False , port = 7777 , show_vars = False , vars = None , namespace = None ) : canvas = create_canvas ( src , format , outputfile , iterations > 1 , buff , window , title , fullscreen = fullscreen , show_vars = show_vars ) if grammar == DRAWBOT : from shoebot . grammar import DrawBot bot = DrawBot ( canvas , namespace = namespace , vars = vars ) else : from shoebot . grammar import NodeBot bot = NodeBot ( canvas , namespace = namespace , vars = vars ) if server : from shoebot . sbio import SocketServer socket_server = SocketServer ( bot , "" , port = port ) return bot
4893	def _collect_certificate_data ( self , enterprise_enrollment ) : if self . certificates_api is None : self . certificates_api = CertificatesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : certificate = self . certificates_api . get_course_certificate ( course_id , username ) completed_date = certificate . get ( 'created_date' ) if completed_date : completed_date = parse_datetime ( completed_date ) else : completed_date = timezone . now ( ) is_passing = certificate . get ( 'is_passing' ) grade = self . grade_passing if is_passing else self . grade_failing except HttpNotFoundError : completed_date = None grade = self . grade_incomplete is_passing = False return completed_date , grade , is_passing
1203	def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) A_conjugate = self . fn_x ( conjugate ) if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual
11629	def _loadNamelistIncludes ( item , unique_glyphs , cache ) : includes = item [ "includes" ] = [ ] charset = item [ "charset" ] = set ( ) | item [ "ownCharset" ] noCharcode = item [ "noCharcode" ] = set ( ) | item [ "ownNoCharcode" ] dirname = os . path . dirname ( item [ "fileName" ] ) for include in item [ "header" ] [ "includes" ] : includeFile = os . path . join ( dirname , include ) try : includedItem = readNamelist ( includeFile , unique_glyphs , cache ) except NamelistRecursionError : continue if includedItem in includes : continue includes . append ( includedItem ) charset |= includedItem [ "charset" ] noCharcode |= includedItem [ "ownNoCharcode" ] return item
2837	def transfer ( self , data , assert_ss = True , deassert_ss = True ) : if self . _mosi is None : raise RuntimeError ( 'Write attempted with no MOSI pin specified.' ) if self . _miso is None : raise RuntimeError ( 'Read attempted with no MISO pin specified.' ) if assert_ss and self . _ss is not None : self . _gpio . set_low ( self . _ss ) result = bytearray ( len ( data ) ) for i in range ( len ( data ) ) : for j in range ( 8 ) : if self . _write_shift ( data [ i ] , j ) & self . _mask : self . _gpio . set_high ( self . _mosi ) else : self . _gpio . set_low ( self . _mosi ) self . _gpio . output ( self . _sclk , not self . _clock_base ) if self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) self . _gpio . output ( self . _sclk , self . _clock_base ) if not self . _read_leading : if self . _gpio . is_high ( self . _miso ) : result [ i ] |= self . _read_shift ( self . _mask , j ) else : result [ i ] &= ~ self . _read_shift ( self . _mask , j ) if deassert_ss and self . _ss is not None : self . _gpio . set_high ( self . _ss ) return result
3024	def _in_gce_environment ( ) : if SETTINGS . env_name is not None : return SETTINGS . env_name == 'GCE_PRODUCTION' if NO_GCE_CHECK != 'True' and _detect_gce_environment ( ) : SETTINGS . env_name = 'GCE_PRODUCTION' return True return False
11885	def connect ( self ) : try : self . _socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _socket . settimeout ( TIMEOUT_SECONDS ) self . _socket . connect ( ( self . _ip , self . _port ) ) _LOGGER . debug ( "Successfully created Hub at %s:%s :)" , self . _ip , self . _port ) except socket . error as error : _LOGGER . error ( "Error creating Hub: %s :(" , error ) self . _socket . close ( )
13898	def IterHashes ( iterator_size , hash_length = 7 ) : if not isinstance ( iterator_size , int ) : raise TypeError ( 'iterator_size must be integer.' ) count = 0 while count != iterator_size : count += 1 yield GetRandomHash ( hash_length )
12735	def step ( self , substeps = 2 ) : self . frame_no += 1 dt = self . dt / substeps for _ in range ( substeps ) : self . ode_contactgroup . empty ( ) self . ode_space . collide ( None , self . on_collision ) self . ode_world . step ( dt )
7200	def create_leaflet_viewer ( self , idaho_image_results , filename ) : description = self . describe_images ( idaho_image_results ) if len ( description ) > 0 : functionstring = '' for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : num_images = len ( list ( part . keys ( ) ) ) partname = None if num_images == 1 : partname = [ p for p in list ( part . keys ( ) ) ] [ 0 ] pan_image_id = '' elif num_images == 2 : partname = [ p for p in list ( part . keys ( ) ) if p is not 'PAN' ] [ 0 ] pan_image_id = part [ 'PAN' ] [ 'id' ] if not partname : self . logger . debug ( "Cannot find part for idaho image." ) continue bandstr = { 'RGBN' : '0,1,2' , 'WORLDVIEW_8_BAND' : '4,2,1' , 'PAN' : '0' } . get ( partname , '0,1,2' ) part_boundstr_wkt = part [ partname ] [ 'boundstr' ] part_polygon = from_wkt ( part_boundstr_wkt ) bucketname = part [ partname ] [ 'bucket' ] image_id = part [ partname ] [ 'id' ] W , S , E , N = part_polygon . bounds functionstring += "addLayerToMap('%s','%s',%s,%s,%s,%s,'%s');\n" % ( bucketname , image_id , W , S , E , N , pan_image_id ) __location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) try : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) . decode ( "utf8" ) except AttributeError : with open ( os . path . join ( __location__ , 'leafletmap_template.html' ) , 'r' ) as htmlfile : data = htmlfile . read ( ) data = data . replace ( 'FUNCTIONSTRING' , functionstring ) data = data . replace ( 'CENTERLAT' , str ( S ) ) data = data . replace ( 'CENTERLON' , str ( W ) ) data = data . replace ( 'BANDS' , bandstr ) data = data . replace ( 'TOKEN' , self . gbdx_connection . access_token ) with codecs . open ( filename , 'w' , 'utf8' ) as outputfile : self . logger . debug ( "Saving %s" % filename ) outputfile . write ( data ) else : print ( 'No items returned.' )
13090	def main ( branch ) : try : output = subprocess . check_output ( [ 'git' , 'rev-parse' ] ) . decode ( 'utf-8' ) sys . stdout . write ( output ) except subprocess . CalledProcessError : return ensure_remote_branch_is_tracked ( branch ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , branch ] ) subprocess . check_call ( [ 'git' , 'pull' , '--quiet' ] ) subprocess . check_call ( [ 'git' , 'checkout' , '--quiet' , '%s~0' % branch ] ) subprocess . check_call ( [ 'find' , '.' , '-name' , '"*.pyc"' , '-delete' ] ) print ( 'Your branch is up to date with branch \'origin/%s\'.' % branch )
12336	def pip ( self , package_names , raise_on_error = True ) : if isinstance ( package_names , basestring ) : package_names = [ package_names ] cmd = "pip install -U %s" % ( ' ' . join ( package_names ) ) return self . wait ( cmd , raise_on_error = raise_on_error )
6153	def fir_remez_bsf ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fs = 1.0 , N_bump = 5 ) : n , ff , aa , wts = bandstop_order ( f_pass1 , f_stop1 , f_stop2 , f_pass2 , d_pass , d_stop , fsamp = fs ) if np . mod ( n , 2 ) != 0 : n += 1 N_taps = n N_taps += N_bump b = signal . remez ( N_taps , ff , aa [ 0 : : 2 ] , wts , Hz = 2 , maxiter = 25 , grid_density = 16 ) print ( 'N_bump must be odd to maintain odd filter length' ) print ( 'Remez filter taps = %d.' % N_taps ) return b
5053	def prefetch_users ( persistent_course_grades ) : users = User . objects . filter ( id__in = [ grade . user_id for grade in persistent_course_grades ] ) return { user . id : user for user in users }
10031	def execute ( helper , config , args ) : env = parse_env_config ( config , args . environment ) option_settings = env . get ( 'option_settings' , { } ) settings = parse_option_settings ( option_settings ) for setting in settings : out ( str ( setting ) )
10555	def find_helping_materials ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'helpingmaterial' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ HelpingMaterial ( helping ) for helping in res ] else : return res except : raise
8718	def file_remove ( self , path ) : log . info ( 'Remove ' + path ) cmd = 'file.remove("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
10528	def _pybossa_req ( method , domain , id = None , payload = None , params = { } , headers = { 'content-type' : 'application/json' } , files = None ) : url = _opts [ 'endpoint' ] + '/api/' + domain if id is not None : url += '/' + str ( id ) if 'api_key' in _opts : params [ 'api_key' ] = _opts [ 'api_key' ] if method == 'get' : r = requests . get ( url , params = params ) elif method == 'post' : if files is None and headers [ 'content-type' ] == 'application/json' : r = requests . post ( url , params = params , headers = headers , data = json . dumps ( payload ) ) else : r = requests . post ( url , params = params , files = files , data = payload ) elif method == 'put' : r = requests . put ( url , params = params , headers = headers , data = json . dumps ( payload ) ) elif method == 'delete' : r = requests . delete ( url , params = params , headers = headers , data = json . dumps ( payload ) ) if r . status_code // 100 == 2 : if r . text and r . text != '""' : return json . loads ( r . text ) else : return True else : return json . loads ( r . text )
7010	def skyview_stamp ( ra , decl , survey = 'DSS2 Red' , scaling = 'Linear' , flip = True , convolvewith = None , forcefetch = False , cachedir = '~/.astrobase/stamp-cache' , timeout = 10.0 , retry_failed = False , savewcsheader = True , verbose = False ) : stampdict = get_stamp ( ra , decl , survey = survey , scaling = scaling , forcefetch = forcefetch , cachedir = cachedir , timeout = timeout , retry_failed = retry_failed , verbose = verbose ) if stampdict : stampfits = pyfits . open ( stampdict [ 'fitsfile' ] ) header = stampfits [ 0 ] . header frame = stampfits [ 0 ] . data stampfits . close ( ) if flip : frame = np . flipud ( frame ) if verbose : LOGINFO ( 'fetched stamp successfully for (%.3f, %.3f)' % ( ra , decl ) ) if convolvewith : convolved = aconv . convolve ( frame , convolvewith ) if savewcsheader : return convolved , header else : return convolved else : if savewcsheader : return frame , header else : return frame else : LOGERROR ( 'could not fetch the requested stamp for ' 'coords: (%.3f, %.3f) from survey: %s and scaling: %s' % ( ra , decl , survey , scaling ) ) return None
13534	def can_remove ( self ) : if self . children . count ( ) == 0 : return True ancestors = set ( self . ancestors_root ( ) ) children = set ( self . children . all ( ) ) return children . issubset ( ancestors )
8308	def ensure_pycairo_context ( self , ctx ) : if self . cairocffi and isinstance ( ctx , self . cairocffi . Context ) : from shoebot . util . cairocffi . cairocffi_to_pycairo import _UNSAFE_cairocffi_context_to_pycairo return _UNSAFE_cairocffi_context_to_pycairo ( ctx ) else : return ctx
10834	def query_admins_by_group_ids ( cls , groups_ids = None ) : assert groups_ids is None or isinstance ( groups_ids , list ) query = db . session . query ( Group . id , func . count ( GroupAdmin . id ) ) . join ( GroupAdmin ) . group_by ( Group . id ) if groups_ids : query = query . filter ( Group . id . in_ ( groups_ids ) ) return query
2012	def instruction ( self ) : try : _decoding_cache = getattr ( self , '_decoding_cache' ) except Exception : _decoding_cache = self . _decoding_cache = { } pc = self . pc if isinstance ( pc , Constant ) : pc = pc . value if pc in _decoding_cache : return _decoding_cache [ pc ] def getcode ( ) : bytecode = self . bytecode for pc_i in range ( pc , len ( bytecode ) ) : yield simplify ( bytecode [ pc_i ] ) . value while True : yield 0 instruction = EVMAsm . disassemble_one ( getcode ( ) , pc = pc , fork = DEFAULT_FORK ) _decoding_cache [ pc ] = instruction return instruction
2857	def read ( self , length ) : if ( 1 > length > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x20 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) logger . debug ( 'SPI read with command {0:2X}.' . format ( command ) ) lengthR = length if length % 2 == 1 : lengthR += 1 lengthR = lengthR / 2 lenremain = length - lengthR len_low = ( lengthR - 1 ) & 0xFF len_high = ( ( lengthR - 1 ) >> 8 ) & 0xFF self . _assert_cs ( ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload1 = self . _ft232h . _poll_read ( lengthR ) self . _ft232h . _write ( str ( bytearray ( ( command , len_low , len_high ) ) ) ) payload2 = self . _ft232h . _poll_read ( lenremain ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
3475	def compartments ( self ) : if self . _compartments is None : self . _compartments = { met . compartment for met in self . _metabolites if met . compartment is not None } return self . _compartments
4292	def validate_project ( project_name ) : if '-' in project_name : return None if keyword . iskeyword ( project_name ) : return None if project_name in dir ( __builtins__ ) : return None try : __import__ ( project_name ) return None except ImportError : return project_name
2366	def walk ( self , * types ) : requested = types if len ( types ) > 0 else [ SuiteFile , ResourceFile , SuiteFolder , Testcase , Keyword ] for thing in self . robot_files : if thing . __class__ in requested : yield thing if isinstance ( thing , SuiteFolder ) : for child in thing . walk ( ) : if child . __class__ in requested : yield child else : for child in thing . walk ( * types ) : yield child
10276	def get_neurommsig_scores ( graph : BELGraph , genes : List [ Gene ] , annotation : str = 'Subgraph' , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None , preprocess : bool = False ) -> Optional [ Mapping [ str , float ] ] : if preprocess : graph = neurommsig_graph_preprocessor . run ( graph ) if not any ( gene in graph for gene in genes ) : logger . debug ( 'no genes mapping to graph' ) return subgraphs = get_subgraphs_by_annotation ( graph , annotation = annotation ) return get_neurommsig_scores_prestratified ( subgraphs = subgraphs , genes = genes , ora_weight = ora_weight , hub_weight = hub_weight , top_percent = top_percent , topology_weight = topology_weight , )
10454	def stateenabled ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name ) if object_handle . AXEnabled : return 1 except LdtpServerException : pass return 0
13678	def filenumber_handle ( self ) : self . __results = [ ] self . __dirs = [ ] self . __files = [ ] self . __ftp = self . connect ( ) self . __ftp . dir ( self . args . path , self . __results . append ) self . logger . debug ( "dir results: {}" . format ( self . __results ) ) self . quit ( ) status = self . ok for data in self . __results : if "<DIR>" in data : self . __dirs . append ( str ( data . split ( ) [ 3 ] ) ) else : self . __files . append ( str ( data . split ( ) [ 2 ] ) ) self . __result = len ( self . __files ) self . logger . debug ( "result: {}" . format ( self . __result ) ) if self . __result > self . args . warning : status = self . warning if self . __result > self . args . critical : status = self . critical self . shortoutput = "Found {0} files in {1}." . format ( self . __result , self . args . path ) [ self . longoutput . append ( line ) for line in self . __results if self . __results ] self . perfdata . append ( "{path}={result};{warn};{crit};0;" . format ( crit = self . args . critical , warn = self . args . warning , result = self . __result , path = self . args . path ) ) self . logger . debug ( "Return status and output." ) status ( self . output ( ) )
3748	def calculate ( self , T , method ) : r if method == GHARAGHEIZI : mu = Gharagheizi_gas_viscosity ( T , self . Tc , self . Pc , self . MW ) elif method == COOLPROP : mu = CoolProp_T_dependent_property ( T , self . CASRN , 'V' , 'g' ) elif method == DIPPR_PERRY_8E : mu = EQ102 ( T , * self . Perrys2_312_coeffs ) elif method == VDI_PPDS : mu = horner ( self . VDI_PPDS_coeffs , T ) elif method == YOON_THODOS : mu = Yoon_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == STIEL_THODOS : mu = Stiel_Thodos ( T , self . Tc , self . Pc , self . MW ) elif method == LUCAS_GAS : mu = lucas_gas ( T , self . Tc , self . Pc , self . Zc , self . MW , self . dipole , CASRN = self . CASRN ) elif method in self . tabular_data : mu = self . interpolate ( T , method ) return mu
13374	def binpath ( * paths ) : package_root = os . path . dirname ( __file__ ) return os . path . normpath ( os . path . join ( package_root , 'bin' , * paths ) )
308	def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax
5833	def create_ml_configuration_from_datasets ( self , dataset_ids ) : available_columns = self . search_template_client . get_available_columns ( dataset_ids ) search_template = self . search_template_client . create ( dataset_ids , available_columns ) return self . create_ml_configuration ( search_template , available_columns , dataset_ids )
8877	def allele_expectation ( bgen , variant_idx ) : r geno = bgen [ "genotype" ] [ variant_idx ] . compute ( ) if geno [ "phased" ] : raise ValueError ( "Allele expectation is define for unphased genotypes only." ) nalleles = bgen [ "variants" ] . loc [ variant_idx , "nalleles" ] . compute ( ) . item ( ) genotypes = get_genotypes ( geno [ "ploidy" ] , nalleles ) expec = [ ] for i in range ( len ( genotypes ) ) : count = asarray ( genotypes_to_allele_counts ( genotypes [ i ] ) , float ) n = count . shape [ 0 ] expec . append ( ( count . T * geno [ "probs" ] [ i , : n ] ) . sum ( 1 ) ) return stack ( expec , axis = 0 )
7455	def _cleanup_and_die ( data ) : tmpfiles = glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*_R*.fastq" ) ) tmpfiles += glob . glob ( os . path . join ( data . dirs . fastqs , "tmp_*.p" ) ) for tmpf in tmpfiles : os . remove ( tmpf )
3992	def get_nginx_configuration_spec ( port_spec_dict , docker_bridge_ip ) : nginx_http_config , nginx_stream_config = "" , "" for port_spec in port_spec_dict [ 'nginx' ] : if port_spec [ 'type' ] == 'http' : nginx_http_config += _nginx_http_spec ( port_spec , docker_bridge_ip ) elif port_spec [ 'type' ] == 'stream' : nginx_stream_config += _nginx_stream_spec ( port_spec , docker_bridge_ip ) return { 'http' : nginx_http_config , 'stream' : nginx_stream_config }
5701	def _feed_calendar_span ( gtfs , stats ) : n_feeds = _n_gtfs_sources ( gtfs ) [ 0 ] max_start = None min_end = None if n_feeds > 1 : for i in range ( n_feeds ) : feed_key = "feed_" + str ( i ) + "_" start_key = feed_key + "calendar_start" end_key = feed_key + "calendar_end" calendar_span = gtfs . conn . cursor ( ) . execute ( 'SELECT min(date), max(date) FROM trips, days ' 'WHERE trips.trip_I = days.trip_I AND trip_id LIKE ?;' , ( feed_key + '%' , ) ) . fetchone ( ) stats [ start_key ] = calendar_span [ 0 ] stats [ end_key ] = calendar_span [ 1 ] if calendar_span [ 0 ] is not None and calendar_span [ 1 ] is not None : if not max_start and not min_end : max_start = calendar_span [ 0 ] min_end = calendar_span [ 1 ] else : if gtfs . get_day_start_ut ( calendar_span [ 0 ] ) > gtfs . get_day_start_ut ( max_start ) : max_start = calendar_span [ 0 ] if gtfs . get_day_start_ut ( calendar_span [ 1 ] ) < gtfs . get_day_start_ut ( min_end ) : min_end = calendar_span [ 1 ] stats [ "latest_feed_start_date" ] = max_start stats [ "earliest_feed_end_date" ] = min_end else : stats [ "latest_feed_start_date" ] = stats [ "start_date" ] stats [ "earliest_feed_end_date" ] = stats [ "end_date" ] return stats
10136	def _detect_or_validate ( self , val ) : if isinstance ( val , list ) or isinstance ( val , dict ) or isinstance ( val , SortableDict ) or isinstance ( val , Grid ) : self . _assert_version ( VER_3_0 )
7293	def get_form ( self ) : self . set_fields ( ) if self . post_data_dict is not None : self . set_post_data ( ) return self . form
3458	def main ( argv ) : source , target , tag = argv if "a" in tag : bump = "alpha" if "b" in tag : bump = "beta" else : bump = find_bump ( target , tag ) filename = "{}.md" . format ( tag ) destination = copy ( join ( source , filename ) , target ) build_hugo_md ( destination , tag , bump )
6368	def precision_gain ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) random_precision = self . cond_pos_pop ( ) / self . population ( ) return self . precision ( ) / random_precision
8684	def load ( self , origin_passphrase , keys = None , key_file = None ) : self . _assert_valid_stash ( ) if not ( bool ( keys ) ^ bool ( key_file ) ) : raise GhostError ( 'You must either provide a path to an exported stash file ' 'or a list of key dicts to import' ) if key_file : with open ( key_file ) as stash_file : keys = json . loads ( stash_file . read ( ) ) decrypt = origin_passphrase != self . passphrase if decrypt : stub = Stash ( TinyDBStorage ( 'stub' ) , origin_passphrase ) for key in keys : self . put ( name = key [ 'name' ] , value = stub . _decrypt ( key [ 'value' ] ) if decrypt else key [ 'value' ] , metadata = key [ 'metadata' ] , description = key [ 'description' ] , lock = key . get ( 'lock' ) , key_type = key . get ( 'type' ) , encrypt = decrypt )
5886	def close ( self ) : if self . fetcher is not None : self . shutdown_network ( ) self . finalizer . atexit = False
12773	def _step_to_marker_frame ( self , frame_no , dt = None ) : self . markers . detach ( ) self . markers . reposition ( frame_no ) self . markers . attach ( frame_no ) self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) yield states self . ode_world . step ( dt or self . dt ) self . ode_contactgroup . empty ( )
2708	def limit_keyphrases ( path , phrase_limit = 20 ) : rank_thresh = None if isinstance ( path , str ) : lex = [ ] for meta in json_iter ( path ) : rl = RankedLexeme ( ** meta ) lex . append ( rl ) else : lex = path if len ( lex ) > 0 : rank_thresh = statistics . mean ( [ rl . rank for rl in lex ] ) else : rank_thresh = 0 used = 0 for rl in lex : if rl . pos [ 0 ] != "v" : if ( used > phrase_limit ) or ( rl . rank < rank_thresh ) : return used += 1 yield rl . text . replace ( " - " , "-" )
11797	def nconflicts ( self , var , val , assignment ) : "Return the number of conflicts var=val has with other variables." def conflict ( var2 ) : return ( var2 in assignment and not self . constraints ( var , val , var2 , assignment [ var2 ] ) ) return count_if ( conflict , self . neighbors [ var ] )
11014	def set_real_value_class ( self ) : if self . value_class is not None and isinstance ( self . value_class , str ) : module_name , dot , class_name = self . value_class . rpartition ( "." ) module = __import__ ( module_name , fromlist = [ class_name ] ) self . value_class = getattr ( module , class_name ) self . _initialized = True
4583	def convert_mode ( image , mode = 'RGB' ) : deprecated . deprecated ( 'util.gif.convert_model' ) return image if ( image . mode == mode ) else image . convert ( mode = mode )
5278	def preprocess_constraints ( ml , cl , n ) : "Create a graph of constraints for both must- and cannot-links" ml_graph , cl_graph = { } , { } for i in range ( n ) : ml_graph [ i ] = set ( ) cl_graph [ i ] = set ( ) def add_both ( d , i , j ) : d [ i ] . add ( j ) d [ j ] . add ( i ) for ( i , j ) in ml : ml_graph [ i ] . add ( j ) ml_graph [ j ] . add ( i ) for ( i , j ) in cl : cl_graph [ i ] . add ( j ) cl_graph [ j ] . add ( i ) def dfs ( i , graph , visited , component ) : visited [ i ] = True for j in graph [ i ] : if not visited [ j ] : dfs ( j , graph , visited , component ) component . append ( i ) visited = [ False ] * n neighborhoods = [ ] for i in range ( n ) : if not visited [ i ] and ml_graph [ i ] : component = [ ] dfs ( i , ml_graph , visited , component ) for x1 in component : for x2 in component : if x1 != x2 : ml_graph [ x1 ] . add ( x2 ) neighborhoods . append ( component ) for ( i , j ) in cl : for x in ml_graph [ i ] : add_both ( cl_graph , x , j ) for y in ml_graph [ j ] : add_both ( cl_graph , i , y ) for x in ml_graph [ i ] : for y in ml_graph [ j ] : add_both ( cl_graph , x , y ) for i in ml_graph : for j in ml_graph [ i ] : if j != i and j in cl_graph [ i ] : raise InconsistentConstraintsException ( 'Inconsistent constraints between {} and {}' . format ( i , j ) ) return ml_graph , cl_graph , neighborhoods
5197	def configure_database ( db_config ) : db_config . analog [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 1 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 1 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . analog [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . analog [ 2 ] . svariation = opendnp3 . StaticAnalogVariation . Group30Var1 db_config . analog [ 2 ] . evariation = opendnp3 . EventAnalogVariation . Group32Var7 db_config . binary [ 1 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 1 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 1 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2 db_config . binary [ 2 ] . clazz = opendnp3 . PointClass . Class2 db_config . binary [ 2 ] . svariation = opendnp3 . StaticBinaryVariation . Group1Var2 db_config . binary [ 2 ] . evariation = opendnp3 . EventBinaryVariation . Group2Var2
10544	def update_task ( task ) : try : task_id = task . id task = _forbidden_attributes ( task ) res = _pybossa_req ( 'put' , 'task' , task_id , payload = task . data ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
1564	def get_metrics_collector ( self ) : if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : raise RuntimeError ( "Metrics collector is not registered in this context" ) return self . metrics_collector
11126	def dump_copy ( self , path , relativePath , name = None , description = None , replace = False , verbose = False ) : relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' if name is None : _ , name = os . path . split ( path ) self . add_directory ( relativePath ) realPath = os . path . join ( self . __path , relativePath ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage if name in dict . __getitem__ ( dirInfoDict , "files" ) : if not replace : if verbose : warnings . warn ( "a file with the name '%s' is already defined in repository dictionary info. Set replace flag to True if you want to replace the existing file" % ( name ) ) return dump = "raise Exception(\"dump is ambiguous for copied file '$FILE_PATH' \")" pull = "raise Exception(\"pull is ambiguous for copied file '$FILE_PATH' \")" try : shutil . copyfile ( path , os . path . join ( realPath , name ) ) except Exception as e : if verbose : warnings . warn ( e ) return klass = None dict . __getitem__ ( dirInfoDict , "files" ) [ name ] = { "dump" : dump , "pull" : pull , "timestamp" : datetime . utcnow ( ) , "id" : str ( uuid . uuid1 ( ) ) , "class" : klass , "description" : description } self . save ( )
12630	def compose_err_msg ( msg , ** kwargs ) : updated_msg = msg for k , v in sorted ( kwargs . items ( ) ) : if isinstance ( v , _basestring ) : updated_msg += "\n" + k + ": " + v return updated_msg
74	def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , "alpha" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )
7760	def _call_timeout_handlers ( self ) : sources_handled = 0 now = time . time ( ) schedule = None while self . _timeout_handlers : schedule , handler = self . _timeout_handlers [ 0 ] if schedule <= now : logger . debug ( "About to call a timeout handler: {0!r}" . format ( handler ) ) self . _timeout_handlers = self . _timeout_handlers [ 1 : ] result = handler ( ) logger . debug ( " handler result: {0!r}" . format ( result ) ) rec = handler . _pyxmpp_recurring if rec : logger . debug ( " recurring, restarting in {0} s" . format ( handler . _pyxmpp_timeout ) ) self . _timeout_handlers . append ( ( now + handler . _pyxmpp_timeout , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) elif rec is None and result is not None : logger . debug ( " auto-recurring, restarting in {0} s" . format ( result ) ) self . _timeout_handlers . append ( ( now + result , handler ) ) self . _timeout_handlers . sort ( key = lambda x : x [ 0 ] ) sources_handled += 1 else : break if self . check_events ( ) : return 0 , sources_handled if self . _timeout_handlers and schedule : timeout = schedule - now else : timeout = None return timeout , sources_handled
4792	def is_unicode ( self ) : if type ( self . val ) is not unicode : self . _err ( 'Expected <%s> to be unicode, but was <%s>.' % ( self . val , type ( self . val ) . __name__ ) ) return self
3940	async def listen ( self ) : retries = 0 need_new_sid = True while retries <= self . _max_retries : if retries > 0 : backoff_seconds = self . _retry_backoff_base ** retries logger . info ( 'Backing off for %s seconds' , backoff_seconds ) await asyncio . sleep ( backoff_seconds ) if need_new_sid : await self . _fetch_channel_sid ( ) need_new_sid = False self . _chunk_parser = ChunkParser ( ) try : await self . _longpoll_request ( ) except ChannelSessionError as err : logger . warning ( 'Long-polling interrupted: %s' , err ) need_new_sid = True except exceptions . NetworkError as err : logger . warning ( 'Long-polling request failed: %s' , err ) else : retries = 0 continue retries += 1 logger . info ( 'retry attempt count is now %s' , retries ) if self . _is_connected : self . _is_connected = False await self . on_disconnect . fire ( ) logger . error ( 'Ran out of retries for long-polling request' )
1246	def disconnect ( self ) : if not self . socket : logging . warning ( "No active socket to close!" ) return self . socket . close ( ) self . socket = None
5640	def remove_all_trips_fully_outside_buffer ( db_conn , center_lat , center_lon , buffer_km , update_secondary_data = True ) : distance_function_str = add_wgs84_distance_function_to_db ( db_conn ) stops_within_buffer_query_sql = "SELECT stop_I FROM stops WHERE CAST(" + distance_function_str + "(lat, lon, {lat} , {lon}) AS INT) < {d_m}" . format ( lat = float ( center_lat ) , lon = float ( center_lon ) , d_m = int ( 1000 * buffer_km ) ) select_all_trip_Is_where_stop_I_is_within_buffer_sql = "SELECT distinct(trip_I) FROM stop_times WHERE stop_I IN (" + stops_within_buffer_query_sql + ")" trip_Is_to_remove_sql = "SELECT trip_I FROM trips WHERE trip_I NOT IN ( " + select_all_trip_Is_where_stop_I_is_within_buffer_sql + ")" trip_Is_to_remove = pandas . read_sql ( trip_Is_to_remove_sql , db_conn ) [ "trip_I" ] . values trip_Is_to_remove_string = "," . join ( [ str ( trip_I ) for trip_I in trip_Is_to_remove ] ) remove_all_trips_fully_outside_buffer_sql = "DELETE FROM trips WHERE trip_I IN (" + trip_Is_to_remove_string + ")" remove_all_stop_times_where_trip_I_fully_outside_buffer_sql = "DELETE FROM stop_times WHERE trip_I IN (" + trip_Is_to_remove_string + ")" db_conn . execute ( remove_all_trips_fully_outside_buffer_sql ) db_conn . execute ( remove_all_stop_times_where_trip_I_fully_outside_buffer_sql ) delete_stops_not_in_stop_times_and_not_as_parent_stop ( db_conn ) db_conn . execute ( DELETE_ROUTES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_SHAPES_NOT_REFERENCED_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAYS_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_DAY_TRIPS2_ENTRIES_NOT_PRESENT_IN_TRIPS_SQL ) db_conn . execute ( DELETE_CALENDAR_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_CALENDAR_DATES_ENTRIES_FOR_NON_REFERENCE_SERVICE_IS_SQL ) db_conn . execute ( DELETE_FREQUENCIES_ENTRIES_NOT_PRESENT_IN_TRIPS ) db_conn . execute ( DELETE_AGENCIES_NOT_REFERENCED_IN_ROUTES_SQL ) if update_secondary_data : update_secondary_data_copies ( db_conn )
8334	def findAllPrevious ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousGenerator , ** kwargs )
12099	def get_root_directory ( self , timestamp = None ) : if timestamp is None : timestamp = self . timestamp if self . timestamp_format is not None : root_name = ( time . strftime ( self . timestamp_format , timestamp ) + '-' + self . batch_name ) else : root_name = self . batch_name path = os . path . join ( self . output_directory , * ( self . subdir + [ root_name ] ) ) return os . path . abspath ( path )
5054	def get_identity_provider ( provider_id ) : try : from third_party_auth . provider import Registry except ImportError as exception : LOGGER . warning ( "Could not import Registry from third_party_auth.provider" ) LOGGER . warning ( exception ) Registry = None try : return Registry and Registry . get ( provider_id ) except ValueError : return None
4010	def get_docker_client ( ) : env = get_docker_env ( ) host , cert_path , tls_verify = env [ 'DOCKER_HOST' ] , env [ 'DOCKER_CERT_PATH' ] , env [ 'DOCKER_TLS_VERIFY' ] params = { 'base_url' : host . replace ( 'tcp://' , 'https://' ) , 'timeout' : None , 'version' : 'auto' } if tls_verify and cert_path : params [ 'tls' ] = docker . tls . TLSConfig ( client_cert = ( os . path . join ( cert_path , 'cert.pem' ) , os . path . join ( cert_path , 'key.pem' ) ) , ca_cert = os . path . join ( cert_path , 'ca.pem' ) , verify = True , ssl_version = None , assert_hostname = False ) return docker . Client ( ** params )
666	def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )
13219	def shell ( self , expect = pexpect ) : dsn = self . connection_dsn ( ) log . debug ( 'connection string: %s' % dsn ) child = expect . spawn ( 'psql "%s"' % dsn ) if self . _connect_args [ 'password' ] is not None : child . expect ( 'Password: ' ) child . sendline ( self . _connect_args [ 'password' ] ) child . interact ( )
9350	def check_digit ( num ) : sum = 0 digits = str ( num ) [ : - 1 ] [ : : - 1 ] for i , n in enumerate ( digits ) : if ( i + 1 ) % 2 != 0 : digit = int ( n ) * 2 if digit > 9 : sum += ( digit - 9 ) else : sum += digit else : sum += int ( n ) return ( ( divmod ( sum , 10 ) [ 0 ] + 1 ) * 10 - sum ) % 10
604	def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax
11391	def contribute_to_class ( self , cls , name ) : super ( EmbeddedMediaField , self ) . contribute_to_class ( cls , name ) register_field ( cls , self ) cls . _meta . add_virtual_field ( EmbeddedSignalCreator ( self ) )
417	def find_datasets ( self , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( "[Database] FAIL! Cannot find any dataset: {}" . format ( kwargs ) ) return False print ( "[Database] Find {} datasets SUCCESS, took: {}s" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list
13165	def traverse ( element , query , deep = False ) : part = query [ 0 ] if not part : query = query [ 1 : ] part = query [ 0 ] deep = True part , predicate = xpath_re . match ( query [ 0 ] ) . groups ( ) for c in element . _children : if part in ( '*' , c . tagname ) and c . _match ( predicate ) : if len ( query ) == 1 : yield c else : for e in traverse ( c , query [ 1 : ] ) : yield e if deep : for e in traverse ( c , query , deep = True ) : yield e
4799	def is_file ( self ) : self . exists ( ) if not os . path . isfile ( self . val ) : self . _err ( 'Expected <%s> to be a file, but was not.' % self . val ) return self
2513	def get_file_name ( self , f_term ) : for _ , _ , name in self . graph . triples ( ( f_term , self . spdx_namespace [ 'fileName' ] , None ) ) : return name return
12106	def _launch_process_group ( self , process_commands , streams_path ) : processes = [ ] for cmd , tid in process_commands : job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_path = os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) stderr_path = os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) process = { 'tid' : tid , 'cmd' : cmd , 'stdout' : stdout_path , 'stderr' : stderr_path } processes . append ( process ) json_path = os . path . join ( self . root_directory , self . json_name % ( tid ) ) with open ( json_path , 'w' ) as json_file : json . dump ( processes , json_file , sort_keys = True , indent = 4 ) p = subprocess . Popen ( [ self . script_path , json_path , self . batch_name , str ( len ( processes ) ) , str ( self . max_concurrency ) ] ) if p . wait ( ) != 0 : raise EnvironmentError ( "Script command exit with code: %d" % p . poll ( ) )
7251	def batch_workflow_status ( self , batch_workflow_id ) : self . logger . debug ( 'Get status of batch workflow: ' + batch_workflow_id ) url = '%(base_url)s/batch_workflows/%(batch_id)s' % { 'base_url' : self . base_url , 'batch_id' : batch_workflow_id } r = self . gbdx_connection . get ( url ) return r . json ( )
7531	def _plot_dag ( dag , results , snames ) : try : import matplotlib . pyplot as plt from matplotlib . dates import date2num from matplotlib . cm import gist_rainbow plt . figure ( "dag_layout" , figsize = ( 10 , 10 ) ) nx . draw ( dag , pos = nx . spring_layout ( dag ) , node_color = 'pink' , with_labels = True ) plt . savefig ( "./dag_layout.png" , bbox_inches = 'tight' , dpi = 200 ) pos = { } colors = { } for node in dag : mtd = results [ node ] . metadata start = date2num ( mtd . started ) _ , _ , sname = node . split ( "-" , 2 ) sid = snames . index ( sname ) pos [ node ] = ( start + sid , start * 1e6 ) colors [ node ] = mtd . engine_id plt . figure ( "dag_starttimes" , figsize = ( 10 , 16 ) ) nx . draw ( dag , pos , node_list = colors . keys ( ) , node_color = colors . values ( ) , cmap = gist_rainbow , with_labels = True ) plt . savefig ( "./dag_starttimes.png" , bbox_inches = 'tight' , dpi = 200 ) except Exception as inst : LOGGER . warning ( inst )
10567	def _check_filters ( song , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : include = True if include_filters : if all_includes : if not all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False else : if not any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in include_filters ) : include = False if exclude_filters : if all_excludes : if all ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False else : if any ( field in song and _check_field_value ( song [ field ] , pattern ) for field , pattern in exclude_filters ) : include = False return include
11505	def move_folder ( self , token , folder_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.folder.move' , parameters ) return response
468	def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice
1689	def InTemplateArgumentList ( self , clean_lines , linenum , pos ) : while linenum < clean_lines . NumLines ( ) : line = clean_lines . elided [ linenum ] match = Match ( r'^[^{};=\[\]\.<>]*(.)' , line [ pos : ] ) if not match : linenum += 1 pos = 0 continue token = match . group ( 1 ) pos += len ( match . group ( 0 ) ) if token in ( '{' , '}' , ';' ) : return False if token in ( '>' , '=' , '[' , ']' , '.' ) : return True if token != '<' : pos += 1 if pos >= len ( line ) : linenum += 1 pos = 0 continue ( _ , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , pos - 1 ) if end_pos < 0 : return False linenum = end_line pos = end_pos return False
13564	def get_field_names ( obj , ignore_auto = True , ignore_relations = True , exclude = [ ] ) : from django . db . models import ( AutoField , ForeignKey , ManyToManyField , ManyToOneRel , OneToOneField , OneToOneRel ) for field in obj . _meta . get_fields ( ) : if ignore_auto and isinstance ( field , AutoField ) : continue if ignore_relations and ( isinstance ( field , ForeignKey ) or isinstance ( field , ManyToManyField ) or isinstance ( field , ManyToOneRel ) or isinstance ( field , OneToOneRel ) or isinstance ( field , OneToOneField ) ) : a = 1 a continue if field . name in exclude : continue yield field . name
1672	def ProcessFile ( filename , vlevel , extra_check_functions = None ) : _SetVerboseLevel ( vlevel ) _BackupFilters ( ) if not ProcessConfigOverrides ( filename ) : _RestoreFilters ( ) return lf_lines = [ ] crlf_lines = [ ] try : if filename == '-' : lines = codecs . StreamReaderWriter ( sys . stdin , codecs . getreader ( 'utf8' ) , codecs . getwriter ( 'utf8' ) , 'replace' ) . read ( ) . split ( '\n' ) else : lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\n' ) for linenum in range ( len ( lines ) - 1 ) : if lines [ linenum ] . endswith ( '\r' ) : lines [ linenum ] = lines [ linenum ] . rstrip ( '\r' ) crlf_lines . append ( linenum + 1 ) else : lf_lines . append ( linenum + 1 ) except IOError : _cpplint_state . PrintError ( "Skipping input '%s': Can't open for reading\n" % filename ) _RestoreFilters ( ) return file_extension = filename [ filename . rfind ( '.' ) + 1 : ] if filename != '-' and file_extension not in GetAllExtensions ( ) : bazel_gen_files = set ( [ "external/local_config_cc/libtool" , "external/local_config_cc/make_hashed_objlist.py" , "external/local_config_cc/wrapped_ar" , "external/local_config_cc/wrapped_clang" , "external/local_config_cc/xcrunwrapper.sh" , ] ) if not filename in bazel_gen_files : _cpplint_state . PrintError ( 'Ignoring %s; not a valid file name ' '(%s)\n' % ( filename , ', ' . join ( GetAllExtensions ( ) ) ) ) else : ProcessFileData ( filename , file_extension , lines , Error , extra_check_functions ) if lf_lines and crlf_lines : for linenum in crlf_lines : Error ( filename , linenum , 'whitespace/newline' , 1 , 'Unexpected \\r (^M) found; better to use only \\n' ) _RestoreFilters ( )
2332	def uniform_noise ( points ) : return np . random . rand ( 1 ) * np . random . uniform ( points , 1 ) + random . sample ( [ 2 , - 2 ] , 1 )
2975	def cmd_destroy ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . destroy ( )
5564	def input ( self ) : delimiters = dict ( zoom = self . init_zoom_levels , bounds = self . init_bounds , process_bounds = self . bounds , effective_bounds = self . effective_bounds ) raw_inputs = { get_hash ( v ) : v for zoom in self . init_zoom_levels if "input" in self . _params_at_zoom [ zoom ] for key , v in _flatten_tree ( self . _params_at_zoom [ zoom ] [ "input" ] ) if v is not None } initalized_inputs = { } for k , v in raw_inputs . items ( ) : if isinstance ( v , str ) : logger . debug ( "load input reader for simple input %s" , v ) try : reader = load_input_reader ( dict ( path = absolute_path ( path = v , base_dir = self . config_dir ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for simple input %s is %s" , v , reader ) elif isinstance ( v , dict ) : logger . debug ( "load input reader for abstract input %s" , v ) try : reader = load_input_reader ( dict ( abstract = deepcopy ( v ) , pyramid = self . process_pyramid , pixelbuffer = self . process_pyramid . pixelbuffer , delimiters = delimiters , conf_dir = self . config_dir ) , readonly = self . mode == "readonly" ) except Exception as e : logger . exception ( e ) raise MapcheteDriverError ( "error when loading input %s: %s" % ( v , e ) ) logger . debug ( "input reader for abstract input %s is %s" , v , reader ) else : raise MapcheteConfigError ( "invalid input type %s" , type ( v ) ) reader . bbox ( out_crs = self . process_pyramid . crs ) initalized_inputs [ k ] = reader return initalized_inputs
270	def check_intraday ( estimate , returns , positions , transactions ) : if estimate == 'infer' : if positions is not None and transactions is not None : if detect_intraday ( positions , transactions ) : warnings . warn ( 'Detected intraday strategy; inferring positi' + 'ons from transactions. Set estimate_intraday' + '=False to disable.' ) return estimate_intraday ( returns , positions , transactions ) else : return positions else : return positions elif estimate : if positions is not None and transactions is not None : return estimate_intraday ( returns , positions , transactions ) else : raise ValueError ( 'Positions and txns needed to estimate intraday' ) else : return positions
13894	def MatchMasks ( filename , masks ) : import fnmatch if not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] for i_mask in masks : if fnmatch . fnmatch ( filename , i_mask ) : return True return False
2780	def get_object ( cls , api_token , domain , record_id ) : record = cls ( token = api_token , domain = domain , id = record_id ) record . load ( ) return record
12307	def auto_get_repo ( autooptions , debug = False ) : pluginmgr = plugins_get_mgr ( ) repomgr = pluginmgr . get ( what = 'repomanager' , name = 'git' ) repo = None try : if debug : print ( "Looking repo" ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) except : try : print ( "Checking and cloning if the dataset exists on backend" ) url = autooptions [ 'remoteurl' ] if debug : print ( "Doesnt exist. trying to clone: {}" . format ( url ) ) common_clone ( url ) repo = repomgr . lookup ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] ) if debug : print ( "Cloning successful" ) except : yes = input ( "Repo doesnt exist. Should I create one? [yN]" ) if yes == 'y' : setup = "git" if autooptions [ 'remoteurl' ] . startswith ( 's3://' ) : setup = 'git+s3' repo = common_init ( username = autooptions [ 'username' ] , reponame = autooptions [ 'reponame' ] , setup = setup , force = True , options = autooptions ) if debug : print ( "Successfully inited repo" ) else : raise Exception ( "Cannot load repo" ) repo . options = autooptions return repo
9682	def set_fan_power ( self , power ) : if power > 255 : raise ValueError ( "The fan power should be a single byte (0-255)." ) a = self . cnxn . xfer ( [ 0x42 ] ) [ 0 ] sleep ( 10e-3 ) b = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] c = self . cnxn . xfer ( [ power ] ) [ 0 ] sleep ( 0.1 ) return True if a == 0xF3 and b == 0x42 and c == 0x00 else False
2939	def deserialize_logical ( self , node ) : term1_attrib = node . getAttribute ( 'left-field' ) term1_value = node . getAttribute ( 'left-value' ) op = node . nodeName . lower ( ) term2_attrib = node . getAttribute ( 'right-field' ) term2_value = node . getAttribute ( 'right-value' ) if op not in _op_map : _exc ( 'Invalid operator' ) if term1_attrib != '' and term1_value != '' : _exc ( 'Both, left-field and left-value attributes found' ) elif term1_attrib == '' and term1_value == '' : _exc ( 'left-field or left-value attribute required' ) elif term1_value != '' : left = term1_value else : left = operators . Attrib ( term1_attrib ) if term2_attrib != '' and term2_value != '' : _exc ( 'Both, right-field and right-value attributes found' ) elif term2_attrib == '' and term2_value == '' : _exc ( 'right-field or right-value attribute required' ) elif term2_value != '' : right = term2_value else : right = operators . Attrib ( term2_attrib ) return _op_map [ op ] ( left , right )
5797	def _get_func_info ( docstring , def_lineno , code_lines , prefix ) : def_index = def_lineno - 1 definition = code_lines [ def_index ] definition = definition . rstrip ( ) while not definition . endswith ( ':' ) : def_index += 1 definition += '\n' + code_lines [ def_index ] . rstrip ( ) definition = textwrap . dedent ( definition ) . rstrip ( ':' ) definition = definition . replace ( '\n' , '\n' + prefix ) description = '' found_colon = False params = '' for line in docstring . splitlines ( ) : if line and line [ 0 ] == ':' : found_colon = True if not found_colon : if description : description += '\n' description += line else : if params : params += '\n' params += line description = description . strip ( ) description_md = '' if description : description_md = "%s%s" % ( prefix , description . replace ( '\n' , '\n' + prefix ) ) description_md = re . sub ( '\n>(\\s+)\n' , '\n>\n' , description_md ) params = params . strip ( ) if params : definition += ( ':\n%s ' % prefix ) definition = re . sub ( '\n>(\\s+)\n' , '\n>\n' , definition ) for search , replace in definition_replacements . items ( ) : definition = definition . replace ( search , replace ) return ( definition , description_md )
5233	def all_folders ( path_name , keyword = '' , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword : folders = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/*{keyword}*' ) if os . path . isdir ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : folders = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isdir ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : folders = filter_by_dates ( folders , date_fmt = date_fmt ) return folders
6372	def accuracy ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) return ( self . _tp + self . _tn ) / self . population ( )
8142	def scale ( self , w = 1.0 , h = 1.0 ) : from types import FloatType w0 , h0 = self . img . size if type ( w ) == FloatType : w = int ( w * w0 ) if type ( h ) == FloatType : h = int ( h * h0 ) self . img = self . img . resize ( ( w , h ) , INTERPOLATION ) self . w = w self . h = h
5610	def _shift_required ( tiles ) : if tiles [ 0 ] [ 0 ] . tile_pyramid . is_global : tile_cols = sorted ( list ( set ( [ t [ 0 ] . col for t in tiles ] ) ) ) if tile_cols == list ( range ( min ( tile_cols ) , max ( tile_cols ) + 1 ) ) : return False else : def gen_groups ( items ) : j = items [ 0 ] group = [ j ] for i in items [ 1 : ] : if i == j + 1 : group . append ( i ) else : yield group group = [ i ] j = i yield group groups = list ( gen_groups ( tile_cols ) ) if len ( groups ) == 1 : return False normal_distance = groups [ - 1 ] [ - 1 ] - groups [ 0 ] [ 0 ] antimeridian_distance = ( groups [ 0 ] [ - 1 ] + tiles [ 0 ] [ 0 ] . tile_pyramid . matrix_width ( tiles [ 0 ] [ 0 ] . zoom ) ) - groups [ - 1 ] [ 0 ] return antimeridian_distance < normal_distance else : return False
10929	def do_internal_run ( self , initial_count = 0 , subblock = None , update_derr = True ) : self . _inner_run_counter = initial_count good_step = True n_good_steps = 0 CLOG . debug ( 'Running...' ) _last_residuals = self . calc_residuals ( ) . copy ( ) while ( ( self . _inner_run_counter < self . run_length ) & good_step & ( not self . check_terminate ( ) ) ) : if self . check_Broyden_J ( ) and self . _inner_run_counter != 0 : self . update_Broyden_J ( ) if self . check_update_eig_J ( ) and self . _inner_run_counter != 0 : self . update_eig_J ( ) er0 = 1 * self . error delta_vals = self . find_LM_updates ( self . calc_grad ( ) , do_correct_damping = False , subblock = subblock ) er1 = self . update_function ( self . param_vals + delta_vals ) good_step = er1 < er0 if good_step : n_good_steps += 1 CLOG . debug ( '%f\t%f' % ( er0 , er1 ) ) self . update_param_vals ( delta_vals , incremental = True ) self . _last_residuals = _last_residuals . copy ( ) if update_derr : self . _last_error = er0 self . error = er1 _last_residuals = self . calc_residuals ( ) . copy ( ) else : er0_0 = self . update_function ( self . param_vals ) CLOG . debug ( 'Bad step!' ) if np . abs ( er0 - er0_0 ) > 1e-6 : raise RuntimeError ( 'Function updates are not exact.' ) self . _inner_run_counter += 1 return n_good_steps
8497	def _output ( calls , args ) : if args . natural_sort or args . source : calls = sorted ( calls , key = lambda c : ( c . filename , c . lineno ) ) else : calls = sorted ( calls , key = lambda c : c . key ) out = [ ] if args . only_keys : keys = set ( ) for call in calls : if call . key in keys : continue out . append ( _format_call ( call , args ) ) keys . add ( call . key ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' ) return keys = set ( ) for call in calls : if call . default : keys . add ( call . key ) for call in calls : if not args . all and not call . default and call . key in keys : continue out . append ( _format_call ( call , args ) ) out = '\n' . join ( out ) if args . color : out = _colorize ( out ) print ( out , end = ' ' )
877	def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )
8992	def rows_after ( self ) : rows_after = [ ] for mesh in self . produced_meshes : if mesh . is_consumed ( ) : row = mesh . consuming_row if rows_after not in rows_after : rows_after . append ( row ) return rows_after
2752	def get_all_domains ( self ) : data = self . get_data ( "domains/" ) domains = list ( ) for jsoned in data [ 'domains' ] : domain = Domain ( ** jsoned ) domain . token = self . token domains . append ( domain ) return domains
9989	def import_funcs ( self , module ) : newcells = self . _impl . new_cells_from_module ( module ) return get_interfaces ( newcells )
3659	def add_coeffs ( self , Tmin , Tmax , coeffs ) : self . n += 1 if not self . Ts : self . Ts = [ Tmin , Tmax ] self . coeff_sets = [ coeffs ] else : for ind , T in enumerate ( self . Ts ) : if Tmin < T : self . Ts . insert ( ind , Tmin ) self . coeff_sets . insert ( ind , coeffs ) return self . Ts . append ( Tmax ) self . coeff_sets . append ( coeffs )
13040	def process ( self , nemo ) : self . __nemo__ = nemo for annotation in self . __annotations__ : annotation . target . expanded = frozenset ( self . __getinnerreffs__ ( objectId = annotation . target . objectId , subreference = annotation . target . subreference ) )
2626	def cancel ( self , job_ids ) : if self . linger is True : logger . debug ( "Ignoring cancel requests due to linger mode" ) return [ False for x in job_ids ] try : self . client . terminate_instances ( InstanceIds = list ( job_ids ) ) except Exception as e : logger . error ( "Caught error while attempting to remove instances: {0}" . format ( job_ids ) ) raise e else : logger . debug ( "Removed the instances: {0}" . format ( job_ids ) ) for job_id in job_ids : self . resources [ job_id ] [ "status" ] = "COMPLETED" for job_id in job_ids : self . instances . remove ( job_id ) return [ True for x in job_ids ]
8658	def similar_to ( partial_zipcode , zips = _zips ) : return [ z for z in zips if z [ "zip_code" ] . startswith ( partial_zipcode ) ]
11232	def run_excel_to_html ( ) : parser = argparse . ArgumentParser ( prog = 'excel_to_html' ) parser . add_argument ( '-p' , nargs = '?' , help = 'Path to an excel file for conversion.' ) parser . add_argument ( '-s' , nargs = '?' , help = 'The name of a sheet in our excel file. Defaults to "Sheet1".' , ) parser . add_argument ( '-css' , nargs = '?' , help = 'Space separated css classes to append to the table.' ) parser . add_argument ( '-m' , action = 'store_true' , help = 'Merge, attempt to combine merged cells.' ) parser . add_argument ( '-c' , nargs = '?' , help = 'Caption for creating an accessible table.' ) parser . add_argument ( '-d' , nargs = '?' , help = 'Two strings separated by a | character. The first string \ is for the html "summary" attribute and the second string is for the html "details" attribute. \ both values must be provided and nothing more.' , ) parser . add_argument ( '-r' , action = 'store_true' , help = 'Row headers. Does the table have row headers?' ) args = parser . parse_args ( ) inputs = { 'p' : args . p , 's' : args . s , 'css' : args . css , 'm' : args . m , 'c' : args . c , 'd' : args . d , 'r' : args . r , } p = inputs [ 'p' ] s = inputs [ 's' ] if inputs [ 's' ] else 'Sheet1' css = inputs [ 'css' ] if inputs [ 'css' ] else '' m = inputs [ 'm' ] if inputs [ 'm' ] else False c = inputs [ 'c' ] if inputs [ 'c' ] else '' d = inputs [ 'd' ] . split ( '|' ) if inputs [ 'd' ] else [ ] r = inputs [ 'r' ] if inputs [ 'r' ] else False html = fp . excel_to_html ( p , sheetname = s , css_classes = css , caption = c , details = d , row_headers = r , merge = m ) print ( html )
5764	def _unarmor_pem ( data , password = None ) : object_type , headers , der_bytes = pem . unarmor ( data ) type_regex = '^((DSA|EC|RSA) PRIVATE KEY|ENCRYPTED PRIVATE KEY|PRIVATE KEY|PUBLIC KEY|RSA PUBLIC KEY|CERTIFICATE)' armor_type = re . match ( type_regex , object_type ) if not armor_type : raise ValueError ( pretty_message ( ) ) pem_header = armor_type . group ( 1 ) data = data . strip ( ) if pem_header in set ( [ 'RSA PRIVATE KEY' , 'DSA PRIVATE KEY' , 'EC PRIVATE KEY' ] ) : algo = armor_type . group ( 2 ) . lower ( ) return ( 'private key' , algo , _unarmor_pem_openssl_private ( headers , der_bytes , password ) ) key_type = pem_header . lower ( ) algo = None if key_type == 'encrypted private key' : key_type = 'private key' elif key_type == 'rsa public key' : key_type = 'public key' algo = 'rsa' return ( key_type , algo , der_bytes )
5513	def bytes_per_second ( ftp , retr = True ) : tot_bytes = 0 if retr : def request_file ( ) : ftp . voidcmd ( 'TYPE I' ) conn = ftp . transfercmd ( "retr " + TESTFN ) return conn with contextlib . closing ( request_file ( ) ) as conn : register_memory ( ) stop_at = time . time ( ) + 1.0 while stop_at > time . time ( ) : chunk = conn . recv ( BUFFER_LEN ) if not chunk : a = time . time ( ) ftp . voidresp ( ) conn . close ( ) conn = request_file ( ) stop_at += time . time ( ) - a tot_bytes += len ( chunk ) try : while chunk : chunk = conn . recv ( BUFFER_LEN ) ftp . voidresp ( ) conn . close ( ) except ( ftplib . error_temp , ftplib . error_perm ) : pass else : ftp . voidcmd ( 'TYPE I' ) with contextlib . closing ( ftp . transfercmd ( "STOR " + TESTFN ) ) as conn : register_memory ( ) chunk = b'x' * BUFFER_LEN stop_at = time . time ( ) + 1 while stop_at > time . time ( ) : tot_bytes += conn . send ( chunk ) ftp . voidresp ( ) return tot_bytes
7569	def fullcomp ( seq ) : seq = seq . replace ( "A" , 'u' ) . replace ( 'T' , 'v' ) . replace ( 'C' , 'p' ) . replace ( 'G' , 'z' ) . replace ( 'u' , 'T' ) . replace ( 'v' , 'A' ) . replace ( 'p' , 'G' ) . replace ( 'z' , 'C' ) seq = seq . replace ( 'R' , 'u' ) . replace ( 'K' , 'v' ) . replace ( 'Y' , 'b' ) . replace ( 'M' , 'o' ) . replace ( 'u' , 'Y' ) . replace ( 'v' , 'M' ) . replace ( 'b' , 'R' ) . replace ( 'o' , 'K' ) seq = seq . replace ( 'r' , 'u' ) . replace ( 'k' , 'v' ) . replace ( 'y' , 'b' ) . replace ( 'm' , 'o' ) . replace ( 'u' , 'y' ) . replace ( 'v' , 'm' ) . replace ( 'b' , 'r' ) . replace ( 'o' , 'k' ) return seq
3609	def post_async ( self , url , data , callback = None , params = None , headers = None ) : params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , None ) self . _authenticate ( params , headers ) data = json . dumps ( data , cls = JSONEncoder ) process_pool . apply_async ( make_post_request , args = ( endpoint , data , params , headers ) , callback = callback )
5434	def parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) : job_params = [ ] for col in header : col_type = '--env' col_value = col if col . startswith ( '-' ) : col_type , col_value = split_pair ( col , ' ' , 1 ) if col_type == '--env' : job_params . append ( job_model . EnvParam ( col_value ) ) elif col_type == '--label' : job_params . append ( job_model . LabelParam ( col_value ) ) elif col_type == '--input' or col_type == '--input-recursive' : name = input_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . InputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) elif col_type == '--output' or col_type == '--output-recursive' : name = output_file_param_util . get_variable_name ( col_value ) job_params . append ( job_model . OutputFileParam ( name , recursive = ( col_type . endswith ( 'recursive' ) ) ) ) else : raise ValueError ( 'Unrecognized column header: %s' % col ) return job_params
8491	def _parse_hosts ( self , hosts ) : if hosts is None : return if isinstance ( hosts , six . string_types ) : hosts = [ host . strip ( ) for host in hosts . split ( ',' ) ] hosts = [ host . split ( ':' ) for host in hosts ] hosts = [ ( host [ 0 ] , int ( host [ 1 ] ) ) for host in hosts ] return tuple ( hosts )
7871	def set_payload ( self , payload ) : if isinstance ( payload , ElementClass ) : self . _payload = [ XMLPayload ( payload ) ] elif isinstance ( payload , StanzaPayload ) : self . _payload = [ payload ] else : raise TypeError ( "Bad payload type" ) self . _dirty = True
4222	def disable ( ) : root = platform . config_root ( ) try : os . makedirs ( root ) except OSError : pass filename = os . path . join ( root , 'keyringrc.cfg' ) if os . path . exists ( filename ) : msg = "Refusing to overwrite {filename}" . format ( ** locals ( ) ) raise RuntimeError ( msg ) with open ( filename , 'w' ) as file : file . write ( '[backend]\ndefault-keyring=keyring.backends.null.Keyring' )
2355	def find_elements ( self , strategy , locator ) : return self . driver_adapter . find_elements ( strategy , locator , root = self . root )
1470	def getStmgrsRegSummary ( self , tmaster , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) request_str = reg_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch stmgrsregistrationsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) reg_response . ParseFromString ( result . body ) ret = { } for stmgr in reg_response . registered_stmgrs : ret [ stmgr ] = True for stmgr in reg_response . absent_stmgrs : ret [ stmgr ] = False raise tornado . gen . Return ( ret )
10190	def tell ( self , message , sender = no_sender ) : if sender is not no_sender and not isinstance ( sender , ActorRef ) : raise ValueError ( "Sender must be actor reference" ) self . _cell . send_message ( message , sender )
2641	def shutdown ( self ) : self . is_alive = False logging . debug ( "Waking management thread" ) self . incoming_q . put ( None ) self . _queue_management_thread . join ( ) logging . debug ( "Exiting thread" ) self . worker . join ( ) return True
4337	def pad ( self , start_duration = 0.0 , end_duration = 0.0 ) : if not is_number ( start_duration ) or start_duration < 0 : raise ValueError ( "Start duration must be a positive number." ) if not is_number ( end_duration ) or end_duration < 0 : raise ValueError ( "End duration must be positive." ) effect_args = [ 'pad' , '{:f}' . format ( start_duration ) , '{:f}' . format ( end_duration ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'pad' ) return self
11927	def run_server ( self , port ) : try : self . server = MultiThreadedHTTPServer ( ( '0.0.0.0' , port ) , Handler ) except socket . error , e : logger . error ( str ( e ) ) sys . exit ( 1 ) logger . info ( "HTTP serve at http://0.0.0.0:%d (ctrl-c to stop) ..." % port ) try : self . server . serve_forever ( ) except KeyboardInterrupt : logger . info ( "^C received, shutting down server" ) self . shutdown_server ( )
3385	def _reproject ( self , p ) : nulls = self . problem . nullspace equalities = self . problem . equalities if np . allclose ( equalities . dot ( p ) , self . problem . b , rtol = 0 , atol = self . feasibility_tol ) : new = p else : LOGGER . info ( "feasibility violated in sample" " %d, trying to reproject" % self . n_samples ) new = nulls . dot ( nulls . T . dot ( p ) ) if any ( new != p ) : LOGGER . info ( "reprojection failed in sample" " %d, using random point in space" % self . n_samples ) new = self . _random_point ( ) return new
3731	def checkCAS ( CASRN ) : try : check = CASRN [ - 1 ] CASRN = CASRN [ : : - 1 ] [ 1 : ] productsum = 0 i = 1 for num in CASRN : if num == '-' : pass else : productsum += i * int ( num ) i += 1 return ( productsum % 10 == int ( check ) ) except : return False
5479	def _cancel_batch ( batch_fn , cancel_fn , ops ) : canceled = [ ] failed = [ ] def handle_cancel_response ( request_id , response , exception ) : del response if exception : msg = 'error %s: %s' % ( exception . resp . status , exception . resp . reason ) if exception . resp . status == FAILED_PRECONDITION_CODE : detail = json . loads ( exception . content ) status = detail . get ( 'error' , { } ) . get ( 'status' ) if status == FAILED_PRECONDITION_STATUS : msg = 'Not running' failed . append ( { 'name' : request_id , 'msg' : msg } ) else : canceled . append ( { 'name' : request_id } ) return batch = batch_fn ( callback = handle_cancel_response ) ops_by_name = { } for op in ops : op_name = op . get_field ( 'internal-id' ) ops_by_name [ op_name ] = op batch . add ( cancel_fn ( name = op_name , body = { } ) , request_id = op_name ) batch . execute ( ) canceled_ops = [ ops_by_name [ op [ 'name' ] ] for op in canceled ] error_messages = [ ] for fail in failed : op = ops_by_name [ fail [ 'name' ] ] error_messages . append ( "Error canceling '%s': %s" % ( get_operation_full_job_id ( op ) , fail [ 'msg' ] ) ) return canceled_ops , error_messages
2761	def get_certificate ( self , id ) : return Certificate . get_object ( api_token = self . token , cert_id = id )
13170	def path ( self , include_root = False ) : path = '%s[%d]' % ( self . tagname , self . index or 0 ) p = self . parent while p is not None : if p . parent or include_root : path = '%s[%d]/%s' % ( p . tagname , p . index or 0 , path ) p = p . parent return path
6795	def syncdb ( self , site = None , all = 0 , database = None , ignore_errors = 1 ) : r = self . local_renderer ignore_errors = int ( ignore_errors ) post_south = self . version_tuple >= ( 1 , 7 , 0 ) use_run_syncdb = self . version_tuple >= ( 1 , 9 , 0 ) r . env . db_syncdb_all_flag = '--all' if int ( all ) else '' r . env . db_syncdb_database = '' if database : r . env . db_syncdb_database = ' --database=%s' % database if self . is_local : r . env . project_dir = r . env . local_project_dir site = site or self . genv . SITE for _site , site_data in r . iter_unique_databases ( site = site ) : r . env . SITE = _site with self . settings ( warn_only = ignore_errors ) : if post_south : if use_run_syncdb : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --run-syncdb --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} migrate --noinput {db_syncdb_database}' ) else : r . run_or_local ( 'export SITE={SITE}; export ROLE={ROLE}; cd {project_dir}; ' '{manage_cmd} syncdb --noinput {db_syncdb_all_flag} {db_syncdb_database}' )
10381	def _get_drug_target_interactions ( manager : Optional [ 'bio2bel_drugbank.manager' ] = None ) -> Mapping [ str , List [ str ] ] : if manager is None : import bio2bel_drugbank manager = bio2bel_drugbank . Manager ( ) if not manager . is_populated ( ) : manager . populate ( ) return manager . get_drug_to_hgnc_symbols ( )
12142	def _info ( self , source , key , filetype , ignore ) : specs , mdata = [ ] , { } mdata_clashes = set ( ) for spec in source . specs : if key not in spec : raise Exception ( "Key %r not available in 'source'." % key ) mdata = dict ( ( k , v ) for ( k , v ) in filetype . metadata ( spec [ key ] ) . items ( ) if k not in ignore ) mdata_spec = { } mdata_spec . update ( spec ) mdata_spec . update ( mdata ) specs . append ( mdata_spec ) mdata_clashes = mdata_clashes | ( set ( spec . keys ( ) ) & set ( mdata . keys ( ) ) ) if mdata_clashes : self . warning ( "Loaded metadata keys overriding source keys." ) return specs
8876	def compute_dosage ( expec , alt = None ) : r if alt is None : return expec [ ... , - 1 ] try : return expec [ : , alt ] except NotImplementedError : alt = asarray ( alt , int ) return asarray ( expec , float ) [ : , alt ]
11002	def pack_args ( self ) : mapper = { 'psf-kfki' : 'kfki' , 'psf-alpha' : 'alpha' , 'psf-n2n1' : 'n2n1' , 'psf-sigkf' : 'sigkf' , 'psf-sph6-ab' : 'sph6_ab' , 'psf-laser-wavelength' : 'laser_wavelength' , 'psf-pinhole-width' : 'pinhole_width' } bads = [ self . zscale , 'psf-zslab' ] d = { } for k , v in iteritems ( mapper ) : if k in self . param_dict : d [ v ] = self . param_dict [ k ] d . update ( { 'polar_angle' : self . polar_angle , 'normalize' : self . normalize , 'include_K3_det' : self . use_J1 } ) if self . polychromatic : d . update ( { 'nkpts' : self . nkpts } ) d . update ( { 'k_dist' : self . k_dist } ) if self . do_pinhole : d . update ( { 'nlpts' : self . num_line_pts } ) d . update ( { 'use_laggauss' : True } ) return d
2815	def convert_avgpool ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width = params [ 'kernel_shape' ] else : height , width = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width = params [ 'strides' ] else : stride_height , stride_width = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , _ , _ = params [ 'pads' ] else : padding_h , padding_w = params [ 'padding' ] input_name = inputs [ 0 ] pad = 'valid' if height % 2 == 1 and width % 2 == 1 and height // 2 == padding_h and width // 2 == padding_w and stride_height == 1 and stride_width == 1 : pad = 'same' else : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding2D ( padding = ( padding_h , padding_w ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . AveragePooling2D ( pool_size = ( height , width ) , strides = ( stride_height , stride_width ) , padding = pad , name = tf_name , data_format = 'channels_first' ) layers [ scope_name ] = pooling ( layers [ input_name ] )
6008	def load_noise_map ( noise_map_path , noise_map_hdu , pixel_scale , image , background_noise_map , exposure_time_map , convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map , convert_from_electrons , gain , convert_from_adus ) : noise_map_options = sum ( [ convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map ] ) if noise_map_options > 1 : raise exc . DataException ( 'You have specified more than one method to load the noise_map map, e.g.:' 'convert_noise_map_from_weight_map | ' 'convert_noise_map_from_inverse_noise_map |' 'noise_map_from_image_and_background_noise_map' ) if noise_map_options == 0 and noise_map_path is not None : return NoiseMap . from_fits_with_pixel_scale ( file_path = noise_map_path , hdu = noise_map_hdu , pixel_scale = pixel_scale ) elif convert_noise_map_from_weight_map and noise_map_path is not None : weight_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_noise_map_from_inverse_noise_map and noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = noise_map_path , hdu = noise_map_hdu ) return NoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) elif noise_map_from_image_and_background_noise_map : if background_noise_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a ' 'background noise_map map is not supplied.' ) if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the noise-map from the image and background noise_map map if a' 'gain is not supplied to convert from adus' ) return NoiseMap . from_image_and_background_noise_map ( pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) else : raise exc . DataException ( 'A noise_map map was not loaded, specify a noise_map_path or option to compute a noise_map map.' )
1383	def unregister_watch ( self , uid ) : Log . info ( "Unregister a watch with uid: " + str ( uid ) ) self . watches . pop ( uid , None )
4365	def decode ( rawstr , json_loads = default_json_loads ) : decoded_msg = { } try : rawstr = rawstr . decode ( 'utf-8' ) except AttributeError : pass split_data = rawstr . split ( ":" , 3 ) msg_type = split_data [ 0 ] msg_id = split_data [ 1 ] endpoint = split_data [ 2 ] data = '' if msg_id != '' : if "+" in msg_id : msg_id = msg_id . split ( '+' ) [ 0 ] decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = 'data' else : decoded_msg [ 'id' ] = int ( msg_id ) decoded_msg [ 'ack' ] = True msg_type_id = int ( msg_type ) if msg_type_id in MSG_VALUES : decoded_msg [ 'type' ] = MSG_VALUES [ int ( msg_type ) ] else : raise Exception ( "Unknown message type: %s" % msg_type ) decoded_msg [ 'endpoint' ] = endpoint if len ( split_data ) > 3 : data = split_data [ 3 ] if msg_type == "0" : pass elif msg_type == "1" : decoded_msg [ 'qs' ] = data elif msg_type == "2" : pass elif msg_type == "3" : decoded_msg [ 'data' ] = data elif msg_type == "4" : decoded_msg [ 'data' ] = json_loads ( data ) elif msg_type == "5" : try : data = json_loads ( data ) except ValueError : print ( "Invalid JSON event message" , data ) decoded_msg [ 'args' ] = [ ] else : decoded_msg [ 'name' ] = data . pop ( 'name' ) if 'args' in data : decoded_msg [ 'args' ] = data [ 'args' ] else : decoded_msg [ 'args' ] = [ ] elif msg_type == "6" : if '+' in data : ackId , data = data . split ( '+' ) decoded_msg [ 'ackId' ] = int ( ackId ) decoded_msg [ 'args' ] = json_loads ( data ) else : decoded_msg [ 'ackId' ] = int ( data ) decoded_msg [ 'args' ] = [ ] elif msg_type == "7" : if '+' in data : reason , advice = data . split ( '+' ) decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( reason ) ] decoded_msg [ 'advice' ] = ADVICES_VALUES [ int ( advice ) ] else : decoded_msg [ 'advice' ] = '' if data != '' : decoded_msg [ 'reason' ] = REASONS_VALUES [ int ( data ) ] else : decoded_msg [ 'reason' ] = '' elif msg_type == "8" : pass return decoded_msg
3880	async def _handle_watermark_notification ( self , watermark_notification ) : conv_id = watermark_notification . conversation_id . id res = parsers . parse_watermark_notification ( watermark_notification ) await self . on_watermark_notification . fire ( res ) try : conv = await self . _get_or_fetch_conversation ( conv_id ) except exceptions . NetworkError : logger . warning ( 'Failed to fetch conversation for watermark notification: %s' , conv_id ) else : await conv . on_watermark_notification . fire ( res )
8229	def ximport ( self , libName ) : lib = __import__ ( libName ) self . _namespace [ libName ] = lib lib . _ctx = self return lib
8264	def swarm ( self , x , y , r = 100 ) : sc = _ctx . stroke ( 0 , 0 , 0 , 0 ) sw = _ctx . strokewidth ( 0 ) _ctx . push ( ) _ctx . transform ( _ctx . CORNER ) _ctx . translate ( x , y ) for i in _range ( r * 3 ) : clr = choice ( self ) . copy ( ) clr . alpha -= 0.5 * random ( ) _ctx . fill ( clr ) clr = choice ( self ) _ctx . stroke ( clr ) _ctx . strokewidth ( 10 * random ( ) ) _ctx . rotate ( 360 * random ( ) ) r2 = r * 0.5 * random ( ) _ctx . oval ( r * random ( ) , 0 , r2 , r2 ) _ctx . pop ( ) _ctx . strokewidth ( sw ) if sc is None : _ctx . nostroke ( ) else : _ctx . stroke ( sc )
2493	def create_annotation_node ( self , annotation ) : annotation_node = URIRef ( str ( annotation . spdx_id ) ) type_triple = ( annotation_node , RDF . type , self . spdx_namespace . Annotation ) self . graph . add ( type_triple ) annotator_node = Literal ( annotation . annotator . to_value ( ) ) self . graph . add ( ( annotation_node , self . spdx_namespace . annotator , annotator_node ) ) annotation_date_node = Literal ( annotation . annotation_date_iso_format ) annotation_triple = ( annotation_node , self . spdx_namespace . annotationDate , annotation_date_node ) self . graph . add ( annotation_triple ) if annotation . has_comment : comment_node = Literal ( annotation . comment ) comment_triple = ( annotation_node , RDFS . comment , comment_node ) self . graph . add ( comment_triple ) annotation_type_node = Literal ( annotation . annotation_type ) annotation_type_triple = ( annotation_node , self . spdx_namespace . annotationType , annotation_type_node ) self . graph . add ( annotation_type_triple ) return annotation_node
2709	def limit_sentences ( path , word_limit = 100 ) : word_count = 0 if isinstance ( path , str ) : path = json_iter ( path ) for meta in path : if not isinstance ( meta , SummarySent ) : p = SummarySent ( ** meta ) else : p = meta sent_text = p . text . strip ( ) . split ( " " ) sent_len = len ( sent_text ) if ( word_count + sent_len ) > word_limit : break else : word_count += sent_len yield sent_text , p . idx
12415	def send ( self , * args , ** kwargs ) : self . write ( * args , ** kwargs ) self . flush ( )
7177	def retype_file ( src , pyi_dir , targets , * , quiet = False , hg = False ) : with tokenize . open ( src ) as src_buffer : src_encoding = src_buffer . encoding src_node = lib2to3_parse ( src_buffer . read ( ) ) try : with open ( ( pyi_dir / src . name ) . with_suffix ( '.pyi' ) ) as pyi_file : pyi_txt = pyi_file . read ( ) except FileNotFoundError : if not quiet : print ( f'warning: .pyi file for source {src} not found in {pyi_dir}' , file = sys . stderr , ) else : pyi_ast = ast3 . parse ( pyi_txt ) assert isinstance ( pyi_ast , ast3 . Module ) reapply_all ( pyi_ast . body , src_node ) fix_remaining_type_comments ( src_node ) targets . mkdir ( parents = True , exist_ok = True ) with open ( targets / src . name , 'w' , encoding = src_encoding ) as target_file : target_file . write ( lib2to3_unparse ( src_node , hg = hg ) ) return targets / src . name
10023	def environment_name_for_cname ( self , env_cname ) : envs = self . get_environments ( ) for env in envs : if env [ 'Status' ] != 'Terminated' and 'CNAME' in env and env [ 'CNAME' ] and env [ 'CNAME' ] . lower ( ) . startswith ( env_cname . lower ( ) + '.' ) : return env [ 'EnvironmentName' ] return None
3743	def ViswanathNatarajan2 ( T , A , B ) : mu = exp ( A + B / T ) mu = mu / 1000. mu = mu * 10 return mu
3894	async def _async_main ( example_coroutine , client , args ) : task = asyncio . ensure_future ( client . connect ( ) ) on_connect = asyncio . Future ( ) client . on_connect . add_observer ( lambda : on_connect . set_result ( None ) ) done , _ = await asyncio . wait ( ( on_connect , task ) , return_when = asyncio . FIRST_COMPLETED ) await asyncio . gather ( * done ) try : await example_coroutine ( client , args ) except asyncio . CancelledError : pass finally : await client . disconnect ( ) await task
12265	def wrap ( f_df , xref , size = 1 ) : memoized_f_df = lrucache ( lambda x : f_df ( restruct ( x , xref ) ) , size ) objective = compose ( first , memoized_f_df ) gradient = compose ( destruct , second , memoized_f_df ) return objective , gradient
11016	def deploy ( context ) : config = context . obj header ( 'Generating HTML...' ) pelican ( config , '--verbose' , production = True ) header ( 'Removing unnecessary output...' ) unnecessary_paths = [ 'author' , 'category' , 'tag' , 'feeds' , 'tags.html' , 'authors.html' , 'categories.html' , 'archives.html' , ] for path in unnecessary_paths : remove_path ( os . path . join ( config [ 'OUTPUT_DIR' ] , path ) ) if os . environ . get ( 'TRAVIS' ) : header ( 'Setting up Git...' ) run ( 'git config user.name ' + run ( 'git show --format="%cN" -s' , capture = True ) ) run ( 'git config user.email ' + run ( 'git show --format="%cE" -s' , capture = True ) ) github_token = os . environ . get ( 'GITHUB_TOKEN' ) repo_slug = os . environ . get ( 'TRAVIS_REPO_SLUG' ) origin = 'https://{}@github.com/{}.git' . format ( github_token , repo_slug ) run ( 'git remote set-url origin ' + origin ) header ( 'Rewriting gh-pages branch...' ) run ( 'ghp-import -m "{message}" {dir}' . format ( message = 'Deploying {}' . format ( choose_commit_emoji ( ) ) , dir = config [ 'OUTPUT_DIR' ] , ) ) header ( 'Pushing to GitHub...' ) run ( 'git push origin gh-pages:gh-pages --force' )
2900	def get_tasks ( self , state = Task . ANY_MASK ) : return [ t for t in Task . Iterator ( self . task_tree , state ) ]
6643	def getExtraIncludes ( self ) : if 'extraIncludes' in self . description : return [ os . path . normpath ( x ) for x in self . description [ 'extraIncludes' ] ] else : return [ ]
4502	def clear ( self ) : self . _desc = { } for key , value in merge . DEFAULT_PROJECT . items ( ) : if key not in self . _HIDDEN : self . _desc [ key ] = type ( value ) ( )
1009	def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print "\n==== PY Iteration: %d =====" % ( self . iterationIdx ) print "Active cols:" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output
1100	def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : r started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line
3828	async def query_presence ( self , query_presence_request ) : response = hangouts_pb2 . QueryPresenceResponse ( ) await self . _pb_request ( 'presence/querypresence' , query_presence_request , response ) return response
93	def _compute_resized_shape ( from_shape , to_shape ) : if is_np_array ( from_shape ) : from_shape = from_shape . shape if is_np_array ( to_shape ) : to_shape = to_shape . shape to_shape_computed = list ( from_shape ) if to_shape is None : pass elif isinstance ( to_shape , tuple ) : do_assert ( len ( from_shape ) in [ 2 , 3 ] ) do_assert ( len ( to_shape ) in [ 2 , 3 ] ) if len ( from_shape ) == 3 and len ( to_shape ) == 3 : do_assert ( from_shape [ 2 ] == to_shape [ 2 ] ) elif len ( to_shape ) == 3 : to_shape_computed . append ( to_shape [ 2 ] ) do_assert ( all ( [ v is None or is_single_number ( v ) for v in to_shape [ 0 : 2 ] ] ) , "Expected the first two entries in to_shape to be None or numbers, " + "got types %s." % ( str ( [ type ( v ) for v in to_shape [ 0 : 2 ] ] ) , ) ) for i , from_shape_i in enumerate ( from_shape [ 0 : 2 ] ) : if to_shape [ i ] is None : to_shape_computed [ i ] = from_shape_i elif is_single_integer ( to_shape [ i ] ) : to_shape_computed [ i ] = to_shape [ i ] else : to_shape_computed [ i ] = int ( np . round ( from_shape_i * to_shape [ i ] ) ) elif is_single_integer ( to_shape ) or is_single_float ( to_shape ) : to_shape_computed = _compute_resized_shape ( from_shape , ( to_shape , to_shape ) ) else : raise Exception ( "Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int " + "or single float, got %s." % ( type ( to_shape ) , ) ) return tuple ( to_shape_computed )
4920	def program_detail ( self , request , pk , program_uuid ) : enterprise_customer_catalog = self . get_object ( ) program = enterprise_customer_catalog . get_program ( program_uuid ) if not program : raise Http404 context = self . get_serializer_context ( ) context [ 'enterprise_customer_catalog' ] = enterprise_customer_catalog serializer = serializers . ProgramDetailSerializer ( program , context = context ) return Response ( serializer . data )
6056	def extracted_array_2d_from_array_2d_and_coordinates ( array_2d , y0 , y1 , x0 , x1 ) : new_shape = ( y1 - y0 , x1 - x0 ) resized_array = np . zeros ( shape = new_shape ) for y_resized , y in enumerate ( range ( y0 , y1 ) ) : for x_resized , x in enumerate ( range ( x0 , x1 ) ) : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] return resized_array
10060	def jsonschemas ( self ) : _jsonschemas = { k : v [ 'jsonschema' ] for k , v in self . app . config [ 'DEPOSIT_RECORDS_UI_ENDPOINTS' ] . items ( ) if 'jsonschema' in v } return defaultdict ( lambda : self . app . config [ 'DEPOSIT_DEFAULT_JSONSCHEMA' ] , _jsonschemas )
5878	def get_video ( self , node ) : video = Video ( ) video . _embed_code = self . get_embed_code ( node ) video . _embed_type = self . get_embed_type ( node ) video . _width = self . get_width ( node ) video . _height = self . get_height ( node ) video . _src = self . get_src ( node ) video . _provider = self . get_provider ( video . src ) return video
7093	def create_widget ( self ) : self . init_options ( ) MapFragment . newInstance ( self . options ) . then ( self . on_map_fragment_created ) self . widget = FrameLayout ( self . get_context ( ) ) self . map = GoogleMap ( __id__ = bridge . generate_id ( ) )
7909	def __presence_available ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_available_presence ( MucPresence ( stanza ) ) return True
5386	def _format_task_name ( job_id , task_id , task_attempt ) : docker_name = '%s.%s' % ( job_id , 'task' if task_id is None else task_id ) if task_attempt is not None : docker_name += '.' + str ( task_attempt ) return 'dsub-{}' . format ( _convert_suffix_to_docker_chars ( docker_name ) )
4589	def serpentine_x ( x , y , matrix ) : if y % 2 : return matrix . columns - 1 - x , y return x , y
2590	def stage_in ( self , file , executor ) : if file . scheme == 'ftp' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _ftp_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'http' or file . scheme == 'https' : working_dir = self . dfk . executors [ executor ] . working_dir stage_in_app = self . _http_stage_in_app ( executor = executor ) app_fut = stage_in_app ( working_dir , outputs = [ file ] ) return app_fut . _outputs [ 0 ] elif file . scheme == 'globus' : globus_ep = self . _get_globus_endpoint ( executor ) stage_in_app = self . _globus_stage_in_app ( ) app_fut = stage_in_app ( globus_ep , outputs = [ file ] ) return app_fut . _outputs [ 0 ] else : raise Exception ( 'Staging in with unknown file scheme {} is not supported' . format ( file . scheme ) )
2657	def notify ( self , event_id ) : self . _event_buffer . extend ( [ event_id ] ) self . _event_count += 1 if self . _event_count >= self . threshold : logger . debug ( "Eventcount >= threshold" ) self . make_callback ( kind = "event" )
12616	def is_img ( obj ) : try : get_data = getattr ( obj , 'get_data' ) get_affine = getattr ( obj , 'get_affine' ) return isinstance ( get_data , collections . Callable ) and isinstance ( get_affine , collections . Callable ) except AttributeError : return False
2441	def add_annotation_date ( self , doc , annotation_date ) : if len ( doc . annotations ) != 0 : if not self . annotation_date_set : self . annotation_date_set = True date = utils . datetime_from_iso_format ( annotation_date ) if date is not None : doc . annotations [ - 1 ] . annotation_date = date return True else : raise SPDXValueError ( 'Annotation::AnnotationDate' ) else : raise CardinalityError ( 'Annotation::AnnotationDate' ) else : raise OrderError ( 'Annotation::AnnotationDate' )
6122	def zoom_region ( self ) : where = np . array ( np . where ( np . invert ( self . astype ( 'bool' ) ) ) ) y0 , x0 = np . amin ( where , axis = 1 ) y1 , x1 = np . amax ( where , axis = 1 ) return [ y0 , y1 + 1 , x0 , x1 + 1 ]
6131	def convert_frames_to_video ( tar_file_path , output_path = "output.mp4" , framerate = 60 , overwrite = False ) : output_path = os . path . abspath ( output_path ) if os . path . isfile ( output_path ) and not overwrite : raise ValueError ( "The output path {:s} already exists. To overwrite that file, you can pass overwrite=True to this function." . format ( output_path ) ) with tempfile . TemporaryDirectory ( ) as tmp_dir : with tarfile . open ( tar_file_path ) as tar : tar . extractall ( tmp_dir ) args = [ "ffmpeg" , "-r" , str ( framerate ) , "-i" , r"%07d.png" , "-vcodec" , "libx264" , "-preset" , "slow" , "-crf" , "18" ] if overwrite : args . append ( "-y" ) args . append ( output_path ) try : subprocess . check_call ( args , cwd = tmp_dir ) except subprocess . CalledProcessError as e : print ( ) raise print ( "Saved output as {:s}" . format ( output_path ) ) return output_path
133	def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered
6756	def param_changed_to ( self , key , to_value , from_value = None ) : last_value = getattr ( self . last_manifest , key ) current_value = self . current_manifest . get ( key ) if from_value is not None : return last_value == from_value and current_value == to_value return last_value != to_value and current_value == to_value
11830	def child_node ( self , problem , action ) : "Fig. 3.10" next = problem . result ( self . state , action ) return Node ( next , self , action , problem . path_cost ( self . path_cost , self . state , action , next ) )
4894	def _collect_grades_data ( self , enterprise_enrollment , course_details ) : if self . grades_api is None : self . grades_api = GradesApiClient ( self . user ) course_id = enterprise_enrollment . course_id username = enterprise_enrollment . enterprise_customer_user . user . username try : grades_data = self . grades_api . get_course_grade ( course_id , username ) except HttpNotFoundError as error : if hasattr ( error , 'content' ) : response_content = json . loads ( error . content ) if response_content . get ( 'error_code' , '' ) == 'user_not_enrolled' : LOGGER . info ( "User [%s] not enrolled in course [%s], enterprise enrollment [%d]" , username , course_id , enterprise_enrollment . pk ) return None , None , None LOGGER . error ( "No grades data found for [%d]: [%s], [%s]" , enterprise_enrollment . pk , course_id , username ) return None , None , None course_end_date = course_details . get ( 'end' ) if course_end_date is not None : course_end_date = parse_datetime ( course_end_date ) now = timezone . now ( ) is_passing = grades_data . get ( 'passed' ) if course_end_date is not None and course_end_date < now : completed_date = course_end_date grade = self . grade_passing if is_passing else self . grade_failing elif is_passing : completed_date = now grade = self . grade_passing else : completed_date = None grade = self . grade_incomplete return completed_date , grade , is_passing
6221	def set_position ( self , x , y , z ) : self . position = Vector3 ( [ x , y , z ] )
2547	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : if annotation_type . endswith ( 'annotationType_other' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'OTHER' return True elif annotation_type . endswith ( 'annotationType_review' ) : self . annotation_type_set = True doc . annotations [ - 1 ] . annotation_type = 'REVIEW' return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
4720	def tcase_enter ( trun , tsuite , tcase ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:enter" ) cij . emph ( "rnr:tcase:enter { fname: %r }" % tcase [ "fname" ] ) cij . emph ( "rnr:tcase:enter { log_fpath: %r }" % tcase [ "log_fpath" ] ) rcode = 0 for hook in tcase [ "hooks" ] [ "enter" ] : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tcase:exit: { rcode: %r }" % rcode , rcode ) return rcode
11822	def is_compatible ( cls , value ) : if not hasattr ( cls , 'value_type' ) : raise NotImplementedError ( 'You must define a `value_type` attribute or override the ' '`is_compatible()` method on `SettingValueModel` subclasses.' ) return isinstance ( value , cls . value_type )
2382	def from_resolver ( cls , spec_resolver ) : spec_validators = cls . _get_spec_validators ( spec_resolver ) return validators . extend ( Draft4Validator , spec_validators )
8759	def get_subnets ( context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker = None , filters = None , fields = None ) : LOG . info ( "get_subnets for tenant %s with filters %s fields %s" % ( context . tenant_id , filters , fields ) ) filters = filters or { } subnets = db_api . subnet_find ( context , limit = limit , page_reverse = page_reverse , sorts = sorts , marker_obj = marker , join_dns = True , join_routes = True , join_pool = True , ** filters ) for subnet in subnets : cache = subnet . get ( "_allocation_pool_cache" ) if not cache : db_api . subnet_update_set_alloc_pool_cache ( context , subnet , subnet . allocation_pools ) return v . _make_subnets_list ( subnets , fields = fields )
8716	def node_heap ( self ) : log . info ( 'Heap' ) res = self . __exchange ( 'print(node.heap())' ) log . info ( res ) return int ( res . split ( '\r\n' ) [ 1 ] )
7070	def precision ( ntp , nfp ) : if ( ntp + nfp ) > 0 : return ntp / ( ntp + nfp ) else : return np . nan
7579	def result_files ( self ) : reps = OPJ ( self . workdir , self . name + "-K-*-rep-*_f" ) repfiles = glob . glob ( reps ) return repfiles
4742	def env_export ( prefix , exported , env ) : for exp in exported : ENV [ "_" . join ( [ prefix , exp ] ) ] = env [ exp ]
9137	def clear_cache ( module_name : str , keep_database : bool = True ) -> None : data_dir = get_data_dir ( module_name ) if not os . path . exists ( data_dir ) : return for name in os . listdir ( data_dir ) : if name in { 'config.ini' , 'cfg.ini' } : continue if name == 'cache.db' and keep_database : continue path = os . path . join ( data_dir , name ) if os . path . isdir ( path ) : shutil . rmtree ( path ) else : os . remove ( path ) os . rmdir ( data_dir )
882	def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )
12652	def generate_config ( output_directory ) : if not op . isdir ( output_directory ) : os . makedirs ( output_directory ) config_file = op . join ( output_directory , "config.ini" ) open_file = open ( config_file , "w" ) open_file . write ( "[BOOL]\nManualNIfTIConv=0\n" ) open_file . close ( ) return config_file
8903	def add_syncable_models ( ) : import django . apps from morango . models import SyncableModel from morango . manager import SyncableModelManager from morango . query import SyncableModelQuerySet model_list = [ ] for model_class in django . apps . apps . get_models ( ) : if issubclass ( model_class , SyncableModel ) : name = model_class . __name__ if _multiple_self_ref_fk_check ( model_class ) : raise InvalidMorangoModelConfiguration ( "Syncing models with more than 1 self referential ForeignKey is not supported." ) try : from mptt import models from morango . utils . morango_mptt import MorangoMPTTModel , MorangoMPTTTreeManager , MorangoTreeQuerySet if issubclass ( model_class , models . MPTTModel ) : if not issubclass ( model_class , MorangoMPTTModel ) : raise InvalidMorangoModelConfiguration ( "{} that inherits from MPTTModel, should instead inherit from MorangoMPTTModel." . format ( name ) ) if not isinstance ( model_class . objects , MorangoMPTTTreeManager ) : raise InvalidMPTTManager ( "Manager for {} must inherit from MorangoMPTTTreeManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , MorangoTreeQuerySet ) : raise InvalidMPTTQuerySet ( "Queryset for {} model must inherit from MorangoTreeQuerySet." . format ( name ) ) except ImportError : pass if not isinstance ( model_class . objects , SyncableModelManager ) : raise InvalidSyncableManager ( "Manager for {} must inherit from SyncableModelManager." . format ( name ) ) if not isinstance ( model_class . objects . none ( ) , SyncableModelQuerySet ) : raise InvalidSyncableQueryset ( "Queryset for {} model must inherit from SyncableModelQuerySet." . format ( name ) ) if model_class . _meta . many_to_many : raise UnsupportedFieldType ( "{} model with a ManyToManyField is not supported in morango." ) if not hasattr ( model_class , 'morango_model_name' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_model_name attribute" . format ( name ) ) if not hasattr ( model_class , 'morango_profile' ) : raise InvalidMorangoModelConfiguration ( "{} model must define a morango_profile attribute" . format ( name ) ) profile = model_class . morango_profile _profile_models [ profile ] = _profile_models . get ( profile , [ ] ) if model_class . morango_model_name is not None : _insert_model_into_profile_dict ( model_class , profile ) for profile , model_list in iteritems ( _profile_models ) : syncable_models_dict = OrderedDict ( ) for model_class in model_list : syncable_models_dict [ model_class . morango_model_name ] = model_class _profile_models [ profile ] = syncable_models_dict
6130	def get ( self , * args , ** kwargs ) : try : req_func = self . session . get if self . session else requests . get req = req_func ( * args , ** kwargs ) req . raise_for_status ( ) self . failed_last = False return req except requests . exceptions . RequestException as e : self . log_error ( e ) for i in range ( 1 , self . num_retries ) : sleep_time = self . retry_rate * i self . log_function ( "Retrying in %s seconds" % sleep_time ) self . _sleep ( sleep_time ) try : req = requests . get ( * args , ** kwargs ) req . raise_for_status ( ) self . log_function ( "New request successful" ) return req except requests . exceptions . RequestException : self . log_function ( "New request failed" ) if not self . failed_last : self . failed_last = True raise ApiError ( e ) else : raise FatalApiError ( e )
6878	def _validate_sqlitecurve_filters ( filterstring , lccolumns ) : stringelems = _squeeze ( filterstring ) . lower ( ) stringelems = filterstring . replace ( '(' , '' ) stringelems = stringelems . replace ( ')' , '' ) stringelems = stringelems . replace ( ',' , '' ) stringelems = stringelems . replace ( "'" , '"' ) stringelems = stringelems . replace ( '\n' , ' ' ) stringelems = stringelems . replace ( '\t' , ' ' ) stringelems = _squeeze ( stringelems ) stringelems = stringelems . split ( ' ' ) stringelems = [ x . strip ( ) for x in stringelems ] stringwords = [ ] for x in stringelems : try : float ( x ) except ValueError as e : stringwords . append ( x ) stringwords2 = [ ] for x in stringwords : if not ( x . startswith ( '"' ) and x . endswith ( '"' ) ) : stringwords2 . append ( x ) stringwords2 = [ x for x in stringwords2 if len ( x ) > 0 ] wordset = set ( stringwords2 ) allowedwords = SQLITE_ALLOWED_WORDS + lccolumns checkset = set ( allowedwords ) validatecheck = list ( wordset - checkset ) if len ( validatecheck ) > 0 : LOGWARNING ( "provided SQL filter string '%s' " "contains non-allowed keywords" % filterstring ) return None else : return filterstring
7664	def to_samples ( self , times , confidence = False ) : times = np . asarray ( times ) if times . ndim != 1 or np . any ( times < 0 ) : raise ParameterError ( 'times must be 1-dimensional and non-negative' ) idx = np . argsort ( times ) samples = times [ idx ] values = [ list ( ) for _ in samples ] confidences = [ list ( ) for _ in samples ] for obs in self . data : start = np . searchsorted ( samples , obs . time ) end = np . searchsorted ( samples , obs . time + obs . duration , side = 'right' ) for i in range ( start , end ) : values [ idx [ i ] ] . append ( obs . value ) confidences [ idx [ i ] ] . append ( obs . confidence ) if confidence : return values , confidences else : return values
7145	def transfer_multiple ( self , destinations , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( destinations , priority , payment_id , unlock_time , account = self . index , relay = relay )
1680	def SetVerboseLevel ( self , level ) : last_verbose_level = self . verbose_level self . verbose_level = level return last_verbose_level
5162	def __intermediate_interface ( self , interface , uci_name ) : interface . update ( { '.type' : 'interface' , '.name' : uci_name , 'ifname' : interface . pop ( 'name' ) } ) if 'network' in interface : del interface [ 'network' ] if 'mac' in interface : if interface . get ( 'type' ) != 'wireless' : interface [ 'macaddr' ] = interface [ 'mac' ] del interface [ 'mac' ] if 'autostart' in interface : interface [ 'auto' ] = interface [ 'autostart' ] del interface [ 'autostart' ] if 'disabled' in interface : interface [ 'enabled' ] = not interface [ 'disabled' ] del interface [ 'disabled' ] if 'wireless' in interface : del interface [ 'wireless' ] if 'addresses' in interface : del interface [ 'addresses' ] return interface
5697	def get_median_lat_lon_of_stops ( gtfs ) : stops = gtfs . get_table ( "stops" ) median_lat = numpy . percentile ( stops [ 'lat' ] . values , 50 ) median_lon = numpy . percentile ( stops [ 'lon' ] . values , 50 ) return median_lat , median_lon
11943	def _get ( self , * args , ** kwargs ) : messages , all_retrieved = super ( StorageMixin , self ) . _get ( * args , ** kwargs ) if self . user . is_authenticated ( ) : inbox_messages = self . backend . inbox_list ( self . user ) else : inbox_messages = [ ] return messages + inbox_messages , all_retrieved
8939	def _zipped ( self , docs_base ) : with pushd ( docs_base ) : with tempfile . NamedTemporaryFile ( prefix = 'pythonhosted-' , delete = False ) as ziphandle : pass zip_name = shutil . make_archive ( ziphandle . name , 'zip' ) notify . info ( "Uploading {:.1f} MiB from '{}' to '{}'..." . format ( os . path . getsize ( zip_name ) / 1024.0 , zip_name , self . target ) ) with io . open ( zip_name , 'rb' ) as zipread : try : yield zipread finally : os . remove ( ziphandle . name ) os . remove ( ziphandle . name + '.zip' )
2508	def get_extr_lics_comment ( self , extr_lics ) : comment_list = list ( self . graph . triples ( ( extr_lics , RDFS . comment , None ) ) ) if len ( comment_list ) > 1 : self . more_than_one_error ( 'extracted license comment' ) return elif len ( comment_list ) == 1 : return comment_list [ 0 ] [ 2 ] else : return
3319	def create ( self , path , lock ) : self . _lock . acquire_write ( ) try : assert lock . get ( "token" ) is None assert lock . get ( "expire" ) is None , "Use timeout instead of expire" assert path and "/" in path org_path = path path = normalize_lock_root ( path ) lock [ "root" ] = path timeout = float ( lock . get ( "timeout" ) ) if timeout is None : timeout = LockStorageDict . LOCK_TIME_OUT_DEFAULT elif timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout validate_lock ( lock ) token = generate_lock_token ( ) lock [ "token" ] = token self . _dict [ token ] = lock key = "URL2TOKEN:{}" . format ( path ) if key not in self . _dict : self . _dict [ key ] = [ token ] else : tokList = self . _dict [ key ] tokList . append ( token ) self . _dict [ key ] = tokList self . _flush ( ) _logger . debug ( "LockStorageDict.set({!r}): {}" . format ( org_path , lock_string ( lock ) ) ) return lock finally : self . _lock . release ( )
13148	def freeze ( self ) : data = super ( IndexBuilder , self ) . freeze ( ) try : base_file_names = data [ 'docnames' ] except KeyError : base_file_names = data [ 'filenames' ] store = { } c = itertools . count ( ) for prefix , items in iteritems ( data [ 'objects' ] ) : for name , ( index , typeindex , _ , shortanchor ) in iteritems ( items ) : objtype = data [ 'objtypes' ] [ typeindex ] if objtype . startswith ( 'cpp:' ) : split = name . rsplit ( '::' , 1 ) if len ( split ) != 2 : warnings . warn ( "What's up with %s?" % str ( ( prefix , name , objtype ) ) ) continue prefix , name = split last_prefix = prefix . split ( '::' ) [ - 1 ] else : last_prefix = prefix . split ( '.' ) [ - 1 ] store [ next ( c ) ] = { 'filename' : base_file_names [ index ] , 'objtype' : objtype , 'prefix' : prefix , 'last_prefix' : last_prefix , 'name' : name , 'shortanchor' : shortanchor , } data . update ( { 'store' : store } ) return data
9556	def _apply_record_checks ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for check , modulus in self . _record_checks : if i % modulus == 0 : rdict = self . _as_dict ( r ) try : check ( rdict ) except RecordError as e : code = e . code if e . code is not None else RECORD_CHECK_FAILED p = { 'code' : code } if not summarize : message = e . message if e . message is not None else MESSAGES [ RECORD_CHECK_FAILED ] p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context if e . details is not None : p [ 'details' ] = e . details yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( check . __name__ , check . __doc__ ) if context is not None : p [ 'context' ] = context yield p
10522	def oneleft ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarhorizontal ( window_name , object_name ) : raise LdtpServerException ( 'Object not horizontal scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
11453	def convert_all ( cls , records ) : out = [ "<collection>" ] for rec in records : conversion = cls ( rec ) out . append ( conversion . convert ( ) ) out . append ( "</collection>" ) return "\n" . join ( out )
8676	def migrate_stash ( source_stash_path , source_passphrase , source_backend , destination_stash_path , destination_passphrase , destination_backend ) : click . echo ( 'Migrating all keys from {0} to {1}...' . format ( source_stash_path , destination_stash_path ) ) try : migrate ( src_path = source_stash_path , src_passphrase = source_passphrase , src_backend = source_backend , dst_path = destination_stash_path , dst_passphrase = destination_passphrase , dst_backend = destination_backend ) except GhostError as ex : sys . exit ( ex ) click . echo ( 'Migration complete!' )
13561	def launch ( title , items , selected = None ) : resp = { "code" : - 1 , "done" : False } curses . wrapper ( Menu , title , items , selected , resp ) return resp
12698	def _parse_control_fields ( self , fields , tag_id = "tag" ) : for field in fields : params = field . params if tag_id not in params : continue self . controlfields [ params [ tag_id ] ] = field . getContent ( ) . strip ( )
3855	async def _sync_all_conversations ( client ) : conv_states = [ ] sync_timestamp = None request = hangouts_pb2 . SyncRecentConversationsRequest ( request_header = client . get_request_header ( ) , max_conversations = CONVERSATIONS_PER_REQUEST , max_events_per_conversation = 1 , sync_filter = [ hangouts_pb2 . SYNC_FILTER_INBOX , hangouts_pb2 . SYNC_FILTER_ARCHIVED , ] ) for _ in range ( MAX_CONVERSATION_PAGES ) : logger . info ( 'Requesting conversations page %s' , request . last_event_timestamp ) response = await client . sync_recent_conversations ( request ) conv_states = list ( response . conversation_state ) + conv_states sync_timestamp = parsers . from_timestamp ( response . response_header . current_server_time ) if response . continuation_end_timestamp == 0 : logger . info ( 'Reached final conversations page' ) break else : request . last_event_timestamp = response . continuation_end_timestamp else : logger . warning ( 'Exceeded maximum number of conversation pages' ) logger . info ( 'Synced %s total conversations' , len ( conv_states ) ) return conv_states , sync_timestamp
906	def anomalyProbability ( self , value , anomalyScore , timestamp = None ) : if timestamp is None : timestamp = self . _iteration dataPoint = ( timestamp , value , anomalyScore ) if self . _iteration < self . _probationaryPeriod : likelihood = 0.5 else : if ( ( self . _distribution is None ) or ( self . _iteration % self . _reestimationPeriod == 0 ) ) : numSkipRecords = self . _calcSkipRecords ( numIngested = self . _iteration , windowSize = self . _historicalScores . maxlen , learningPeriod = self . _learningPeriod ) _ , _ , self . _distribution = estimateAnomalyLikelihoods ( self . _historicalScores , skipRecords = numSkipRecords ) likelihoods , _ , self . _distribution = updateAnomalyLikelihoods ( [ dataPoint ] , self . _distribution ) likelihood = 1.0 - likelihoods [ 0 ] self . _historicalScores . append ( dataPoint ) self . _iteration += 1 return likelihood
9594	def execute_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
8504	def get_key ( self ) : if not isinstance ( self . key , Unparseable ) : return self . key line = self . source [ self . col_offset : ] regex = re . compile ( ) match = regex . match ( line ) if not match : return Unparseable ( ) return "<%s>" % match . group ( 1 )
5496	def from_file ( cls , file , * args , ** kwargs ) : try : cache = shelve . open ( file ) return cls ( file , cache , * args , ** kwargs ) except OSError as e : logger . debug ( "Loading {0} failed" . format ( file ) ) raise e
1490	def tail ( filename , n ) : size = os . path . getsize ( filename ) with open ( filename , "rb" ) as f : fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) try : for i in xrange ( size - 1 , - 1 , - 1 ) : if fm [ i ] == '\n' : n -= 1 if n == - 1 : break return fm [ i + 1 if i else 0 : ] . splitlines ( ) finally : fm . close ( )
12767	def forces ( self , dx_tm1 = None ) : cfm = self . cfms [ self . _frame_no ] [ : , None ] kp = self . erp / ( cfm * self . world . dt ) kd = ( 1 - self . erp ) / cfm dx = self . distances ( ) F = kp * dx if dx_tm1 is not None : bad = np . isnan ( dx ) | np . isnan ( dx_tm1 ) F [ ~ bad ] += ( kd * ( dx - dx_tm1 ) / self . world . dt ) [ ~ bad ] return F
7567	def splitalleles ( consensus ) : allele1 = list ( consensus ) allele2 = list ( consensus ) hidx = [ i for ( i , j ) in enumerate ( consensus ) if j in "RKSWYMrkswym" ] for idx in hidx : hsite = consensus [ idx ] if hsite . isupper ( ) : allele1 [ idx ] = PRIORITY [ hsite ] allele2 [ idx ] = MINOR [ hsite ] else : allele1 [ idx ] = MINOR [ hsite . upper ( ) ] allele2 [ idx ] = PRIORITY [ hsite . upper ( ) ] allele1 = "" . join ( allele1 ) allele2 = "" . join ( allele2 ) return allele1 , allele2
2007	def _deserialize_uint ( data , nbytes = 32 , padding = 0 , offset = 0 ) : assert isinstance ( data , ( bytearray , Array ) ) value = ABI . _readBE ( data , nbytes , padding = True , offset = offset ) value = Operators . ZEXTEND ( value , ( nbytes + padding ) * 8 ) return value
3627	def normalize_cols ( table ) : longest_row_len = max ( [ len ( row ) for row in table ] ) for row in table : while len ( row ) < longest_row_len : row . append ( '' ) return table
737	def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] with FileRecordStream ( outputFile , write = True , fields = fields ) as o : files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] while not all ( r is None for r in records ) : indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] r = min ( records , key = itemgetter ( * key ) ) o . appendRecord ( r ) index = records . index ( r ) records [ index ] = files [ index ] . getNextRecord ( ) for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )
9709	def heappop_max ( heap ) : lastelt = heap . pop ( ) if heap : returnitem = heap [ 0 ] heap [ 0 ] = lastelt _siftup_max ( heap , 0 ) return returnitem return lastelt
5623	def path_exists ( path ) : if path . startswith ( ( "http://" , "https://" ) ) : try : urlopen ( path ) . info ( ) return True except HTTPError as e : if e . code == 404 : return False else : raise elif path . startswith ( "s3://" ) : bucket = get_boto3_bucket ( path . split ( "/" ) [ 2 ] ) key = "/" . join ( path . split ( "/" ) [ 3 : ] ) for obj in bucket . objects . filter ( Prefix = key ) : if obj . key == key : return True else : return False else : logger . debug ( "%s exists: %s" , path , os . path . exists ( path ) ) return os . path . exists ( path )
9674	def get_days_span ( self , month_index ) : is_first_month = month_index == 0 is_last_month = month_index == self . __len__ ( ) - 1 y = int ( self . start_date . year + ( self . start_date . month + month_index ) / 13 ) m = int ( ( self . start_date . month + month_index ) % 12 or 12 ) total = calendar . monthrange ( y , m ) [ 1 ] if is_first_month and is_last_month : return ( self . end_date - self . start_date ) . days + 1 else : if is_first_month : return total - self . start_date . day + 1 elif is_last_month : return self . end_date . day else : return total
11796	def min_conflicts_value ( csp , var , current ) : return argmin_random_tie ( csp . domains [ var ] , lambda val : csp . nconflicts ( var , val , current ) )
236	def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax
10577	def get_assay ( self ) : masses_sum = sum ( self . compound_masses ) return [ m / masses_sum for m in self . compound_masses ]
6910	def generate_transit_lightcurve ( times , mags = None , errs = None , paramdists = { 'transitperiod' : sps . uniform ( loc = 0.1 , scale = 49.9 ) , 'transitdepth' : sps . uniform ( loc = 1.0e-4 , scale = 2.0e-2 ) , 'transitduration' : sps . uniform ( loc = 0.01 , scale = 0.29 ) } , magsarefluxes = False , ) : if mags is None : mags = np . full_like ( times , 0.0 ) if errs is None : errs = np . full_like ( times , 0.0 ) epoch = npr . random ( ) * ( times . max ( ) - times . min ( ) ) + times . min ( ) period = paramdists [ 'transitperiod' ] . rvs ( size = 1 ) depth = paramdists [ 'transitdepth' ] . rvs ( size = 1 ) duration = paramdists [ 'transitduration' ] . rvs ( size = 1 ) ingduration = npr . random ( ) * ( 0.5 * duration - 0.05 * duration ) + 0.05 * duration if magsarefluxes and depth < 0.0 : depth = - depth elif not magsarefluxes and depth > 0.0 : depth = - depth modelmags , phase , ptimes , pmags , perrs = ( transits . trapezoid_transit_func ( [ period , epoch , depth , duration , ingduration ] , times , mags , errs ) ) timeind = np . argsort ( ptimes ) mtimes = ptimes [ timeind ] mmags = modelmags [ timeind ] merrs = perrs [ timeind ] modeldict = { 'vartype' : 'planet' , 'params' : { x : np . asscalar ( y ) for x , y in zip ( [ 'transitperiod' , 'transitepoch' , 'transitdepth' , 'transitduration' , 'ingressduration' ] , [ period , epoch , depth , duration , ingduration ] ) } , 'times' : mtimes , 'mags' : mmags , 'errs' : merrs , 'varperiod' : period , 'varamplitude' : depth } return modeldict
5788	def _bcrypt_encrypt ( cipher , key , data , iv , padding ) : key_handle = None try : key_handle = _bcrypt_create_key_handle ( cipher , key ) if iv is None : iv_len = 0 else : iv_len = len ( iv ) flags = 0 if padding is True : flags = BcryptConst . BCRYPT_BLOCK_PADDING out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) iv_buffer = buffer_from_bytes ( iv ) if iv else null ( ) res = bcrypt . BCryptEncrypt ( key_handle , data , len ( data ) , null ( ) , iv_buffer , iv_len , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) finally : if key_handle : bcrypt . BCryptDestroyKey ( key_handle )
10793	def separate_particles_into_groups ( s , region_size = 40 , bounds = None ) : imtile = ( s . oshape . translate ( - s . pad ) if bounds is None else util . Tile ( bounds [ 0 ] , bounds [ 1 ] ) ) region = util . Tile ( region_size , dim = s . dim ) trange = np . ceil ( imtile . shape . astype ( 'float' ) / region . shape ) translations = util . Tile ( trange ) . coords ( form = 'vector' ) translations = translations . reshape ( - 1 , translations . shape [ - 1 ] ) groups = [ ] positions = s . obj_get_positions ( ) for v in translations : tmptile = region . copy ( ) . translate ( region . shape * v - s . pad ) groups . append ( find_particles_in_tile ( positions , tmptile ) ) return [ g for g in groups if len ( g ) > 0 ]
516	def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )
9466	def conference_record_stop ( self , call_params ) : path = '/' + self . api_version + '/ConferenceRecordStop/' method = 'POST' return self . request ( path , method , call_params )
12067	def annotate ( abf ) : msg = "SWHLab %s " % str ( swhlab . VERSION ) msg += "ID:%s " % abf . ID msg += "CH:%d " % abf . channel msg += "PROTOCOL:%s " % abf . protoComment msg += "COMMAND: %d%s " % ( abf . holding , abf . units ) msg += "GENERATED:%s " % '{0:%Y-%m-%d %H:%M:%S}' . format ( datetime . datetime . now ( ) ) pylab . annotate ( msg , ( .001 , .001 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'bottom' , color = '#999999' , family = 'monospace' , size = 8 , weight = 'bold' ) if abf . nADC > 1 : msg = "Ch %d/%d" % ( abf . channel + 1 , abf . nADC ) pylab . annotate ( msg , ( .01 , .99 ) , xycoords = 'figure fraction' , ha = 'left' , va = 'top' , color = '#FF0000' , family = 'monospace' , size = 12 , weight = 'bold' )
2045	def set_storage_data ( self , storage_address , offset , value ) : self . _world_state [ storage_address ] [ 'storage' ] [ offset ] = value
8695	def __set_baudrate ( self , baud ) : log . info ( 'Changing communication to %s baud' , baud ) self . __writeln ( UART_SETUP . format ( baud = baud ) ) time . sleep ( 0.1 ) try : self . _port . setBaudrate ( baud ) except AttributeError : self . _port . baudrate = baud
8507	def _get_param_names ( self ) : template = Template ( self . yaml_string ) names = [ 'yaml_string' ] for match in re . finditer ( template . pattern , template . template ) : name = match . group ( 'named' ) or match . group ( 'braced' ) assert name is not None names . append ( name ) return names
1495	def find_closing_braces ( self , query ) : if query [ 0 ] != '(' : raise Exception ( "Trying to find closing braces for no opening braces" ) num_open_braces = 0 for i in range ( len ( query ) ) : c = query [ i ] if c == '(' : num_open_braces += 1 elif c == ')' : num_open_braces -= 1 if num_open_braces == 0 : return i raise Exception ( "No closing braces found" )
7439	def _build_stat ( self , idx ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) newdat = pd . DataFrame ( [ self . samples [ i ] . stats_dfs [ idx ] for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' ) return newdat
9050	def bernoulli_sample ( offset , G , heritability = 0.5 , causal_variants = None , causal_variance = 0 , random_state = None , ) : r link = LogitLink ( ) mean , cov = _mean_cov ( offset , G , heritability , causal_variants , causal_variance , random_state ) lik = BernoulliProdLik ( link ) sampler = GGPSampler ( lik , mean , cov ) return sampler . sample ( random_state )
4141	def _arburg2 ( X , order ) : x = np . array ( X ) N = len ( x ) if order <= 0. : raise ValueError ( "order must be > 0" ) rho = sum ( abs ( x ) ** 2. ) / N den = rho * 2. * N ef = np . zeros ( N , dtype = complex ) eb = np . zeros ( N , dtype = complex ) for j in range ( 0 , N ) : ef [ j ] = x [ j ] eb [ j ] = x [ j ] a = np . zeros ( 1 , dtype = complex ) a [ 0 ] = 1 ref = np . zeros ( order , dtype = complex ) temp = 1. E = np . zeros ( order + 1 ) E [ 0 ] = rho for m in range ( 0 , order ) : efp = ef [ 1 : ] ebp = eb [ 0 : - 1 ] num = - 2. * np . dot ( ebp . conj ( ) . transpose ( ) , efp ) den = np . dot ( efp . conj ( ) . transpose ( ) , efp ) den += np . dot ( ebp , ebp . conj ( ) . transpose ( ) ) ref [ m ] = num / den ef = efp + ref [ m ] * ebp eb = ebp + ref [ m ] . conj ( ) . transpose ( ) * efp a . resize ( len ( a ) + 1 ) a = a + ref [ m ] * np . flipud ( a ) . conjugate ( ) E [ m + 1 ] = ( 1 - ref [ m ] . conj ( ) . transpose ( ) * ref [ m ] ) * E [ m ] return a , E [ - 1 ] , ref
663	def getCentreAndSpreadOffsets ( spaceShape , spreadShape , stepSize = 1 ) : from nupic . math . cross import cross shape = spaceShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : centerOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) centerOffsets = list ( cross ( yPositions , xPositions ) ) numCenterOffsets = len ( centerOffsets ) print "centerOffsets:" , centerOffsets shape = spreadShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : spreadOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) spreadOffsets = list ( cross ( yPositions , xPositions ) ) spreadOffsets . remove ( ( 0 , 0 ) ) spreadOffsets . insert ( 0 , ( 0 , 0 ) ) numSpreadOffsets = len ( spreadOffsets ) print "spreadOffsets:" , spreadOffsets return centerOffsets , spreadOffsets
11264	def stdout ( prev , endl = '\n' , thru = False ) : for i in prev : sys . stdout . write ( str ( i ) + endl ) if thru : yield i
7649	def _open ( name_or_fdesc , mode = 'r' , fmt = 'auto' ) : open_map = { 'jams' : open , 'json' : open , 'jamz' : gzip . open , 'gz' : gzip . open } if hasattr ( name_or_fdesc , 'read' ) or hasattr ( name_or_fdesc , 'write' ) : yield name_or_fdesc elif isinstance ( name_or_fdesc , six . string_types ) : if fmt == 'auto' : _ , ext = os . path . splitext ( name_or_fdesc ) ext = ext [ 1 : ] else : ext = fmt try : ext = ext . lower ( ) if ext in [ 'jamz' , 'gz' ] and 't' not in mode : mode = '{:s}t' . format ( mode ) with open_map [ ext ] ( name_or_fdesc , mode = mode ) as fdesc : yield fdesc except KeyError : raise ParameterError ( 'Unknown JAMS extension ' 'format: "{:s}"' . format ( ext ) ) else : raise ParameterError ( 'Invalid filename or ' 'descriptor: {}' . format ( name_or_fdesc ) )
8283	def _curvepoint ( self , t , x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , handles = False ) : mint = 1 - t x01 = x0 * mint + x1 * t y01 = y0 * mint + y1 * t x12 = x1 * mint + x2 * t y12 = y1 * mint + y2 * t x23 = x2 * mint + x3 * t y23 = y2 * mint + y3 * t out_c1x = x01 * mint + x12 * t out_c1y = y01 * mint + y12 * t out_c2x = x12 * mint + x23 * t out_c2y = y12 * mint + y23 * t out_x = out_c1x * mint + out_c2x * t out_y = out_c1y * mint + out_c2y * t if not handles : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y ) else : return ( out_x , out_y , out_c1x , out_c1y , out_c2x , out_c2y , x01 , y01 , x23 , y23 )
3294	def is_locked ( self ) : if self . provider . lock_manager is None : return False return self . provider . lock_manager . is_url_locked ( self . get_ref_url ( ) )
6063	def mass_within_ellipse_in_units ( self , major_axis , unit_mass = 'angular' , kpc_per_arcsec = None , critical_surface_density = None ) : self . check_units_of_radius_and_critical_surface_density ( radius = major_axis , critical_surface_density = critical_surface_density ) profile = self . new_profile_with_units_converted ( unit_length = major_axis . unit_length , unit_mass = 'angular' , kpc_per_arcsec = kpc_per_arcsec , critical_surface_density = critical_surface_density ) mass_angular = dim . Mass ( value = quad ( profile . mass_integral , a = 0.0 , b = major_axis , args = ( self . axis_ratio , ) ) [ 0 ] , unit_mass = 'angular' ) return mass_angular . convert ( unit_mass = unit_mass , critical_surface_density = critical_surface_density )
12373	def get_first ( ) : client = po . connect ( ) all_droplets = client . droplets . list ( ) id = all_droplets [ 0 ] [ 'id' ] return client . droplets . get ( id )
9874	def aggregate ( l ) : tree = radix . Radix ( ) for item in l : try : tree . add ( item ) except ( ValueError ) as err : raise Exception ( "ERROR: invalid IP prefix: {}" . format ( item ) ) return aggregate_tree ( tree ) . prefixes ( )
7790	def update_item ( self , item ) : self . _lock . acquire ( ) try : state = item . update_state ( ) self . _items_list . sort ( ) if item . state == 'purged' : self . _purged += 1 if self . _purged > 0.25 * self . max_items : self . purge_items ( ) return state finally : self . _lock . release ( )
9847	def _load_cpp4 ( self , filename ) : ccp4 = CCP4 . CCP4 ( ) ccp4 . read ( filename ) grid , edges = ccp4 . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
12580	def to_file ( self , outpath ) : if not self . has_mask ( ) and not self . is_smoothed ( ) : save_niigz ( outpath , self . img ) else : save_niigz ( outpath , self . get_data ( masked = True , smoothed = True ) , self . get_header ( ) , self . get_affine ( ) )
11808	def samples ( self , nwords ) : n = self . n nminus1gram = ( '' , ) * ( n - 1 ) output = [ ] for i in range ( nwords ) : if nminus1gram not in self . cond_prob : nminus1gram = ( '' , ) * ( n - 1 ) wn = self . cond_prob [ nminus1gram ] . sample ( ) output . append ( wn ) nminus1gram = nminus1gram [ 1 : ] + ( wn , ) return ' ' . join ( output )
3485	def _create_parameter ( model , pid , value , sbo = None , constant = True , units = None , flux_udef = None ) : parameter = model . createParameter ( ) parameter . setId ( pid ) parameter . setValue ( value ) parameter . setConstant ( constant ) if sbo : parameter . setSBOTerm ( sbo ) if units : parameter . setUnits ( flux_udef . getId ( ) )
10625	def _calculate_Hfr_coal ( self , T ) : m_C = 0 m_H = 0 m_O = 0 m_N = 0 m_S = 0 Hfr = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) formula = compound . split ( '[' ) [ 0 ] if stoich . element_mass_fraction ( formula , 'C' ) == 1.0 : m_C += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'H' ) == 1.0 : m_H += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'O' ) == 1.0 : m_O += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'N' ) == 1.0 : m_N += self . _compound_mfrs [ index ] elif stoich . element_mass_fraction ( formula , 'S' ) == 1.0 : m_S += self . _compound_mfrs [ index ] else : dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr += dHfr m_total = m_C + m_H + m_O + m_N + m_S y_C = m_C / m_total y_H = m_H / m_total y_O = m_O / m_total y_N = m_N / m_total y_S = m_S / m_total hmodel = coals . DafHTy ( ) H = hmodel . calculate ( T = T + 273.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 H298 = hmodel . calculate ( T = 298.15 , y_C = y_C , y_H = y_H , y_O = y_O , y_N = y_N , y_S = y_S ) / 3.6e6 Hdaf = H - H298 + self . _DH298 Hdaf *= m_total Hfr += Hdaf return Hfr
11938	def add_message_for ( users , level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : BackendClass = stored_messages_settings . STORAGE_BACKEND backend = BackendClass ( ) m = backend . create_message ( level , message_text , extra_tags , date , url ) backend . archive_store ( users , m ) backend . inbox_store ( users , m )
7311	def is_valid_ip ( ip_address ) : try : ip = ipaddress . ip_address ( u'' + ip_address ) return True except ValueError as e : return False
2093	def lookup_stdout ( self , pk = None , start_line = None , end_line = None , full = True ) : stdout_url = '%s%s/stdout/' % ( self . unified_job_type , pk ) payload = { 'format' : 'json' , 'content_encoding' : 'base64' , 'content_format' : 'ansi' } if start_line : payload [ 'start_line' ] = start_line if end_line : payload [ 'end_line' ] = end_line debug . log ( 'Requesting a copy of job standard output' , header = 'details' ) resp = client . get ( stdout_url , params = payload ) . json ( ) content = b64decode ( resp [ 'content' ] ) return content . decode ( 'utf-8' , 'replace' )
3519	def matomo ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return MatomoNode ( )
808	def handleLogOutput ( self , output ) : if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut
4527	def report ( function , * args , ** kwds ) : try : function ( * args , ** kwds ) except Exception : traceback . print_exc ( )
949	def _createPeriodicActivities ( self ) : periodicActivities = [ ] class MetricsReportCb ( object ) : def __init__ ( self , taskRunner ) : self . __taskRunner = taskRunner return def __call__ ( self ) : self . __taskRunner . _getAndEmitExperimentMetrics ( ) reportMetrics = PeriodicActivityRequest ( repeating = True , period = 1000 , cb = MetricsReportCb ( self ) ) periodicActivities . append ( reportMetrics ) class IterationProgressCb ( object ) : PROGRESS_UPDATE_PERIOD_TICKS = 1000 def __init__ ( self , taskLabel , requestedIterationCount , logger ) : self . __taskLabel = taskLabel self . __requestedIterationCount = requestedIterationCount self . __logger = logger self . __numIterationsSoFar = 0 def __call__ ( self ) : self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS self . __logger . debug ( "%s: ITERATION PROGRESS: %s of %s" % ( self . __taskLabel , self . __numIterationsSoFar , self . __requestedIterationCount ) ) iterationProgressCb = IterationProgressCb ( taskLabel = self . __task [ 'taskLabel' ] , requestedIterationCount = self . __task [ 'iterationCount' ] , logger = self . __logger ) iterationProgressReporter = PeriodicActivityRequest ( repeating = True , period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , cb = iterationProgressCb ) periodicActivities . append ( iterationProgressReporter ) return periodicActivities
2284	def predict ( self , a , b , ** kwargs ) : binning_alg = kwargs . get ( 'bins' , 'fd' ) return metrics . adjusted_mutual_info_score ( bin_variable ( a , bins = binning_alg ) , bin_variable ( b , bins = binning_alg ) )
11609	def multiply ( self , multiplier , axis = None ) : if self . finalized : if multiplier . ndim == 1 : if axis == 0 : raise NotImplementedError ( 'The method is not yet implemented for the axis.' ) elif axis == 1 : sz = len ( multiplier ) multiplier_mat = lil_matrix ( ( sz , sz ) ) multiplier_mat . setdiag ( multiplier ) for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] * multiplier_mat elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices ] else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif multiplier . ndim == 2 : if axis == 0 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] . data *= multiplier [ self . data [ hid ] . indices , hid ] elif axis == 1 : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier ) elif axis == 2 : for hid in xrange ( self . shape [ 1 ] ) : multiplier_vec = multiplier [ hid , : ] multiplier_vec = multiplier_vec . ravel ( ) self . data [ hid ] . data *= multiplier_vec . repeat ( np . diff ( self . data [ hid ] . indptr ) ) else : raise RuntimeError ( 'The axis should be 0, 1, or 2.' ) elif isinstance ( multiplier , Sparse3DMatrix ) : for hid in xrange ( self . shape [ 1 ] ) : self . data [ hid ] = self . data [ hid ] . multiply ( multiplier . data [ hid ] ) else : raise RuntimeError ( 'The multiplier should be 1, 2 dimensional numpy array or a Sparse3DMatrix object.' ) else : raise RuntimeError ( 'The original matrix must be finalized.' )
7181	def fix_remaining_type_comments ( node ) : assert node . type == syms . file_input last_n = None for n in node . post_order ( ) : if last_n is not None : if n . type == token . NEWLINE and is_assignment ( last_n ) : fix_variable_annotation_type_comment ( n , last_n ) elif n . type == syms . funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 1 ) elif n . type == syms . async_funcdef and last_n . type == syms . suite : fix_signature_annotation_type_comment ( n , last_n , offset = 2 ) last_n = n
6750	def register ( self ) : self . _set_defaults ( ) all_satchels [ self . name . upper ( ) ] = self manifest_recorder [ self . name ] = self . record_manifest if self . required_system_packages : required_system_packages [ self . name . upper ( ) ] = self . required_system_packages
11167	def keys ( self ) : return self . options . keys ( ) + [ p . name for p in self . positional_args ]
10474	def _isSingleCharacter ( keychr ) : if not keychr : return False if len ( keychr ) == 1 : return True return keychr . count ( '<' ) == 1 and keychr . count ( '>' ) == 1 and keychr [ 0 ] == '<' and keychr [ - 1 ] == '>'
2717	def add_droplets ( self , droplet ) : droplets = droplet if not isinstance ( droplets , list ) : droplets = [ droplet ] resources = self . __extract_resources_from_droplets ( droplets ) if len ( resources ) > 0 : return self . __add_resources ( resources ) return False
10571	def template_to_filepath ( template , metadata , template_patterns = None ) : if template_patterns is None : template_patterns = TEMPLATE_PATTERNS metadata = metadata if isinstance ( metadata , dict ) else _mutagen_fields_to_single_value ( metadata ) assert isinstance ( metadata , dict ) suggested_filename = get_suggested_filename ( metadata ) . replace ( '.mp3' , '' ) if template == os . getcwd ( ) or template == '%suggested%' : filepath = suggested_filename else : t = template . replace ( '%suggested%' , suggested_filename ) filepath = _replace_template_patterns ( t , metadata , template_patterns ) return filepath
6202	def merge_da ( ts_d , ts_par_d , ts_a , ts_par_a ) : ts = np . hstack ( [ ts_d , ts_a ] ) ts_par = np . hstack ( [ ts_par_d , ts_par_a ] ) a_ch = np . hstack ( [ np . zeros ( ts_d . shape [ 0 ] , dtype = bool ) , np . ones ( ts_a . shape [ 0 ] , dtype = bool ) ] ) index_sort = ts . argsort ( ) return ts [ index_sort ] , a_ch [ index_sort ] , ts_par [ index_sort ]
8708	def __got_ack ( self ) : log . debug ( 'waiting for ack' ) res = self . _port . read ( 1 ) log . debug ( 'ack read %s' , hexify ( res ) ) return res == ACK
12719	def axes ( self ) : return [ np . array ( self . ode_obj . getAxis ( i ) ) for i in range ( self . ADOF or self . LDOF ) ]
11458	def keep_only_fields ( self ) : for tag in self . record . keys ( ) : if tag not in self . fields_list : record_delete_fields ( self . record , tag )
9031	def _expand_produced_mesh ( self , mesh , mesh_index , row_position , passed ) : if not mesh . is_consumed ( ) : return row = mesh . consuming_row position = Point ( row_position . x - mesh . index_in_consuming_row + mesh_index , row_position . y + INSTRUCTION_HEIGHT ) self . _expand ( row , position , passed )
10256	def get_causal_central_nodes ( graph : BELGraph , func : str ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_central ( graph , node ) }
7846	def add_item ( self , jid , node = None , name = None , action = None ) : return DiscoItem ( self , jid , node , name , action )
10755	def iso_name_increment ( name , is_dir = False , max_length = 8 ) : if not is_dir and '.' in name : name , ext = name . rsplit ( '.' ) ext = '.{}' . format ( ext ) else : ext = '' for position , char in reversed ( list ( enumerate ( name ) ) ) : if char not in string . digits : break base , tag = name [ : position + 1 ] , name [ position + 1 : ] tag = str ( int ( tag or 0 ) + 1 ) if len ( tag ) + len ( base ) > max_length : base = base [ : max_length - len ( tag ) ] return '' . join ( [ base , tag , ext ] )
790	def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' ' _eng_last_update_time=UTC_TIMESTAMP() ' ' WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( "Tried to change the status of job %d to %s, but " "this job belongs to some other CJM" % ( jobID , status ) )
13840	def ConsumeBool ( self ) : try : result = ParseBool ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
11855	def predictor ( self , ( i , j , A , alpha , Bb ) ) : "Add to chart any rules for B that could help extend this edge." B = Bb [ 0 ] if B in self . grammar . rules : for rhs in self . grammar . rewrites_for ( B ) : self . add_edge ( [ j , j , B , [ ] , rhs ] )
1443	def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . EXEC_COUNT , key = stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . EXEC_COUNT , key = global_stream_id ) self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
7594	def get_player ( self , * tags : crtag , ** params : keys ) : url = self . api . PLAYER + '/' + ',' . join ( tags ) return self . _get_model ( url , FullPlayer , ** params )
3477	def _dissociate_gene ( self , cobra_gene ) : self . _genes . discard ( cobra_gene ) cobra_gene . _reaction . discard ( self )
6879	def _smartcast ( castee , caster , subval = None ) : try : return caster ( castee ) except Exception as e : if caster is float or caster is int : return nan elif caster is str : return '' else : return subval
9789	def projects ( ctx , page ) : user = get_username_or_local ( ctx . obj . get ( 'username' ) ) page = page or 1 try : response = PolyaxonClient ( ) . bookmark . projects ( username = user , page = page ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) meta = get_meta_response ( response ) if meta : Printer . print_header ( 'Bookmarked projects for user `{}`.' . format ( user ) ) Printer . print_header ( 'Navigation:' ) dict_tabulate ( meta ) else : Printer . print_header ( 'No bookmarked projects found for user `{}`.' . format ( user ) ) objects = [ Printer . add_status_color ( o . to_light_dict ( humanize_values = True ) ) for o in response [ 'results' ] ] objects = list_dicts_to_tabulate ( objects ) if objects : Printer . print_header ( "Projects:" ) dict_tabulate ( objects , is_list_dict = True )
10689	def _create_air ( ) : name = "Air" namel = name . lower ( ) mm = 28.9645 ds_dict = _create_ds_dict ( [ "dataset-air-lienhard2015" , "dataset-air-lienhard2018" ] ) active_ds = "dataset-air-lienhard2018" model_dict = { "rho" : IgRhoT ( mm , 101325.0 ) , "beta" : IgBetaT ( ) } model_type = "polynomialmodelt" for property in [ "Cp" , "mu" , "k" ] : name = f"data/{namel}-{property.lower()}-{model_type}-{active_ds}.json" model_dict [ property ] = PolynomialModelT . read ( _path ( name ) ) material = Material ( name , StateOfMatter . gas , model_dict ) return material , ds_dict
1428	def build_extra_args_dict ( cl_args ) : component_parallelism = cl_args [ 'component_parallelism' ] runtime_configs = cl_args [ 'runtime_config' ] container_number = cl_args [ 'container_number' ] if ( component_parallelism and runtime_configs ) or ( container_number and runtime_configs ) : raise Exception ( "(component-parallelism or container_num) and runtime-config " + "can't be updated at the same time" ) dict_extra_args = { } nothing_set = True if component_parallelism : dict_extra_args . update ( { 'component_parallelism' : component_parallelism } ) nothing_set = False if container_number : dict_extra_args . update ( { 'container_number' : container_number } ) nothing_set = False if runtime_configs : dict_extra_args . update ( { 'runtime_config' : runtime_configs } ) nothing_set = False if nothing_set : raise Exception ( "Missing arguments --component-parallelism or --runtime-config or --container-number" ) if cl_args [ 'dry_run' ] : dict_extra_args . update ( { 'dry_run' : True } ) if 'dry_run_format' in cl_args : dict_extra_args . update ( { 'dry_run_format' : cl_args [ "dry_run_format" ] } ) return dict_extra_args
5495	def make_aware ( dt ) : return dt if dt . tzinfo else dt . replace ( tzinfo = timezone . utc )
8343	def decompose ( self ) : contents = [ i for i in self . contents ] for i in contents : if isinstance ( i , Tag ) : i . decompose ( ) else : i . extract ( ) self . extract ( )
7513	def locichunk ( args ) : data , optim , pnames , snppad , smask , start , samplecov , locuscov , upper = args hslice = [ start , start + optim ] co5 = h5py . File ( data . database , 'r' ) afilt = co5 [ "filters" ] [ hslice [ 0 ] : hslice [ 1 ] , ] aedge = co5 [ "edges" ] [ hslice [ 0 ] : hslice [ 1 ] , ] asnps = co5 [ "snps" ] [ hslice [ 0 ] : hslice [ 1 ] , ] io5 = h5py . File ( data . clust_database , 'r' ) if upper : aseqs = np . char . upper ( io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] ) else : aseqs = io5 [ "seqs" ] [ hslice [ 0 ] : hslice [ 1 ] , ] keep = np . where ( np . sum ( afilt , axis = 1 ) == 0 ) [ 0 ] store = [ ] for iloc in keep : edg = aedge [ iloc ] args = [ iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ] if edg [ 4 ] : outstr , samplecov , locuscov = enter_pairs ( * args ) store . append ( outstr ) else : outstr , samplecov , locuscov = enter_singles ( * args ) store . append ( outstr ) tmpo = os . path . join ( data . dirs . outfiles , data . name + ".loci.{}" . format ( start ) ) with open ( tmpo , 'w' ) as tmpout : tmpout . write ( "\n" . join ( store ) + "\n" ) io5 . close ( ) co5 . close ( ) return samplecov , locuscov , start
8450	def has_env_vars ( * env_vars ) : for env_var in env_vars : if not os . environ . get ( env_var ) : msg = ( 'Must set {} environment variable. View docs for setting up environment at {}' ) . format ( env_var , temple . constants . TEMPLE_DOCS_URL ) raise temple . exceptions . InvalidEnvironmentError ( msg )
9698	def deliveries ( self ) : key = make_key ( event = self . object . event , owner_name = self . object . owner . username , identifier = self . object . identifier ) return redis . lrange ( key , 0 , 20 )
5096	def refresh_persistent_maps ( self ) : for robot in self . _robots : resp2 = ( requests . get ( urljoin ( self . ENDPOINT , 'users/me/robots/{}/persistent_maps' . format ( robot . serial ) ) , headers = self . _headers ) ) resp2 . raise_for_status ( ) self . _persistent_maps . update ( { robot . serial : resp2 . json ( ) } )
5799	def walk_ast ( node , code_lines , sections , md_chunks ) : if isinstance ( node , _ast . FunctionDef ) : key = ( 'function' , node . name ) if key not in sections : return docstring = ast . get_docstring ( node ) def_lineno = node . lineno + len ( node . decorator_list ) definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> ' ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , definition , description_md ) + "\n" md_chunks [ key ] = md_chunk . replace ( '>\n\n' , '' ) elif isinstance ( node , _ast . ClassDef ) : if ( 'class' , node . name ) not in sections : return for subnode in node . body : if isinstance ( subnode , _ast . FunctionDef ) : node_id = node . name + '.' + subnode . name method_key = ( 'method' , node_id ) is_method = method_key in sections attribute_key = ( 'attribute' , node_id ) is_attribute = attribute_key in sections is_constructor = subnode . name == '__init__' if not is_constructor and not is_attribute and not is_method : continue docstring = ast . get_docstring ( subnode ) def_lineno = subnode . lineno + len ( subnode . decorator_list ) if not docstring : continue if is_method or is_constructor : definition , description_md = _get_func_info ( docstring , def_lineno , code_lines , '> > ' ) if is_constructor : key = ( 'class' , node . name ) class_docstring = ast . get_docstring ( node ) or '' class_description = textwrap . dedent ( class_docstring ) . strip ( ) if class_description : class_description_md = "> %s\n>" % ( class_description . replace ( "\n" , "\n> " ) ) else : class_description_md = '' md_chunk = textwrap . dedent ( ) . strip ( ) % ( node . name , class_description_md , definition , description_md ) md_chunk = md_chunk . replace ( '\n\n\n' , '\n\n' ) else : key = method_key md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , definition , description_md ) if md_chunk [ - 5 : ] == '\n> >\n' : md_chunk = md_chunk [ 0 : - 5 ] else : key = attribute_key description = textwrap . dedent ( docstring ) . strip ( ) description_md = "> > %s" % ( description . replace ( "\n" , "\n> > " ) ) md_chunk = textwrap . dedent ( ) . strip ( ) % ( subnode . name , description_md ) md_chunks [ key ] = re . sub ( '[ \\t]+\n' , '\n' , md_chunk . rstrip ( ) ) elif isinstance ( node , _ast . If ) : for subast in node . body : walk_ast ( subast , code_lines , sections , md_chunks ) for subast in node . orelse : walk_ast ( subast , code_lines , sections , md_chunks )
5094	def refresh_robots ( self ) : resp = requests . get ( urljoin ( self . ENDPOINT , 'dashboard' ) , headers = self . _headers ) resp . raise_for_status ( ) for robot in resp . json ( ) [ 'robots' ] : if robot [ 'mac_address' ] is None : continue try : self . _robots . add ( Robot ( name = robot [ 'name' ] , serial = robot [ 'serial' ] , secret = robot [ 'secret_key' ] , traits = robot [ 'traits' ] , endpoint = robot [ 'nucleo_url' ] ) ) except requests . exceptions . HTTPError : print ( "Your '{}' robot is offline." . format ( robot [ 'name' ] ) ) continue self . refresh_persistent_maps ( ) for robot in self . _robots : robot . has_persistent_maps = robot . serial in self . _persistent_maps
8480	def get ( name , default = None , allow_default = True ) : return Config ( ) . get ( name , default , allow_default = allow_default )
11465	def cd ( self , folder ) : if folder . startswith ( '/' ) : self . _ftp . cwd ( folder ) else : for subfolder in folder . split ( '/' ) : if subfolder : self . _ftp . cwd ( subfolder )
11564	def servo_config ( self , pin , min_pulse = 544 , max_pulse = 2400 ) : self . set_pin_mode ( pin , self . SERVO , self . OUTPUT ) command = [ pin , min_pulse & 0x7f , ( min_pulse >> 7 ) & 0x7f , max_pulse & 0x7f , ( max_pulse >> 7 ) & 0x7f ] self . _command_handler . send_sysex ( self . _command_handler . SERVO_CONFIG , command )
13177	def get_cache_key ( prefix , * args , ** kwargs ) : hash_args_kwargs = hash ( tuple ( kwargs . iteritems ( ) ) + args ) return '{}_{}' . format ( prefix , hash_args_kwargs )
1844	def JO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC )
6269	def on_resize ( self , width , height ) : self . width , self . height = width , height self . buffer_width , self . buffer_height = width , height self . resize ( width , height )
13272	def generic_masked ( arr , attrs = None , minv = None , maxv = None , mask_nan = True ) : attrs = attrs or { } if 'valid_min' in attrs : minv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_min' ] ) if 'valid_max' in attrs : maxv = safe_attribute_typing ( arr . dtype , attrs [ 'valid_max' ] ) if 'valid_range' in attrs : vr = attrs [ 'valid_range' ] minv = safe_attribute_typing ( arr . dtype , vr [ 0 ] ) maxv = safe_attribute_typing ( arr . dtype , vr [ 1 ] ) try : info = np . iinfo ( arr . dtype ) except ValueError : info = np . finfo ( arr . dtype ) minv = minv if minv is not None else info . min maxv = maxv if maxv is not None else info . max if mask_nan is True : arr = np . ma . fix_invalid ( arr ) return np . ma . masked_outside ( arr , minv , maxv )
10065	def json_serializer ( pid , data , * args ) : if data is not None : response = Response ( json . dumps ( data . dumps ( ) ) , mimetype = 'application/json' ) else : response = Response ( mimetype = 'application/json' ) return response
151	def from_shapely ( geometry , label = None ) : import shapely . geometry if isinstance ( geometry , shapely . geometry . MultiPolygon ) : return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) elif isinstance ( geometry , shapely . geometry . Polygon ) : return MultiPolygon ( [ Polygon . from_shapely ( geometry , label = label ) ] ) elif isinstance ( geometry , shapely . geometry . collection . GeometryCollection ) : ia . do_assert ( all ( [ isinstance ( poly , shapely . geometry . Polygon ) for poly in geometry . geoms ] ) ) return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) else : raise Exception ( "Unknown datatype '%s'. Expected shapely.geometry.Polygon or " "shapely.geometry.MultiPolygon or " "shapely.geometry.collections.GeometryCollection." % ( type ( geometry ) , ) )
10785	def add_missing_particles ( st , rad = 'calc' , tries = 50 , ** kwargs ) : if rad == 'calc' : rad = guess_add_radii ( st ) guess , npart = feature_guess ( st , rad , ** kwargs ) tries = np . min ( [ tries , npart ] ) accepts , new_poses = check_add_particles ( st , guess [ : tries ] , rad = rad , ** kwargs ) return accepts , new_poses
9905	def ping ( self ) : self . __validate_ping_param ( ) ping_proc = subprocrunner . SubprocessRunner ( self . __get_ping_command ( ) ) ping_proc . run ( ) return PingResult ( ping_proc . stdout , ping_proc . stderr , ping_proc . returncode )
10483	def _match ( self , ** kwargs ) : for k in kwargs . keys ( ) : try : val = getattr ( self , k ) except _a11y . Error : return False if sys . version_info [ : 2 ] <= ( 2 , 6 ) : if isinstance ( val , basestring ) : if not fnmatch . fnmatch ( unicode ( val ) , kwargs [ k ] ) : return False else : if val != kwargs [ k ] : return False elif sys . version_info [ 0 ] == 3 : if isinstance ( val , str ) : if not fnmatch . fnmatch ( val , str ( kwargs [ k ] ) ) : return False else : if val != kwargs [ k ] : return False else : if isinstance ( val , str ) or isinstance ( val , unicode ) : if not fnmatch . fnmatch ( val , kwargs [ k ] ) : return False else : if val != kwargs [ k ] : return False return True
4624	def _get_encrypted_masterpassword ( self ) : if not self . unlocked ( ) : raise WalletLocked aes = AESCipher ( self . password ) return "{}${}" . format ( self . _derive_checksum ( self . masterkey ) , aes . encrypt ( self . masterkey ) )
1991	def ls ( self , glob_str ) : path = os . path . join ( self . uri , glob_str ) return [ os . path . split ( s ) [ 1 ] for s in glob . glob ( path ) ]
1829	def JA ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC )
4901	def get_course_enrollments ( self , enterprise_customer , days ) : return CourseEnrollment . objects . filter ( created__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
12542	def get_attributes ( self , attributes , default = '' ) : if isinstance ( attributes , str ) : attributes = [ attributes ] attrs = [ getattr ( self , attr , default ) for attr in attributes ] if len ( attrs ) == 1 : return attrs [ 0 ] return tuple ( attrs )
3460	def single_reaction_deletion ( model , reaction_list = None , method = "fba" , solution = None , processes = None , ** kwargs ) : return _multi_deletion ( model , 'reaction' , element_lists = _element_lists ( model . reactions , reaction_list ) , method = method , solution = solution , processes = processes , ** kwargs )
12687	def get_notification_language ( user ) : if getattr ( settings , "NOTIFICATION_LANGUAGE_MODULE" , False ) : try : app_label , model_name = settings . NOTIFICATION_LANGUAGE_MODULE . split ( "." ) model = models . get_model ( app_label , model_name ) language_model = model . _default_manager . get ( user__id__exact = user . id ) if hasattr ( language_model , "language" ) : return language_model . language except ( ImportError , ImproperlyConfigured , model . DoesNotExist ) : raise LanguageStoreNotAvailable raise LanguageStoreNotAvailable
1796	def XADD ( cpu , dest , src ) : MASK = ( 1 << dest . size ) - 1 SIGN_MASK = 1 << ( dest . size - 1 ) arg0 = dest . read ( ) arg1 = src . read ( ) temp = ( arg1 + arg0 ) & MASK src . write ( arg0 ) dest . write ( temp ) tempCF = Operators . OR ( Operators . ULT ( temp , arg0 ) , Operators . ULT ( temp , arg1 ) ) cpu . CF = tempCF cpu . AF = ( ( arg0 ^ arg1 ) ^ temp ) & 0x10 != 0 cpu . ZF = temp == 0 cpu . SF = ( temp & SIGN_MASK ) != 0 cpu . OF = ( ( ( arg0 ^ arg1 ^ SIGN_MASK ) & ( temp ^ arg1 ) ) & SIGN_MASK ) != 0 cpu . PF = cpu . _calculate_parity_flag ( temp )
4499	def _json ( self , response , status_code ) : if isinstance ( status_code , numbers . Integral ) : status_code = ( status_code , ) if response . status_code in status_code : return response . json ( ) else : raise RuntimeError ( "Response has status " "code {} not {}" . format ( response . status_code , status_code ) )
8274	def colors ( self , n = 10 , d = 0.035 ) : s = sum ( [ w for clr , rng , w in self . ranges ] ) colors = colorlist ( ) for i in _range ( n ) : r = random ( ) for clr , rng , weight in self . ranges : if weight / s >= r : break r -= weight / s colors . append ( rng ( clr , d ) ) return colors
13868	def weekday ( when , weekday , start = mon ) : if isinstance ( when , datetime ) : when = when . date ( ) today = when . weekday ( ) delta = weekday - today if weekday < start and today >= start : delta += 7 elif weekday >= start and today < start : delta -= 7 return when + timedelta ( days = delta )
6458	def _m_degree ( self , term ) : mdeg = 0 last_was_vowel = False for letter in term : if letter in self . _vowels : last_was_vowel = True else : if last_was_vowel : mdeg += 1 last_was_vowel = False return mdeg
10385	def get_walks_exhaustive ( graph , node , length ) : if 0 == length : return ( node , ) , return tuple ( ( node , key ) + path for neighbor in graph . edge [ node ] for path in get_walks_exhaustive ( graph , neighbor , length - 1 ) if node not in path for key in graph . edge [ node ] [ neighbor ] )
10459	def isEmpty ( cls , datatype = None ) : if not datatype : datatype = AppKit . NSString if not isinstance ( datatype , types . ListType ) : datatype = [ datatype ] pp = pprint . PrettyPrinter ( ) logging . debug ( 'Desired datatypes: %s' % pp . pformat ( datatype ) ) opt_dict = { } logging . debug ( 'Results filter is: %s' % pp . pformat ( opt_dict ) ) try : log_msg = 'Request to verify pasteboard is empty' logging . debug ( log_msg ) pb = AppKit . NSPasteboard . generalPasteboard ( ) its_empty = not bool ( pb . canReadObjectForClasses_options_ ( datatype , opt_dict ) ) except ValueError as error : logging . error ( error ) raise return bool ( its_empty )
9373	def is_valid_url ( url ) : regex = re . compile ( r'^(?:http|ftp)s?://' r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+(?:[A-Z]{2,6}\.?|[A-Z0-9-]{2,}\.?)|' r'localhost|' r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})' r'(?::\d+)?' r'(?:/?|[/?]\S+)$' , re . IGNORECASE ) if regex . match ( url ) : logger . info ( "URL given as config" ) return True else : return False
3838	async def set_group_link_sharing_enabled ( self , set_group_link_sharing_enabled_request ) : response = hangouts_pb2 . SetGroupLinkSharingEnabledResponse ( ) await self . _pb_request ( 'conversations/setgrouplinksharingenabled' , set_group_link_sharing_enabled_request , response ) return response
3417	def _cell ( x ) : x_no_none = [ i if i is not None else "" for i in x ] return array ( x_no_none , dtype = np_object )
9783	def update ( ctx , name , description , tags ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) update_dict = { } if name : update_dict [ 'name' ] = name if description : update_dict [ 'description' ] = description tags = validate_tags ( tags ) if tags : update_dict [ 'tags' ] = tags if not update_dict : Printer . print_warning ( 'No argument was provided to update the build.' ) sys . exit ( 0 ) try : response = PolyaxonClient ( ) . build_job . update_build ( user , project_name , _build , update_dict ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not update build `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Build updated." ) get_build_details ( response )
4383	def is_denied ( self , role , method , resource ) : return ( role , method , resource ) in self . _denied
3635	def bid ( self , trade_id , bid , fast = False ) : method = 'PUT' url = 'trade/%s/bid' % trade_id if not fast : rc = self . tradeStatus ( trade_id ) [ 0 ] if rc [ 'currentBid' ] >= bid or self . credits < bid : return False data = { 'bid' : bid } try : rc = self . __request__ ( method , url , data = json . dumps ( data ) , params = { 'sku_b' : self . sku_b } , fast = fast ) [ 'auctionInfo' ] [ 0 ] except PermissionDenied : return False if rc [ 'bidState' ] == 'highest' or ( rc [ 'tradeState' ] == 'closed' and rc [ 'bidState' ] == 'buyNow' ) : return True else : return False
5359	def execute_nonstop_tasks ( self , tasks_cls ) : self . execute_batch_tasks ( tasks_cls , self . conf [ 'sortinghat' ] [ 'sleep_for' ] , self . conf [ 'general' ] [ 'min_update_delay' ] , False )
3919	async def _load ( self ) : try : conv_events = await self . _conversation . get_events ( self . _conversation . events [ 0 ] . id_ ) except ( IndexError , hangups . NetworkError ) : conv_events = [ ] if not conv_events : self . _first_loaded = True if self . _focus_position == self . POSITION_LOADING and conv_events : self . set_focus ( conv_events [ - 1 ] . id_ ) else : self . _modified ( ) self . _refresh_watermarked_events ( ) self . _is_loading = False
7388	def node_radius ( self , node ) : return self . get_idx ( node ) * self . scale + self . internal_radius
721	def getAllMetrics ( self ) : result = self . getReportMetrics ( ) result . update ( self . getOptimizationMetrics ( ) ) return result
3925	def _update_tabs ( self ) : text = [ ] for num , widget in enumerate ( self . _widgets ) : palette = ( 'active_tab' if num == self . _tab_index else 'inactive_tab' ) text += [ ( palette , ' {} ' . format ( self . _widget_title [ widget ] ) ) , ( 'tab_background' , ' ' ) , ] self . _tabs . set_text ( text ) self . _frame . contents [ 'body' ] = ( self . _widgets [ self . _tab_index ] , None )
7053	def _read_pklc ( lcfile ) : if lcfile . endswith ( '.gz' ) : try : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with gzip . open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) else : try : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd ) except UnicodeDecodeError : with open ( lcfile , 'rb' ) as infd : lcdict = pickle . load ( infd , encoding = 'latin1' ) return lcdict
9202	def count_cycles ( series , ndigits = None , left = False , right = False ) : counts = defaultdict ( float ) round_ = _get_round_function ( ndigits ) for low , high , mult in extract_cycles ( series , left = left , right = right ) : delta = round_ ( abs ( high - low ) ) counts [ delta ] += mult return sorted ( counts . items ( ) )
4531	def construct ( cls , project , ** desc ) : return cls ( project . drivers , maker = project . maker , ** desc )
312	def downside_risk ( returns , required_return = 0 , period = DAILY ) : return ep . downside_risk ( returns , required_return = required_return , period = period )
5033	def get ( self , request , enterprise_customer_uuid ) : context = self . _build_context ( request , enterprise_customer_uuid ) transmit_courses_metadata_form = TransmitEnterpriseCoursesForm ( ) context . update ( { self . ContextParameters . TRANSMIT_COURSES_METADATA_FORM : transmit_courses_metadata_form } ) return render ( request , self . template , context )
7734	def nfkc ( data ) : if isinstance ( data , list ) : data = u"" . join ( data ) return unicodedata . normalize ( "NFKC" , data )
1857	def BT ( cpu , dest , src ) : if dest . type == 'register' : cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) value = cpu . read_int ( addr + base , 8 ) cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 else : raise NotImplementedError ( f"Unknown operand for BT: {dest.type}" )
8746	def get_floatingips_count ( context , filters = None ) : LOG . info ( 'get_floatingips_count for tenant %s filters %s' % ( context . tenant_id , filters ) ) if filters is None : filters = { } filters [ '_deallocated' ] = False filters [ 'address_type' ] = ip_types . FLOATING count = db_api . ip_address_count_all ( context , filters ) LOG . info ( 'Found %s floating ips for tenant %s' % ( count , context . tenant_id ) ) return count
10010	def get_command_names ( ) : ret = [ ] for f in os . listdir ( COMMAND_MODULE_PATH ) : if os . path . isfile ( os . path . join ( COMMAND_MODULE_PATH , f ) ) and f . endswith ( COMMAND_MODULE_SUFFIX ) : ret . append ( f [ : - len ( COMMAND_MODULE_SUFFIX ) ] ) return ret
9212	def get_channel_image ( self , channel , img_size = 300 , skip_cache = False ) : from bs4 import BeautifulSoup from wikipedia . exceptions import PageError import re import wikipedia wikipedia . set_lang ( 'fr' ) if not channel : _LOGGER . error ( 'Channel is not set. Could not retrieve image.' ) return if channel in self . _cache_channel_img and not skip_cache : img = self . _cache_channel_img [ channel ] _LOGGER . debug ( 'Cache hit: %s -> %s' , channel , img ) return img channel_info = self . get_channel_info ( channel ) query = channel_info [ 'wiki_page' ] if not query : _LOGGER . debug ( 'Wiki page is not set for channel %s' , channel ) return _LOGGER . debug ( 'Query: %s' , query ) if 'max_img_size' in channel_info : if img_size > channel_info [ 'max_img_size' ] : _LOGGER . info ( 'Requested image size is bigger than the max, ' 'setting it to %s' , channel_info [ 'max_img_size' ] ) img_size = channel_info [ 'max_img_size' ] try : page = wikipedia . page ( query ) _LOGGER . debug ( 'Wikipedia article title: %s' , page . title ) soup = BeautifulSoup ( page . html ( ) , 'html.parser' ) images = soup . find_all ( 'img' ) img_src = None for i in images : if i [ 'alt' ] . startswith ( 'Image illustrative' ) : img_src = re . sub ( r'\d+px' , '{}px' . format ( img_size ) , i [ 'src' ] ) img = 'https:{}' . format ( img_src ) if img_src else None self . _cache_channel_img [ channel ] = img return img except PageError : _LOGGER . error ( 'Could not fetch channel image for %s' , channel )
6885	def main ( ) : import signal signal . signal ( signal . SIGPIPE , signal . SIG_DFL ) import argparse aparser = argparse . ArgumentParser ( description = 'read a HAT LC of any format and output to stdout' ) aparser . add_argument ( 'hatlcfile' , action = 'store' , type = str , help = ( "path to the light curve you want to read and pipe to stdout" ) ) aparser . add_argument ( '--describe' , action = 'store_true' , default = False , help = ( "don't dump the columns, show only object info and LC metadata" ) ) args = aparser . parse_args ( ) filetoread = args . hatlcfile if not os . path . exists ( filetoread ) : LOGERROR ( "file provided: %s doesn't seem to exist" % filetoread ) sys . exit ( 1 ) filename = os . path . basename ( filetoread ) if filename . endswith ( '-hatlc.csv.gz' ) or filename . endswith ( '-csvlc.gz' ) : if args . describe : describe ( read_csvlc ( filename ) ) sys . exit ( 0 ) else : with gzip . open ( filename , 'rb' ) as infd : for line in infd : print ( line . decode ( ) , end = '' ) elif filename . endswith ( '-hatlc.sqlite.gz' ) : lcdict , msg = read_and_filter_sqlitecurve ( filetoread ) describe ( lcdict , offsetwith = '#' ) if args . describe : sys . exit ( 0 ) apertures = sorted ( lcdict [ 'lcapertures' ] . keys ( ) ) for aper in apertures : COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_MAG_COLUMNS } ) COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_ERR_COLUMNS } ) COLUMNDEFS . update ( { '%s_%s' % ( x , aper ) : COLUMNDEFS [ x ] for x in LC_FLAG_COLUMNS } ) formstr = ',' . join ( [ COLUMNDEFS [ x ] [ 1 ] for x in lcdict [ 'columns' ] ] ) ndet = lcdict [ 'objectinfo' ] [ 'ndet' ] for ind in range ( ndet ) : line = [ lcdict [ x ] [ ind ] for x in lcdict [ 'columns' ] ] formline = formstr % tuple ( line ) print ( formline ) else : LOGERROR ( 'unrecognized HATLC file: %s' % filetoread ) sys . exit ( 1 )
1583	def generate ( ) : data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) return REQID ( data_bytes )
12161	def userFolder ( ) : path = os . path . expanduser ( "~" ) + "/.swhlab/" if not os . path . exists ( path ) : print ( "creating" , path ) os . mkdir ( path ) return os . path . abspath ( path )
11595	def _rc_renamenx ( self , src , dst ) : "Rename key ``src`` to ``dst`` if ``dst`` doesn't already exist" if self . exists ( dst ) : return False return self . _rc_rename ( src , dst )
5124	def show_type ( self , edge_type , ** kwargs ) : for v in self . g . nodes ( ) : e = ( v , v ) if self . g . is_edge ( e ) and self . g . ep ( e , 'edge_type' ) == edge_type : ei = self . g . edge_index [ e ] self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_highlight' ] ) self . g . set_vp ( v , 'vertex_color' , self . edge2queue [ ei ] . colors [ 'vertex_color' ] ) else : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_inactive' ] ) self . g . set_vp ( v , 'vertex_color' , [ 0 , 0 , 0 , 0.9 ] ) for e in self . g . edges ( ) : if self . g . ep ( e , 'edge_type' ) == edge_type : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_active' ] ) else : self . g . set_ep ( e , 'edge_color' , self . colors [ 'edge_inactive' ] ) self . draw ( update_colors = False , ** kwargs ) self . _update_all_colors ( )
4060	def item_template ( self , itemtype ) : template_name = "item_template_" + itemtype query_string = "/items/new?itemType={i}" . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return copy . deepcopy ( self . templates [ template_name ] [ "tmplt" ] ) retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
637	def read ( cls , proto ) : protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 return connections
13731	def validate_is_not_none ( config_val , evar ) : if config_val is None : raise ValueError ( "Value for environment variable '{evar_name}' can't " "be empty." . format ( evar_name = evar . name ) ) return config_val
5508	def worker ( f ) : @ functools . wraps ( f ) async def wrapper ( cls , connection , rest ) : try : await f ( cls , connection , rest ) except asyncio . CancelledError : connection . response ( "426" , "transfer aborted" ) connection . response ( "226" , "abort successful" ) return wrapper
13811	def GetTopLevelContainingType ( self ) : desc = self while desc . containing_type is not None : desc = desc . containing_type return desc
7813	def _decode_alt_names ( self , alt_names ) : for alt_name in alt_names : tname = alt_name . getName ( ) comp = alt_name . getComponent ( ) if tname == "dNSName" : key = "DNS" value = _decode_asn1_string ( comp ) elif tname == "uniformResourceIdentifier" : key = "URI" value = _decode_asn1_string ( comp ) elif tname == "otherName" : oid = comp . getComponentByName ( "type-id" ) value = comp . getComponentByName ( "value" ) if oid == XMPPADDR_OID : key = "XmppAddr" value = der_decoder . decode ( value , asn1Spec = UTF8String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) elif oid == SRVNAME_OID : key = "SRVName" value = der_decoder . decode ( value , asn1Spec = IA5String ( ) ) [ 0 ] value = _decode_asn1_string ( value ) else : logger . debug ( "Unknown other name: {0}" . format ( oid ) ) continue else : logger . debug ( "Unsupported general name: {0}" . format ( tname ) ) continue self . alt_names [ key ] . append ( value )
2821	def convert_dropout ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting dropout ...' ) if names == 'short' : tf_name = 'DO' + random_string ( 6 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) dropout = keras . layers . Dropout ( rate = params [ 'ratio' ] , name = tf_name ) layers [ scope_name ] = dropout ( layers [ inputs [ 0 ] ] )
1820	def SETP ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
13707	def squeeze_words ( line , width = 60 ) : while ( ' ' in line ) and ( len ( line ) > width ) : head , _ , tail = line . rpartition ( ' ' ) line = ' ' . join ( ( head , tail ) ) return line
11642	def yaml_write_data ( yaml_data , filename ) : with open ( filename , 'w' ) as fd : yaml . dump ( yaml_data , fd , default_flow_style = False ) return True return False
7981	def auth_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise LegacyAuthenticationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
5020	def validate_image_size ( image ) : config = get_app_config ( ) valid_max_image_size_in_bytes = config . valid_max_image_size * 1024 if config and not image . size <= valid_max_image_size_in_bytes : raise ValidationError ( _ ( "The logo image file size must be less than or equal to %s KB." ) % config . valid_max_image_size )
3341	def parse_xml_body ( environ , allow_empty = False ) : clHeader = environ . get ( "CONTENT_LENGTH" , "" ) . strip ( ) if clHeader == "" : requestbody = "" else : try : content_length = int ( clHeader ) if content_length < 0 : raise DAVError ( HTTP_BAD_REQUEST , "Negative content-length." ) except ValueError : raise DAVError ( HTTP_BAD_REQUEST , "content-length is not numeric." ) if content_length == 0 : requestbody = "" else : requestbody = environ [ "wsgi.input" ] . read ( content_length ) environ [ "wsgidav.all_input_read" ] = 1 if requestbody == "" : if allow_empty : return None else : raise DAVError ( HTTP_BAD_REQUEST , "Body must not be empty." ) try : rootEL = etree . fromstring ( requestbody ) except Exception as e : raise DAVError ( HTTP_BAD_REQUEST , "Invalid XML format." , src_exception = e ) if environ . get ( "wsgidav.dump_request_body" ) : _logger . info ( "{} XML request body:\n{}" . format ( environ [ "REQUEST_METHOD" ] , compat . to_native ( xml_to_bytes ( rootEL , pretty_print = True ) ) , ) ) environ [ "wsgidav.dump_request_body" ] = False return rootEL
3496	def reaction_weight ( reaction ) : if len ( reaction . metabolites ) != 1 : raise ValueError ( 'Reaction weight is only defined for single ' 'metabolite products or educts.' ) met , coeff = next ( iteritems ( reaction . metabolites ) ) return [ coeff * met . formula_weight ]
4345	def stats ( self , input_filepath ) : effect_args = [ 'channels' , '1' , 'stats' ] _ , _ , stats_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stats_dict = { } lines = stats_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stats_dict [ key ] = value return stats_dict
5184	def edges ( self , ** kwargs ) : edges = self . _query ( 'edges' , ** kwargs ) for edge in edges : identifier_source = edge [ 'source_type' ] + '[' + edge [ 'source_title' ] + ']' identifier_target = edge [ 'target_type' ] + '[' + edge [ 'target_title' ] + ']' yield Edge ( source = self . resources [ identifier_source ] , target = self . resources [ identifier_target ] , relationship = edge [ 'relationship' ] , node = edge [ 'certname' ] )
1730	def match ( self , string , pos ) : return self . pat . match ( string , int ( pos ) )
6235	def get_time ( self ) -> float : if self . paused : return self . pause_time return mixer . music . get_pos ( ) / 1000.0
13061	def get_passage ( self , objectId , subreference ) : passage = self . resolver . getTextualNode ( textId = objectId , subreference = subreference , metadata = True ) return passage
11623	def _equivalent ( self , char , prev , next , implicitA ) : result = [ ] if char . isVowel == False : result . append ( char . chr ) if char . isConsonant and ( ( next is not None and next . isConsonant ) or next is None ) : result . append ( DevanagariCharacter . _VIRAMA ) else : if prev is None or prev . isConsonant == False : result . append ( char . chr ) else : if char . _dependentVowel is not None : result . append ( char . _dependentVowel ) return result
5445	def _parse_image_uri ( self , raw_uri ) : docker_uri = os . path . join ( self . _relative_path , raw_uri . replace ( 'https://' , 'https/' , 1 ) ) return docker_uri
6946	def jhk_to_imag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , IJHK , IJH , IJK , IHK , IJ , IH , IK )
1321	def Maximize ( self , waitTime : float = OPERATION_WAIT_TIME ) -> bool : if self . IsTopLevel ( ) : return self . ShowWindow ( SW . ShowMaximized , waitTime ) return False
12517	def extract_datasets ( h5file , h5path = '/' ) : if isinstance ( h5file , str ) : _h5file = h5py . File ( h5file , mode = 'r' ) else : _h5file = h5file _datasets = get_datasets ( _h5file , h5path ) datasets = OrderedDict ( ) try : for ds in _datasets : datasets [ ds . name . split ( '/' ) [ - 1 ] ] = ds [ : ] except : raise RuntimeError ( 'Error reading datasets in {}/{}.' . format ( _h5file . filename , h5path ) ) finally : if isinstance ( h5file , str ) : _h5file . close ( ) return datasets
6902	def load_xmatch_external_catalogs ( xmatchto , xmatchkeys , outfile = None ) : outdict = { } for xc , xk in zip ( xmatchto , xmatchkeys ) : parsed_catdef = _parse_xmatch_catalog_header ( xc , xk ) if not parsed_catdef : continue ( infd , catdefdict , catcolinds , catcoldtypes , catcolnames , catcolunits ) = parsed_catdef catarr = np . genfromtxt ( infd , usecols = catcolinds , names = xk , dtype = ',' . join ( catcoldtypes ) , comments = '#' , delimiter = '|' , autostrip = True ) infd . close ( ) catshortname = os . path . splitext ( os . path . basename ( xc ) ) [ 0 ] catshortname = catshortname . replace ( '.csv' , '' ) objra , objdecl = ( catarr [ catdefdict [ 'colra' ] ] , catarr [ catdefdict [ 'coldec' ] ] ) cosdecl = np . cos ( np . radians ( objdecl ) ) sindecl = np . sin ( np . radians ( objdecl ) ) cosra = np . cos ( np . radians ( objra ) ) sinra = np . sin ( np . radians ( objra ) ) xyz = np . column_stack ( ( cosra * cosdecl , sinra * cosdecl , sindecl ) ) kdt = cKDTree ( xyz , copy_data = True ) catoutdict = { 'kdtree' : kdt , 'data' : catarr , 'columns' : xk , 'colnames' : catcolnames , 'colunits' : catcolunits , 'name' : catdefdict [ 'name' ] , 'desc' : catdefdict [ 'description' ] } outdict [ catshortname ] = catoutdict if outfile is not None : if sys . platform == 'darwin' : dumpbytes = pickle . dumps ( outdict , protocol = pickle . HIGHEST_PROTOCOL ) max_bytes = 2 ** 31 - 1 with open ( outfile , 'wb' ) as outfd : for idx in range ( 0 , len ( dumpbytes ) , max_bytes ) : outfd . write ( dumpbytes [ idx : idx + max_bytes ] ) else : with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outfile else : return outdict
3980	def _add_active_assets ( specs ) : specs [ 'assets' ] = { } for spec in specs . get_apps_and_libs ( ) : for asset in spec [ 'assets' ] : if not specs [ 'assets' ] . get ( asset [ 'name' ] ) : specs [ 'assets' ] [ asset [ 'name' ] ] = { } specs [ 'assets' ] [ asset [ 'name' ] ] [ 'required_by' ] = set ( ) specs [ 'assets' ] [ asset [ 'name' ] ] [ 'used_by' ] = set ( ) specs [ 'assets' ] [ asset [ 'name' ] ] [ 'used_by' ] . add ( spec . name ) if asset [ 'required' ] : specs [ 'assets' ] [ asset [ 'name' ] ] [ 'required_by' ] . add ( spec . name )
12715	def positions ( self ) : return [ self . ode_obj . getPosition ( i ) for i in range ( self . LDOF ) ]
7103	def data ( self , X = None , y = None , sentences = None ) : self . X = X self . y = y self . sentences = sentences
8715	def file_print ( self , filename ) : log . info ( 'Printing ' + filename ) res = self . __exchange ( PRINT_FILE . format ( filename = filename ) ) log . info ( res ) return res
13902	def ensure_specifier_exists ( db_spec ) : local_match = LOCAL_RE . match ( db_spec ) remote_match = REMOTE_RE . match ( db_spec ) plain_match = PLAIN_RE . match ( db_spec ) if local_match : db_name = local_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True elif remote_match : hostname , portnum , database = map ( remote_match . groupdict ( ) . get , ( 'hostname' , 'portnum' , 'database' ) ) server = shortcuts . get_server ( server_url = ( 'http://%s:%s' % ( hostname , portnum ) ) ) if database not in server : server . create ( database ) return True elif plain_match : db_name = plain_match . groupdict ( ) . get ( 'database' ) server = shortcuts . get_server ( ) if db_name not in server : server . create ( db_name ) return True return False
10895	def set_filter ( self , slices , values ) : self . filters = [ [ sl , values [ sl ] ] for sl in slices ]
1096	def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )
12740	def _parse_corporations ( self , datafield , subfield , roles = [ "any" ] ) : if len ( datafield ) != 3 : raise ValueError ( "datafield parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subield have to be 3 chars long!" ) parsed_corporations = [ ] for corporation in self . get_subfields ( datafield , subfield ) : other_subfields = corporation . other_subfields if "4" in other_subfields and roles != [ "any" ] : corp_roles = other_subfields [ "4" ] relevant = any ( map ( lambda role : role in roles , corp_roles ) ) if not relevant : continue name = "" place = "" date = "" name = corporation if "c" in other_subfields : place = "," . join ( other_subfields [ "c" ] ) if "d" in other_subfields : date = "," . join ( other_subfields [ "d" ] ) parsed_corporations . append ( Corporation ( name , place , date ) ) return parsed_corporations
5730	def read ( self , count ) : new_index = self . index + count if new_index > self . len : buf = self . raw_text [ self . index : ] else : buf = self . raw_text [ self . index : new_index ] self . index = new_index return buf
2748	def get_all_droplets ( self , tag_name = None ) : params = dict ( ) if tag_name : params [ "tag_name" ] = tag_name data = self . get_data ( "droplets/" , params = params ) droplets = list ( ) for jsoned in data [ 'droplets' ] : droplet = Droplet ( ** jsoned ) droplet . token = self . token for net in droplet . networks [ 'v4' ] : if net [ 'type' ] == 'private' : droplet . private_ip_address = net [ 'ip_address' ] if net [ 'type' ] == 'public' : droplet . ip_address = net [ 'ip_address' ] if droplet . networks [ 'v6' ] : droplet . ip_v6_address = droplet . networks [ 'v6' ] [ 0 ] [ 'ip_address' ] if "backups" in droplet . features : droplet . backups = True else : droplet . backups = False if "ipv6" in droplet . features : droplet . ipv6 = True else : droplet . ipv6 = False if "private_networking" in droplet . features : droplet . private_networking = True else : droplet . private_networking = False droplets . append ( droplet ) return droplets
11082	def freeze ( value ) : if isinstance ( value , list ) : return FrozenList ( * value ) if isinstance ( value , dict ) : return FrozenDict ( ** value ) return value
1985	def save_stream ( self , key , binary = False ) : s = io . BytesIO ( ) if binary else io . StringIO ( ) yield s self . save_value ( key , s . getvalue ( ) )
12622	def have_same_shape ( array1 , array2 , nd_to_check = None ) : shape1 = array1 . shape shape2 = array2 . shape if nd_to_check is not None : if len ( shape1 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the first image: \n{}\n.' . format ( shape1 ) raise ValueError ( msg ) elif len ( shape2 ) < nd_to_check : msg = 'Number of dimensions to check {} is out of bounds for the shape of the second image: \n{}\n.' . format ( shape2 ) raise ValueError ( msg ) shape1 = shape1 [ : nd_to_check ] shape2 = shape2 [ : nd_to_check ] return shape1 == shape2
13134	def import_domaindump ( ) : parser = argparse . ArgumentParser ( description = "Imports users, groups and computers result files from the ldapdomaindump tool, will resolve the names from domain_computers output for IPs" ) parser . add_argument ( "files" , nargs = '+' , help = "The domaindump files to import" ) arguments = parser . parse_args ( ) domain_users_file = '' domain_groups_file = '' computer_count = 0 user_count = 0 stats = { } for filename in arguments . files : if filename . endswith ( 'domain_computers.json' ) : print_notification ( 'Parsing domain computers' ) computer_count = parse_domain_computers ( filename ) if computer_count : stats [ 'hosts' ] = computer_count print_success ( "{} hosts imported" . format ( computer_count ) ) elif filename . endswith ( 'domain_users.json' ) : domain_users_file = filename elif filename . endswith ( 'domain_groups.json' ) : domain_groups_file = filename if domain_users_file : print_notification ( "Parsing domain users" ) user_count = parse_domain_users ( domain_users_file , domain_groups_file ) if user_count : print_success ( "{} users imported" . format ( user_count ) ) stats [ 'users' ] = user_count Logger ( ) . log ( "import_domaindump" , 'Imported domaindump, found {} user, {} systems' . format ( user_count , computer_count ) , stats )
2606	def check_memo ( self , task_id , task ) : if not self . memoize or not task [ 'memoize' ] : task [ 'hashsum' ] = None return None , None hashsum = self . make_hash ( task ) present = False result = None if hashsum in self . memo_lookup_table : present = True result = self . memo_lookup_table [ hashsum ] logger . info ( "Task %s using result from cache" , task_id ) task [ 'hashsum' ] = hashsum return present , result
6849	def needs_initrole ( self , stop_on_error = False ) : ret = False target_host_present = self . is_present ( ) if not target_host_present : default_host_present = self . is_present ( self . env . default_hostname ) if default_host_present : if self . verbose : print ( 'Target host missing and default host present so host init required.' ) ret = True else : if self . verbose : print ( 'Target host missing but default host also missing, ' 'so no host init required.' ) else : if self . verbose : print ( 'Target host is present so no host init required.' ) return ret
3917	def from_conversation_event ( conversation , conv_event , prev_conv_event , datetimefmt , watermark_users = None ) : user = conversation . get_user ( conv_event . user_id ) if prev_conv_event is not None : is_new_day = ( conv_event . timestamp . astimezone ( tz = None ) . date ( ) != prev_conv_event . timestamp . astimezone ( tz = None ) . date ( ) ) else : is_new_day = False if isinstance ( conv_event , hangups . ChatMessageEvent ) : return MessageWidget ( conv_event . timestamp , conv_event . text , datetimefmt , user , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . RenameEvent ) : if conv_event . new_name == '' : text = ( '{} cleared the conversation name' . format ( user . first_name ) ) else : text = ( '{} renamed the conversation to {}' . format ( user . first_name , conv_event . new_name ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . MembershipChangeEvent ) : event_users = [ conversation . get_user ( user_id ) for user_id in conv_event . participant_ids ] names = ', ' . join ( [ user . full_name for user in event_users ] ) if conv_event . type_ == hangups . MEMBERSHIP_CHANGE_TYPE_JOIN : text = ( '{} added {} to the conversation' . format ( user . first_name , names ) ) else : text = ( '{} left the conversation' . format ( names ) ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . HangoutEvent ) : text = { hangups . HANGOUT_EVENT_TYPE_START : ( 'A Hangout call is starting.' ) , hangups . HANGOUT_EVENT_TYPE_END : ( 'A Hangout call ended.' ) , hangups . HANGOUT_EVENT_TYPE_ONGOING : ( 'A Hangout call is ongoing.' ) , } . get ( conv_event . event_type , 'Unknown Hangout call event.' ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) elif isinstance ( conv_event , hangups . GroupLinkSharingModificationEvent ) : status_on = hangups . GROUP_LINK_SHARING_STATUS_ON status_text = ( 'on' if conv_event . new_status == status_on else 'off' ) text = '{} turned {} joining by link.' . format ( user . first_name , status_text ) return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users ) else : text = 'Unknown conversation event' return MessageWidget ( conv_event . timestamp , text , datetimefmt , show_date = is_new_day , watermark_users = watermark_users )
6861	def prep_root_password ( self , password = None , ** kwargs ) : r = self . database_renderer ( ** kwargs ) r . env . root_password = password or r . genv . get ( 'db_root_password' ) r . sudo ( "DEBIAN_FRONTEND=noninteractive dpkg --configure -a" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password password {root_password}'" ) r . sudo ( "debconf-set-selections <<< 'mysql-server mysql-server/root_password_again password {root_password}'" )
5765	def _decrypt_encrypted_data ( encryption_algorithm_info , encrypted_content , password ) : decrypt_func = crypto_funcs [ encryption_algorithm_info . encryption_cipher ] if encryption_algorithm_info . kdf == 'pbkdf2' : if encryption_algorithm_info . encryption_cipher == 'rc5' : raise ValueError ( pretty_message ( ) ) enc_key = pbkdf2 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length ) enc_iv = encryption_algorithm_info . encryption_iv plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pbkdf1' : derived_output = pbkdf1 ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length + 8 ) enc_key = derived_output [ 0 : 8 ] enc_iv = derived_output [ 8 : 16 ] plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) elif encryption_algorithm_info . kdf == 'pkcs12_kdf' : enc_key = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . key_length , 1 ) if encryption_algorithm_info . encryption_cipher == 'rc4' : plaintext = decrypt_func ( enc_key , encrypted_content ) else : enc_iv = pkcs12_kdf ( encryption_algorithm_info . kdf_hmac , password , encryption_algorithm_info . kdf_salt , encryption_algorithm_info . kdf_iterations , encryption_algorithm_info . encryption_block_size , 2 ) plaintext = decrypt_func ( enc_key , encrypted_content , enc_iv ) return plaintext
6481	def mem_size ( self ) : data_len = self . _data_mem_size node_count = len ( list ( self . xml_doc . iter ( tag = etree . Element ) ) ) if self . compressed : size = 52 * node_count + data_len + 630 else : tags_len = 0 for e in self . xml_doc . iter ( tag = etree . Element ) : e_len = max ( len ( e . tag ) , 8 ) e_len = ( e_len + 3 ) & ~ 3 tags_len += e_len size = 56 * node_count + data_len + 630 + tags_len return ( size + 8 ) & ~ 7
13470	def copy ( src , dst ) : ( szip , dzip ) = ( src . endswith ( ".zip" ) , dst . endswith ( ".zip" ) ) logging . info ( "Copy: %s => %s" % ( src , dst ) ) if szip and dzip : shutil . copy2 ( src , dst ) elif szip : with zipfile . ZipFile ( src , mode = 'r' ) as z : tmpdir = tempfile . mkdtemp ( ) try : z . extractall ( tmpdir ) if len ( z . namelist ( ) ) != 1 : raise RuntimeError ( "The zip file '%s' should only have one " "compressed file" % src ) tmpfile = join ( tmpdir , z . namelist ( ) [ 0 ] ) try : os . remove ( dst ) except OSError : pass shutil . move ( tmpfile , dst ) finally : shutil . rmtree ( tmpdir , ignore_errors = True ) elif dzip : with zipfile . ZipFile ( dst , mode = 'w' , compression = ZIP_DEFLATED ) as z : z . write ( src , arcname = basename ( src ) ) else : shutil . copy2 ( src , dst )
7364	async def set_tz ( self ) : settings = await self . api . account . settings . get ( ) tz = settings . time_zone . tzinfo_name os . environ [ 'TZ' ] = tz time . tzset ( )
8856	def on_current_tab_changed ( self ) : self . menuEdit . clear ( ) self . menuModes . clear ( ) self . menuPanels . clear ( ) editor = self . tabWidget . current_widget ( ) self . menuEdit . setEnabled ( editor is not None ) self . menuModes . setEnabled ( editor is not None ) self . menuPanels . setEnabled ( editor is not None ) self . actionSave . setEnabled ( editor is not None ) self . actionSave_as . setEnabled ( editor is not None ) self . actionConfigure_run . setEnabled ( editor is not None ) self . actionRun . setEnabled ( editor is not None ) if editor is not None : self . setup_mnu_edit ( editor ) self . setup_mnu_modes ( editor ) self . setup_mnu_panels ( editor ) self . widgetOutline . set_editor ( editor ) self . _update_status_bar ( editor )
252	def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out
11403	def create_record ( marcxml = None , verbose = CFG_BIBRECORD_DEFAULT_VERBOSE_LEVEL , correct = CFG_BIBRECORD_DEFAULT_CORRECT , parser = '' , sort_fields_by_indicators = False , keep_singletons = CFG_BIBRECORD_KEEP_SINGLETONS ) : if marcxml is None : return { } try : rec = _create_record_lxml ( marcxml , verbose , correct , keep_singletons = keep_singletons ) except InvenioBibRecordParserError as ex1 : return ( None , 0 , str ( ex1 ) ) if sort_fields_by_indicators : _record_sort_by_indicators ( rec ) errs = [ ] if correct : errs = _correct_record ( rec ) return ( rec , int ( not errs ) , errs )
10333	def group_nodes_by_annotation_filtered ( graph : BELGraph , node_predicates : NodePredicates = None , annotation : str = 'Subgraph' , ) -> Mapping [ str , Set [ BaseEntity ] ] : node_filter = concatenate_node_predicates ( node_predicates ) return { key : { node for node in nodes if node_filter ( graph , node ) } for key , nodes in group_nodes_by_annotation ( graph , annotation ) . items ( ) }
4889	def update_course ( self , course , enterprise_customer , enterprise_context ) : course [ 'course_runs' ] = self . update_course_runs ( course_runs = course . get ( 'course_runs' ) or [ ] , enterprise_customer = enterprise_customer , enterprise_context = enterprise_context , ) marketing_url = course . get ( 'marketing_url' ) if marketing_url : query_parameters = dict ( enterprise_context , ** utils . get_enterprise_utm_context ( enterprise_customer ) ) course . update ( { 'marketing_url' : utils . update_query_parameters ( marketing_url , query_parameters ) } ) course . update ( enterprise_context ) return course
12432	def create ( self ) : self . create_virtualenv ( ) self . create_project ( ) self . create_uwsgi_script ( ) self . create_nginx_config ( ) self . create_manage_scripts ( ) logging . info ( '** Make sure to set proper permissions for the webserver user account on the var and log directories in the project root' )
8248	def rotate_ryb ( self , angle = 180 ) : h = self . h * 360 angle = angle % 360 wheel = [ ( 0 , 0 ) , ( 15 , 8 ) , ( 30 , 17 ) , ( 45 , 26 ) , ( 60 , 34 ) , ( 75 , 41 ) , ( 90 , 48 ) , ( 105 , 54 ) , ( 120 , 60 ) , ( 135 , 81 ) , ( 150 , 103 ) , ( 165 , 123 ) , ( 180 , 138 ) , ( 195 , 155 ) , ( 210 , 171 ) , ( 225 , 187 ) , ( 240 , 204 ) , ( 255 , 219 ) , ( 270 , 234 ) , ( 285 , 251 ) , ( 300 , 267 ) , ( 315 , 282 ) , ( 330 , 298 ) , ( 345 , 329 ) , ( 360 , 0 ) ] for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if y0 <= h <= y1 : a = 1.0 * x0 + ( x1 - x0 ) * ( h - y0 ) / ( y1 - y0 ) break a = ( a + angle ) % 360 for i in _range ( len ( wheel ) - 1 ) : x0 , y0 = wheel [ i ] x1 , y1 = wheel [ i + 1 ] if y1 < y0 : y1 += 360 if x0 <= a <= x1 : h = 1.0 * y0 + ( y1 - y0 ) * ( a - x0 ) / ( x1 - x0 ) break h = h % 360 return Color ( h / 360 , self . s , self . brightness , self . a , mode = "hsb" , name = "" )
11144	def to_repo_relative_path ( self , path , split = False ) : path = os . path . normpath ( path ) if path == '.' : path = '' path = path . split ( self . __path ) [ - 1 ] . strip ( os . sep ) if split : return path . split ( os . sep ) else : return path
6765	def load_table ( self , table_name , src , dst = 'localhost' , name = None , site = None ) : r = self . database_renderer ( name = name , site = site ) r . env . table_name = table_name r . run ( 'psql --user={dst_db_user} --host={dst_db_host} --command="DROP TABLE IF EXISTS {table_name} CASCADE;"' ) r . run ( 'pg_dump -t {table_name} --user={dst_db_user} --host={dst_db_host} | psql --user={src_db_user} --host={src_db_host}' )
10704	def get_device ( _id ) : url = DEVICE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False return arequest . json ( )
13104	def start_scan ( self , scan_id ) : requests . post ( self . url + 'scans/{}/launch' . format ( scan_id ) , verify = False , headers = self . headers )
8686	def _decrypt ( self , hexified_value ) : encrypted_value = binascii . unhexlify ( hexified_value ) with warnings . catch_warnings ( ) : warnings . simplefilter ( "ignore" ) jsonified_value = self . cipher . decrypt ( encrypted_value ) . decode ( 'ascii' ) value = json . loads ( jsonified_value ) return value
10164	def get_personalities ( self , line ) : return [ split ( '\W+' , i ) [ 1 ] for i in line . split ( ':' ) [ 1 ] . split ( ' ' ) if i . startswith ( '[' ) ]
13019	def _assemble_select ( self , sql_str , columns , * args , ** kwargs ) : warnings . warn ( "_assemble_select has been depreciated for _assemble_with_columns. It will be removed in a future version." , DeprecationWarning ) return self . _assemble_with_columns ( sql_str , columns , * args , ** kwargs )
5942	def _get_gmx_docs ( self ) : if self . _doc_cache is not None : return self . _doc_cache try : logging . disable ( logging . CRITICAL ) rc , header , docs = self . run ( 'h' , stdout = PIPE , stderr = PIPE , use_input = False ) except : logging . critical ( "Invoking command {0} failed when determining its doc string. Proceed with caution" . format ( self . command_name ) ) self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache finally : logging . disable ( logging . NOTSET ) m = re . match ( self . doc_pattern , docs , re . DOTALL ) if m is None : m = re . match ( self . doc_pattern , header , re . DOTALL ) if m is None : self . _doc_cache = "(No Gromacs documentation available)" return self . _doc_cache self . _doc_cache = m . group ( 'DOCS' ) return self . _doc_cache
5569	def zoom_index_gen ( mp = None , out_dir = None , zoom = None , geojson = False , gpkg = False , shapefile = False , txt = False , vrt = False , fieldname = "location" , basepath = None , for_gdal = True , threading = False , ) : for zoom in get_zoom_levels ( process_zoom_levels = zoom ) : with ExitStack ( ) as es : index_writers = [ ] if geojson : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GeoJSON" , out_path = _index_file_path ( out_dir , zoom , "geojson" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if gpkg : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "GPKG" , out_path = _index_file_path ( out_dir , zoom , "gpkg" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if shapefile : index_writers . append ( es . enter_context ( VectorFileWriter ( driver = "ESRI Shapefile" , out_path = _index_file_path ( out_dir , zoom , "shp" ) , crs = mp . config . output_pyramid . crs , fieldname = fieldname ) ) ) if txt : index_writers . append ( es . enter_context ( TextFileWriter ( out_path = _index_file_path ( out_dir , zoom , "txt" ) ) ) ) if vrt : index_writers . append ( es . enter_context ( VRTFileWriter ( out_path = _index_file_path ( out_dir , zoom , "vrt" ) , output = mp . config . output , out_pyramid = mp . config . output_pyramid ) ) ) logger . debug ( "use the following index writers: %s" , index_writers ) def _worker ( tile ) : tile_path = _tile_path ( orig_path = mp . config . output . get_path ( tile ) , basepath = basepath , for_gdal = for_gdal ) indexes = [ i for i in index_writers if not i . entry_exists ( tile = tile , path = tile_path ) ] if indexes : output_exists = mp . config . output . tiles_exist ( output_tile = tile ) else : output_exists = None return tile , tile_path , indexes , output_exists with concurrent . futures . ThreadPoolExecutor ( ) as executor : for task in concurrent . futures . as_completed ( ( executor . submit ( _worker , i ) for i in mp . config . output_pyramid . tiles_from_geom ( mp . config . area_at_zoom ( zoom ) , zoom ) ) ) : tile , tile_path , indexes , output_exists = task . result ( ) if indexes and output_exists : logger . debug ( "%s exists" , tile_path ) logger . debug ( "write to %s indexes" % len ( indexes ) ) for index in indexes : index . write ( tile , tile_path ) yield tile
9478	def parse_string ( self , string ) : dom = minidom . parseString ( string ) return self . parse_dom ( dom )
678	def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )
940	def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( "--" ) and len ( arg ) > 2 : break if arg . startswith ( "-" ) and len ( arg ) > 1 : break if arg == "." : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( "Empty arg list for option %r expecting one or more args " "(remaining tokens: %r)" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )
13829	def remove ( self , collection , ** kwargs ) : callback = kwargs . pop ( 'callback' ) yield Op ( self . db [ collection ] . remove , kwargs ) callback ( )
12007	def _get_algorithm_info ( self , algorithm_info ) : if algorithm_info [ 'algorithm' ] not in self . ALGORITHMS : raise Exception ( 'Algorithm not supported: %s' % algorithm_info [ 'algorithm' ] ) algorithm = self . ALGORITHMS [ algorithm_info [ 'algorithm' ] ] algorithm_info . update ( algorithm ) return algorithm_info
3705	def Townsend_Hales ( T , Tc , Vc , omega ) : r Tr = T / Tc return Vc / ( 1 + 0.85 * ( 1 - Tr ) + ( 1.692 + 0.986 * omega ) * ( 1 - Tr ) ** ( 1 / 3. ) )
13476	def kill ( self ) : assert self . has_started ( ) , "called kill() on a non-active GeventLoop" self . _stop_event . set ( ) self . _greenlet . kill ( ) self . _clear ( )
6380	def sim_manhattan ( src , tar , qval = 2 , alphabet = None ) : return Manhattan ( ) . sim ( src , tar , qval , alphabet )
6787	def preview ( self , components = None , ask = 0 ) : ask = int ( ask ) self . init ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) print ( '\n%i changes found for host %s.\n' % ( len ( component_order ) , self . genv . host_string ) ) if component_order and plan_funcs : if self . verbose : print ( 'These components have changed:\n' ) for component in sorted ( component_order ) : print ( ( ' ' * 4 ) + component ) print ( 'Deployment plan for host %s:\n' % self . genv . host_string ) for func_name , _ in plan_funcs : print ( success_str ( ( ' ' * 4 ) + func_name ) ) if component_order : print ( ) if ask and self . genv . host_string == self . genv . hosts [ - 1 ] : if component_order : if not raw_input ( 'Begin deployment? [yn] ' ) . strip ( ) . lower ( ) . startswith ( 'y' ) : sys . exit ( 0 ) else : sys . exit ( 0 )
9763	def upload ( sync = True ) : project = ProjectManager . get_config_or_raise ( ) files = IgnoreManager . get_unignored_file_paths ( ) try : with create_tarfile ( files , project . name ) as file_path : with get_files_in_current_directory ( 'repo' , [ file_path ] ) as ( files , files_size ) : try : PolyaxonClient ( ) . project . upload_repo ( project . user , project . name , files , files_size , sync = sync ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not upload code for project `{}`.' . format ( project . name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) Printer . print_error ( 'Check the project exists, ' 'and that you have access rights, ' 'this could happen as well when uploading large files.' 'If you are running a notebook and mounting the code to the notebook, ' 'you should stop it before uploading.' ) sys . exit ( 1 ) Printer . print_success ( 'Files uploaded.' ) except Exception as e : Printer . print_error ( "Could not upload the file." ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 )
9651	def check_shastore_version ( from_store , settings ) : sprint = settings [ "sprint" ] error = settings [ "error" ] sprint ( "checking .shastore version for potential incompatibilities" , level = "verbose" ) if not from_store or 'sake version' not in from_store : errmes = [ "Since you've used this project last, a new version of " , "sake was installed that introduced backwards incompatible" , " changes. Run 'sake clean', and rebuild before continuing\n" ] errmes = " " . join ( errmes ) error ( errmes ) sys . exit ( 1 )
12978	def deleteByPk ( self , pk ) : obj = self . mdl . objects . getOnlyIndexedFields ( pk ) if not obj : return 0 return self . deleteOne ( obj )
8986	def _instructions_changed ( self , change ) : if change . adds ( ) : for index , instruction in change . items ( ) : if isinstance ( instruction , dict ) : in_row = self . _parser . instruction_in_row ( self , instruction ) self . instructions [ index ] = in_row else : instruction . transfer_to_row ( self )
10894	def filtered_image ( self , im ) : q = np . fft . fftn ( im ) for k , v in self . filters : q [ k ] -= v return np . real ( np . fft . ifftn ( q ) )
7408	def worker ( self ) : fullseqs = self . sample_loci ( ) liters = itertools . product ( * self . imap . values ( ) ) hashval = uuid . uuid4 ( ) . hex weights = [ ] for ridx , lidx in enumerate ( liters ) : a , b , c , d = lidx sub = { } for i in lidx : if self . rmap [ i ] == "p1" : sub [ "A" ] = fullseqs [ i ] elif self . rmap [ i ] == "p2" : sub [ "B" ] = fullseqs [ i ] elif self . rmap [ i ] == "p3" : sub [ "C" ] = fullseqs [ i ] else : sub [ "D" ] = fullseqs [ i ] nex = [ ] for tax in list ( "ABCD" ) : nex . append ( ">{} {}" . format ( tax , sub [ tax ] ) ) nsites , nvar = count_var ( nex ) if nvar > self . minsnps : nexus = "{} {}\n" . format ( 4 , len ( fullseqs [ a ] ) ) + "\n" . join ( nex ) treeorder = self . run_tree_inference ( nexus , "{}.{}" . format ( hashval , ridx ) ) weights . append ( treeorder ) rfiles = glob . glob ( os . path . join ( tempfile . tempdir , "*{}*" . format ( hashval ) ) ) for rfile in rfiles : if os . path . exists ( rfile ) : os . remove ( rfile ) trees = [ "ABCD" , "ACBD" , "ADBC" ] wdict = { i : float ( weights . count ( i ) ) / len ( weights ) for i in trees } return wdict
3248	def _get_base ( group , ** conn ) : group [ '_version' ] = 1 group . update ( get_group_api ( group [ 'GroupName' ] , users = False , ** conn ) [ 'Group' ] ) group [ 'CreateDate' ] = get_iso_string ( group [ 'CreateDate' ] ) return group
6439	def euclidean ( src , tar , qval = 2 , normalized = False , alphabet = None ) : return Euclidean ( ) . dist_abs ( src , tar , qval , normalized , alphabet )
8815	def update_interfaces ( self , added_sg , updated_sg , removed_sg ) : if not ( added_sg or updated_sg or removed_sg ) : return with self . sessioned ( ) as session : self . _set_security_groups ( session , added_sg ) self . _unset_security_groups ( session , removed_sg ) combined = added_sg + updated_sg + removed_sg self . _refresh_interfaces ( session , combined )
1034	def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )
8689	def _construct_key ( self , values ) : key = { } for column , value in zip ( self . keys . columns , values ) : key . update ( { column . name : value } ) return key
1146	def copy ( x ) : cls = type ( x ) copier = _copy_dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , "__copy__" , None ) if copier : return copier ( x ) reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(shallow)copyable object of type %s" % cls ) return _reconstruct ( x , rv , 0 )
2506	def get_extr_lic_name ( self , extr_lic ) : extr_name_list = list ( self . graph . triples ( ( extr_lic , self . spdx_namespace [ 'licenseName' ] , None ) ) ) if len ( extr_name_list ) > 1 : self . more_than_one_error ( 'extracted license name' ) return elif len ( extr_name_list ) == 0 : return return self . to_special_value ( extr_name_list [ 0 ] [ 2 ] )
11647	def fit_transform ( self , X , y = None ) : n = X . shape [ 0 ] if X . shape != ( n , n ) : raise TypeError ( "Input must be a square matrix." ) memory = get_memory ( self . memory ) discard_X = not self . copy and self . negatives_likely vals , vecs = memory . cache ( scipy . linalg . eigh , ignore = [ 'overwrite_a' ] ) ( X , overwrite_a = discard_X ) vals = vals [ : , None ] self . clip_ = np . dot ( vecs , np . sign ( vals ) * vecs . T ) if discard_X or vals [ 0 , 0 ] < 0 : del X np . abs ( vals , out = vals ) X = np . dot ( vecs , vals * vecs . T ) del vals , vecs X = Symmetrize ( copy = False ) . fit_transform ( X ) return X
3899	def main ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( 'protofilepath' ) args = parser . parse_args ( ) out_file = compile_protofile ( args . protofilepath ) with open ( out_file , 'rb' ) as proto_file : file_descriptor_set = descriptor_pb2 . FileDescriptorSet . FromString ( proto_file . read ( ) ) for file_descriptor in file_descriptor_set . file : locations = { } for location in file_descriptor . source_code_info . location : locations [ tuple ( location . path ) ] = location print ( make_comment ( 'This file was automatically generated from {} and ' 'should not be edited directly.' . format ( args . protofilepath ) ) ) for index , message_desc in enumerate ( file_descriptor . message_type ) : generate_message_doc ( message_desc , locations , ( 4 , index ) ) for index , enum_desc in enumerate ( file_descriptor . enum_type ) : generate_enum_doc ( enum_desc , locations , ( 5 , index ) )
9977	def fix_lamdaline ( source ) : strio = io . StringIO ( source ) gen = tokenize . generate_tokens ( strio . readline ) tkns = [ ] try : for t in gen : tkns . append ( t ) except tokenize . TokenError : pass lambda_pos = [ ( t . type , t . string ) for t in tkns ] . index ( ( tokenize . NAME , "lambda" ) ) tkns = tkns [ lambda_pos : ] lastop_pos = ( len ( tkns ) - 1 - [ t . type for t in tkns [ : : - 1 ] ] . index ( tokenize . OP ) ) lastop = tkns [ lastop_pos ] fiedlineno = lastop . start [ 0 ] fixedline = lastop . line [ : lastop . start [ 1 ] ] + lastop . line [ lastop . end [ 1 ] : ] tkns = tkns [ : lastop_pos ] fixedlines = "" last_lineno = 0 for t in tkns : if last_lineno == t . start [ 0 ] : continue elif t . start [ 0 ] == fiedlineno : fixedlines += fixedline last_lineno = t . start [ 0 ] else : fixedlines += t . line last_lineno = t . start [ 0 ] return fixedlines
9048	def gradient ( self ) : grad = { } for i , f in enumerate ( self . _covariances ) : for varname , g in f . gradient ( ) . items ( ) : grad [ f"{self._name}[{i}].{varname}" ] = g return grad
6694	def static ( self ) : fn = self . render_to_file ( 'ip/ip_interfaces_static.template' ) r = self . local_renderer r . put ( local_path = fn , remote_path = r . env . interfaces_fn , use_sudo = True )
1090	def iconcat ( a , b ) : "Same as a += b, for a and b sequences." if not hasattr ( a , '__getitem__' ) : msg = "'%s' object can't be concatenated" % type ( a ) . __name__ raise TypeError ( msg ) a += b return a
2373	def statements ( self ) : if len ( self . rows ) == 0 : return [ ] current_statement = Statement ( self . rows [ 0 ] ) current_statement . startline = self . rows [ 0 ] . linenumber current_statement . endline = self . rows [ 0 ] . linenumber statements = [ ] for row in self . rows [ 1 : ] : if len ( row ) > 0 and row [ 0 ] == "..." : current_statement += row [ 1 : ] current_statement . endline = row . linenumber else : if len ( current_statement ) > 0 : statements . append ( current_statement ) current_statement = Statement ( row ) current_statement . startline = row . linenumber current_statement . endline = row . linenumber if len ( current_statement ) > 0 : statements . append ( current_statement ) while ( len ( statements [ - 1 ] ) == 0 or ( ( len ( statements [ - 1 ] ) == 1 ) and len ( statements [ - 1 ] [ 0 ] ) == 0 ) ) : statements . pop ( ) return statements
8300	def handle ( self , data , source = None ) : decoded = decodeOSC ( data ) self . dispatch ( decoded , source )
7288	def get_field_value ( self , field_key ) : def get_value ( document , field_key ) : if document is None : return None current_key , new_key_array = trim_field_key ( document , field_key ) key_array_digit = int ( new_key_array [ - 1 ] ) if new_key_array and has_digit ( new_key_array ) else None new_key = make_key ( new_key_array ) if key_array_digit is not None and len ( new_key_array ) > 0 : if len ( new_key_array ) == 1 : return_data = document . _data . get ( current_key , [ ] ) elif isinstance ( document , BaseList ) : return_list = [ ] if len ( document ) > 0 : return_list = [ get_value ( doc , new_key ) for doc in document ] return_data = return_list else : return_data = get_value ( getattr ( document , current_key ) , new_key ) elif len ( new_key_array ) > 0 : return_data = get_value ( document . _data . get ( current_key ) , new_key ) else : try : return_data = ( document . _data . get ( None , None ) if current_key == "id" else document . _data . get ( current_key , None ) ) except : return_data = document . _data . get ( current_key , None ) return return_data if self . is_initialized : return get_value ( self . model_instance , field_key ) else : return None
13765	def insert ( self , index , value ) : self . _list . insert ( index , value ) self . _sync ( )
4549	def draw_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : _draw_fast_hline ( setter , x + r , y , w - 2 * r , color , aa ) _draw_fast_hline ( setter , x + r , y + h - 1 , w - 2 * r , color , aa ) _draw_fast_vline ( setter , x , y + r , h - 2 * r , color , aa ) _draw_fast_vline ( setter , x + w - 1 , y + r , h - 2 * r , color , aa ) _draw_circle_helper ( setter , x + r , y + r , r , 1 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + r , r , 2 , color , aa ) _draw_circle_helper ( setter , x + w - r - 1 , y + h - r - 1 , r , 4 , color , aa ) _draw_circle_helper ( setter , x + r , y + h - r - 1 , r , 8 , color , aa )
3780	def load_all_methods ( self ) : r methods = [ ] Tmins , Tmaxs = [ ] , [ ] if self . CASRN in [ '7732-18-5' , '67-56-1' , '64-17-5' ] : methods . append ( TEST_METHOD_1 ) self . TEST_METHOD_1_Tmin = 200. self . TEST_METHOD_1_Tmax = 350 self . TEST_METHOD_1_coeffs = [ 1 , .002 ] Tmins . append ( self . TEST_METHOD_1_Tmin ) Tmaxs . append ( self . TEST_METHOD_1_Tmax ) if self . CASRN in [ '67-56-1' ] : methods . append ( TEST_METHOD_2 ) self . TEST_METHOD_2_Tmin = 300. self . TEST_METHOD_2_Tmax = 400 self . TEST_METHOD_2_coeffs = [ 1 , .003 ] Tmins . append ( self . TEST_METHOD_2_Tmin ) Tmaxs . append ( self . TEST_METHOD_2_Tmax ) self . all_methods = set ( methods ) if Tmins and Tmaxs : self . Tmin = min ( Tmins ) self . Tmax = max ( Tmaxs )
1324	def threadFunc ( root ) : th = threading . currentThread ( ) auto . Logger . WriteLine ( '\nThis is running in a new thread. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan ) time . sleep ( 2 ) auto . InitializeUIAutomationInCurrentThread ( ) auto . GetConsoleWindow ( ) . CaptureToImage ( 'console_newthread.png' ) newRoot = auto . GetRootControl ( ) auto . EnumAndLogControl ( newRoot , 1 ) auto . UninitializeUIAutomationInCurrentThread ( ) auto . Logger . WriteLine ( '\nThread exits. {} {}' . format ( th . ident , th . name ) , auto . ConsoleColor . Cyan )
8459	def read_temple_config ( ) : with open ( temple . constants . TEMPLE_CONFIG_FILE ) as temple_config_file : return yaml . load ( temple_config_file , Loader = yaml . SafeLoader )
5307	def rgb_to_ansi16 ( r , g , b , use_bright = False ) : ansi_b = round ( b / 255.0 ) << 2 ansi_g = round ( g / 255.0 ) << 1 ansi_r = round ( r / 255.0 ) ansi = ( 90 if use_bright else 30 ) + ( ansi_b | ansi_g | ansi_r ) return ansi
13064	def make_coins ( self , collection , text , subreference = "" , lang = None ) : if lang is None : lang = self . __default_lang__ return "url_ver=Z39.88-2004" "&ctx_ver=Z39.88-2004" "&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" "&rft_id={cid}" "&rft.genre=bookitem" "&rft.btitle={title}" "&rft.edition={edition}" "&rft.au={author}" "&rft.atitle={pages}" "&rft.language={language}" "&rft.pages={pages}" . format ( title = quote ( str ( text . get_title ( lang ) ) ) , author = quote ( str ( text . get_creator ( lang ) ) ) , cid = url_for ( ".r_collection" , objectId = collection . id , _external = True ) , language = collection . lang , pages = quote ( subreference ) , edition = quote ( str ( text . get_description ( lang ) ) ) )
589	def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( "Invalid argument type \'%s\'. WaitRecord " "must be a number." % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( "Invalid value. autoDetectWaitRecord value " "must be valid record within output stream. Current minimum ROWID in " "output stream is %d." % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords for state in self . saved_states : self . _updateState ( state )
4288	def read_settings ( filename = None ) : logger = logging . getLogger ( __name__ ) logger . info ( "Reading settings ..." ) settings = _DEFAULT_CONFIG . copy ( ) if filename : logger . debug ( "Settings file: %s" , filename ) settings_path = os . path . dirname ( filename ) tempdict = { } with open ( filename ) as f : code = compile ( f . read ( ) , filename , 'exec' ) exec ( code , tempdict ) settings . update ( ( k , v ) for k , v in tempdict . items ( ) if k not in [ '__builtins__' ] ) paths = [ 'source' , 'destination' , 'watermark' ] if os . path . isdir ( join ( settings_path , settings [ 'theme' ] ) ) and os . path . isdir ( join ( settings_path , settings [ 'theme' ] , 'templates' ) ) : paths . append ( 'theme' ) for p in paths : path = settings [ p ] if path and not isabs ( path ) : settings [ p ] = abspath ( normpath ( join ( settings_path , path ) ) ) logger . debug ( "Rewrite %s : %s -> %s" , p , path , settings [ p ] ) for key in ( 'img_size' , 'thumb_size' , 'video_size' ) : w , h = settings [ key ] if h > w : settings [ key ] = ( h , w ) logger . warning ( "The %s setting should be specified with the " "largest value first." , key ) if not settings [ 'img_processor' ] : logger . info ( 'No Processor, images will not be resized' ) logger . debug ( 'Settings:\n%s' , pformat ( settings , width = 120 ) ) return settings
8958	def walk ( self , ** kwargs ) : lead = '' if 'with_root' in kwargs and kwargs . pop ( 'with_root' ) : lead = self . root . rstrip ( os . sep ) + os . sep for base , dirs , files in os . walk ( self . root , ** kwargs ) : prefix = base [ len ( self . root ) : ] . lstrip ( os . sep ) bits = prefix . split ( os . sep ) if prefix else [ ] for dirname in dirs [ : ] : path = '/' . join ( bits + [ dirname ] ) inclusive = self . included ( path , is_dir = True ) if inclusive : yield lead + path + '/' elif inclusive is False : dirs . remove ( dirname ) for filename in files : path = '/' . join ( bits + [ filename ] ) if self . included ( path ) : yield lead + path
2577	def _gather_all_deps ( self , args , kwargs ) : depends = [ ] count = 0 for dep in args : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for key in kwargs : dep = kwargs [ key ] if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) for dep in kwargs . get ( 'inputs' , [ ] ) : if isinstance ( dep , Future ) : if self . tasks [ dep . tid ] [ 'status' ] not in FINAL_STATES : count += 1 depends . extend ( [ dep ] ) return count , depends
12046	def pickle_save ( thing , fname ) : pickle . dump ( thing , open ( fname , "wb" ) , pickle . HIGHEST_PROTOCOL ) return thing
3388	def _bounds_dist ( self , p ) : prob = self . problem lb_dist = ( p - prob . variable_bounds [ 0 , ] ) . min ( ) ub_dist = ( prob . variable_bounds [ 1 , ] - p ) . min ( ) if prob . bounds . shape [ 0 ] > 0 : const = prob . inequalities . dot ( p ) const_lb_dist = ( const - prob . bounds [ 0 , ] ) . min ( ) const_ub_dist = ( prob . bounds [ 1 , ] - const ) . min ( ) lb_dist = min ( lb_dist , const_lb_dist ) ub_dist = min ( ub_dist , const_ub_dist ) return np . array ( [ lb_dist , ub_dist ] )
4886	def update_throttle_scope ( self ) : self . scope = SERVICE_USER_SCOPE self . rate = self . get_rate ( ) self . num_requests , self . duration = self . parse_rate ( self . rate )
10468	def launchAppByBundleId ( bundleID ) : ws = AppKit . NSWorkspace . sharedWorkspace ( ) r = ws . launchAppWithBundleIdentifier_options_additionalEventParamDescriptor_launchIdentifier_ ( bundleID , AppKit . NSWorkspaceLaunchAllowingClassicStartup , AppKit . NSAppleEventDescriptor . nullDescriptor ( ) , None ) if not r [ 0 ] : raise RuntimeError ( 'Error launching specified application.' )
12205	def url_builder ( self , endpoint , * , root = None , params = None , url_params = None ) : if root is None : root = self . ROOT scheme , netloc , path , _ , _ = urlsplit ( root ) return urlunsplit ( ( scheme , netloc , urljoin ( path , endpoint ) , urlencode ( url_params or { } ) , '' , ) ) . format ( ** params or { } )
127	def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( "Cannot compute the polygon's area because it contains less than three points." ) poly = self . to_shapely_polygon ( ) return poly . area
7316	def create_query ( self , attr ) : field = attr [ 0 ] operator = attr [ 1 ] value = attr [ 2 ] model = self . model if '.' in field : field_items = field . split ( '.' ) field_name = getattr ( model , field_items [ 0 ] , None ) class_name = field_name . property . mapper . class_ new_model = getattr ( class_name , field_items [ 1 ] ) return field_name . has ( OPERATORS [ operator ] ( new_model , value ) ) return OPERATORS [ operator ] ( getattr ( model , field , None ) , value )
9314	def _format_datetime ( dttm ) : if dttm . tzinfo is None or dttm . tzinfo . utcoffset ( dttm ) is None : zoned = pytz . utc . localize ( dttm ) else : zoned = dttm . astimezone ( pytz . utc ) ts = zoned . strftime ( "%Y-%m-%dT%H:%M:%S" ) ms = zoned . strftime ( "%f" ) precision = getattr ( dttm , "precision" , None ) if precision == "second" : pass elif precision == "millisecond" : ts = ts + "." + ms [ : 3 ] elif zoned . microsecond > 0 : ts = ts + "." + ms . rstrip ( "0" ) return ts + "Z"
2220	def lookup ( self , data ) : for func in self . lazy_init : func ( ) for type , func in self . func_registry . items ( ) : if isinstance ( data , type ) : return func
11353	def make_user_agent ( component = None ) : packageinfo = pkg_resources . require ( "harvestingkit" ) [ 0 ] useragent = "{0}/{1}" . format ( packageinfo . project_name , packageinfo . version ) if component is not None : useragent += " {0}" . format ( component ) return useragent
2064	def migrate ( self , expression , name_migration_map = None ) : if name_migration_map is None : name_migration_map = { } object_migration_map = { } foreign_vars = itertools . filterfalse ( self . is_declared , get_variables ( expression ) ) for foreign_var in foreign_vars : if foreign_var . name in name_migration_map : migrated_name = name_migration_map [ foreign_var . name ] native_var = self . get_variable ( migrated_name ) assert native_var is not None , "name_migration_map contains a variable that does not exist in this ConstraintSet" object_migration_map [ foreign_var ] = native_var else : migrated_name = foreign_var . name if migrated_name in self . _declarations : migrated_name = self . _make_unique_name ( f'{foreign_var.name}_migrated' ) if isinstance ( foreign_var , Bool ) : new_var = self . new_bool ( name = migrated_name ) elif isinstance ( foreign_var , BitVec ) : new_var = self . new_bitvec ( foreign_var . size , name = migrated_name ) elif isinstance ( foreign_var , Array ) : new_var = self . new_array ( index_max = foreign_var . index_max , index_bits = foreign_var . index_bits , value_bits = foreign_var . value_bits , name = migrated_name ) . array else : raise NotImplemented ( f"Unknown expression type {type(var)} encountered during expression migration" ) object_migration_map [ foreign_var ] = new_var name_migration_map [ foreign_var . name ] = new_var . name migrated_expression = replace ( expression , object_migration_map ) return migrated_expression
9085	def update_backend ( use_pypi = False , index = 'dev' , build = True , user = None , version = None ) : get_vars ( ) if value_asbool ( build ) : upload_backend ( index = index , user = user ) with fab . cd ( '{apphome}' . format ( ** AV ) ) : if value_asbool ( use_pypi ) : command = 'bin/pip install --upgrade briefkasten' else : command = 'bin/pip install --upgrade --pre -i {ploy_default_publish_devpi}/briefkasten/{index}/+simple/ briefkasten' . format ( index = index , user = user , ** AV ) if version : command = '%s==%s' % ( command , version ) fab . sudo ( command ) briefkasten_ctl ( 'restart' )
10439	def stopprocessmonitor ( self , process_name ) : if process_name in self . _process_stats : self . _process_stats [ process_name ] . stop ( ) return 1
10831	def get ( cls , group , admin ) : try : ga = cls . query . filter_by ( group = group , admin_id = admin . get_id ( ) , admin_type = resolve_admin_type ( admin ) ) . one ( ) return ga except Exception : return None
5727	def _get_responses_windows ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : try : self . gdb_process . stdout . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stdout . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stdout . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stdout" ) except IOError : pass try : self . gdb_process . stderr . flush ( ) if PYTHON3 : raw_output = self . gdb_process . stderr . readline ( ) . replace ( b"\r" , b"\n" ) else : raw_output = self . gdb_process . stderr . read ( ) . replace ( b"\r" , b"\n" ) responses += self . _get_responses_list ( raw_output , "stderr" ) except IOError : pass if time . time ( ) > timeout_time_sec : break return responses
4875	def validate_tpa_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : tpa_client = ThirdPartyAuthApiClient ( ) username = tpa_client . get_username_from_remote_id ( enterprise_customer . identity_provider , value ) user = User . objects . get ( username = username ) return models . EnterpriseCustomerUser . objects . get ( user_id = user . id , enterprise_customer = enterprise_customer ) except ( models . EnterpriseCustomerUser . DoesNotExist , User . DoesNotExist ) : pass return None
6636	def publish ( self , registry = None ) : if ( registry is None ) or ( registry == registry_access . Registry_Base_URL ) : if 'private' in self . description and self . description [ 'private' ] : return "this %s is private and cannot be published" % ( self . description_filename . split ( '.' ) [ 0 ] ) upload_archive = os . path . join ( self . path , 'upload.tar.gz' ) fsutils . rmF ( upload_archive ) fd = os . open ( upload_archive , os . O_CREAT | os . O_EXCL | os . O_RDWR | getattr ( os , "O_BINARY" , 0 ) ) with os . fdopen ( fd , 'rb+' ) as tar_file : tar_file . truncate ( ) self . generateTarball ( tar_file ) logger . debug ( 'generated tar file of length %s' , tar_file . tell ( ) ) tar_file . seek ( 0 ) shasum = hashlib . sha256 ( ) while True : chunk = tar_file . read ( 1000 ) if not chunk : break shasum . update ( chunk ) logger . debug ( 'generated tar file has hash %s' , shasum . hexdigest ( ) ) tar_file . seek ( 0 ) with self . findAndOpenReadme ( ) as readme_file_wrapper : if not readme_file_wrapper : logger . warning ( "no readme.md file detected" ) with open ( self . getDescriptionFile ( ) , 'r' ) as description_file : return registry_access . publish ( self . getRegistryNamespace ( ) , self . getName ( ) , self . getVersion ( ) , description_file , tar_file , readme_file_wrapper . file , readme_file_wrapper . extension ( ) . lower ( ) , registry = registry )
12522	def die ( msg , code = - 1 ) : sys . stderr . write ( msg + "\n" ) sys . exit ( code )
6980	def _epd_function ( coeffs , fluxes , xcc , ycc , bgv , bge ) : epdf = ( coeffs [ 0 ] + coeffs [ 1 ] * npsin ( 2 * MPI * xcc ) + coeffs [ 2 ] * npcos ( 2 * MPI * xcc ) + coeffs [ 3 ] * npsin ( 2 * MPI * ycc ) + coeffs [ 4 ] * npcos ( 2 * MPI * ycc ) + coeffs [ 5 ] * npsin ( 4 * MPI * xcc ) + coeffs [ 6 ] * npcos ( 4 * MPI * xcc ) + coeffs [ 7 ] * npsin ( 4 * MPI * ycc ) + coeffs [ 8 ] * npcos ( 4 * MPI * ycc ) + coeffs [ 9 ] * bgv + coeffs [ 10 ] * bge ) return epdf
9531	def value_to_string ( self , obj ) : value = self . value_from_object ( obj ) return b64encode ( self . _dump ( value ) ) . decode ( 'ascii' )
12848	def add_safety_checks ( meta , members ) : for member_name , member_value in members . items ( ) : members [ member_name ] = meta . add_safety_check ( member_name , member_value )
11982	def is_valid_ip ( self , ip ) : if not isinstance ( ip , ( IPv4Address , CIDR ) ) : if str ( ip ) . find ( '/' ) == - 1 : ip = IPv4Address ( ip ) else : ip = CIDR ( ip ) if isinstance ( ip , IPv4Address ) : if ip < self . _first_ip or ip > self . _last_ip : return False elif isinstance ( ip , CIDR ) : if ip . _nm . _ip_dec == 0xFFFFFFFE and self . _nm . _ip_dec != 0xFFFFFFFE : compare_to_first = self . _net_ip . _ip_dec compare_to_last = self . _bc_ip . _ip_dec else : compare_to_first = self . _first_ip . _ip_dec compare_to_last = self . _last_ip . _ip_dec if ip . _first_ip . _ip_dec < compare_to_first or ip . _last_ip . _ip_dec > compare_to_last : return False return True
525	def _inhibitColumnsGlobal ( self , overlaps , density ) : numActive = int ( density * self . _numColumns ) sortedWinnerIndices = numpy . argsort ( overlaps , kind = 'mergesort' ) start = len ( sortedWinnerIndices ) - numActive while start < len ( sortedWinnerIndices ) : i = sortedWinnerIndices [ start ] if overlaps [ i ] >= self . _stimulusThreshold : break else : start += 1 return sortedWinnerIndices [ start : ] [ : : - 1 ]
11956	def is_bin ( ip ) : try : ip = str ( ip ) if len ( ip ) != 32 : return False dec = int ( ip , 2 ) except ( TypeError , ValueError ) : return False if dec > 4294967295 or dec < 0 : return False return True
6286	def supports_file ( cls , meta ) : path = Path ( meta . path ) for ext in cls . file_extensions : if path . suffixes [ : len ( ext ) ] == ext : return True return False
8971	def next ( self , data ) : self . __length += 1 result = self . __kdf . calculate ( self . __key , data , 64 ) self . __key = result [ : 32 ] return result [ 32 : ]
3034	def _oauth2_web_server_flow_params ( kwargs ) : params = { 'access_type' : 'offline' , 'response_type' : 'code' , } params . update ( kwargs ) approval_prompt = params . get ( 'approval_prompt' ) if approval_prompt is not None : logger . warning ( 'The approval_prompt parameter for OAuth2WebServerFlow is ' 'deprecated. Please use the prompt parameter instead.' ) if approval_prompt == 'force' : logger . warning ( 'approval_prompt="force" has been adjusted to ' 'prompt="consent"' ) params [ 'prompt' ] = 'consent' del params [ 'approval_prompt' ] return params
9332	def empty_like ( array , dtype = None ) : array = numpy . asarray ( array ) if dtype is None : dtype = array . dtype return anonymousmemmap ( array . shape , dtype )
5875	def get_images_bytesize_match ( self , images ) : cnt = 0 max_bytes_size = 15728640 good_images = [ ] for image in images : if cnt > 30 : return good_images src = self . parser . getAttribute ( image , attr = 'src' ) src = self . build_image_path ( src ) src = self . add_schema_if_none ( src ) local_image = self . get_local_image ( src ) if local_image : filesize = local_image . bytes if ( filesize == 0 or filesize > self . images_min_bytes ) and filesize < max_bytes_size : good_images . append ( image ) else : images . remove ( image ) cnt += 1 return good_images if len ( good_images ) > 0 else None
12019	def find_imports ( self , pbds ) : imports = list ( set ( self . uses ) . difference ( set ( self . defines ) ) ) for imp in imports : for p in pbds : if imp in p . defines : self . imports . append ( p . name ) break self . imports = list ( set ( self . imports ) ) for import_file in self . imports : self . lines . insert ( 2 , 'import "{}";' . format ( import_file ) )
9068	def _lml_arbitrary_scale ( self ) : s = self . scale D = self . _D n = len ( self . _y ) lml = - self . _df * log2pi - n * log ( s ) lml -= sum ( npsum ( log ( d ) ) for d in D ) d = ( mTQ - yTQ for ( mTQ , yTQ ) in zip ( self . _mTQ , self . _yTQ ) ) lml -= sum ( ( i / j ) @ i for ( i , j ) in zip ( d , D ) ) / s return lml / 2
5332	def get_panels ( config ) : task = TaskPanels ( config ) task . execute ( ) task = TaskPanelsMenu ( config ) task . execute ( ) logging . info ( "Panels creation finished!" )
3233	def list_rules ( client = None , ** kwargs ) : result = client . list_rules ( ** kwargs ) if not result . get ( "Rules" ) : result . update ( { "Rules" : [ ] } ) return result
6502	def strings_in_dictionary ( dictionary ) : strings = [ value for value in six . itervalues ( dictionary ) if not isinstance ( value , dict ) ] for child_dict in [ dv for dv in six . itervalues ( dictionary ) if isinstance ( dv , dict ) ] : strings . extend ( SearchResultProcessor . strings_in_dictionary ( child_dict ) ) return strings
2631	def _status ( self ) : job_id_list = ' ' . join ( self . resources . keys ( ) ) cmd = "condor_q {0} -af:jr JobStatus" . format ( job_id_list ) retcode , stdout , stderr = super ( ) . execute_wait ( cmd ) for line in stdout . strip ( ) . split ( '\n' ) : parts = line . split ( ) job_id = parts [ 0 ] status = translate_table . get ( parts [ 1 ] , 'UNKNOWN' ) self . resources [ job_id ] [ 'status' ] = status
5804	def detect_client_auth_request ( server_handshake_bytes ) : for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type == b'\x0d' : return True return False
1272	def create_distributions ( self ) : distributions = dict ( ) for name in sorted ( self . actions_spec ) : action = self . actions_spec [ name ] if self . distributions_spec is not None and name in self . distributions_spec : kwargs = dict ( action ) kwargs [ 'scope' ] = name kwargs [ 'summary_labels' ] = self . summary_labels distributions [ name ] = Distribution . from_spec ( spec = self . distributions_spec [ name ] , kwargs = kwargs ) elif action [ 'type' ] == 'bool' : distributions [ name ] = Bernoulli ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'int' : distributions [ name ] = Categorical ( shape = action [ 'shape' ] , num_actions = action [ 'num_actions' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'float' : if 'min_value' in action : distributions [ name ] = Beta ( shape = action [ 'shape' ] , min_value = action [ 'min_value' ] , max_value = action [ 'max_value' ] , scope = name , summary_labels = self . summary_labels ) else : distributions [ name ] = Gaussian ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) return distributions
1185	def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , "next" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished
4271	def get_exif_tags ( data , datetime_format = '%c' ) : logger = logging . getLogger ( __name__ ) simple = { } for tag in ( 'Model' , 'Make' , 'LensModel' ) : if tag in data : if isinstance ( data [ tag ] , tuple ) : simple [ tag ] = data [ tag ] [ 0 ] . strip ( ) else : simple [ tag ] = data [ tag ] . strip ( ) if 'FNumber' in data : fnumber = data [ 'FNumber' ] try : simple [ 'fstop' ] = float ( fnumber [ 0 ] ) / fnumber [ 1 ] except Exception : logger . debug ( 'Skipped invalid FNumber: %r' , fnumber , exc_info = True ) if 'FocalLength' in data : focal = data [ 'FocalLength' ] try : simple [ 'focal' ] = round ( float ( focal [ 0 ] ) / focal [ 1 ] ) except Exception : logger . debug ( 'Skipped invalid FocalLength: %r' , focal , exc_info = True ) if 'ExposureTime' in data : exptime = data [ 'ExposureTime' ] if isinstance ( exptime , tuple ) : try : simple [ 'exposure' ] = str ( fractions . Fraction ( exptime [ 0 ] , exptime [ 1 ] ) ) except ZeroDivisionError : logger . info ( 'Invalid ExposureTime: %r' , exptime ) elif isinstance ( exptime , int ) : simple [ 'exposure' ] = str ( exptime ) else : logger . info ( 'Unknown format for ExposureTime: %r' , exptime ) if data . get ( 'ISOSpeedRatings' ) : simple [ 'iso' ] = data [ 'ISOSpeedRatings' ] if 'DateTimeOriginal' in data : date = data [ 'DateTimeOriginal' ] . rsplit ( '\x00' ) [ 0 ] try : simple [ 'dateobj' ] = datetime . strptime ( date , '%Y:%m:%d %H:%M:%S' ) simple [ 'datetime' ] = simple [ 'dateobj' ] . strftime ( datetime_format ) except ( ValueError , TypeError ) as e : logger . info ( 'Could not parse DateTimeOriginal: %s' , e ) if 'GPSInfo' in data : info = data [ 'GPSInfo' ] lat_info = info . get ( 'GPSLatitude' ) lon_info = info . get ( 'GPSLongitude' ) lat_ref_info = info . get ( 'GPSLatitudeRef' ) lon_ref_info = info . get ( 'GPSLongitudeRef' ) if lat_info and lon_info and lat_ref_info and lon_ref_info : try : lat = dms_to_degrees ( lat_info ) lon = dms_to_degrees ( lon_info ) except ( ZeroDivisionError , ValueError , TypeError ) : logger . info ( 'Failed to read GPS info' ) else : simple [ 'gps' ] = { 'lat' : - lat if lat_ref_info != 'N' else lat , 'lon' : - lon if lon_ref_info != 'E' else lon , } return simple
13750	def one_to_many ( clsname , ** kw ) : @ declared_attr def o2m ( cls ) : cls . _references ( ( clsname , cls . __name__ ) ) return relationship ( clsname , ** kw ) return o2m
567	def __validateExperimentControl ( self , control ) : taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , "opfTaskSchema.json" ) validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , "taskLabel type: %r" % type ( taskLabel ) assert len ( taskLabel ) > 0 , "empty string taskLabel not is allowed" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , "Duplcate task labels are not allowed: %s" % taskLabelDuplicates return
6156	def stereo_FM ( x , fs = 2.4e6 , file_name = 'test.wav' ) : N1 = 10 b = signal . firwin ( 64 , 2 * 200e3 / float ( fs ) ) y = signal . lfilter ( b , 1 , x ) z = ss . downsample ( y , N1 ) z_bb = discrim ( z ) b12 = signal . firwin ( 128 , 2 * 12e3 / ( float ( fs ) / N1 ) ) y_lpr = signal . lfilter ( b12 , 1 , z_bb ) b19 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 19 - 5 , 19 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) z_bb19 = signal . lfilter ( b19 , 1 , z_bb ) theta , phi_error = pilot_PLL ( z_bb19 , 19000 , fs / N1 , 2 , 10 , 0.707 ) b38 = signal . firwin ( 128 , 2 * 1e3 * np . array ( [ 38 - 5 , 38 + 5 ] ) / ( float ( fs ) / N1 ) , pass_zero = False ) x_lmr = signal . lfilter ( b38 , 1 , z_bb ) x_lmr = 2 * np . sqrt ( 2 ) * np . cos ( 2 * theta ) * x_lmr y_lmr = signal . lfilter ( b12 , 1 , x_lmr ) y_left = y_lpr + y_lmr y_right = y_lpr - y_lmr N2 = 5 fs2 = float ( fs ) / ( N1 * N2 ) y_left_DN2 = ss . downsample ( y_left , N2 ) y_right_DN2 = ss . downsample ( y_right , N2 ) a_de = np . exp ( - 2.1 * 1e3 * 2 * np . pi / fs2 ) z_left = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_left_DN2 ) z_right = signal . lfilter ( [ 1 - a_de ] , [ 1 , - a_de ] , y_right_DN2 ) z_out = np . hstack ( ( np . array ( [ z_left ] ) . T , ( np . array ( [ z_right ] ) . T ) ) ) ss . to_wav ( file_name , 48000 , z_out / 2 ) print ( 'Done!' ) return z_bb , theta , y_lpr , y_lmr , z_out
13505	def exists ( self , server ) : try : server . get ( 'challenge' , replacements = { 'slug' : self . slug } ) except Exception : return False return True
2078	def associate_notification_template ( self , job_template , notification_template , status ) : return self . _assoc ( 'notification_templates_%s' % status , job_template , notification_template )
11067	def create_acl ( self , name ) : if name in self . _acl : return False self . _acl [ name ] = { 'allow' : [ ] , 'deny' : [ ] } return True
5013	def _call_search_students_recursively ( self , sap_search_student_url , all_inactive_learners , page_size , start_at ) : search_student_paginated_url = '{sap_search_student_url}&{pagination_criterion}' . format ( sap_search_student_url = sap_search_student_url , pagination_criterion = '$count=true&$top={page_size}&$skip={start_at}' . format ( page_size = page_size , start_at = start_at , ) , ) try : response = self . session . get ( search_student_paginated_url ) sap_inactive_learners = response . json ( ) except ( ConnectionError , Timeout ) : LOGGER . warning ( 'Unable to fetch inactive learners from SAP searchStudent API with url ' '"{%s}".' , search_student_paginated_url , ) return None if 'error' in sap_inactive_learners : LOGGER . warning ( 'SAP searchStudent API for customer %s and base url %s returned response with ' 'error message "%s" and with error code "%s".' , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . sapsf_base_url , sap_inactive_learners [ 'error' ] . get ( 'message' ) , sap_inactive_learners [ 'error' ] . get ( 'code' ) , ) return None new_page_start_at = page_size + start_at all_inactive_learners += sap_inactive_learners [ 'value' ] if sap_inactive_learners [ '@odata.count' ] > new_page_start_at : return self . _call_search_students_recursively ( sap_search_student_url , all_inactive_learners , page_size = page_size , start_at = new_page_start_at , ) return all_inactive_learners
11916	def render_to ( self , path , template , ** data ) : html = self . render ( template , ** data ) with open ( path , 'w' ) as f : f . write ( html . encode ( charset ) )
4451	def search ( self , query ) : args , query = self . _mk_query_args ( query ) st = time . time ( ) res = self . redis . execute_command ( self . SEARCH_CMD , * args ) return Result ( res , not query . _no_content , duration = ( time . time ( ) - st ) * 1000.0 , has_payload = query . _with_payloads )
10174	def set_bookmark ( self ) : def _success_date ( ) : bookmark = { 'date' : self . new_bookmark or datetime . datetime . utcnow ( ) . strftime ( self . doc_id_suffix ) } yield dict ( _index = self . last_index_written , _type = self . bookmark_doc_type , _source = bookmark ) if self . last_index_written : bulk ( self . client , _success_date ( ) , stats_only = True )
184	def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance
3628	def pad_cells ( table ) : col_sizes = [ max ( map ( len , col ) ) for col in zip ( * table ) ] for row in table : for cell_num , cell in enumerate ( row ) : row [ cell_num ] = pad_to ( cell , col_sizes [ cell_num ] ) return table
10003	def rename ( self , name ) : self . _impl . system . rename_model ( new_name = name , old_name = self . name )
112	def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )
8094	def edges ( s , edges , alpha = 1.0 , weighted = False , directed = False ) : p = s . _ctx . BezierPath ( ) if directed and s . stroke : pd = s . _ctx . BezierPath ( ) if weighted and s . fill : pw = [ s . _ctx . BezierPath ( ) for i in range ( 11 ) ] if len ( edges ) == 0 : return for e in edges : try : s2 = e . node1 . graph . styles [ e . node1 . style ] except : s2 = s if s2 . edge : s2 . edge ( s2 , p , e , alpha ) if directed and s . stroke : s2 . edge_arrow ( s2 , pd , e , radius = 10 ) if weighted and s . fill : s2 . edge ( s2 , pw [ int ( e . weight * 10 ) ] , e , alpha ) s . _ctx . autoclosepath ( False ) s . _ctx . nofill ( ) s . _ctx . nostroke ( ) if weighted and s . fill : r = e . node1 . __class__ ( None ) . r s . _ctx . stroke ( s . fill . r , s . fill . g , s . fill . b , s . fill . a * 0.65 * alpha ) for w in range ( 1 , len ( pw ) ) : s . _ctx . strokewidth ( r * w * 0.1 ) s . _ctx . drawpath ( pw [ w ] . copy ( ) ) if s . stroke : s . _ctx . strokewidth ( s . strokewidth ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) s . _ctx . drawpath ( p . copy ( ) ) if directed and s . stroke : clr = s . _ctx . color ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a * 0.65 * alpha ) clr . a *= 1.3 s . _ctx . stroke ( clr ) s . _ctx . drawpath ( pd . copy ( ) ) for e in edges : try : s2 = self . styles [ e . node1 . style ] except : s2 = s if s2 . edge_label : s2 . edge_label ( s2 , e , alpha )
3543	def python_source_files ( path , tests_dirs ) : if isdir ( path ) : for root , dirs , files in os . walk ( path ) : dirs [ : ] = [ d for d in dirs if os . path . join ( root , d ) not in tests_dirs ] for filename in files : if filename . endswith ( '.py' ) : yield os . path . join ( root , filename ) else : yield path
11986	async def _upload_file ( self , full_path ) : rel_path = os . path . relpath ( full_path , self . folder ) key = s3_key ( os . path . join ( self . key , rel_path ) ) ct = self . content_types . get ( key . split ( '.' ) [ - 1 ] ) with open ( full_path , 'rb' ) as fp : file = fp . read ( ) try : await self . botocore . upload_file ( self . bucket , file , key = key , ContentType = ct ) except Exception as exc : LOGGER . error ( 'Could not upload "%s": %s' , key , exc ) self . failures [ key ] = self . all . pop ( full_path ) return size = self . all . pop ( full_path ) self . success [ key ] = size self . total_size += size percentage = 100 * ( 1 - len ( self . all ) / self . total_files ) message = '{0:.0f}% completed - uploaded "{1}" - {2}' . format ( percentage , key , convert_bytes ( size ) ) LOGGER . info ( message )
8203	def set_size ( self , size ) : if self . size is None : self . size = size return size else : return self . size
3303	def _read_config_file ( config_file , verbose ) : config_file = os . path . abspath ( config_file ) if not os . path . exists ( config_file ) : raise RuntimeError ( "Couldn't open configuration file '{}'." . format ( config_file ) ) if config_file . endswith ( ".json" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as json_file : minified = jsmin ( json_file . read ( ) ) conf = json . loads ( minified ) elif config_file . endswith ( ".yaml" ) : with io . open ( config_file , mode = "r" , encoding = "utf-8" ) as yaml_file : conf = yaml . safe_load ( yaml_file ) else : try : import imp conf = { } configmodule = imp . load_source ( "configuration_module" , config_file ) for k , v in vars ( configmodule ) . items ( ) : if k . startswith ( "__" ) : continue elif isfunction ( v ) : continue conf [ k ] = v except Exception : exc_type , exc_value = sys . exc_info ( ) [ : 2 ] exc_info_list = traceback . format_exception_only ( exc_type , exc_value ) exc_text = "\n" . join ( exc_info_list ) print ( "Failed to read configuration file: " + config_file + "\nDue to " + exc_text , file = sys . stderr , ) raise conf [ "_config_file" ] = config_file return conf
341	def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s
4996	def default_content_filter ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and not instance . content_filter : instance . content_filter = get_default_catalog_content_filter ( ) instance . save ( )
6700	def apt_key_exists ( keyid ) : gpg_cmd = 'gpg --ignore-time-conflict --no-options --no-default-keyring --keyring /etc/apt/trusted.gpg' with settings ( hide ( 'everything' ) , warn_only = True ) : res = run ( '%(gpg_cmd)s --fingerprint %(keyid)s' % locals ( ) ) return res . succeeded
4158	def ma ( X , Q , M ) : if Q <= 0 or Q >= M : raise ValueError ( 'Q(MA) must be in ]0,lag[' ) a , rho , _c = yulewalker . aryule ( X , M , 'biased' ) a = np . insert ( a , 0 , 1 ) ma_params , _p , _c = yulewalker . aryule ( a , Q , 'biased' ) return ma_params , rho
3528	def get_required_setting ( setting , value_re , invalid_msg ) : try : value = getattr ( settings , setting ) except AttributeError : raise AnalyticalException ( "%s setting: not found" % setting ) if not value : raise AnalyticalException ( "%s setting is not set" % setting ) value = str ( value ) if not value_re . search ( value ) : raise AnalyticalException ( "%s setting: %s: '%s'" % ( setting , invalid_msg , value ) ) return value
10046	def extract_actions_from_class ( record_class ) : for name in dir ( record_class ) : method = getattr ( record_class , name , None ) if method and getattr ( method , '__deposit_action__' , False ) : yield method . __name__
10453	def waittillguinotexist ( self , window_name , object_name = '' , guiTimeOut = 30 ) : timeout = 0 while timeout < guiTimeOut : if not self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
1303	def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )
13043	def create_pipe_workers ( configfile , directory ) : type_map = { 'service' : ServiceSearch , 'host' : HostSearch , 'range' : RangeSearch , 'user' : UserSearch } config = configparser . ConfigParser ( ) config . read ( configfile ) if not len ( config . sections ( ) ) : print_error ( "No named pipes configured" ) return print_notification ( "Starting {} pipes in directory {}" . format ( len ( config . sections ( ) ) , directory ) ) workers = [ ] for name in config . sections ( ) : section = config [ name ] query = create_query ( section ) object_type = type_map [ section [ 'type' ] ] args = ( name , os . path . join ( directory , name ) , object_type , query , section [ 'format' ] , bool ( section . get ( 'unique' , 0 ) ) ) workers . append ( multiprocessing . Process ( target = pipe_worker , args = args ) ) return workers
11671	def _flann_args ( self , X = None ) : "The dictionary of arguments to give to FLANN." args = { 'cores' : self . _n_jobs } if self . flann_algorithm == 'auto' : if X is None or X . dim > 5 : args [ 'algorithm' ] = 'linear' else : args [ 'algorithm' ] = 'kdtree_single' else : args [ 'algorithm' ] = self . flann_algorithm if self . flann_args : args . update ( self . flann_args ) try : FLANNParameters ( ) . update ( args ) except AttributeError as e : msg = "flann_args contains an invalid argument:\n {}" raise TypeError ( msg . format ( e ) ) return args
9910	def confirm ( self ) : self . email . is_verified = True self . email . save ( ) signals . email_verified . send ( email = self . email , sender = self . __class__ ) logger . info ( "Verified email address: %s" , self . email . email )
5597	def is_on_edge ( self ) : return ( self . left <= self . tile_pyramid . left or self . bottom <= self . tile_pyramid . bottom or self . right >= self . tile_pyramid . right or self . top >= self . tile_pyramid . top )
8847	def mouseMoveEvent ( self , e ) : super ( PyInteractiveConsole , self ) . mouseMoveEvent ( e ) cursor = self . cursorForPosition ( e . pos ( ) ) assert isinstance ( cursor , QtGui . QTextCursor ) p = cursor . positionInBlock ( ) usd = cursor . block ( ) . userData ( ) if usd and usd . start_pos_in_block <= p <= usd . end_pos_in_block : if QtWidgets . QApplication . overrideCursor ( ) is None : QtWidgets . QApplication . setOverrideCursor ( QtGui . QCursor ( QtCore . Qt . PointingHandCursor ) ) else : if QtWidgets . QApplication . overrideCursor ( ) is not None : QtWidgets . QApplication . restoreOverrideCursor ( )
3407	def eval_gpr ( expr , knockouts ) : if isinstance ( expr , Expression ) : return eval_gpr ( expr . body , knockouts ) elif isinstance ( expr , Name ) : return expr . id not in knockouts elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : return any ( eval_gpr ( i , knockouts ) for i in expr . values ) elif isinstance ( op , And ) : return all ( eval_gpr ( i , knockouts ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name__ ) elif expr is None : return True else : raise TypeError ( "unsupported operation " + repr ( expr ) )
6758	def set_site_specifics ( self , site ) : r = self . local_renderer site_data = self . genv . sites [ site ] . copy ( ) r . env . site = site if self . verbose : print ( 'set_site_specifics.data:' ) pprint ( site_data , indent = 4 ) local_ns = { } for k , v in list ( site_data . items ( ) ) : if k . startswith ( self . name + '_' ) : _k = k [ len ( self . name + '_' ) : ] local_ns [ _k ] = v del site_data [ k ] r . env . update ( local_ns ) r . env . update ( site_data )
10828	def delete ( cls , group , user ) : with db . session . begin_nested ( ) : cls . query . filter_by ( group = group , user_id = user . get_id ( ) ) . delete ( )
4990	def get ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : try : course_run_id = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : context_data = get_global_context ( request , enterprise_customer ) error_code = 'ENTRV000' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and program_uuid {program_uuid} ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) kwargs [ 'course_id' ] = course_run_id with transaction . atomic ( ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = request . user . id ) enterprise_customer_user . update_session ( request ) resource_id = course_run_id or program_uuid if self . eligible_for_direct_audit_enrollment ( request , enterprise_customer , resource_id , course_key ) : try : enterprise_customer_user . enroll ( resource_id , 'audit' , cohort = request . GET . get ( 'cohort' , None ) ) track_enrollment ( 'direct-audit-enrollment' , request . user . id , resource_id , request . get_full_path ( ) ) except ( CourseEnrollmentDowngradeError , CourseEnrollmentPermissionError ) : pass return redirect ( LMS_COURSEWARE_URL . format ( course_id = resource_id ) ) return self . redirect ( request , * args , ** kwargs )
2862	def _i2c_write_bytes ( self , data ) : for byte in data : self . _command . append ( str ( bytearray ( ( 0x11 , 0x00 , 0x00 , byte ) ) ) ) self . _ft232h . output_pins ( { 0 : GPIO . LOW , 1 : GPIO . HIGH } , write = False ) self . _command . append ( self . _ft232h . mpsse_gpio ( ) * _REPEAT_DELAY ) self . _command . append ( '\x22\x00' ) self . _expected += len ( data )
6615	def receive_all ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . receive ( )
3776	def T_dependent_property_derivative ( self , T , order = 1 ) : r if self . method : if self . test_method_validity ( T , self . method ) : try : return self . calculate_derivative ( T , self . method , order ) except : pass sorted_valid_methods = self . select_valid_methods ( T ) for method in sorted_valid_methods : try : return self . calculate_derivative ( T , method , order ) except : pass return None
7474	def write_to_fullarr ( data , sample , sidx ) : LOGGER . info ( "writing fullarr %s %s" , sample . name , sidx ) with h5py . File ( data . clust_database , 'r+' ) as io5 : chunk = io5 [ "catgs" ] . attrs [ "chunksize" ] [ 0 ] catg = io5 [ "catgs" ] nall = io5 [ "nalleles" ] smpio = os . path . join ( data . dirs . across , sample . name + '.tmp.h5' ) with h5py . File ( smpio ) as indat : newcatg = indat [ "icatg" ] onall = indat [ "inall" ] for cidx in xrange ( 0 , catg . shape [ 0 ] , chunk ) : end = cidx + chunk catg [ cidx : end , sidx : sidx + 1 , : ] = np . expand_dims ( newcatg [ cidx : end , : ] , axis = 1 ) nall [ : , sidx : sidx + 1 ] = np . expand_dims ( onall , axis = 1 )
676	def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID
3256	def mosaic_coverages ( self , store ) : params = dict ( ) url = build_url ( self . service_url , [ "workspaces" , store . workspace . name , "coveragestores" , store . name , "coverages.json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to get mosaic coverages {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp . json ( )
6205	def populations_slices ( particles , num_pop_list ) : slices = [ ] i_prev = 0 for num_pop in num_pop_list : slices . append ( slice ( i_prev , i_prev + num_pop ) ) i_prev += num_pop return slices
12037	def matrixValues ( matrix , key ) : assert key in matrix . dtype . names col = matrix . dtype . names . index ( key ) values = np . empty ( len ( matrix ) ) * np . nan for i in range ( len ( matrix ) ) : values [ i ] = matrix [ i ] [ col ] return values
11928	def get_files_stat ( self ) : if not exists ( Post . src_dir ) : logger . error ( SourceDirectoryNotFound . __doc__ ) sys . exit ( SourceDirectoryNotFound . exit_code ) paths = [ ] for fn in ls ( Post . src_dir ) : if fn . endswith ( src_ext ) : paths . append ( join ( Post . src_dir , fn ) ) if exists ( config . filepath ) : paths . append ( config . filepath ) files = dict ( ( p , stat ( p ) . st_mtime ) for p in paths ) return files
7765	def disconnect ( self ) : with self . lock : if self . stream : if self . settings [ u"initial_presence" ] : self . send ( Presence ( stanza_type = "unavailable" ) ) self . stream . disconnect ( )
6016	def signal_to_noise_map ( self ) : signal_to_noise_map = np . divide ( self . image , self . noise_map ) signal_to_noise_map [ signal_to_noise_map < 0 ] = 0 return signal_to_noise_map
4477	def norm_remote_path ( path ) : path = os . path . normpath ( path ) if path . startswith ( os . path . sep ) : return path [ 1 : ] else : return path
5771	def _advapi32_verify ( certificate_or_public_key , signature , data , hash_algorithm , rsa_pss_padding = False ) : algo = certificate_or_public_key . algorithm if algo == 'rsa' and rsa_pss_padding : hash_length = { 'sha1' : 20 , 'sha224' : 28 , 'sha256' : 32 , 'sha384' : 48 , 'sha512' : 64 } . get ( hash_algorithm , 0 ) decrypted_signature = raw_rsa_public_crypt ( certificate_or_public_key , signature ) key_size = certificate_or_public_key . bit_size if not verify_pss_padding ( hash_algorithm , hash_length , key_size , data , decrypted_signature ) : raise SignatureError ( 'Signature is invalid' ) return if algo == 'rsa' and hash_algorithm == 'raw' : padded_plaintext = raw_rsa_public_crypt ( certificate_or_public_key , signature ) try : plaintext = remove_pkcs1v15_signature_padding ( certificate_or_public_key . byte_size , padded_plaintext ) if not constant_compare ( plaintext , data ) : raise ValueError ( ) except ( ValueError ) : raise SignatureError ( 'Signature is invalid' ) return hash_handle = None try : alg_id = { 'md5' : Advapi32Const . CALG_MD5 , 'sha1' : Advapi32Const . CALG_SHA1 , 'sha256' : Advapi32Const . CALG_SHA_256 , 'sha384' : Advapi32Const . CALG_SHA_384 , 'sha512' : Advapi32Const . CALG_SHA_512 , } [ hash_algorithm ] hash_handle_pointer = new ( advapi32 , 'HCRYPTHASH *' ) res = advapi32 . CryptCreateHash ( certificate_or_public_key . context_handle , alg_id , null ( ) , 0 , hash_handle_pointer ) handle_error ( res ) hash_handle = unwrap ( hash_handle_pointer ) res = advapi32 . CryptHashData ( hash_handle , data , len ( data ) , 0 ) handle_error ( res ) if algo == 'dsa' : try : signature = algos . DSASignature . load ( signature ) . to_p1363 ( ) half_len = len ( signature ) // 2 signature = signature [ half_len : ] + signature [ : half_len ] except ( ValueError , OverflowError , TypeError ) : raise SignatureError ( 'Signature is invalid' ) reversed_signature = signature [ : : - 1 ] res = advapi32 . CryptVerifySignatureW ( hash_handle , reversed_signature , len ( signature ) , certificate_or_public_key . key_handle , null ( ) , 0 ) handle_error ( res ) finally : if hash_handle : advapi32 . CryptDestroyHash ( hash_handle )
6884	def find_lc_timegroups ( lctimes , mingap = 4.0 ) : lc_time_diffs = [ ( lctimes [ x ] - lctimes [ x - 1 ] ) for x in range ( 1 , len ( lctimes ) ) ] lc_time_diffs = np . array ( lc_time_diffs ) group_start_indices = np . where ( lc_time_diffs > mingap ) [ 0 ] if len ( group_start_indices ) > 0 : group_indices = [ ] for i , gindex in enumerate ( group_start_indices ) : if i == 0 : group_indices . append ( slice ( 0 , gindex + 1 ) ) else : group_indices . append ( slice ( group_start_indices [ i - 1 ] + 1 , gindex + 1 ) ) group_indices . append ( slice ( group_start_indices [ - 1 ] + 1 , len ( lctimes ) ) ) else : group_indices = [ slice ( 0 , len ( lctimes ) ) ] return len ( group_indices ) , group_indices
5729	def main ( verbose = True ) : find_executable ( MAKE_CMD ) if not find_executable ( MAKE_CMD ) : print ( 'Could not find executable "%s". Ensure it is installed and on your $PATH.' % MAKE_CMD ) exit ( 1 ) subprocess . check_output ( [ MAKE_CMD , "-C" , SAMPLE_C_CODE_DIR , "--quiet" ] ) gdbmi = GdbController ( verbose = verbose ) responses = gdbmi . write ( "-file-exec-and-symbols %s" % SAMPLE_C_BINARY ) responses = gdbmi . write ( "-file-list-exec-source-files" ) responses = gdbmi . write ( "-break-insert main" ) responses = gdbmi . write ( "-exec-run" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-next" ) responses = gdbmi . write ( "-exec-continue" ) gdbmi . exit ( )
10869	def calc_pts_hg ( npts = 20 ) : pts_hg , wts_hg = np . polynomial . hermite . hermgauss ( npts * 2 ) pts_hg = pts_hg [ npts : ] wts_hg = wts_hg [ npts : ] * np . exp ( pts_hg * pts_hg ) return pts_hg , wts_hg
10264	def collapse_orthologies_by_namespace ( graph : BELGraph , victim_namespace : Strings , survivor_namespace : str ) -> None : _collapse_edge_by_namespace ( graph , victim_namespace , survivor_namespace , ORTHOLOGOUS )
9440	def request ( self , path , method = None , data = { } ) : if not path : raise ValueError ( 'Invalid path parameter' ) if method and method not in [ 'GET' , 'POST' , 'DELETE' , 'PUT' ] : raise NotImplementedError ( 'HTTP %s method not implemented' % method ) if path [ 0 ] == '/' : uri = self . url + path else : uri = self . url + '/' + path if APPENGINE : return json . loads ( self . _appengine_fetch ( uri , data , method ) ) return json . loads ( self . _urllib2_fetch ( uri , data , method ) )
2746	def edit ( self ) : input_params = { "name" : self . name , "public_key" : self . public_key , } data = self . get_data ( "account/keys/%s" % self . id , type = PUT , params = input_params ) if data : self . id = data [ 'ssh_key' ] [ 'id' ]
388	def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out
724	def getDataRowCount ( self ) : inputRowCountAfterAggregation = 0 while True : record = self . getNextRecord ( ) if record is None : return inputRowCountAfterAggregation inputRowCountAfterAggregation += 1 if inputRowCountAfterAggregation > 10000 : raise RuntimeError ( 'No end of datastream found.' )
3864	async def send_message ( self , segments , image_file = None , image_id = None , image_user_id = None ) : async with self . _send_message_lock : if image_file : try : uploaded_image = await self . _client . upload_image ( image_file , return_uploaded_image = True ) except exceptions . NetworkError as e : logger . warning ( 'Failed to upload image: {}' . format ( e ) ) raise image_id = uploaded_image . image_id try : request = hangouts_pb2 . SendChatMessageRequest ( request_header = self . _client . get_request_header ( ) , event_request_header = self . _get_event_request_header ( ) , message_content = hangouts_pb2 . MessageContent ( segment = [ seg . serialize ( ) for seg in segments ] , ) , ) if image_id is not None : request . existing_media . photo . photo_id = image_id if image_user_id is not None : request . existing_media . photo . user_id = image_user_id request . existing_media . photo . is_custom_user_id = True await self . _client . send_chat_message ( request ) except exceptions . NetworkError as e : logger . warning ( 'Failed to send message: {}' . format ( e ) ) raise
9738	def get_3d_markers_no_label_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabelResidual , component_info , data , component_position )
5852	def get_dataset_file ( self , dataset_id , file_path , version = None ) : return self . get_dataset_files ( dataset_id , "^{}$" . format ( file_path ) , version_number = version ) [ 0 ]
6739	def check_settings_for_differences ( old , new , as_bool = False , as_tri = False ) : assert not as_bool or not as_tri old = old or { } new = new or { } changes = set ( k for k in set ( new . iterkeys ( ) ) . intersection ( old . iterkeys ( ) ) if new [ k ] != old [ k ] ) if changes and as_bool : return True added_keys = set ( new . iterkeys ( ) ) . difference ( old . iterkeys ( ) ) if added_keys and as_bool : return True if not as_tri : changes . update ( added_keys ) deled_keys = set ( old . iterkeys ( ) ) . difference ( new . iterkeys ( ) ) if deled_keys and as_bool : return True if as_bool : return False if not as_tri : changes . update ( deled_keys ) if as_tri : return added_keys , changes , deled_keys return changes
4507	def get_device ( self , id = None ) : if id is None : if not self . devices : raise ValueError ( 'No default device for %s' % self . hardware_id ) id , ( device , version ) = sorted ( self . devices . items ( ) ) [ 0 ] elif id in self . devices : device , version = self . devices [ id ] else : error = 'Unable to find device with ID %s' % id log . error ( error ) raise ValueError ( error ) log . info ( "Using COM Port: %s, Device ID: %s, Device Ver: %s" , device , id , version ) return id , device , version
858	def _getStartRow ( self , bookmark ) : bookMarkDict = json . loads ( bookmark ) realpath = os . path . realpath ( self . _filename ) bookMarkFile = bookMarkDict . get ( 'filepath' , None ) if bookMarkFile != realpath : print ( "Ignoring bookmark due to mismatch between File's " "filename realpath vs. bookmark; realpath: %r; bookmark: %r" ) % ( realpath , bookMarkDict ) return 0 else : return bookMarkDict [ 'currentRow' ]
13856	def getbalance ( self , url = 'http://services.ambientmobile.co.za/credits' ) : postXMLList = [ ] postXMLList . append ( "<api-key>%s</api-key>" % self . api_key ) postXMLList . append ( "<password>%s</password>" % self . password ) postXML = '<sms>%s</sms>' % "" . join ( postXMLList ) result = self . curl ( url , postXML ) if result . get ( "credits" , None ) : return result [ "credits" ] else : raise AmbientSMSError ( result [ "status" ] )
6811	def pre_deploy ( self ) : for service in self . genv . services : service = service . strip ( ) . upper ( ) funcs = common . service_pre_deployers . get ( service ) if funcs : print ( 'Running pre-deployments for service %s...' % ( service , ) ) for func in funcs : func ( )
3707	def Amgat ( xs , Vms ) : r if not none_and_length_check ( [ xs , Vms ] ) : raise Exception ( 'Function inputs are incorrect format' ) return mixing_simple ( xs , Vms )
9060	def beta ( self ) : from numpy_sugar . linalg import rsolve return rsolve ( self . _X [ "VT" ] , rsolve ( self . _X [ "tX" ] , self . mean ( ) ) )
9232	def fetch_date_of_tag ( self , tag ) : if self . options . verbose > 1 : print ( "\tFetching date for tag {}" . format ( tag [ "name" ] ) ) gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ tag [ "commit" ] [ "sha" ] ] . get ( ) if rc == 200 : return data [ "committer" ] [ "date" ] self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
1554	def _add_in_streams ( self , bolt ) : if self . inputs is None : return input_dict = self . _sanitize_inputs ( ) for global_streamid , gtype in input_dict . items ( ) : in_stream = bolt . inputs . add ( ) in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , global_streamid . stream_id ) ) if isinstance ( gtype , Grouping . FIELDS ) : in_stream . gtype = gtype . gtype in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) elif isinstance ( gtype , Grouping . CUSTOM ) : in_stream . gtype = gtype . gtype in_stream . custom_grouping_object = gtype . python_serialized in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) else : in_stream . gtype = gtype
11891	def set_all ( self , red , green , blue , brightness ) : command = "C {},{},{},{},{},\r\n" . format ( self . _zid , red , green , blue , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set all %s: %s" , repr ( command ) , response ) return response
7869	def _decode_attributes ( self ) : try : from_jid = self . _element . get ( 'from' ) if from_jid : self . _from_jid = JID ( from_jid ) to_jid = self . _element . get ( 'to' ) if to_jid : self . _to_jid = JID ( to_jid ) except ValueError : raise JIDMalformedProtocolError self . _stanza_type = self . _element . get ( 'type' ) self . _stanza_id = self . _element . get ( 'id' ) lang = self . _element . get ( XML_LANG_QNAME ) if lang : self . _language = lang
7256	def get_strip_metadata ( self , catID ) : self . logger . debug ( 'Retrieving strip catalog metadata' ) url = '%(base_url)s/record/%(catID)s?includeRelationships=false' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) if r . status_code == 200 : return r . json ( ) [ 'properties' ] elif r . status_code == 404 : self . logger . debug ( 'Strip not found: %s' % catID ) r . raise_for_status ( ) else : self . logger . debug ( 'There was a problem retrieving catid: %s' % catID ) r . raise_for_status ( )
4975	def get_global_context ( request , enterprise_customer ) : platform_name = get_configuration_value ( "PLATFORM_NAME" , settings . PLATFORM_NAME ) return { 'enterprise_customer' : enterprise_customer , 'LMS_SEGMENT_KEY' : settings . LMS_SEGMENT_KEY , 'LANGUAGE_CODE' : get_language_from_request ( request ) , 'tagline' : get_configuration_value ( "ENTERPRISE_TAGLINE" , settings . ENTERPRISE_TAGLINE ) , 'platform_description' : get_configuration_value ( "PLATFORM_DESCRIPTION" , settings . PLATFORM_DESCRIPTION , ) , 'LMS_ROOT_URL' : settings . LMS_ROOT_URL , 'platform_name' : platform_name , 'header_logo_alt_text' : _ ( '{platform_name} home page' ) . format ( platform_name = platform_name ) , 'welcome_text' : constants . WELCOME_TEXT . format ( platform_name = platform_name ) , 'enterprise_welcome_text' : constants . ENTERPRISE_WELCOME_TEXT . format ( enterprise_customer_name = enterprise_customer . name , platform_name = platform_name , strong_start = '<strong>' , strong_end = '</strong>' , line_break = '<br/>' , privacy_policy_link_start = "<a href='{pp_url}' target='_blank'>" . format ( pp_url = get_configuration_value ( 'PRIVACY' , 'https://www.edx.org/edx-privacy-policy' , type = 'url' ) , ) , privacy_policy_link_end = "</a>" , ) , }
11720	def get_directory ( self , path_to_directory , timeout = 30 , backoff = 0.4 , max_wait = 4 ) : response = None started_at = None time_elapsed = 0 i = 0 while time_elapsed < timeout : response = self . _get ( '{0}.zip' . format ( path_to_directory ) ) if response : break else : if started_at is None : started_at = time . time ( ) time . sleep ( min ( backoff * ( 2 ** i ) , max_wait ) ) i += 1 time_elapsed = time . time ( ) - started_at return response
4684	def getPublicKeys ( self , current = False ) : pubkeys = self . store . getPublicKeys ( ) if not current : return pubkeys pubs = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : pubs . append ( pubkey ) return pubs
11880	def scanAllProcessesForCwd ( searchPortion , isExactMatch = False ) : pids = getAllRunningPids ( ) cwdResults = [ scanProcessForCwd ( pid , searchPortion , isExactMatch ) for pid in pids ] ret = { } for i in range ( len ( pids ) ) : if cwdResults [ i ] is not None : ret [ pids [ i ] ] = cwdResults [ i ] return ret
1478	def _get_heron_support_processes ( self ) : retval = { } retval [ self . heron_shell_ids [ self . shard ] ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval
12324	def save ( self ) : if self . code : raise HolviError ( "Orders cannot be updated" ) send_json = self . to_holvi_dict ( ) send_json . update ( { 'pool' : self . api . connection . pool } ) url = six . u ( self . api . base_url + "order/" ) stat = self . api . connection . make_post ( url , send_json ) code = stat [ "details_uri" ] . split ( "/" ) [ - 2 ] return ( stat [ "checkout_uri" ] , self . api . get_order ( code ) )
9251	def generate_log_for_all_tags ( self ) : if self . options . verbose : print ( "Generating log..." ) self . issues2 = copy . deepcopy ( self . issues ) log1 = "" if self . options . with_unreleased : log1 = self . generate_unreleased_section ( ) log = "" for index in range ( len ( self . filtered_tags ) - 1 ) : log += self . do_generate_log_for_all_tags_part1 ( log , index ) if self . options . tag_separator and log1 : log = log1 + self . options . tag_separator + log else : log = log1 + log if len ( self . filtered_tags ) != 0 : log += self . do_generate_log_for_all_tags_part2 ( log ) return log
5125	def simulate ( self , n = 1 , t = None ) : if not self . _initialized : msg = ( "Network has not been initialized. " "Call '.initialize()' first." ) raise QueueingToolError ( msg ) if t is None : for dummy in range ( n ) : self . _simulate_next_event ( slow = False ) else : now = self . _t while self . _t < now + t : self . _simulate_next_event ( slow = False )
961	def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( "." . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger
10512	def registerevent ( self , event_name , fn_name , * args ) : if not isinstance ( event_name , str ) : raise ValueError ( "event_name should be string" ) self . _pollEvents . _callback [ event_name ] = [ event_name , fn_name , args ] return self . _remote_registerevent ( event_name )
5030	def get_learner_data_records ( self , enterprise_enrollment , completed_date = None , is_passing = False , ** kwargs ) : completed_timestamp = completed_date . strftime ( "%F" ) if isinstance ( completed_date , datetime ) else None if enterprise_enrollment . enterprise_customer_user . get_remote_id ( ) is not None : DegreedLearnerDataTransmissionAudit = apps . get_model ( 'degreed' , 'DegreedLearnerDataTransmissionAudit' ) return [ DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = parse_course_key ( enterprise_enrollment . course_id ) , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) , DegreedLearnerDataTransmissionAudit ( enterprise_course_enrollment_id = enterprise_enrollment . id , degreed_user_email = enterprise_enrollment . enterprise_customer_user . user_email , course_id = enterprise_enrollment . course_id , course_completed = completed_date is not None and is_passing , completed_timestamp = completed_timestamp , ) ] else : LOGGER . debug ( 'No learner data was sent for user [%s] because a Degreed user ID could not be found.' , enterprise_enrollment . enterprise_customer_user . username )
11849	def percept ( self , agent ) : "By default, agent perceives things within a default radius." return [ self . thing_percept ( thing , agent ) for thing in self . things_near ( agent . location ) ]
4771	def contains ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] not in self . val : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain key <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : self . _err ( 'Expected <%s> to contain item <%s>, but did not.' % ( self . val , items [ 0 ] ) ) else : missing = [ ] for i in items : if i not in self . val : missing . append ( i ) if missing : if self . _check_dict_like ( self . val , return_as_bool = True ) : self . _err ( 'Expected <%s> to contain keys %s, but did not contain key%s %s.' % ( self . val , self . _fmt_items ( items ) , '' if len ( missing ) == 0 else 's' , self . _fmt_items ( missing ) ) ) else : self . _err ( 'Expected <%s> to contain items %s, but did not contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( missing ) ) ) return self
1750	def mappings ( self ) : result = [ ] for m in self . maps : if isinstance ( m , AnonMap ) : result . append ( ( m . start , m . end , m . perms , 0 , '' ) ) elif isinstance ( m , FileMap ) : result . append ( ( m . start , m . end , m . perms , m . _offset , m . _filename ) ) else : result . append ( ( m . start , m . end , m . perms , 0 , m . name ) ) return sorted ( result )
2186	def load ( self , cfgstr = None ) : from six . moves import cPickle as pickle cfgstr = self . _rectify_cfgstr ( cfgstr ) dpath = self . dpath fname = self . fname verbose = self . verbose if not self . enabled : if verbose > 1 : self . log ( '[cacher] ... cache disabled: fname={}' . format ( self . fname ) ) raise IOError ( 3 , 'Cache Loading Is Disabled' ) fpath = self . get_fpath ( cfgstr = cfgstr ) if not exists ( fpath ) : if verbose > 2 : self . log ( '[cacher] ... cache does not exist: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) raise IOError ( 2 , 'No such file or directory: %r' % ( fpath , ) ) else : if verbose > 3 : self . log ( '[cacher] ... cache exists: ' 'dpath={} fname={} cfgstr={}' . format ( basename ( dpath ) , fname , cfgstr ) ) try : with open ( fpath , 'rb' ) as file_ : data = pickle . load ( file_ ) except Exception as ex : if verbose > 0 : self . log ( 'CORRUPTED? fpath = %s' % ( fpath , ) ) if verbose > 1 : self . log ( '[cacher] ... CORRUPTED? dpath={} cfgstr={}' . format ( basename ( dpath ) , cfgstr ) ) if isinstance ( ex , ( EOFError , IOError , ImportError ) ) : raise IOError ( str ( ex ) ) else : if verbose > 1 : self . log ( '[cacher] ... unknown reason for exception' ) raise else : if self . verbose > 2 : self . log ( '[cacher] ... {} cache hit' . format ( self . fname ) ) elif verbose > 1 : self . log ( '[cacher] ... cache hit' ) return data
5774	def ecdsa_sign ( private_key , data , hash_algorithm ) : if private_key . algorithm != 'ec' : raise ValueError ( 'The key specified is not an EC private key' ) return _sign ( private_key , data , hash_algorithm )
8114	def angle ( x0 , y0 , x1 , y1 ) : return degrees ( atan2 ( y1 - y0 , x1 - x0 ) )
803	def modelsGetResultAndStatus ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( "Wrong modelIDs type: %r" ) % type ( modelIDs ) assert len ( modelIDs ) >= 1 , "modelIDs is empty" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getResultAndStatusNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , "Didn't find modelIDs: %r" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getResultAndStatusNamedTuple . _make ( r ) for r in rows ]
12757	def add_torques ( self , torques ) : j = 0 for joint in self . joints : joint . add_torques ( list ( torques [ j : j + joint . ADOF ] ) + [ 0 ] * ( 3 - joint . ADOF ) ) j += joint . ADOF
11382	def do_url_scheme ( parser , token ) : args = token . split_contents ( ) if len ( args ) != 1 : raise template . TemplateSyntaxError ( '%s takes no parameters.' % args [ 0 ] ) return OEmbedURLSchemeNode ( )
9188	def admin_print_styles ( request ) : styles = [ ] with db_connect ( cursor_factory = DictCursor ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) for row in cursor . fetchall ( ) : styles . append ( { 'print_style' : row [ 'print_style' ] , 'title' : row [ 'title' ] , 'type' : row [ 'type' ] , 'revised' : row [ 'revised' ] , 'tag' : row [ 'tag' ] , 'commit_id' : row [ 'commit_id' ] , 'number' : row [ 'count' ] , 'bad' : row [ 'bad' ] , 'link' : request . route_path ( 'admin-print-style-single' , style = row [ 'print_style' ] ) } ) return { 'styles' : styles }
13267	def gml_to_geojson ( el ) : if el . get ( 'srsName' ) not in ( 'urn:ogc:def:crs:EPSG::4326' , None ) : if el . get ( 'srsName' ) == 'EPSG:4326' : return _gmlv2_to_geojson ( el ) else : raise NotImplementedError ( "Unrecognized srsName %s" % el . get ( 'srsName' ) ) tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}pos' % NS_GML ) ) [ 0 ] elif tag == 'LineString' : coordinates = _reverse_gml_coords ( el . findtext ( '{%s}posList' % NS_GML ) ) elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:exterior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) + el . xpath ( 'gml:interior/gml:LinearRing/gml:posList' , namespaces = NSMAP ) : coordinates . append ( _reverse_gml_coords ( ring . text ) ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' ) : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
5499	def add_tweets ( self , url , last_modified , tweets ) : try : self . cache [ url ] = { "last_modified" : last_modified , "tweets" : tweets } self . mark_updated ( ) return True except TypeError : return False
4423	async def handle_event ( self , event ) : if isinstance ( event , ( TrackStuckEvent , TrackExceptionEvent ) ) or isinstance ( event , TrackEndEvent ) and event . reason == 'FINISHED' : await self . play ( )
3190	def update ( self , list_id , segment_id , data ) : self . list_id = list_id self . segment_id = segment_id if 'name' not in data : raise KeyError ( 'The list segment must have a name' ) return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'segments' , segment_id ) , data = data )
12684	def query ( self , input = '' , params = { } ) : payload = { 'input' : input , 'appid' : self . appid } for key , value in params . items ( ) : if isinstance ( value , ( list , tuple ) ) : payload [ key ] = ',' . join ( value ) else : payload [ key ] = value try : r = requests . get ( "http://api.wolframalpha.com/v2/query" , params = payload ) if r . status_code != 200 : raise Exception ( 'Invalid response status code: %s' % ( r . status_code ) ) if r . encoding != 'utf-8' : raise Exception ( 'Invalid encoding: %s' % ( r . encoding ) ) except Exception , e : return Result ( error = e ) return Result ( xml = r . text )
3259	def get_resource ( self , name = None , store = None , workspace = None ) : resources = self . get_resources ( names = name , stores = store , workspaces = workspace ) return self . _return_first_item ( resources )
8398	def breaks ( self , limits ) : vmin = np . max ( [ self . domain [ 0 ] , limits [ 0 ] ] ) vmax = np . min ( [ self . domain [ 1 ] , limits [ 1 ] ] ) breaks = np . asarray ( self . breaks_ ( [ vmin , vmax ] ) ) breaks = breaks . compress ( ( breaks >= self . domain [ 0 ] ) & ( breaks <= self . domain [ 1 ] ) ) return breaks
2674	def init ( src , minimal = False ) : templates_path = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , 'project_templates' , ) for filename in os . listdir ( templates_path ) : if ( minimal and filename == 'event.json' ) or filename . endswith ( '.pyc' ) : continue dest_path = os . path . join ( templates_path , filename ) if not os . path . isdir ( dest_path ) : copy ( dest_path , src )
11191	def write ( proto_dataset_uri , input ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri ) _validate_and_put_readme ( proto_dataset , input . read ( ) )
8928	def prep ( ctx , commit = True ) : cfg = config . load ( ) scm = scm_provider ( cfg . project_root , commit = commit , ctx = ctx ) if not scm . workdir_is_clean ( ) : notify . failure ( "You have uncommitted changes, please commit or stash them!" ) setup_cfg = cfg . rootjoin ( 'setup.cfg' ) if os . path . exists ( setup_cfg ) : with io . open ( setup_cfg , encoding = 'utf-8' ) as handle : data = handle . readlines ( ) changed = False for i , line in enumerate ( data ) : if any ( line . startswith ( i ) for i in ( 'tag_build' , 'tag_date' ) ) : data [ i ] = '#' + data [ i ] changed = True if changed and commit : notify . info ( "Rewriting 'setup.cfg'..." ) with io . open ( setup_cfg , 'w' , encoding = 'utf-8' ) as handle : handle . write ( '' . join ( data ) ) scm . add_file ( 'setup.cfg' ) elif changed : notify . warning ( "WOULD rewrite 'setup.cfg', but --no-commit was passed" ) else : notify . warning ( "Cannot rewrite 'setup.cfg', none found!" ) ctx . run ( 'python setup.py -q develop -U' ) version = capture ( 'python setup.py --version' ) ctx . run ( 'invoke clean --all build --docs release.dist' ) for distfile in os . listdir ( 'dist' ) : trailer = distfile . split ( '-' + version ) [ 1 ] trailer , _ = os . path . splitext ( trailer ) if trailer and trailer [ 0 ] not in '.-' : notify . failure ( "The version found in 'dist' seems to be" " a pre-release one! [{}{}]" . format ( version , trailer ) ) scm . commit ( ctx . rituals . release . commit . message . format ( version = version ) ) scm . tag ( ctx . rituals . release . tag . name . format ( version = version ) , ctx . rituals . release . tag . message . format ( version = version ) )
3613	def filterchain_all ( request , app , model , field , foreign_key_app_name , foreign_key_model_name , foreign_key_field_name , value ) : model_class = get_model ( app , model ) keywords = get_keywords ( field , value ) foreign_model_class = get_model ( foreign_key_app_name , foreign_key_model_name ) if not any ( [ ( isinstance ( f , ChainedManyToManyField ) or isinstance ( f , ChainedForeignKey ) ) for f in foreign_model_class . _meta . get_fields ( ) ] ) : raise PermissionDenied ( "Smart select disallowed" ) limit_choices_to = get_limit_choices_to ( foreign_key_app_name , foreign_key_model_name , foreign_key_field_name ) queryset = get_queryset ( model_class , limit_choices_to = limit_choices_to ) filtered = list ( do_filter ( queryset , keywords ) ) if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( filtered ) ) excluded = list ( do_filter ( queryset , keywords , exclude = True ) ) if not getattr ( model_class . _meta , 'ordering' , False ) : sort_results ( list ( excluded ) ) empty_choice = { 'value' : "" , 'display' : "---------" } serialized_results = ( serialize_results ( filtered ) + [ empty_choice ] + serialize_results ( excluded ) ) return JsonResponse ( serialized_results , safe = False )
13737	def get_context ( request , model = None ) : param_values = get_param_values ( request , model = model ) context = param_values . pop ( 'orb_context' , { } ) if isinstance ( context , ( unicode , str ) ) : context = projex . rest . unjsonify ( context ) has_limit = 'limit' in context or 'limit' in param_values orb_context = orb . Context ( ** context ) used = set ( ) query_context = { } for key in orb . Context . Defaults : if key in param_values : used . add ( key ) query_context [ key ] = param_values . get ( key ) schema_values = { } if model : for key , value in request . matchdict . items ( ) : if model . schema ( ) . column ( key , raise_ = False ) : schema_values [ key ] = value for key , value in param_values . items ( ) : root_key = key . split ( '.' ) [ 0 ] schema_object = model . schema ( ) . column ( root_key , raise_ = False ) or model . schema ( ) . collector ( root_key ) if schema_object : value = param_values . pop ( key ) if isinstance ( schema_object , orb . Collector ) and type ( value ) not in ( tuple , list ) : value = [ value ] schema_values [ key ] = value query_context [ 'scope' ] = { 'request' : request } try : default_context = request . orb_default_context except AttributeError : try : query_context [ 'scope' ] . update ( request . orb_scope ) except AttributeError : pass else : if 'scope' in default_context : query_context [ 'scope' ] . update ( default_context . pop ( 'scope' ) ) for k , v in default_context . items ( ) : query_context . setdefault ( k , v ) orb_context . update ( query_context ) return schema_values , orb_context
8232	def set_callbacks ( self , ** kwargs ) : for name in self . SUPPORTED_CALLBACKS : func = kwargs . get ( name , getattr ( self , name ) ) setattr ( self , name , func )
3930	def _get_authorization_code ( session , credentials_prompt ) : browser = Browser ( session , OAUTH2_LOGIN_URL ) email = credentials_prompt . get_email ( ) browser . submit_form ( FORM_SELECTOR , { EMAIL_SELECTOR : email } ) password = credentials_prompt . get_password ( ) browser . submit_form ( FORM_SELECTOR , { PASSWORD_SELECTOR : password } ) if browser . has_selector ( TOTP_CHALLENGE_SELECTOR ) : browser . submit_form ( TOTP_CHALLENGE_SELECTOR , { } ) elif browser . has_selector ( PHONE_CHALLENGE_SELECTOR ) : browser . submit_form ( PHONE_CHALLENGE_SELECTOR , { } ) if browser . has_selector ( VERIFICATION_FORM_SELECTOR ) : if browser . has_selector ( TOTP_CODE_SELECTOR ) : input_selector = TOTP_CODE_SELECTOR elif browser . has_selector ( PHONE_CODE_SELECTOR ) : input_selector = PHONE_CODE_SELECTOR else : raise GoogleAuthError ( 'Unknown verification code input' ) verfification_code = credentials_prompt . get_verification_code ( ) browser . submit_form ( VERIFICATION_FORM_SELECTOR , { input_selector : verfification_code } ) try : return browser . get_cookie ( 'oauth_code' ) except KeyError : raise GoogleAuthError ( 'Authorization code cookie not found' )
2130	def list ( self , ** kwargs ) : data , self . endpoint = self . data_endpoint ( kwargs ) r = super ( Resource , self ) . list ( ** data ) self . configure_display ( r ) return r
8787	def set ( self , model , value ) : self . validate ( value ) self . _pop ( model ) value = self . serialize ( value ) model . tags . append ( value )
11360	def fix_dashes ( string ) : string = string . replace ( u'\u05BE' , '-' ) string = string . replace ( u'\u1806' , '-' ) string = string . replace ( u'\u2E3A' , '-' ) string = string . replace ( u'\u2E3B' , '-' ) string = unidecode ( string ) return re . sub ( r'--+' , '-' , string )
6544	def terminate ( self ) : if not self . is_terminated : log . debug ( "terminal client terminated" ) try : self . exec_command ( b"Quit" ) except BrokenPipeError : pass except socket . error as e : if e . errno != errno . ECONNRESET : raise self . app . close ( ) self . is_terminated = True
2628	def teardown ( self ) : self . shut_down_instance ( self . instances ) self . instances = [ ] try : self . client . delete_internet_gateway ( InternetGatewayId = self . internet_gateway ) self . internet_gateway = None self . client . delete_route_table ( RouteTableId = self . route_table ) self . route_table = None for subnet in list ( self . sn_ids ) : self . client . delete_subnet ( SubnetId = subnet ) self . sn_ids . remove ( subnet ) self . client . delete_security_group ( GroupId = self . sg_id ) self . sg_id = None self . client . delete_vpc ( VpcId = self . vpc_id ) self . vpc_id = None except Exception as e : logger . error ( "{}" . format ( e ) ) raise e self . show_summary ( ) os . remove ( self . config [ 'state_file_path' ] )
8622	def get_user_by_id ( session , user_id , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'users/{}' . format ( user_id ) , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise UserNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6922	def autocorr_magseries ( times , mags , errs , maxlags = 1000 , func = _autocorr_func3 , fillgaps = 0.0 , filterwindow = 11 , forcetimebin = None , sigclip = 3.0 , magsarefluxes = False , verbose = True ) : interpolated = fill_magseries_gaps ( times , mags , errs , fillgaps = fillgaps , forcetimebin = forcetimebin , sigclip = sigclip , magsarefluxes = magsarefluxes , filterwindow = filterwindow , verbose = verbose ) if not interpolated : print ( 'failed to interpolate light curve to minimum cadence!' ) return None itimes , imags = interpolated [ 'itimes' ] , interpolated [ 'imags' ] , if maxlags : lags = nparange ( 0 , maxlags ) else : lags = nparange ( itimes . size ) series_stdev = 1.483 * npmedian ( npabs ( imags ) ) if func != _autocorr_func3 : autocorr = nparray ( [ func ( imags , x , imags . size , 0.0 , series_stdev ) for x in lags ] ) else : autocorr = _autocorr_func3 ( imags , lags [ 0 ] , imags . size , 0.0 , series_stdev ) if maxlags is not None : autocorr = autocorr [ : maxlags ] interpolated . update ( { 'minitime' : itimes . min ( ) , 'lags' : lags , 'acf' : autocorr } ) return interpolated
6041	def unmasked_sparse_to_sparse ( self ) : return mapping_util . unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask = self . regular_grid . mask , unmasked_sparse_grid_pixel_centres = self . unmasked_sparse_grid_pixel_centres , total_sparse_pixels = self . total_sparse_pixels ) . astype ( 'int' )
8544	def get_datacenter ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
2117	def convert ( self , value , param , ctx ) : choice = super ( MappedChoice , self ) . convert ( value , param , ctx ) ix = self . choices . index ( choice ) return self . actual_choices [ ix ]
6099	def luminosity_integral ( self , x , axis_ratio ) : r = x * axis_ratio return 2 * np . pi * r * self . intensities_from_grid_radii ( x )
7269	def attribute ( * args , ** kw ) : return operator ( kind = Operator . Type . ATTRIBUTE , * args , ** kw )
9075	def sendMultiPart ( smtp , gpg_context , sender , recipients , subject , text , attachments ) : sent = 0 for to in recipients : if not to . startswith ( '<' ) : uid = '<%s>' % to else : uid = to if not checkRecipient ( gpg_context , uid ) : continue msg = MIMEMultipart ( ) msg [ 'From' ] = sender msg [ 'To' ] = to msg [ 'Subject' ] = subject msg [ "Date" ] = formatdate ( localtime = True ) msg . preamble = u'This is an email in encrypted multipart format.' attach = MIMEText ( str ( gpg_context . encrypt ( text . encode ( 'utf-8' ) , uid , always_trust = True ) ) ) attach . set_charset ( 'UTF-8' ) msg . attach ( attach ) for attachment in attachments : with open ( attachment , 'rb' ) as fp : attach = MIMEBase ( 'application' , 'octet-stream' ) attach . set_payload ( str ( gpg_context . encrypt_file ( fp , uid , always_trust = True ) ) ) attach . add_header ( 'Content-Disposition' , 'attachment' , filename = basename ( '%s.pgp' % attachment ) ) msg . attach ( attach ) smtp . begin ( ) smtp . sendmail ( sender , to , msg . as_string ( ) ) smtp . quit ( ) sent += 1 return sent
10886	def corners ( self ) : corners = [ ] for ind in itertools . product ( * ( ( 0 , 1 ) , ) * self . dim ) : ind = np . array ( ind ) corners . append ( self . l + ind * self . r ) return np . array ( corners )
11328	def select ( options = None ) : if not options : return None width = len ( str ( len ( options ) ) ) for x , option in enumerate ( options ) : sys . stdout . write ( '{:{width}}) {}\n' . format ( x + 1 , option , width = width ) ) sys . stdout . write ( '{:>{width}} ' . format ( '#?' , width = width + 1 ) ) sys . stdout . flush ( ) if sys . stdin . isatty ( ) : try : response = raw_input ( ) . strip ( ) except ( EOFError , KeyboardInterrupt ) : response = '' else : sys . stdin = open ( "/dev/tty" ) try : response = '' while True : response += sys . stdin . read ( 1 ) if response . endswith ( '\n' ) : break except ( EOFError , KeyboardInterrupt ) : sys . stdout . flush ( ) pass try : response = int ( response ) - 1 except ValueError : return None if response < 0 or response >= len ( options ) : return None return options [ response ]
4020	def _init_docker_vm ( ) : if not _dusty_vm_exists ( ) : log_to_client ( 'Initializing new Dusty VM with Docker Machine' ) machine_options = [ '--driver' , 'virtualbox' , '--virtualbox-cpu-count' , '-1' , '--virtualbox-boot2docker-url' , constants . CONFIG_BOOT2DOCKER_URL , '--virtualbox-memory' , str ( get_config_value ( constants . CONFIG_VM_MEM_SIZE ) ) , '--virtualbox-hostonly-nictype' , constants . VM_NIC_TYPE ] check_call_demoted ( [ 'docker-machine' , 'create' ] + machine_options + [ constants . VM_MACHINE_NAME ] , redirect_stderr = True )
9746	def datagram_received ( self , datagram , address ) : size , _ = RTheader . unpack_from ( datagram , 0 ) info , = struct . unpack_from ( "{0}s" . format ( size - 3 - 8 ) , datagram , RTheader . size ) base_port , = QRTDiscoveryBasePort . unpack_from ( datagram , size - 2 ) if self . receiver is not None : self . receiver ( QRTDiscoveryResponse ( info , address [ 0 ] , base_port ) )
2385	def from_spec_resolver ( cls , spec_resolver ) : deref = DerefValidatorDecorator ( spec_resolver ) for key , validator_callable in iteritems ( cls . validators ) : yield key , deref ( validator_callable )
13355	def profil_annuel ( df , func = 'mean' ) : func = _get_funky ( func ) res = df . groupby ( lambda x : x . month ) . aggregate ( func ) res . index = [ cal . month_name [ i ] for i in range ( 1 , 13 ) ] return res
8731	def get_period_seconds ( period ) : if isinstance ( period , six . string_types ) : try : name = 'seconds_per_' + period . lower ( ) result = globals ( ) [ name ] except KeyError : msg = "period not in (second, minute, hour, day, month, year)" raise ValueError ( msg ) elif isinstance ( period , numbers . Number ) : result = period elif isinstance ( period , datetime . timedelta ) : result = period . days * get_period_seconds ( 'day' ) + period . seconds else : raise TypeError ( 'period must be a string or integer' ) return result
7956	def _initiate_starttls ( self , ** kwargs ) : if self . _tls_state == "connected" : raise RuntimeError ( "Already TLS-connected" ) kwargs [ "do_handshake_on_connect" ] = False logger . debug ( "Wrapping the socket into ssl" ) self . _socket = ssl . wrap_socket ( self . _socket , ** kwargs ) self . _set_state ( "tls-handshake" ) self . _continue_tls_handshake ( )
13813	def MessageToJson ( message , including_default_value_fields = False ) : js = _MessageToJsonObject ( message , including_default_value_fields ) return json . dumps ( js , indent = 2 )
1684	def Begin ( self , function_name ) : self . in_a_function = True self . lines_in_function = 0 self . current_function = function_name
170	def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5
11240	def copy_web_file_to_local ( file_path , target_path ) : response = urllib . request . urlopen ( file_path ) f = open ( target_path , 'w' ) f . write ( response . read ( ) ) f . close ( )
8827	def update_ports_for_sg ( self , context , portid , jobid ) : port = db_api . port_find ( context , id = portid , scope = db_api . ONE ) if not port : LOG . warning ( "Port not found" ) return net_driver = port_api . _get_net_driver ( port . network , port = port ) base_net_driver = port_api . _get_net_driver ( port . network ) sg_list = [ sg for sg in port . security_groups ] success = False error = None retries = 3 retry_delay = 2 for retry in xrange ( retries ) : try : net_driver . update_port ( context , port_id = port [ "backend_key" ] , mac_address = port [ "mac_address" ] , device_id = port [ "device_id" ] , base_net_driver = base_net_driver , security_groups = sg_list ) success = True error = None break except Exception as error : LOG . warning ( "Could not connect to redis, but retrying soon" ) time . sleep ( retry_delay ) status_str = "" if not success : status_str = "Port %s update failed after %d tries. Error: %s" % ( portid , retries , error ) update_body = dict ( completed = True , status = status_str ) update_body = dict ( job = update_body ) job_api . update_job ( context . elevated ( ) , jobid , update_body )
10491	def dragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _postQueuedEvents ( interval = interval )
8672	def list_keys ( key_name , max_suggestions , cutoff , jsonify , locked , key_type , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase , quiet = jsonify ) try : keys = stash . list ( key_name = key_name , max_suggestions = max_suggestions , cutoff = cutoff , locked_only = locked , key_type = key_type ) except GhostError as ex : sys . exit ( ex ) if jsonify : click . echo ( json . dumps ( keys , indent = 4 , sort_keys = True ) ) elif not keys : click . echo ( 'The stash is empty. Go on, put some keys in there...' ) else : click . echo ( 'Listing all keys...' ) click . echo ( _prettify_list ( keys ) )
7164	def add_intent ( self , name , lines , reload_cache = False ) : self . intents . add ( name , lines , reload_cache ) self . padaos . add_intent ( name , lines ) self . must_train = True
8468	def parseConfig ( cls , value ) : if 'enabled' in value : value [ 'enabled' ] = bool ( value [ 'enabled' ] ) if 'exclude_paths' in value : value [ 'exclude_paths' ] = [ n . strip ( ) for n in ast . literal_eval ( value [ 'exclude_paths' ] ) ] return value
2968	def _sm_relieve_pain ( self , * args , ** kwargs ) : _logger . info ( "Ending the degradation for blockade %s" % self . _blockade_name ) self . _do_reset_all ( ) millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
13701	def _before ( self ) : if request . path in self . excluded_routes : request . _tracy_exclude = True return request . _tracy_start_time = monotonic ( ) client = request . headers . get ( trace_header_client , None ) require_client = current_app . config . get ( "TRACY_REQUIRE_CLIENT" , False ) if client is None and require_client : abort ( 400 , "Missing %s header" % trace_header_client ) request . _tracy_client = client request . _tracy_id = request . headers . get ( trace_header_id , new_id ( ) )
8875	def allele_frequency ( expec ) : r expec = asarray ( expec , float ) if expec . ndim != 2 : raise ValueError ( "Expectation matrix must be bi-dimensional." ) ploidy = expec . shape [ - 1 ] return expec . sum ( - 2 ) / ploidy
11300	def unregister ( self , provider_class ) : if not issubclass ( provider_class , BaseProvider ) : raise TypeError ( '%s must be a subclass of BaseProvider' % provider_class . __name__ ) if provider_class not in self . _registered_providers : raise NotRegistered ( '%s is not registered' % provider_class . __name__ ) self . _registered_providers . remove ( provider_class ) self . invalidate_providers ( )
226	def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs
5435	def tasks_file_to_task_descriptors ( tasks , retries , input_file_param_util , output_file_param_util ) : task_descriptors = [ ] path = tasks [ 'path' ] task_min = tasks . get ( 'min' ) task_max = tasks . get ( 'max' ) param_file = dsub_util . load_file ( path ) reader = csv . reader ( param_file , delimiter = '\t' ) header = six . advance_iterator ( reader ) job_params = parse_tasks_file_header ( header , input_file_param_util , output_file_param_util ) for row in reader : task_id = reader . line_num - 1 if task_min and task_id < task_min : continue if task_max and task_id > task_max : continue if len ( row ) != len ( job_params ) : dsub_util . print_error ( 'Unexpected number of fields %s vs %s: line %s' % ( len ( row ) , len ( job_params ) , reader . line_num ) ) envs = set ( ) inputs = set ( ) outputs = set ( ) labels = set ( ) for i in range ( 0 , len ( job_params ) ) : param = job_params [ i ] name = param . name if isinstance ( param , job_model . EnvParam ) : envs . add ( job_model . EnvParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . LabelParam ) : labels . add ( job_model . LabelParam ( name , row [ i ] ) ) elif isinstance ( param , job_model . InputFileParam ) : inputs . add ( input_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) elif isinstance ( param , job_model . OutputFileParam ) : outputs . add ( output_file_param_util . make_param ( name , row [ i ] , param . recursive ) ) task_descriptors . append ( job_model . TaskDescriptor ( { 'task-id' : task_id , 'task-attempt' : 1 if retries else None } , { 'labels' : labels , 'envs' : envs , 'inputs' : inputs , 'outputs' : outputs } , job_model . Resources ( ) ) ) if not task_descriptors : raise ValueError ( 'No tasks added from %s' % path ) return task_descriptors
4663	def new_tx ( self , * args , ** kwargs ) : builder = self . transactionbuilder_class ( * args , blockchain_instance = self , ** kwargs ) self . _txbuffers . append ( builder ) return builder
6578	def _base_repr ( self , and_also = None ) : items = [ "=" . join ( ( key , repr ( getattr ( self , key ) ) ) ) for key in sorted ( self . _fields . keys ( ) ) ] if items : output = ", " . join ( items ) else : output = None if and_also : return "{}({}, {})" . format ( self . __class__ . __name__ , output , and_also ) else : return "{}({})" . format ( self . __class__ . __name__ , output )
9162	def processor ( ) : registry = get_current_registry ( ) settings = registry . settings connection_string = settings [ CONNECTION_STRING ] channels = _get_channels ( settings ) with psycopg2 . connect ( connection_string ) as conn : conn . set_isolation_level ( ISOLATION_LEVEL_AUTOCOMMIT ) with conn . cursor ( ) as cursor : for channel in channels : cursor . execute ( 'LISTEN {}' . format ( channel ) ) logger . debug ( 'Waiting for notifications on channel "{}"' . format ( channel ) ) registry . notify ( ChannelProcessingStartUpEvent ( ) ) rlist = [ conn ] wlist = [ ] xlist = [ ] timeout = 5 while True : if select . select ( rlist , wlist , xlist , timeout ) != ( [ ] , [ ] , [ ] ) : conn . poll ( ) while conn . notifies : notif = conn . notifies . pop ( 0 ) logger . debug ( 'Got NOTIFY: pid={} channel={} payload={}' . format ( notif . pid , notif . channel , notif . payload ) ) event = create_pg_notify_event ( notif ) try : registry . notify ( event ) except Exception : logger . exception ( 'Logging an uncaught exception' )
1174	def lock ( self , function , argument ) : if self . testandset ( ) : function ( argument ) else : self . queue . append ( ( function , argument ) )
12634	def transform ( self ) : if self . dcmf1 is None or self . dcmf2 is None : return np . inf for field_name in self . field_weights : if ( str ( getattr ( self . dcmf1 , field_name , '' ) ) != str ( getattr ( self . dcmf2 , field_name , '' ) ) ) : return False return True
8441	def _parse_link_header ( headers ) : links = { } if 'link' in headers : link_headers = headers [ 'link' ] . split ( ', ' ) for link_header in link_headers : ( url , rel ) = link_header . split ( '; ' ) url = url [ 1 : - 1 ] rel = rel [ 5 : - 1 ] links [ rel ] = url return links
7359	def predict_subsequences ( self , sequence_dict , peptide_lengths = None ) : if isinstance ( sequence_dict , string_types ) : sequence_dict = { "seq" : sequence_dict } elif isinstance ( sequence_dict , ( list , tuple ) ) : sequence_dict = { seq : seq for seq in sequence_dict } peptide_lengths = self . _check_peptide_lengths ( peptide_lengths ) peptide_set = set ( [ ] ) peptide_to_name_offset_pairs = defaultdict ( list ) for name , sequence in sequence_dict . items ( ) : for peptide_length in peptide_lengths : for i in range ( len ( sequence ) - peptide_length + 1 ) : peptide = sequence [ i : i + peptide_length ] peptide_set . add ( peptide ) peptide_to_name_offset_pairs [ peptide ] . append ( ( name , i ) ) peptide_list = sorted ( peptide_set ) binding_predictions = self . predict_peptides ( peptide_list ) results = [ ] for binding_prediction in binding_predictions : for name , offset in peptide_to_name_offset_pairs [ binding_prediction . peptide ] : results . append ( binding_prediction . clone_with_updates ( source_sequence_name = name , offset = offset ) ) self . _check_results ( results , peptides = peptide_set , alleles = self . alleles ) return BindingPredictionCollection ( results )
6736	def reboot_or_dryrun ( * args , ** kwargs ) : from fabric . state import connections verbose = get_verbose ( ) dryrun = get_dryrun ( kwargs . get ( 'dryrun' ) ) kwargs . setdefault ( 'wait' , 120 ) wait = int ( kwargs [ 'wait' ] ) command = kwargs . get ( 'command' , 'reboot' ) now = int ( kwargs . get ( 'now' , 0 ) ) print ( 'now:' , now ) if now : command += ' now' timeout = int ( kwargs . get ( 'timeout' , 30 ) ) reconnect_hostname = kwargs . pop ( 'new_hostname' , env . host_string ) if 'dryrun' in kwargs : del kwargs [ 'dryrun' ] if dryrun : print ( '%s sudo: %s' % ( render_command_prefix ( ) , command ) ) else : if is_local ( ) : if raw_input ( 'reboot localhost now? ' ) . strip ( ) [ 0 ] . lower ( ) != 'y' : return attempts = int ( round ( float ( wait ) / float ( timeout ) ) ) with settings ( warn_only = True ) : _sudo ( command ) env . host_string = reconnect_hostname success = False for attempt in xrange ( attempts ) : if verbose : print ( 'Waiting for %s seconds, wait %i of %i' % ( timeout , attempt + 1 , attempts ) ) time . sleep ( timeout ) try : if verbose : print ( 'Reconnecting to:' , env . host_string ) connections . connect ( env . host_string ) with settings ( timeout = timeout ) : _run ( 'echo hello' ) success = True break except Exception as e : print ( 'Exception:' , e ) if not success : raise Exception ( 'Reboot failed or took longer than %s seconds.' % wait )
5769	def _advapi32_load_key ( key_object , key_info , container ) : key_type = 'public' if isinstance ( key_info , keys . PublicKeyInfo ) else 'private' algo = key_info . algorithm if algo == 'rsa' : provider = Advapi32Const . MS_ENH_RSA_AES_PROV else : provider = Advapi32Const . MS_ENH_DSS_DH_PROV context_handle = None key_handle = None try : context_handle = open_context_handle ( provider , verify_only = key_type == 'public' ) blob = _advapi32_create_blob ( key_info , key_type , algo ) buffer_ = buffer_from_bytes ( blob ) key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , buffer_ , len ( blob ) , null ( ) , 0 , key_handle_pointer ) handle_error ( res ) key_handle = unwrap ( key_handle_pointer ) output = container ( key_handle , key_object ) output . context_handle = context_handle if algo == 'rsa' : ex_blob = _advapi32_create_blob ( key_info , key_type , algo , signing = False ) ex_buffer = buffer_from_bytes ( ex_blob ) ex_key_handle_pointer = new ( advapi32 , 'HCRYPTKEY *' ) res = advapi32 . CryptImportKey ( context_handle , ex_buffer , len ( ex_blob ) , null ( ) , 0 , ex_key_handle_pointer ) handle_error ( res ) output . ex_key_handle = unwrap ( ex_key_handle_pointer ) return output except ( Exception ) : if key_handle : advapi32 . CryptDestroyKey ( key_handle ) if context_handle : close_context_handle ( context_handle ) raise
10436	def verifypartialtablecell ( self , window_name , object_name , row_index , column_index , row_text ) : try : value = getcellvalue ( window_name , object_name , row_index , column_index ) if re . searchmatch ( row_text , value ) : return 1 except LdtpServerException : pass return 0
5734	def _get_result_msg_and_payload ( result , stream ) : groups = _GDB_MI_RESULT_RE . match ( result ) . groups ( ) token = int ( groups [ 0 ] ) if groups [ 0 ] != "" else None message = groups [ 1 ] if groups [ 2 ] is None : payload = None else : stream . advance_past_chars ( [ "," ] ) payload = _parse_dict ( stream ) return token , message , payload
4325	def dcshift ( self , shift = 0.0 ) : if not is_number ( shift ) or shift < - 2 or shift > 2 : raise ValueError ( 'shift must be a number between -2 and 2.' ) effect_args = [ 'dcshift' , '{:f}' . format ( shift ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'dcshift' ) return self
5592	def tiles_from_bbox ( self , geometry , zoom ) : for tile in self . tile_pyramid . tiles_from_bbox ( geometry , zoom ) : yield self . tile ( * tile . id )
7239	def window_at ( self , geom , window_shape ) : y_size , x_size = window_shape [ 0 ] , window_shape [ 1 ] bounds = box ( * geom . bounds ) px = ops . transform ( self . __geo_transform__ . rev , bounds ) . centroid miny , maxy = int ( px . y - y_size / 2 ) , int ( px . y + y_size / 2 ) minx , maxx = int ( px . x - x_size / 2 ) , int ( px . x + x_size / 2 ) _ , y_max , x_max = self . shape if minx < 0 or miny < 0 or maxx > x_max or maxy > y_max : raise ValueError ( "Input geometry resulted in a window outside of the image" ) return self [ : , miny : maxy , minx : maxx ]
8668	def lock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Locking key...' ) stash . lock ( key_name = key_name ) click . echo ( 'Key locked successfully' ) except GhostError as ex : sys . exit ( ex )
13314	def remove ( self ) : self . run_hook ( 'preremove' ) utils . rmtree ( self . path ) self . run_hook ( 'postremove' )
7527	def align_and_parse ( handle , max_internal_indels = 5 , is_gbs = False ) : try : with open ( handle , 'rb' ) as infile : clusts = infile . read ( ) . split ( "//\n//\n" ) clusts = [ i for i in clusts if i ] if not clusts : raise IPyradError except ( IOError , IPyradError ) : LOGGER . debug ( "skipping empty chunk - {}" . format ( handle ) ) return 0 highindels = 0 try : aligned = persistent_popen_align3 ( clusts , 200 , is_gbs ) except Exception as inst : LOGGER . debug ( "Error in handle - {} - {}" . format ( handle , inst ) ) aligned = [ ] refined = [ ] for clust in aligned : filtered = aligned_indel_filter ( clust , max_internal_indels ) if not filtered : refined . append ( clust ) else : highindels += 1 if refined : outhandle = handle . rsplit ( "." , 1 ) [ 0 ] + ".aligned" with open ( outhandle , 'wb' ) as outfile : outfile . write ( "\n//\n//\n" . join ( refined ) + "\n" ) log_level = logging . getLevelName ( LOGGER . getEffectiveLevel ( ) ) if not log_level == "DEBUG" : os . remove ( handle ) return highindels
12190	def _format_message ( self , channel , text ) : payload = { 'type' : 'message' , 'id' : next ( self . _msg_ids ) } payload . update ( channel = channel , text = text ) return json . dumps ( payload )
7178	def lib2to3_parse ( src_txt ) : grammar = pygram . python_grammar_no_print_statement drv = driver . Driver ( grammar , pytree . convert ) if src_txt [ - 1 ] != '\n' : nl = '\r\n' if '\r\n' in src_txt [ : 1024 ] else '\n' src_txt += nl try : result = drv . parse_string ( src_txt , True ) except ParseError as pe : lineno , column = pe . context [ 1 ] lines = src_txt . splitlines ( ) try : faulty_line = lines [ lineno - 1 ] except IndexError : faulty_line = "<line number missing in source>" raise ValueError ( f"Cannot parse: {lineno}:{column}: {faulty_line}" ) from None if isinstance ( result , Leaf ) : result = Node ( syms . file_input , [ result ] ) return result
3741	def omega_mixture ( omegas , zs , CASRNs = None , Method = None , AvailableMethods = False ) : r def list_methods ( ) : methods = [ ] if none_and_length_check ( [ zs , omegas ] ) : methods . append ( 'SIMPLE' ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'SIMPLE' : _omega = mixing_simple ( zs , omegas ) elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
3725	def dipole_moment ( CASRN , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in _dipole_CCDB . index and not np . isnan ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) : methods . append ( CCCBDB ) if CASRN in _dipole_Muller . index and not np . isnan ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) : methods . append ( MULLER ) if CASRN in _dipole_Poling . index and not np . isnan ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) : methods . append ( POLING ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == CCCBDB : _dipole = float ( _dipole_CCDB . at [ CASRN , 'Dipole' ] ) elif Method == MULLER : _dipole = float ( _dipole_Muller . at [ CASRN , 'Dipole' ] ) elif Method == POLING : _dipole = float ( _dipole_Poling . at [ CASRN , 'Dipole' ] ) elif Method == NONE : _dipole = None else : raise Exception ( 'Failure in in function' ) return _dipole
9504	def distance_to_point ( self , p ) : if self . start <= p <= self . end : return 0 else : return min ( abs ( self . start - p ) , abs ( self . end - p ) )
5530	def _process_worker ( process , process_tile ) : logger . debug ( ( process_tile . id , "running on %s" % current_process ( ) . name ) ) if ( process . config . mode == "continue" and process . config . output . tiles_exist ( process_tile ) ) : logger . debug ( ( process_tile . id , "tile exists, skipping" ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = "output already exists" , written = False , write_msg = "nothing written" ) else : with Timer ( ) as t : try : output = process . execute ( process_tile , raise_nodata = True ) except MapcheteNodataTile : output = None processor_message = "processed in %s" % t logger . debug ( ( process_tile . id , processor_message ) ) writer_info = process . write ( process_tile , output ) return ProcessInfo ( tile = process_tile , processed = True , process_msg = processor_message , written = writer_info . written , write_msg = writer_info . write_msg )
7175	def main ( src , pyi_dir , target_dir , incremental , quiet , replace_any , hg , traceback ) : Config . incremental = incremental Config . replace_any = replace_any returncode = 0 for src_entry in src : for file , error , exc_type , tb in retype_path ( Path ( src_entry ) , pyi_dir = Path ( pyi_dir ) , targets = Path ( target_dir ) , src_explicitly_given = True , quiet = quiet , hg = hg , ) : print ( f'error: {file}: {error}' , file = sys . stderr ) if traceback : print ( 'Traceback (most recent call last):' , file = sys . stderr ) for line in tb : print ( line , file = sys . stderr , end = '' ) print ( f'{exc_type.__name__}: {error}' , file = sys . stderr ) returncode += 1 if not src and not quiet : print ( 'warning: no sources given' , file = sys . stderr ) sys . exit ( min ( returncode , 125 ) )
3634	def search ( self , ctype , level = None , category = None , assetId = None , defId = None , min_price = None , max_price = None , min_buy = None , max_buy = None , league = None , club = None , position = None , zone = None , nationality = None , rare = False , playStyle = None , start = 0 , page_size = itemsPerPage [ 'transferMarket' ] , fast = False ) : method = 'GET' url = 'transfermarket' if start == 0 : events = [ self . pin . event ( 'page_view' , 'Hub - Transfers' ) , self . pin . event ( 'page_view' , 'Transfer Market Search' ) ] self . pin . send ( events , fast = fast ) params = { 'start' : start , 'num' : page_size , 'type' : ctype , } if level : params [ 'lev' ] = level if category : params [ 'cat' ] = category if assetId : params [ 'maskedDefId' ] = assetId if defId : params [ 'definitionId' ] = defId if min_price : params [ 'micr' ] = min_price if max_price : params [ 'macr' ] = max_price if min_buy : params [ 'minb' ] = min_buy if max_buy : params [ 'maxb' ] = max_buy if league : params [ 'leag' ] = league if club : params [ 'team' ] = club if position : params [ 'pos' ] = position if zone : params [ 'zone' ] = zone if nationality : params [ 'nat' ] = nationality if rare : params [ 'rare' ] = 'SP' if playStyle : params [ 'playStyle' ] = playStyle rc = self . __request__ ( method , url , params = params , fast = fast ) if start == 0 : events = [ self . pin . event ( 'page_view' , 'Transfer Market Results - List View' ) , self . pin . event ( 'page_view' , 'Item - Detail View' ) ] self . pin . send ( events , fast = fast ) return [ itemParse ( i ) for i in rc . get ( 'auctionInfo' , ( ) ) ]
11924	def render_to ( path , template , ** data ) : try : renderer . render_to ( path , template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
8104	def update ( self ) : try : self . manager . handle ( self . socket . recv ( 1024 ) ) except socket . error : pass
11363	def download_file ( from_url , to_filename = None , chunk_size = 1024 * 8 , retry_count = 3 ) : if not to_filename : to_filename = get_temporary_file ( ) session = requests . Session ( ) adapter = requests . adapters . HTTPAdapter ( max_retries = retry_count ) session . mount ( from_url , adapter ) response = session . get ( from_url , stream = True ) with open ( to_filename , 'wb' ) as fd : for chunk in response . iter_content ( chunk_size ) : fd . write ( chunk ) return to_filename
7275	def seek ( self , relative_position ) : self . _player_interface . Seek ( Int64 ( 1000.0 * 1000 * relative_position ) ) self . seekEvent ( self , relative_position )
12375	def allowed_operations ( self ) : if self . slug is not None : return self . meta . detail_allowed_operations return self . meta . list_allowed_operations
3587	def remove ( self , cbobject ) : with self . _lock : if cbobject in self . _metadata : del self . _metadata [ cbobject ]
7061	def sqs_delete_queue ( queue_url , client = None ) : if not client : client = boto3 . client ( 'sqs' ) try : client . delete_queue ( QueueUrl = queue_url ) return True except Exception as e : LOGEXCEPTION ( 'could not delete the specified queue: %s' % ( queue_url , ) ) return False
7489	def importvcf ( vcffile , locifile ) : try : with open ( invcffile , 'r' ) as invcf : for line in invcf : if line . split ( ) [ 0 ] == "#CHROM" : names_col = line . split ( ) . index ( "FORMAT" ) + 1 names = line . split ( ) [ names_col : ] LOGGER . debug ( "Got names - %s" , names ) break print ( "wat" ) except Exception : print ( "wat" )
9533	def unsign ( self , signed_value , ttl = None ) : h_size , d_size = struct . calcsize ( '>cQ' ) , self . digest . digest_size fmt = '>cQ%ds%ds' % ( len ( signed_value ) - h_size - d_size , d_size ) try : version , timestamp , value , sig = struct . unpack ( fmt , signed_value ) except struct . error : raise BadSignature ( 'Signature is not valid' ) if version != self . version : raise BadSignature ( 'Signature version not supported' ) if ttl is not None : if isinstance ( ttl , datetime . timedelta ) : ttl = ttl . total_seconds ( ) age = abs ( time . time ( ) - timestamp ) if age > ttl + _MAX_CLOCK_SKEW : raise SignatureExpired ( 'Signature age %s > %s seconds' % ( age , ttl ) ) try : self . signature ( signed_value [ : - d_size ] ) . verify ( sig ) except InvalidSignature : raise BadSignature ( 'Signature "%s" does not match' % binascii . b2a_base64 ( sig ) ) return value
4805	def _err ( self , msg ) : out = '%s%s' % ( '[%s] ' % self . description if len ( self . description ) > 0 else '' , msg ) if self . kind == 'warn' : print ( out ) return self elif self . kind == 'soft' : global _soft_err _soft_err . append ( out ) return self else : raise AssertionError ( out )
354	def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( "network is None." ) if sess is None : raise ValueError ( "session is None." ) if not os . path . exists ( name ) : logging . error ( "file {} doesn't exist." . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( "[*] Load {} SUCCESS!" . format ( name ) ) return network
8332	def findNextSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . nextSiblingGenerator , ** kwargs )
5792	def _cert_callback ( callback , der_cert , reason ) : if not callback : return callback ( x509 . Certificate . load ( der_cert ) , reason )
2712	def pretty_print ( obj , indent = False ) : if indent : return json . dumps ( obj , sort_keys = True , indent = 2 , separators = ( ',' , ': ' ) ) else : return json . dumps ( obj , sort_keys = True )
12905	def copy ( self ) : return self . __class__ ( name = self . name , valueType = self . valueType , defaultValue = self . defaultValue , hashIndex = self . hashIndex )
5089	def dropHistoricalTable ( apps , schema_editor ) : table_name = 'sap_success_factors_historicalsapsuccessfactorsenterprisecus80ad' if table_name in connection . introspection . table_names ( ) : migrations . DeleteModel ( name = table_name , )
12692	def write_tersoff_potential ( parameters ) : lines = [ ] for ( e1 , e2 , e3 ) , params in parameters . items ( ) : if len ( params ) != 14 : raise ValueError ( 'tersoff three body potential expects 14 parameters' ) lines . append ( ' ' . join ( [ e1 , e2 , e3 ] + [ '{:16.8g}' . format ( _ ) for _ in params ] ) ) return '\n' . join ( lines )
7586	def _get_boots ( arr , nboots ) : boots = np . zeros ( ( nboots , ) ) for bidx in xrange ( nboots ) : lidx = np . random . randint ( 0 , arr . shape [ 0 ] , arr . shape [ 0 ] ) tarr = arr [ lidx ] _ , _ , dst = _prop_dstat ( tarr ) boots [ bidx ] = dst return boots
5143	def search_for_comment ( self , lineno , default = None ) : if not self . index : self . make_index ( ) block = self . index . get ( lineno , None ) text = getattr ( block , 'text' , default ) return text
3143	def get ( self , file_id , ** queryparams ) : self . file_id = file_id return self . _mc_client . _get ( url = self . _build_path ( file_id ) , ** queryparams )
11533	def setup ( self , port ) : port = str ( port ) self . _serial = serial . Serial ( port , 115200 , timeout = 2 ) time . sleep ( 2 ) if not self . _serial . is_open : raise RuntimeError ( 'Could not connect to Arduino' ) self . _serial . write ( b'\x01' ) if self . _serial . read ( ) != b'\x06' : raise RuntimeError ( 'Could not connect to Arduino' ) ps = [ p for p in self . available_pins ( ) if p [ 'digital' ] [ 'output' ] ] for pin in ps : self . _set_pin_direction ( pin [ 'id' ] , ahio . Direction . Output )
13459	def create_ical ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) start = event . start_date start = datetime . datetime ( start . year , start . month , start . day ) if event . end_date : end = event . end_date end = datetime . datetime ( end . year , end . month , end . day ) else : end = start cal = card_me . iCalendar ( ) cal . add ( 'method' ) . value = 'PUBLISH' vevent = cal . add ( 'vevent' ) vevent . add ( 'dtstart' ) . value = start vevent . add ( 'dtend' ) . value = end vevent . add ( 'dtstamp' ) . value = datetime . datetime . now ( ) vevent . add ( 'summary' ) . value = event . name response = HttpResponse ( cal . serialize ( ) , content_type = 'text/calendar' ) response [ 'Filename' ] = 'filename.ics' response [ 'Content-Disposition' ] = 'attachment; filename=filename.ics' return response
12398	def gen_methods ( self , * args , ** kwargs ) : token = args [ 0 ] inst = self . inst prefix = self . _method_prefix for method_key in self . gen_method_keys ( * args , ** kwargs ) : method = getattr ( inst , prefix + method_key , None ) if method is not None : yield method typename = type ( token ) . __name__ yield from self . check_basetype ( token , typename , self . builtins . get ( typename ) ) for basetype_name in self . interp_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . types , basetype_name , None ) ) for basetype_name in self . abc_types : yield from self . check_basetype ( token , basetype_name , getattr ( self . collections , basetype_name , None ) ) yield from self . gen_generic ( )
2535	def set_doc_comment ( self , doc , comment ) : if not self . doc_comment_set : self . doc_comment_set = True doc . comment = comment else : raise CardinalityError ( 'Document::Comment' )
8146	def levels ( self ) : h = self . img . histogram ( ) r = h [ 0 : 255 ] g = h [ 256 : 511 ] b = h [ 512 : 767 ] a = h [ 768 : 1024 ] return r , g , b , a
9973	def _get_namedrange ( book , rangename , sheetname = None ) : def cond ( namedef ) : if namedef . type . upper ( ) == "RANGE" : if namedef . name . upper ( ) == rangename . upper ( ) : if sheetname is None : if not namedef . localSheetId : return True else : sheet_id = [ sht . upper ( ) for sht in book . sheetnames ] . index ( sheetname . upper ( ) ) if namedef . localSheetId == sheet_id : return True return False def get_destinations ( name_def ) : from openpyxl . formula import Tokenizer from openpyxl . utils . cell import SHEETRANGE_RE if name_def . type == "RANGE" : tok = Tokenizer ( "=" + name_def . value ) for part in tok . items : if part . subtype == "RANGE" : m = SHEETRANGE_RE . match ( part . value ) if m . group ( "quoted" ) : sheet_name = m . group ( "quoted" ) else : sheet_name = m . group ( "notquoted" ) yield sheet_name , m . group ( "cells" ) namedef = next ( ( item for item in book . defined_names . definedName if cond ( item ) ) , None ) if namedef is None : return None dests = get_destinations ( namedef ) xlranges = [ ] sheetnames_upper = [ name . upper ( ) for name in book . sheetnames ] for sht , addr in dests : if sheetname : sht = sheetname index = sheetnames_upper . index ( sht . upper ( ) ) xlranges . append ( book . worksheets [ index ] [ addr ] ) if len ( xlranges ) == 1 : return xlranges [ 0 ] else : return xlranges
12547	def abs_img ( img ) : bool_img = np . abs ( read_img ( img ) . get_data ( ) ) return bool_img . astype ( int )
4649	def json ( self ) : if not self . _is_constructed ( ) or self . _is_require_reconstruction ( ) : self . constructTx ( ) return dict ( self )
5947	def unlink_f ( path ) : try : os . unlink ( path ) except OSError as err : if err . errno != errno . ENOENT : raise
10153	def _extract_operation_from_view ( self , view , args ) : op = { 'responses' : { 'default' : { 'description' : 'UNDOCUMENTED RESPONSE' } } , } renderer = args . get ( 'renderer' , '' ) if "json" in renderer : produces = [ 'application/json' ] elif renderer == 'xml' : produces = [ 'text/xml' ] else : produces = None if produces : op . setdefault ( 'produces' , produces ) consumes = args . get ( 'content_type' ) if consumes is not None : consumes = to_list ( consumes ) consumes = [ x for x in consumes if not callable ( x ) ] op [ 'consumes' ] = consumes is_colander = self . _is_colander_schema ( args ) if is_colander : schema = self . _extract_transform_colander_schema ( args ) parameters = self . parameters . from_schema ( schema ) else : parameters = None if parameters : op [ 'parameters' ] = parameters if isinstance ( view , six . string_types ) : if 'klass' in args : ob = args [ 'klass' ] view_ = getattr ( ob , view . lower ( ) ) docstring = trim ( view_ . __doc__ ) else : docstring = str ( trim ( view . __doc__ ) ) if docstring and self . summary_docstrings : op [ 'summary' ] = docstring if 'response_schemas' in args : op [ 'responses' ] = self . responses . from_schema_mapping ( args [ 'response_schemas' ] ) if 'tags' in args : op [ 'tags' ] = args [ 'tags' ] if 'operation_id' in args : op [ 'operationId' ] = args [ 'operation_id' ] if 'api_security' in args : op [ 'security' ] = args [ 'api_security' ] return op
10012	def parse_option_settings ( option_settings ) : ret = [ ] for namespace , params in list ( option_settings . items ( ) ) : for key , value in list ( params . items ( ) ) : ret . append ( ( namespace , key , value ) ) return ret
1311	def KeyboardInput ( wVk : int , wScan : int , dwFlags : int = KeyboardEventFlag . KeyDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( KEYBDINPUT ( wVk , wScan , dwFlags , time_ , None ) )
11841	def TraceAgent ( agent ) : old_program = agent . program def new_program ( percept ) : action = old_program ( percept ) print '%s perceives %s and does %s' % ( agent , percept , action ) return action agent . program = new_program return agent
890	def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]
1358	def get_argument_component ( self ) : try : component = self . get_argument ( constants . PARAM_COMPONENT ) return component except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
9305	def handle_date_mismatch ( self , req ) : req_datetime = self . get_request_date ( req ) new_key_date = req_datetime . strftime ( '%Y%m%d' ) self . regenerate_signing_key ( date = new_key_date )
12363	def get ( self , id , ** kwargs ) : return ( super ( MutableCollection , self ) . get ( ( id , ) , ** kwargs ) . get ( self . singular , None ) )
2662	def _hold_block ( self , block_id ) : managers = self . connected_managers for manager in managers : if manager [ 'block_id' ] == block_id : logger . debug ( "[HOLD_BLOCK]: Sending hold to manager:{}" . format ( manager [ 'manager' ] ) ) self . hold_worker ( manager [ 'manager' ] )
6029	def set_xy_labels ( units , kpc_per_arcsec , xlabelsize , ylabelsize , xyticksize ) : if units in 'arcsec' or kpc_per_arcsec is None : plt . xlabel ( 'x (arcsec)' , fontsize = xlabelsize ) plt . ylabel ( 'y (arcsec)' , fontsize = ylabelsize ) elif units in 'kpc' : plt . xlabel ( 'x (kpc)' , fontsize = xlabelsize ) plt . ylabel ( 'y (kpc)' , fontsize = ylabelsize ) else : raise exc . PlottingException ( 'The units supplied to the plotted are not a valid string (must be pixels | ' 'arcsec | kpc)' ) plt . tick_params ( labelsize = xyticksize )
4145	def speriodogram ( x , NFFT = None , detrend = True , sampling = 1. , scale_by_freq = True , window = 'hamming' , axis = 0 ) : x = np . array ( x ) if x . ndim == 1 : axis = 0 r = x . shape [ 0 ] w = Window ( r , window ) w = w . data elif x . ndim == 2 : logging . debug ( '2D array. each row is a 1D array' ) [ r , c ] = x . shape w = np . array ( [ Window ( r , window ) . data for this in range ( c ) ] ) . reshape ( r , c ) if NFFT is None : NFFT = len ( x ) isreal = np . isrealobj ( x ) if detrend == True : m = np . mean ( x , axis = axis ) else : m = 0 if isreal == True : if x . ndim == 2 : res = ( abs ( rfft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( rfft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r else : if x . ndim == 2 : res = ( abs ( fft ( x * w - m , NFFT , axis = 0 ) ) ) ** 2. / r else : res = ( abs ( fft ( x * w - m , NFFT , axis = - 1 ) ) ) ** 2. / r if scale_by_freq is True : df = sampling / float ( NFFT ) res *= 2 * np . pi / df if x . ndim == 1 : return res . transpose ( ) else : return res
2468	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True if validations . validate_file_lics_comment ( text ) : self . file ( doc ) . license_comment = str_from_text ( text ) else : raise SPDXValueError ( 'File::LicenseComment' ) else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
5477	def parse_rfc3339_utc_string ( rfc3339_utc_string ) : m = re . match ( r'(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2}).?(\d*)Z' , rfc3339_utc_string ) if not m : return None groups = m . groups ( ) if len ( groups [ 6 ] ) not in ( 0 , 3 , 6 , 9 ) : return None g = [ int ( val ) for val in groups [ : 6 ] ] fraction = groups [ 6 ] if not fraction : micros = 0 elif len ( fraction ) == 3 : micros = int ( fraction ) * 1000 elif len ( fraction ) == 6 : micros = int ( fraction ) elif len ( fraction ) == 9 : micros = int ( round ( int ( fraction ) / 1000 ) ) else : assert False , 'Fraction length not 0, 6, or 9: {}' . len ( fraction ) try : return datetime ( g [ 0 ] , g [ 1 ] , g [ 2 ] , g [ 3 ] , g [ 4 ] , g [ 5 ] , micros , tzinfo = pytz . utc ) except ValueError as e : assert False , 'Could not parse RFC3339 datestring: {} exception: {}' . format ( rfc3339_utc_string , e )
5443	def _validate_paths_or_fail ( uri , recursive ) : path , filename = os . path . split ( uri ) if '[' in uri or ']' in uri : raise ValueError ( 'Square bracket (character ranges) are not supported: %s' % uri ) if '?' in uri : raise ValueError ( 'Question mark wildcards are not supported: %s' % uri ) if '*' in path : raise ValueError ( 'Path wildcard (*) are only supported for files: %s' % uri ) if '**' in filename : raise ValueError ( 'Recursive wildcards ("**") not supported: %s' % uri ) if filename in ( '..' , '.' ) : raise ValueError ( 'Path characters ".." and "." not supported ' 'for file names: %s' % uri ) if not recursive and not filename : raise ValueError ( 'Input or output values that are not recursive must ' 'reference a filename or wildcard: %s' % uri )
10224	def get_chaotic_pairs ( graph : BELGraph ) -> SetOfNodePairs : cg = get_causal_subgraph ( graph ) results = set ( ) for u , v , d in cg . edges ( data = True ) : if d [ RELATION ] not in CAUSAL_INCREASE_RELATIONS : continue if cg . has_edge ( v , u ) and any ( dd [ RELATION ] in CAUSAL_INCREASE_RELATIONS for dd in cg [ v ] [ u ] . values ( ) ) : results . add ( tuple ( sorted ( [ u , v ] , key = str ) ) ) return results
11477	def _create_or_reuse_folder ( local_folder , parent_folder_id , reuse_existing = False ) : local_folder_name = os . path . basename ( local_folder ) folder_id = None if reuse_existing : children = session . communicator . folder_children ( session . token , parent_folder_id ) folders = children [ 'folders' ] for folder in folders : if folder [ 'name' ] == local_folder_name : folder_id = folder [ 'folder_id' ] break if folder_id is None : new_folder = session . communicator . create_folder ( session . token , local_folder_name , parent_folder_id ) folder_id = new_folder [ 'folder_id' ] return folder_id
5970	def MD_restrained ( dirname = 'MD_POSRES' , ** kwargs ) : logger . info ( "[{dirname!s}] Setting up MD with position restraints..." . format ( ** vars ( ) ) ) kwargs . setdefault ( 'struct' , 'em/em.pdb' ) kwargs . setdefault ( 'qname' , 'PR_GMX' ) kwargs . setdefault ( 'define' , '-DPOSRES' ) kwargs . setdefault ( 'nstxout' , '50000' ) kwargs . setdefault ( 'nstvout' , '50000' ) kwargs . setdefault ( 'nstfout' , '0' ) kwargs . setdefault ( 'nstlog' , '500' ) kwargs . setdefault ( 'nstenergy' , '2500' ) kwargs . setdefault ( 'nstxtcout' , '5000' ) kwargs . setdefault ( 'refcoord_scaling' , 'com' ) kwargs . setdefault ( 'Pcoupl' , "Berendsen" ) new_kwargs = _setup_MD ( dirname , ** kwargs ) new_kwargs . pop ( 'define' , None ) new_kwargs . pop ( 'refcoord_scaling' , None ) new_kwargs . pop ( 'Pcoupl' , None ) return new_kwargs
8729	def strptime ( s , fmt , tzinfo = None ) : res = time . strptime ( s , fmt ) return datetime . datetime ( tzinfo = tzinfo , * res [ : 6 ] )
5142	def make_index ( self ) : for prev , block in zip ( self . blocks [ : - 1 ] , self . blocks [ 1 : ] ) : if not block . is_comment : self . index [ block . start_lineno ] = prev
2572	def dbm_starter ( priority_msgs , resource_msgs , * args , ** kwargs ) : dbm = DatabaseManager ( * args , ** kwargs ) dbm . start ( priority_msgs , resource_msgs )
13408	def setupUI ( self ) : labelSizePolicy = QSizePolicy ( QSizePolicy . Fixed , QSizePolicy . Fixed ) labelSizePolicy . setHorizontalStretch ( 0 ) labelSizePolicy . setVerticalStretch ( 0 ) menuSizePolicy = QSizePolicy ( QSizePolicy . Expanding , QSizePolicy . Fixed ) menuSizePolicy . setHorizontalStretch ( 0 ) menuSizePolicy . setVerticalStretch ( 0 ) logTypeLayout = QHBoxLayout ( ) logTypeLayout . setSpacing ( 0 ) typeLabel = QLabel ( "Log Type:" ) typeLabel . setMinimumSize ( QSize ( 65 , 0 ) ) typeLabel . setMaximumSize ( QSize ( 65 , 16777215 ) ) typeLabel . setSizePolicy ( labelSizePolicy ) logTypeLayout . addWidget ( typeLabel ) self . logType = QComboBox ( self ) self . logType . setMinimumSize ( QSize ( 100 , 0 ) ) self . logType . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . logType . sizePolicy ( ) . hasHeightForWidth ( ) ) self . logType . setSizePolicy ( menuSizePolicy ) logTypeLayout . addWidget ( self . logType ) logTypeLayout . setStretch ( 1 , 6 ) programLayout = QHBoxLayout ( ) programLayout . setSpacing ( 0 ) programLabel = QLabel ( "Program:" ) programLabel . setMinimumSize ( QSize ( 60 , 0 ) ) programLabel . setMaximumSize ( QSize ( 60 , 16777215 ) ) programLabel . setSizePolicy ( labelSizePolicy ) programLayout . addWidget ( programLabel ) self . programName = QComboBox ( self ) self . programName . setMinimumSize ( QSize ( 100 , 0 ) ) self . programName . setMaximumSize ( QSize ( 150 , 16777215 ) ) menuSizePolicy . setHeightForWidth ( self . programName . sizePolicy ( ) . hasHeightForWidth ( ) ) self . programName . setSizePolicy ( menuSizePolicy ) programLayout . addWidget ( self . programName ) programLayout . setStretch ( 1 , 6 ) if self . initialInstance : self . logButton = QPushButton ( "+" , self ) self . logButton . setToolTip ( "Add logbook" ) else : self . logButton = QPushButton ( "-" ) self . logButton . setToolTip ( "Remove logbook" ) self . logButton . setMinimumSize ( QSize ( 16 , 16 ) ) self . logButton . setMaximumSize ( QSize ( 16 , 16 ) ) self . logButton . setObjectName ( "roundButton" ) self . logButton . setStyleSheet ( "QPushButton {border-radius: 8px;}" ) self . _logSelectLayout = QHBoxLayout ( ) self . _logSelectLayout . setSpacing ( 6 ) self . _logSelectLayout . addLayout ( logTypeLayout ) self . _logSelectLayout . addLayout ( programLayout ) self . _logSelectLayout . addWidget ( self . logButton ) self . _logSelectLayout . setStretch ( 0 , 6 ) self . _logSelectLayout . setStretch ( 1 , 6 )
8120	def intersection ( self , b ) : if not self . intersects ( b ) : return None mx , my = max ( self . x , b . x ) , max ( self . y , b . y ) return Bounds ( mx , my , min ( self . x + self . width , b . x + b . width ) - mx , min ( self . y + self . height , b . y + b . height ) - my )
3719	def conductivity ( CASRN = None , AvailableMethods = False , Method = None , full_info = True ) : r def list_methods ( ) : methods = [ ] if CASRN in Lange_cond_pure . index : methods . append ( LANGE_COND ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == LANGE_COND : kappa = float ( Lange_cond_pure . at [ CASRN , 'Conductivity' ] ) if full_info : T = float ( Lange_cond_pure . at [ CASRN , 'T' ] ) elif Method == NONE : kappa , T = None , None else : raise Exception ( 'Failure in in function' ) if full_info : return kappa , T else : return kappa
5931	def scale_dihedrals ( mol , dihedrals , scale , banned_lines = None ) : if banned_lines is None : banned_lines = [ ] new_dihedrals = [ ] for dh in mol . dihedrals : atypes = dh . atom1 . get_atomtype ( ) , dh . atom2 . get_atomtype ( ) , dh . atom3 . get_atomtype ( ) , dh . atom4 . get_atomtype ( ) atypes = [ a . replace ( "_" , "" ) . replace ( "=" , "" ) for a in atypes ] if dh . gromacs [ 'param' ] != [ ] : for p in dh . gromacs [ 'param' ] : p [ 'kch' ] *= scale new_dihedrals . append ( dh ) continue for iswitch in range ( 32 ) : if ( iswitch % 2 == 0 ) : a1 = atypes [ 0 ] a2 = atypes [ 1 ] a3 = atypes [ 2 ] a4 = atypes [ 3 ] else : a1 = atypes [ 3 ] a2 = atypes [ 2 ] a3 = atypes [ 1 ] a4 = atypes [ 0 ] if ( ( iswitch // 2 ) % 2 == 1 ) : a1 = "X" if ( ( iswitch // 4 ) % 2 == 1 ) : a2 = "X" if ( ( iswitch // 8 ) % 2 == 1 ) : a3 = "X" if ( ( iswitch // 16 ) % 2 == 1 ) : a4 = "X" key = "{0}-{1}-{2}-{3}-{4}" . format ( a1 , a2 , a3 , a4 , dh . gromacs [ 'func' ] ) if ( key in dihedrals ) : for i , dt in enumerate ( dihedrals [ key ] ) : dhA = copy . deepcopy ( dh ) param = copy . deepcopy ( dt . gromacs [ 'param' ] ) if not dihedrals [ key ] [ 0 ] . line in banned_lines : for p in param : p [ 'kchi' ] *= scale dhA . gromacs [ 'param' ] = param if i == 0 : dhA . comment = "; banned lines {0} found={1}\n" . format ( " " . join ( map ( str , banned_lines ) ) , 1 if dt . line in banned_lines else 0 ) dhA . comment += "; parameters for types {}-{}-{}-{}-9 at LINE({})\n" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype , dt . line ) . replace ( "_" , "" ) name = "{}-{}-{}-{}-9" . format ( dhA . atom1 . atomtype , dhA . atom2 . atomtype , dhA . atom3 . atomtype , dhA . atom4 . atomtype ) . replace ( "_" , "" ) new_dihedrals . append ( dhA ) break mol . dihedrals = new_dihedrals return mol
9582	def write_elements ( fd , mtp , data , is_name = False ) : fmt = etypes [ mtp ] [ 'fmt' ] if isinstance ( data , Sequence ) : if fmt == 's' or is_name : if isinstance ( data , bytes ) : if is_name and len ( data ) > 31 : raise ValueError ( 'Name "{}" is too long (max. 31 ' 'characters allowed)' . format ( data ) ) fmt = '{}s' . format ( len ( data ) ) data = ( data , ) else : fmt = '' . join ( '{}s' . format ( len ( s ) ) for s in data ) else : l = len ( data ) if l == 0 : fmt = '' if l > 1 : fmt = '{}{}' . format ( l , fmt ) else : data = ( data , ) num_bytes = struct . calcsize ( fmt ) if num_bytes <= 4 : if num_bytes < 4 : fmt += '{}x' . format ( 4 - num_bytes ) fd . write ( struct . pack ( 'hh' + fmt , etypes [ mtp ] [ 'n' ] , * chain ( [ num_bytes ] , data ) ) ) return fd . write ( struct . pack ( 'b3xI' , etypes [ mtp ] [ 'n' ] , num_bytes ) ) mod8 = num_bytes % 8 if mod8 : fmt += '{}x' . format ( 8 - mod8 ) fd . write ( struct . pack ( fmt , * data ) )
7232	def get ( self , ID , index = 'vector-web-s' ) : url = self . get_url % index r = self . gbdx_connection . get ( url + ID ) r . raise_for_status ( ) return r . json ( )
829	def encodedBitDescription ( self , bitOffset , formatted = False ) : ( prevFieldName , prevFieldOffset ) = ( None , None ) description = self . getDescription ( ) for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if formatted : offset = offset + i if bitOffset == offset - 1 : prevFieldName = "separator" prevFieldOffset = bitOffset break if bitOffset < offset : break ( prevFieldName , prevFieldOffset ) = ( name , offset ) width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) if prevFieldOffset is None or bitOffset > self . getWidth ( ) : raise IndexError ( "Bit is outside of allowable range: [0 - %d]" % width ) return ( prevFieldName , bitOffset - prevFieldOffset )
2213	def delete ( path , verbose = False ) : if not os . path . exists ( path ) : if os . path . islink ( path ) : if verbose : print ( 'Deleting broken link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : print ( 'Deleting broken directory link="{}"' . format ( path ) ) os . rmdir ( path ) elif os . path . isfile ( path ) : if verbose : print ( 'Deleting broken file link="{}"' . format ( path ) ) os . unlink ( path ) else : if verbose : print ( 'Not deleting non-existant path="{}"' . format ( path ) ) else : if os . path . islink ( path ) : if verbose : print ( 'Deleting symbolic link="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isfile ( path ) : if verbose : print ( 'Deleting file="{}"' . format ( path ) ) os . unlink ( path ) elif os . path . isdir ( path ) : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) if sys . platform . startswith ( 'win32' ) : from ubelt import _win32_links _win32_links . _win32_rmtree ( path , verbose = verbose ) else : import shutil shutil . rmtree ( path )
3749	def calculate_P ( self , T , P , method ) : r if method == COOLPROP : mu = PropsSI ( 'V' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : mu = self . interpolate_P ( T , P , method ) return mu
1651	def _ClassifyInclude ( fileinfo , include , is_system ) : is_cpp_h = include in _CPP_HEADERS if is_system and os . path . splitext ( include ) [ 1 ] in [ '.hpp' , '.hxx' , '.h++' ] : is_system = False if is_system : if is_cpp_h : return _CPP_SYS_HEADER else : return _C_SYS_HEADER target_dir , target_base = ( os . path . split ( _DropCommonSuffixes ( fileinfo . RepositoryName ( ) ) ) ) include_dir , include_base = os . path . split ( _DropCommonSuffixes ( include ) ) target_dir_pub = os . path . normpath ( target_dir + '/../public' ) target_dir_pub = target_dir_pub . replace ( '\\' , '/' ) if target_base == include_base and ( include_dir == target_dir or include_dir == target_dir_pub ) : return _LIKELY_MY_HEADER target_first_component = _RE_FIRST_COMPONENT . match ( target_base ) include_first_component = _RE_FIRST_COMPONENT . match ( include_base ) if ( target_first_component and include_first_component and target_first_component . group ( 0 ) == include_first_component . group ( 0 ) ) : return _POSSIBLE_MY_HEADER return _OTHER_HEADER
3550	def list_characteristics ( self ) : paths = self . _props . Get ( _SERVICE_INTERFACE , 'Characteristics' ) return map ( BluezGattCharacteristic , get_provider ( ) . _get_objects_by_path ( paths ) )
10777	def make_clean_figure ( figsize , remove_tooltips = False , remove_keybindings = False ) : tooltip = mpl . rcParams [ 'toolbar' ] if remove_tooltips : mpl . rcParams [ 'toolbar' ] = 'None' fig = pl . figure ( figsize = figsize ) mpl . rcParams [ 'toolbar' ] = tooltip if remove_keybindings : fig . canvas . mpl_disconnect ( fig . canvas . manager . key_press_handler_id ) return fig
9993	def get_dynspace ( self , args , kwargs = None ) : node = get_node ( self , * convert_args ( args , kwargs ) ) key = node [ KEY ] if key in self . param_spaces : return self . param_spaces [ key ] else : last_self = self . system . self self . system . self = self try : space_args = self . eval_formula ( node ) finally : self . system . self = last_self if space_args is None : space_args = { "bases" : [ self ] } else : if "bases" in space_args : bases = get_impls ( space_args [ "bases" ] ) if isinstance ( bases , StaticSpaceImpl ) : space_args [ "bases" ] = [ bases ] elif bases is None : space_args [ "bases" ] = [ self ] else : space_args [ "bases" ] = bases else : space_args [ "bases" ] = [ self ] space_args [ "arguments" ] = node_get_args ( node ) space = self . _new_dynspace ( ** space_args ) self . param_spaces [ key ] = space space . inherit ( clear_value = False ) return space
5985	def inversion_psf_shape_tag_from_inversion_psf_shape ( inversion_psf_shape ) : if inversion_psf_shape is None : return '' else : y = str ( inversion_psf_shape [ 0 ] ) x = str ( inversion_psf_shape [ 1 ] ) return ( '_inv_psf_' + y + 'x' + x )
12251	def get_all_keys ( self , * args , ** kwargs ) : if kwargs . pop ( 'force' , None ) : headers = kwargs . get ( 'headers' , args [ 0 ] if len ( args ) else None ) or dict ( ) headers [ 'force' ] = True kwargs [ 'headers' ] = headers return super ( Bucket , self ) . get_all_keys ( * args , ** kwargs )
10950	def sample ( field , inds = None , slicer = None , flat = True ) : if inds is not None : out = field . ravel ( ) [ inds ] elif slicer is not None : out = field [ slicer ] . ravel ( ) else : out = field if flat : return out . ravel ( ) return out
13746	def create_item ( self , hash_key , start = 0 , extra_attrs = None ) : table = self . get_table ( ) now = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) attrs = { 'created_on' : now , 'modified_on' : now , 'count' : start , } if extra_attrs : attrs . update ( extra_attrs ) item = table . new_item ( hash_key = hash_key , attrs = attrs , ) return item
6467	def csi ( self , capname , * args ) : value = curses . tigetstr ( capname ) if value is None : return b'' else : return curses . tparm ( value , * args )
7278	def play_sync ( self ) : self . play ( ) logger . info ( "Playing synchronously" ) try : time . sleep ( 0.05 ) logger . debug ( "Wait for playing to start" ) while self . is_playing ( ) : time . sleep ( 0.05 ) except DBusException : logger . error ( "Cannot play synchronously any longer as DBus calls timed out." )
5551	def snap_bounds ( bounds = None , pyramid = None , zoom = None ) : if not isinstance ( bounds , ( tuple , list ) ) : raise TypeError ( "bounds must be either a tuple or a list" ) if len ( bounds ) != 4 : raise ValueError ( "bounds has to have exactly four values" ) if not isinstance ( pyramid , BufferedTilePyramid ) : raise TypeError ( "pyramid has to be a BufferedTilePyramid" ) bounds = Bounds ( * bounds ) lb = pyramid . tile_from_xy ( bounds . left , bounds . bottom , zoom , on_edge_use = "rt" ) . bounds rt = pyramid . tile_from_xy ( bounds . right , bounds . top , zoom , on_edge_use = "lb" ) . bounds return Bounds ( lb . left , lb . bottom , rt . right , rt . top )
2924	def _predict ( self , my_task , seen = None , looked_ahead = 0 ) : if my_task . _is_finished ( ) : return if seen is None : seen = [ ] elif self in seen : return if not my_task . _is_finished ( ) : self . _predict_hook ( my_task ) if not my_task . _is_definite ( ) : if looked_ahead + 1 >= self . lookahead : return seen . append ( self ) for child in my_task . children : child . task_spec . _predict ( child , seen [ : ] , looked_ahead + 1 )
3579	def clear_cached_data ( self ) : for device in self . list_devices ( ) : if device . is_connected : continue adapter = dbus . Interface ( self . _bus . get_object ( 'org.bluez' , device . _adapter ) , _ADAPTER_INTERFACE ) adapter . RemoveDevice ( device . _device . object_path )
9005	def add_new_pattern ( self , id_ , name = None ) : if name is None : name = id_ pattern = self . _parser . new_pattern ( id_ , name ) self . _patterns . append ( pattern ) return pattern
3076	def credentials ( self ) : ctx = _app_ctx_stack . top if not hasattr ( ctx , _CREDENTIALS_KEY ) : ctx . google_oauth2_credentials = self . storage . get ( ) return ctx . google_oauth2_credentials
5568	def bounds_at_zoom ( self , zoom = None ) : return ( ) if self . area_at_zoom ( zoom ) . is_empty else Bounds ( * self . area_at_zoom ( zoom ) . bounds )
12208	def add ( self , * entries ) : for entry in entries : if isinstance ( entry , string_types ) : self . _add_entries ( database . parse_string ( entry , bib_format = 'bibtex' ) ) else : self . _add_entries ( entry )
7902	def set_stream ( self , stream ) : self . jid = stream . me self . stream = stream for r in self . rooms . values ( ) : r . set_stream ( stream )
7917	def are_domains_equal ( domain1 , domain2 ) : domain1 = domain1 . encode ( "idna" ) domain2 = domain2 . encode ( "idna" ) return domain1 . lower ( ) == domain2 . lower ( )
13552	def _post_resource ( self , url , body ) : headers = { "Content-Type" : "application/json" , "Accept" : "application/json" } if self . token : headers [ "W-Token" ] = "%s" % self . token response = WhenIWork_DAO ( ) . postURL ( url , headers , json . dumps ( body ) ) if not ( response . status == 200 or response . status == 204 ) : raise DataFailureException ( url , response . status , response . data ) return json . loads ( response . data )
2212	def touch ( fpath , mode = 0o666 , dir_fd = None , verbose = 0 , ** kwargs ) : if verbose : print ( 'Touching file {}' . format ( fpath ) ) if six . PY2 : with open ( fpath , 'a' ) : os . utime ( fpath , None ) else : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fpath , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fpath , dir_fd = None if os . supports_fd else dir_fd , ** kwargs ) return fpath
6742	def render_to_string ( template , extra = None ) : from jinja2 import Template extra = extra or { } final_fqfn = find_template ( template ) assert final_fqfn , 'Template not found: %s' % template template_content = open ( final_fqfn , 'r' ) . read ( ) t = Template ( template_content ) if extra : context = env . copy ( ) context . update ( extra ) else : context = env rendered_content = t . render ( ** context ) rendered_content = rendered_content . replace ( '&quot;' , '"' ) return rendered_content
13809	def get_version ( relpath ) : from os . path import dirname , join if '__file__' not in globals ( ) : root = '.' else : root = dirname ( __file__ ) for line in open ( join ( root , relpath ) , 'rb' ) : line = line . decode ( 'cp437' ) if '__version__' in line : if '"' in line : return line . split ( '"' ) [ 1 ] elif "'" in line : return line . split ( "'" ) [ 1 ]
2926	def create_package ( self ) : self . input_path_prefix = None for filename in self . input_files : if not os . path . isfile ( filename ) : raise ValueError ( '%s does not exist or is not a file' % filename ) if self . input_path_prefix : full = os . path . abspath ( os . path . dirname ( filename ) ) while not ( full . startswith ( self . input_path_prefix ) and self . input_path_prefix ) : self . input_path_prefix = self . input_path_prefix [ : - 1 ] else : self . input_path_prefix = os . path . abspath ( os . path . dirname ( filename ) ) self . bpmn = { } for filename in self . input_files : bpmn = ET . parse ( filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn for filename , bpmn in list ( self . bpmn . items ( ) ) : bpmn = self . pre_parse_and_validate ( bpmn , filename ) self . bpmn [ os . path . abspath ( filename ) ] = bpmn for filename , bpmn in list ( self . bpmn . items ( ) ) : self . parser . add_bpmn_xml ( bpmn , filename = filename ) self . wf_spec = self . parser . get_spec ( self . entry_point_process ) self . package_zip = zipfile . ZipFile ( self . package_file , "w" , compression = zipfile . ZIP_DEFLATED ) done_files = set ( ) for spec in self . wf_spec . get_specs_depth_first ( ) : filename = spec . file if filename not in done_files : done_files . add ( filename ) bpmn = self . bpmn [ os . path . abspath ( filename ) ] self . write_to_package_zip ( "%s.bpmn" % spec . name , ET . tostring ( bpmn . getroot ( ) ) ) self . write_file_to_package_zip ( "src/" + self . _get_zip_path ( filename ) , filename ) self . _call_editor_hook ( 'package_for_editor' , spec , filename ) self . write_meta_data ( ) self . write_manifest ( ) self . package_zip . close ( )
2564	def start ( self ) : self . comm . Barrier ( ) logger . debug ( "Manager synced with workers" ) self . _kill_event = threading . Event ( ) self . _task_puller_thread = threading . Thread ( target = self . pull_tasks , args = ( self . _kill_event , ) ) self . _result_pusher_thread = threading . Thread ( target = self . push_results , args = ( self . _kill_event , ) ) self . _task_puller_thread . start ( ) self . _result_pusher_thread . start ( ) start = None result_counter = 0 task_recv_counter = 0 task_sent_counter = 0 logger . info ( "Loop start" ) while not self . _kill_event . is_set ( ) : time . sleep ( LOOP_SLOWDOWN ) timer = time . time ( ) + 0.05 counter = min ( 10 , comm . size ) while time . time ( ) < timer : info = MPI . Status ( ) if counter > 10 : logger . debug ( "Hit max mpi events per round" ) break if not self . comm . Iprobe ( status = info ) : logger . debug ( "Timer expired, processed {} mpi events" . format ( counter ) ) break else : tag = info . Get_tag ( ) logger . info ( "Message with tag {} received" . format ( tag ) ) counter += 1 if tag == RESULT_TAG : result = self . recv_result_from_workers ( ) self . pending_result_queue . put ( result ) result_counter += 1 elif tag == TASK_REQUEST_TAG : worker_rank = self . recv_task_request_from_workers ( ) self . ready_worker_queue . put ( worker_rank ) else : logger . error ( "Unknown tag {} - ignoring this message and continuing" . format ( tag ) ) available_worker_cnt = self . ready_worker_queue . qsize ( ) available_task_cnt = self . pending_task_queue . qsize ( ) logger . debug ( "[MAIN] Ready workers: {} Ready tasks: {}" . format ( available_worker_cnt , available_task_cnt ) ) this_round = min ( available_worker_cnt , available_task_cnt ) for i in range ( this_round ) : worker_rank = self . ready_worker_queue . get ( ) task = self . pending_task_queue . get ( ) comm . send ( task , dest = worker_rank , tag = worker_rank ) task_sent_counter += 1 logger . debug ( "Assigning worker:{} task:{}" . format ( worker_rank , task [ 'task_id' ] ) ) if not start : start = time . time ( ) logger . debug ( "Tasks recvd:{} Tasks dispatched:{} Results recvd:{}" . format ( task_recv_counter , task_sent_counter , result_counter ) ) self . _task_puller_thread . join ( ) self . _result_pusher_thread . join ( ) self . task_incoming . close ( ) self . result_outgoing . close ( ) self . context . term ( ) delta = time . time ( ) - start logger . info ( "mpi_worker_pool ran for {} seconds" . format ( delta ) )
1787	def DAA ( cpu ) : cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) oldAL = cpu . AL cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( cpu . CF , cpu . AL < oldAL ) , cpu . CF ) cpu . CF = Operators . OR ( ( cpu . AL & 0xf0 ) > 0x90 , cpu . CF ) cpu . AL = Operators . ITEBV ( 8 , cpu . CF , cpu . AL + 0x60 , cpu . AL ) cpu . ZF = cpu . AL == 0 cpu . SF = ( cpu . AL & 0x80 ) != 0 cpu . PF = cpu . _calculate_parity_flag ( cpu . AL )
7349	def parse_token ( response ) : items = response . split ( "&" ) items = [ item . split ( "=" ) for item in items ] return { key : value for key , value in items }
2488	def create_disjunction_node ( self , disjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . DisjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( disjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
4062	def show_condition_operators ( self , condition ) : permitted_operators = self . savedsearch . conditions_operators . get ( condition ) permitted_operators_list = set ( [ self . savedsearch . operators . get ( op ) for op in permitted_operators ] ) return permitted_operators_list
12725	def cfms ( self , cfms ) : _set_params ( self . ode_obj , 'CFM' , cfms , self . ADOF + self . LDOF )
10742	def print_profile ( function ) : import memory_profiler def wrapper ( * args , ** kwargs ) : m = StringIO ( ) pr = cProfile . Profile ( ) pr . enable ( ) temp_func = memory_profiler . profile ( func = function , stream = m , precision = 4 ) output = temp_func ( * args , ** kwargs ) print ( m . getvalue ( ) ) pr . disable ( ) ps = pstats . Stats ( pr ) ps . sort_stats ( 'cumulative' ) . print_stats ( '(?!.*memory_profiler.*)(^.*$)' , 20 ) m . close ( ) return output return wrapper
1212	def WorkerAgentGenerator ( agent_class ) : if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : def __init__ ( self , model = None , ** kwargs ) : self . model = model if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( "network" ) super ( WorkerAgent , self ) . __init__ ( ** kwargs ) def initialize_model ( self ) : return self . model return WorkerAgent
903	def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )
2613	def pack_apply_message ( f , args , kwargs , buffer_threshold = MAX_BYTES , item_threshold = MAX_ITEMS ) : arg_bufs = list ( chain . from_iterable ( serialize_object ( arg , buffer_threshold , item_threshold ) for arg in args ) ) kw_keys = sorted ( kwargs . keys ( ) ) kwarg_bufs = list ( chain . from_iterable ( serialize_object ( kwargs [ key ] , buffer_threshold , item_threshold ) for key in kw_keys ) ) info = dict ( nargs = len ( args ) , narg_bufs = len ( arg_bufs ) , kw_keys = kw_keys ) msg = [ pickle . dumps ( can ( f ) , PICKLE_PROTOCOL ) ] msg . append ( pickle . dumps ( info , PICKLE_PROTOCOL ) ) msg . extend ( arg_bufs ) msg . extend ( kwarg_bufs ) return msg
12534	def update ( self , dicomset ) : if not isinstance ( dicomset , DicomFileSet ) : raise ValueError ( 'Given dicomset is not a DicomFileSet.' ) self . items = list ( set ( self . items ) . update ( dicomset ) )
8850	def setup_editor ( self , editor ) : editor . cursorPositionChanged . connect ( self . on_cursor_pos_changed ) try : m = editor . modes . get ( modes . GoToAssignmentsMode ) except KeyError : pass else : assert isinstance ( m , modes . GoToAssignmentsMode ) m . out_of_doc . connect ( self . on_goto_out_of_doc )
2302	def orient_undirected_graph ( self , data , graph ) : self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) self . arguments [ '{SCORE}' ] = self . scores [ self . score ] fe = DataFrame ( nx . adj_matrix ( graph , weight = None ) . todense ( ) ) fg = DataFrame ( 1 - fe . values ) results = self . _run_gies ( data , fixedGaps = fg , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
1722	def is_lval ( t ) : if not t : return False i = iter ( t ) if i . next ( ) not in IDENTIFIER_START : return False return all ( e in IDENTIFIER_PART for e in i )
5500	def get_tweets ( self , url , limit = None ) : try : tweets = self . cache [ url ] [ "tweets" ] self . mark_updated ( ) return sorted ( tweets , reverse = True ) [ : limit ] except KeyError : return [ ]
11296	def process_response ( self , resp , multiple_rates ) : self . _check_for_exceptions ( resp , multiple_rates ) rates = { } for result in resp [ 'results' ] : rate = ZipTaxClient . _cast_tax_rate ( result [ 'taxSales' ] ) rates [ result [ 'geoCity' ] ] = rate if not multiple_rates : return rates [ list ( rates . keys ( ) ) [ 0 ] ] return rates
4734	def round_data ( filter_data ) : for index , _ in enumerate ( filter_data ) : filter_data [ index ] [ 0 ] = round ( filter_data [ index ] [ 0 ] / 100.0 ) * 100.0 return filter_data
5821	def version ( self ) : ver = Version ( ) ver . conn = self . conn ver . attrs = { 'service_id' : self . attrs [ 'id' ] , } ver . save ( ) return ver
379	def featurewise_norm ( x , mean = None , std = None , epsilon = 1e-7 ) : if mean : x = x - mean if std : x = x / ( std + epsilon ) return x
1410	def filter_bolts ( table , header ) : bolts_info = [ ] for row in table : if row [ 0 ] == 'bolt' : bolts_info . append ( row ) return bolts_info , header
3226	def get_creds_from_kwargs ( kwargs ) : creds = { 'key_file' : kwargs . pop ( 'key_file' , None ) , 'http_auth' : kwargs . pop ( 'http_auth' , None ) , 'project' : kwargs . get ( 'project' , None ) , 'user_agent' : kwargs . pop ( 'user_agent' , None ) , 'api_version' : kwargs . pop ( 'api_version' , 'v1' ) } return ( creds , kwargs )
13618	def get_branches ( self ) : return [ self . _sanitize ( branch ) for branch in self . _git . branch ( color = "never" ) . splitlines ( ) ]
5725	def write ( self , mi_cmd_to_write , timeout_sec = DEFAULT_GDB_TIMEOUT_SEC , raise_error_on_timeout = True , read_response = True , ) : self . verify_valid_gdb_subprocess ( ) if timeout_sec < 0 : self . logger . warning ( "timeout_sec was negative, replacing with 0" ) timeout_sec = 0 if type ( mi_cmd_to_write ) in [ str , unicode ] : pass elif type ( mi_cmd_to_write ) == list : mi_cmd_to_write = "\n" . join ( mi_cmd_to_write ) else : raise TypeError ( "The gdb mi command must a be str or list. Got " + str ( type ( mi_cmd_to_write ) ) ) self . logger . debug ( "writing: %s" , mi_cmd_to_write ) if not mi_cmd_to_write . endswith ( "\n" ) : mi_cmd_to_write_nl = mi_cmd_to_write + "\n" else : mi_cmd_to_write_nl = mi_cmd_to_write if USING_WINDOWS : outputready = [ self . stdin_fileno ] else : _ , outputready , _ = select . select ( [ ] , self . write_list , [ ] , timeout_sec ) for fileno in outputready : if fileno == self . stdin_fileno : self . gdb_process . stdin . write ( mi_cmd_to_write_nl . encode ( ) ) self . gdb_process . stdin . flush ( ) else : self . logger . error ( "got unexpected fileno %d" % fileno ) if read_response is True : return self . get_gdb_response ( timeout_sec = timeout_sec , raise_error_on_timeout = raise_error_on_timeout ) else : return [ ]
7191	def _load_info ( self ) : url = '%s/prefix?duration=36000' % self . base_url r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
887	def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) segment = connections . createSegment ( cell ) if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( "All segments should be created with the TM createSegment method." ) return segment
7718	def free ( self ) : if not self . borrowed : self . xmlnode . unlinkNode ( ) self . xmlnode . freeNode ( ) self . xmlnode = None
5704	def _scan_footpaths ( self , stop_id , walk_departure_time ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ stop_id ] , data = True ) : d_walk = data [ "d_walk" ] arrival_time = walk_departure_time + d_walk / self . _walk_speed self . _update_stop_label ( neighbor , arrival_time )
3242	def boto3_cached_conn ( service , service_type = 'client' , future_expiration_minutes = 15 , account_number = None , assume_role = None , session_name = 'cloudaux' , region = 'us-east-1' , return_credentials = False , external_id = None , arn_partition = 'aws' ) : key = ( account_number , assume_role , session_name , external_id , region , service_type , service , arn_partition ) if key in CACHE : retval = _get_cached_creds ( key , service , service_type , region , future_expiration_minutes , return_credentials ) if retval : return retval role = None if assume_role : sts = boto3 . session . Session ( ) . client ( 'sts' ) if not all ( [ account_number , assume_role ] ) : raise ValueError ( "Account number and role to assume are both required" ) arn = 'arn:{partition}:iam::{0}:role/{1}' . format ( account_number , assume_role , partition = arn_partition ) assume_role_kwargs = { 'RoleArn' : arn , 'RoleSessionName' : session_name } if external_id : assume_role_kwargs [ 'ExternalId' ] = external_id role = sts . assume_role ( ** assume_role_kwargs ) if service_type == 'client' : conn = _client ( service , region , role ) elif service_type == 'resource' : conn = _resource ( service , region , role ) if role : CACHE [ key ] = role if return_credentials : return conn , role [ 'Credentials' ] return conn
994	def _generateRangeDescription ( self , ranges ) : desc = "" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += "%.2f" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += ", " return desc
4518	def fillTriangle ( self , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : md . fill_triangle ( self . set , x0 , y0 , x1 , y1 , x2 , y2 , color , aa )
11487	def _search_folder_for_item_or_folder ( name , folder_id ) : session . token = verify_credentials ( ) children = session . communicator . folder_children ( session . token , folder_id ) for folder in children [ 'folders' ] : if folder [ 'name' ] == name : return False , folder [ 'folder_id' ] for item in children [ 'items' ] : if item [ 'name' ] == name : return True , item [ 'item_id' ] return False , - 1
6255	def load ( self ) : self . meta . resolved_path = self . find_data ( self . meta . path ) if not self . meta . resolved_path : raise ImproperlyConfigured ( "Data file '{}' not found" . format ( self . meta . path ) ) print ( "Loading:" , self . meta . path ) with open ( self . meta . resolved_path , 'r' ) as fd : return fd . read ( )
4358	def send_packet ( self , pkt ) : self . put_client_msg ( packet . encode ( pkt , self . json_dumps ) )
8011	def from_request ( cls , request , webhook_id = PAYPAL_WEBHOOK_ID ) : headers = fix_django_headers ( request . META ) assert headers try : body = request . body . decode ( request . encoding or "utf-8" ) except Exception : body = "(error decoding body)" ip = request . META [ "REMOTE_ADDR" ] obj = cls . objects . create ( headers = headers , body = body , remote_ip = ip ) try : obj . valid = obj . verify ( PAYPAL_WEBHOOK_ID ) if obj . valid : obj . process ( save = False ) except Exception as e : max_length = WebhookEventTrigger . _meta . get_field ( "exception" ) . max_length obj . exception = str ( e ) [ : max_length ] obj . traceback = format_exc ( ) finally : obj . save ( ) return obj
5927	def configuration ( self ) : configuration = { 'configfilename' : self . filename , 'logfilename' : self . getpath ( 'Logging' , 'logfilename' ) , 'loglevel_console' : self . getLogLevel ( 'Logging' , 'loglevel_console' ) , 'loglevel_file' : self . getLogLevel ( 'Logging' , 'loglevel_file' ) , 'configdir' : self . getpath ( 'DEFAULT' , 'configdir' ) , 'qscriptdir' : self . getpath ( 'DEFAULT' , 'qscriptdir' ) , 'templatesdir' : self . getpath ( 'DEFAULT' , 'templatesdir' ) , } configuration [ 'path' ] = [ os . path . curdir , configuration [ 'qscriptdir' ] , configuration [ 'templatesdir' ] ] return configuration
4836	def get_paginated_catalogs ( self , querystring = None ) : return self . _load_data ( self . CATALOGS_ENDPOINT , default = [ ] , querystring = querystring , traverse_pagination = False , many = False )
6403	def ipa_to_features ( ipa ) : features = [ ] pos = 0 ipa = normalize ( 'NFD' , text_type ( ipa . lower ( ) ) ) maxsymlen = max ( len ( _ ) for _ in _PHONETIC_FEATURES ) while pos < len ( ipa ) : found_match = False for i in range ( maxsymlen , 0 , - 1 ) : if ( pos + i - 1 <= len ( ipa ) and ipa [ pos : pos + i ] in _PHONETIC_FEATURES ) : features . append ( _PHONETIC_FEATURES [ ipa [ pos : pos + i ] ] ) pos += i found_match = True if not found_match : features . append ( - 1 ) pos += 1 return features
192	def SimplexNoiseAlpha ( first = None , second = None , per_channel = False , size_px_max = ( 2 , 16 ) , upscale_method = None , iterations = ( 1 , 3 ) , aggregation_method = "max" , sigmoid = True , sigmoid_thresh = None , name = None , deterministic = False , random_state = None ) : upscale_method_default = iap . Choice ( [ "nearest" , "linear" , "cubic" ] , p = [ 0.05 , 0.6 , 0.35 ] ) sigmoid_thresh_default = iap . Normal ( 0.0 , 5.0 ) noise = iap . SimplexNoise ( size_px_max = size_px_max , upscale_method = upscale_method if upscale_method is not None else upscale_method_default ) if iterations != 1 : noise = iap . IterativeNoiseAggregator ( noise , iterations = iterations , aggregation_method = aggregation_method ) if sigmoid is False or ( ia . is_single_number ( sigmoid ) and sigmoid <= 0.01 ) : noise = iap . Sigmoid . create_for_noise ( noise , threshold = sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default , activated = sigmoid ) if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return AlphaElementwise ( factor = noise , first = first , second = second , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
2157	def _auto_help_text ( self , help_text ) : api_doc_delimiter = '=====API DOCS=====' begin_api_doc = help_text . find ( api_doc_delimiter ) if begin_api_doc >= 0 : end_api_doc = help_text . rfind ( api_doc_delimiter ) + len ( api_doc_delimiter ) help_text = help_text [ : begin_api_doc ] + help_text [ end_api_doc : ] an_prefix = ( 'a' , 'e' , 'i' , 'o' ) if not self . resource_name . lower ( ) . startswith ( an_prefix ) : help_text = help_text . replace ( 'an object' , 'a %s' % self . resource_name ) if self . resource_name . lower ( ) . endswith ( 'y' ) : help_text = help_text . replace ( 'objects' , '%sies' % self . resource_name [ : - 1 ] , ) help_text = help_text . replace ( 'object' , self . resource_name ) help_text = help_text . replace ( 'keyword argument' , 'option' ) help_text = help_text . replace ( 'raise an exception' , 'abort with an error' ) for match in re . findall ( r'`([\w_]+)`' , help_text ) : option = '--%s' % match . replace ( '_' , '-' ) help_text = help_text . replace ( '`%s`' % match , option ) return help_text
3015	def _from_parsed_json_keyfile ( cls , keyfile_dict , scopes , token_uri = None , revoke_uri = None ) : creds_type = keyfile_dict . get ( 'type' ) if creds_type != client . SERVICE_ACCOUNT : raise ValueError ( 'Unexpected credentials type' , creds_type , 'Expected' , client . SERVICE_ACCOUNT ) service_account_email = keyfile_dict [ 'client_email' ] private_key_pkcs8_pem = keyfile_dict [ 'private_key' ] private_key_id = keyfile_dict [ 'private_key_id' ] client_id = keyfile_dict [ 'client_id' ] if not token_uri : token_uri = keyfile_dict . get ( 'token_uri' , oauth2client . GOOGLE_TOKEN_URI ) if not revoke_uri : revoke_uri = keyfile_dict . get ( 'revoke_uri' , oauth2client . GOOGLE_REVOKE_URI ) signer = crypt . Signer . from_string ( private_key_pkcs8_pem ) credentials = cls ( service_account_email , signer , scopes = scopes , private_key_id = private_key_id , client_id = client_id , token_uri = token_uri , revoke_uri = revoke_uri ) credentials . _private_key_pkcs8_pem = private_key_pkcs8_pem return credentials
8738	def get_ports_count ( context , filters = None ) : LOG . info ( "get_ports_count for tenant %s filters %s" % ( context . tenant_id , filters ) ) return db_api . port_count_all ( context , join_security_groups = True , ** filters )
9065	def value ( self ) : if not self . _fix [ "beta" ] : self . _update_beta ( ) if not self . _fix [ "scale" ] : self . _update_scale ( ) return self . lml ( )
5182	def nodes ( self , unreported = 2 , with_status = False , ** kwargs ) : nodes = self . _query ( 'nodes' , ** kwargs ) now = datetime . datetime . utcnow ( ) if type ( nodes ) == dict : nodes = [ nodes , ] if with_status : latest_events = self . event_counts ( query = EqualsOperator ( "latest_report?" , True ) , summarize_by = 'certname' ) for node in nodes : node [ 'status_report' ] = None node [ 'events' ] = None if with_status : status = [ s for s in latest_events if s [ 'subject' ] [ 'title' ] == node [ 'certname' ] ] try : node [ 'status_report' ] = node [ 'latest_report_status' ] if status : node [ 'events' ] = status [ 0 ] except KeyError : if status : node [ 'events' ] = status = status [ 0 ] if status [ 'successes' ] > 0 : node [ 'status_report' ] = 'changed' if status [ 'noops' ] > 0 : node [ 'status_report' ] = 'noop' if status [ 'failures' ] > 0 : node [ 'status_report' ] = 'failed' else : node [ 'status_report' ] = 'unchanged' if node [ 'report_timestamp' ] is not None : try : last_report = json_to_datetime ( node [ 'report_timestamp' ] ) last_report = last_report . replace ( tzinfo = None ) unreported_border = now - timedelta ( hours = unreported ) if last_report < unreported_border : delta = ( now - last_report ) node [ 'unreported' ] = True node [ 'unreported_time' ] = '{0}d {1}h {2}m' . format ( delta . days , int ( delta . seconds / 3600 ) , int ( ( delta . seconds % 3600 ) / 60 ) ) except AttributeError : node [ 'unreported' ] = True if not node [ 'report_timestamp' ] : node [ 'unreported' ] = True yield Node ( self , name = node [ 'certname' ] , deactivated = node [ 'deactivated' ] , expired = node [ 'expired' ] , report_timestamp = node [ 'report_timestamp' ] , catalog_timestamp = node [ 'catalog_timestamp' ] , facts_timestamp = node [ 'facts_timestamp' ] , status_report = node [ 'status_report' ] , noop = node . get ( 'latest_report_noop' ) , noop_pending = node . get ( 'latest_report_noop_pending' ) , events = node [ 'events' ] , unreported = node . get ( 'unreported' ) , unreported_time = node . get ( 'unreported_time' ) , report_environment = node [ 'report_environment' ] , catalog_environment = node [ 'catalog_environment' ] , facts_environment = node [ 'facts_environment' ] , latest_report_hash = node . get ( 'latest_report_hash' ) , cached_catalog_status = node . get ( 'cached_catalog_status' ) )
7083	def fourier_sinusoidal_residual ( fourierparams , times , mags , errs ) : modelmags , phase , ptimes , pmags , perrs = ( fourier_sinusoidal_func ( fourierparams , times , mags , errs ) ) return ( pmags - modelmags ) / perrs
1745	def _set_perms ( self , perms ) : assert isinstance ( perms , str ) and len ( perms ) <= 3 and perms . strip ( ) in [ '' , 'r' , 'w' , 'x' , 'rw' , 'r x' , 'rx' , 'rwx' , 'wx' , ] self . _perms = perms
8389	def write ( self , text , hashline = b"# {}" ) : u if not text . endswith ( b"\n" ) : text += b"\n" actual_hash = hashlib . sha1 ( text ) . hexdigest ( ) with open ( self . filename , "wb" ) as f : f . write ( text ) f . write ( hashline . decode ( "utf8" ) . format ( actual_hash ) . encode ( "utf8" ) ) f . write ( b"\n" )
13380	def join_dicts ( * dicts ) : out_dict = { } for d in dicts : for k , v in d . iteritems ( ) : if not type ( v ) in JOINERS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) JOINERS [ type ( v ) ] ( out_dict , k , v ) return out_dict
8496	def _parse_and_output ( filename , args ) : relpath = os . path . dirname ( filename ) if os . path . isfile ( filename ) : calls = _parse_file ( filename , relpath ) elif os . path . isdir ( filename ) : calls = _parse_dir ( filename , relpath ) else : _error ( "Could not determine file type: %r" , filename ) if not calls : _error ( "No pyconfig calls." ) if args . load_configs : keys = set ( ) for call in calls : keys . add ( call . key ) conf = pyconfig . Config ( ) for key , value in conf . settings . items ( ) : if key in keys : continue calls . append ( _PyconfigCall ( 'set' , key , value , [ None ] * 4 ) ) _output ( calls , args )
146	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) polygons = [ poly . project ( self . shape , shape ) for poly in self . polygons ] return PolygonsOnImage ( polygons , shape )
298	def plot_turnover ( returns , transactions , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_turnover = txn . get_turnover ( positions , transactions ) df_turnover_by_month = df_turnover . resample ( "M" ) . mean ( ) df_turnover . plot ( color = 'steelblue' , alpha = 1.0 , lw = 0.5 , ax = ax , ** kwargs ) df_turnover_by_month . plot ( color = 'orangered' , alpha = 0.5 , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_turnover . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . legend ( [ 'Daily turnover' , 'Average daily turnover, by month' , 'Average daily turnover, net' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( 'Daily turnover' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylim ( ( 0 , 2 ) ) ax . set_ylabel ( 'Turnover' ) ax . set_xlabel ( '' ) return ax
2133	def _workflow_node_structure ( node_results ) : node_list_pos = { } for i , node_result in enumerate ( node_results ) : for rel in [ 'success' , 'failure' , 'always' ] : node_result [ '{0}_backlinks' . format ( rel ) ] = [ ] node_list_pos [ node_result [ 'id' ] ] = i for node_result in node_results : for rel in [ 'success' , 'failure' , 'always' ] : for sub_node_id in node_result [ '{0}_nodes' . format ( rel ) ] : j = node_list_pos [ sub_node_id ] node_results [ j ] [ '{0}_backlinks' . format ( rel ) ] . append ( node_result [ 'id' ] ) root_nodes = [ ] for node_result in node_results : is_root = True for rel in [ 'success' , 'failure' , 'always' ] : if node_result [ '{0}_backlinks' . format ( rel ) ] != [ ] : is_root = False break if is_root : root_nodes . append ( node_result [ 'id' ] ) def branch_schema ( node_id ) : i = node_list_pos [ node_id ] node_dict = node_results [ i ] ret_dict = { "id" : node_id } for fd in NODE_STANDARD_FIELDS : val = node_dict . get ( fd , None ) if val is not None : if fd == 'unified_job_template' : job_type = node_dict [ 'summary_fields' ] [ 'unified_job_template' ] [ 'unified_job_type' ] ujt_key = JOB_TYPES [ job_type ] ret_dict [ ujt_key ] = val else : ret_dict [ fd ] = val for rel in [ 'success' , 'failure' , 'always' ] : sub_node_id_list = node_dict [ '{0}_nodes' . format ( rel ) ] if len ( sub_node_id_list ) == 0 : continue relationship_name = '{0}_nodes' . format ( rel ) ret_dict [ relationship_name ] = [ ] for sub_node_id in sub_node_id_list : ret_dict [ relationship_name ] . append ( branch_schema ( sub_node_id ) ) return ret_dict schema_dict = [ ] for root_node_id in root_nodes : schema_dict . append ( branch_schema ( root_node_id ) ) return schema_dict
8025	def multiglob_compile ( globs , prefix = False ) : if not globs : return re . compile ( '^$' ) elif prefix : globs = [ x + '*' for x in globs ] return re . compile ( '|' . join ( fnmatch . translate ( x ) for x in globs ) )
9615	def elements ( self , using , value ) : return self . _execute ( Command . FIND_CHILD_ELEMENTS , { 'using' : using , 'value' : value } )
5507	def _image_name_from_url ( url ) : find = r'https?://|[^\w]' replace = '_' return re . sub ( find , replace , url ) . strip ( '_' )
5753	def get_page_url ( page_num , current_app , url_view_name , url_extra_args , url_extra_kwargs , url_param_name , url_get_params , url_anchor ) : if url_view_name is not None : url_extra_kwargs [ url_param_name ] = page_num try : url = reverse ( url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch as e : if settings . SETTINGS_MODULE : if django . VERSION < ( 1 , 9 , 0 ) : separator = '.' else : separator = ':' project_name = settings . SETTINGS_MODULE . split ( '.' ) [ 0 ] try : url = reverse ( project_name + separator + url_view_name , args = url_extra_args , kwargs = url_extra_kwargs , current_app = current_app ) except NoReverseMatch : raise e else : raise e else : url = '' url_get_params = url_get_params or QueryDict ( url ) url_get_params = url_get_params . copy ( ) url_get_params [ url_param_name ] = str ( page_num ) if len ( url_get_params ) > 0 : if not isinstance ( url_get_params , QueryDict ) : tmp = QueryDict ( mutable = True ) tmp . update ( url_get_params ) url_get_params = tmp url += '?' + url_get_params . urlencode ( ) if ( url_anchor is not None ) : url += '#' + url_anchor return url
3573	def peripheral_didUpdateValueForCharacteristic_error_ ( self , peripheral , characteristic , error ) : logger . debug ( 'peripheral_didUpdateValueForCharacteristic_error called' ) if error is not None : return device = device_list ( ) . get ( peripheral ) if device is not None : device . _characteristic_changed ( characteristic )
12718	def angle_rates ( self ) : return [ self . ode_obj . getAngleRate ( i ) for i in range ( self . ADOF ) ]
2482	def parse ( self , data ) : try : return self . yacc . parse ( data , lexer = self . lex ) except : return None
10721	def get_command ( namespace ) : cmd = [ "pylint" , namespace . package ] + arg_map [ namespace . package ] if namespace . ignore : cmd . append ( "--ignore=%s" % namespace . ignore ) return cmd
13282	def _parse_command ( self , source , start_index ) : parsed_elements = [ ] running_index = start_index for element in self . elements : opening_bracket = element [ 'bracket' ] closing_bracket = self . _brackets [ opening_bracket ] element_start = None element_end = None for i , c in enumerate ( source [ running_index : ] , start = running_index ) : if c == element [ 'bracket' ] : element_start = i break elif c == '\n' : if element [ 'required' ] is True : content = self . _parse_whitespace_argument ( source [ running_index : ] , self . name ) return ParsedCommand ( self . name , [ { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : content . strip ( ) } ] , start_index , source [ start_index : i ] ) else : break if element_start is None and element [ 'required' ] is False : continue elif element_start is None and element [ 'required' ] is True : message = ( 'Parsing command {0} at index {1:d}, ' 'did not detect element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) balance = 1 for i , c in enumerate ( source [ element_start + 1 : ] , start = element_start + 1 ) : if c == opening_bracket : balance += 1 elif c == closing_bracket : balance -= 1 if balance == 0 : element_end = i break if balance > 0 : message = ( 'Parsing command {0} at index {1:d}, ' 'did not find closing bracket for required ' 'command element {2:d}' . format ( self . name , start_index , element [ 'index' ] ) ) raise CommandParserError ( message ) element_content = source [ element_start + 1 : element_end ] parsed_element = { 'index' : element [ 'index' ] , 'name' : element [ 'name' ] , 'content' : element_content . strip ( ) } parsed_elements . append ( parsed_element ) running_index = element_end + 1 command_source = source [ start_index : running_index ] parsed_command = ParsedCommand ( self . name , parsed_elements , start_index , command_source ) return parsed_command
8428	def gradient_n_pal ( colors , values = None , name = 'gradientn' ) : if values is None : colormap = mcolors . LinearSegmentedColormap . from_list ( name , colors ) else : colormap = mcolors . LinearSegmentedColormap . from_list ( name , list ( zip ( values , colors ) ) ) def _gradient_n_pal ( vals ) : return ratios_to_colors ( vals , colormap ) return _gradient_n_pal
532	def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( "getParameter -- parameter name '%s' does not exist in region %s of type %s" % ( paramName , self . name , self . type ) ) return getter ( paramName )
13169	def _match ( self , pred ) : if not pred : return True pred = pred [ 1 : - 1 ] if pred . startswith ( '@' ) : pred = pred [ 1 : ] if '=' in pred : attr , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] return self . attrs . get ( attr ) == value else : return pred in self . attrs elif num_re . match ( pred ) : index = int ( pred ) if index < 0 : if self . parent : return self . index == ( len ( self . parent . _children ) + index ) else : return index == 0 else : return index == self . index else : if '=' in pred : tag , value = pred . split ( '=' , 1 ) if value [ 0 ] in ( '"' , "'" ) : value = value [ 1 : ] if value [ - 1 ] in ( '"' , "'" ) : value = value [ : - 1 ] for c in self . _children : if c . tagname == tag and c . data == value : return True else : for c in self . _children : if c . tagname == pred : return True return False
2824	def convert_sigmoid ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting sigmoid ...' ) if names == 'short' : tf_name = 'SIGM' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) sigmoid = keras . layers . Activation ( 'sigmoid' , name = tf_name ) layers [ scope_name ] = sigmoid ( layers [ inputs [ 0 ] ] )
7419	def sample_cleanup ( data , sample ) : umap1file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap1.fastq" ) umap2file = os . path . join ( data . dirs . edits , sample . name + "-tmp-umap2.fastq" ) unmapped = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) samplesam = os . path . join ( data . dirs . refmapping , sample . name + ".sam" ) split1 = os . path . join ( data . dirs . edits , sample . name + "-split1.fastq" ) split2 = os . path . join ( data . dirs . edits , sample . name + "-split2.fastq" ) refmap_derep = os . path . join ( data . dirs . edits , sample . name + "-refmap_derep.fastq" ) for f in [ umap1file , umap2file , unmapped , samplesam , split1 , split2 , refmap_derep ] : try : os . remove ( f ) except : pass
7184	def copy_arguments_to_annotations ( args , type_comment , * , is_method = False ) : if isinstance ( type_comment , ast3 . Ellipsis ) : return expected = len ( args . args ) if args . vararg : expected += 1 expected += len ( args . kwonlyargs ) if args . kwarg : expected += 1 actual = len ( type_comment ) if isinstance ( type_comment , list ) else 1 if expected != actual : if is_method and expected - actual == 1 : pass else : raise ValueError ( f"number of arguments in type comment doesn't match; " + f"expected {expected}, found {actual}" ) if isinstance ( type_comment , list ) : next_value = type_comment . pop else : _tc = type_comment def next_value ( index : int = 0 ) -> ast3 . expr : return _tc for arg in args . args [ expected - actual : ] : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . vararg : ensure_no_annotation ( args . vararg . annotation ) args . vararg . annotation = next_value ( 0 ) for arg in args . kwonlyargs : ensure_no_annotation ( arg . annotation ) arg . annotation = next_value ( 0 ) if args . kwarg : ensure_no_annotation ( args . kwarg . annotation ) args . kwarg . annotation = next_value ( 0 )
10285	def get_subgraph_edges ( graph : BELGraph , annotation : str , value : str , source_filter = None , target_filter = None , ) : if source_filter is None : source_filter = keep_node_permissive if target_filter is None : target_filter = keep_node_permissive for u , v , k , data in graph . edges ( keys = True , data = True ) : if not edge_has_annotation ( data , annotation ) : continue if data [ ANNOTATIONS ] [ annotation ] == value and source_filter ( graph , u ) and target_filter ( graph , v ) : yield u , v , k , data
6373	def accuracy_gain ( self ) : r if self . population ( ) == 0 : return float ( 'NaN' ) random_accuracy = ( self . cond_pos_pop ( ) / self . population ( ) ) ** 2 + ( self . cond_neg_pop ( ) / self . population ( ) ) ** 2 return self . accuracy ( ) / random_accuracy
1577	def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : if not shell_port : return None if not instance_id : return "http://%s:%d/browse/log-files" % ( host , shell_port ) else : return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id )
8288	def get_child_by_name ( parent , name ) : def iterate_children ( widget , name ) : if widget . get_name ( ) == name : return widget try : for w in widget . get_children ( ) : result = iterate_children ( w , name ) if result is not None : return result else : continue except AttributeError : pass return iterate_children ( parent , name )
11165	def ctime ( self ) : try : return self . _stat . st_ctime except : self . _stat = self . stat ( ) return self . ctime
8047	def parse_from_import_statement ( self ) : self . log . debug ( "parsing from/import statement." ) is_future_import = self . _parse_from_import_source ( ) self . _parse_from_import_names ( is_future_import )
4850	def _transmit_create ( self , channel_metadata_item_map ) : for chunk in chunks ( channel_metadata_item_map , self . enterprise_configuration . transmission_chunk_size ) : serialized_chunk = self . _serialize_items ( list ( chunk . values ( ) ) ) try : self . client . create_content_metadata ( serialized_chunk ) except ClientError as exc : LOGGER . error ( 'Failed to update [%s] content metadata items for integrated channel [%s] [%s]' , len ( chunk ) , self . enterprise_configuration . enterprise_customer . name , self . enterprise_configuration . channel_code , ) LOGGER . error ( exc ) else : self . _create_transmissions ( chunk )
181	def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )
3502	def assess_products ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : warn ( 'use assess_component instead' , DeprecationWarning ) return assess_component ( model , reaction , 'products' , flux_coefficient_cutoff , solver )
13659	def route ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , route ( * components ) ) return f return _factory
4811	def evaluate ( best_processed_path , model ) : x_test_char , x_test_type , y_test = prepare_feature ( best_processed_path , option = 'test' ) y_predict = model . predict ( [ x_test_char , x_test_type ] ) y_predict = ( y_predict . ravel ( ) > 0.5 ) . astype ( int ) f1score = f1_score ( y_test , y_predict ) precision = precision_score ( y_test , y_predict ) recall = recall_score ( y_test , y_predict ) return f1score , precision , recall
7885	def _emit_element ( self , element , level , declared_prefixes ) : declarations = { } declared_prefixes = dict ( declared_prefixes ) name = element . tag prefixed = self . _make_prefixed ( name , True , declared_prefixes , declarations ) start_tag = u"<{0}" . format ( prefixed ) end_tag = u"</{0}>" . format ( prefixed ) for name , value in element . items ( ) : prefixed = self . _make_prefixed ( name , False , declared_prefixes , declarations ) start_tag += u' {0}={1}' . format ( prefixed , quoteattr ( value ) ) declarations = self . _make_ns_declarations ( declarations , declared_prefixes ) if declarations : start_tag += u" " + declarations children = [ ] for child in element : children . append ( self . _emit_element ( child , level + 1 , declared_prefixes ) ) if not children and not element . text : start_tag += u"/>" end_tag = u"" text = u"" else : start_tag += u">" if level > 0 and element . text : text = escape ( element . text ) else : text = u"" if level > 1 and element . tail : tail = escape ( element . tail ) else : tail = u"" return start_tag + text + u'' . join ( children ) + end_tag + tail
8946	def run ( self , cmd , * args , ** kwargs ) : runner = self . ctx . run if self . ctx else None return run ( cmd , runner = runner , * args , ** kwargs )
414	def delete_model ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . Model . delete_many ( kwargs ) logging . info ( "[Database] Delete Model SUCCESS" )
11093	def n_file ( self ) : self . assert_is_dir_and_exists ( ) n = 0 for _ in self . select_file ( recursive = True ) : n += 1 return n
3153	def all ( self , list_id , ** queryparams ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' ) , ** queryparams )
632	def destroySegment ( self , segment ) : for synapse in segment . _synapses : self . _removeSynapseFromPresynapticMap ( synapse ) self . _numSynapses -= len ( segment . _synapses ) segments = self . _cells [ segment . cell ] . _segments i = segments . index ( segment ) del segments [ i ] self . _freeFlatIdxs . append ( segment . flatIdx ) self . _segmentForFlatIdx [ segment . flatIdx ] = None
9010	def index_of_first_produced_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_produced_meshes else : self . _raise_not_found_error ( ) return index
11045	def init_storage_dir ( storage_dir ) : storage_path = FilePath ( storage_dir ) default_cert_path = storage_path . child ( 'default.pem' ) if not default_cert_path . exists ( ) : default_cert_path . setContent ( generate_wildcard_pem_bytes ( ) ) unmanaged_certs_path = storage_path . child ( 'unmanaged-certs' ) if not unmanaged_certs_path . exists ( ) : unmanaged_certs_path . createDirectory ( ) certs_path = storage_path . child ( 'certs' ) if not certs_path . exists ( ) : certs_path . createDirectory ( ) return storage_path , certs_path
8287	def adjacency ( graph , directed = False , reversed = False , stochastic = False , heuristic = None ) : v = { } for n in graph . nodes : v [ n . id ] = { } for e in graph . edges : id1 = e . node1 . id id2 = e . node2 . id if reversed : id1 , id2 = id2 , id1 v [ id1 ] [ id2 ] = 1.0 - e . weight * 0.5 if heuristic : v [ id1 ] [ id2 ] += heuristic ( id1 , id2 ) if not directed : v [ id2 ] [ id1 ] = v [ id1 ] [ id2 ] if stochastic : for id1 in v : d = sum ( v [ id1 ] . values ( ) ) for id2 in v [ id1 ] : v [ id1 ] [ id2 ] /= d return v
857	def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]
340	def log_if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )
13058	def transform ( self , work , xml , objectId , subreference = None ) : if str ( objectId ) in self . _transform : func = self . _transform [ str ( objectId ) ] else : func = self . _transform [ "default" ] if isinstance ( func , str ) : with open ( func ) as f : xslt = etree . XSLT ( etree . parse ( f ) ) return etree . tostring ( xslt ( xml ) , encoding = str , method = "html" , xml_declaration = None , pretty_print = False , with_tail = True , standalone = None ) elif isinstance ( func , Callable ) : return func ( work , xml , objectId , subreference ) elif func is None : return etree . tostring ( xml , encoding = str )
1455	def add_ckpt_state ( self , ckpt_id , ckpt_state ) : self . _flush_remaining ( ) msg = ckptmgr_pb2 . StoreInstanceStateCheckpoint ( ) istate = ckptmgr_pb2 . InstanceStateCheckpoint ( ) istate . checkpoint_id = ckpt_id istate . state = ckpt_state msg . state . CopyFrom ( istate ) self . _push_tuple_to_stream ( msg )
1273	def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration
6057	def resized_array_2d_from_array_2d_and_resized_shape ( array_2d , resized_shape , origin = ( - 1 , - 1 ) , pad_value = 0.0 ) : y_is_even = int ( array_2d . shape [ 0 ] ) % 2 == 0 x_is_even = int ( array_2d . shape [ 1 ] ) % 2 == 0 if origin is ( - 1 , - 1 ) : if y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) elif not y_is_even : y_centre = int ( array_2d . shape [ 0 ] / 2 ) if x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) elif not x_is_even : x_centre = int ( array_2d . shape [ 1 ] / 2 ) origin = ( y_centre , x_centre ) resized_array = np . zeros ( shape = resized_shape ) if y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 elif not y_is_even : y_min = origin [ 0 ] - int ( resized_shape [ 0 ] / 2 ) y_max = origin [ 0 ] + int ( ( resized_shape [ 0 ] / 2 ) ) + 1 if x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 elif not x_is_even : x_min = origin [ 1 ] - int ( resized_shape [ 1 ] / 2 ) x_max = origin [ 1 ] + int ( ( resized_shape [ 1 ] / 2 ) ) + 1 for y_resized , y in enumerate ( range ( y_min , y_max ) ) : for x_resized , x in enumerate ( range ( x_min , x_max ) ) : if y >= 0 and y < array_2d . shape [ 0 ] and x >= 0 and x < array_2d . shape [ 1 ] : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = array_2d [ y , x ] else : if y_resized >= 0 and y_resized < resized_shape [ 0 ] and x_resized >= 0 and x_resized < resized_shape [ 1 ] : resized_array [ y_resized , x_resized ] = pad_value return resized_array
107	def max_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . max , cval = cval , preserve_dtype = preserve_dtype )
3617	def get_settings ( self ) : try : logger . info ( 'GET SETTINGS ON %s' , self . index_name ) return self . __index . get_settings ( ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'ERROR DURING GET_SETTINGS ON %s: %s' , self . model , e )
9441	def reload_config ( self , call_params ) : path = '/' + self . api_version + '/ReloadConfig/' method = 'POST' return self . request ( path , method , call_params )
2263	def dict_take ( dict_ , keys , default = util_const . NoParam ) : r if default is util_const . NoParam : for key in keys : yield dict_ [ key ] else : for key in keys : yield dict_ . get ( key , default )
5751	def already_downloaded ( filename ) : cur_file = os . path . join ( c . bview_dir , filename ) old_file = os . path . join ( c . bview_dir , 'old' , filename ) if not os . path . exists ( cur_file ) and not os . path . exists ( old_file ) : return False return True
7163	def answer_display ( self , s = '' ) : padding = len ( max ( self . questions . keys ( ) , key = len ) ) + 5 for key in list ( self . answers . keys ( ) ) : s += '{:>{}} : {}\n' . format ( key , padding , self . answers [ key ] ) return s
2911	def _find_ancestor_from_name ( self , name ) : if self . parent is None : return None if self . parent . get_name ( ) == name : return self . parent return self . parent . _find_ancestor_from_name ( name )
9595	def execute_async_script ( self , script , * args ) : return self . _execute ( Command . EXECUTE_ASYNC_SCRIPT , { 'script' : script , 'args' : list ( args ) } )
9433	def _load_savefile_header ( file_h ) : try : raw_savefile_header = file_h . read ( 24 ) except UnicodeDecodeError : print ( "\nMake sure the input file is opened in read binary, 'rb'\n" ) raise InvalidEncoding ( "Could not read file; it might not be opened in binary mode." ) if raw_savefile_header [ : 4 ] in [ struct . pack ( ">I" , _MAGIC_NUMBER ) , struct . pack ( ">I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'big' unpacked = struct . unpack ( '>IhhIIII' , raw_savefile_header ) elif raw_savefile_header [ : 4 ] in [ struct . pack ( "<I" , _MAGIC_NUMBER ) , struct . pack ( "<I" , _MAGIC_NUMBER_NS ) ] : byte_order = b'little' unpacked = struct . unpack ( '<IhhIIII' , raw_savefile_header ) else : raise UnknownMagicNumber ( "No supported Magic Number found" ) ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type ) = unpacked header = __pcap_header__ ( magic , major , minor , tz_off , ts_acc , snaplen , ll_type , ctypes . c_char_p ( byte_order ) , magic == _MAGIC_NUMBER_NS ) if not __validate_header__ ( header ) : raise InvalidHeader ( "Invalid Header" ) else : return header
4706	def read ( self , path ) : with open ( path , "rb" ) as fout : memmove ( self . m_buf , fout . read ( self . m_size ) , self . m_size )
11439	def _get_children_by_tag_name ( node , name ) : try : return [ child for child in node . childNodes if child . nodeName == name ] except TypeError : return [ ]
267	def vectorize ( func ) : def wrapper ( df , * args , ** kwargs ) : if df . ndim == 1 : return func ( df , * args , ** kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , ** kwargs ) return wrapper
1296	def demo_update ( self ) : fetches = self . demo_optimization_output self . monitored_session . run ( fetches = fetches )
7423	def ref_muscle_chunker ( data , sample ) : LOGGER . info ( 'entering ref_muscle_chunker' ) regions = bedtools_merge ( data , sample ) if len ( regions ) > 0 : get_overlapping_reads ( data , sample , regions ) else : msg = "No reads mapped to reference sequence - {}" . format ( sample . name ) LOGGER . warn ( msg )
2510	def handle_extracted_license ( self , extr_lic ) : lic = self . parse_only_extr_license ( extr_lic ) if lic is not None : self . doc . add_extr_lic ( lic ) return lic
10891	def intersection ( tiles , * args ) : tiles = listify ( tiles ) + listify ( args ) if len ( tiles ) < 2 : return tiles [ 0 ] tile = tiles [ 0 ] l , r = tile . l . copy ( ) , tile . r . copy ( ) for tile in tiles [ 1 : ] : l = amax ( l , tile . l ) r = amin ( r , tile . r ) return Tile ( l , r , dtype = l . dtype )
6594	def run_multiple ( self , eventLoops ) : self . nruns += len ( eventLoops ) return self . communicationChannel . put_multiple ( eventLoops )
9904	def post_process ( self , group , event , is_new , is_sample , ** kwargs ) : if not self . is_configured ( group . project ) : return host = self . get_option ( 'server_host' , group . project ) port = int ( self . get_option ( 'server_port' , group . project ) ) prefix = self . get_option ( 'prefix' , group . project ) hostname = self . get_option ( 'hostname' , group . project ) or socket . gethostname ( ) resolve_age = group . project . get_option ( 'sentry:resolve_age' , None ) now = int ( time . time ( ) ) template = '%s.%%s[%s]' % ( prefix , group . project . slug ) level = group . get_level_display ( ) label = template % level groups = group . project . group_set . filter ( status = STATUS_UNRESOLVED ) if resolve_age : oldest = timezone . now ( ) - timedelta ( hours = int ( resolve_age ) ) groups = groups . filter ( last_seen__gt = oldest ) num_errors = groups . filter ( level = group . level ) . count ( ) metric = Metric ( hostname , label , num_errors , now ) log . info ( 'will send %s=%s to zabbix' , label , num_errors ) send_to_zabbix ( [ metric ] , host , port )
1444	def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) global_stream_id = source_component + "/" + stream_id self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = global_stream_id )
11142	def remove_repository ( self , path = None , removeEmptyDirs = True ) : assert isinstance ( removeEmptyDirs , bool ) , "removeEmptyDirs must be boolean" if path is not None : if path != self . __path : repo = Repository ( ) repo . load_repository ( path ) else : repo = self else : repo = self assert repo . path is not None , "path is not given and repository is not initialized" for fdict in reversed ( repo . get_repository_state ( ) ) : relaPath = list ( fdict ) [ 0 ] realPath = os . path . join ( repo . path , relaPath ) path , name = os . path . split ( realPath ) if fdict [ relaPath ] [ 'type' ] == 'file' : if os . path . isfile ( realPath ) : os . remove ( realPath ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileInfo % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileLock % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileLock % name ) ) if os . path . isfile ( os . path . join ( repo . path , path , self . __fileClass % name ) ) : os . remove ( os . path . join ( repo . path , path , self . __fileClass % name ) ) elif fdict [ relaPath ] [ 'type' ] == 'dir' : if os . path . isfile ( os . path . join ( realPath , self . __dirInfo ) ) : os . remove ( os . path . join ( realPath , self . __dirInfo ) ) if os . path . isfile ( os . path . join ( realPath , self . __dirLock ) ) : os . remove ( os . path . join ( realPath , self . __dirLock ) ) if not len ( os . listdir ( realPath ) ) and removeEmptyDirs : shutil . rmtree ( realPath ) if os . path . isfile ( os . path . join ( repo . path , self . __repoFile ) ) : os . remove ( os . path . join ( repo . path , self . __repoFile ) ) if os . path . isfile ( os . path . join ( repo . path , self . __repoLock ) ) : os . remove ( os . path . join ( repo . path , self . __repoLock ) )
5008	def _create_session ( self ) : session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT oauth_access_token , expires_at = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , self . enterprise_configuration . sapsf_user_id , self . enterprise_configuration . user_type ) session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
1858	def BTC ( cpu , dest , src ) : if dest . type == 'register' : value = dest . read ( ) pos = src . read ( ) % dest . size cpu . CF = value & ( 1 << pos ) == 1 << pos dest . write ( value ^ ( 1 << pos ) ) elif dest . type == 'memory' : addr , pos = cpu . _getMemoryBit ( dest , src ) base , size , ty = cpu . get_descriptor ( cpu . DS ) addr += base value = cpu . read_int ( addr , 8 ) cpu . CF = value & ( 1 << pos ) == 1 << pos value = value ^ ( 1 << pos ) cpu . write_int ( addr , value , 8 ) else : raise NotImplementedError ( f"Unknown operand for BTC: {dest.type}" )
11679	def connect ( self ) : try : logger . info ( u'Connecting %s:%d' % ( self . host , self . port ) ) self . sock . connect ( ( self . host , self . port ) ) except socket . error : raise ConnectionError ( ) self . state = CONNECTED
9721	async def start ( self , rtfromfile = False ) : cmd = "start" + ( " rtfromfile" if rtfromfile else "" ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
4380	def deny ( self , role , method , resource , with_children = False ) : if with_children : for r in role . get_children ( ) : permission = ( r . get_name ( ) , method , resource ) if permission not in self . _denied : self . _denied . append ( permission ) permission = ( role . get_name ( ) , method , resource ) if permission not in self . _denied : self . _denied . append ( permission )
3224	def _build_google_client ( service , api_version , http_auth ) : client = build ( service , api_version , http = http_auth ) return client
9366	def account_number ( ) : account = [ random . randint ( 1 , 9 ) for _ in range ( 20 ) ] return "" . join ( map ( str , account ) )
11827	def boggle_neighbors ( n2 , cache = { } ) : if cache . get ( n2 ) : return cache . get ( n2 ) n = exact_sqrt ( n2 ) neighbors = [ None ] * n2 for i in range ( n2 ) : neighbors [ i ] = [ ] on_top = i < n on_bottom = i >= n2 - n on_left = i % n == 0 on_right = ( i + 1 ) % n == 0 if not on_top : neighbors [ i ] . append ( i - n ) if not on_left : neighbors [ i ] . append ( i - n - 1 ) if not on_right : neighbors [ i ] . append ( i - n + 1 ) if not on_bottom : neighbors [ i ] . append ( i + n ) if not on_left : neighbors [ i ] . append ( i + n - 1 ) if not on_right : neighbors [ i ] . append ( i + n + 1 ) if not on_left : neighbors [ i ] . append ( i - 1 ) if not on_right : neighbors [ i ] . append ( i + 1 ) cache [ n2 ] = neighbors return neighbors
1773	def pop ( cpu , size ) : assert size in ( 16 , cpu . address_bit_size ) base , _ , _ = cpu . get_descriptor ( cpu . SS ) address = cpu . STACK + base value = cpu . read_int ( address , size ) cpu . STACK = cpu . STACK + size // 8 return value
9325	def _validate_server ( self ) : if not self . _title : msg = "No 'title' in Server Discovery for request '{}'" raise ValidationError ( msg . format ( self . url ) )
9132	def count ( cls , session : Optional [ Session ] = None ) -> int : if session is None : session = _make_session ( ) count = session . query ( cls ) . count ( ) session . close ( ) return count
5663	def get_trip_points ( cur , route_id , offset = 0 , tripid_glob = '' ) : extra_where = '' if tripid_glob : extra_where = "AND trip_id GLOB '%s'" % tripid_glob cur . execute ( 'SELECT seq, lat, lon ' 'FROM (select trip_I from route ' ' LEFT JOIN trips USING (route_I) ' ' WHERE route_id=? %s limit 1 offset ? ) ' 'JOIN stop_times USING (trip_I) ' 'LEFT JOIN stop USING (stop_id) ' 'ORDER BY seq' % extra_where , ( route_id , offset ) ) stop_points = [ dict ( seq = row [ 0 ] , lat = row [ 1 ] , lon = row [ 2 ] ) for row in cur ] return stop_points
9019	def _rows ( self , spec ) : rows = self . new_row_collection ( ) for row in spec : rows . append ( self . _row ( row ) ) return rows
10523	def getallitem ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) object_handle . Press ( ) self . wait ( 1 ) child = None try : if not object_handle . AXChildren : raise LdtpServerException ( u"Unable to find menu" ) children = object_handle . AXChildren [ 0 ] if not children : raise LdtpServerException ( u"Unable to find menu" ) children = children . AXChildren items = [ ] for child in children : label = self . _get_title ( child ) if label : items . append ( label ) finally : if child : child . Cancel ( ) return items
11463	def connect ( self ) : self . _ftp . connect ( ) self . _ftp . login ( user = self . _username , passwd = self . _passwd )
3441	def rename_genes ( cobra_model , rename_dict ) : recompute_reactions = set ( ) remove_genes = [ ] for old_name , new_name in iteritems ( rename_dict ) : try : gene_index = cobra_model . genes . index ( old_name ) except ValueError : gene_index = None old_gene_present = gene_index is not None new_gene_present = new_name in cobra_model . genes if old_gene_present and new_gene_present : old_gene = cobra_model . genes . get_by_id ( old_name ) if old_gene is not cobra_model . genes . get_by_id ( new_name ) : remove_genes . append ( old_gene ) recompute_reactions . update ( old_gene . _reaction ) elif old_gene_present and not new_gene_present : gene = cobra_model . genes [ gene_index ] cobra_model . genes . _dict . pop ( gene . id ) gene . id = new_name cobra_model . genes [ gene_index ] = gene elif not old_gene_present and new_gene_present : pass else : pass cobra_model . repair ( ) class Renamer ( NodeTransformer ) : def visit_Name ( self , node ) : node . id = rename_dict . get ( node . id , node . id ) return node gene_renamer = Renamer ( ) for rxn , rule in iteritems ( get_compiled_gene_reaction_rules ( cobra_model ) ) : if rule is not None : rxn . _gene_reaction_rule = ast2str ( gene_renamer . visit ( rule ) ) for rxn in recompute_reactions : rxn . gene_reaction_rule = rxn . _gene_reaction_rule for i in remove_genes : cobra_model . genes . remove ( i )
8700	def __exchange ( self , output , timeout = None ) : self . __writeln ( output ) self . _port . flush ( ) return self . __expect ( timeout = timeout or self . _timeout )
385	def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list
12239	def beale ( theta ) : x , y = theta A = 1.5 - x + x * y B = 2.25 - x + x * y ** 2 C = 2.625 - x + x * y ** 3 obj = A ** 2 + B ** 2 + C ** 2 grad = np . array ( [ 2 * A * ( y - 1 ) + 2 * B * ( y ** 2 - 1 ) + 2 * C * ( y ** 3 - 1 ) , 2 * A * x + 4 * B * x * y + 6 * C * x * y ** 2 ] ) return obj , grad
1589	def set_topology_context ( self , metrics_collector ) : Log . debug ( "Setting topology context" ) cluster_config = self . get_topology_config ( ) cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) task_to_component_map = self . _get_task_to_comp_map ( ) self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , self . my_task_id , metrics_collector , self . topology_pex_abs_path )
10494	def clickMouseButtonRight ( self , coord ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonRight , modFlags ) self . _postQueuedEvents ( )
12746	def as_flat_array ( iterables ) : arr = [ ] for x in iterables : arr . extend ( x ) return np . array ( arr )
2557	def clean_attribute ( attribute ) : attribute = { 'cls' : 'class' , 'className' : 'class' , 'class_name' : 'class' , 'fr' : 'for' , 'html_for' : 'for' , 'htmlFor' : 'for' , } . get ( attribute , attribute ) if attribute [ 0 ] == '_' : attribute = attribute [ 1 : ] if attribute in set ( [ 'http_equiv' ] ) or attribute . startswith ( 'data_' ) : attribute = attribute . replace ( '_' , '-' ) . lower ( ) if attribute . split ( '_' ) [ 0 ] in ( 'xlink' , 'xml' , 'xmlns' ) : attribute = attribute . replace ( '_' , ':' , 1 ) . lower ( ) return attribute
13530	def add_child ( self , ** kwargs ) : data_class = self . graph . data_content_type . model_class ( ) node = Node . objects . create ( graph = self . graph ) data_class . objects . create ( node = node , ** kwargs ) node . parents . add ( self ) self . children . add ( node ) return node
12102	def _launch_process_group ( self , process_commands , streams_path ) : processes = { } def check_complete_processes ( wait = False ) : result = False for proc in list ( processes ) : if wait : proc . wait ( ) if proc . poll ( ) is not None : self . debug ( "Process %d exited with code %d." % ( processes [ proc ] [ 'tid' ] , proc . poll ( ) ) ) processes [ proc ] [ 'stdout' ] . close ( ) processes [ proc ] [ 'stderr' ] . close ( ) del processes [ proc ] result = True return result for cmd , tid in process_commands : self . debug ( "Starting process %d..." % tid ) job_timestamp = time . strftime ( '%H%M%S' ) basename = "%s_%s_tid_%d" % ( self . batch_name , job_timestamp , tid ) stdout_handle = open ( os . path . join ( streams_path , "%s.o.%d" % ( basename , tid ) ) , "wb" ) stderr_handle = open ( os . path . join ( streams_path , "%s.e.%d" % ( basename , tid ) ) , "wb" ) proc = subprocess . Popen ( cmd , stdout = stdout_handle , stderr = stderr_handle ) processes [ proc ] = { 'tid' : tid , 'stdout' : stdout_handle , 'stderr' : stderr_handle } if self . max_concurrency : while len ( processes ) >= self . max_concurrency : if not check_complete_processes ( len ( processes ) == 1 ) : time . sleep ( 0.1 ) while len ( processes ) > 0 : if not check_complete_processes ( True ) : time . sleep ( 0.1 )
8465	def run ( self ) : filename = ".DS_Store" command = "find {path} -type f -name \"{filename}\" " . format ( path = self . path , filename = filename ) cmd = CommandHelper ( command ) cmd . execute ( ) files = cmd . output . split ( "\n" ) for f in files : if not f . endswith ( filename ) : continue rel_path = f . replace ( self . path , "" ) if rel_path . startswith ( tuple ( self . CONFIG [ 'exclude_paths' ] ) ) : continue issue = Issue ( ) issue . name = "File .DS_Store detected" issue . potential = False issue . severity = Issue . SEVERITY_LOW issue . file = rel_path self . saveIssue ( issue )
5688	def get_transit_events ( self , start_time_ut = None , end_time_ut = None , route_type = None ) : table_name = self . _get_day_trips_table_name ( ) event_query = "SELECT stop_I, seq, trip_I, route_I, routes.route_id AS route_id, routes.type AS route_type, " "shape_id, day_start_ut+dep_time_ds AS dep_time_ut, day_start_ut+arr_time_ds AS arr_time_ut " "FROM " + table_name + " " "JOIN trips USING(trip_I) " "JOIN routes USING(route_I) " "JOIN stop_times USING(trip_I)" where_clauses = [ ] if end_time_ut : where_clauses . append ( table_name + ".start_time_ut< {end_time_ut}" . format ( end_time_ut = end_time_ut ) ) where_clauses . append ( "dep_time_ut <={end_time_ut}" . format ( end_time_ut = end_time_ut ) ) if start_time_ut : where_clauses . append ( table_name + ".end_time_ut > {start_time_ut}" . format ( start_time_ut = start_time_ut ) ) where_clauses . append ( "arr_time_ut >={start_time_ut}" . format ( start_time_ut = start_time_ut ) ) if route_type is not None : assert route_type in ALL_ROUTE_TYPES where_clauses . append ( "routes.type={route_type}" . format ( route_type = route_type ) ) if len ( where_clauses ) > 0 : event_query += " WHERE " for i , where_clause in enumerate ( where_clauses ) : if i is not 0 : event_query += " AND " event_query += where_clause event_query += " ORDER BY trip_I, day_start_ut+dep_time_ds;" events_result = pd . read_sql_query ( event_query , self . conn ) from_indices = numpy . nonzero ( ( events_result [ 'trip_I' ] [ : - 1 ] . values == events_result [ 'trip_I' ] [ 1 : ] . values ) * ( events_result [ 'seq' ] [ : - 1 ] . values < events_result [ 'seq' ] [ 1 : ] . values ) ) [ 0 ] to_indices = from_indices + 1 assert ( events_result [ 'trip_I' ] [ from_indices ] . values == events_result [ 'trip_I' ] [ to_indices ] . values ) . all ( ) trip_Is = events_result [ 'trip_I' ] [ from_indices ] from_stops = events_result [ 'stop_I' ] [ from_indices ] to_stops = events_result [ 'stop_I' ] [ to_indices ] shape_ids = events_result [ 'shape_id' ] [ from_indices ] dep_times = events_result [ 'dep_time_ut' ] [ from_indices ] arr_times = events_result [ 'arr_time_ut' ] [ to_indices ] route_types = events_result [ 'route_type' ] [ from_indices ] route_ids = events_result [ 'route_id' ] [ from_indices ] route_Is = events_result [ 'route_I' ] [ from_indices ] durations = arr_times . values - dep_times . values assert ( durations >= 0 ) . all ( ) from_seqs = events_result [ 'seq' ] [ from_indices ] to_seqs = events_result [ 'seq' ] [ to_indices ] data_tuples = zip ( from_stops , to_stops , dep_times , arr_times , shape_ids , route_types , route_ids , trip_Is , durations , from_seqs , to_seqs , route_Is ) columns = [ "from_stop_I" , "to_stop_I" , "dep_time_ut" , "arr_time_ut" , "shape_id" , "route_type" , "route_id" , "trip_I" , "duration" , "from_seq" , "to_seq" , "route_I" ] df = pd . DataFrame . from_records ( data_tuples , columns = columns ) return df
8249	def nearest_hue ( self , primary = False ) : if self . is_black : return "black" elif self . is_white : return "white" elif self . is_grey : return "grey" if primary : hues = primary_hues else : hues = named_hues . keys ( ) nearest , d = "" , 1.0 for hue in hues : if abs ( self . hue - named_hues [ hue ] ) % 1 < d : nearest , d = hue , abs ( self . hue - named_hues [ hue ] ) % 1 return nearest
3561	def advertised ( self ) : uuids = [ ] try : uuids = self . _props . Get ( _INTERFACE , 'UUIDs' ) except dbus . exceptions . DBusException as ex : if ex . get_dbus_name ( ) != 'org.freedesktop.DBus.Error.InvalidArgs' : raise ex return [ uuid . UUID ( str ( x ) ) for x in uuids ]
13718	def _camelcase_to_underscore ( url ) : def upper2underscore ( text ) : for char in text : if char . islower ( ) : yield char else : yield '_' if char . isalpha ( ) : yield char . lower ( ) return '' . join ( upper2underscore ( url ) )
2455	def set_pkg_licenses_concluded ( self , doc , licenses ) : self . assert_package_exists ( ) if not self . package_conc_lics_set : self . package_conc_lics_set = True if validations . validate_lics_conc ( licenses ) : doc . package . conc_lics = licenses return True else : raise SPDXValueError ( 'Package::ConcludedLicenses' ) else : raise CardinalityError ( 'Package::ConcludedLicenses' )
11503	def folder_children ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.children' , parameters ) return response
13168	def children ( self , name = None , reverse = False ) : elems = self . _children if reverse : elems = reversed ( elems ) for elem in elems : if name is None or elem . tagname == name : yield elem
11312	def update_oai_info ( self ) : for field in record_get_field_instances ( self . record , '909' , ind1 = "C" , ind2 = "O" ) : new_subs = [ ] for tag , value in field [ 0 ] : if tag == "o" : new_subs . append ( ( "a" , value ) ) else : new_subs . append ( ( tag , value ) ) if value in [ "CERN" , "CDS" , "ForCDS" ] : self . tag_as_cern = True record_add_field ( self . record , '024' , ind1 = "8" , subfields = new_subs ) record_delete_fields ( self . record , '909' )
8540	def _read_config ( self , filename = None ) : if filename : self . _config_filename = filename else : try : import appdirs except ImportError : raise Exception ( "Missing dependency for determining config path. Please install " "the 'appdirs' Python module." ) self . _config_filename = appdirs . user_config_dir ( _LIBRARY_NAME , "ProfitBricks" ) + ".ini" if not self . _config : self . _config = configparser . ConfigParser ( ) self . _config . optionxform = str self . _config . read ( self . _config_filename )
5365	def format ( self , record ) : if isinstance ( self . fmt , dict ) : self . _fmt = self . fmt [ record . levelname ] if sys . version_info > ( 3 , 2 ) : if self . style not in logging . _STYLES : raise ValueError ( 'Style must be one of: %s' % ',' . join ( list ( logging . _STYLES . keys ( ) ) ) ) self . _style = logging . _STYLES [ self . style ] [ 0 ] ( self . _fmt ) if sys . version_info > ( 2 , 7 ) : message = super ( LevelFormatter , self ) . format ( record ) else : message = ColoredFormatter . format ( self , record ) return message
8778	def _check_collisions ( self , new_range , existing_ranges ) : def _contains ( num , r1 ) : return ( num >= r1 [ 0 ] and num <= r1 [ 1 ] ) def _is_overlap ( r1 , r2 ) : return ( _contains ( r1 [ 0 ] , r2 ) or _contains ( r1 [ 1 ] , r2 ) or _contains ( r2 [ 0 ] , r1 ) or _contains ( r2 [ 1 ] , r1 ) ) for existing_range in existing_ranges : if _is_overlap ( new_range , existing_range ) : return True return False
7383	def plot_axis ( self , rs , theta ) : xs , ys = get_cartesian ( rs , theta ) self . ax . plot ( xs , ys , 'black' , alpha = 0.3 )
4844	def is_course_in_catalog ( self , catalog_id , course_id ) : try : course_run_id = str ( CourseKey . from_string ( course_id ) ) except InvalidKeyError : course_run_id = None endpoint = self . client . catalogs ( catalog_id ) . contains if course_run_id : resp = endpoint . get ( course_run_id = course_run_id ) else : resp = endpoint . get ( course_id = course_id ) return resp . get ( 'courses' , { } ) . get ( course_id , False )
6293	def render ( self , program : moderngl . Program , mode = None , vertices = - 1 , first = 0 , instances = 1 ) : vao = self . instance ( program ) if mode is None : mode = self . mode vao . render ( mode , vertices = vertices , first = first , instances = instances )
6820	def configure_modevasive ( self ) : r = self . local_renderer if r . env . modevasive_enabled : self . install_packages ( ) fn = r . render_to_file ( 'apache/apache_modevasive.template.conf' ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/mod-evasive.conf' , use_sudo = True ) r . put ( local_path = fn , remote_path = '/etc/apache2/mods-available/evasive.conf' , use_sudo = True ) self . enable_mod ( 'evasive' ) else : if self . last_manifest . modevasive_enabled : self . disable_mod ( 'evasive' )
6759	def get_package_list ( self ) : os_version = self . os_version self . vprint ( 'os_version:' , os_version ) req_packages1 = self . required_system_packages if req_packages1 : deprecation ( 'The required_system_packages attribute is deprecated, ' 'use the packager_system_packages property instead.' ) req_packages2 = self . packager_system_packages patterns = [ ( os_version . type , os_version . distro , os_version . release ) , ( os_version . distro , os_version . release ) , ( os_version . type , os_version . distro ) , ( os_version . distro , ) , os_version . distro , ] self . vprint ( 'req_packages1:' , req_packages1 ) self . vprint ( 'req_packages2:' , req_packages2 ) package_list = None found = False for pattern in patterns : self . vprint ( 'pattern:' , pattern ) for req_packages in ( req_packages1 , req_packages2 ) : if pattern in req_packages : package_list = req_packages [ pattern ] found = True break if not found : print ( 'Warning: No operating system pattern found for %s' % ( os_version , ) ) self . vprint ( 'package_list:' , package_list ) return package_list
7442	def branch ( self , newname , subsamples = None , infile = None ) : remove = 0 if ( newname == self . name or os . path . exists ( os . path . join ( self . paramsdict [ "project_dir" ] , newname + ".assembly" ) ) ) : print ( "{}Assembly object named {} already exists" . format ( self . _spacer , newname ) ) else : self . _check_name ( newname ) if newname . startswith ( "params-" ) : newname = newname . split ( "params-" ) [ 1 ] newobj = copy . deepcopy ( self ) newobj . name = newname newobj . paramsdict [ "assembly_name" ] = newname if subsamples and infile : print ( BRANCH_NAMES_AND_INPUT ) if infile : if infile [ 0 ] == "-" : remove = 1 infile = infile [ 1 : ] if os . path . exists ( infile ) : subsamples = _read_sample_names ( infile ) if remove : subsamples = list ( set ( self . samples . keys ( ) ) - set ( subsamples ) ) if subsamples : for sname in subsamples : if sname in self . samples : newobj . samples [ sname ] = copy . deepcopy ( self . samples [ sname ] ) else : print ( "Sample name not found: {}" . format ( sname ) ) newobj . samples = { name : sample for name , sample in newobj . samples . items ( ) if name in subsamples } else : for sample in self . samples : newobj . samples [ sample ] = copy . deepcopy ( self . samples [ sample ] ) newobj . save ( ) return newobj
4461	def transform ( self , jam ) : yield jam for jam_out in self . transformer . transform ( jam ) : yield jam_out
8744	def get_floatingip ( context , id , fields = None ) : LOG . info ( 'get_floatingip %s for tenant %s' % ( id , context . tenant_id ) ) filters = { 'address_type' : ip_types . FLOATING , '_deallocated' : False } floating_ip = db_api . floating_ip_find ( context , id = id , scope = db_api . ONE , ** filters ) if not floating_ip : raise q_exc . FloatingIpNotFound ( id = id ) return v . _make_floating_ip_dict ( floating_ip )
2859	def transfer ( self , data ) : if ( len ( data ) > 65536 ) : print ( 'the FTDI chip is limited to 65536 bytes (64 KB) of input/output per command!' ) print ( 'use for loops for larger reads' ) exit ( 1 ) command = 0x30 | ( self . lsbfirst << 3 ) | ( self . read_clock_ve << 2 ) | self . write_clock_ve logger . debug ( 'SPI transfer with command {0:2X}.' . format ( command ) ) data1 = data [ : len ( data ) / 2 ] data2 = data [ len ( data ) / 2 : ] len_low1 = ( len ( data1 ) - 1 ) & 0xFF len_high1 = ( ( len ( data1 ) - 1 ) >> 8 ) & 0xFF len_low2 = ( len ( data2 ) - 1 ) & 0xFF len_high2 = ( ( len ( data2 ) - 1 ) >> 8 ) & 0xFF payload1 = '' payload2 = '' self . _assert_cs ( ) if len ( data1 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low1 , len_high1 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data1 ) ) ) payload1 = self . _ft232h . _poll_read ( len ( data1 ) ) if len ( data2 ) > 0 : self . _ft232h . _write ( str ( bytearray ( ( command , len_low2 , len_high2 ) ) ) ) self . _ft232h . _write ( str ( bytearray ( data2 ) ) ) payload2 = self . _ft232h . _poll_read ( len ( data2 ) ) self . _deassert_cs ( ) return bytearray ( payload1 + payload2 )
7977	def _post_connect ( self ) : if not self . initiator : if "plain" in self . auth_methods or "digest" in self . auth_methods : self . set_iq_get_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage1 ) self . set_iq_set_handler ( "query" , "jabber:iq:auth" , self . auth_in_stage2 ) elif self . registration_callback : iq = Iq ( stanza_type = "get" ) iq . set_content ( Register ( ) ) self . set_response_handlers ( iq , self . registration_form_received , self . registration_error ) self . send ( iq ) return ClientStream . _post_connect ( self )
3324	def lock_string ( lock_dict ) : if not lock_dict : return "Lock: None" if lock_dict [ "expire" ] < 0 : expire = "Infinite ({})" . format ( lock_dict [ "expire" ] ) else : expire = "{} (in {} seconds)" . format ( util . get_log_time ( lock_dict [ "expire" ] ) , lock_dict [ "expire" ] - time . time ( ) ) return "Lock(<{}..>, '{}', {}, {}, depth-{}, until {}" . format ( lock_dict . get ( "token" , "?" * 30 ) [ 18 : 22 ] , lock_dict . get ( "root" ) , lock_dict . get ( "principal" ) , lock_dict . get ( "scope" ) , lock_dict . get ( "depth" ) , expire , )
1802	def LEA ( cpu , dest , src ) : dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) )
10965	def get ( self ) : fields = [ c . get ( ) for c in self . comps ] return self . field_reduce_func ( fields )
10782	def _feature_guess ( im , rad , minmass = None , use_tp = False , trim_edge = False ) : if minmass is None : minmass = rad ** 3 * 4 / 3. * np . pi * 0.01 if use_tp : diameter = np . ceil ( 2 * rad ) diameter += 1 - ( diameter % 2 ) df = peri . trackpy . locate ( im , int ( diameter ) , minmass = minmass ) npart = np . array ( df [ 'mass' ] ) . size guess = np . zeros ( [ npart , 3 ] ) guess [ : , 0 ] = df [ 'z' ] guess [ : , 1 ] = df [ 'y' ] guess [ : , 2 ] = df [ 'x' ] mass = df [ 'mass' ] else : guess , mass = initializers . local_max_featuring ( im , radius = rad , minmass = minmass , trim_edge = trim_edge ) npart = guess . shape [ 0 ] inds = np . argsort ( mass ) [ : : - 1 ] return guess [ inds ] . copy ( ) , npart
12379	def post ( self , request , response ) : if self . slug is not None : raise http . exceptions . NotImplemented ( ) self . assert_operations ( 'create' ) data = self . _clean ( None , self . request . read ( deserialize = True ) ) item = self . create ( data ) self . response . status = http . client . CREATED self . make_response ( item )
10460	def _ldtpize_accessible ( self , acc ) : actual_role = self . _get_role ( acc ) label = self . _get_title ( acc ) if re . match ( "AXWindow" , actual_role , re . M | re . U | re . L ) : strip = r"( |\n)" else : strip = r"( |:|\.|_|\n)" if label : label = re . sub ( strip , u"" , label ) role = abbreviated_roles . get ( actual_role , "ukn" ) if self . _ldtp_debug and role == "ukn" : print ( actual_role , acc ) return role , label
7936	def _set_state ( self , state ) : logger . debug ( " _set_state({0!r})" . format ( state ) ) self . _state = state self . _state_cond . notify ( )
12572	def put ( self , key , value , attrs = None , format = None , append = False , ** kwargs ) : if not isinstance ( value , np . ndarray ) : super ( NumpyHDFStore , self ) . put ( key , value , format , append , ** kwargs ) else : group = self . get_node ( key ) if group is not None and not append : self . _handle . removeNode ( group , recursive = True ) group = None if group is None : paths = key . split ( '/' ) path = '/' for p in paths : if not len ( p ) : continue new_path = path if not path . endswith ( '/' ) : new_path += '/' new_path += p group = self . get_node ( new_path ) if group is None : group = self . _handle . createGroup ( path , p ) path = new_path ds_name = kwargs . get ( 'ds_name' , self . _array_dsname ) ds = self . _handle . createArray ( group , ds_name , value ) if attrs is not None : for key in attrs : setattr ( ds . attrs , key , attrs [ key ] ) self . _handle . flush ( ) return ds
11976	def _sub ( self , other ) : if isinstance ( other , self . __class__ ) : sub = self . _ip_dec - other . _ip_dec if isinstance ( other , int ) : sub = self . _ip_dec - other else : other = self . __class__ ( other ) sub = self . _ip_dec - other . _ip_dec return sub
3465	def flux ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . forward_variable . primal - self . reverse_variable . primal except AttributeError : raise RuntimeError ( "reaction '{}' is not part of a model" . format ( self . id ) ) except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
12211	def invalidate_cache ( user , size = None ) : sizes = set ( AUTO_GENERATE_AVATAR_SIZES ) if size is not None : sizes . add ( size ) for prefix in cached_funcs : for size in sizes : cache . delete ( get_cache_key ( user , size , prefix ) )
6420	def readfile ( fn ) : with open ( path . join ( HERE , fn ) , 'r' , encoding = 'utf-8' ) as f : return f . read ( )
1460	def resolve_heron_suffix_issue ( abs_pex_path , class_path ) : importer = zipimport . zipimporter ( abs_pex_path ) importer . load_module ( "heron" ) to_load_lst = class_path . split ( '.' ) [ 1 : - 1 ] loaded = [ 'heron' ] loaded_mod = None for to_load in to_load_lst : sub_importer = zipimport . zipimporter ( os . path . join ( abs_pex_path , '/' . join ( loaded ) ) ) loaded_mod = sub_importer . load_module ( to_load ) loaded . append ( to_load ) return loaded_mod
3292	def set_property_value ( self , name , value , dry_run = False ) : assert value is None or xml_tools . is_etree_element ( value ) if name in _lockPropertyNames : raise DAVError ( HTTP_FORBIDDEN , err_condition = PRECONDITION_CODE_ProtectedProperty ) config = self . environ [ "wsgidav.config" ] mutableLiveProps = config . get ( "mutable_live_props" , [ ] ) if ( name . startswith ( "{DAV:}" ) and name in _standardLivePropNames and name in mutableLiveProps ) : if name in ( "{DAV:}getlastmodified" , "{DAV:}last_modified" ) : try : return self . set_last_modified ( self . path , value . text , dry_run ) except Exception : _logger . warning ( "Provider does not support set_last_modified on {}." . format ( self . path ) ) raise DAVError ( HTTP_FORBIDDEN ) if name . startswith ( "{urn:schemas-microsoft-com:}" ) : agent = self . environ . get ( "HTTP_USER_AGENT" , "None" ) win32_emu = config . get ( "hotfixes" , { } ) . get ( "emulate_win32_lastmod" , False ) if win32_emu and "MiniRedir/6.1" not in agent : if "Win32LastModifiedTime" in name : return self . set_last_modified ( self . path , value . text , dry_run ) elif "Win32FileAttributes" in name : return True elif "Win32CreationTime" in name : return True elif "Win32LastAccessTime" in name : return True pm = self . provider . prop_manager if pm and not name . startswith ( "{DAV:}" ) : refUrl = self . get_ref_url ( ) if value is None : return pm . remove_property ( refUrl , name , dry_run , self . environ ) else : value = etree . tostring ( value ) return pm . write_property ( refUrl , name , value , dry_run , self . environ ) raise DAVError ( HTTP_FORBIDDEN )
2940	def deserialize_condition ( self , workflow , start_node ) : condition = None spec_name = None for node in start_node . childNodes : if node . nodeType != minidom . Node . ELEMENT_NODE : continue if node . nodeName . lower ( ) == 'successor' : if spec_name is not None : _exc ( 'Duplicate task name %s' % spec_name ) if node . firstChild is None : _exc ( 'Successor tag without a task name' ) spec_name = node . firstChild . nodeValue elif node . nodeName . lower ( ) in _op_map : if condition is not None : _exc ( 'Multiple conditions are not yet supported' ) condition = self . deserialize_logical ( node ) else : _exc ( 'Unknown node: %s' % node . nodeName ) if condition is None : _exc ( 'Missing condition in conditional statement' ) if spec_name is None : _exc ( 'A %s has no task specified' % start_node . nodeName ) return condition , spec_name
137	def to_shapely_polygon ( self ) : import shapely . geometry return shapely . geometry . Polygon ( [ ( point [ 0 ] , point [ 1 ] ) for point in self . exterior ] )
6947	def jhk_to_sdssu ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSU_JHK , SDSSU_JH , SDSSU_JK , SDSSU_HK , SDSSU_J , SDSSU_H , SDSSU_K )
12053	def getIDsFromFiles ( files ) : if type ( files ) is str : files = glob . glob ( files + "/*.*" ) IDs = [ ] for fname in files : if fname [ - 4 : ] . lower ( ) == '.abf' : ext = fname . split ( '.' ) [ - 1 ] IDs . append ( os . path . basename ( fname ) . replace ( '.' + ext , '' ) ) return sorted ( IDs )
11147	def is_repository_file ( self , relativePath ) : relativePath = self . to_repo_relative_path ( path = relativePath , split = False ) if relativePath == '' : return False , False , False , False relaDir , name = os . path . split ( relativePath ) fileOnDisk = os . path . isfile ( os . path . join ( self . __path , relativePath ) ) infoOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileInfo % name ) ) classOnDisk = os . path . isfile ( os . path . join ( self . __path , os . path . dirname ( relativePath ) , self . __fileClass % name ) ) cDir = self . __repo [ 'walk_repo' ] if len ( relaDir ) : for dirname in relaDir . split ( os . sep ) : dList = [ d for d in cDir if isinstance ( d , dict ) ] if not len ( dList ) : cDir = None break cDict = [ d for d in dList if dirname in d ] if not len ( cDict ) : cDir = None break cDir = cDict [ 0 ] [ dirname ] if cDir is None : return False , fileOnDisk , infoOnDisk , classOnDisk if str ( name ) not in [ str ( i ) for i in cDir ] : return False , fileOnDisk , infoOnDisk , classOnDisk return True , fileOnDisk , infoOnDisk , classOnDisk
9777	def pprint ( value ) : click . echo ( json . dumps ( value , sort_keys = True , indent = 4 , separators = ( ',' , ': ' ) ) )
12685	def pods ( self ) : if not self . xml_tree : return [ ] return [ Pod ( elem ) for elem in self . xml_tree . findall ( 'pod' ) ]
6472	def color_ramp ( self , size ) : color = PALETTE . get ( self . option . palette , { } ) color = color . get ( self . term . colors , None ) color_ramp = [ ] if color is not None : ratio = len ( color ) / float ( size ) for i in range ( int ( size ) ) : color_ramp . append ( self . term . color ( color [ int ( ratio * i ) ] ) ) return color_ramp
2002	def _type_size ( ty ) : if ty [ 0 ] in ( 'int' , 'uint' , 'bytesM' , 'function' ) : return 32 elif ty [ 0 ] in ( 'tuple' ) : result = 0 for ty_i in ty [ 1 ] : result += ABI . _type_size ( ty_i ) return result elif ty [ 0 ] in ( 'array' ) : rep = ty [ 1 ] result = 32 return result elif ty [ 0 ] in ( 'bytes' , 'string' ) : result = 32 return result raise ValueError
10057	def delete ( self , pid , record , key ) : try : del record . files [ str ( key ) ] record . commit ( ) db . session . commit ( ) return make_response ( '' , 204 ) except KeyError : abort ( 404 , 'The specified object does not exist or has already ' 'been deleted.' )
2927	def write_file_to_package_zip ( self , filename , src_filename ) : f = open ( src_filename ) with f : data = f . read ( ) self . manifest [ filename ] = md5hash ( data ) self . package_zip . write ( src_filename , filename )
11419	def record_move_subfield ( rec , tag , subfield_position , new_subfield_position , field_position_global = None , field_position_local = None ) : subfields = record_get_subfields ( rec , tag , field_position_global = field_position_global , field_position_local = field_position_local ) try : subfield = subfields . pop ( subfield_position ) subfields . insert ( new_subfield_position , subfield ) except IndexError : raise InvenioBibRecordFieldError ( "There is no subfield with position '%d'." % subfield_position )
9336	def get ( self , Q ) : while self . Errors . empty ( ) : try : return Q . get ( timeout = 1 ) except queue . Empty : if not self . is_alive ( ) : try : return Q . get ( timeout = 0 ) except queue . Empty : raise StopProcessGroup else : continue else : raise StopProcessGroup
1217	def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )
1644	def GetPreviousNonBlankLine ( clean_lines , linenum ) : prevlinenum = linenum - 1 while prevlinenum >= 0 : prevline = clean_lines . elided [ prevlinenum ] if not IsBlankLine ( prevline ) : return ( prevline , prevlinenum ) prevlinenum -= 1 return ( '' , - 1 )
10698	def get ( self , key , default = None ) : if self . in_memory : return self . _memory_db . get ( key , default ) else : db = self . _read_file ( ) return db . get ( key , default )
6494	def set_mappings ( cls , index_name , doc_type , mappings ) : cache . set ( cls . get_cache_item_name ( index_name , doc_type ) , mappings )
9840	def __array ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'type' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: type was "%s", not a string.' % tok . text ) self . currentobject [ 'type' ] = tok . value ( ) elif tok . equals ( 'rank' ) : tok = self . __consume ( ) try : self . currentobject [ 'rank' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: rank was "%s", not an integer.' % tok . text ) elif tok . equals ( 'items' ) : tok = self . __consume ( ) try : self . currentobject [ 'size' ] = tok . value ( 'INTEGER' ) except ValueError : raise DXParseError ( 'array: items was "%s", not an integer.' % tok . text ) elif tok . equals ( 'data' ) : tok = self . __consume ( ) if not tok . iscode ( 'STRING' ) : raise DXParseError ( 'array: data was "%s", not a string.' % tok . text ) if tok . text != 'follows' : raise NotImplementedError ( 'array: Only the "data follows header" format is supported.' ) if not self . currentobject [ 'size' ] : raise DXParseError ( "array: missing number of items" ) self . currentobject [ 'array' ] = [ ] while len ( self . currentobject [ 'array' ] ) < self . currentobject [ 'size' ] : self . currentobject [ 'array' ] . extend ( self . dxfile . readline ( ) . strip ( ) . split ( ) ) elif tok . equals ( 'attribute' ) : attribute = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'string' ) : raise DXParseError ( 'array: "string" expected.' ) value = self . __consume ( ) . value ( ) else : raise DXParseError ( 'array: ' + str ( tok ) + ' not recognized.' )
10829	def accept ( self ) : with db . session . begin_nested ( ) : self . state = MembershipState . ACTIVE db . session . merge ( self )
11441	def _correct_record ( record ) : errors = [ ] for tag in record . keys ( ) : upper_bound = '999' n = len ( tag ) if n > 3 : i = n - 3 while i > 0 : upper_bound = '%s%s' % ( '0' , upper_bound ) i -= 1 if tag == '!' : errors . append ( ( 1 , '(field number(s): ' + str ( [ f [ 4 ] for f in record [ tag ] ] ) + ')' ) ) record [ '000' ] = record . pop ( tag ) tag = '000' elif not ( '001' <= tag <= upper_bound or tag in ( 'FMT' , 'FFT' , 'BDR' , 'BDM' ) ) : errors . append ( 2 ) record [ '000' ] = record . pop ( tag ) tag = '000' fields = [ ] for field in record [ tag ] : if field [ 0 ] == [ ] and field [ 3 ] == '' : errors . append ( ( 8 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) subfields = [ ] for subfield in field [ 0 ] : if subfield [ 0 ] == '!' : errors . append ( ( 3 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) newsub = ( '' , subfield [ 1 ] ) else : newsub = subfield subfields . append ( newsub ) if field [ 1 ] == '!' : errors . append ( ( 4 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind1 = " " else : ind1 = field [ 1 ] if field [ 2 ] == '!' : errors . append ( ( 5 , '(field number: ' + str ( field [ 4 ] ) + ')' ) ) ind2 = " " else : ind2 = field [ 2 ] fields . append ( ( subfields , ind1 , ind2 , field [ 3 ] , field [ 4 ] ) ) record [ tag ] = fields return errors
842	def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( "index out of bounds" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId
441	def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params
8263	def swatch ( self , x , y , w = 35 , h = 35 , padding = 0 , roundness = 0 ) : for clr in self : clr . swatch ( x , y , w , h , roundness ) y += h + padding
6539	def matches_masks ( target , masks ) : for mask in masks : if mask . search ( target ) : return True return False
9882	def alpha ( reliability_data = None , value_counts = None , value_domain = None , level_of_measurement = 'interval' , dtype = np . float64 ) : if ( reliability_data is None ) == ( value_counts is None ) : raise ValueError ( "Either reliability_data or value_counts must be provided, but not both." ) if value_counts is None : if type ( reliability_data ) is not np . ndarray : reliability_data = np . array ( reliability_data ) value_domain = value_domain or np . unique ( reliability_data [ ~ np . isnan ( reliability_data ) ] ) value_counts = _reliability_data_to_value_counts ( reliability_data , value_domain ) else : if value_domain : assert value_counts . shape [ 1 ] == len ( value_domain ) , "The value domain should be equal to the number of columns of value_counts." else : value_domain = tuple ( range ( value_counts . shape [ 1 ] ) ) distance_metric = _distance_metric ( level_of_measurement ) o = _coincidences ( value_counts , value_domain , dtype = dtype ) n_v = np . sum ( o , axis = 0 ) n = np . sum ( n_v ) e = _random_coincidences ( value_domain , n , n_v ) d = _distances ( value_domain , distance_metric , n_v ) return 1 - np . sum ( o * d ) / np . sum ( e * d )
3422	def get_context ( obj ) : try : return obj . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass try : return obj . _model . _contexts [ - 1 ] except ( AttributeError , IndexError ) : pass return None
483	def getSwarmModelParams ( modelID ) : cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ "jobId" , "genDescription" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) with open ( baseDescriptionFilePath , mode = "wb" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) with open ( descriptionFilePath , mode = "wb" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )
5867	def _inactivate_organization ( organization ) : [ _inactivate_organization_course_relationship ( record ) for record in internal . OrganizationCourse . objects . filter ( organization_id = organization . id , active = True ) ] [ _inactivate_record ( record ) for record in internal . Organization . objects . filter ( id = organization . id , active = True ) ]
5651	def _scan_footpaths_to_departure_stop ( self , connection_dep_stop , connection_dep_time , arrival_time_target ) : for _ , neighbor , data in self . _walk_network . edges_iter ( nbunch = [ connection_dep_stop ] , data = True ) : d_walk = data [ 'd_walk' ] neighbor_dep_time = connection_dep_time - d_walk / self . _walk_speed pt = LabelTimeSimple ( departure_time = neighbor_dep_time , arrival_time_target = arrival_time_target ) self . _stop_profiles [ neighbor ] . update_pareto_optimal_tuples ( pt )
3530	def get_identity ( context , prefix = None , identity_func = None , user = None ) : if prefix is not None : try : return context [ '%s_identity' % prefix ] except KeyError : pass try : return context [ 'analytical_identity' ] except KeyError : pass if getattr ( settings , 'ANALYTICAL_AUTO_IDENTIFY' , True ) : try : if user is None : user = get_user_from_context ( context ) if get_user_is_authenticated ( user ) : if identity_func is not None : return identity_func ( user ) else : return user . get_username ( ) except ( KeyError , AttributeError ) : pass return None
4241	def ip2long ( ip ) : try : return int ( binascii . hexlify ( socket . inet_aton ( ip ) ) , 16 ) except socket . error : return int ( binascii . hexlify ( socket . inet_pton ( socket . AF_INET6 , ip ) ) , 16 )
8863	def run_pep8 ( request_data ) : import pycodestyle from pyqode . python . backend . pep8utils import CustomChecker WARNING = 1 code = request_data [ 'code' ] path = request_data [ 'path' ] max_line_length = request_data [ 'max_line_length' ] ignore_rules = request_data [ 'ignore_rules' ] ignore_rules += [ 'W291' , 'W292' , 'W293' , 'W391' ] pycodestyle . MAX_LINE_LENGTH = max_line_length pep8style = pycodestyle . StyleGuide ( parse_argv = False , config_file = '' , checker_class = CustomChecker ) try : results = pep8style . input_file ( path , lines = code . splitlines ( True ) ) except Exception : _logger ( ) . exception ( 'Failed to run PEP8 analysis with data=%r' % request_data ) return [ ] else : messages = [ ] for line_number , offset , code , text , doc in results : if code in ignore_rules : continue messages . append ( ( '[PEP8] %s: %s' % ( code , text ) , WARNING , line_number - 1 ) ) return messages
10585	def get_child_account ( self , account_name ) : if r'/' in account_name : accs_in_path = account_name . split ( r'/' , 1 ) curr_acc = self [ accs_in_path [ 0 ] ] if curr_acc is None : return None return curr_acc . get_child_account ( accs_in_path [ 1 ] ) pass else : return self [ account_name ]
10359	def shuffle_relations ( graph : BELGraph , percentage : Optional [ str ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_edges ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) edges = result . edges ( keys = True ) for _ in range ( swaps ) : ( s1 , t1 , k1 ) , ( s2 , t2 , k2 ) = random . sample ( edges , 2 ) result [ s1 ] [ t1 ] [ k1 ] , result [ s2 ] [ t2 ] [ k2 ] = result [ s2 ] [ t2 ] [ k2 ] , result [ s1 ] [ t1 ] [ k1 ] return result
7835	def __from_xml ( self , xmlnode ) : self . fields = [ ] self . reported_fields = [ ] self . items = [ ] self . title = None self . instructions = None if ( xmlnode . type != "element" or xmlnode . name != "x" or xmlnode . ns ( ) . content != DATAFORM_NS ) : raise ValueError ( "Not a form: " + xmlnode . serialize ( ) ) self . type = xmlnode . prop ( "type" ) if not self . type in self . allowed_types : raise BadRequestProtocolError ( "Bad form type: %r" % ( self . type , ) ) child = xmlnode . children while child : if child . type != "element" or child . ns ( ) . content != DATAFORM_NS : pass elif child . name == "title" : self . title = from_utf8 ( child . getContent ( ) ) elif child . name == "instructions" : self . instructions = from_utf8 ( child . getContent ( ) ) elif child . name == "field" : self . fields . append ( Field . _new_from_xml ( child ) ) elif child . name == "item" : self . items . append ( Item . _new_from_xml ( child ) ) elif child . name == "reported" : self . __get_reported ( child ) child = child . next
8348	def convert_charref ( self , name ) : try : n = int ( name ) except ValueError : return if not 0 <= n <= 127 : return return self . convert_codepoint ( n )
8285	def _get_length ( self , segmented = False , precision = 10 ) : if not segmented : return sum ( self . _segment_lengths ( n = precision ) , 0.0 ) else : return self . _segment_lengths ( relative = True , n = precision )
12529	def load_command_table ( self , args ) : with CommandSuperGroup ( __name__ , self , 'rcctl.custom_cluster#{}' ) as super_group : with super_group . group ( 'cluster' ) as group : group . command ( 'select' , 'select' ) with CommandSuperGroup ( __name__ , self , 'rcctl.custom_reliablecollections#{}' , client_factory = client_create ) as super_group : with super_group . group ( 'dictionary' ) as group : group . command ( 'query' , 'query_reliabledictionary' ) group . command ( 'execute' , 'execute_reliabledictionary' ) group . command ( 'schema' , 'get_reliabledictionary_schema' ) group . command ( 'list' , 'get_reliabledictionary_list' ) group . command ( 'type-schema' , 'get_reliabledictionary_type_schema' ) with ArgumentsContext ( self , 'dictionary' ) as ac : ac . argument ( 'application_name' , options_list = [ '--application-name' , '-a' ] ) ac . argument ( 'service_name' , options_list = [ '--service-name' , '-s' ] ) ac . argument ( 'dictionary_name' , options_list = [ '--dictionary-name' , '-d' ] ) ac . argument ( 'output_file' , options_list = [ '--output-file' , '-out' ] ) ac . argument ( 'input_file' , options_list = [ '--input-file' , '-in' ] ) ac . argument ( 'query_string' , options_list = [ '--query-string' , '-q' ] ) ac . argument ( 'type_name' , options_list = [ '--type-name' , '-t' ] ) return OrderedDict ( self . command_table )
1095	def escape ( pattern ) : "Escape all non-alphanumeric characters in pattern." s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == "\000" : s [ i ] = "\\000" else : s [ i ] = "\\" + c return pattern [ : 0 ] . join ( s )
5043	def send_messages ( cls , http_request , message_requests ) : deduplicated_messages = set ( message_requests ) for msg_type , text in deduplicated_messages : message_function = getattr ( messages , msg_type ) message_function ( http_request , text )
5777	def _advapi32_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP out_len = new ( advapi32 , 'DWORD *' , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , null ( ) , out_len , 0 ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) write_to_buffer ( buffer , data ) pointer_set ( out_len , len ( data ) ) res = advapi32 . CryptEncrypt ( certificate_or_public_key . ex_key_handle , null ( ) , True , flags , buffer , out_len , buffer_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) ) [ : : - 1 ]
1369	def create_tar ( tar_filename , files , config_dir , config_files ) : with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : for filename in files : if os . path . isfile ( filename ) : tar . add ( filename , arcname = os . path . basename ( filename ) ) else : raise Exception ( "%s is not an existing file" % filename ) if os . path . isdir ( config_dir ) : tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) else : raise Exception ( "%s is not an existing directory" % config_dir ) for filename in config_files : if os . path . isfile ( filename ) : arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) tar . add ( filename , arcname = arcfile ) else : raise Exception ( "%s is not an existing file" % filename )
4473	def __recursive_transform ( self , jam , steps ) : if len ( steps ) > 0 : head_transformer = steps [ 0 ] [ 1 ] for t_jam in head_transformer . transform ( jam ) : for q in self . __recursive_transform ( t_jam , steps [ 1 : ] ) : yield q else : yield jam
12933	def as_dict ( self , * args , ** kwargs ) : self_as_dict = super ( ClinVarAllele , self ) . as_dict ( * args , ** kwargs ) self_as_dict [ 'hgvs' ] = self . hgvs self_as_dict [ 'clnalleleid' ] = self . clnalleleid self_as_dict [ 'clnsig' ] = self . clnsig self_as_dict [ 'clndn' ] = self . clndn self_as_dict [ 'clndisdb' ] = self . clndisdb self_as_dict [ 'clnvi' ] = self . clnvi return self_as_dict
13024	def get ( self , pk ) : if type ( pk ) == str : try : pk = int ( pk ) except ValueError : pass return self . select ( "SELECT {0} FROM " + self . table + " WHERE " + self . pk + " = {1};" , self . columns , pk )
5951	def strftime ( self , fmt = "%d:%H:%M:%S" ) : substitutions = { "%d" : str ( self . days ) , "%H" : "{0:02d}" . format ( self . dhours ) , "%h" : str ( 24 * self . days + self . dhours ) , "%M" : "{0:02d}" . format ( self . dminutes ) , "%S" : "{0:02d}" . format ( self . dseconds ) , } s = fmt for search , replacement in substitutions . items ( ) : s = s . replace ( search , replacement ) return s
2562	def recv_task_request_from_workers ( self ) : info = MPI . Status ( ) comm . recv ( source = MPI . ANY_SOURCE , tag = TASK_REQUEST_TAG , status = info ) worker_rank = info . Get_source ( ) logger . info ( "Received task request from worker:{}" . format ( worker_rank ) ) return worker_rank
4686	def encrypt ( self , message ) : if not message : return None nonce = str ( random . getrandbits ( 64 ) ) try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( self . from_account [ "options" ] [ "memo_key" ] ) except KeyNotFound : raise MissingKeyError ( "Memo private key {} for {} could not be found" . format ( self . from_account [ "options" ] [ "memo_key" ] , self . from_account [ "name" ] ) ) if not memo_wif : raise MissingKeyError ( "Memo key for %s missing!" % self . from_account [ "name" ] ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix enc = memo . encode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( self . to_account [ "options" ] [ "memo_key" ] , prefix = self . chain_prefix ) , nonce , message , ) return { "message" : enc , "nonce" : nonce , "from" : self . from_account [ "options" ] [ "memo_key" ] , "to" : self . to_account [ "options" ] [ "memo_key" ] , }
1225	def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter
64	def is_out_of_image ( self , image , fully = True , partly = False ) : if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully
5674	def get_main_database_path ( self ) : cur = self . conn . cursor ( ) cur . execute ( "PRAGMA database_list" ) rows = cur . fetchall ( ) for row in rows : if row [ 1 ] == str ( "main" ) : return row [ 2 ]
9965	def _to_attrdict ( self , attrs = None ) : result = self . _baseattrs for attr in attrs : if hasattr ( self , attr ) : result [ attr ] = getattr ( self , attr ) . _to_attrdict ( attrs ) return result
9639	def emit ( self , record ) : try : if self . max_messages : p = self . redis_client . pipeline ( ) p . rpush ( self . key , self . format ( record ) ) p . ltrim ( self . key , - self . max_messages , - 1 ) p . execute ( ) else : self . redis_client . rpush ( self . key , self . format ( record ) ) except redis . RedisError : pass
11747	def routes_simple ( self ) : routes = [ ] for bundle in self . _registered_bundles : bundle_path = bundle [ 'path' ] for blueprint in bundle [ 'blueprints' ] : bp_path = blueprint [ 'path' ] for child in blueprint [ 'routes' ] : routes . append ( ( child [ 'endpoint' ] , bundle_path + bp_path + child [ 'path' ] , child [ 'methods' ] ) ) return routes
13049	def check_service ( service ) : service . add_tag ( 'header_scan' ) http = False try : result = requests . head ( 'http://{}:{}' . format ( service . address , service . port ) , timeout = 1 ) print_success ( "Found http service on {}:{}" . format ( service . address , service . port ) ) service . add_tag ( 'http' ) http = True try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass if not http : try : result = requests . head ( 'https://{}:{}' . format ( service . address , service . port ) , verify = False , timeout = 3 ) service . add_tag ( 'https' ) print_success ( "Found https service on {}:{}" . format ( service . address , service . port ) ) try : service . banner = result . headers [ 'Server' ] except KeyError : pass except ( ConnectionError , ConnectTimeout , ReadTimeout , Error ) : pass service . save ( )
9688	def read_bin_boundaries ( self ) : config = [ ] data = { } self . cnxn . xfer ( [ 0x33 ] ) sleep ( 10e-3 ) for i in range ( 30 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] config . append ( resp ) for i in range ( 0 , 14 ) : data [ "Bin Boundary {0}" . format ( i ) ] = self . _16bit_unsigned ( config [ 2 * i ] , config [ 2 * i + 1 ] ) return data
7126	def add_download_total ( rows ) : total_row = [ "" ] * len ( rows [ 0 ] ) total_row [ 0 ] = "Total" total_downloads , downloads_column = get_download_total ( rows ) total_row [ downloads_column ] = str ( total_downloads ) rows . append ( total_row ) return rows
12965	def allOnlyFields ( self , fields , cascadeFetch = False ) : matchedKeys = self . getPrimaryKeys ( ) if matchedKeys : return self . getMultipleOnlyFields ( matchedKeys , fields , cascadeFetch = cascadeFetch ) return IRQueryableList ( [ ] , mdl = self . mdl )
4900	def handle ( self , * args , ** options ) : if not CourseEnrollment : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) days , enterprise_customer = self . parse_arguments ( * args , ** options ) if enterprise_customer : try : lrs_configuration = XAPILRSConfiguration . objects . get ( active = True , enterprise_customer = enterprise_customer ) except XAPILRSConfiguration . DoesNotExist : raise CommandError ( 'No xAPI Configuration found for "{enterprise_customer}"' . format ( enterprise_customer = enterprise_customer . name ) ) self . send_xapi_statements ( lrs_configuration , days ) else : for lrs_configuration in XAPILRSConfiguration . objects . filter ( active = True ) : self . send_xapi_statements ( lrs_configuration , days )
8974	def new_knitting_pattern_set_loader ( specification = DefaultSpecification ( ) ) : parser = specification . new_parser ( specification ) loader = specification . new_loader ( parser . knitting_pattern_set ) return loader
11127	def update_file ( self , value , relativePath , name = None , description = False , klass = False , dump = False , pull = False , ACID = None , verbose = False ) : if ACID is None : ACID = self . __ACID assert isinstance ( ACID , bool ) , "ACID must be boolean" relativePath = os . path . normpath ( relativePath ) if relativePath == '.' : relativePath = '' assert name != '.pyrepinfo' , "'.pyrepinfo' is not allowed as file name in main repository directory" assert name != '.pyrepstate' , "'.pyrepstate' is not allowed as file name in main repository directory" assert name != '.pyreplock' , "'.pyreplock' is not allowed as file name in main repository directory" if name is None : assert len ( relativePath ) , "name must be given when relative path is given as empty string or as a simple dot '.'" relativePath , name = os . path . split ( relativePath ) fileInfoDict , errorMessage = self . get_file_info ( relativePath , name ) assert fileInfoDict is not None , errorMessage realPath = os . path . join ( self . __path , relativePath ) if verbose : if not os . path . isfile ( os . path . join ( realPath , name ) ) : warnings . warn ( "file '%s' is in repository but does not exist in the system. It is therefore being recreated." % os . path . join ( realPath , name ) ) if not dump : dump = fileInfoDict [ "dump" ] if not pull : pull = fileInfoDict [ "pull" ] if ACID : savePath = os . path . join ( tempfile . gettempdir ( ) , name ) else : savePath = os . path . join ( realPath , name ) try : exec ( dump . replace ( "$FILE_PATH" , str ( savePath ) ) ) except Exception as e : message = "unable to dump the file (%s)" % e if 'pickle.dump(' in dump : message += '\nmore info: %s' % str ( get_pickling_errors ( value ) ) raise Exception ( message ) if ACID : try : shutil . copyfile ( savePath , os . path . join ( realPath , name ) ) except Exception as e : os . remove ( savePath ) if verbose : warnings . warn ( e ) return os . remove ( savePath ) fileInfoDict [ "timestamp" ] = datetime . utcnow ( ) if description is not False : fileInfoDict [ "description" ] = description if klass is not False : assert inspect . isclass ( klass ) , "klass must be a class definition" fileInfoDict [ "class" ] = klass self . save ( )
13752	def _reference_table ( cls , ref_table ) : cols = [ ( sa . Column ( ) , refcol ) for refcol in ref_table . primary_key ] for col , refcol in cols : setattr ( cls , "%s_%s" % ( ref_table . name , refcol . name ) , col ) cls . __table__ . append_constraint ( sa . ForeignKeyConstraint ( * zip ( * cols ) ) )
5856	def create_dataset_version ( self , dataset_id ) : failure_message = "Failed to create dataset version for dataset {}" . format ( dataset_id ) number = self . _get_success_json ( self . _post_json ( routes . create_dataset_version ( dataset_id ) , data = { } , failure_message = failure_message ) ) [ 'dataset_scoped_id' ] return DatasetVersion ( number = number )
9522	def merge_to_one_seq ( infile , outfile , seqname = 'union' ) : seq_reader = sequences . file_reader ( infile ) seqs = [ ] for seq in seq_reader : seqs . append ( copy . copy ( seq ) ) new_seq = '' . join ( [ seq . seq for seq in seqs ] ) if type ( seqs [ 0 ] ) == sequences . Fastq : new_qual = '' . join ( [ seq . qual for seq in seqs ] ) seqs [ : ] = [ ] merged = sequences . Fastq ( seqname , new_seq , new_qual ) else : merged = sequences . Fasta ( seqname , new_seq ) seqs [ : ] = [ ] f = utils . open_file_write ( outfile ) print ( merged , file = f ) utils . close ( f )
12349	def create ( self , name , region , size , image , ssh_keys = None , backups = None , ipv6 = None , private_networking = None , wait = True ) : if ssh_keys and not isinstance ( ssh_keys , ( list , tuple ) ) : raise TypeError ( "ssh_keys must be a list" ) resp = self . post ( name = name , region = region , size = size , image = image , ssh_keys = ssh_keys , private_networking = private_networking , backups = backups , ipv6 = ipv6 ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) if wait : droplet . wait ( ) droplet = self . get ( resp [ self . singular ] [ 'id' ] ) return droplet
7736	def map ( self , data ) : result = [ ] for char in data : ret = None for lookup in self . mapping : ret = lookup ( char ) if ret is not None : break if ret is not None : result . append ( ret ) else : result . append ( char ) return result
8446	def switch ( template , version ) : temple . update . update ( new_template = template , new_version = version )
13370	def is_redirecting ( path ) : candidate = unipath ( path , '.cpenv' ) return os . path . exists ( candidate ) and os . path . isfile ( candidate )
7393	def mods_genre ( self ) : type2genre = { 'conference' : 'conference publication' , 'book chapter' : 'bibliography' , 'unpublished' : 'article' } tp = str ( self . type ) . lower ( ) return type2genre . get ( tp , tp )
2487	def create_conjunction_node ( self , conjunction ) : node = BNode ( ) type_triple = ( node , RDF . type , self . spdx_namespace . ConjunctiveLicenseSet ) self . graph . add ( type_triple ) licenses = self . licenses_from_tree ( conjunction ) for lic in licenses : member_triple = ( node , self . spdx_namespace . member , lic ) self . graph . add ( member_triple ) return node
11108	def walk_directories_relative_path ( self , relativePath = "" ) : def walk_directories ( directory , relativePath ) : directories = dict . __getitem__ ( directory , 'directories' ) dirNames = dict . keys ( directories ) for d in sorted ( dirNames ) : yield os . path . join ( relativePath , d ) for k in sorted ( dict . keys ( directories ) ) : path = os . path . join ( relativePath , k ) dir = dict . __getitem__ ( directories , k ) for e in walk_directories ( dir , path ) : yield e dir , errorMessage = self . get_directory_info ( relativePath ) assert dir is not None , errorMessage return walk_directories ( dir , relativePath = '' )
4664	def detail ( self , * args , ** kwargs ) : prefix = kwargs . pop ( "prefix" , default_prefix ) kwargs [ "votes" ] = list ( set ( kwargs [ "votes" ] ) ) return OrderedDict ( [ ( "memo_key" , PublicKey ( kwargs [ "memo_key" ] , prefix = prefix ) ) , ( "voting_account" , ObjectId ( kwargs [ "voting_account" ] , "account" ) ) , ( "num_witness" , Uint16 ( kwargs [ "num_witness" ] ) ) , ( "num_committee" , Uint16 ( kwargs [ "num_committee" ] ) ) , ( "votes" , Array ( [ VoteId ( o ) for o in kwargs [ "votes" ] ] ) ) , ( "extensions" , Set ( [ ] ) ) , ] )
5165	def __intermediate_proto ( self , interface , address ) : address_proto = address . pop ( 'proto' , 'static' ) if 'proto' not in interface : return address_proto else : return interface . pop ( 'proto' )
5702	def route_frequencies ( gtfs , results_by_mode = False ) : day = gtfs . get_suitable_date_for_daily_extract ( ) query = ( " SELECT f.route_I, type, frequency FROM routes as r" " JOIN" " (SELECT route_I, COUNT(route_I) as frequency" " FROM" " (SELECT date, route_I, trip_I" " FROM day_stop_times" " WHERE date = '{day}'" " GROUP by route_I, trip_I)" " GROUP BY route_I) as f" " ON f.route_I = r.route_I" " ORDER BY frequency DESC" . format ( day = day ) ) return pd . DataFrame ( gtfs . execute_custom_query_pandas ( query ) )
7013	def read_hatpi_pklc ( lcfile ) : try : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) lcdict = pickle . load ( infd ) infd . close ( ) return lcdict except UnicodeDecodeError : if lcfile . endswith ( '.gz' ) : infd = gzip . open ( lcfile , 'rb' ) else : infd = open ( lcfile , 'rb' ) LOGWARNING ( 'pickle %s was probably from Python 2 ' 'and failed to load without using "latin1" encoding. ' 'This is probably a numpy issue: ' 'http://stackoverflow.com/q/11305790' % lcfile ) lcdict = pickle . load ( infd , encoding = 'latin1' ) infd . close ( ) return lcdict
11031	def get_json_field ( self , field , ** kwargs ) : d = self . request ( 'GET' , headers = { 'Accept' : 'application/json' } , ** kwargs ) d . addCallback ( raise_for_status ) d . addCallback ( raise_for_header , 'Content-Type' , 'application/json' ) d . addCallback ( json_content ) d . addCallback ( self . _get_json_field , field ) return d
4534	def fill ( self , color , start = 0 , end = - 1 ) : start = max ( start , 0 ) if end < 0 or end >= self . numLEDs : end = self . numLEDs - 1 for led in range ( start , end + 1 ) : self . _set_base ( led , color )
6967	def smooth_magseries_gaussfilt ( mags , windowsize , windowfwhm = 7 ) : convkernel = Gaussian1DKernel ( windowfwhm , x_size = windowsize ) smoothed = convolve ( mags , convkernel , boundary = 'extend' ) return smoothed
11773	def information_content ( values ) : "Number of bits to represent the probability distribution in values." probabilities = normalize ( removeall ( 0 , values ) ) return sum ( - p * log2 ( p ) for p in probabilities )
6681	def move ( self , source , destination , use_sudo = False ) : func = use_sudo and run_as_root or self . run func ( '/bin/mv {0} {1}' . format ( quote ( source ) , quote ( destination ) ) )
11464	def download ( self , source_file , target_folder = '' ) : current_folder = self . _ftp . pwd ( ) if not target_folder . startswith ( '/' ) : target_folder = join ( getcwd ( ) , target_folder ) folder = os . path . dirname ( source_file ) self . cd ( folder ) if folder . startswith ( "/" ) : folder = folder [ 1 : ] destination_folder = join ( target_folder , folder ) if not os . path . exists ( destination_folder ) : print ( "Creating folder" , destination_folder ) os . makedirs ( destination_folder ) source_file = os . path . basename ( source_file ) destination = join ( destination_folder , source_file ) try : with open ( destination , 'wb' ) as result : self . _ftp . retrbinary ( 'RETR %s' % ( source_file , ) , result . write ) except error_perm as e : print ( e ) remove ( join ( target_folder , source_file ) ) raise self . _ftp . cwd ( current_folder )
9625	def register ( self , cls ) : preview = cls ( site = self ) logger . debug ( 'Registering %r with %r' , preview , self ) index = self . __previews . setdefault ( preview . module , { } ) index [ cls . __name__ ] = preview
13142	def recover_triples_from_mapping ( indexes , ents : bidict , rels : bidict ) : triples = [ ] for t in indexes : triples . append ( kgedata . Triple ( ents . inverse [ t . head ] , rels . inverse [ t . relation ] , ents . inverse [ t . tail ] ) ) return triples
4339	def pitch ( self , n_semitones , quick = False ) : if not is_number ( n_semitones ) : raise ValueError ( "n_semitones must be a positive number" ) if n_semitones < - 12 or n_semitones > 12 : logger . warning ( "Using an extreme pitch shift. " "Quality of results will be poor" ) if not isinstance ( quick , bool ) : raise ValueError ( "quick must be a boolean." ) effect_args = [ 'pitch' ] if quick : effect_args . append ( '-q' ) effect_args . append ( '{:f}' . format ( n_semitones * 100. ) ) self . effects . extend ( effect_args ) self . effects_log . append ( 'pitch' ) return self
12911	def union ( self , other , recursive = True , overwrite = False ) : if not isinstance ( other , composite ) : raise AssertionError ( 'Cannot union composite and {} types' . format ( type ( other ) ) ) if self . meta_type != other . meta_type : return composite ( [ self , other ] ) if self . meta_type == 'list' : keep = [ ] for item in self . _list : keep . append ( item ) for item in other . _list : if item not in self . _list : keep . append ( item ) return composite ( keep ) elif self . meta_type == 'dict' : keep = { } for key in list ( set ( list ( self . _dict . keys ( ) ) + list ( other . _dict . keys ( ) ) ) ) : left = self . _dict . get ( key ) right = other . _dict . get ( key ) if recursive and isinstance ( left , composite ) and isinstance ( right , composite ) : keep [ key ] = left . union ( right , recursive = recursive , overwrite = overwrite ) elif left == right : keep [ key ] = left elif left is None : keep [ key ] = right elif right is None : keep [ key ] = left elif overwrite : keep [ key ] = right else : keep [ key ] = composite ( [ left , right ] ) return composite ( keep ) return
2266	def map_vals ( func , dict_ ) : if not hasattr ( func , '__call__' ) : func = func . __getitem__ keyval_list = [ ( key , func ( val ) ) for key , val in six . iteritems ( dict_ ) ] dictclass = OrderedDict if isinstance ( dict_ , OrderedDict ) else dict newdict = dictclass ( keyval_list ) return newdict
13461	def event_update_list ( request , slug ) : event = get_object_or_404 ( Event , slug = slug ) updates = Update . objects . filter ( event__slug = slug ) if event . recently_ended ( ) : updates = updates . order_by ( 'id' ) else : updates = updates . order_by ( '-id' ) return render ( request , 'happenings/updates/update_list.html' , { 'event' : event , 'object_list' : updates , } )
8208	def angle ( self , x0 , y0 , x1 , y1 ) : a = degrees ( atan ( ( y1 - y0 ) / ( x1 - x0 + 0.00001 ) ) ) + 360 if x1 - x0 < 0 : a += 180 return a
4687	def decrypt ( self , message ) : if not message : return None try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "to" ] ) pubkey = message [ "from" ] except KeyNotFound : try : memo_wif = self . blockchain . wallet . getPrivateKeyForPublicKey ( message [ "from" ] ) pubkey = message [ "to" ] except KeyNotFound : raise MissingKeyError ( "None of the required memo keys are installed!" "Need any of {}" . format ( [ message [ "to" ] , message [ "from" ] ] ) ) if not hasattr ( self , "chain_prefix" ) : self . chain_prefix = self . blockchain . prefix return memo . decode_memo ( self . privatekey_class ( memo_wif ) , self . publickey_class ( pubkey , prefix = self . chain_prefix ) , message . get ( "nonce" ) , message . get ( "message" ) , )
5789	def handle_openssl_error ( result , exception_class = None ) : if result > 0 : return if exception_class is None : exception_class = OSError error_num = libcrypto . ERR_get_error ( ) buffer = buffer_from_bytes ( 120 ) libcrypto . ERR_error_string ( error_num , buffer ) error_string = byte_string_from_buffer ( buffer ) raise exception_class ( _try_decode ( error_string ) )
7276	def set_position ( self , position ) : self . _player_interface . SetPosition ( ObjectPath ( "/not/used" ) , Int64 ( position * 1000.0 * 1000 ) ) self . positionEvent ( self , position )
11668	def quadratic ( Ks , dim , rhos , required = None ) : r N = rhos . shape [ 0 ] Ks = np . asarray ( Ks ) Bs = ( Ks - 1 ) / np . pi ** ( dim / 2 ) * gamma ( dim / 2 + 1 ) est = Bs / ( N - 1 ) * np . mean ( rhos ** ( - dim ) , axis = 0 ) return est
13833	def ParseInteger ( text , is_signed = False , is_long = False ) : try : if is_long : result = long ( text , 0 ) else : result = int ( text , 0 ) except ValueError : raise ValueError ( 'Couldn\'t parse integer: %s' % text ) checker = _INTEGER_CHECKERS [ 2 * int ( is_long ) + int ( is_signed ) ] checker . CheckValue ( result ) return result
6091	def cache ( func ) : def wrapper ( instance : GeometryProfile , grid : np . ndarray , * args , ** kwargs ) : if not hasattr ( instance , "cache" ) : instance . cache = { } key = ( func . __name__ , grid . tobytes ( ) ) if key not in instance . cache : instance . cache [ key ] = func ( instance , grid ) return instance . cache [ key ] return wrapper
263	def plot_factor_contribution_to_perf ( perf_attrib_data , ax = None , title = 'Cumulative common returns attribution' , ) : if ax is None : ax = plt . gca ( ) factors_to_plot = perf_attrib_data . drop ( [ 'total_returns' , 'common_returns' ] , axis = 'columns' , errors = 'ignore' ) factors_cumulative = pd . DataFrame ( ) for factor in factors_to_plot : factors_cumulative [ factor ] = ep . cum_returns ( factors_to_plot [ factor ] ) for col in factors_cumulative : ax . plot ( factors_cumulative [ col ] ) ax . axhline ( 0 , color = 'k' ) configure_legend ( ax , change_colors = True ) ax . set_ylabel ( 'Cumulative returns by factor' ) ax . set_title ( title ) return ax
1008	def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )
5987	def compute_deflections_at_next_plane ( plane_index , total_planes ) : if plane_index < total_planes - 1 : return True elif plane_index == total_planes - 1 : return False else : raise exc . RayTracingException ( 'A galaxy was not correctly allocated its previous / next redshifts' )
7189	def fix_line_numbers ( body ) : r maxline = 0 for node in body . pre_order ( ) : maxline += node . prefix . count ( '\n' ) if isinstance ( node , Leaf ) : node . lineno = maxline maxline += str ( node . value ) . count ( '\n' )
7666	def _key ( cls , obs ) : if not isinstance ( obs , Observation ) : raise JamsError ( '{} must be of type jams.Observation' . format ( obs ) ) return obs . time
309	def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ "green" , "orange" , "orangered" , "darkred" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes
6513	def _most_popular_gender ( self , name , counter ) : if name not in self . names : return self . unknown_value max_count , max_tie = ( 0 , 0 ) best = self . names [ name ] . keys ( ) [ 0 ] for gender , country_values in self . names [ name ] . items ( ) : count , tie = counter ( country_values ) if count > max_count or ( count == max_count and tie > max_tie ) : max_count , max_tie , best = count , tie , gender return best if max_count > 0 else self . unknown_value
10707	def create_vacation ( body ) : arequest = requests . post ( VACATIONS_URL , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '200' : _LOGGER . error ( "Failed to create vacation. " + status_code ) _LOGGER . error ( arequest . json ( ) ) return False return arequest . json ( )
1286	def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops
4562	def recurse ( desc , pre = 'pre_recursion' , post = None , python_path = None ) : def call ( f , desc ) : if isinstance ( f , str ) : f = getattr ( datatype , f , None ) return f and f ( desc ) desc = load . load_if_filename ( desc ) or desc desc = construct . to_type_constructor ( desc , python_path ) datatype = desc . get ( 'datatype' ) desc = call ( pre , desc ) or desc for child_name in getattr ( datatype , 'CHILDREN' , [ ] ) : child = desc . get ( child_name ) if child : is_plural = child_name . endswith ( 's' ) remove_s = is_plural and child_name != 'drivers' cname = child_name [ : - 1 ] if remove_s else child_name new_path = python_path or ( 'bibliopixel.' + cname ) if is_plural : if isinstance ( child , ( dict , str ) ) : child = [ child ] for i , c in enumerate ( child ) : child [ i ] = recurse ( c , pre , post , new_path ) desc [ child_name ] = child else : desc [ child_name ] = recurse ( child , pre , post , new_path ) d = call ( post , desc ) return desc if d is None else d
10511	def onwindowcreate ( self , window_name , fn_name , * args ) : self . _pollEvents . _callback [ window_name ] = [ "onwindowcreate" , fn_name , args ] return self . _remote_onwindowcreate ( window_name )
8527	def add_child ( self , child ) : if not isinstance ( child , ChildMixin ) : raise TypeError ( 'Requires instance of TreeElement. ' 'Got {}' . format ( type ( child ) ) ) child . parent = self self . _children . append ( child )
13757	def get_path_extension ( path ) : file_path , file_ext = os . path . splitext ( path ) return file_ext . lstrip ( '.' )
6871	def estimate_achievable_tmid_precision ( snr , t_ingress_min = 10 , t_duration_hr = 2.14 ) : t_ingress = t_ingress_min * u . minute t_duration = t_duration_hr * u . hour theta = t_ingress / t_duration sigma_tc = ( 1 / snr * t_duration * np . sqrt ( theta / 2 ) ) LOGINFO ( 'assuming t_ingress = {:.1f}' . format ( t_ingress ) ) LOGINFO ( 'assuming t_duration = {:.1f}' . format ( t_duration ) ) LOGINFO ( 'measured SNR={:.2f}\n\t' . format ( snr ) + ' . format ( sigma_tc . to ( u . minute ) , sigma_tc . to ( u . hour ) , sigma_tc . to ( u . day ) ) ) return sigma_tc . to ( u . day ) . value
3724	def load_group_assignments_DDBST ( ) : if DDBST_UNIFAC_assignments : return None with open ( os . path . join ( folder , 'DDBST UNIFAC assignments.tsv' ) ) as f : _group_assignments = [ DDBST_UNIFAC_assignments , DDBST_MODIFIED_UNIFAC_assignments , DDBST_PSRK_assignments ] for line in f . readlines ( ) : key , valids , original , modified , PSRK = line . split ( '\t' ) valids = [ True if i == '1' else False for i in valids . split ( ' ' ) ] for groups , storage , valid in zip ( [ original , modified , PSRK ] , _group_assignments , valids ) : if valid : groups = groups . rstrip ( ) . split ( ' ' ) d_data = { } for i in range ( int ( len ( groups ) / 2 ) ) : d_data [ int ( groups [ i * 2 ] ) ] = int ( groups [ i * 2 + 1 ] ) storage [ key ] = d_data
3179	def get ( self , batch_webhook_id , ** queryparams ) : self . batch_webhook_id = batch_webhook_id return self . _mc_client . _get ( url = self . _build_path ( batch_webhook_id ) , ** queryparams )
5841	def submit_design_run ( self , data_view_id , num_candidates , effort , target = None , constraints = [ ] , sampler = "Default" ) : if effort > 30 : raise CitrinationClientError ( "Parameter effort must be less than 30 to trigger a design run" ) if target is not None : target = target . to_dict ( ) constraint_dicts = [ c . to_dict ( ) for c in constraints ] body = { "num_candidates" : num_candidates , "target" : target , "effort" : effort , "constraints" : constraint_dicts , "sampler" : sampler } url = routes . submit_data_view_design ( data_view_id ) response = self . _post_json ( url , body ) . json ( ) return DesignRun ( response [ "data" ] [ "design_run" ] [ "uid" ] )
2686	def curated ( name ) : return cached_download ( 'https://docs.mikeboers.com/pyav/samples/' + name , os . path . join ( 'pyav-curated' , name . replace ( '/' , os . path . sep ) ) )
3744	def _round_whole_even ( i ) : r if i % .5 == 0 : if ( i + 0.5 ) % 2 == 0 : i = i + 0.5 else : i = i - 0.5 else : i = round ( i , 0 ) return int ( i )
9790	def _remove_trailing_spaces ( line ) : while line . endswith ( ' ' ) and not line . endswith ( '\\ ' ) : line = line [ : - 1 ] return line . replace ( '\\ ' , ' ' )
9016	def instruction_in_row ( self , row , specification ) : whole_instruction_ = self . _as_instruction ( specification ) return self . _spec . new_instruction_in_row ( row , whole_instruction_ )
12479	def rcfile ( appname , section = None , args = { } , strip_dashes = True ) : if strip_dashes : for k in args . keys ( ) : args [ k . lstrip ( '-' ) ] = args . pop ( k ) environ = get_environment ( appname ) if section is None : section = appname config = get_config ( appname , section , args . get ( 'config' , '' ) , args . get ( 'path' , '' ) ) config = merge ( merge ( args , config ) , environ ) if not config : raise IOError ( 'Could not find any rcfile for application ' '{}.' . format ( appname ) ) return config
12615	def count ( self , table_name , sample ) : return len ( list ( search_sample ( table = self . table ( table_name ) , sample = sample ) ) )
9939	def find_in_app ( self , app , path ) : storage = self . storages . get ( app , None ) if storage : if storage . exists ( path ) : matched_path = storage . path ( path ) if matched_path : return matched_path
8664	def _build_dict_from_key_value ( keys_and_values ) : key_dict = { } for key_value in keys_and_values : if '=' not in key_value : raise GhostError ( 'Pair {0} is not of `key=value` format' . format ( key_value ) ) key , value = key_value . split ( '=' , 1 ) key_dict . update ( { str ( key ) : str ( value ) } ) return key_dict
8153	def simple_traceback ( ex , source ) : exc_type , exc_value , exc_tb = sys . exc_info ( ) exc = traceback . format_exception ( exc_type , exc_value , exc_tb ) source_arr = source . splitlines ( ) exc_location = exc [ - 2 ] for i , err in enumerate ( exc ) : if 'exec source in ns' in err : exc_location = exc [ i + 1 ] break fn = exc_location . split ( ',' ) [ 0 ] [ 8 : - 1 ] line_number = int ( exc_location . split ( ',' ) [ 1 ] . replace ( 'line' , '' ) . strip ( ) ) err_msgs = [ ] err_where = ' ' . join ( exc [ i - 1 ] . split ( ',' ) [ 1 : ] ) . strip ( ) err_msgs . append ( 'Error in the Shoebot script at %s:' % err_where ) for i in xrange ( max ( 0 , line_number - 5 ) , line_number ) : if fn == "<string>" : line = source_arr [ i ] else : line = linecache . getline ( fn , i + 1 ) err_msgs . append ( '%s: %s' % ( i + 1 , line . rstrip ( ) ) ) err_msgs . append ( ' %s^ %s' % ( len ( str ( i ) ) * ' ' , exc [ - 1 ] . rstrip ( ) ) ) err_msgs . append ( '' ) err_msgs . append ( exc [ 0 ] . rstrip ( ) ) for err in exc [ 3 : ] : err_msgs . append ( err . rstrip ( ) ) return '\n' . join ( err_msgs )
10558	def download ( self , songs , template = None ) : if not template : template = os . getcwd ( ) songnum = 0 total = len ( songs ) results = [ ] errors = { } pad = len ( str ( total ) ) for result in self . _download ( songs , template ) : song_id = songs [ songnum ] [ 'id' ] songnum += 1 downloaded , error = result if downloaded : logger . info ( "({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})" . format ( num = songnum , pad = pad , total = total , file = downloaded [ song_id ] , song_id = song_id ) ) results . append ( { 'result' : 'downloaded' , 'id' : song_id , 'filepath' : downloaded [ song_id ] } ) elif error : title = songs [ songnum ] . get ( 'title' , "<empty>" ) artist = songs [ songnum ] . get ( 'artist' , "<empty>" ) album = songs [ songnum ] . get ( 'album' , "<empty>" ) logger . info ( "({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})" . format ( num = songnum , pad = pad , total = total , title = title , artist = artist , album = album , song_id = song_id ) ) results . append ( { 'result' : 'error' , 'id' : song_id , 'message' : error [ song_id ] } ) if errors : logger . info ( "\n\nThe following errors occurred:\n" ) for filepath , e in errors . items ( ) : logger . info ( "{file} | {error}" . format ( file = filepath , error = e ) ) logger . info ( "\nThese files may need to be synced again.\n" ) return results
6498	def remove ( self , doc_type , doc_ids , ** kwargs ) : try : actions = [ ] for doc_id in doc_ids : log . debug ( "Removing document of type %s and index %s" , doc_type , doc_id ) action = { '_op_type' : 'delete' , "_index" : self . index_name , "_type" : doc_type , "_id" : doc_id } actions . append ( action ) bulk ( self . _es , actions , ** kwargs ) except BulkIndexError as ex : valid_errors = [ error for error in ex . errors if error [ 'delete' ] [ 'status' ] != 404 ] if valid_errors : log . exception ( "An error occurred while removing documents from the index." ) raise
7720	def set_history ( self , parameters ) : for child in xml_element_iter ( self . xmlnode . children ) : if get_node_ns_uri ( child ) == MUC_NS and child . name == "history" : child . unlinkNode ( ) child . freeNode ( ) break if parameters . maxchars and parameters . maxchars < 0 : raise ValueError ( "History parameter maxchars must be positive" ) if parameters . maxstanzas and parameters . maxstanzas < 0 : raise ValueError ( "History parameter maxstanzas must be positive" ) if parameters . maxseconds and parameters . maxseconds < 0 : raise ValueError ( "History parameter maxseconds must be positive" ) hnode = self . xmlnode . newChild ( self . xmlnode . ns ( ) , "history" , None ) if parameters . maxchars is not None : hnode . setProp ( "maxchars" , str ( parameters . maxchars ) ) if parameters . maxstanzas is not None : hnode . setProp ( "maxstanzas" , str ( parameters . maxstanzas ) ) if parameters . maxseconds is not None : hnode . setProp ( "maxseconds" , str ( parameters . maxseconds ) ) if parameters . since is not None : hnode . setProp ( "since" , parameters . since . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) )
7075	def run_periodfinding ( simbasedir , pfmethods = ( 'gls' , 'pdm' , 'bls' ) , pfkwargs = ( { } , { } , { 'startp' : 1.0 , 'maxtransitduration' : 0.3 } ) , getblssnr = False , sigclip = 5.0 , nperiodworkers = 10 , ncontrolworkers = 4 , liststartindex = None , listmaxobjects = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] pfdir = os . path . join ( simbasedir , 'periodfinding' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) if liststartindex : lcfpaths = lcfpaths [ liststartindex : ] if listmaxobjects : lcfpaths = lcfpaths [ : listmaxobjects ] pfinfo = periodsearch . parallel_pf ( lcfpaths , pfdir , lcformat = fakelc_formatkey , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nperiodworkers = nperiodworkers , ncontrolworkers = ncontrolworkers ) with open ( os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' ) , 'wb' ) as outfd : pickle . dump ( pfinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-periodsearch.pkl' )
3944	def serialize ( self ) : segment = hangouts_pb2 . Segment ( type = self . type_ , text = self . text , formatting = hangouts_pb2 . Formatting ( bold = self . is_bold , italic = self . is_italic , strikethrough = self . is_strikethrough , underline = self . is_underline , ) , ) if self . link_target is not None : segment . link_data . link_target = self . link_target return segment
95	def quokka_segmentation_map ( size = None , extract = None ) : from imgaug . augmentables . segmaps import SegmentationMapOnImage with open ( QUOKKA_ANNOTATIONS_FP , "r" ) as f : json_dict = json . load ( f ) xx = [ ] yy = [ ] for kp_dict in json_dict [ "polygons" ] [ 0 ] [ "keypoints" ] : x = kp_dict [ "x" ] y = kp_dict [ "y" ] xx . append ( x ) yy . append ( y ) img_seg = np . zeros ( ( 643 , 960 , 1 ) , dtype = np . float32 ) rr , cc = skimage . draw . polygon ( np . array ( yy ) , np . array ( xx ) , shape = img_seg . shape ) img_seg [ rr , cc ] = 1.0 if extract is not None : bb = _quokka_normalize_extract ( extract ) img_seg = bb . extract_from_image ( img_seg ) segmap = SegmentationMapOnImage ( img_seg , shape = img_seg . shape [ 0 : 2 ] + ( 3 , ) ) if size is not None : shape_resized = _compute_resized_shape ( img_seg . shape , size ) segmap = segmap . resize ( shape_resized [ 0 : 2 ] ) segmap . shape = tuple ( shape_resized [ 0 : 2 ] ) + ( 3 , ) return segmap
9894	def _uptime_solaris ( ) : global __boottime try : kstat = ctypes . CDLL ( 'libkstat.so' ) except ( AttributeError , OSError ) : return None KSTAT_STRLEN = 31 class anon_union ( ctypes . Union ) : _fields_ = [ ( 'c' , ctypes . c_char * 16 ) , ( 'time' , ctypes . c_int ) ] class kstat_named_t ( ctypes . Structure ) : _fields_ = [ ( 'name' , ctypes . c_char * KSTAT_STRLEN ) , ( 'data_type' , ctypes . c_char ) , ( 'value' , anon_union ) ] kstat . kstat_open . restype = ctypes . c_void_p kstat . kstat_lookup . restype = ctypes . c_void_p kstat . kstat_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ] kstat . kstat_read . restype = ctypes . c_int kstat . kstat_read . argtypes = [ ctypes . c_void_p , ctypes . c_void_p , ctypes . c_void_p ] kstat . kstat_data_lookup . restype = ctypes . POINTER ( kstat_named_t ) kstat . kstat_data_lookup . argtypes = [ ctypes . c_void_p , ctypes . c_char_p ] kc = kstat . kstat_open ( ) if not kc : return None ksp = kstat . kstat_lookup ( kc , 'unix' , 0 , 'system_misc' ) if ksp and kstat . kstat_read ( kc , ksp , None ) != - 1 : data = kstat . kstat_data_lookup ( ksp , 'boot_time' ) if data : __boottime = data . contents . value . time kstat . kstat_close ( kc ) if __boottime is not None : return time . time ( ) - __boottime return None
3970	def _get_build_path ( app_spec ) : if os . path . isabs ( app_spec [ 'build' ] ) : return app_spec [ 'build' ] return os . path . join ( Repo ( app_spec [ 'repo' ] ) . local_path , app_spec [ 'build' ] )
8184	def update ( self , iterations = 10 ) : self . alpha += 0.05 self . alpha = min ( self . alpha , 1.0 ) if self . layout . i == 0 : self . layout . prepare ( ) self . layout . i += 1 elif self . layout . i == 1 : self . layout . iterate ( ) elif self . layout . i < self . layout . n : n = min ( iterations , self . layout . i / 10 + 1 ) for i in range ( n ) : self . layout . iterate ( ) min_ , max = self . layout . bounds self . x = _ctx . WIDTH - max . x * self . d - min_ . x * self . d self . y = _ctx . HEIGHT - max . y * self . d - min_ . y * self . d self . x /= 2 self . y /= 2 return not self . layout . done
8382	def textpath ( self , i ) : if len ( self . _textpaths ) == i : self . _ctx . font ( self . font , self . fontsize ) txt = self . q [ i ] if len ( self . q ) > 1 : txt += " (" + str ( i + 1 ) + "/" + str ( len ( self . q ) ) + ")" p = self . _ctx . textpath ( txt , 0 , 0 , width = self . _w ) h = self . _ctx . textheight ( txt , width = self . _w ) self . _textpaths . append ( ( p , h ) ) return self . _textpaths [ i ]
8526	def find_match ( self ) : for pattern , callback in self . rules : match = pattern . match ( self . source , pos = self . pos ) if not match : continue try : node = callback ( match ) except IgnoredMatchException : pass else : self . seen . append ( node ) return match raise NoMatchException ( 'None of the known patterns match for {}' '' . format ( self . source [ self . pos : ] ) )
6510	def _parse ( self , filename ) : self . names = { } with codecs . open ( filename , encoding = "iso8859-1" ) as f : for line in f : if any ( map ( lambda c : 128 < ord ( c ) < 160 , line ) ) : line = line . encode ( "iso8859-1" ) . decode ( "windows-1252" ) self . _eat_name_line ( line . strip ( ) )
7216	def register ( self , task_json = None , json_filename = None ) : if not task_json and not json_filename : raise Exception ( "Both task json and filename can't be none." ) if task_json and json_filename : raise Exception ( "Both task json and filename can't be provided." ) if json_filename : task_json = json . load ( open ( json_filename , 'r' ) ) r = self . gbdx_connection . post ( self . _base_url , json = task_json ) raise_for_status ( r ) return r . text
11607	def social_widget_render ( parser , token ) : bits = token . split_contents ( ) tag_name = bits [ 0 ] if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" % tag_name ) args = [ ] kwargs = { } bits = bits [ 1 : ] if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to %s tag" % tag_name ) name , value = match . groups ( ) if name : name = name . replace ( '-' , '_' ) kwargs [ name ] = parser . compile_filter ( value ) else : args . append ( parser . compile_filter ( value ) ) return SocialWidgetNode ( args , kwargs )
5851	def get_dataset_files ( self , dataset_id , glob = "." , is_dir = False , version_number = None ) : if version_number is None : latest = True else : latest = False data = { "download_request" : { "glob" : glob , "isDir" : is_dir , "latest" : latest } } failure_message = "Failed to get matched files in dataset {}" . format ( dataset_id ) versions = self . _get_success_json ( self . _post_json ( routes . matched_files ( dataset_id ) , data , failure_message = failure_message ) ) [ 'versions' ] if version_number is None : version = versions [ 0 ] else : try : version = list ( filter ( lambda v : v [ 'number' ] == version_number , versions ) ) [ 0 ] except IndexError : raise ResourceNotFoundException ( ) return list ( map ( lambda f : DatasetFile ( path = f [ 'filename' ] , url = f [ 'url' ] ) , version [ 'files' ] ) )
3564	def read_value ( self , timeout_sec = TIMEOUT_SEC ) : self . _value_read . clear ( ) self . _device . _peripheral . readValueForCharacteristic_ ( self . _characteristic ) if not self . _value_read . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting to read characteristic value!' ) return self . _characteristic . value ( )
11506	def create_item ( self , token , name , parent_id , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'name' ] = name parameters [ 'parentid' ] = parent_id optional_keys = [ 'description' , 'uuid' , 'privacy' ] for key in optional_keys : if key in kwargs : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.item.create' , parameters ) return response
4700	def get_sizeof_descriptor_table ( version = "Denali" ) : if version == "Denali" : return sizeof ( DescriptorTableDenali ) elif version == "Spec20" : return sizeof ( DescriptorTableSpec20 ) elif version == "Spec12" : return 0 else : raise RuntimeError ( "Error version!" )
1413	def _get_packing_plan_with_watch ( self , topologyName , callback , isWatching ) : path = self . get_packing_plan_path ( topologyName ) if isWatching : LOG . info ( "Adding data watch for path: " + path ) @ self . client . DataWatch ( path ) def watch_packing_plan ( data , stats ) : if data : packing_plan = PackingPlan ( ) packing_plan . ParseFromString ( data ) callback ( packing_plan ) else : callback ( None ) return isWatching
8065	def drawdaisy ( x , y , color = '#fefefe' ) : _ctx . push ( ) _fill = _ctx . fill ( ) _stroke = _ctx . stroke ( ) sc = ( 1.0 / _ctx . HEIGHT ) * float ( y * 0.5 ) * 4.0 _ctx . strokewidth ( sc * 2.0 ) _ctx . stroke ( '#3B240B' ) _ctx . line ( x + ( sin ( x * 0.1 ) * 10.0 ) , y + 80 , x + sin ( _ctx . FRAME * 0.1 ) , y ) _ctx . translate ( - 20 , 0 ) _ctx . scale ( sc ) _ctx . fill ( color ) _ctx . nostroke ( ) for angle in xrange ( 0 , 360 , 45 ) : _ctx . rotate ( degrees = 45 ) _ctx . rect ( x , y , 40 , 8 , 1 ) _ctx . fill ( '#F7FE2E' ) _ctx . ellipse ( x + 15 , y , 10 , 10 ) _ctx . fill ( _fill ) _ctx . stroke ( _stroke ) _ctx . pop ( )
10538	def create_category ( name , description ) : try : category = dict ( name = name , short_name = name . lower ( ) . replace ( " " , "" ) , description = description ) res = _pybossa_req ( 'post' , 'category' , payload = category ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
1191	def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None
11187	def create ( quiet , name , base_uri , symlink_path ) : _validate_name ( name ) admin_metadata = dtoolcore . generate_admin_metadata ( name ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) if parsed_base_uri . scheme == "symlink" : if symlink_path is None : raise click . UsageError ( "Need to specify symlink path using the -s/--symlink-path option" ) if symlink_path : base_uri = dtoolcore . utils . sanitise_uri ( "symlink:" + parsed_base_uri . path ) parsed_base_uri = dtoolcore . utils . generous_parse_uri ( base_uri ) proto_dataset = dtoolcore . generate_proto_dataset ( admin_metadata = admin_metadata , base_uri = dtoolcore . utils . urlunparse ( parsed_base_uri ) , config_path = CONFIG_PATH ) if symlink_path : symlink_abspath = os . path . abspath ( symlink_path ) proto_dataset . _storage_broker . symlink_path = symlink_abspath try : proto_dataset . create ( ) except dtoolcore . storagebroker . StorageBrokerOSError as err : raise click . UsageError ( str ( err ) ) proto_dataset . put_readme ( "" ) if quiet : click . secho ( proto_dataset . uri ) else : click . secho ( "Created proto dataset " , nl = False , fg = "green" ) click . secho ( proto_dataset . uri ) click . secho ( "Next steps: " ) step = 1 if parsed_base_uri . scheme != "symlink" : click . secho ( "{}. Add raw data, eg:" . format ( step ) ) click . secho ( " dtool add item my_file.txt {}" . format ( proto_dataset . uri ) , fg = "cyan" ) if parsed_base_uri . scheme == "file" : data_path = proto_dataset . _storage_broker . _data_abspath click . secho ( " Or use your system commands, e.g: " ) click . secho ( " mv my_data_directory {}/" . format ( data_path ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Add descriptive metadata, e.g: " . format ( step ) ) click . secho ( " dtool readme interactive {}" . format ( proto_dataset . uri ) , fg = "cyan" ) step = step + 1 click . secho ( "{}. Convert the proto dataset into a dataset: " . format ( step ) ) click . secho ( " dtool freeze {}" . format ( proto_dataset . uri ) , fg = "cyan" )
5483	def setup_service ( api_name , api_version , credentials = None ) : if not credentials : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return apiclient . discovery . build ( api_name , api_version , credentials = credentials )
9296	def get_database ( self , model ) : for router in self . routers : r = router . get_database ( model ) if r is not None : return r return self . get ( 'default' )
5754	def bootstrap_paginate ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) < 2 : raise TemplateSyntaxError ( "'%s' takes at least one argument" " (Page object reference)" % bits [ 0 ] ) page = parser . compile_filter ( bits [ 1 ] ) kwargs = { } bits = bits [ 2 : ] kwarg_re = re . compile ( r'(\w+)=(.+)' ) if len ( bits ) : for bit in bits : match = kwarg_re . match ( bit ) if not match : raise TemplateSyntaxError ( "Malformed arguments to bootstrap_pagination paginate tag" ) name , value = match . groups ( ) kwargs [ name ] = parser . compile_filter ( value ) return BootstrapPaginationNode ( page , kwargs )
11174	def settingshelp ( self , width = 0 ) : out = [ ] out . append ( self . _wrap ( self . docs [ 'title' ] , width = width ) ) if self . docs [ 'description' ] : out . append ( self . _wrap ( self . docs [ 'description' ] , indent = 2 , width = width ) ) out . append ( '' ) out . append ( 'SETTINGS:' ) out . append ( self . strsettings ( indent = 2 , width = width ) ) out . append ( '' ) return '\n' . join ( out )
2663	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : external_block_id = str ( len ( self . blocks ) ) launch_cmd = self . launch_cmd . format ( block_id = external_block_id ) internal_block = self . provider . submit ( launch_cmd , 1 , 1 ) logger . debug ( "Launched block {}->{}" . format ( external_block_id , internal_block ) ) if not internal_block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) r . extend ( [ external_block_id ] ) self . blocks [ external_block_id ] = internal_block else : logger . error ( "No execution provider available" ) r = None return r
898	def addSpatialNoise ( self , sequence , amount ) : newSequence = [ ] for pattern in sequence : if pattern is not None : pattern = self . patternMachine . addNoise ( pattern , amount ) newSequence . append ( pattern ) return newSequence
6362	def encode ( self , word , max_length = - 1 ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _uc_set ) word = word . replace ( 'LL' , 'L' ) word = word . replace ( 'R' , 'R' ) sdx = word . translate ( self . _trans ) if max_length > 0 : sdx = ( sdx + ( '0' * max_length ) ) [ : max_length ] return sdx
9859	def set_parameter ( self , key , value ) : if value is None or isinstance ( value , ( int , float , bool ) ) : value = str ( value ) if key . endswith ( '64' ) : value = urlsafe_b64encode ( value . encode ( 'utf-8' ) ) value = value . replace ( b ( '=' ) , b ( '' ) ) self . _parameters [ key ] = value
12399	def parse ( cls , s , required = False ) : req = pkg_resources . Requirement . parse ( s ) return cls ( req , required = required )
4129	def _autocov ( s , ** kwargs ) : debias = kwargs . pop ( 'debias' , True ) axis = kwargs . get ( 'axis' , - 1 ) if debias : s = _remove_bias ( s , axis ) kwargs [ 'debias' ] = False return _crosscov ( s , s , ** kwargs )
13859	def contents ( self , f , text ) : text += self . _read ( f . abs_path ) + "\r\n" return text
8420	def same_log10_order_of_magnitude ( x , delta = 0.1 ) : dmin = np . log10 ( np . min ( x ) * ( 1 - delta ) ) dmax = np . log10 ( np . max ( x ) * ( 1 + delta ) ) return np . floor ( dmin ) == np . floor ( dmax )
10720	def get_parser ( ) : parser = argparse . ArgumentParser ( ) parser . add_argument ( "package" , choices = arg_map . keys ( ) , help = "designates the package to test" ) parser . add_argument ( "--ignore" , help = "ignore these files" ) return parser
3772	def phase_select_property ( phase = None , s = None , l = None , g = None , V_over_F = None ) : r if phase == 's' : return s elif phase == 'l' : return l elif phase == 'g' : return g elif phase == 'two-phase' : return None elif phase is None : return None else : raise Exception ( 'Property not recognized' )
3111	def locked_get ( self ) : serialized = self . _dictionary . get ( self . _key ) if serialized is None : return None credentials = client . OAuth2Credentials . from_json ( serialized ) credentials . set_store ( self ) return credentials
2287	def parallel_graph_evaluation ( data , adj_matrix , nb_runs = 16 , nb_jobs = None , ** kwargs ) : nb_jobs = SETTINGS . get_default ( nb_jobs = nb_jobs ) if nb_runs == 1 : return graph_evaluation ( data , adj_matrix , ** kwargs ) else : output = Parallel ( n_jobs = nb_jobs ) ( delayed ( graph_evaluation ) ( data , adj_matrix , idx = run , gpu_id = run % SETTINGS . GPU , ** kwargs ) for run in range ( nb_runs ) ) return np . mean ( output )
5406	def _get_mount_actions ( self , mounts , mnt_datadisk ) : actions_to_add = [ ] for mount in mounts : bucket = mount . value [ len ( 'gs://' ) : ] mount_path = mount . docker_path actions_to_add . extend ( [ google_v2_pipelines . build_action ( name = 'mount-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' , 'RUN_IN_BACKGROUND' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ '--implicit-dirs' , '--foreground' , '-o ro' , bucket , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) , google_v2_pipelines . build_action ( name = 'mount-wait-{}' . format ( bucket ) , flags = [ 'ENABLE_FUSE' ] , image_uri = _GCSFUSE_IMAGE , mounts = [ mnt_datadisk ] , commands = [ 'wait' , os . path . join ( providers_util . DATA_MOUNT_POINT , mount_path ) ] ) ] ) return actions_to_add
6375	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _umlauts ) wlen = len ( word ) - 1 if wlen > 3 : if wlen > 5 : if word [ - 3 : ] == 'nen' : return word [ : - 3 ] if wlen > 4 : if word [ - 2 : ] in { 'en' , 'se' , 'es' , 'er' } : return word [ : - 2 ] if word [ - 1 ] in { 'e' , 'n' , 'r' , 's' } : return word [ : - 1 ] return word
10379	def calculate_concordance_by_annotation ( graph , annotation , key , cutoff = None ) : return { value : calculate_concordance ( subgraph , key , cutoff = cutoff ) for value , subgraph in get_subgraphs_by_annotation ( graph , annotation ) . items ( ) }
294	def plot_exposures ( returns , positions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) pos_no_cash = positions . drop ( 'cash' , axis = 1 ) l_exp = pos_no_cash [ pos_no_cash > 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) s_exp = pos_no_cash [ pos_no_cash < 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) net_exp = pos_no_cash . sum ( axis = 1 ) / positions . sum ( axis = 1 ) ax . fill_between ( l_exp . index , 0 , l_exp . values , label = 'Long' , color = 'green' , alpha = 0.5 ) ax . fill_between ( s_exp . index , 0 , s_exp . values , label = 'Short' , color = 'red' , alpha = 0.5 ) ax . plot ( net_exp . index , net_exp . values , label = 'Net' , color = 'black' , linestyle = 'dotted' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( "Exposure" ) ax . set_ylabel ( 'Exposure' ) ax . legend ( loc = 'lower left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
10789	def guess_invert ( st ) : pos = st . obj_get_positions ( ) pxinds_ar = np . round ( pos ) . astype ( 'int' ) inim = st . ishape . translate ( - st . pad ) . contains ( pxinds_ar ) pxinds_tuple = tuple ( pxinds_ar [ inim ] . T ) pxvals = st . data [ pxinds_tuple ] invert = np . median ( pxvals ) < np . median ( st . data ) return invert
11152	def sha256file ( abspath , nbytes = 0 , chunk_size = DEFAULT_CHUNK_SIZE ) : return get_file_fingerprint ( abspath , hashlib . sha256 , nbytes = nbytes , chunk_size = chunk_size )
12	def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : assert mode in ( 'two_sided' , 'causal' ) if len ( y ) < 2 * radius + 1 : return np . ones_like ( y ) * y . mean ( ) elif mode == 'two_sided' : convkernel = np . ones ( 2 * radius + 1 ) out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) if valid_only : out [ : radius ] = out [ - radius : ] = np . nan elif mode == 'causal' : convkernel = np . ones ( radius ) out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) out = out [ : - radius + 1 ] if valid_only : out [ : radius ] = np . nan return out
13716	def next_item ( self ) : queue = self . queue try : item = queue . get ( block = True , timeout = 5 ) return item except Exception : return None
9828	def write ( self , file , optstring = "" , quote = False ) : classid = str ( self . id ) if quote : classid = '"' + classid + '"' file . write ( 'object ' + classid + ' class ' + str ( self . name ) + ' ' + optstring + '\n' )
11254	def attrs ( prev , attr_names ) : for obj in prev : attr_values = [ ] for name in attr_names : if hasattr ( obj , name ) : attr_values . append ( getattr ( obj , name ) ) yield attr_values
5005	def get_enterprise_customer_for_running_pipeline ( request , pipeline ) : sso_provider_id = request . GET . get ( 'tpa_hint' ) if pipeline : sso_provider_id = Registry . get_from_pipeline ( pipeline ) . provider_id return get_enterprise_customer_for_sso ( sso_provider_id )
2602	def engine_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-engine.json' )
3884	def from_conv_part_data ( conv_part_data , self_user_id ) : user_id = UserID ( chat_id = conv_part_data . id . chat_id , gaia_id = conv_part_data . id . gaia_id ) return User ( user_id , conv_part_data . fallback_name , None , None , [ ] , ( self_user_id == user_id ) or ( self_user_id is None ) )
1937	def get_abi ( self , hsh : bytes ) -> Dict [ str , Any ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) sig = self . _function_signatures_by_selector . get ( hsh ) if sig is not None : return dict ( self . _function_abi_items_by_signature [ sig ] ) item = self . _fallback_function_abi_item if item is not None : return dict ( item ) return { 'payable' : False , 'stateMutability' : 'nonpayable' , 'type' : 'fallback' }
3008	def _credentials_from_request ( request ) : if ( oauth2_settings . storage_model is None or request . user . is_authenticated ( ) ) : return get_storage ( request ) . get ( ) else : return None
528	def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )
3839	async def set_presence ( self , set_presence_request ) : response = hangouts_pb2 . SetPresenceResponse ( ) await self . _pb_request ( 'presence/setpresence' , set_presence_request , response ) return response
6708	def get_file_hash ( fin , block_size = 2 ** 20 ) : if isinstance ( fin , six . string_types ) : fin = open ( fin ) h = hashlib . sha512 ( ) while True : data = fin . read ( block_size ) if not data : break try : h . update ( data ) except TypeError : h . update ( data . encode ( 'utf-8' ) ) return h . hexdigest ( )
5265	def backslashcase ( string ) : str1 = re . sub ( r"_" , r"\\" , snakecase ( string ) ) return str1
13116	def create_connection ( conf ) : host_config = { } host_config [ 'hosts' ] = [ conf . get ( 'jackal' , 'host' ) ] if int ( conf . get ( 'jackal' , 'use_ssl' ) ) : host_config [ 'use_ssl' ] = True if conf . get ( 'jackal' , 'ca_certs' ) : host_config [ 'ca_certs' ] = conf . get ( 'jackal' , 'ca_certs' ) if int ( conf . get ( 'jackal' , 'client_certs' ) ) : host_config [ 'client_cert' ] = conf . get ( 'jackal' , 'client_cert' ) host_config [ 'client_key' ] = conf . get ( 'jackal' , 'client_key' ) host_config [ 'ssl_assert_hostname' ] = False connections . create_connection ( ** host_config )
860	def getTemporalDelay ( inferenceElement , key = None ) : if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) return 0
2419	def write_extracted_licenses ( lics , out ) : write_value ( 'LicenseID' , lics . identifier , out ) if lics . full_name is not None : write_value ( 'LicenseName' , lics . full_name , out ) if lics . comment is not None : write_text_value ( 'LicenseComment' , lics . comment , out ) for xref in sorted ( lics . cross_ref ) : write_value ( 'LicenseCrossReference' , xref , out ) write_text_value ( 'ExtractedText' , lics . text , out )
9011	def index_of_first_consumed_mesh_in_row ( self ) : index = 0 for instruction in self . row_instructions : if instruction is self : break index += instruction . number_of_consumed_meshes else : self . _raise_not_found_error ( ) return index
4546	def fill_circle ( setter , x0 , y0 , r , color = None ) : _draw_fast_vline ( setter , x0 , y0 - r , 2 * r + 1 , color ) _fill_circle_helper ( setter , x0 , y0 , r , 3 , 0 , color )
4118	def _swapsides ( data ) : N = len ( data ) return np . concatenate ( ( data [ N // 2 + 1 : ] , data [ 0 : N // 2 ] ) )
207	def draw ( self , size = None , cmap = "jet" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = "nearest" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn
9577	def read_var_header ( fd , endian ) : mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) next_pos = fd . tell ( ) + num_bytes if mtpn == etypes [ 'miCOMPRESSED' ] [ 'n' ] : data = fd . read ( num_bytes ) dcor = zlib . decompressobj ( ) fd_var = BytesIO ( dcor . decompress ( data ) ) del data fd = fd_var if dcor . flush ( ) != b'' : raise ParseError ( 'Error in compressed data.' ) mtpn , num_bytes = unpack ( endian , 'II' , fd . read ( 8 ) ) if mtpn != etypes [ 'miMATRIX' ] [ 'n' ] : raise ParseError ( 'Expecting miMATRIX type number {}, ' 'got {}' . format ( etypes [ 'miMATRIX' ] [ 'n' ] , mtpn ) ) header = read_header ( fd , endian ) return header , next_pos , fd
719	def queryModelIDs ( self ) : jobID = self . getJobID ( ) modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) return modelIDs
10209	def check_write_permissions ( file ) : try : open ( file , 'a' ) except IOError : print ( "Can't open file {}. " "Please grant write permissions or change the path in your config" . format ( file ) ) sys . exit ( 1 )
13626	def Delimited ( value , parser = Text , delimiter = u',' , encoding = None ) : value = Text ( value , encoding ) if value is None or value == u'' : return [ ] return map ( parser , value . split ( delimiter ) )
8388	def merge_configs ( main , tweaks ) : for section in tweaks . sections ( ) : for option in tweaks . options ( section ) : value = tweaks . get ( section , option ) if option . endswith ( "+" ) : option = option [ : - 1 ] value = main . get ( section , option ) + value main . set ( section , option , value )
12281	def run ( self , cmd , * args ) : if self . manager is None : raise Exception ( "Fatal internal error: Missing repository manager" ) if cmd not in dir ( self . manager ) : raise Exception ( "Fatal internal error: Invalid command {} being run" . format ( cmd ) ) func = getattr ( self . manager , cmd ) repo = self return func ( repo , * args )
2560	def heartbeat ( self ) : heartbeat = ( HEARTBEAT_CODE ) . to_bytes ( 4 , "little" ) r = self . task_incoming . send ( heartbeat ) logger . debug ( "Return from heartbeat : {}" . format ( r ) )
11588	def _rc_brpoplpush ( self , src , dst , timeout = 0 ) : rpop = self . brpop ( src , timeout ) if rpop is not None : self . lpush ( dst , rpop [ 1 ] ) return rpop [ 1 ] return None
6145	def DSP_callback_toc ( self ) : if self . Tcapture > 0 : self . DSP_toc . append ( time . time ( ) - self . start_time )
392	def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None
13698	def wait ( self , timeout = None ) : if timeout is None : timeout = self . _timeout while self . _process . check_readable ( timeout ) : self . _flush ( )
13029	def exploit_single ( self , ip , operating_system ) : result = None if "Windows Server 2008" in operating_system or "Windows 7" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit7.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) elif "Windows Server 2012" in operating_system or "Windows 10" in operating_system or "Windows 8.1" in operating_system : result = subprocess . run ( [ 'python2' , os . path . join ( self . datadir , 'MS17-010' , 'eternalblue_exploit8.py' ) , str ( ip ) , os . path . join ( self . datadir , 'final_combined.bin' ) , "12" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : return [ "System target could not be automatically identified" ] return result . stdout . decode ( 'utf-8' ) . split ( '\n' )
8879	def fit ( self , X , y = None ) : X = check_array ( X ) self . tree = BallTree ( X , leaf_size = self . leaf_size , metric = self . metric ) dist_train = self . tree . query ( X , k = 2 ) [ 0 ] if self . threshold == 'auto' : self . threshold_value = 0.5 * sqrt ( var ( dist_train [ : , 1 ] ) ) + mean ( dist_train [ : , 1 ] ) elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) data_test = safe_indexing ( dist_train [ : , 1 ] , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) AD . append ( data_test ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
1259	def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )
12283	def lookup ( self , username = None , reponame = None , key = None ) : if key is None : key = self . key ( username , reponame ) if key not in self . repos : raise UnknownRepository ( ) return self . repos [ key ]
4198	def get_short_module_name ( module_name , obj_name ) : parts = module_name . split ( '.' ) short_name = module_name for i in range ( len ( parts ) - 1 , 0 , - 1 ) : short_name = '.' . join ( parts [ : i ] ) try : exec ( 'from %s import %s' % ( short_name , obj_name ) ) except ImportError : short_name = '.' . join ( parts [ : ( i + 1 ) ] ) break return short_name
5830	def create ( self , configuration , name , description ) : data = { "configuration" : configuration , "name" : name , "description" : description } failure_message = "Dataview creation failed" result = self . _get_success_json ( self . _post_json ( 'v1/data_views' , data , failure_message = failure_message ) ) data_view_id = result [ 'data' ] [ 'id' ] return data_view_id
13099	def getAnnotations ( self , targets , wildcard = "." , include = None , exclude = None , limit = None , start = 1 , expand = False , ** kwargs ) : return 0 , [ ]
9925	def get_queryset ( self ) : oldest = timezone . now ( ) - app_settings . PASSWORD_RESET_EXPIRATION queryset = super ( ValidPasswordResetTokenManager , self ) . get_queryset ( ) return queryset . filter ( created_at__gt = oldest )
972	def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] )
851	def rewind ( self ) : super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = "excel" ) self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) self . _recordCount = 0
266	def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset
3165	def get ( self , workflow_id , email_id , subscriber_hash ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . workflow_id = workflow_id self . email_id = email_id self . subscriber_hash = subscriber_hash return self . _mc_client . _get ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' , subscriber_hash ) )
5922	def create ( logger_name , logfile = 'gromacs.log' ) : logger = logging . getLogger ( logger_name ) logger . setLevel ( logging . DEBUG ) logfile = logging . FileHandler ( logfile ) logfile_formatter = logging . Formatter ( '%(asctime)s %(name)-12s %(levelname)-8s %(message)s' ) logfile . setFormatter ( logfile_formatter ) logger . addHandler ( logfile ) console = logging . StreamHandler ( ) console . setLevel ( logging . INFO ) formatter = logging . Formatter ( '%(name)-12s: %(levelname)-8s %(message)s' ) console . setFormatter ( formatter ) logger . addHandler ( console ) return logger
8543	def _get_password ( self , password , use_config = True , config_filename = None , use_keyring = HAS_KEYRING ) : if not password and use_config : if self . _config is None : self . _read_config ( config_filename ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if not password and use_keyring : logger = logging . getLogger ( __name__ ) question = ( "Please enter your password for {} on {}: " . format ( self . username , self . host_base ) ) if HAS_KEYRING : password = keyring . get_password ( self . keyring_identificator , self . username ) if password is None : password = getpass . getpass ( question ) try : keyring . set_password ( self . keyring_identificator , self . username , password ) except keyring . errors . PasswordSetError as error : logger . warning ( "Storing password in keyring '%s' failed: %s" , self . keyring_identificator , error ) else : logger . warning ( "Install the 'keyring' Python module to store your password " "securely in your keyring!" ) password = self . _config . get ( "credentials" , "password" , fallback = None ) if password is None : password = getpass . getpass ( question ) store_plaintext_passwords = self . _config . get ( "preferences" , "store-plaintext-passwords" , fallback = None ) if store_plaintext_passwords != "no" : question = ( "Do you want to store your password in plain text in " + self . _config_filename ( ) ) answer = ask ( question , [ "yes" , "no" , "never" ] , "no" ) if answer == "yes" : self . _config . set ( "credentials" , "password" , password ) self . _save_config ( ) elif answer == "never" : if "preferences" not in self . _config : self . _config . add_section ( "preferences" ) self . _config . set ( "preferences" , "store-plaintext-passwords" , "no" ) self . _save_config ( ) return password
5157	def _add_install ( self , context ) : contents = self . _render_template ( 'install.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/install.sh" , "contents" : contents , "mode" : "755" } )
12113	def file_supported ( cls , filename ) : if not isinstance ( filename , str ) : return False ( _ , ext ) = os . path . splitext ( filename ) if ext not in cls . extensions : return False else : return True
4928	def transform_image ( self , content_metadata_item ) : image_url = '' if content_metadata_item [ 'content_type' ] in [ 'course' , 'program' ] : image_url = content_metadata_item . get ( 'card_image_url' ) elif content_metadata_item [ 'content_type' ] == 'courserun' : image_url = content_metadata_item . get ( 'image_url' ) return image_url
6410	def lehmer_mean ( nums , exp = 2 ) : r return sum ( x ** exp for x in nums ) / sum ( x ** ( exp - 1 ) for x in nums )
11654	def fit ( self , X , y = None , ** params ) : X = as_features ( X , stack = True ) self . transformer . fit ( X . stacked_features , y , ** params ) return self
10025	def get_versions ( self ) : response = self . ebs . describe_application_versions ( application_name = self . app_name ) return response [ 'DescribeApplicationVersionsResponse' ] [ 'DescribeApplicationVersionsResult' ] [ 'ApplicationVersions' ]
2242	def split_modpath ( modpath , check = True ) : if six . PY2 : if modpath . endswith ( '.pyc' ) : modpath = modpath [ : - 1 ] modpath_ = abspath ( expanduser ( modpath ) ) if check : if not exists ( modpath_ ) : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) if isdir ( modpath_ ) and not exists ( join ( modpath , '__init__.py' ) ) : raise ValueError ( 'modpath={} is not a module' . format ( modpath ) ) full_dpath , fname_ext = split ( modpath_ ) _relmod_parts = [ fname_ext ] dpath = full_dpath while exists ( join ( dpath , '__init__.py' ) ) : dpath , dname = split ( dpath ) _relmod_parts . append ( dname ) relmod_parts = _relmod_parts [ : : - 1 ] rel_modpath = os . path . sep . join ( relmod_parts ) return dpath , rel_modpath
4032	def _randone ( d , limit = 20 , grouprefs = None ) : if grouprefs is None : grouprefs = { } ret = '' for i in d : if i [ 0 ] == sre_parse . IN : ret += choice ( _in ( i [ 1 ] ) ) elif i [ 0 ] == sre_parse . LITERAL : ret += unichr ( i [ 1 ] ) elif i [ 0 ] == sre_parse . CATEGORY : ret += choice ( CATEGORIES . get ( i [ 1 ] , [ '' ] ) ) elif i [ 0 ] == sre_parse . ANY : ret += choice ( CATEGORIES [ 'category_any' ] ) elif i [ 0 ] == sre_parse . MAX_REPEAT or i [ 0 ] == sre_parse . MIN_REPEAT : if i [ 1 ] [ 1 ] + 1 - i [ 1 ] [ 0 ] >= limit : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 0 ] + limit - 1 else : min , max = i [ 1 ] [ 0 ] , i [ 1 ] [ 1 ] for _ in range ( randint ( min , max ) ) : ret += _randone ( list ( i [ 1 ] [ 2 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . BRANCH : ret += _randone ( choice ( i [ 1 ] [ 1 ] ) , limit , grouprefs ) elif i [ 0 ] == sre_parse . SUBPATTERN or i [ 0 ] == sre_parse . ASSERT : subexpr = i [ 1 ] [ 1 ] if IS_PY36_OR_GREATER and i [ 0 ] == sre_parse . SUBPATTERN : subexpr = i [ 1 ] [ 3 ] subp = _randone ( subexpr , limit , grouprefs ) if i [ 1 ] [ 0 ] : grouprefs [ i [ 1 ] [ 0 ] ] = subp ret += subp elif i [ 0 ] == sre_parse . AT : continue elif i [ 0 ] == sre_parse . NOT_LITERAL : c = list ( CATEGORIES [ 'category_any' ] ) if unichr ( i [ 1 ] ) in c : c . remove ( unichr ( i [ 1 ] ) ) ret += choice ( c ) elif i [ 0 ] == sre_parse . GROUPREF : ret += grouprefs [ i [ 1 ] ] elif i [ 0 ] == sre_parse . ASSERT_NOT : pass else : print ( '[!] cannot handle expression "%s"' % str ( i ) ) return ret
7979	def _try_auth ( self ) : if self . authenticated : self . __logger . debug ( "try_auth: already authenticated" ) return self . __logger . debug ( "trying auth: %r" % ( self . _auth_methods_left , ) ) if not self . _auth_methods_left : raise LegacyAuthenticationError ( "No allowed authentication methods available" ) method = self . _auth_methods_left [ 0 ] if method . startswith ( "sasl:" ) : return ClientStream . _try_auth ( self ) elif method not in ( "plain" , "digest" ) : self . _auth_methods_left . pop ( 0 ) self . __logger . debug ( "Skipping unknown auth method: %s" % method ) return self . _try_auth ( ) elif self . available_auth_methods is not None : if method in self . available_auth_methods : self . _auth_methods_left . pop ( 0 ) self . auth_method_used = method if method == "digest" : self . _digest_auth_stage2 ( self . auth_stanza ) else : self . _plain_auth_stage2 ( self . auth_stanza ) self . auth_stanza = None return else : self . __logger . debug ( "Skipping unavailable auth method: %s" % method ) else : self . _auth_stage1 ( )
7120	def _convert_item ( self , obj ) : if isinstance ( obj , dict ) and not isinstance ( obj , DotDict ) : obj = DotDict ( obj ) elif isinstance ( obj , list ) : for i , item in enumerate ( obj ) : if isinstance ( item , dict ) and not isinstance ( item , DotDict ) : obj [ i ] = DotDict ( item ) return obj
8237	def right_complement ( clr ) : right = split_complementary ( clr ) [ 2 ] colors = complementary ( clr ) colors [ 3 ] . h = right . h colors [ 4 ] . h = right . h colors [ 5 ] . h = right . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 5 ] , colors [ 4 ] , colors [ 3 ] ) return colors
39	def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]
13897	def DumpDirHashToStringIO ( directory , stringio , base = '' , exclude = None , include = None ) : import fnmatch import os files = [ ( os . path . join ( directory , i ) , i ) for i in os . listdir ( directory ) ] files = [ i for i in files if os . path . isfile ( i [ 0 ] ) ] for fullname , filename in files : if include is not None : if not fnmatch . fnmatch ( fullname , include ) : continue if exclude is not None : if fnmatch . fnmatch ( fullname , exclude ) : continue md5 = Md5Hex ( fullname ) if base : stringio . write ( '%s/%s=%s\n' % ( base , filename , md5 ) ) else : stringio . write ( '%s=%s\n' % ( filename , md5 ) )
1457	def valid_java_classpath ( classpath ) : paths = classpath . split ( ':' ) for path_entry in paths : if not valid_path ( path_entry . strip ( ) ) : return False return True
3698	def Hsub ( T = 298.15 , P = 101325 , MW = None , AvailableMethods = False , Method = None , CASRN = '' ) : def list_methods ( ) : methods = [ ] if CASRN in GharagheiziHsub_data . index : methods . append ( 'Ghazerati Appendix, at 298K' ) methods . append ( 'None' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'Ghazerati Appendix, at 298K' : _Hsub = float ( GharagheiziHsub_data . at [ CASRN , 'Hsub' ] ) elif Method == 'None' or not _Hsub or not MW : return None else : raise Exception ( 'Failure in in function' ) _Hsub = property_molar_to_mass ( _Hsub , MW ) return _Hsub
34	def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )
6249	def get_effect_class ( self , effect_name : str , package_name : str = None ) -> Type [ 'Effect' ] : return self . _project . get_effect_class ( effect_name , package_name = package_name )
2089	def get ( self , pk = None , ** kwargs ) : if kwargs . pop ( 'include_debug_header' , True ) : debug . log ( 'Getting the record.' , header = 'details' ) response = self . read ( pk = pk , fail_on_no_results = True , fail_on_multiple_results = True , ** kwargs ) return response [ 'results' ] [ 0 ]
10083	def edit ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) record_pid , record = self . fetch_published ( ) assert PIDStatus . REGISTERED == record_pid . status assert record [ '_deposit' ] == self [ '_deposit' ] self . model . json = self . _prepare_edit ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
6006	def load_ccd_data_from_fits ( image_path , pixel_scale , image_hdu = 0 , resized_ccd_shape = None , resized_ccd_origin_pixels = None , resized_ccd_origin_arcsec = None , psf_path = None , psf_hdu = 0 , resized_psf_shape = None , renormalize_psf = True , noise_map_path = None , noise_map_hdu = 0 , noise_map_from_image_and_background_noise_map = False , convert_noise_map_from_weight_map = False , convert_noise_map_from_inverse_noise_map = False , background_noise_map_path = None , background_noise_map_hdu = 0 , convert_background_noise_map_from_weight_map = False , convert_background_noise_map_from_inverse_noise_map = False , poisson_noise_map_path = None , poisson_noise_map_hdu = 0 , poisson_noise_map_from_image = False , convert_poisson_noise_map_from_weight_map = False , convert_poisson_noise_map_from_inverse_noise_map = False , exposure_time_map_path = None , exposure_time_map_hdu = 0 , exposure_time_map_from_single_value = None , exposure_time_map_from_inverse_noise_map = False , background_sky_map_path = None , background_sky_map_hdu = 0 , convert_from_electrons = False , gain = None , convert_from_adus = False , lens_name = None ) : image = load_image ( image_path = image_path , image_hdu = image_hdu , pixel_scale = pixel_scale ) background_noise_map = load_background_noise_map ( background_noise_map_path = background_noise_map_path , background_noise_map_hdu = background_noise_map_hdu , pixel_scale = pixel_scale , convert_background_noise_map_from_weight_map = convert_background_noise_map_from_weight_map , convert_background_noise_map_from_inverse_noise_map = convert_background_noise_map_from_inverse_noise_map ) if background_noise_map is not None : inverse_noise_map = 1.0 / background_noise_map else : inverse_noise_map = None exposure_time_map = load_exposure_time_map ( exposure_time_map_path = exposure_time_map_path , exposure_time_map_hdu = exposure_time_map_hdu , pixel_scale = pixel_scale , shape = image . shape , exposure_time = exposure_time_map_from_single_value , exposure_time_map_from_inverse_noise_map = exposure_time_map_from_inverse_noise_map , inverse_noise_map = inverse_noise_map ) poisson_noise_map = load_poisson_noise_map ( poisson_noise_map_path = poisson_noise_map_path , poisson_noise_map_hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale , convert_poisson_noise_map_from_weight_map = convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map = convert_poisson_noise_map_from_inverse_noise_map , image = image , exposure_time_map = exposure_time_map , poisson_noise_map_from_image = poisson_noise_map_from_image , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) noise_map = load_noise_map ( noise_map_path = noise_map_path , noise_map_hdu = noise_map_hdu , pixel_scale = pixel_scale , image = image , background_noise_map = background_noise_map , exposure_time_map = exposure_time_map , convert_noise_map_from_weight_map = convert_noise_map_from_weight_map , convert_noise_map_from_inverse_noise_map = convert_noise_map_from_inverse_noise_map , noise_map_from_image_and_background_noise_map = noise_map_from_image_and_background_noise_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) psf = load_psf ( psf_path = psf_path , psf_hdu = psf_hdu , pixel_scale = pixel_scale , renormalize = renormalize_psf ) background_sky_map = load_background_sky_map ( background_sky_map_path = background_sky_map_path , background_sky_map_hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) image = CCDData ( image = image , pixel_scale = pixel_scale , psf = psf , noise_map = noise_map , background_noise_map = background_noise_map , poisson_noise_map = poisson_noise_map , exposure_time_map = exposure_time_map , background_sky_map = background_sky_map , gain = gain , name = lens_name ) if resized_ccd_shape is not None : image = image . new_ccd_data_with_resized_arrays ( new_shape = resized_ccd_shape , new_centre_pixels = resized_ccd_origin_pixels , new_centre_arcsec = resized_ccd_origin_arcsec ) if resized_psf_shape is not None : image = image . new_ccd_data_with_resized_psf ( new_shape = resized_psf_shape ) if convert_from_electrons : image = image . new_ccd_data_converted_from_electrons ( ) elif convert_from_adus : image = image . new_ccd_data_converted_from_adus ( gain = gain ) return image
8592	def restore_snapshot ( self , datacenter_id , volume_id , snapshot_id ) : data = { 'snapshotId' : snapshot_id } response = self . _perform_request ( url = '/datacenters/%s/volumes/%s/restore-snapshot' % ( datacenter_id , volume_id ) , method = 'POST-ACTION' , data = urlencode ( data ) ) return response
11283	def iter ( self , prev = None ) : if self . next : generator = self . next . iter ( self . func ( prev , * self . args , ** self . kw ) ) else : generator = self . func ( prev , * self . args , ** self . kw ) return generator
1342	def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , data_format = 'channels_last' ) : from PIL import Image images , labels = [ ] , [ ] basepath = os . path . dirname ( __file__ ) samplepath = os . path . join ( basepath , 'data' ) files = os . listdir ( samplepath ) for idx in range ( index , index + batchsize ) : i = idx % 20 file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) path = os . path . join ( samplepath , file ) image = Image . open ( path ) if dataset == 'imagenet' : image = image . resize ( shape ) image = np . asarray ( image , dtype = np . float32 ) if dataset != 'mnist' and data_format == 'channels_first' : image = np . transpose ( image , ( 2 , 0 , 1 ) ) images . append ( image ) labels . append ( label ) labels = np . array ( labels ) images = np . stack ( images ) return images , labels
3354	def append ( self , object ) : the_id = object . id self . _check ( the_id ) self . _dict [ the_id ] = len ( self ) list . append ( self , object )
4878	def validate ( self , data ) : lms_user_id = data . get ( 'lms_user_id' ) tpa_user_id = data . get ( 'tpa_user_id' ) user_email = data . get ( 'user_email' ) if not lms_user_id and not tpa_user_id and not user_email : raise serializers . ValidationError ( 'At least one of the following fields must be specified and map to an EnterpriseCustomerUser: ' 'lms_user_id, tpa_user_id, user_email' ) return data
1269	def _fly ( self , board , layers , things , the_plot ) : if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )
2867	def readU8 ( self , register ) : result = self . _bus . read_byte_data ( self . _address , register ) & 0xFF self . _logger . debug ( "Read 0x%02X from register 0x%02X" , result , register ) return result
628	def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )
10047	def check_oauth2_scope ( can_method , * myscopes ) : def check ( record , * args , ** kwargs ) : @ require_api_auth ( ) @ require_oauth_scopes ( * myscopes ) def can ( self ) : return can_method ( record ) return type ( 'CheckOAuth2Scope' , ( ) , { 'can' : can } ) ( ) return check
4360	def _receiver_loop ( self ) : while True : rawdata = self . get_server_msg ( ) if not rawdata : continue try : pkt = packet . decode ( rawdata , self . json_loads ) except ( ValueError , KeyError , Exception ) as e : self . error ( 'invalid_packet' , "There was a decoding error when dealing with packet " "with event: %s... (%s)" % ( rawdata [ : 20 ] , e ) ) continue if pkt [ 'type' ] == 'heartbeat' : continue if pkt [ 'type' ] == 'disconnect' and pkt [ 'endpoint' ] == '' : self . kill ( detach = True ) continue endpoint = pkt [ 'endpoint' ] if endpoint not in self . namespaces : self . error ( "no_such_namespace" , "The endpoint you tried to connect to " "doesn't exist: %s" % endpoint , endpoint = endpoint ) continue elif endpoint in self . active_ns : pkt_ns = self . active_ns [ endpoint ] else : new_ns_class = self . namespaces [ endpoint ] pkt_ns = new_ns_class ( self . environ , endpoint , request = self . request ) for cls in type ( pkt_ns ) . __mro__ : if hasattr ( cls , 'initialize' ) : cls . initialize ( pkt_ns ) self . active_ns [ endpoint ] = pkt_ns retval = pkt_ns . process_packet ( pkt ) if pkt . get ( 'ack' ) == "data" and pkt . get ( 'id' ) : if type ( retval ) is tuple : args = list ( retval ) else : args = [ retval ] returning_ack = dict ( type = 'ack' , ackId = pkt [ 'id' ] , args = args , endpoint = pkt . get ( 'endpoint' , '' ) ) self . send_packet ( returning_ack ) if not self . connected : self . kill ( detach = True ) return
13198	def read ( cls , root_tex_path ) : root_dir = os . path . dirname ( root_tex_path ) tex_source = read_tex_file ( root_tex_path ) tex_macros = get_macros ( tex_source ) tex_source = replace_macros ( tex_source , tex_macros ) return cls ( tex_source , root_dir = root_dir )
135	def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot reorder polygon points, because it contains no points." ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( "Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )
10070	def preserve ( method = None , result = True , fields = None ) : if method is None : return partial ( preserve , result = result , fields = fields ) fields = fields or ( '_deposit' , ) @ wraps ( method ) def wrapper ( self , * args , ** kwargs ) : data = { field : self [ field ] for field in fields if field in self } result_ = method ( self , * args , ** kwargs ) replace = result_ if result else self for field in data : replace [ field ] = data [ field ] return result_ return wrapper
1763	def pop_bytes ( self , nbytes , force = False ) : data = self . read_bytes ( self . STACK , nbytes , force = force ) self . STACK += nbytes return data
12417	def replaced_directory ( dirname ) : if dirname [ - 1 ] == '/' : dirname = dirname [ : - 1 ] full_path = os . path . abspath ( dirname ) if not os . path . isdir ( full_path ) : raise AttributeError ( 'dir_name must be a directory' ) base , name = os . path . split ( full_path ) tempdir = tempfile . mkdtemp ( ) shutil . move ( full_path , tempdir ) os . mkdir ( full_path ) try : yield tempdir finally : shutil . rmtree ( full_path ) moved = os . path . join ( tempdir , name ) shutil . move ( moved , base ) shutil . rmtree ( tempdir )
9215	def t_istringapostrophe_css_string ( self , t ) : r'[^\'@]+' t . lexer . lineno += t . value . count ( '\n' ) return t
5028	def transmit_learner_data ( self , user ) : exporter = self . get_learner_data_exporter ( user ) transmitter = self . get_learner_data_transmitter ( ) transmitter . transmit ( exporter )
8978	def _binary_file ( self , file ) : if self . __text_is_expected : file = TextWrapper ( file , self . __encoding ) self . __dump_to_file ( file )
11457	def match ( self , query = None , ** kwargs ) : from invenio . search_engine import perform_request_search if not query : recid = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = "035:%s" % ( recid , ) , of = "id" ) else : if "recid" not in kwargs : kwargs [ "recid" ] = self . record [ "001" ] [ 0 ] [ 3 ] return perform_request_search ( p = query % kwargs , of = "id" )
289	def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , ** kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , ** kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , ** kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax
9156	def stroke_linecap ( self , linecap ) : linecap = getattr ( pgmagick . LineCap , "%sCap" % linecap . title ( ) ) linecap = pgmagick . DrawableStrokeLineCap ( linecap ) self . drawer . append ( linecap )
1673	def PrintCategories ( ) : sys . stderr . write ( '' . join ( ' %s\n' % cat for cat in _ERROR_CATEGORIES ) ) sys . exit ( 0 )
1192	def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'
10580	def calculate ( self , ** state ) : super ( ) . calculate ( ** state ) return self . mm * self . P / R / state [ "T" ]
4492	def fetch ( args ) : storage , remote_path = split_storage ( args . remote ) local_path = args . local if local_path is None : _ , local_path = os . path . split ( remote_path ) local_path_exists = os . path . exists ( local_path ) if local_path_exists and not args . force and not args . update : sys . exit ( "Local file %s already exists, not overwriting." % local_path ) directory , _ = os . path . split ( local_path ) if directory : makedirs ( directory , exist_ok = True ) osf = _setup_osf ( args ) project = osf . project ( args . project ) store = project . storage ( storage ) for file_ in store . files : if norm_remote_path ( file_ . path ) == remote_path : if local_path_exists and not args . force and args . update : if file_ . hashes . get ( 'md5' ) == checksum ( local_path ) : print ( "Local file %s already matches remote." % local_path ) break with open ( local_path , 'wb' ) as fp : file_ . write_to ( fp ) break
865	def _readConfigFile ( cls , filename , path = None ) : outputProperties = dict ( ) if path is None : filePath = cls . findConfigFile ( filename ) else : filePath = os . path . join ( path , filename ) try : if filePath is not None : try : _getLoggerBase ( ) . debug ( "Loading config file: %s" , filePath ) with open ( filePath , 'r' ) as inp : contents = inp . read ( ) except Exception : raise RuntimeError ( "Expected configuration file at %s" % filePath ) else : try : contents = resource_string ( "nupic.support" , filename ) except Exception as resourceException : if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : contents = '<configuration/>' else : raise resourceException elements = ElementTree . XML ( contents ) if elements . tag != 'configuration' : raise RuntimeError ( "Expected top-level element to be 'configuration' " "but got '%s'" % ( elements . tag ) ) propertyElements = elements . findall ( './property' ) for propertyItem in propertyElements : propInfo = dict ( ) propertyAttributes = list ( propertyItem ) for propertyAttribute in propertyAttributes : propInfo [ propertyAttribute . tag ] = propertyAttribute . text name = propInfo . get ( 'name' , None ) if 'value' in propInfo and propInfo [ 'value' ] is None : value = '' else : value = propInfo . get ( 'value' , None ) if value is None : if 'novalue' in propInfo : continue else : raise RuntimeError ( "Missing 'value' element within the property " "element: => %s " % ( str ( propInfo ) ) ) restOfValue = value value = '' while True : pos = restOfValue . find ( '${env.' ) if pos == - 1 : value += restOfValue break value += restOfValue [ 0 : pos ] varTailPos = restOfValue . find ( '}' , pos ) if varTailPos == - 1 : raise RuntimeError ( "Trailing environment variable tag delimiter '}'" " not found in %r" % ( restOfValue ) ) varname = restOfValue [ pos + 6 : varTailPos ] if varname not in os . environ : raise RuntimeError ( "Attempting to use the value of the environment" " variable %r, which is not defined" % ( varname ) ) envVarValue = os . environ [ varname ] value += envVarValue restOfValue = restOfValue [ varTailPos + 1 : ] if name is None : raise RuntimeError ( "Missing 'name' element within following property " "element:\n => %s " % ( str ( propInfo ) ) ) propInfo [ 'value' ] = value outputProperties [ name ] = propInfo return outputProperties except Exception : _getLoggerBase ( ) . exception ( "Error while parsing configuration file: %s." , filePath ) raise
5677	def get_trip_trajectories_within_timespan ( self , start , end , use_shapes = True , filter_name = None ) : trips = [ ] trip_df = self . get_tripIs_active_in_range ( start , end ) print ( "gtfs_viz.py: fetched " + str ( len ( trip_df ) ) + " trip ids" ) shape_cache = { } for row in trip_df . itertuples ( ) : trip_I = row . trip_I day_start_ut = row . day_start_ut shape_id = row . shape_id trip = { } name , route_type = self . get_route_name_and_type_of_tripI ( trip_I ) trip [ 'route_type' ] = int ( route_type ) trip [ 'name' ] = str ( name ) if filter_name and ( name != filter_name ) : continue stop_lats = [ ] stop_lons = [ ] stop_dep_times = [ ] shape_breaks = [ ] stop_seqs = [ ] stop_time_df = self . get_trip_stop_time_data ( trip_I , day_start_ut ) for stop_row in stop_time_df . itertuples ( ) : stop_lats . append ( float ( stop_row . lat ) ) stop_lons . append ( float ( stop_row . lon ) ) stop_dep_times . append ( float ( stop_row . dep_time_ut ) ) try : stop_seqs . append ( int ( stop_row . seq ) ) except TypeError : stop_seqs . append ( None ) if use_shapes : try : shape_breaks . append ( int ( stop_row . shape_break ) ) except ( TypeError , ValueError ) : shape_breaks . append ( None ) if use_shapes : if shape_id not in shape_cache : shape_cache [ shape_id ] = shapes . get_shape_points2 ( self . conn . cursor ( ) , shape_id ) shape_data = shape_cache [ shape_id ] try : trip [ 'times' ] = shapes . interpolate_shape_times ( shape_data [ 'd' ] , shape_breaks , stop_dep_times ) trip [ 'lats' ] = shape_data [ 'lats' ] trip [ 'lons' ] = shape_data [ 'lons' ] start_break = shape_breaks [ 0 ] end_break = shape_breaks [ - 1 ] trip [ 'times' ] = trip [ 'times' ] [ start_break : end_break + 1 ] trip [ 'lats' ] = trip [ 'lats' ] [ start_break : end_break + 1 ] trip [ 'lons' ] = trip [ 'lons' ] [ start_break : end_break + 1 ] except : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons else : trip [ 'times' ] = stop_dep_times trip [ 'lats' ] = stop_lats trip [ 'lons' ] = stop_lons trips . append ( trip ) return { "trips" : trips }
12992	def line_chunker ( text , getreffs , lines = 30 ) : level = len ( text . citation ) source_reffs = [ reff . split ( ":" ) [ - 1 ] for reff in getreffs ( level = level ) ] reffs = [ ] i = 0 while i + lines - 1 < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ i + lines - 1 ] , source_reffs [ i ] ] ) ) i += lines if i < len ( source_reffs ) : reffs . append ( tuple ( [ source_reffs [ i ] + "-" + source_reffs [ len ( source_reffs ) - 1 ] , source_reffs [ i ] ] ) ) return reffs
2981	def cmd_daemon ( opts ) : if opts . data_dir is None : raise BlockadeError ( "You must supply a data directory for the daemon" ) rest . start ( data_dir = opts . data_dir , port = opts . port , debug = opts . debug , host_exec = get_host_exec ( ) )
3825	async def get_group_conversation_url ( self , get_group_conversation_url_request ) : response = hangouts_pb2 . GetGroupConversationUrlResponse ( ) await self . _pb_request ( 'conversations/getgroupconversationurl' , get_group_conversation_url_request , response ) return response
2161	def get_command ( self , ctx , name ) : if not hasattr ( self . resource , name ) : return None method = getattr ( self . resource , name ) attrs = getattr ( method , '_cli_command_attrs' , { } ) help_text = inspect . getdoc ( method ) attrs [ 'help' ] = self . _auto_help_text ( help_text or '' ) ignore_defaults = attrs . pop ( 'ignore_defaults' , False ) new_method = self . _echo_method ( method ) click_params = getattr ( method , '__click_params__' , [ ] ) new_method . __click_params__ = copy ( click_params ) new_method = with_global_options ( new_method ) fao = attrs . pop ( 'use_fields_as_options' , True ) if fao : for field in reversed ( self . resource . fields ) : if not field . is_option : continue if not isinstance ( fao , bool ) and field . name not in fao : continue args = [ field . option ] if field . key : args . insert ( 0 , field . key ) short_fields = { 'name' : 'n' , 'description' : 'd' , 'inventory' : 'i' , 'extra_vars' : 'e' } if field . name in short_fields : args . append ( '-' + short_fields [ field . name ] ) option_help = field . help if isinstance ( field . type , StructuredInput ) : option_help += ' Use @ to get JSON or YAML from a file.' if field . required : option_help = '[REQUIRED] ' + option_help elif field . read_only : option_help = '[READ ONLY] ' + option_help option_help = '[FIELD]' + option_help click . option ( * args , default = field . default if not ignore_defaults else None , help = option_help , type = field . type , show_default = field . show_default , multiple = field . multiple , is_eager = False ) ( new_method ) cmd = click . command ( name = name , cls = ActionSubcommand , ** attrs ) ( new_method ) code = six . get_function_code ( method ) if 'pk' in code . co_varnames : click . argument ( 'pk' , nargs = 1 , required = False , type = str , metavar = '[ID]' ) ( cmd ) return cmd
679	def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record
5099	def _matrix2dict ( matrix , etype = False ) : n = len ( matrix ) adj = { k : { } for k in range ( n ) } for k in range ( n ) : for j in range ( n ) : if matrix [ k , j ] != 0 : adj [ k ] [ j ] = { } if not etype else matrix [ k , j ] return adj
13604	def system ( self , cmd , fake_code = False ) : try : if self . options . dry_run : def fake_system ( cmd ) : self . print_message ( cmd ) return fake_code return fake_system ( cmd ) except AttributeError : self . logger . warnning ( "fake mode enabled," "but you don't set '--dry-run' option " "in your argparser options" ) pass return os . system ( cmd )
2071	def col_transform ( self , col , digits ) : if col is None or float ( col ) < 0.0 : return None else : col = self . number_to_base ( int ( col ) , self . base , digits ) if len ( col ) == digits : return col else : return [ 0 for _ in range ( digits - len ( col ) ) ] + col
11176	def parse ( self , argv ) : if len ( argv ) < self . nargs : raise BadNumberOfArguments ( self . nargs , len ( argv ) ) if self . nargs == 1 : return self . parse_argument ( argv . pop ( 0 ) ) return [ self . parse_argument ( argv . pop ( 0 ) ) for tmp in range ( self . nargs ) ]
1043	def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node
6962	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , bytes ) : return obj . decode ( ) elif isinstance ( obj , complex ) : return ( obj . real , obj . imag ) elif ( isinstance ( obj , ( float , np . float64 , np . float_ ) ) and not np . isfinite ( obj ) ) : return None elif isinstance ( obj , ( np . int8 , np . int16 , np . int32 , np . int64 ) ) : return int ( obj ) else : return json . JSONEncoder . default ( self , obj )
10574	def get_local_playlists ( filepaths , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local playlists..." ) included_playlists = [ ] excluded_playlists = [ ] supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_PLAYLIST_FORMATS , max_depth = max_depth ) included_playlists , excluded_playlists = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) logger . info ( "Excluded {0} local playlists" . format ( len ( excluded_playlists ) ) ) logger . info ( "Loaded {0} local playlists" . format ( len ( included_playlists ) ) ) return included_playlists , excluded_playlists
531	def getOutputNames ( self ) : outputs = self . getSpec ( ) . outputs return [ outputs . getByIndex ( i ) [ 0 ] for i in xrange ( outputs . getCount ( ) ) ]
9313	def sign_sha256 ( key , msg ) : if isinstance ( msg , text_type ) : msg = msg . encode ( 'utf-8' ) return hmac . new ( key , msg , hashlib . sha256 ) . digest ( )
4215	def name ( cls ) : parent , sep , mod_name = cls . __module__ . rpartition ( '.' ) mod_name = mod_name . replace ( '_' , ' ' ) return ' ' . join ( [ mod_name , cls . __name__ ] )
817	def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : total = 0 for i in pos : total += counts [ i ] total = float ( total ) for i in pos : x [ i ] = counts [ i ] / total else : x [ pos ] = 1 return x
5266	def sentencecase ( string ) : joiner = ' ' string = re . sub ( r"[\-_\.\s]" , joiner , str ( string ) ) if not string : return string return capitalcase ( trimcase ( re . sub ( r"[A-Z]" , lambda matched : joiner + lowercase ( matched . group ( 0 ) ) , string ) ) )
7800	def encode ( self ) : if self . data is None : return "" elif not self . data : return "=" else : ret = standard_b64encode ( self . data ) return ret . decode ( "us-ascii" )
4409	async def connect ( self , channel_id : int ) : ws = self . _lavalink . bot . _connection . _get_websocket ( int ( self . guild_id ) ) await ws . voice_state ( self . guild_id , str ( channel_id ) )
3792	def solve_T ( self , P , V , quick = True ) : r self . Tc = sum ( self . Tcs ) / self . N return super ( type ( self ) . __mro__ [ - 3 ] , self ) . solve_T ( P = P , V = V , quick = quick )
1188	def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''
4456	def limit ( self , offset , num ) : limit = Limit ( offset , num ) if self . _groups : self . _groups [ - 1 ] . limit = limit else : self . _limit = limit return self
7303	def set_mongonaut_base ( self ) : if hasattr ( self , "app_label" ) : return None self . app_label = self . kwargs . get ( 'app_label' ) self . document_name = self . kwargs . get ( 'document_name' ) self . models_name = self . kwargs . get ( 'models_name' , 'models' ) self . model_name = "{0}.{1}" . format ( self . app_label , self . models_name ) self . models = import_module ( self . model_name )
5367	def compact_interval_string ( value_list ) : if not value_list : return '' value_list . sort ( ) interval_list = [ ] curr = [ ] for val in value_list : if curr and ( val > curr [ - 1 ] + 1 ) : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) curr = [ val ] else : curr . append ( val ) if curr : interval_list . append ( ( curr [ 0 ] , curr [ - 1 ] ) ) return ',' . join ( [ '{}-{}' . format ( pair [ 0 ] , pair [ 1 ] ) if pair [ 0 ] != pair [ 1 ] else str ( pair [ 0 ] ) for pair in interval_list ] )
8781	def create_locks ( context , network_ids , addresses ) : for address in addresses : address_model = None try : address_model = _find_or_create_address ( context , network_ids , address ) lock_holder = None if address_model . lock_id : lock_holder = db_api . lock_holder_find ( context , lock_id = address_model . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if not lock_holder : LOG . info ( "Creating lock holder on IPAddress %s with id %s" , address_model . address_readable , address_model . id ) db_api . lock_holder_create ( context , address_model , name = LOCK_NAME , type = "ip_address" ) except Exception : LOG . exception ( "Failed to create lock holder on IPAddress %s" , address_model ) continue context . session . flush ( )
1381	def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : if not tmaster or not tmaster . host or not tmaster . stats_port : return exception_request = tmaster_pb2 . ExceptionLogRequest ( ) exception_request . component_name = component_name if len ( instances ) > 0 : exception_request . instances . extend ( instances ) request_str = exception_request . SerializeToString ( ) port = str ( tmaster . stats_port ) host = tmaster . host url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) Log . debug ( "Creating request object." ) request = tornado . httpclient . HTTPRequest ( url , body = request_str , method = 'POST' , request_timeout = 5 ) Log . debug ( 'Making HTTP call to fetch exceptionsummary url: %s' , url ) try : client = tornado . httpclient . AsyncHTTPClient ( ) result = yield client . fetch ( request ) Log . debug ( "HTTP call complete." ) except tornado . httpclient . HTTPError as e : raise Exception ( str ( e ) ) responseCode = result . code if responseCode >= 400 : message = "Error in getting exceptions from Tmaster, code: " + responseCode Log . error ( message ) raise tornado . gen . Return ( { "message" : message } ) exception_response = tmaster_pb2 . ExceptionLogResponse ( ) exception_response . ParseFromString ( result . body ) if exception_response . status . status == common_pb2 . NOTOK : if exception_response . status . HasField ( "message" ) : raise tornado . gen . Return ( { "message" : exception_response . status . message } ) ret = [ ] for exception_log in exception_response . exceptions : ret . append ( { 'class_name' : exception_log . stacktrace , 'lasttime' : exception_log . lasttime , 'firsttime' : exception_log . firsttime , 'count' : str ( exception_log . count ) } ) raise tornado . gen . Return ( ret )
9765	def check ( file , version , definition ) : file = file or 'polyaxonfile.yaml' specification = check_polyaxonfile ( file ) . specification if version : Printer . decorate_format_value ( 'The version is: {}' , specification . version , 'yellow' ) if definition : job_condition = ( specification . is_job or specification . is_build or specification . is_notebook or specification . is_tensorboard ) if specification . is_experiment : Printer . decorate_format_value ( 'This polyaxon specification has {}' , 'One experiment' , 'yellow' ) if job_condition : Printer . decorate_format_value ( 'This {} polyaxon specification is valid' , specification . kind , 'yellow' ) if specification . is_group : experiments_def = specification . experiments_def click . echo ( 'This polyaxon specification has experiment group with the following definition:' ) get_group_experiments_info ( ** experiments_def ) return specification
62	def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height
5810	def _parse_hello_extensions ( data ) : if data == b'' : return extentions_length = int_from_bytes ( data [ 0 : 2 ] ) extensions_start = 2 extensions_end = 2 + extentions_length pointer = extensions_start while pointer < extensions_end : extension_type = int_from_bytes ( data [ pointer : pointer + 2 ] ) extension_length = int_from_bytes ( data [ pointer + 2 : pointer + 4 ] ) yield ( extension_type , data [ pointer + 4 : pointer + 4 + extension_length ] ) pointer += 4 + extension_length
10217	def prerender ( graph : BELGraph ) -> Mapping [ str , Mapping [ str , Any ] ] : import bio2bel_hgnc from bio2bel_hgnc . models import HumanGene graph : BELGraph = graph . copy ( ) enrich_protein_and_rna_origins ( graph ) collapse_all_variants ( graph ) genes : Set [ Gene ] = get_nodes_by_function ( graph , GENE ) hgnc_symbols = { gene . name for gene in genes if gene . namespace . lower ( ) == 'hgnc' } result = { } hgnc_manager = bio2bel_hgnc . Manager ( ) human_genes = ( hgnc_manager . session . query ( HumanGene . symbol , HumanGene . location ) . filter ( HumanGene . symbol . in_ ( hgnc_symbols ) ) . all ( ) ) for human_gene in human_genes : result [ human_gene . symbol ] = { 'name' : human_gene . symbol , 'chr' : ( human_gene . location . split ( 'q' ) [ 0 ] if 'q' in human_gene . location else human_gene . location . split ( 'p' ) [ 0 ] ) , } df = get_df ( ) for _ , ( gene_id , symbol , start , stop ) in df [ df [ 'Symbol' ] . isin ( hgnc_symbols ) ] . iterrows ( ) : result [ symbol ] [ 'start' ] = start result [ symbol ] [ 'stop' ] = stop return result
13327	def remove ( name_or_path ) : click . echo ( ) try : r = cpenv . resolve ( name_or_path ) except cpenv . ResolveError as e : click . echo ( e ) return obj = r . resolved [ 0 ] if not isinstance ( obj , cpenv . VirtualEnvironment ) : click . echo ( '{} is a module. Use `cpenv module remove` instead.' ) return click . echo ( format_objects ( [ obj ] ) ) click . echo ( ) user_confirmed = click . confirm ( red ( 'Are you sure you want to remove this environment?' ) ) if user_confirmed : click . echo ( 'Attempting to remove...' , nl = False ) try : obj . remove ( ) except Exception as e : click . echo ( bold_red ( 'FAIL' ) ) click . echo ( e ) else : click . echo ( bold_green ( 'OK!' ) )
13279	def child_begin_handler ( self , scache , * args ) : pdesc = self . pdesc depth = scache . depth sib_seq = self . sib_seq sibs_len = self . sibs_len pdesc_level = scache . pdesc_level desc = copy . deepcopy ( pdesc ) desc = reset_parent_desc_template ( desc ) desc [ 'depth' ] = depth desc [ 'parent_breadth_path' ] = copy . deepcopy ( desc [ 'breadth_path' ] ) desc [ 'sib_seq' ] = sib_seq desc [ 'parent_path' ] = copy . deepcopy ( desc [ 'path' ] ) desc [ 'path' ] . append ( sib_seq ) update_desc_lsib_path ( desc ) update_desc_rsib_path ( desc , sibs_len ) if ( depth == 1 ) : pass else : update_desc_lcin_path ( desc , pdesc_level ) update_desc_rcin_path ( desc , sibs_len , pdesc_level ) return ( desc )
11177	def parsestr ( self , argstr ) : argv = shlex . split ( argstr , comments = True ) if len ( argv ) != 1 : raise BadNumberOfArguments ( 1 , len ( argv ) ) arg = argv [ 0 ] lower = arg . lower ( ) if lower in self . true : return True if lower in self . false : return False raise BadArgument ( arg , "Allowed values are " + self . allowed + '.' )
12146	def analyzeSingle ( abfFname ) : assert os . path . exists ( abfFname ) and abfFname . endswith ( ".abf" ) ABFfolder , ABFfname = os . path . split ( abfFname ) abfID = os . path . splitext ( ABFfname ) [ 0 ] IN = INDEX ( ABFfolder ) IN . analyzeABF ( abfID ) IN . scan ( ) IN . html_single_basic ( [ abfID ] , overwrite = True ) IN . html_single_plot ( [ abfID ] , overwrite = True ) IN . scan ( ) IN . html_index ( ) return
398	def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )
9666	def write_dot_file ( G , filename ) : with io . open ( filename , "w" ) as fh : fh . write ( "strict digraph DependencyDiagram {\n" ) edge_list = G . edges ( ) node_list = set ( G . nodes ( ) ) if edge_list : for edge in sorted ( edge_list ) : source , targ = edge node_list = node_list - set ( source ) node_list = node_list - set ( targ ) line = '"{}" -> "{}";\n' fh . write ( line . format ( source , targ ) ) if node_list : for node in sorted ( node_list ) : line = '"{}"\n' . format ( node ) fh . write ( line ) fh . write ( "}" )
13793	def handle_add_fun ( self , function_name ) : function_name = function_name . strip ( ) try : function = get_function ( function_name ) except Exception , exc : self . wfile . write ( js_error ( exc ) + NEWLINE ) return if not getattr ( function , 'view_decorated' , None ) : self . functions [ function_name ] = ( self . function_counter , function ) else : self . functions [ function_name ] = ( self . function_counter , function ( self . log ) ) self . function_counter += 1 return True
10238	def count_citations ( graph : BELGraph , ** annotations ) -> Counter : citations = defaultdict ( set ) annotation_dict_filter = build_edge_data_filter ( annotations ) for u , v , _ , d in filter_edges ( graph , annotation_dict_filter ) : if CITATION not in d : continue citations [ u , v ] . add ( ( d [ CITATION ] [ CITATION_TYPE ] , d [ CITATION ] [ CITATION_REFERENCE ] . strip ( ) ) ) return Counter ( itt . chain . from_iterable ( citations . values ( ) ) )
13784	def get_tm_session ( session_factory , transaction_manager ) : dbsession = session_factory ( ) zope . sqlalchemy . register ( dbsession , transaction_manager = transaction_manager ) return dbsession
3163	def create ( self , workflow_id , email_id , data ) : self . workflow_id = workflow_id self . email_id = email_id if 'email_address' not in data : raise KeyError ( 'The automation email queue must have an email_address' ) check_email ( data [ 'email_address' ] ) response = self . _mc_client . _post ( url = self . _build_path ( workflow_id , 'emails' , email_id , 'queue' ) , data = data ) if response is not None : self . subscriber_hash = response [ 'id' ] else : self . subscriber_hash = None return response
1477	def _get_instance_plans ( self , packing_plan , container_id ) : this_container_plan = None for container_plan in packing_plan . container_plans : if container_plan . id == container_id : this_container_plan = container_plan if this_container_plan is None : return None return this_container_plan . instance_plans
12927	def _parse_allele_data ( self ) : return [ Allele ( sequence = x ) for x in [ self . ref_allele ] + self . alt_alleles ]
12070	def tryLoadingFrom ( tryPath , moduleName = 'swhlab' ) : if not 'site-packages' in swhlab . __file__ : print ( "loaded custom swhlab module from" , os . path . dirname ( swhlab . __file__ ) ) return while len ( tryPath ) > 5 : sp = tryPath + "/swhlab/" if os . path . isdir ( sp ) and os . path . exists ( sp + "/__init__.py" ) : if not os . path . dirname ( tryPath ) in sys . path : sys . path . insert ( 0 , os . path . dirname ( tryPath ) ) print ( "#" * 80 ) print ( "# WARNING: using site-packages swhlab module" ) print ( "#" * 80 ) tryPath = os . path . dirname ( tryPath ) return
7957	def _continue_tls_handshake ( self ) : try : logger . debug ( " do_handshake()" ) self . _socket . do_handshake ( ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_READ : self . _tls_state = "want_read" logger . debug ( " want_read" ) self . _state_cond . notify ( ) return elif err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : self . _tls_state = "want_write" logger . debug ( " want_write" ) self . _write_queue . appendleft ( TLSHandshake ) return else : raise self . _tls_state = "connected" self . _set_state ( "connected" ) self . _auth_properties [ 'security-layer' ] = "TLS" if "tls-unique" in CHANNEL_BINDING_TYPES : try : tls_unique = self . _socket . get_channel_binding ( "tls-unique" ) except ValueError : pass else : self . _auth_properties [ 'channel-binding' ] = { "tls-unique" : tls_unique } try : cipher = self . _socket . cipher ( ) except AttributeError : cipher = "unknown" cert = get_certificate_from_ssl_socket ( self . _socket ) self . event ( TLSConnectedEvent ( cipher , cert ) )
4779	def is_in ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : for i in items : if self . val == i : return self self . _err ( 'Expected <%s> to be in %s, but was not.' % ( self . val , self . _fmt_items ( items ) ) )
9948	def new_space_from_excel ( self , book , range_ , sheet = None , name = None , names_row = None , param_cols = None , space_param_order = None , cells_param_order = None , transpose = False , names_col = None , param_rows = None , ) : space = self . _impl . new_space_from_excel ( book , range_ , sheet , name , names_row , param_cols , space_param_order , cells_param_order , transpose , names_col , param_rows , ) return get_interfaces ( space )
9066	def delta ( self ) : v = float ( self . _logistic . value ) if v > 0.0 : v = 1 / ( 1 + exp ( - v ) ) else : v = exp ( v ) v = v / ( v + 1.0 ) return min ( max ( v , epsilon . tiny ) , 1 - epsilon . tiny )
4895	def get_enterprise_user_id ( self , obj ) : enterprise_learner = EnterpriseCustomerUser . objects . filter ( user_id = obj . id ) . first ( ) return enterprise_learner and enterprise_learner . id
5979	def edge_pixels_from_mask ( mask ) : edge_pixel_total = total_edge_pixels_from_mask ( mask ) edge_pixels = np . zeros ( edge_pixel_total ) edge_index = 0 regular_index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : if mask [ y + 1 , x ] or mask [ y - 1 , x ] or mask [ y , x + 1 ] or mask [ y , x - 1 ] or mask [ y + 1 , x + 1 ] or mask [ y + 1 , x - 1 ] or mask [ y - 1 , x + 1 ] or mask [ y - 1 , x - 1 ] : edge_pixels [ edge_index ] = regular_index edge_index += 1 regular_index += 1 return edge_pixels
4077	def run_3to2 ( args = None ) : args = BASE_ARGS_3TO2 if args is None else BASE_ARGS_3TO2 + args try : proc = subprocess . Popen ( [ '3to2' ] + args , stderr = subprocess . PIPE ) except OSError : for path in glob . glob ( '*.egg' ) : if os . path . isdir ( path ) and path not in sys . path : sys . path . append ( path ) try : from lib3to2 . main import main as lib3to2_main except ImportError : raise OSError ( '3to2 script is unavailable.' ) else : if lib3to2_main ( 'lib3to2.fixes' , args ) : raise Exception ( 'lib3to2 parsing error' ) else : num_errors = 0 while proc . poll ( ) is None : line = proc . stderr . readline ( ) sys . stderr . write ( line ) num_errors += line . count ( ': ParseError: ' ) if proc . returncode or num_errors : raise Exception ( 'lib3to2 parsing error' )
2498	def handle_package_has_file ( self , package , package_node ) : file_nodes = map ( self . handle_package_has_file_helper , package . files ) triples = [ ( package_node , self . spdx_namespace . hasFile , node ) for node in file_nodes ] for triple in triples : self . graph . add ( triple )
629	def _bitForCoordinate ( cls , coordinate , n ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getUInt32 ( n )
1816	def SETNO ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) )
8649	def delete_milestone_request ( session , milestone_request_id ) : params_data = { 'action' : 'delete' , } endpoint = 'milestone_requests/{}' . format ( milestone_request_id ) response = make_put_request ( session , endpoint , params_data = params_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise MilestoneRequestNotDeletedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9063	def unfix ( self , param ) : if param == "delta" : self . _unfix ( "logistic" ) else : self . _fix [ param ] = False
8945	def url_as_file ( url , ext = None ) : if ext : ext = '.' + ext . strip ( '.' ) url_hint = 'www-{}-' . format ( urlparse ( url ) . hostname or 'any' ) if url . startswith ( 'file://' ) : url = os . path . abspath ( url [ len ( 'file://' ) : ] ) if os . path . isabs ( url ) : with open ( url , 'rb' ) as handle : content = handle . read ( ) else : content = requests . get ( url ) . content with tempfile . NamedTemporaryFile ( suffix = ext or '' , prefix = url_hint , delete = False ) as handle : handle . write ( content ) try : yield handle . name finally : if os . path . exists ( handle . name ) : os . remove ( handle . name )
12139	def from_pattern ( cls , pattern , filetype = None , key = 'filename' , root = None , ignore = [ ] ) : filepattern = FilePattern ( key , pattern , root = root ) if FileInfo . filetype and filetype is None : filetype = FileInfo . filetype elif filetype is None : raise Exception ( "The filetype argument must be supplied unless " "an appropriate default has been specified as " "FileInfo.filetype" ) return FileInfo ( filepattern , key , filetype , ignore = ignore )
13403	def acceptedUser ( self , logType ) : from urllib2 import urlopen , URLError , HTTPError import json isApproved = False userName = str ( self . logui . userName . text ( ) ) if userName == "" : return False if logType == "MCC" : networkFault = False data = [ ] log_url = "https://mccelog.slac.stanford.edu/elog/dev/mgibbs/dev_json_user_list.php/?username=" + userName try : data = urlopen ( log_url , None , 5 ) . read ( ) data = json . loads ( data ) except URLError as error : print ( "URLError: " + str ( error . reason ) ) networkFault = True except HTTPError as error : print ( "HTTPError: " + str ( error . reason ) ) networkFault = True if networkFault : msgBox = QMessageBox ( ) msgBox . setText ( "Cannot connect to MCC Log Server!" ) msgBox . setInformativeText ( "Use entered User name anyway?" ) msgBox . setStandardButtons ( QMessageBox . Ok | QMessageBox . Cancel ) msgBox . setDefaultButton ( QMessageBox . Ok ) if msgBox . exec_ ( ) == QMessageBox . Ok : isApproved = True if data != [ ] and ( data is not None ) : isApproved = True else : isApproved = True return isApproved
3251	def save ( self , obj , content_type = "application/xml" ) : rest_url = obj . href data = obj . message ( ) headers = { "Content-type" : content_type , "Accept" : content_type } logger . debug ( "{} {}" . format ( obj . save_method , obj . href ) ) resp = self . http_request ( rest_url , method = obj . save_method . lower ( ) , data = data , headers = headers ) if resp . status_code not in ( 200 , 201 ) : raise FailedRequestError ( 'Failed to save to Geoserver catalog: {}, {}' . format ( resp . status_code , resp . text ) ) self . _cache . clear ( ) return resp
9550	def ivalidate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , context = None , report_unexpected_exceptions = True ) : unique_sets = self . _init_unique_sets ( ) for i , r in enumerate ( data ) : if expect_header_row and i == ignore_lines : for p in self . _apply_header_checks ( i , r , summarize , context ) : yield p elif i >= ignore_lines : skip = False for p in self . _apply_skips ( i , r , summarize , report_unexpected_exceptions , context ) : if p is True : skip = True else : yield p if not skip : for p in self . _apply_each_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_value_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_length_checks ( i , r , summarize , context ) : yield p for p in self . _apply_value_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_checks ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_record_predicates ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_unique_checks ( i , r , unique_sets , summarize ) : yield p for p in self . _apply_check_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_assert_methods ( i , r , summarize , report_unexpected_exceptions , context ) : yield p for p in self . _apply_finally_assert_methods ( summarize , report_unexpected_exceptions , context ) : yield p
2990	def cross_origin ( * args , ** kwargs ) : _options = kwargs def decorator ( f ) : LOG . debug ( "Enabling %s for cross_origin using options:%s" , f , _options ) if _options . get ( 'automatic_options' , True ) : f . required_methods = getattr ( f , 'required_methods' , set ( ) ) f . required_methods . add ( 'OPTIONS' ) f . provide_automatic_options = False def wrapped_function ( * args , ** kwargs ) : options = get_cors_options ( current_app , _options ) if options . get ( 'automatic_options' ) and request . method == 'OPTIONS' : resp = current_app . make_default_options_response ( ) else : resp = make_response ( f ( * args , ** kwargs ) ) set_cors_headers ( resp , options ) setattr ( resp , FLASK_CORS_EVALUATED , True ) return resp return update_wrapper ( wrapped_function , f ) return decorator
9793	def _matches_patterns ( path , patterns ) : for glob in patterns : try : if PurePath ( path ) . match ( glob ) : return True except TypeError : pass return False
249	def get_txn_vol ( transactions ) : txn_norm = transactions . copy ( ) txn_norm . index = txn_norm . index . normalize ( ) amounts = txn_norm . amount . abs ( ) prices = txn_norm . price values = amounts * prices daily_amounts = amounts . groupby ( amounts . index ) . sum ( ) daily_values = values . groupby ( values . index ) . sum ( ) daily_amounts . name = "txn_shares" daily_values . name = "txn_volume" return pd . concat ( [ daily_values , daily_amounts ] , axis = 1 )
8028	def groupify ( function ) : @ wraps ( function ) def wrapper ( paths , * args , ** kwargs ) : groups = { } for path in paths : key = function ( path , * args , ** kwargs ) if key is not None : groups . setdefault ( key , set ( ) ) . add ( path ) return groups return wrapper
2437	def add_review_date ( self , doc , reviewed ) : if len ( doc . reviews ) != 0 : if not self . review_date_set : self . review_date_set = True date = utils . datetime_from_iso_format ( reviewed ) if date is not None : doc . reviews [ - 1 ] . review_date = date return True else : raise SPDXValueError ( 'Review::ReviewDate' ) else : raise CardinalityError ( 'Review::ReviewDate' ) else : raise OrderError ( 'Review::ReviewDate' )
7110	def fit ( self , X , y ) : trainer = pycrfsuite . Trainer ( verbose = True ) for xseq , yseq in zip ( X , y ) : trainer . append ( xseq , yseq ) trainer . set_params ( self . params ) if self . filename : filename = self . filename else : filename = 'model.tmp' trainer . train ( filename ) tagger = pycrfsuite . Tagger ( ) tagger . open ( filename ) self . estimator = tagger
11777	def replicated_dataset ( dataset , weights , n = None ) : "Copy dataset, replicating each example in proportion to its weight." n = n or len ( dataset . examples ) result = copy . copy ( dataset ) result . examples = weighted_replicate ( dataset . examples , weights , n ) return result
12014	def calc_centroids ( self ) : self . cm = np . zeros ( ( len ( self . postcard ) , 2 ) ) for i in range ( len ( self . postcard ) ) : target = self . postcard [ i ] target [ self . targets != 1 ] = 0.0 self . cm [ i ] = center_of_mass ( target )
11220	def compare ( self , jwt : 'Jwt' , compare_dates : bool = False ) -> bool : if self . secret != jwt . secret : return False if self . payload != jwt . payload : return False if self . alg != jwt . alg : return False if self . header != jwt . header : return False expected_claims = self . registered_claims actual_claims = jwt . registered_claims if not compare_dates : strip = [ 'exp' , 'nbf' , 'iat' ] expected_claims = { k : { v if k not in strip else None } for k , v in expected_claims . items ( ) } actual_claims = { k : { v if k not in strip else None } for k , v in actual_claims . items ( ) } if expected_claims != actual_claims : return False return True
7001	def parallel_pf ( lclist , outdir , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , pfmethods = ( 'gls' , 'pdm' , 'mav' , 'win' ) , pfkwargs = ( { } , { } , { } , { } ) , sigclip = 10.0 , getblssnr = False , nperiodworkers = NCPUS , ncontrolworkers = 1 , liststartindex = None , listmaxobjects = None , minobservations = 500 , excludeprocessed = True ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if ( liststartindex is not None ) and ( listmaxobjects is None ) : lclist = lclist [ liststartindex : ] elif ( liststartindex is None ) and ( listmaxobjects is not None ) : lclist = lclist [ : listmaxobjects ] elif ( liststartindex is not None ) and ( listmaxobjects is not None ) : lclist = lclist [ liststartindex : liststartindex + listmaxobjects ] tasklist = [ ( x , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nperiodworkers , minobservations , excludeprocessed ) for x in lclist ] with ProcessPoolExecutor ( max_workers = ncontrolworkers ) as executor : resultfutures = executor . map ( _runpf_worker , tasklist ) results = [ x for x in resultfutures ] return results
12938	def setDefaultRedisConnectionParams ( connectionParams ) : global _defaultRedisConnectionParams _defaultRedisConnectionParams . clear ( ) for key , value in connectionParams . items ( ) : _defaultRedisConnectionParams [ key ] = value clearRedisPools ( )
11631	def _readNamelist ( currentlyIncluding , cache , namFilename , unique_glyphs ) : filename = os . path . abspath ( os . path . normcase ( namFilename ) ) if filename in currentlyIncluding : raise NamelistRecursionError ( filename ) currentlyIncluding . add ( filename ) try : result = __readNamelist ( cache , filename , unique_glyphs ) finally : currentlyIncluding . remove ( filename ) return result
9708	def get_sanitizer ( self ) : sanitizer = self . sanitizer if not sanitizer : default_sanitizer = settings . CONFIG . get ( self . SANITIZER_KEY ) field_settings = getattr ( self , 'field_settings' , None ) if isinstance ( field_settings , six . string_types ) : profiles = settings . CONFIG . get ( self . SANITIZER_PROFILES_KEY , { } ) sanitizer = profiles . get ( field_settings , default_sanitizer ) else : sanitizer = default_sanitizer if isinstance ( sanitizer , six . string_types ) : sanitizer = import_string ( sanitizer ) return sanitizer or noop
6650	def exec_helper ( self , cmd , builddir ) : try : child = subprocess . Popen ( cmd , cwd = builddir ) child . wait ( ) except OSError as e : if e . errno == errno . ENOENT : if cmd [ 0 ] == 'cmake' : return 'CMake is not installed, please follow the installation instructions at http://docs.yottabuild.org/#installing' else : return '%s is not installed' % ( cmd [ 0 ] ) else : return 'command %s failed' % ( cmd ) if child . returncode : return 'command %s failed' % ( cmd )
5304	def sanitize_color_palette ( colorpalette ) : new_palette = { } def __make_valid_color_name ( name ) : if len ( name ) == 1 : name = name [ 0 ] return name [ : 1 ] . lower ( ) + name [ 1 : ] return name [ 0 ] . lower ( ) + '' . join ( word . capitalize ( ) for word in name [ 1 : ] ) for key , value in colorpalette . items ( ) : if isinstance ( value , str ) : value = utils . hex_to_rgb ( value ) new_palette [ __make_valid_color_name ( key . split ( ) ) ] = value return new_palette
7122	def seeded_auth_token ( client , service , seed ) : hash_func = hashlib . md5 ( ) token = ',' . join ( ( client , service , seed ) ) . encode ( 'utf-8' ) hash_func . update ( token ) return hash_func . hexdigest ( )
2586	def _command_server ( self , kill_event ) : logger . debug ( "[COMMAND] Command Server Starting" ) while not kill_event . is_set ( ) : try : command_req = self . command_channel . recv_pyobj ( ) logger . debug ( "[COMMAND] Received command request: {}" . format ( command_req ) ) if command_req == "OUTSTANDING_C" : outstanding = self . pending_task_queue . qsize ( ) for manager in self . _ready_manager_queue : outstanding += len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) reply = outstanding elif command_req == "WORKERS" : num_workers = 0 for manager in self . _ready_manager_queue : num_workers += self . _ready_manager_queue [ manager ] [ 'worker_count' ] reply = num_workers elif command_req == "MANAGERS" : reply = [ ] for manager in self . _ready_manager_queue : resp = { 'manager' : manager . decode ( 'utf-8' ) , 'block_id' : self . _ready_manager_queue [ manager ] [ 'block_id' ] , 'worker_count' : self . _ready_manager_queue [ manager ] [ 'worker_count' ] , 'tasks' : len ( self . _ready_manager_queue [ manager ] [ 'tasks' ] ) , 'active' : self . _ready_manager_queue [ manager ] [ 'active' ] } reply . append ( resp ) elif command_req . startswith ( "HOLD_WORKER" ) : cmd , s_manager = command_req . split ( ';' ) manager = s_manager . encode ( 'utf-8' ) logger . info ( "[CMD] Received HOLD_WORKER for {}" . format ( manager ) ) if manager in self . _ready_manager_queue : self . _ready_manager_queue [ manager ] [ 'active' ] = False reply = True else : reply = False elif command_req == "SHUTDOWN" : logger . info ( "[CMD] Received SHUTDOWN command" ) kill_event . set ( ) reply = True else : reply = None logger . debug ( "[COMMAND] Reply: {}" . format ( reply ) ) self . command_channel . send_pyobj ( reply ) except zmq . Again : logger . debug ( "[COMMAND] is alive" ) continue
469	def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : tl . logging . info ( "Creating vocabulary." ) counter = Counter ( ) for c in sentences : counter . update ( c ) tl . logging . info ( " Total words: %d" % len ( counter ) ) word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) word_counts = [ ( "<PAD>" , 0 ) ] + word_counts tl . logging . info ( " Words in vocabulary: %d" % len ( word_counts ) ) with tf . gfile . FastGFile ( word_counts_output_file , "w" ) as f : f . write ( "\n" . join ( [ "%s %d" % ( w , c ) for w , c in word_counts ] ) ) tl . logging . info ( " Wrote vocabulary file: %s" % word_counts_output_file ) reverse_vocab = [ x [ 0 ] for x in word_counts ] unk_id = len ( reverse_vocab ) vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) vocab = SimpleVocabulary ( vocab_dict , unk_id ) return vocab
9018	def new_pattern ( self , id_ , name , rows = None ) : if rows is None : rows = self . new_row_collection ( ) return self . _spec . new_pattern ( id_ , name , rows , self )
12867	def cleanup ( self , app ) : if hasattr ( self . database . obj , 'close_all' ) : self . database . close_all ( )
4394	def adsSyncReadReqEx2 ( port , address , index_group , index_offset , data_type , return_ctypes = False ) : sync_read_request = _adsDLL . AdsSyncReadReqEx2 ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) index_group_c = ctypes . c_ulong ( index_group ) index_offset_c = ctypes . c_ulong ( index_offset ) if data_type == PLCTYPE_STRING : data = ( STRING_BUFFER * PLCTYPE_STRING ) ( ) else : data = data_type ( ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . c_ulong ( ctypes . sizeof ( data ) ) bytes_read = ctypes . c_ulong ( ) bytes_read_pointer = ctypes . pointer ( bytes_read ) error_code = sync_read_request ( port , ams_address_pointer , index_group_c , index_offset_c , data_length , data_pointer , bytes_read_pointer , ) if error_code : raise ADSError ( error_code ) if data_type != PLCTYPE_STRING and bytes_read . value != data_length . value : raise RuntimeError ( "Insufficient data (expected {0} bytes, {1} were read)." . format ( data_length . value , bytes_read . value ) ) if return_ctypes : return data if data_type == PLCTYPE_STRING : return data . value . decode ( "utf-8" ) if type ( data_type ) . __name__ == "PyCArrayType" : return [ i for i in data ] if hasattr ( data , "value" ) : return data . value return data
6079	def intensities_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . intensities_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
13616	def scaffold ( ) : click . echo ( "A whole new site? Awesome." ) title = click . prompt ( "What's the title?" ) url = click . prompt ( "Great. What's url? http://" ) click . echo ( "Got it. Creating %s..." % url )
4088	def asyncSlot ( * args ) : def outer_decorator ( fn ) : @ Slot ( * args ) @ functools . wraps ( fn ) def wrapper ( * args , ** kwargs ) : asyncio . ensure_future ( fn ( * args , ** kwargs ) ) return wrapper return outer_decorator
3464	def reverse_id ( self ) : return '_' . join ( ( self . id , 'reverse' , hashlib . md5 ( self . id . encode ( 'utf-8' ) ) . hexdigest ( ) [ 0 : 5 ] ) )
4723	def trun_setup ( conf ) : declr = None try : with open ( conf [ "TESTPLAN_FPATH" ] ) as declr_fd : declr = yaml . safe_load ( declr_fd ) except AttributeError as exc : cij . err ( "rnr: %r" % exc ) if not declr : return None trun = copy . deepcopy ( TRUN ) trun [ "ver" ] = cij . VERSION trun [ "conf" ] = copy . deepcopy ( conf ) trun [ "res_root" ] = conf [ "OUTPUT" ] trun [ "aux_root" ] = os . sep . join ( [ trun [ "res_root" ] , "_aux" ] ) trun [ "evars" ] . update ( copy . deepcopy ( declr . get ( "evars" , { } ) ) ) os . makedirs ( trun [ "aux_root" ] ) hook_names = declr . get ( "hooks" , [ ] ) if "lock" not in hook_names : hook_names = [ "lock" ] + hook_names if hook_names [ 0 ] != "lock" : return None trun [ "hooks" ] = hooks_setup ( trun , trun , hook_names ) for enum , declr in enumerate ( declr [ "testsuites" ] ) : tsuite = tsuite_setup ( trun , declr , enum ) if tsuite is None : cij . err ( "main::FAILED: setting up tsuite: %r" % tsuite ) return 1 trun [ "testsuites" ] . append ( tsuite ) trun [ "progress" ] [ "UNKN" ] += len ( tsuite [ "testcases" ] ) return trun
4955	def get_actor ( self , username , email ) : return Agent ( name = username , mbox = 'mailto:{email}' . format ( email = email ) , )
5063	def get_course_track_selection_url ( course_run , query_parameters ) : try : course_root = reverse ( 'course_modes_choose' , kwargs = { 'course_id' : course_run [ 'key' ] } ) except KeyError : LOGGER . exception ( "KeyError while parsing course run data.\nCourse Run: \n[%s]" , course_run , ) raise url = '{}{}' . format ( settings . LMS_ROOT_URL , course_root ) course_run_url = update_query_parameters ( url , query_parameters ) return course_run_url
4471	def _transform ( self , jam , state ) : if not hasattr ( jam . sandbox , 'muda' ) : raise RuntimeError ( 'No muda state found in jams sandbox.' ) jam_w = copy . deepcopy ( jam ) jam_w . sandbox . muda [ 'history' ] . append ( { 'transformer' : self . __serialize__ , 'state' : state } ) if hasattr ( self , 'audio' ) : self . audio ( jam_w . sandbox . muda , state ) if hasattr ( self , 'metadata' ) : self . metadata ( jam_w . file_metadata , state ) for query , function_name in six . iteritems ( self . dispatch ) : function = getattr ( self , function_name ) for matched_annotation in jam_w . search ( namespace = query ) : function ( matched_annotation , state ) return jam_w
13881	def MoveDirectory ( source_dir , target_dir ) : if not IsDir ( source_dir ) : from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( source_dir ) if Exists ( target_dir ) : from . _exceptions import DirectoryAlreadyExistsError raise DirectoryAlreadyExistsError ( target_dir ) from six . moves . urllib . parse import urlparse source_url = urlparse ( source_dir ) target_url = urlparse ( target_dir ) if _UrlIsLocal ( source_url ) and _UrlIsLocal ( target_url ) : import shutil shutil . move ( source_dir , target_dir ) elif source_url . scheme == 'ftp' and target_url . scheme == 'ftp' : from . _exceptions import NotImplementedProtocol raise NotImplementedProtocol ( target_url . scheme ) else : raise NotImplementedError ( 'Can only move directories local->local or ftp->ftp' )
12234	def pref ( preference , field = None , verbose_name = None , help_text = '' , static = True , readonly = False ) : try : bound = bind_proxy ( ( preference , ) , field = field , verbose_name = verbose_name , help_text = help_text , static = static , readonly = readonly , ) return bound [ 0 ] except IndexError : return
4079	def get_version ( ) : version = _get_attrib ( ) . get ( 'version' ) if not version : match = re . search ( r"LanguageTool-?.*?(\S+)$" , get_directory ( ) ) if match : version = match . group ( 1 ) return version
4604	def history ( self , first = 0 , last = 0 , limit = - 1 , only_ops = [ ] , exclude_ops = [ ] ) : _limit = 100 cnt = 0 if first < 0 : first = 0 while True : txs = self . blockchain . rpc . get_account_history ( self [ "id" ] , "1.11.{}" . format ( last ) , _limit , "1.11.{}" . format ( first - 1 ) , api = "history" , ) for i in txs : if ( exclude_ops and self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in exclude_ops ) : continue if ( not only_ops or self . operations . getOperationNameForId ( i [ "op" ] [ 0 ] ) in only_ops ) : cnt += 1 yield i if limit >= 0 and cnt >= limit : return if not txs : log . info ( "No more history returned from API node" ) break if len ( txs ) < _limit : log . info ( "Less than {} have been returned." . format ( _limit ) ) break first = int ( txs [ - 1 ] [ "id" ] . split ( "." ) [ 2 ] )
5934	def to_int64 ( a ) : def promote_i4 ( typestr ) : if typestr [ 1 : ] == 'i4' : typestr = typestr [ 0 ] + 'i8' return typestr dtype = [ ( name , promote_i4 ( typestr ) ) for name , typestr in a . dtype . descr ] return a . astype ( dtype )
2485	def to_special_value ( self , value ) : if isinstance ( value , utils . NoAssert ) : return self . spdx_namespace . noassertion elif isinstance ( value , utils . SPDXNone ) : return self . spdx_namespace . none else : return Literal ( value )
5680	def get_trip_counts_per_day ( self ) : query = "SELECT date, count(*) AS number_of_trips FROM day_trips GROUP BY date" trip_counts_per_day = pd . read_sql_query ( query , self . conn , index_col = "date" ) max_day = trip_counts_per_day . index . max ( ) min_day = trip_counts_per_day . index . min ( ) min_date = datetime . datetime . strptime ( min_day , '%Y-%m-%d' ) max_date = datetime . datetime . strptime ( max_day , '%Y-%m-%d' ) num_days = ( max_date - min_date ) . days dates = [ min_date + datetime . timedelta ( days = x ) for x in range ( num_days + 1 ) ] trip_counts = [ ] date_strings = [ ] for date in dates : date_string = date . strftime ( "%Y-%m-%d" ) date_strings . append ( date_string ) try : value = trip_counts_per_day . loc [ date_string , 'number_of_trips' ] except KeyError : value = 0 trip_counts . append ( value ) for date_string in trip_counts_per_day . index : assert date_string in date_strings data = { "date" : dates , "date_str" : date_strings , "trip_counts" : trip_counts } return pd . DataFrame ( data )
1336	def name ( self ) : names = ( criterion . name ( ) for criterion in self . _criteria ) return '__' . join ( sorted ( names ) )
4307	def _validate_file_formats ( input_filepath_list , combine_type ) : _validate_sample_rates ( input_filepath_list , combine_type ) if combine_type == 'concatenate' : _validate_num_channels ( input_filepath_list , combine_type )
13679	def register_json ( self , data ) : j = json . loads ( data ) self . last_data_timestamp = datetime . datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) try : for v in j : self . data [ v [ self . id_key ] ] = { } self . data [ v [ self . id_key ] ] [ self . id_key ] = v [ self . id_key ] self . data [ v [ self . id_key ] ] [ self . value_key ] = v [ self . value_key ] if self . unit_key in v : self . data [ v [ self . id_key ] ] [ self . unit_key ] = v [ self . unit_key ] if self . threshold_key in v : self . data [ v [ self . id_key ] ] [ self . threshold_key ] = v [ self . threshold_key ] for k in self . other_keys : if k in v : self . data [ v [ self . id_key ] ] [ k ] = v [ k ] if self . sensor_time_key in v : self . data [ v [ self . sensor_time_key ] ] [ self . sensor_time_key ] = v [ self . sensor_time_key ] self . data [ v [ self . id_key ] ] [ self . time_key ] = self . last_data_timestamp except KeyError as e : print ( "The main key was not found on the serial input line: " + str ( e ) ) except ValueError as e : print ( "No valid JSON string received. Waiting for the next turn." ) print ( "The error was: " + str ( e ) )
9083	def get_by_uri ( self , uri ) : if not is_uri ( uri ) : raise ValueError ( '%s is not a valid URI.' % uri ) csuris = [ csuri for csuri in self . concept_scheme_uri_map . keys ( ) if uri . startswith ( csuri ) ] for csuri in csuris : c = self . get_provider ( csuri ) . get_by_uri ( uri ) if c : return c for p in self . providers . values ( ) : c = p . get_by_uri ( uri ) if c : return c return False
10960	def scramble_positions ( p , delete_frac = 0.1 ) : probs = [ 1 - delete_frac , delete_frac ] m = np . random . choice ( [ True , False ] , p . shape [ 0 ] , p = probs ) jumble = np . random . randn ( m . sum ( ) , 3 ) return p [ m ] + jumble
5595	def to_dict ( self ) : return dict ( grid = self . grid . to_dict ( ) , metatiling = self . metatiling , tile_size = self . tile_size , pixelbuffer = self . pixelbuffer )
688	def saveRecords ( self , path = 'myOutput' ) : numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) import csv with open ( path + '.csv' , 'wb' ) as f : writer = csv . writer ( f ) writer . writerow ( self . getAllFieldNames ( ) ) writer . writerow ( self . getAllDataTypes ( ) ) writer . writerow ( self . getAllFlags ( ) ) writer . writerows ( self . getAllRecords ( ) ) if self . verbosity > 0 : print '******' , numRecords , 'records exported in numenta format to file:' , path , '******\n'
3522	def _hashable_bytes ( data ) : if isinstance ( data , bytes ) : return data elif isinstance ( data , str ) : return data . encode ( 'ascii' ) else : raise TypeError ( data )
13467	def set_Courant_Snyder ( self , beta , alpha , emit = None , emit_n = None ) : self . _store_emit ( emit = emit , emit_n = emit_n ) self . _sx = _np . sqrt ( beta * self . emit ) self . _sxp = _np . sqrt ( ( 1 + alpha ** 2 ) / beta * self . emit ) self . _sxxp = - alpha * self . emit
10302	def set_percentage ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) if not a : return 0.0 return len ( a & b ) / len ( a )
8336	def findPreviousSiblings ( self , name = None , attrs = { } , text = None , limit = None , ** kwargs ) : return self . _findAll ( name , attrs , text , limit , self . previousSiblingGenerator , ** kwargs )
477	def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) try : multi_bleu_path , _ = urllib . request . urlretrieve ( "https://raw.githubusercontent.com/moses-smt/mosesdecoder/" "master/scripts/generic/multi-bleu.perl" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : tl . logging . info ( "Unable to fetch multi-bleu.perl script, using local." ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( "\n" . join ( hypotheses ) . encode ( "utf-8" ) ) hypothesis_file . write ( b"\n" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( "\n" . join ( references ) . encode ( "utf-8" ) ) reference_file . write ( b"\n" ) reference_file . flush ( ) with open ( hypothesis_file . name , "r" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ "-lc" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( "utf-8" ) bleu_score = re . search ( r"BLEU = (.+?)," , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( "multi-bleu.perl script returned non-zero exit code" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )
13151	def log_update ( entity , update ) : p = { 'on' : entity , 'update' : update } _log ( TYPE_CODES . UPDATE , p )
4767	def is_not_same_as ( self , other ) : if self . val is other : self . _err ( 'Expected <%s> to be not identical to <%s>, but was.' % ( self . val , other ) ) return self
8943	def search_file_upwards ( name , base = None ) : base = base or os . getcwd ( ) while base != os . path . dirname ( base ) : if os . path . exists ( os . path . join ( base , name ) ) : return base base = os . path . dirname ( base ) return None
4630	def _derive_y_from_x ( self , x , is_even ) : curve = ecdsa . SECP256k1 . curve a , b , p = curve . a ( ) , curve . b ( ) , curve . p ( ) alpha = ( pow ( x , 3 , p ) + a * x + b ) % p beta = ecdsa . numbertheory . square_root_mod_prime ( alpha , p ) if ( beta % 2 ) == is_even : beta = p - beta return beta
12999	def hr_diagram_figure ( cluster ) : temps , lums = round_teff_luminosity ( cluster ) x , y = temps , lums colors , color_mapper = hr_diagram_color_helper ( temps ) x_range = [ max ( x ) + max ( x ) * 0.05 , min ( x ) - min ( x ) * 0.05 ] source = ColumnDataSource ( data = dict ( x = x , y = y , color = colors ) ) pf = figure ( y_axis_type = 'log' , x_range = x_range , name = 'hr' , tools = 'box_select,lasso_select,reset,hover' , title = 'H-R Diagram for {0}' . format ( cluster . name ) ) pf . select ( BoxSelectTool ) . select_every_mousemove = False pf . select ( LassoSelectTool ) . select_every_mousemove = False hover = pf . select ( HoverTool ) [ 0 ] hover . tooltips = [ ( "Temperature (Kelvin)" , "@x{0}" ) , ( "Luminosity (solar units)" , "@y{0.00}" ) ] _diagram ( source = source , plot_figure = pf , name = 'hr' , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) return pf
4435	async def get_tracks ( self , query ) : log . debug ( 'Requesting tracks for query {}' . format ( query ) ) async with self . http . get ( self . rest_uri + quote ( query ) , headers = { 'Authorization' : self . password } ) as res : return await res . json ( content_type = None )
10697	def color_run ( start_color , end_color , step_count , inclusive = True , to_color = True ) : if isinstance ( start_color , Color ) : start_color = start_color . rgb if isinstance ( end_color , Color ) : end_color = end_color . rgb step = tuple ( ( end_color [ i ] - start_color [ i ] ) / step_count for i in range ( 3 ) ) add = lambda x , y : tuple ( sum ( z ) for z in zip ( x , y ) ) mult = lambda x , y : tuple ( y * z for z in x ) run = [ add ( start_color , mult ( step , i ) ) for i in range ( 1 , step_count ) ] if inclusive : run = [ start_color ] + run + [ end_color ] return run if not to_color else [ Color ( c ) for c in run ]
9548	def add_unique_check ( self , key , code = UNIQUE_CHECK_FAILED , message = MESSAGES [ UNIQUE_CHECK_FAILED ] ) : if isinstance ( key , basestring ) : assert key in self . _field_names , 'unexpected field name: %s' % key else : for f in key : assert f in self . _field_names , 'unexpected field name: %s' % key t = key , code , message self . _unique_checks . append ( t )
5694	def create_table ( self , conn ) : cur = conn . cursor ( ) if self . tabledef is None : return if not self . tabledef . startswith ( 'CREATE' ) : cur . execute ( 'CREATE TABLE IF NOT EXISTS %s %s' % ( self . table , self . tabledef ) ) else : cur . execute ( self . tabledef ) conn . commit ( )
1925	def get_group ( name : str ) -> _Group : global _groups if name in _groups : return _groups [ name ] group = _Group ( name ) _groups [ name ] = group return group
9365	def email_address ( user = None ) : if not user : user = user_name ( ) else : user = user . strip ( ) . replace ( ' ' , '_' ) . lower ( ) return user + '@' + domain_name ( )
4690	def encode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Checksum " raw = bytes ( message , "utf8" ) checksum = hashlib . sha256 ( raw ) . digest ( ) raw = checksum [ 0 : 4 ] + raw " Padding " raw = _pad ( raw , 16 ) " Encryption " return hexlify ( aes . encrypt ( raw ) ) . decode ( "ascii" )
3551	def list_descriptors ( self ) : paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , 'Descriptors' ) return map ( BluezGattDescriptor , get_provider ( ) . _get_objects_by_path ( paths ) )
6387	def _sb_r2 ( self , term , r1_prefixes = None ) : r1_start = self . _sb_r1 ( term , r1_prefixes ) return r1_start + self . _sb_r1 ( term [ r1_start : ] )
11060	def stop ( self ) : if self . webserver is not None : self . webserver . stop ( ) if not self . test_mode : self . plugins . save_state ( )
8424	def husl_palette ( n_colors = 6 , h = .01 , s = .9 , l = .65 ) : hues = np . linspace ( 0 , 1 , n_colors + 1 ) [ : - 1 ] hues += h hues %= 1 hues *= 359 s *= 99 l *= 99 palette = [ husl . husl_to_rgb ( h_i , s , l ) for h_i in hues ] return palette
5061	def get_enterprise_customer_for_user ( auth_user ) : EnterpriseCustomerUser = apps . get_model ( 'enterprise' , 'EnterpriseCustomerUser' ) try : return EnterpriseCustomerUser . objects . get ( user_id = auth_user . id ) . enterprise_customer except EnterpriseCustomerUser . DoesNotExist : return None
6685	def update ( kernel = False ) : manager = MANAGER cmds = { 'yum -y --color=never' : { False : '--exclude=kernel* update' , True : 'update' } } cmd = cmds [ manager ] [ kernel ] run_as_root ( "%(manager)s %(cmd)s" % locals ( ) )
5151	def merge_config ( template , config , list_identifiers = None ) : result = template . copy ( ) for key , value in config . items ( ) : if isinstance ( value , dict ) : node = result . get ( key , OrderedDict ( ) ) result [ key ] = merge_config ( node , value ) elif isinstance ( value , list ) and isinstance ( result . get ( key ) , list ) : result [ key ] = merge_list ( result [ key ] , value , list_identifiers ) else : result [ key ] = value return result
7624	def pattern_to_mireval ( ann ) : patterns = defaultdict ( lambda : defaultdict ( list ) ) for time , observation in zip ( * ann . to_event_values ( ) ) : pattern_id = observation [ 'pattern_id' ] occurrence_id = observation [ 'occurrence_id' ] obs = ( time , observation [ 'midi_pitch' ] ) patterns [ pattern_id ] [ occurrence_id ] . append ( obs ) return [ list ( _ . values ( ) ) for _ in six . itervalues ( patterns ) ]
9527	def to_boulderio ( infile , outfile ) : seq_reader = sequences . file_reader ( infile ) f_out = utils . open_file_write ( outfile ) for sequence in seq_reader : print ( "SEQUENCE_ID=" + sequence . id , file = f_out ) print ( "SEQUENCE_TEMPLATE=" + sequence . seq , file = f_out ) print ( "=" , file = f_out ) utils . close ( f_out )
11890	def set_brightness ( self , brightness ) : command = "C {},,,,{},\r\n" . format ( self . _zid , brightness ) response = self . _hub . send_command ( command ) _LOGGER . debug ( "Set brightness %s: %s" , repr ( command ) , response ) return response
3962	def prep_for_start_local_env ( pull_repos ) : if pull_repos : update_managed_repos ( force = True ) assembled_spec = spec_assembler . get_assembled_specs ( ) if not assembled_spec [ constants . CONFIG_BUNDLES_KEY ] : raise RuntimeError ( 'No bundles are activated. Use `dusty bundles` to activate bundles before running `dusty up`.' ) virtualbox . initialize_docker_vm ( )
5366	def replace_print ( fileobj = sys . stderr ) : printer = _Printer ( fileobj ) previous_stdout = sys . stdout sys . stdout = printer try : yield printer finally : sys . stdout = previous_stdout
2643	def filepath ( self ) : if hasattr ( self , 'local_path' ) : return self . local_path if self . scheme in [ 'ftp' , 'http' , 'https' , 'globus' ] : return self . filename elif self . scheme in [ 'file' ] : return self . path else : raise Exception ( 'Cannot return filepath for unknown scheme {}' . format ( self . scheme ) )
12546	def apply_mask ( img , mask ) : from . mask import apply_mask vol , _ = apply_mask ( img , mask ) return vector_to_volume ( vol , read_img ( mask ) . get_data ( ) . astype ( bool ) )
1926	def save ( f ) : global _groups c = { } for group_name , group in _groups . items ( ) : section = { var . name : var . value for var in group . updated_vars ( ) } if not section : continue c [ group_name ] = section yaml . safe_dump ( c , f , line_break = True )
7862	def handle_tls_connected_event ( self , event ) : if self . settings [ "tls_verify_peer" ] : valid = self . settings [ "tls_verify_callback" ] ( event . stream , event . peer_certificate ) if not valid : raise SSLError ( "Certificate verification failed" ) event . stream . tls_established = True with event . stream . lock : event . stream . _restart_stream ( )
7806	def verify_jid_against_srv_name ( self , jid , srv_type ) : srv_prefix = u"_" + srv_type + u"." srv_prefix_l = len ( srv_prefix ) for srv in self . alt_names . get ( "SRVName" , [ ] ) : logger . debug ( "checking {0!r} against {1!r}" . format ( jid , srv ) ) if not srv . startswith ( srv_prefix ) : logger . debug ( "{0!r} does not start with {1!r}" . format ( srv , srv_prefix ) ) continue try : srv_jid = JID ( srv [ srv_prefix_l : ] ) except ValueError : continue if srv_jid == jid : logger . debug ( "Match!" ) return True return False
9514	def is_complete_orf ( self ) : if len ( self ) % 3 != 0 or len ( self ) < 6 : return False orfs = self . orfs ( ) complete_orf = intervals . Interval ( 0 , len ( self ) - 1 ) for orf in orfs : if orf == complete_orf : return True return False
8976	def file ( self , file = None ) : if file is None : file = StringIO ( ) self . _file ( file ) return file
4180	def _coeff4 ( N , a0 , a1 , a2 , a3 ) : if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) N1 = N - 1. w = a0 - a1 * cos ( 2. * pi * n / N1 ) + a2 * cos ( 4. * pi * n / N1 ) - a3 * cos ( 6. * pi * n / N1 ) return w
11572	def output_entire_buffer ( self ) : green = 0 red = 0 for row in range ( 0 , 8 ) : for col in range ( 0 , 8 ) : if self . display_buffer [ row ] [ col ] == self . LED_GREEN : green |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_RED : red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_YELLOW : green |= 1 << col red |= 1 << col elif self . display_buffer [ row ] [ col ] == self . LED_OFF : green &= ~ ( 1 << col ) red &= ~ ( 1 << col ) self . firmata . i2c_write ( 0x70 , row * 2 , 0 , green ) self . firmata . i2c_write ( 0x70 , row * 2 + 1 , 0 , red )
8632	def search_projects ( session , query , search_filter = None , project_details = None , user_details = None , limit = 10 , offset = 0 , active_only = None ) : search_data = { 'query' : query , 'limit' : limit , 'offset' : offset , } if search_filter : search_data . update ( search_filter ) if project_details : search_data . update ( project_details ) if user_details : search_data . update ( user_details ) endpoint = 'projects/{}' . format ( 'active' if active_only else 'all' ) response = make_get_request ( session , endpoint , params_data = search_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8046	def parse_definitions ( self , class_ , all = False ) : while self . current is not None : self . log . debug ( "parsing definition list, current token is %r (%s)" , self . current . kind , self . current . value , ) self . log . debug ( "got_newline: %s" , self . stream . got_logical_newline ) if all and self . current . value == "__all__" : self . parse_all ( ) elif ( self . current . kind == tk . OP and self . current . value == "@" and self . stream . got_logical_newline ) : self . consume ( tk . OP ) self . parse_decorators ( ) elif self . current . value in [ "def" , "class" ] : yield self . parse_definition ( class_ . _nest ( self . current . value ) ) elif self . current . kind == tk . INDENT : self . consume ( tk . INDENT ) for definition in self . parse_definitions ( class_ ) : yield definition elif self . current . kind == tk . DEDENT : self . consume ( tk . DEDENT ) return elif self . current . value == "from" : self . parse_from_import_statement ( ) else : self . stream . move ( )
10794	def create_comparison_state ( image , position , radius = 5.0 , snr = 20 , method = 'constrained-cubic' , extrapad = 2 , zscale = 1.0 ) : image = common . pad ( image , extrapad , 0 ) s = init . create_single_particle_state ( imsize = np . array ( image . shape ) , sigma = 1.0 / snr , radius = radius , psfargs = { 'params' : np . array ( [ 2.0 , 1.0 , 3.0 ] ) , 'error' : 1e-6 , 'threads' : 2 } , objargs = { 'method' : method } , stateargs = { 'sigmapad' : False , 'pad' : 4 , 'zscale' : zscale } ) s . obj . pos [ 0 ] = position + s . pad + extrapad s . reset ( ) s . model_to_true_image ( ) timage = 1 - np . pad ( image , s . pad , mode = 'constant' , constant_values = 0 ) timage = s . psf . execute ( timage ) return s , timage [ s . inner ]
8669	def unlock_key ( key_name , stash , passphrase , backend ) : stash = _get_stash ( backend , stash , passphrase ) try : click . echo ( 'Unlocking key...' ) stash . unlock ( key_name = key_name ) click . echo ( 'Key unlocked successfully' ) except GhostError as ex : sys . exit ( ex )
12716	def position_rates ( self ) : return [ self . ode_obj . getPositionRate ( i ) for i in range ( self . LDOF ) ]
11593	def _rc_msetnx ( self , mapping ) : for k in iterkeys ( mapping ) : if self . exists ( k ) : return False return self . _rc_mset ( mapping )
10848	def noformat ( self ) : try : formats = { } for h in self . get_handlers ( ) : formats [ h ] = h . formatter self . set_formatter ( formatter = 'quiet' ) yield except Exception as e : raise finally : for k , v in iteritems ( formats ) : k . formatter = v
1616	def Search ( pattern , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . search ( s )
10913	def find_particles_in_tile ( positions , tile ) : bools = tile . contains ( positions ) return np . arange ( bools . size ) [ bools ]
11751	def get_blueprint_routes ( app , base_path ) : routes = [ ] for child in app . url_map . iter_rules ( ) : if child . rule . startswith ( base_path ) : relative_path = child . rule [ len ( base_path ) : ] routes . append ( { 'path' : relative_path , 'endpoint' : child . endpoint , 'methods' : list ( child . methods ) } ) return routes
13263	def get_parameters ( self ) : if self . plugin_class is None : sig = inspect . signature ( self . func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if not parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . KEYWORD_ONLY , parameter . POSITIONAL_OR_KEYWORD ] : raise RuntimeError ( "Task {} contains an unsupported {} parameter" . format ( parameter , parameter . kind ) ) yield parameter else : var_keyword_seen = set ( ) for cls in inspect . getmro ( self . plugin_class ) : if issubclass ( cls , BasePlugin ) and hasattr ( cls , self . func . __name__ ) : func = getattr ( cls , self . func . __name__ ) logger . debug ( "Found method %s from class %s" , func , cls ) var_keyword_found = False sig = inspect . signature ( func ) for index , parameter in enumerate ( sig . parameters . values ( ) ) : if index == 0 : continue if parameter . kind == inspect . Parameter . VAR_KEYWORD : var_keyword_found = True continue if parameter . kind in [ parameter . POSITIONAL_ONLY , parameter . VAR_POSITIONAL ] : raise RuntimeError ( "Task {} contains an unsupported parameter \"{}\"" . format ( func , parameter ) ) if not parameter . name in var_keyword_seen : var_keyword_seen . add ( parameter . name ) logger . debug ( "Found parameter %s (%s)" , parameter , parameter . kind ) yield parameter if not var_keyword_found : break
5636	def mod2md ( module , title , title_api_section , toc = True , maxdepth = 0 ) : docstr = module . __doc__ text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 api_md = [ ] api_sec = [ ] if title_api_section and module . __all__ : sections . append ( ( level + 1 , title_api_section ) ) for name in module . __all__ : api_sec . append ( ( level + 2 , "`" + name + "`" ) ) api_md += [ '' , '' ] entry = module . __dict__ [ name ] if entry . __doc__ : md , sec = doc2md ( entry . __doc__ , "`" + name + "`" , min_level = level + 2 , more_info = True , toc = False ) api_sec += sec api_md += md sections += api_sec head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] ) md += [ '' , '' , make_heading ( level + 1 , title_api_section ) , ] if toc : md += [ '' ] md += make_toc ( api_sec , 1 ) md += api_md return "\n" . join ( md )
2339	def weighted_mean_and_std ( values , weights ) : average = np . average ( values , weights = weights , axis = 0 ) variance = np . dot ( weights , ( values - average ) ** 2 ) / weights . sum ( ) return ( average , np . sqrt ( variance ) )
4904	def populate_data_sharing_consent ( apps , schema_editor ) : DataSharingConsent = apps . get_model ( 'consent' , 'DataSharingConsent' ) EnterpriseCourseEnrollment = apps . get_model ( 'enterprise' , 'EnterpriseCourseEnrollment' ) User = apps . get_model ( 'auth' , 'User' ) for enrollment in EnterpriseCourseEnrollment . objects . all ( ) : user = User . objects . get ( pk = enrollment . enterprise_customer_user . user_id ) data_sharing_consent , __ = DataSharingConsent . objects . get_or_create ( username = user . username , enterprise_customer = enrollment . enterprise_customer_user . enterprise_customer , course_id = enrollment . course_id , ) if enrollment . consent_granted is not None : data_sharing_consent . granted = enrollment . consent_granted else : consent_state = enrollment . enterprise_customer_user . data_sharing_consent . first ( ) if consent_state is not None : data_sharing_consent . granted = consent_state . state in [ 'enabled' , 'external' ] else : data_sharing_consent . granted = False data_sharing_consent . save ( )
4717	def tsuite_exit ( trun , tsuite ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit" ) rcode = 0 for hook in reversed ( tsuite [ "hooks" ] [ "exit" ] ) : rcode = script_run ( trun , hook ) if rcode : break if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:tsuite:exit { rcode: %r } " % rcode , rcode ) return rcode
13583	def admin_obj_link ( obj , display = '' ) : url = reverse ( 'admin:%s_%s_changelist' % ( obj . _meta . app_label , obj . _meta . model_name ) ) url += '?id__exact=%s' % obj . id text = str ( obj ) if display : text = display return format_html ( '<a href="{}">{}</a>' , url , text )
2014	def _top ( self , n = 0 ) : if len ( self . stack ) - n < 0 : raise StackUnderflow ( ) return self . stack [ n - 1 ]
6931	def xmatch_cpdir_external_catalogs ( cpdir , xmatchpkl , cpfileglob = 'checkplot-*.pkl*' , xmatchradiusarcsec = 2.0 , updateexisting = True , resultstodir = None ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return xmatch_cplist_external_catalogs ( cplist , xmatchpkl , xmatchradiusarcsec = xmatchradiusarcsec , updateexisting = updateexisting , resultstodir = resultstodir )
5668	def stop_to_stop_network_for_route_type ( gtfs , route_type , link_attributes = None , start_time_ut = None , end_time_ut = None ) : if link_attributes is None : link_attributes = DEFAULT_STOP_TO_STOP_LINK_ATTRIBUTES assert ( route_type in route_types . TRANSIT_ROUTE_TYPES ) stops_dataframe = gtfs . get_stops_for_route_type ( route_type ) net = networkx . DiGraph ( ) _add_stops_to_net ( net , stops_dataframe ) events_df = gtfs . get_transit_events ( start_time_ut = start_time_ut , end_time_ut = end_time_ut , route_type = route_type ) if len ( net . nodes ( ) ) < 2 : assert events_df . shape [ 0 ] == 0 link_event_groups = events_df . groupby ( [ 'from_stop_I' , 'to_stop_I' ] , sort = False ) for key , link_events in link_event_groups : from_stop_I , to_stop_I = key assert isinstance ( link_events , pd . DataFrame ) if link_attributes is None : net . add_edge ( from_stop_I , to_stop_I ) else : link_data = { } if "duration_min" in link_attributes : link_data [ 'duration_min' ] = float ( link_events [ 'duration' ] . min ( ) ) if "duration_max" in link_attributes : link_data [ 'duration_max' ] = float ( link_events [ 'duration' ] . max ( ) ) if "duration_median" in link_attributes : link_data [ 'duration_median' ] = float ( link_events [ 'duration' ] . median ( ) ) if "duration_avg" in link_attributes : link_data [ 'duration_avg' ] = float ( link_events [ 'duration' ] . mean ( ) ) if "n_vehicles" in link_attributes : link_data [ 'n_vehicles' ] = int ( link_events . shape [ 0 ] ) if "capacity_estimate" in link_attributes : link_data [ 'capacity_estimate' ] = route_types . ROUTE_TYPE_TO_APPROXIMATE_CAPACITY [ route_type ] * int ( link_events . shape [ 0 ] ) if "d" in link_attributes : from_lat = net . node [ from_stop_I ] [ 'lat' ] from_lon = net . node [ from_stop_I ] [ 'lon' ] to_lat = net . node [ to_stop_I ] [ 'lat' ] to_lon = net . node [ to_stop_I ] [ 'lon' ] distance = wgs84_distance ( from_lat , from_lon , to_lat , to_lon ) link_data [ 'd' ] = int ( distance ) if "distance_shape" in link_attributes : assert "shape_id" in link_events . columns . values found = None for i , shape_id in enumerate ( link_events [ "shape_id" ] . values ) : if shape_id is not None : found = i break if found is None : link_data [ "distance_shape" ] = None else : link_event = link_events . iloc [ found ] distance = gtfs . get_shape_distance_between_stops ( link_event [ "trip_I" ] , int ( link_event [ "from_seq" ] ) , int ( link_event [ "to_seq" ] ) ) link_data [ 'distance_shape' ] = distance if "route_I_counts" in link_attributes : link_data [ "route_I_counts" ] = link_events . groupby ( "route_I" ) . size ( ) . to_dict ( ) net . add_edge ( from_stop_I , to_stop_I , attr_dict = link_data ) return net
13735	def get_api_error ( response ) : error_class = _status_code_to_class . get ( response . status_code , APIError ) return error_class ( response )
9643	def pydevd ( context ) : global pdevd_not_available if pdevd_not_available : return '' try : import pydevd except ImportError : pdevd_not_available = True return '' render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) for var in availables : locals ( ) [ var ] = context [ var ] try : pydevd . settrace ( ) except socket . error : pdevd_not_available = True return ''
11026	def sort_pem_objects ( pem_objects ) : keys , certs , ca_certs = [ ] , [ ] , [ ] for pem_object in pem_objects : if isinstance ( pem_object , pem . Key ) : keys . append ( pem_object ) else : if _is_ca ( pem_object ) : ca_certs . append ( pem_object ) else : certs . append ( pem_object ) [ key ] , [ cert ] = keys , certs return key , cert , ca_certs
5967	def solvate ( struct = 'top/protein.pdb' , top = 'top/system.top' , distance = 0.9 , boxtype = 'dodecahedron' , concentration = 0 , cation = 'NA' , anion = 'CL' , water = 'tip4p' , solvent_name = 'SOL' , with_membrane = False , ndx = 'main.ndx' , mainselection = '"Protein"' , dirname = 'solvate' , ** kwargs ) : sol = solvate_sol ( struct = struct , top = top , distance = distance , boxtype = boxtype , water = water , solvent_name = solvent_name , with_membrane = with_membrane , dirname = dirname , ** kwargs ) ion = solvate_ion ( struct = sol [ 'struct' ] , top = top , concentration = concentration , cation = cation , anion = anion , solvent_name = solvent_name , ndx = ndx , mainselection = mainselection , dirname = dirname , ** kwargs ) return ion
3599	def delivery ( self , packageName , versionCode = None , offerType = 1 , downloadToken = None , expansion_files = False ) : if versionCode is None : versionCode = self . details ( packageName ) . get ( 'versionCode' ) params = { 'ot' : str ( offerType ) , 'doc' : packageName , 'vc' : str ( versionCode ) } headers = self . getHeaders ( ) if downloadToken is not None : params [ 'dtok' ] = downloadToken response = requests . get ( DELIVERY_URL , headers = headers , params = params , verify = ssl_verify , timeout = 60 , proxies = self . proxies_config ) response = googleplay_pb2 . ResponseWrapper . FromString ( response . content ) if response . commands . displayErrorMessage != "" : raise RequestError ( response . commands . displayErrorMessage ) elif response . payload . deliveryResponse . appDeliveryData . downloadUrl == "" : raise RequestError ( 'App not purchased' ) else : result = { } result [ 'docId' ] = packageName result [ 'additionalData' ] = [ ] downloadUrl = response . payload . deliveryResponse . appDeliveryData . downloadUrl cookie = response . payload . deliveryResponse . appDeliveryData . downloadAuthCookie [ 0 ] cookies = { str ( cookie . name ) : str ( cookie . value ) } result [ 'file' ] = self . _deliver_data ( downloadUrl , cookies ) if not expansion_files : return result for obb in response . payload . deliveryResponse . appDeliveryData . additionalFile : a = { } if obb . fileType == 0 : obbType = 'main' else : obbType = 'patch' a [ 'type' ] = obbType a [ 'versionCode' ] = obb . versionCode a [ 'file' ] = self . _deliver_data ( obb . downloadUrl , None ) result [ 'additionalData' ] . append ( a ) return result
6308	def load_resource_module ( self ) : try : name = '{}.{}' . format ( self . name , 'dependencies' ) self . dependencies_module = importlib . import_module ( name ) except ModuleNotFoundError as err : raise EffectError ( ( "Effect package '{}' has no 'dependencies' module or the module has errors. " "Forwarded error from importlib: {}" ) . format ( self . name , err ) ) try : self . resources = getattr ( self . dependencies_module , 'resources' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has no 'resources' attribute" . format ( name ) ) if not isinstance ( self . resources , list ) : raise EffectError ( "Effect dependencies module '{}': 'resources' is of type {} instead of a list" . format ( name , type ( self . resources ) ) ) try : self . effect_packages = getattr ( self . dependencies_module , 'effect_packages' ) except AttributeError : raise EffectError ( "Effect dependencies module '{}' has 'effect_packages' attribute" . format ( name ) ) if not isinstance ( self . effect_packages , list ) : raise EffectError ( "Effect dependencies module '{}': 'effect_packages' is of type {} instead of a list" . format ( name , type ( self . effects ) ) )
488	def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) self . _creationTracebackString = traceback . format_stack ( ) if self . _clsNumOutstanding >= g_max_concurrency : errorMsg = ( "With numOutstanding=%r, exceeded concurrency limit=%r " "when requesting %r. OTHER TRACKED UNRELEASED " "INSTANCES (%s): %r" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return
11037	def maybe_key ( pem_path ) : acme_key_file = pem_path . child ( u'client.key' ) if acme_key_file . exists ( ) : key = _load_pem_private_key_bytes ( acme_key_file . getContent ( ) ) else : key = generate_private_key ( u'rsa' ) acme_key_file . setContent ( _dump_pem_private_key_bytes ( key ) ) return succeed ( JWKRSA ( key = key ) )
734	def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error
8071	def not_found ( url , wait = 10 ) : try : connection = open ( url , wait ) except HTTP404NotFound : return True except : return False return False
3241	def _get_base ( server_certificate , ** conn ) : server_certificate [ '_version' ] = 1 cert_details = get_server_certificate_api ( server_certificate [ 'ServerCertificateName' ] , ** conn ) if cert_details : server_certificate . update ( cert_details [ 'ServerCertificateMetadata' ] ) server_certificate [ 'CertificateBody' ] = cert_details [ 'CertificateBody' ] server_certificate [ 'CertificateChain' ] = cert_details . get ( 'CertificateChain' , None ) server_certificate [ 'UploadDate' ] = get_iso_string ( server_certificate [ 'UploadDate' ] ) server_certificate [ 'Expiration' ] = get_iso_string ( server_certificate [ 'Expiration' ] ) return server_certificate
7107	def fit ( self , X , y , coef_init = None , intercept_init = None , sample_weight = None ) : super ( SGDClassifier , self ) . fit ( X , y , coef_init , intercept_init , sample_weight )
10671	def _finalise_result_ ( compound , value , mass ) : result = value / 3.6E6 result = result / compound . molar_mass result = result * mass return result
7550	def _debug_off ( ) : if _os . path . exists ( __debugflag__ ) : _os . remove ( __debugflag__ ) __loglevel__ = "ERROR" _LOGGER . info ( "debugging turned off" ) _set_debug_dict ( __loglevel__ )
12055	def ftp_login ( folder = None ) : pwDir = os . path . realpath ( __file__ ) for i in range ( 3 ) : pwDir = os . path . dirname ( pwDir ) pwFile = os . path . join ( pwDir , "passwd.txt" ) print ( " -- looking for login information in:\n [%s]" % pwFile ) try : with open ( pwFile ) as f : lines = f . readlines ( ) username = lines [ 0 ] . strip ( ) password = lines [ 1 ] . strip ( ) print ( " -- found a valid username/password" ) except : print ( " -- password lookup FAILED." ) username = TK_askPassword ( "FTP LOGIN" , "enter FTP username" ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not username or not password : print ( " !! failed getting login info. aborting FTP effort." ) return print ( " username:" , username ) print ( " password:" , "*" * ( len ( password ) ) ) print ( " -- logging in to FTP ..." ) try : ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) if folder : ftp . cwd ( folder ) return ftp except : print ( " !! login failure !!" ) return False
11243	def add_newlines ( f , output , char ) : line_count = get_line_count ( f ) f = open ( f , 'r+' ) output = open ( output , 'r+' ) for line in range ( line_count ) : string = f . readline ( ) string = re . sub ( char , char + '\n' , string ) output . write ( string )
10905	def trisect_image ( imshape , edgepts = 'calc' ) : im_x , im_y = np . meshgrid ( np . arange ( imshape [ 0 ] ) , np . arange ( imshape [ 1 ] ) , indexing = 'ij' ) if np . size ( edgepts ) == 1 : f = np . sqrt ( 2. / 3. ) if edgepts == 'calc' else edgepts lower_edge = ( imshape [ 0 ] * ( 1 - f ) , imshape [ 1 ] * f ) upper_edge = ( imshape [ 0 ] * f , imshape [ 1 ] * ( 1 - f ) ) else : upper_edge , lower_edge = edgepts lower_slope = lower_edge [ 1 ] / max ( float ( imshape [ 0 ] - lower_edge [ 0 ] ) , 1e-9 ) upper_slope = ( imshape [ 1 ] - upper_edge [ 1 ] ) / float ( upper_edge [ 0 ] ) lower_intercept = - lower_slope * lower_edge [ 0 ] upper_intercept = upper_edge [ 1 ] lower_mask = im_y < ( im_x * lower_slope + lower_intercept ) upper_mask = im_y > ( im_x * upper_slope + upper_intercept ) center_mask = - ( lower_mask | upper_mask ) return upper_mask , center_mask , lower_mask
5164	def __intermediate_bridge ( self , interface , i ) : if interface [ 'type' ] == 'bridge' and i < 2 : bridge_members = ' ' . join ( interface . pop ( 'bridge_members' ) ) if bridge_members : interface [ 'ifname' ] = bridge_members else : interface [ 'bridge_empty' ] = True del interface [ 'ifname' ] elif interface [ 'type' ] == 'bridge' and i >= 2 : if 'br-' not in interface [ 'ifname' ] : interface [ 'ifname' ] = 'br-{ifname}' . format ( ** interface ) for attr in [ 'type' , 'bridge_members' , 'stp' , 'gateway' ] : if attr in interface : del interface [ attr ] elif interface [ 'type' ] != 'bridge' : del interface [ 'type' ] return interface
2149	def modify ( self , pk = None , create_on_missing = False , ** kwargs ) : if pk is None and create_on_missing : try : self . get ( ** copy . deepcopy ( kwargs ) ) except exc . NotFound : return self . create ( ** kwargs ) config_item = self . _separate ( kwargs ) notification_type = kwargs . pop ( 'notification_type' , None ) debug . log ( 'Modify everything except notification type and' ' configuration' , header = 'details' ) part_result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , ** kwargs ) if notification_type is None or notification_type == part_result [ 'notification_type' ] : for item in part_result [ 'notification_configuration' ] : if item not in config_item or not config_item [ item ] : to_add = part_result [ 'notification_configuration' ] [ item ] if not ( to_add == '$encrypted$' and item in Resource . encrypted_fields ) : config_item [ item ] = to_add if notification_type is None : kwargs [ 'notification_type' ] = part_result [ 'notification_type' ] else : kwargs [ 'notification_type' ] = notification_type self . _configuration ( kwargs , config_item ) debug . log ( 'Modify notification type and configuration' , header = 'details' ) result = super ( Resource , self ) . modify ( pk = pk , create_on_missing = create_on_missing , ** kwargs ) if 'changed' in result and 'changed' in part_result : result [ 'changed' ] = result [ 'changed' ] or part_result [ 'changed' ] return result
4260	def load_exif ( album ) : if not hasattr ( album . gallery , "exifCache" ) : _restore_cache ( album . gallery ) cache = album . gallery . exifCache for media in album . medias : if media . type == "image" : key = os . path . join ( media . path , media . filename ) if key in cache : media . exif = cache [ key ]
4205	def levdown ( anxt , enxt = None ) : if anxt [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) anxt = anxt [ 1 : ] knxt = anxt [ - 1 ] if knxt == 1.0 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = ( anxt [ 0 : - 1 ] - knxt * numpy . conj ( anxt [ - 2 : : - 1 ] ) ) / ( 1. - abs ( knxt ) ** 2 ) ecur = None if enxt is not None : ecur = enxt / ( 1. - numpy . dot ( knxt . conj ( ) . transpose ( ) , knxt ) ) acur = numpy . insert ( acur , 0 , 1 ) return acur , ecur
10926	def do_run_1 ( self ) : while not self . check_terminate ( ) : self . _has_run = True self . _run1 ( ) self . _num_iter += 1 self . _inner_run_counter += 1
12592	def query_reliabledictionary ( client , application_name , service_name , dictionary_name , query_string , partition_key = None , partition_id = None , output_file = None ) : cluster = Cluster . from_sfclient ( client ) dictionary = cluster . get_application ( application_name ) . get_service ( service_name ) . get_dictionary ( dictionary_name ) start = time . time ( ) if ( partition_id != None ) : result = dictionary . query ( query_string , PartitionLookup . ID , partition_id ) elif ( partition_key != None ) : result = dictionary . query ( query_string , PartitionLookup . KEY , partition_key ) else : result = dictionary . query ( query_string ) if type ( result ) is str : print ( result ) return else : result = json . dumps ( result . get ( "value" ) , indent = 4 ) print ( "Query took " + str ( time . time ( ) - start ) + " seconds" ) if ( output_file == None ) : output_file = "{}-{}-{}-query-output.json" . format ( application_name , service_name , dictionary_name ) with open ( output_file , "w" ) as output : output . write ( result ) print ( ) print ( 'Printed output to: ' + output_file ) print ( result )
7817	def remove_handler ( self , handler ) : with self . lock : if handler in self . handlers : self . handlers . remove ( handler ) self . _update_handlers ( )
4973	def clean_channel_worker_username ( self ) : channel_worker_username = self . cleaned_data [ 'channel_worker_username' ] . strip ( ) try : User . objects . get ( username = channel_worker_username ) except User . DoesNotExist : raise ValidationError ( ValidationMessages . INVALID_CHANNEL_WORKER . format ( channel_worker_username = channel_worker_username ) ) return channel_worker_username
2388	def spell_correct ( string ) : f = tempfile . NamedTemporaryFile ( mode = 'w' ) f . write ( string ) f . flush ( ) f_path = os . path . abspath ( f . name ) try : p = os . popen ( aspell_path + " -a < " + f_path + " --sug-mode=ultra" ) incorrect = p . readlines ( ) p . close ( ) except Exception : log . exception ( "aspell process failed; could not spell check" ) return string , 0 , string finally : f . close ( ) incorrect_words = list ( ) correct_spelling = list ( ) for i in range ( 1 , len ( incorrect ) ) : if ( len ( incorrect [ i ] ) > 10 ) : match = re . search ( ":" , incorrect [ i ] ) if hasattr ( match , "start" ) : begstring = incorrect [ i ] [ 2 : match . start ( ) ] begmatch = re . search ( " " , begstring ) begword = begstring [ 0 : begmatch . start ( ) ] sugstring = incorrect [ i ] [ match . start ( ) + 2 : ] sugmatch = re . search ( "," , sugstring ) if hasattr ( sugmatch , "start" ) : sug = sugstring [ 0 : sugmatch . start ( ) ] incorrect_words . append ( begword ) correct_spelling . append ( sug ) newstring = string markup_string = string already_subbed = [ ] for i in range ( 0 , len ( incorrect_words ) ) : sub_pat = r"\b" + incorrect_words [ i ] + r"\b" sub_comp = re . compile ( sub_pat ) newstring = re . sub ( sub_comp , correct_spelling [ i ] , newstring ) if incorrect_words [ i ] not in already_subbed : markup_string = re . sub ( sub_comp , '<bs>' + incorrect_words [ i ] + "</bs>" , markup_string ) already_subbed . append ( incorrect_words [ i ] ) return newstring , len ( incorrect_words ) , markup_string
3489	def _sbase_notes_dict ( sbase , notes ) : if notes and len ( notes ) > 0 : tokens = [ '<html xmlns = "http://www.w3.org/1999/xhtml" >' ] + [ "<p>{}: {}</p>" . format ( k , v ) for ( k , v ) in notes . items ( ) ] + [ "</html>" ] _check ( sbase . setNotes ( "\n" . join ( tokens ) ) , "Setting notes on sbase: {}" . format ( sbase ) )
2871	def remove_event_detect ( self , pin ) : self . mraa_gpio . Gpio . isrExit ( self . mraa_gpio . Gpio ( pin ) )
12236	def objective ( param_scales = ( 1 , 1 ) , xstar = None , seed = None ) : ndim = len ( param_scales ) def decorator ( func ) : @ wraps ( func ) def wrapper ( theta ) : return func ( theta ) def param_init ( ) : np . random . seed ( seed ) return np . random . randn ( ndim , ) * np . array ( param_scales ) wrapper . ndim = ndim wrapper . param_init = param_init wrapper . xstar = xstar return wrapper return decorator
3151	def update ( self , list_id , webhook_id , data ) : self . list_id = list_id self . webhook_id = webhook_id return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'webhooks' , webhook_id ) , data = data )
2555	def add ( self , * args ) : for obj in args : if isinstance ( obj , numbers . Number ) : obj = str ( obj ) if isinstance ( obj , basestring ) : obj = escape ( obj ) self . children . append ( obj ) elif isinstance ( obj , dom_tag ) : ctx = dom_tag . _with_contexts [ _get_thread_context ( ) ] if ctx and ctx [ - 1 ] : ctx [ - 1 ] . used . add ( obj ) self . children . append ( obj ) obj . parent = self obj . setdocument ( self . document ) elif isinstance ( obj , dict ) : for attr , value in obj . items ( ) : self . set_attribute ( * dom_tag . clean_pair ( attr , value ) ) elif hasattr ( obj , '__iter__' ) : for subobj in obj : self . add ( subobj ) else : raise ValueError ( '%r not a tag or string.' % obj ) if len ( args ) == 1 : return args [ 0 ] return args
10703	def get_usage ( _id ) : url = USAGE_URL % _id arequest = requests . get ( url , headers = HEADERS ) status_code = str ( arequest . status_code ) if status_code == '401' : _LOGGER . error ( "Token expired." ) return False try : return arequest . json ( ) except ValueError : _LOGGER . info ( "Failed to get usage. Not supported by unit?" ) return None
5825	def _patch ( self , route , data , headers = None , failure_message = None ) : headers = self . _get_headers ( headers ) response_lambda = ( lambda : requests . patch ( self . _get_qualified_route ( route ) , headers = headers , data = data , verify = False , proxies = self . proxies ) ) response = check_for_rate_limiting ( response_lambda ( ) , response_lambda ) return self . _handle_response ( response , failure_message )
2274	def _win32_rmtree ( path , verbose = 0 ) : def _rmjunctions ( root ) : subdirs = [ ] for name in os . listdir ( root ) : current = join ( root , name ) if os . path . isdir ( current ) : if _win32_is_junction ( current ) : os . rmdir ( current ) elif not os . path . islink ( current ) : subdirs . append ( current ) for subdir in subdirs : _rmjunctions ( subdir ) if _win32_is_junction ( path ) : if verbose : print ( 'Deleting <JUNCTION> directory="{}"' . format ( path ) ) os . rmdir ( path ) else : if verbose : print ( 'Deleting directory="{}"' . format ( path ) ) _rmjunctions ( path ) import shutil shutil . rmtree ( path )
10449	def getallstates ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) _obj_states = [ ] if object_handle . AXEnabled : _obj_states . append ( "enabled" ) if object_handle . AXFocused : _obj_states . append ( "focused" ) else : try : if object_handle . AXFocused : _obj_states . append ( "focusable" ) except : pass if re . match ( "AXCheckBox" , object_handle . AXRole , re . M | re . U | re . L ) or re . match ( "AXRadioButton" , object_handle . AXRole , re . M | re . U | re . L ) : if object_handle . AXValue : _obj_states . append ( "checked" ) return _obj_states
12903	def _parse_genotype ( self , vcf_fields ) : format_col = vcf_fields [ 8 ] . split ( ':' ) genome_data = vcf_fields [ 9 ] . split ( ':' ) try : gt_idx = format_col . index ( 'GT' ) except ValueError : return [ ] return [ int ( x ) for x in re . split ( r'[\|/]' , genome_data [ gt_idx ] ) if x != '.' ]
2603	def client_file ( self ) : return os . path . join ( self . ipython_dir , 'profile_{0}' . format ( self . profile ) , 'security/ipcontroller-client.json' )
12462	def prepare_args ( config , bootstrap ) : config = copy . deepcopy ( config ) environ = dict ( copy . deepcopy ( os . environ ) ) data = { 'env' : bootstrap [ 'env' ] , 'pip' : pip_cmd ( bootstrap [ 'env' ] , '' , return_path = True ) , 'requirements' : bootstrap [ 'requirements' ] } environ . update ( data ) if isinstance ( config , string_types ) : return config . format ( ** environ ) for key , value in iteritems ( config ) : if not isinstance ( value , string_types ) : continue config [ key ] = value . format ( ** environ ) return config_to_args ( config )
8785	def update_port ( self , context , port_id , ** kwargs ) : LOG . info ( "update_port %s %s" % ( context . tenant_id , port_id ) ) if kwargs . get ( "security_groups" ) : msg = 'ironic driver does not support security group operations.' raise IronicException ( msg = msg ) return { "uuid" : port_id }
88	def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )
3001	def cryptoDF ( token = '' , version = '' ) : df = pd . DataFrame ( crypto ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
12820	def _filename ( draw , result_type = None ) : ascii_char = characters ( min_codepoint = 0x01 , max_codepoint = 0x7f ) if os . name == 'nt' : surrogate = characters ( min_codepoint = 0xD800 , max_codepoint = 0xDFFF ) uni_char = characters ( min_codepoint = 0x1 ) text_strategy = text ( alphabet = one_of ( uni_char , surrogate , ascii_char ) ) def text_to_bytes ( path ) : fs_enc = sys . getfilesystemencoding ( ) try : return path . encode ( fs_enc , 'surrogatepass' ) except UnicodeEncodeError : return path . encode ( fs_enc , 'replace' ) bytes_strategy = text_strategy . map ( text_to_bytes ) else : latin_char = characters ( min_codepoint = 0x01 , max_codepoint = 0xff ) bytes_strategy = text ( alphabet = one_of ( latin_char , ascii_char ) ) . map ( lambda t : t . encode ( 'latin-1' ) ) unix_path_text = bytes_strategy . map ( lambda b : b . decode ( sys . getfilesystemencoding ( ) , 'surrogateescape' if PY3 else 'ignore' ) ) text_strategy = permutations ( draw ( unix_path_text ) ) . map ( u"" . join ) if result_type is None : return draw ( one_of ( bytes_strategy , text_strategy ) ) elif result_type is bytes : return draw ( bytes_strategy ) else : return draw ( text_strategy )
11806	def viterbi_segment ( text , P ) : n = len ( text ) words = [ '' ] + list ( text ) best = [ 1.0 ] + [ 0.0 ] * n for i in range ( n + 1 ) : for j in range ( 0 , i ) : w = text [ j : i ] if P [ w ] * best [ i - len ( w ) ] >= best [ i ] : best [ i ] = P [ w ] * best [ i - len ( w ) ] words [ i ] = w sequence = [ ] i = len ( words ) - 1 while i > 0 : sequence [ 0 : 0 ] = [ words [ i ] ] i = i - len ( words [ i ] ) return sequence , best [ - 1 ]
12064	def lazygo ( watchFolder = '../abfs/' , reAnalyze = False , rebuildSite = False , keepGoing = True , matching = False ) : abfsKnown = [ ] while True : print ( ) pagesNeeded = [ ] for fname in glob . glob ( watchFolder + "/*.abf" ) : ID = os . path . basename ( fname ) . replace ( ".abf" , "" ) if not fname in abfsKnown : if os . path . exists ( fname . replace ( ".abf" , ".rsv" ) ) : continue if matching and not matching in fname : continue abfsKnown . append ( fname ) if os . path . exists ( os . path . dirname ( fname ) + "/swhlab4/" + os . path . basename ( fname ) . replace ( ".abf" , "_info.pkl" ) ) and reAnalyze == False : print ( "already analyzed" , os . path . basename ( fname ) ) if rebuildSite : pagesNeeded . append ( ID ) else : handleNewABF ( fname ) pagesNeeded . append ( ID ) if len ( pagesNeeded ) : print ( " -- rebuilding index page" ) indexing . genIndex ( os . path . dirname ( fname ) , forceIDs = pagesNeeded ) if not keepGoing : return for i in range ( 50 ) : print ( '.' , end = '' ) time . sleep ( .2 )
13120	def argument_count ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . count ( ** vars ( arguments ) )
9576	def read_header ( fd , endian ) : flag_class , nzmax = read_elements ( fd , endian , [ 'miUINT32' ] ) header = { 'mclass' : flag_class & 0x0FF , 'is_logical' : ( flag_class >> 9 & 1 ) == 1 , 'is_global' : ( flag_class >> 10 & 1 ) == 1 , 'is_complex' : ( flag_class >> 11 & 1 ) == 1 , 'nzmax' : nzmax } header [ 'dims' ] = read_elements ( fd , endian , [ 'miINT32' ] ) header [ 'n_dims' ] = len ( header [ 'dims' ] ) if header [ 'n_dims' ] != 2 : raise ParseError ( 'Only matrices with dimension 2 are supported.' ) header [ 'name' ] = read_elements ( fd , endian , [ 'miINT8' ] , is_name = True ) return header
3983	def get_same_container_repos_from_spec ( app_or_library_spec ) : repos = set ( ) app_or_lib_repo = get_repo_of_app_or_library ( app_or_library_spec . name ) if app_or_lib_repo is not None : repos . add ( app_or_lib_repo ) for dependent_name in app_or_library_spec [ 'depends' ] [ 'libs' ] : repos . add ( get_repo_of_app_or_library ( dependent_name ) ) return repos
5447	def _parse_gcs_uri ( self , raw_uri ) : raw_uri = directory_fmt ( raw_uri ) _ , docker_path = _gcs_uri_rewriter ( raw_uri ) docker_uri = os . path . join ( self . _relative_path , docker_path ) return docker_uri
13594	def print_information ( handler , label ) : click . echo ( '=> Latest stable: {tag}' . format ( tag = click . style ( str ( handler . latest_stable or 'N/A' ) , fg = 'yellow' if handler . latest_stable else 'magenta' ) ) ) if label is not None : latest_revision = handler . latest_revision ( label ) click . echo ( '=> Latest relative revision ({label}): {tag}' . format ( label = click . style ( label , fg = 'blue' ) , tag = click . style ( str ( latest_revision or 'N/A' ) , fg = 'yellow' if latest_revision else 'magenta' ) ) )
9780	def build ( ctx , project , build ) : ctx . obj = ctx . obj or { } ctx . obj [ 'project' ] = project ctx . obj [ 'build' ] = build
3941	async def _fetch_channel_sid ( self ) : logger . info ( 'Requesting new gsessionid and SID...' ) self . _sid_param = None self . _gsessionid_param = None res = await self . send_maps ( [ ] ) self . _sid_param , self . _gsessionid_param = _parse_sid_response ( res . body ) logger . info ( 'New SID: {}' . format ( self . _sid_param ) ) logger . info ( 'New gsessionid: {}' . format ( self . _gsessionid_param ) )
5941	def transform_args ( self , * args , ** kwargs ) : newargs = self . _combineargs ( * args , ** kwargs ) return self . _build_arg_list ( ** newargs )
10605	def remove_entity ( self , name ) : entity_to_remove = None for e in self . entities : if e . name == name : entity_to_remove = e if entity_to_remove is not None : self . entities . remove ( entity_to_remove )
4788	def is_alpha ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isalpha ( ) : self . _err ( 'Expected <%s> to contain only alphabetic chars, but did not.' % self . val ) return self
9876	def _ordinal_metric ( _v1 , _v2 , i1 , i2 , n_v ) : if i1 > i2 : i1 , i2 = i2 , i1 return ( np . sum ( n_v [ i1 : ( i2 + 1 ) ] ) - ( n_v [ i1 ] + n_v [ i2 ] ) / 2 ) ** 2
1480	def _start_processes ( self , commands ) : Log . info ( "Start processes" ) processes_to_monitor = { } for ( name , command ) in commands . items ( ) : p = self . _run_process ( name , command ) processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) log_pid_for_process ( name , p . pid ) with self . process_lock : self . processes_to_monitor . update ( processes_to_monitor )
1449	def get_all_zk_state_managers ( conf ) : state_managers = [ ] state_locations = conf . get_state_locations_of_type ( "zookeeper" ) for location in state_locations : name = location [ 'name' ] hostport = location [ 'hostport' ] hostportlist = [ ] for hostportpair in hostport . split ( ',' ) : host = None port = None if ':' in hostport : hostandport = hostportpair . split ( ':' ) if len ( hostandport ) == 2 : host = hostandport [ 0 ] port = int ( hostandport [ 1 ] ) if not host or not port : raise Exception ( "Hostport for %s must be of the format 'host:port'." % ( name ) ) hostportlist . append ( ( host , port ) ) tunnelhost = location [ 'tunnelhost' ] rootpath = location [ 'rootpath' ] LOG . info ( "Connecting to zk hostports: " + str ( hostportlist ) + " rootpath: " + rootpath ) state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) state_managers . append ( state_manager ) return state_managers
58	def extend ( self , all_sides = 0 , top = 0 , right = 0 , bottom = 0 , left = 0 ) : return BoundingBox ( x1 = self . x1 - all_sides - left , x2 = self . x2 + all_sides + right , y1 = self . y1 - all_sides - top , y2 = self . y2 + all_sides + bottom )
7702	def get_items_by_name ( self , name , case_sensitive = True ) : if not case_sensitive and name : name = name . lower ( ) result = [ ] for item in self . _items : if item . name == name : result . append ( item ) elif item . name is None : continue elif not case_sensitive and item . name . lower ( ) == name : result . append ( item ) return result
13466	def set_moments ( self , sx , sxp , sxxp ) : self . _sx = sx self . _sxp = sxp self . _sxxp = sxxp emit = _np . sqrt ( sx ** 2 * sxp ** 2 - sxxp ** 2 ) self . _store_emit ( emit = emit )
8914	def list_services ( self ) : my_services = [ ] for service in self . name_index . values ( ) : my_services . append ( Service ( service ) ) return my_services
6238	def add_point_light ( self , position , radius ) : self . point_lights . append ( PointLight ( position , radius ) )
10968	def setup_passthroughs ( self ) : self . _nopickle = [ ] for c in self . comps : funcs = inspect . getmembers ( c , predicate = inspect . ismethod ) for func in funcs : if func [ 0 ] . startswith ( 'param_' ) : setattr ( self , func [ 0 ] , func [ 1 ] ) self . _nopickle . append ( func [ 0 ] ) funcs = c . exports ( ) for func in funcs : newname = c . category + '_' + func . __func__ . __name__ setattr ( self , newname , func ) self . _nopickle . append ( newname )
7705	def remove_item ( self , jid ) : if jid not in self . _jids : raise KeyError ( jid ) index = self . _jids [ jid ] for i in range ( index , len ( self . _jids ) ) : self . _jids [ self . _items [ i ] . jid ] -= 1 del self . _jids [ jid ] del self . _items [ index ]
2003	def function_call ( type_spec , * args ) : m = re . match ( r"(?P<name>[a-zA-Z_][a-zA-Z_0-9]*)(?P<type>\(.*\))" , type_spec ) if not m : raise EthereumError ( "Function signature expected" ) ABI . _check_and_warn_num_args ( type_spec , * args ) result = ABI . function_selector ( type_spec ) result += ABI . serialize ( m . group ( 'type' ) , * args ) return result
6167	def to_bin ( data , width ) : data_str = bin ( data & ( 2 ** width - 1 ) ) [ 2 : ] . zfill ( width ) return [ int ( x ) for x in tuple ( data_str ) ]
3364	def load_yaml_model ( filename ) : if isinstance ( filename , string_types ) : with io . open ( filename , "r" ) as file_handle : return model_from_dict ( yaml . load ( file_handle ) ) else : return model_from_dict ( yaml . load ( filename ) )
4632	def child ( self , offset256 ) : a = bytes ( self ) + offset256 s = hashlib . sha256 ( a ) . digest ( ) return self . add ( s )
12252	def delete_keys ( self , * args , ** kwargs ) : ikeys = iter ( kwargs . get ( 'keys' , args [ 0 ] if args else [ ] ) ) while True : try : key = ikeys . next ( ) except StopIteration : break if isinstance ( key , basestring ) : mimicdb . backend . srem ( tpl . bucket % self . name , key ) mimicdb . backend . delete ( tpl . key % ( self . name , key ) ) elif isinstance ( key , BotoKey ) or isinstance ( key , Key ) : mimicdb . backend . srem ( tpl . bucket % self . name , key . name ) mimicdb . backend . delete ( tpl . key % ( self . name , key . name ) ) return super ( Bucket , self ) . delete_keys ( * args , ** kwargs )
9913	def _create ( cls , model_class , * args , ** kwargs ) : manager = cls . _get_manager ( model_class ) return manager . create_user ( * args , ** kwargs )
7038	def get_dataset ( lcc_server , dataset_id , strformat = False , page = 1 ) : urlparams = { 'strformat' : 1 if strformat else 0 , 'page' : page , 'json' : 1 } urlqs = urlencode ( urlparams ) dataset_url = '%s/set/%s?%s' % ( lcc_server , dataset_id , urlqs ) LOGINFO ( 'retrieving dataset %s from %s, using URL: %s ...' % ( lcc_server , dataset_id , dataset_url ) ) try : have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } req = Request ( dataset_url , data = None , headers = headers ) resp = urlopen ( req ) dataset = json . loads ( resp . read ( ) ) return dataset except Exception as e : LOGEXCEPTION ( 'could not retrieve the dataset JSON!' ) return None
4379	def allow ( self , role , method , resource , with_children = True ) : if with_children : for r in role . get_children ( ) : permission = ( r . get_name ( ) , method , resource ) if permission not in self . _allowed : self . _allowed . append ( permission ) if role == 'anonymous' : permission = ( role , method , resource ) else : permission = ( role . get_name ( ) , method , resource ) if permission not in self . _allowed : self . _allowed . append ( permission )
10051	def post ( self , pid , record , action ) : record = getattr ( record , action ) ( pid = pid ) db . session . commit ( ) db . session . refresh ( pid ) db . session . refresh ( record . model ) post_action . send ( current_app . _get_current_object ( ) , action = action , pid = pid , deposit = record ) response = self . make_response ( pid , record , 202 if action == 'publish' else 201 ) endpoint = '.{0}_item' . format ( pid . pid_type ) location = url_for ( endpoint , pid_value = pid . pid_value , _external = True ) response . headers . extend ( dict ( Location = location ) ) return response
5155	def type_cast ( self , item , schema = None ) : if schema is None : schema = self . _schema properties = schema [ 'properties' ] for key , value in item . items ( ) : if key not in properties : continue try : json_type = properties [ key ] [ 'type' ] except KeyError : json_type = None if json_type == 'integer' and not isinstance ( value , int ) : value = int ( value ) elif json_type == 'boolean' and not isinstance ( value , bool ) : value = value == '1' item [ key ] = value return item
5258	def parse_operand ( self , buf ) : buf = iter ( buf ) try : operand = 0 for _ in range ( self . operand_size ) : operand <<= 8 operand |= next ( buf ) self . _operand = operand except StopIteration : raise ParseError ( "Not enough data for decoding" )
2069	def get_splice_data ( ) : df = pd . read_csv ( 'source_data/splice/splice.csv' ) X = df . reindex ( columns = [ x for x in df . columns . values if x != 'class' ] ) X [ 'dna' ] = X [ 'dna' ] . map ( lambda x : list ( str ( x ) . strip ( ) ) ) for idx in range ( 60 ) : X [ 'dna_%d' % ( idx , ) ] = X [ 'dna' ] . map ( lambda x : x [ idx ] ) del X [ 'dna' ] y = df . reindex ( columns = [ 'class' ] ) y = preprocessing . LabelEncoder ( ) . fit_transform ( y . values . reshape ( - 1 , ) ) mapping = None return X , y , mapping
5226	def _to_gen_ ( iterable ) : from collections import Iterable for elm in iterable : if isinstance ( elm , Iterable ) and not isinstance ( elm , ( str , bytes ) ) : yield from flatten ( elm ) else : yield elm
1093	def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )
9081	def find ( self , query , ** kwargs ) : if 'providers' not in kwargs : providers = self . get_providers ( ) else : pargs = kwargs [ 'providers' ] if isinstance ( pargs , list ) : providers = self . get_providers ( ids = pargs ) else : providers = self . get_providers ( ** pargs ) kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . find ( query , ** kwarguments ) } for p in providers ]
2433	def set_created_date ( self , doc , created ) : if not self . created_date_set : self . created_date_set = True date = utils . datetime_from_iso_format ( created ) if date is not None : doc . creation_info . created = date return True else : raise SPDXValueError ( 'CreationInfo::Date' ) else : raise CardinalityError ( 'CreationInfo::Created' )
3191	def update ( self , folder_id , data ) : if 'name' not in data : raise KeyError ( 'The template folder must have a name' ) self . folder_id = folder_id return self . _mc_client . _patch ( url = self . _build_path ( folder_id ) , data = data )
3208	def _reformat_policy ( policy ) : policy_name = policy [ 'PolicyName' ] ret = { } ret [ 'type' ] = policy [ 'PolicyTypeName' ] attrs = policy [ 'PolicyAttributeDescriptions' ] if ret [ 'type' ] != 'SSLNegotiationPolicyType' : return policy_name , ret attributes = dict ( ) for attr in attrs : attributes [ attr [ 'AttributeName' ] ] = attr [ 'AttributeValue' ] ret [ 'protocols' ] = dict ( ) ret [ 'protocols' ] [ 'sslv2' ] = bool ( attributes . get ( 'Protocol-SSLv2' ) ) ret [ 'protocols' ] [ 'sslv3' ] = bool ( attributes . get ( 'Protocol-SSLv3' ) ) ret [ 'protocols' ] [ 'tlsv1' ] = bool ( attributes . get ( 'Protocol-TLSv1' ) ) ret [ 'protocols' ] [ 'tlsv1_1' ] = bool ( attributes . get ( 'Protocol-TLSv1.1' ) ) ret [ 'protocols' ] [ 'tlsv1_2' ] = bool ( attributes . get ( 'Protocol-TLSv1.2' ) ) ret [ 'server_defined_cipher_order' ] = bool ( attributes . get ( 'Server-Defined-Cipher-Order' ) ) ret [ 'reference_security_policy' ] = attributes . get ( 'Reference-Security-Policy' , None ) non_ciphers = [ 'Server-Defined-Cipher-Order' , 'Protocol-SSLv2' , 'Protocol-SSLv3' , 'Protocol-TLSv1' , 'Protocol-TLSv1.1' , 'Protocol-TLSv1.2' , 'Reference-Security-Policy' ] ciphers = [ ] for cipher in attributes : if attributes [ cipher ] == 'true' and cipher not in non_ciphers : ciphers . append ( cipher ) ciphers . sort ( ) ret [ 'supported_ciphers' ] = ciphers return policy_name , ret
8226	def _makeInstance ( self , clazz , args , kwargs ) : inst = clazz ( self , * args , ** kwargs ) return inst
10875	def get_polydisp_pts_wts ( kfki , sigkf , dist_type = 'gaussian' , nkpts = 3 ) : if dist_type . lower ( ) == 'gaussian' : pts , wts = np . polynomial . hermite . hermgauss ( nkpts ) kfkipts = np . abs ( kfki + sigkf * np . sqrt ( 2 ) * pts ) elif dist_type . lower ( ) == 'laguerre' or dist_type . lower ( ) == 'gamma' : k_scale = sigkf ** 2 / kfki associated_order = kfki ** 2 / sigkf ** 2 - 1 max_order = 150 if associated_order > max_order or associated_order < ( - 1 + 1e-3 ) : warnings . warn ( 'Numerically unstable sigk, clipping' , RuntimeWarning ) associated_order = np . clip ( associated_order , - 1 + 1e-3 , max_order ) kfkipts , wts = la_roots ( nkpts , associated_order ) kfkipts *= k_scale else : raise ValueError ( 'dist_type must be either gaussian or laguerre' ) return kfkipts , wts / wts . sum ( )
5004	def transmit ( self , payload , ** kwargs ) : kwargs [ 'app_label' ] = 'degreed' kwargs [ 'model_name' ] = 'DegreedLearnerDataTransmissionAudit' kwargs [ 'remote_user_id' ] = 'degreed_user_email' super ( DegreedLearnerTransmitter , self ) . transmit ( payload , ** kwargs )
9621	def gamepad ( self ) : state = _xinput_state ( ) _xinput . XInputGetState ( self . ControllerID - 1 , pointer ( state ) ) self . dwPacketNumber = state . dwPacketNumber return state . XINPUT_GAMEPAD
2227	def _digest_hasher ( hasher , hashlen , base ) : hex_text = hasher . hexdigest ( ) base_text = _convert_hexstr_base ( hex_text , base ) text = base_text [ : hashlen ] return text
11047	def _parse_field_value ( line ) : if line . startswith ( ':' ) : return None , None if ':' not in line : return line , '' field , value = line . split ( ':' , 1 ) value = value [ 1 : ] if value . startswith ( ' ' ) else value return field , value
13105	def cmpToDataStore_uri ( base , ds1 , ds2 ) : ret = difflib . get_close_matches ( base . uri , [ ds1 . uri , ds2 . uri ] , 1 , cutoff = 0.5 ) if len ( ret ) <= 0 : return 0 if ret [ 0 ] == ds1 . uri : return - 1 return 1
2931	def pre_parse_and_validate_signavio ( self , bpmn , filename ) : self . _check_for_disconnected_boundary_events_signavio ( bpmn , filename ) self . _fix_call_activities_signavio ( bpmn , filename ) return bpmn
2907	def _is_descendant_of ( self , parent ) : if self . parent is None : return False if self . parent == parent : return True return self . parent . _is_descendant_of ( parent )
7135	def filter_dict ( d , exclude ) : ret = { } for key , value in d . items ( ) : if key not in exclude : ret . update ( { key : value } ) return ret
461	def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]
7116	def available_sources ( sources ) : for dirs , name in sources : for directory in dirs : fn = os . path . join ( directory , name ) + '.py' if os . path . isfile ( fn ) : yield fn
10814	def add_member ( self , user , state = MembershipState . ACTIVE ) : return Membership . create ( self , user , state )
5108	def next_event_description ( self ) : if self . _departures [ 0 ] . _time < self . _arrivals [ 0 ] . _time : return 2 elif self . _arrivals [ 0 ] . _time < infty : return 1 else : return 0
9191	def _insert_metadata ( cursor , model , publisher , message ) : params = model . metadata . copy ( ) params [ 'publisher' ] = publisher params [ 'publication_message' ] = message params [ '_portal_type' ] = _model_to_portaltype ( model ) params [ 'summary' ] = str ( cnxepub . DocumentSummaryFormatter ( model ) ) for person_field in ATTRIBUTED_ROLE_KEYS : params [ person_field ] = [ parse_user_uri ( x [ 'id' ] ) for x in params . get ( person_field , [ ] ) ] params [ 'parent_ident_hash' ] = parse_parent_ident_hash ( model ) if model . ident_hash is not None : uuid , version = split_ident_hash ( model . ident_hash , split_version = True ) params [ '_uuid' ] = uuid params [ '_major_version' ] , params [ '_minor_version' ] = version cursor . execute ( "SELECT moduleid FROM latest_modules WHERE uuid = %s" , ( uuid , ) ) try : moduleid = cursor . fetchone ( ) [ 0 ] except TypeError : moduleid = None params [ '_moduleid' ] = moduleid cursor . execute ( "SELECT * from document_controls where uuid = %s" , ( uuid , ) ) try : cursor . fetchone ( ) [ 0 ] except TypeError : cursor . execute ( "INSERT INTO document_controls (uuid) VALUES (%s)" , ( uuid , ) ) created = model . metadata . get ( 'created' , None ) stmt = MODULE_INSERTION_TEMPLATE . format ( ** { '__uuid__' : "%(_uuid)s::uuid" , '__major_version__' : "%(_major_version)s" , '__minor_version__' : "%(_minor_version)s" , '__moduleid__' : moduleid is None and "DEFAULT" or "%(_moduleid)s" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) else : created = model . metadata . get ( 'created' , None ) stmt = MODULE_INSERTION_TEMPLATE . format ( ** { '__uuid__' : "DEFAULT" , '__major_version__' : "DEFAULT" , '__minor_version__' : "DEFAULT" , '__moduleid__' : "DEFAULT" , '__created__' : created is None and "DEFAULT" or "%(created)s" , } ) cursor . execute ( stmt , params ) module_ident , ident_hash = cursor . fetchone ( ) _insert_optional_roles ( cursor , model , module_ident ) return module_ident , ident_hash
12258	def sdcone ( x , rho ) : U , V = np . linalg . eigh ( x ) return V . dot ( np . diag ( np . maximum ( U , 0 ) ) . dot ( V . T ) )
2698	def write_dot ( graph , ranks , path = "graph.dot" ) : dot = Digraph ( ) for node in graph . nodes ( ) : dot . node ( node , "%s %0.3f" % ( node , ranks [ node ] ) ) for edge in graph . edges ( ) : dot . edge ( edge [ 0 ] , edge [ 1 ] , constraint = "false" ) with open ( path , 'w' ) as f : f . write ( dot . source )
7752	def process_presence ( self , stanza ) : stanza_type = stanza . stanza_type return self . __try_handlers ( self . _presence_handlers , stanza , stanza_type )
9138	def drop_all ( self , check_first : bool = True ) : self . _metadata . drop_all ( self . engine , checkfirst = check_first ) self . _store_drop ( )
4688	def get_shared_secret ( priv , pub ) : pub_point = pub . point ( ) priv_point = int ( repr ( priv ) , 16 ) res = pub_point * priv_point res_hex = "%032x" % res . x ( ) res_hex = "0" * ( 64 - len ( res_hex ) ) + res_hex return res_hex
7345	async def call_on_response ( self , data ) : since_id = self . kwargs . get ( self . param , 0 ) + 1 if self . fill_gaps : if data [ - 1 ] [ 'id' ] != since_id : max_id = data [ - 1 ] [ 'id' ] - 1 responses = with_max_id ( self . request ( ** self . kwargs , max_id = max_id ) ) async for tweets in responses : data . extend ( tweets ) if data [ - 1 ] [ 'id' ] == self . last_id : data = data [ : - 1 ] if not data and not self . force : raise StopAsyncIteration await self . set_param ( data )
217	def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( "latin-1" ) append_value = value . encode ( "latin-1" ) self . _list . append ( ( append_key , append_value ) )
6190	def set_sim_params ( self , nparams , attr_params ) : for name , value in nparams . items ( ) : val = value [ 0 ] if value [ 0 ] is not None else 'none' self . h5file . create_array ( '/parameters' , name , obj = val , title = value [ 1 ] ) for name , value in attr_params . items ( ) : self . h5file . set_node_attr ( '/parameters' , name , value )
10788	def add_subtract_locally ( st , region_depth = 3 , filter_size = 5 , sigma_cutoff = 8 , ** kwargs ) : tiles = identify_misfeatured_regions ( st , filter_size = filter_size , sigma_cutoff = sigma_cutoff ) n_empty = 0 n_added = 0 new_poses = [ ] for t in tiles : curn , curinds = add_subtract_misfeatured_tile ( st , t , ** kwargs ) if curn == 0 : n_empty += 1 else : n_added += curn new_poses . extend ( st . obj_get_positions ( ) [ curinds ] ) if n_empty > region_depth : break else : pass return n_added , new_poses
9782	def delete ( ctx ) : user , project_name , _build = get_build_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'build' ) ) if not click . confirm ( "Are sure you want to delete build job `{}`" . format ( _build ) ) : click . echo ( 'Existing without deleting build job.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . build_job . delete_build ( user , project_name , _build ) BuildJobManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete job `{}`.' . format ( _build ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Build job `{}` was deleted successfully" . format ( _build ) )
8236	def left_complement ( clr ) : left = split_complementary ( clr ) [ 1 ] colors = complementary ( clr ) colors [ 3 ] . h = left . h colors [ 4 ] . h = left . h colors [ 5 ] . h = left . h colors = colorlist ( colors [ 0 ] , colors [ 2 ] , colors [ 1 ] , colors [ 3 ] , colors [ 4 ] , colors [ 5 ] ) return colors
9618	def UnPlug ( self , force = False ) : if force : _xinput . UnPlugForce ( c_uint ( self . id ) ) else : _xinput . UnPlug ( c_uint ( self . id ) ) while self . id not in self . available_ids ( ) : if self . id == 0 : break
970	def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e
13514	def froude_number ( speed , length ) : g = 9.80665 Fr = speed / np . sqrt ( g * length ) return Fr
13525	def error ( code : int , * args , ** kwargs ) -> HedgehogCommandError : if code == FAILED_COMMAND and len ( args ) >= 1 and args [ 0 ] == "Emergency Shutdown activated" : return EmergencyShutdown ( * args , ** kwargs ) return _errors [ code ] ( * args , ** kwargs )
2850	def _mpsse_enable ( self ) : self . _check ( ftdi . set_bitmode , 0 , 0 ) self . _check ( ftdi . set_bitmode , 0 , 2 )
3846	def parse_typing_status_message ( p ) : return TypingStatusMessage ( conv_id = p . conversation_id . id , user_id = from_participantid ( p . sender_id ) , timestamp = from_timestamp ( p . timestamp ) , status = p . type , )
5574	def available_output_formats ( ) : output_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "w" , "rw" ] ) : output_formats . append ( driver_ . METADATA [ "driver_name" ] ) return output_formats
5722	def _restore_resources ( resources ) : resources = deepcopy ( resources ) for resource in resources : schema = resource [ 'schema' ] for fk in schema . get ( 'foreignKeys' , [ ] ) : _ , name = _restore_path ( fk [ 'reference' ] [ 'resource' ] ) fk [ 'reference' ] [ 'resource' ] = name return resources
3058	def _write_credentials_file ( credentials_file , credentials ) : data = { 'file_version' : 2 , 'credentials' : { } } for key , credential in iteritems ( credentials ) : credential_json = credential . to_json ( ) encoded_credential = _helpers . _from_bytes ( base64 . b64encode ( _helpers . _to_bytes ( credential_json ) ) ) data [ 'credentials' ] [ key ] = encoded_credential credentials_file . seek ( 0 ) json . dump ( data , credentials_file ) credentials_file . truncate ( )
5809	def parse_handshake_messages ( data ) : pointer = 0 data_len = len ( data ) while pointer < data_len : length = int_from_bytes ( data [ pointer + 1 : pointer + 4 ] ) yield ( data [ pointer : pointer + 1 ] , data [ pointer + 4 : pointer + 4 + length ] ) pointer += 4 + length
11498	def get_community_by_name ( self , name , token = None ) : parameters = dict ( ) parameters [ 'name' ] = name if token : parameters [ 'token' ] = token response = self . request ( 'midas.community.get' , parameters ) return response
11395	def load_class ( path ) : package , klass = path . rsplit ( '.' , 1 ) module = import_module ( package ) return getattr ( module , klass )
5482	def retry_auth_check ( exception ) : if isinstance ( exception , apiclient . errors . HttpError ) : if exception . resp . status in HTTP_AUTH_ERROR_CODES : _print_error ( 'Retrying...' ) return True return False
8182	def remove_edge ( self , id1 , id2 ) : for e in list ( self . edges ) : if id1 in ( e . node1 . id , e . node2 . id ) and id2 in ( e . node1 . id , e . node2 . id ) : e . node1 . links . remove ( e . node2 ) e . node2 . links . remove ( e . node1 ) self . edges . remove ( e )
11759	def is_variable ( x ) : "A variable is an Expr with no args and a lowercase symbol as the op." return isinstance ( x , Expr ) and not x . args and is_var_symbol ( x . op )
13595	def confirm ( tag ) : click . echo ( ) if click . confirm ( 'Do you want to create the tag {tag}?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True , abort = True ) : git . create_tag ( tag ) if click . confirm ( 'Do you want to push the tag {tag} into the upstream?' . format ( tag = click . style ( str ( tag ) , fg = 'yellow' ) ) , default = True ) : git . push_tag ( tag ) click . echo ( 'Done!' ) else : git . delete_tag ( tag ) click . echo ( 'Aborted!' )
7963	def _feed_reader ( self , data ) : IN_LOGGER . debug ( "IN: %r" , data ) if data : self . lock . release ( ) try : self . _reader . feed ( data ) finally : self . lock . acquire ( ) else : self . _eof = True self . lock . release ( ) try : self . _stream . stream_eof ( ) finally : self . lock . acquire ( ) if not self . _serializer : if self . _state != "closed" : self . event ( DisconnectedEvent ( self . _dst_addr ) ) self . _set_state ( "closed" )
5232	def all_files ( path_name , keyword = '' , ext = '' , full_path = True , has_date = False , date_fmt = DATE_FMT ) -> list : if not os . path . exists ( path = path_name ) : return [ ] path_name = path_name . replace ( '\\' , '/' ) if keyword or ext : keyword = f'*{keyword}*' if keyword else '*' if not ext : ext = '*' files = sort_by_modified ( [ f . replace ( '\\' , '/' ) for f in glob . iglob ( f'{path_name}/{keyword}.{ext}' ) if os . path . isfile ( f ) and ( f . replace ( '\\' , '/' ) . split ( '/' ) [ - 1 ] [ 0 ] != '~' ) ] ) else : files = sort_by_modified ( [ f'{path_name}/{f}' for f in os . listdir ( path = path_name ) if os . path . isfile ( f'{path_name}/{f}' ) and ( f [ 0 ] != '~' ) ] ) if has_date : files = filter_by_dates ( files , date_fmt = date_fmt ) return files if full_path else [ f . split ( '/' ) [ - 1 ] for f in files ]
2512	def handle_pkg_lic ( self , p_term , predicate , builder_func ) : try : for _ , _ , licenses in self . graph . triples ( ( p_term , predicate , None ) ) : if ( licenses , RDF . type , self . spdx_namespace [ 'ConjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_conjunctive_list ( licenses ) builder_func ( self . doc , lics ) elif ( licenses , RDF . type , self . spdx_namespace [ 'DisjunctiveLicenseSet' ] ) in self . graph : lics = self . handle_disjunctive_list ( licenses ) builder_func ( self . doc , lics ) else : try : lics = self . handle_lics ( licenses ) builder_func ( self . doc , lics ) except SPDXValueError : self . value_error ( 'PKG_SINGLE_LICS' , licenses ) except CardinalityError : self . more_than_one_error ( 'package {0}' . format ( predicate ) )
13865	def fromts ( ts , tzin = None , tzout = None ) : if ts is None : return None when = datetime . utcfromtimestamp ( ts ) . replace ( tzinfo = tzin or utc ) return totz ( when , tzout )
5198	def GetApplicationIIN ( self ) : application_iin = opendnp3 . ApplicationIIN ( ) application_iin . configCorrupt = False application_iin . deviceTrouble = False application_iin . localControl = False application_iin . needTime = False iin_field = application_iin . ToIIN ( ) _log . debug ( 'OutstationApplication.GetApplicationIIN: IINField LSB={}, MSB={}' . format ( iin_field . LSB , iin_field . MSB ) ) return application_iin
928	def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )
1454	def add_data_tuple ( self , stream_id , new_data_tuple , tuple_size_in_bytes ) : if ( self . current_data_tuple_set is None ) or ( self . current_data_tuple_set . stream . id != stream_id ) or ( len ( self . current_data_tuple_set . tuples ) >= self . data_tuple_set_capacity ) or ( self . current_data_tuple_size_in_bytes >= self . max_data_tuple_size_in_bytes ) : self . _init_new_data_tuple ( stream_id ) added_tuple = self . current_data_tuple_set . tuples . add ( ) added_tuple . CopyFrom ( new_data_tuple ) self . current_data_tuple_size_in_bytes += tuple_size_in_bytes self . total_data_emitted_in_bytes += tuple_size_in_bytes
8419	def is_close_to_int ( x ) : if not np . isfinite ( x ) : return False return abs ( x - nearest_int ( x ) ) < 1e-10
2913	def get_state_name ( self ) : state_name = [ ] for state , name in list ( self . state_names . items ( ) ) : if self . _has_state ( state ) : state_name . append ( name ) return '|' . join ( state_name )
7148	def with_payment_id ( self , payment_id = 0 ) : payment_id = numbers . PaymentID ( payment_id ) if not payment_id . is_short ( ) : raise TypeError ( "Payment ID {0} has more than 64 bits and cannot be integrated" . format ( payment_id ) ) prefix = 54 if self . is_testnet ( ) else 25 if self . is_stagenet ( ) else 19 data = bytearray ( [ prefix ] ) + self . _decoded [ 1 : 65 ] + struct . pack ( '>Q' , int ( payment_id ) ) checksum = bytearray ( keccak_256 ( data ) . digest ( ) [ : 4 ] ) return IntegratedAddress ( base58 . encode ( hexlify ( data + checksum ) ) )
771	def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( "Invalid inference element for metric spec: %r" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )
5409	def _validate_ram ( ram_in_mb ) : return int ( GoogleV2CustomMachine . _MEMORY_MULTIPLE * math . ceil ( ram_in_mb / GoogleV2CustomMachine . _MEMORY_MULTIPLE ) )
12880	def _fill ( self , size ) : try : for i in range ( size ) : self . buffer . append ( self . source . next ( ) ) except StopIteration : self . buffer . append ( ( EndOfFile , EndOfFile ) ) self . len = len ( self . buffer )
5859	def __prune_search_template ( self , extract_as_keys , search_template ) : data = { "extract_as_keys" : extract_as_keys , "search_template" : search_template } failure_message = "Failed to prune a search template" return self . _get_success_json ( self . _post_json ( 'v1/search_templates/prune-to-extract-as' , data , failure_message = failure_message ) ) [ 'data' ]
108	def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( "All images provided to draw_grid() must have the same dtype, " + "found %d dtypes (%s)" ) % ( nb_dtypes , ", " . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , "All images are expected to have the same number of channels, " + "but got channel set %s with length %d instead." % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid
13621	def one ( func , n = 0 ) : def _one ( result ) : if _isSequenceTypeNotText ( result ) and len ( result ) > n : return func ( result [ n ] ) return None return maybe ( _one )
2086	def get ( self , pk ) : try : return next ( s for s in self . list ( ) [ 'results' ] if s [ 'id' ] == pk ) except StopIteration : raise exc . NotFound ( 'The requested object could not be found.' )
12036	def dictFlat ( l ) : if type ( l ) is dict : return [ l ] if "numpy" in str ( type ( l ) ) : return l dicts = [ ] for item in l : if type ( item ) == dict : dicts . append ( item ) elif type ( item ) == list : for item2 in item : dicts . append ( item2 ) return dicts
7908	def __presence_error ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_presence ( stanza ) return True
11075	def set ( self , user ) : self . log . info ( "Loading user information for %s/%s" , user . id , user . username ) self . load_user_info ( user ) self . log . info ( "Loading user rights for %s/%s" , user . id , user . username ) self . load_user_rights ( user ) self . log . info ( "Added user: %s/%s" , user . id , user . username ) self . _add_user_to_cache ( user ) return user
12151	def html_single_basic ( self , abfID , launch = False , overwrite = False ) : if type ( abfID ) is str : abfID = [ abfID ] for thisABFid in cm . abfSort ( abfID ) : parentID = cm . parent ( self . groups , thisABFid ) saveAs = os . path . abspath ( "%s/%s_basic.html" % ( self . folder2 , parentID ) ) if overwrite is False and os . path . basename ( saveAs ) in self . files2 : continue filesByType = cm . filesByType ( self . groupFiles [ parentID ] ) html = "" html += '<div style="background-color: #DDDDDD;">' html += '<span class="title">summary of data from: %s</span></br>' % parentID html += '<code>%s</code>' % os . path . abspath ( self . folder1 + "/" + parentID + ".abf" ) html += '</div>' catOrder = [ "experiment" , "plot" , "tif" , "other" ] categories = cm . list_order_by ( filesByType . keys ( ) , catOrder ) for category in [ x for x in categories if len ( filesByType [ x ] ) ] : if category == 'experiment' : html += "<h3>Experimental Data:</h3>" elif category == 'plot' : html += "<h3>Intrinsic Properties:</h3>" elif category == 'tif' : html += "<h3>Micrographs:</h3>" elif category == 'other' : html += "<h3>Additional Files:</h3>" else : html += "<h3>????:</h3>" for fname in filesByType [ category ] : html += self . htmlFor ( fname ) html += '<br>' * 3 print ( "creating" , saveAs , '...' ) style . save ( html , saveAs , launch = launch )
5684	def day_start_ut ( self , ut ) : old_tz = self . set_current_process_time_zone ( ) ut = time . mktime ( time . localtime ( ut ) [ : 3 ] + ( 12 , 00 , 0 , 0 , 0 , - 1 ) ) - 43200 set_process_timezone ( old_tz ) return ut
9209	def get_codec ( bytes_ ) : prefix = extract_prefix ( bytes_ ) try : return CODE_TABLE [ prefix ] except KeyError : raise ValueError ( 'Prefix {} not present in the lookup table' . format ( prefix ) )
13538	def get_location ( self , location_id ) : url = "/2/locations/%s" % location_id return self . location_from_json ( self . _get_resource ( url ) [ "location" ] )
8346	def findAll ( self , name = None , attrs = { } , recursive = True , text = None , limit = None , ** kwargs ) : generator = self . recursiveChildGenerator if not recursive : generator = self . childGenerator return self . _findAll ( name , attrs , text , limit , generator , ** kwargs )
446	def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask
2211	def inject_method ( self , func , name = None ) : new_method = func . __get__ ( self , self . __class__ ) if name is None : name = func . __name__ setattr ( self , name , new_method )
8882	def predict_proba ( self , X ) : check_is_fitted ( self , [ 'inverse_influence_matrix' ] ) X = check_array ( X ) return self . __find_leverages ( X , self . inverse_influence_matrix )
3379	def add_lp_feasibility ( model ) : obj_vars = [ ] prob = model . problem for met in model . metabolites : s_plus = prob . Variable ( "s_plus_" + met . id , lb = 0 ) s_minus = prob . Variable ( "s_minus_" + met . id , lb = 0 ) model . add_cons_vars ( [ s_plus , s_minus ] ) model . constraints [ met . id ] . set_linear_coefficients ( { s_plus : 1.0 , s_minus : - 1.0 } ) obj_vars . append ( s_plus ) obj_vars . append ( s_minus ) model . objective = prob . Objective ( Zero , sloppy = True , direction = "min" ) model . objective . set_linear_coefficients ( { v : 1.0 for v in obj_vars } )
4039	def _cache ( self , response , key ) : thetime = datetime . datetime . utcnow ( ) . replace ( tzinfo = pytz . timezone ( "GMT" ) ) self . templates [ key ] = { "tmplt" : response . json ( ) , "updated" : thetime } return copy . deepcopy ( response . json ( ) )
5644	def can_infect ( self , event ) : if event . from_stop_I != self . stop_I : return False if not self . has_been_visited ( ) : return False else : time_sep = event . dep_time_ut - self . get_min_visit_time ( ) if ( time_sep >= self . min_transfer_time ) or ( event . trip_I == - 1 and time_sep >= 0 ) : return True else : for visit in self . visit_events : if ( event . trip_I == visit . trip_I ) and ( time_sep >= 0 ) : return True return False
8874	def _touch ( fname , mode = 0o666 , dir_fd = None , ** kwargs ) : flags = os . O_CREAT | os . O_APPEND with os . fdopen ( os . open ( fname , flags = flags , mode = mode , dir_fd = dir_fd ) ) as f : os . utime ( f . fileno ( ) if os . utime in os . supports_fd else fname , dir_fd = None if os . supports_fd else dir_fd , ** kwargs , )
7773	def _quote ( data ) : data = data . replace ( b'\\' , b'\\\\' ) data = data . replace ( b'"' , b'\\"' ) return data
2338	def dagify_min_edge ( g ) : while not nx . is_directed_acyclic_graph ( g ) : cycle = next ( nx . simple_cycles ( g ) ) scores = [ ] edges = [ ] for i , j in zip ( cycle [ : 1 ] , cycle [ : 1 ] ) : edges . append ( ( i , j ) ) scores . append ( g [ i ] [ j ] [ 'weight' ] ) i , j = edges [ scores . index ( min ( scores ) ) ] gc = deepcopy ( g ) gc . remove_edge ( i , j ) gc . add_edge ( j , i ) if len ( list ( nx . simple_cycles ( gc ) ) ) < len ( list ( nx . simple_cycles ( g ) ) ) : g . add_edge ( j , i , weight = min ( scores ) ) g . remove_edge ( i , j ) return g
9949	def new_space ( self , name = None , bases = None , formula = None , * , refs = None , source = None , is_derived = False , prefix = "" ) : from modelx . core . space import StaticSpaceImpl if name is None : name = self . spacenamer . get_next ( self . namespace , prefix ) if name in self . namespace : raise ValueError ( "Name '%s' already exists." % name ) if not prefix and not is_valid_name ( name ) : raise ValueError ( "Invalid name '%s'." % name ) space = self . _new_space ( name = name , formula = formula , refs = refs , source = source , is_derived = is_derived , ) self . _set_space ( space ) self . model . spacegraph . add_space ( space ) if bases is not None : if isinstance ( bases , StaticSpaceImpl ) : bases = [ bases ] space . add_bases ( bases ) return space
12400	def add ( self , requirements , required = None ) : if isinstance ( requirements , RequirementsManager ) : requirements = list ( requirements ) elif not isinstance ( requirements , list ) : requirements = [ requirements ] for req in requirements : name = req . project_name if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req , required = required ) elif required is not None : req . required = required add = True if name in self . requirements : for existing_req in self . requirements [ name ] : if req == existing_req : add = False break replace = False if ( req . specs and req . specs [ 0 ] [ 0 ] == '==' and existing_req . specs and existing_req . specs [ 0 ] [ 0 ] == '==' ) : if pkg_resources . parse_version ( req . specs [ 0 ] [ 1 ] ) < pkg_resources . parse_version ( existing_req . specs [ 0 ] [ 1 ] ) : req . requirement = existing_req . requirement replace = True if not ( req . specs and existing_req . specs ) : if existing_req . specs : req . requirement = existing_req . requirement replace = True if replace : req . required |= existing_req . required if existing_req . required_by and not req . required_by : req . required_by = existing_req . required_by self . requirements [ name ] . remove ( existing_req ) break if add : self . requirements [ name ] . append ( req )
8328	def _lastRecursiveChild ( self ) : "Finds the last element beneath this object to be parsed." lastChild = self while hasattr ( lastChild , 'contents' ) and lastChild . contents : lastChild = lastChild . contents [ - 1 ] return lastChild
2238	def _extension_module_tags ( ) : import sysconfig tags = [ ] if six . PY2 : multiarch = sysconfig . get_config_var ( 'MULTIARCH' ) if multiarch is not None : tags . append ( multiarch ) else : tags . append ( sysconfig . get_config_var ( 'SOABI' ) ) tags . append ( 'abi3' ) tags = [ t for t in tags if t ] return tags
1289	def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments
12218	def _bind_args ( sig , param_matchers , args , kwargs ) : bound = sig . bind ( * args , ** kwargs ) if not all ( param_matcher ( bound . arguments [ param_name ] ) for param_name , param_matcher in param_matchers ) : raise TypeError return bound
3421	def model_to_pymatbridge ( model , variable_name = "model" , matlab = None ) : if scipy_sparse is None : raise ImportError ( "`model_to_pymatbridge` requires scipy!" ) if matlab is None : from IPython import get_ipython matlab = get_ipython ( ) . magics_manager . registry [ "MatlabMagics" ] . Matlab model_info = create_mat_dict ( model ) S = model_info [ "S" ] . todok ( ) model_info [ "S" ] = 0 temp_S_name = "cobra_pymatbridge_temp_" + uuid4 ( ) . hex _check ( matlab . set_variable ( variable_name , model_info ) ) _check ( matlab . set_variable ( temp_S_name , S ) ) _check ( matlab . run_code ( "%s.S = %s;" % ( variable_name , temp_S_name ) ) ) for i in model_info . keys ( ) : if i == "S" : continue _check ( matlab . run_code ( "{0}.{1} = {0}.{1}';" . format ( variable_name , i ) ) ) _check ( matlab . run_code ( "clear %s;" % temp_S_name ) )
7231	def create_from_wkt ( self , wkt , item_type , ingest_source , ** attributes ) : geojson = load_wkt ( wkt ) . __geo_interface__ vector = { 'type' : "Feature" , 'geometry' : geojson , 'properties' : { 'item_type' : item_type , 'ingest_source' : ingest_source , 'attributes' : attributes } } return self . create ( vector ) [ 0 ]
4265	def serve ( destination , port , config ) : if os . path . exists ( destination ) : pass elif os . path . exists ( config ) : settings = read_settings ( config ) destination = settings . get ( 'destination' ) if not os . path . exists ( destination ) : sys . stderr . write ( "The '{}' directory doesn't exist, maybe try " "building first?\n" . format ( destination ) ) sys . exit ( 1 ) else : sys . stderr . write ( "The {destination} directory doesn't exist " "and the config file ({config}) could not be read.\n" . format ( destination = destination , config = config ) ) sys . exit ( 2 ) print ( 'DESTINATION : {}' . format ( destination ) ) os . chdir ( destination ) Handler = server . SimpleHTTPRequestHandler httpd = socketserver . TCPServer ( ( "" , port ) , Handler , False ) print ( " * Running on http://127.0.0.1:{}/" . format ( port ) ) try : httpd . allow_reuse_address = True httpd . server_bind ( ) httpd . server_activate ( ) httpd . serve_forever ( ) except KeyboardInterrupt : print ( '\nAll done!' )
3121	def make_signed_jwt ( signer , payload , key_id = None ) : header = { 'typ' : 'JWT' , 'alg' : 'RS256' } if key_id is not None : header [ 'kid' ] = key_id segments = [ _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( header ) ) , _helpers . _urlsafe_b64encode ( _helpers . _json_encode ( payload ) ) , ] signing_input = b'.' . join ( segments ) signature = signer . sign ( signing_input ) segments . append ( _helpers . _urlsafe_b64encode ( signature ) ) logger . debug ( str ( segments ) ) return b'.' . join ( segments )
8789	def _pop ( self , model ) : tags = [ ] for tag in model . tags : if self . is_tag ( tag ) : tags . append ( tag ) if tags : for tag in tags : model . tags . remove ( tag ) return tags
6581	def play ( self , song ) : self . _callbacks . play ( song ) self . _load_track ( song ) time . sleep ( 2 ) while True : try : self . _callbacks . pre_poll ( ) self . _ensure_started ( ) self . _loop_hook ( ) readers , _ , _ = select . select ( self . _get_select_readers ( ) , [ ] , [ ] , 1 ) for handle in readers : if handle . fileno ( ) == self . _control_fd : self . _callbacks . input ( handle . readline ( ) . strip ( ) , song ) else : value = self . _read_from_process ( handle ) if self . _player_stopped ( value ) : return finally : self . _callbacks . post_poll ( )
11917	def render ( template , ** data ) : try : return renderer . render ( template , ** data ) except JinjaTemplateNotFound as e : logger . error ( e . __doc__ + ', Template: %r' % template ) sys . exit ( e . exit_code )
2057	def TBH ( cpu , dest ) : base_addr = dest . get_mem_base_addr ( ) if dest . mem . base in ( 'PC' , 'R15' ) : base_addr = cpu . PC offset = cpu . read_int ( base_addr + dest . get_mem_offset ( ) , 16 ) offset = Operators . ZEXTEND ( offset , cpu . address_bit_size ) cpu . PC += ( offset << 1 )
2016	def _store ( self , offset , value , size = 1 ) : self . memory . write_BE ( offset , value , size ) for i in range ( size ) : self . _publish ( 'did_evm_write_memory' , offset + i , Operators . EXTRACT ( value , ( size - i - 1 ) * 8 , 8 ) )
4073	def eval_environ ( value ) : def eval_environ_str ( value ) : parts = value . split ( ';' ) if len ( parts ) < 2 : return value expr = parts [ 1 ] . lstrip ( ) if not re . match ( "^((\\w+(\\.\\w+)?|'.*?'|\".*?\")\\s+" '(in|==|!=|not in)\\s+' "(\\w+(\\.\\w+)?|'.*?'|\".*?\")" '(\\s+(or|and)\\s+)?)+$' , expr ) : raise ValueError ( 'bad environment marker: %r' % expr ) expr = re . sub ( r"(platform\.\w+)" , r"\1()" , expr ) return parts [ 0 ] if eval ( expr ) else '' if isinstance ( value , list ) : new_value = [ ] for element in value : element = eval_environ_str ( element ) if element : new_value . append ( element ) elif isinstance ( value , str ) : new_value = eval_environ_str ( value ) else : new_value = value return new_value
7628	def namespace ( ns_key ) : if ns_key not in __NAMESPACE__ : raise NamespaceError ( 'Unknown namespace: {:s}' . format ( ns_key ) ) sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservation' ] ) for key in [ 'value' , 'confidence' ] : try : sch [ 'properties' ] [ key ] = __NAMESPACE__ [ ns_key ] [ key ] except KeyError : pass return sch
8993	def folder ( self , folder ) : result = [ ] for root , _ , files in os . walk ( folder ) : for file in files : path = os . path . join ( root , file ) if self . _chooses_path ( path ) : result . append ( self . path ( path ) ) return result
5854	def create_dataset ( self , name = None , description = None , public = False ) : data = { "public" : _convert_bool_to_public_value ( public ) } if name : data [ "name" ] = name if description : data [ "description" ] = description dataset = { "dataset" : data } failure_message = "Unable to create dataset" result = self . _get_success_json ( self . _post_json ( routes . create_dataset ( ) , dataset , failure_message = failure_message ) ) return _dataset_from_response_dict ( result )
8640	def retract_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'retract' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotRetractedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9304	def parse_date ( date_str ) : months = [ 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' ] formats = { r'^(?:\w{3}, )?(\d{2}) (\w{3}) (\d{4})\D.*$' : lambda m : '{}-{:02d}-{}' . format ( m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w+day, (\d{2})-(\w{3})-(\d{2})\D.*$' : lambda m : '{}{}-{:02d}-{}' . format ( str ( datetime . date . today ( ) . year ) [ : 2 ] , m . group ( 3 ) , months . index ( m . group ( 2 ) . lower ( ) ) + 1 , m . group ( 1 ) ) , r'^\w{3} (\w{3}) (\d{1,2}) \d{2}:\d{2}:\d{2} (\d{4})$' : lambda m : '{}-{:02d}-{:02d}' . format ( m . group ( 3 ) , months . index ( m . group ( 1 ) . lower ( ) ) + 1 , int ( m . group ( 2 ) ) ) , r'^(\d{4})(\d{2})(\d{2})T\d{6}Z$' : lambda m : '{}-{}-{}' . format ( * m . groups ( ) ) , r'^(\d{4}-\d{2}-\d{2})(?:[Tt].*)?$' : lambda m : m . group ( 1 ) , } out_date = None for regex , xform in formats . items ( ) : m = re . search ( regex , date_str ) if m : out_date = xform ( m ) break if out_date is None : raise DateFormatError else : return out_date
4272	def create_output_directories ( self ) : check_or_create_dir ( self . dst_path ) if self . medias : check_or_create_dir ( join ( self . dst_path , self . settings [ 'thumb_dir' ] ) ) if self . medias and self . settings [ 'keep_orig' ] : self . orig_path = join ( self . dst_path , self . settings [ 'orig_dir' ] ) check_or_create_dir ( self . orig_path )
5072	def get_configuration_value ( val_name , default = None , ** kwargs ) : if kwargs . get ( 'type' ) == 'url' : return get_url ( val_name ) or default if callable ( get_url ) else default return configuration_helpers . get_value ( val_name , default , ** kwargs ) if configuration_helpers else default
1476	def _get_ckptmgr_process ( self ) : ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , '-Xms%dM' % ckptmgr_ram_mb , '-Xmx%dM' % ckptmgr_ram_mb , '-XX:+PrintCommandLineFlags' , '-verbosegc' , '-XX:+PrintGCDetails' , '-XX:+PrintGCTimeStamps' , '-XX:+PrintGCDateStamps' , '-XX:+PrintGCCause' , '-XX:+UseGCLogFileRotation' , '-XX:NumberOfGCLogFiles=5' , '-XX:GCLogFileSize=100M' , '-XX:+PrintPromotionFailure' , '-XX:+PrintTenuringDistribution' , '-XX:+PrintHeapAtGC' , '-XX:+HeapDumpOnOutOfMemoryError' , '-XX:+UseConcMarkSweepGC' , '-XX:+UseConcMarkSweepGC' , '-Xloggc:log-files/gc.ckptmgr.log' , '-Djava.net.preferIPv4Stack=true' , '-cp' , self . checkpoint_manager_classpath , ckptmgr_main_class , '-t' + self . topology_name , '-i' + self . topology_id , '-c' + self . ckptmgr_ids [ self . shard ] , '-p' + self . checkpoint_manager_port , '-f' + self . stateful_config_file , '-o' + self . override_config_file , '-g' + self . heron_internals_config_file ] retval = { } retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) return retval
4017	def get_lib_volume_mounts ( base_lib_name , assembled_specs ) : volumes = [ _get_lib_repo_volume_mount ( assembled_specs [ 'libs' ] [ base_lib_name ] ) ] volumes . append ( get_command_files_volume_mount ( base_lib_name , test = True ) ) for lib_name in assembled_specs [ 'libs' ] [ base_lib_name ] [ 'depends' ] [ 'libs' ] : lib_spec = assembled_specs [ 'libs' ] [ lib_name ] volumes . append ( _get_lib_repo_volume_mount ( lib_spec ) ) return volumes
13214	def available ( self , timeout = 5 ) : host = self . _connect_args [ 'host' ] port = self . _connect_args [ 'port' ] try : sock = socket . create_connection ( ( host , port ) , timeout = timeout ) sock . close ( ) return True except socket . error : pass return False
13432	def admin_link_move_down ( obj , link_text = 'down' ) : if obj . rank == obj . grouped_filter ( ) . count ( ) : return '' content_type = ContentType . objects . get_for_model ( obj ) link = reverse ( 'awl-rankedmodel-move' , args = ( content_type . id , obj . id , obj . rank + 1 ) ) return '<a href="%s">%s</a>' % ( link , link_text )
3378	def assert_optimal ( model , message = 'optimization failed' ) : status = model . solver . status if status != OPTIMAL : exception_cls = OPTLANG_TO_EXCEPTIONS_DICT . get ( status , OptimizationError ) raise exception_cls ( "{} ({})" . format ( message , status ) )
13158	def insert ( cls , cur , table : str , values : dict ) : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) query = cls . _insert_string . format ( table , keys , value_place_holder [ : - 1 ] ) yield from cur . execute ( query , tuple ( values . values ( ) ) ) return ( yield from cur . fetchone ( ) )
5323	def _interrupt_read ( self ) : data = self . _device . read ( ENDPOINT , REQ_INT_LEN , timeout = TIMEOUT ) LOGGER . debug ( 'Read data: %r' , data ) return data
13445	def create_admin ( username = 'admin' , email = 'admin@admin.com' , password = 'admin' ) : admin = User . objects . create_user ( username , email , password ) admin . is_staff = True admin . is_superuser = True admin . save ( ) return admin
6850	def initrole ( self , check = True ) : if self . env . original_user is None : self . env . original_user = self . genv . user if self . env . original_key_filename is None : self . env . original_key_filename = self . genv . key_filename host_string = None user = None password = None if self . env . login_check : host_string , user , password = self . find_working_password ( usernames = [ self . genv . user , self . env . default_user ] , host_strings = [ self . genv . host_string , self . env . default_hostname ] , ) if self . verbose : print ( 'host.initrole.host_string:' , host_string ) print ( 'host.initrole.user:' , user ) print ( 'host.initrole.password:' , password ) needs = False if host_string is not None : self . genv . host_string = host_string if user is not None : self . genv . user = user if password is not None : self . genv . password = password if not needs : return assert self . env . default_hostname , 'No default hostname set.' assert self . env . default_user , 'No default user set.' self . genv . host_string = self . env . default_hostname if self . env . default_hosts : self . genv . hosts = self . env . default_hosts else : self . genv . hosts = [ self . env . default_hostname ] self . genv . user = self . env . default_user self . genv . password = self . env . default_password self . genv . key_filename = self . env . default_key_filename self . purge_keys ( ) for task_name in self . env . post_initrole_tasks : if self . verbose : print ( 'Calling post initrole task %s' % task_name ) satchel_name , method_name = task_name . split ( '.' ) satchel = self . get_satchel ( name = satchel_name ) getattr ( satchel , method_name ) ( ) print ( '^' * 80 ) print ( 'host.initrole.host_string:' , self . genv . host_string ) print ( 'host.initrole.user:' , self . genv . user ) print ( 'host.initrole.password:' , self . genv . password )
5885	def get_canonical_link ( self ) : if self . article . final_url : kwargs = { 'tag' : 'link' , 'attr' : 'rel' , 'value' : 'canonical' } meta = self . parser . getElementsByTag ( self . article . doc , ** kwargs ) if meta is not None and len ( meta ) > 0 : href = self . parser . getAttribute ( meta [ 0 ] , 'href' ) if href : href = href . strip ( ) o = urlparse ( href ) if not o . hostname : tmp = urlparse ( self . article . final_url ) domain = '%s://%s' % ( tmp . scheme , tmp . hostname ) href = urljoin ( domain , href ) return href return self . article . final_url
11537	def set_pin_direction ( self , pin , direction ) : if type ( pin ) is list : for p in pin : self . set_pin_direction ( p , direction ) return pin_id = self . _pin_mapping . get ( pin , None ) if pin_id and type ( direction ) is ahio . Direction : self . _set_pin_direction ( pin_id , direction ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
3177	def create ( self , list_id , data ) : self . list_id = list_id if 'name' not in data : raise KeyError ( 'The list merge field must have a name' ) if 'type' not in data : raise KeyError ( 'The list merge field must have a type' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'merge-fields' ) , data = data ) if response is not None : self . merge_id = response [ 'merge_id' ] else : self . merge_id = None return response
5633	def find_sections ( lines ) : sections = [ ] for line in lines : if is_heading ( line ) : sections . append ( get_heading ( line ) ) return sections
12900	def get_equalisers ( self ) : if not self . __equalisers : self . __equalisers = yield from self . handle_list ( self . API . get ( 'equalisers' ) ) return self . __equalisers
4211	def compatible_staticpath ( path ) : if VERSION >= ( 1 , 10 ) : return path try : from django . templatetags . static import static return static ( path ) except ImportError : pass try : return '%s/%s' % ( settings . STATIC_URL . rstrip ( '/' ) , path ) except AttributeError : pass try : return '%s/%s' % ( settings . PAGEDOWN_URL . rstrip ( '/' ) , path ) except AttributeError : pass return '%s/%s' % ( settings . MEDIA_URL . rstrip ( '/' ) , path )
10493	def clickMouseButtonLeft ( self , coord , interval = None ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) if interval : self . _postQueuedEvents ( interval = interval ) else : self . _postQueuedEvents ( )
10129	def _map_timezones ( ) : tz_map = { } todo = HAYSTACK_TIMEZONES_SET . copy ( ) for full_tz in pytz . all_timezones : if not bool ( todo ) : break if full_tz in todo : tz_map [ full_tz ] = full_tz todo . discard ( full_tz ) continue if '/' not in full_tz : continue ( prefix , suffix ) = full_tz . split ( '/' , 1 ) if '/' in suffix : continue if suffix in todo : tz_map [ suffix ] = full_tz todo . discard ( suffix ) continue return tz_map
12808	def fetch ( self ) : try : if not self . _last_message_id : messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "limit" : 1 } ) self . _last_message_id = messages [ - 1 ] [ "id" ] messages = self . _connection . get ( "room/%s/recent" % self . _room_id , key = "messages" , parameters = { "since_message_id" : self . _last_message_id } ) except : messages = [ ] if messages : self . _last_message_id = messages [ - 1 ] [ "id" ] self . received ( messages )
7146	def to_atomic ( amount ) : if not isinstance ( amount , ( Decimal , float ) + _integer_types ) : raise ValueError ( "Amount '{}' doesn't have numeric type. Only Decimal, int, long and " "float (not recommended) are accepted as amounts." ) return int ( amount * 10 ** 12 )
9554	def _apply_record_length_checks ( self , i , r , summarize = False , context = None ) : for code , message , modulus in self . _record_length_checks : if i % modulus == 0 : if len ( r ) != len ( self . _field_names ) : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'length' ] = len ( r ) if context is not None : p [ 'context' ] = context yield p
5572	def is_valid_with_config ( self , config ) : validate_values ( config , [ ( "schema" , dict ) , ( "path" , str ) ] ) validate_values ( config [ "schema" ] , [ ( "properties" , dict ) , ( "geometry" , str ) ] ) if config [ "schema" ] [ "geometry" ] not in [ "Geometry" , "Point" , "MultiPoint" , "Line" , "MultiLine" , "Polygon" , "MultiPolygon" ] : raise TypeError ( "invalid geometry type" ) return True
9062	def fix ( self , param ) : if param == "delta" : super ( ) . _fix ( "logistic" ) else : self . _fix [ param ] = True
1129	def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ "newline" ] ) def rule ( parser ) : result = parser . _accept ( "newline" ) if result is unmatched : return result return [ ] return rule
5836	def __get_ml_configuration_status ( self , job_id ) : failure_message = "Get status on ml configuration failed" response = self . _get_success_json ( self . _get ( 'v1/descriptors/builders/simple/default/' + job_id + '/status' , None , failure_message = failure_message ) ) [ 'data' ] return response
9862	async def update_info ( self , * _ ) : query = gql ( ) res = await self . _execute ( query ) if res is None : return errors = res . get ( "errors" , [ ] ) if errors : msg = errors [ 0 ] . get ( "message" , "failed to login" ) _LOGGER . error ( msg ) raise InvalidLogin ( msg ) data = res . get ( "data" ) if not data : return viewer = data . get ( "viewer" ) if not viewer : return self . _name = viewer . get ( "name" ) homes = viewer . get ( "homes" , [ ] ) self . _home_ids = [ ] for _home in homes : home_id = _home . get ( "id" ) self . _all_home_ids += [ home_id ] subs = _home . get ( "subscriptions" ) if subs : status = subs [ 0 ] . get ( "status" , "ended" ) . lower ( ) if not home_id or status != "running" : continue self . _home_ids += [ home_id ]
11744	def goto ( self , rules , symbol ) : return self . closure ( { rule . move_dot ( ) for rule in rules if not rule . at_end and rule . rhs [ rule . pos ] == symbol } , )
8717	def file_compile ( self , path ) : log . info ( 'Compile ' + path ) cmd = 'node.compile("%s")' % path res = self . __exchange ( cmd ) log . info ( res ) return res
4991	def post ( self , request , * args , ** kwargs ) : enterprise_customer_uuid , course_run_id , course_key , program_uuid = RouterView . get_path_variables ( ** kwargs ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_customer_uuid ) if course_key : context_data = get_global_context ( request , enterprise_customer ) try : kwargs [ 'course_id' ] = RouterView . get_course_run_id ( request . user , enterprise_customer , course_key ) except Http404 : error_code = 'ENTRV001' log_message = ( 'Could not find course run with id {course_run_id} ' 'for course key {course_key} and ' 'for enterprise_customer_uuid {enterprise_customer_uuid} ' 'and program {program_uuid}. ' 'Returned error code {error_code} to user {userid}' . format ( course_key = course_key , course_run_id = course_run_id , enterprise_customer_uuid = enterprise_customer_uuid , error_code = error_code , userid = request . user . id , program_uuid = program_uuid , ) ) return render_page_with_error_code_message ( request , context_data , error_code , log_message ) return self . redirect ( request , * args , ** kwargs )
7467	def summarize_results ( self , individual_results = False ) : if ( not self . params . infer_delimit ) & ( not self . params . infer_sptree ) : if individual_results : return [ _parse_00 ( i ) for i in self . files . outfiles ] else : return pd . concat ( [ pd . read_csv ( i , sep = '\t' , index_col = 0 ) for i in self . files . mcmcfiles ] ) . describe ( ) . T if self . params . infer_delimit & ( not self . params . infer_sptree ) : return _parse_01 ( self . files . outfiles , individual = individual_results ) else : return "summary function not yet ready for this type of result"
12460	def parse_args ( args ) : from argparse import ArgumentParser description = ( 'Bootstrap Python projects and libraries with virtualenv ' 'and pip.' ) parser = ArgumentParser ( description = description ) parser . add_argument ( '--version' , action = 'version' , version = __version__ ) parser . add_argument ( '-c' , '--config' , default = DEFAULT_CONFIG , help = 'Path to config file. By default: {0}' . format ( DEFAULT_CONFIG ) ) parser . add_argument ( '-p' , '--pre-requirements' , default = [ ] , nargs = '+' , help = 'List of pre-requirements to check, separated by space.' ) parser . add_argument ( '-e' , '--env' , help = 'Virtual environment name. By default: {0}' . format ( CONFIG [ __script__ ] [ 'env' ] ) ) parser . add_argument ( '-r' , '--requirements' , help = 'Path to requirements file. By default: {0}' . format ( CONFIG [ __script__ ] [ 'requirements' ] ) ) parser . add_argument ( '-d' , '--install-dev-requirements' , action = 'store_true' , default = None , help = 'Install prefixed or suffixed "dev" requirements after ' 'installation of original requirements file or library completed ' 'without errors.' ) parser . add_argument ( '-C' , '--hook' , help = 'Execute this hook after bootstrap process.' ) parser . add_argument ( '--ignore-activated' , action = 'store_true' , default = None , help = 'Ignore pre-activated virtualenv, like on Travis CI.' ) parser . add_argument ( '--recreate' , action = 'store_true' , default = None , help = 'Recreate virtualenv on every run.' ) parser . add_argument ( '-q' , '--quiet' , action = 'store_true' , default = None , help = 'Minimize output, show only error messages.' ) return parser . parse_args ( args )
6423	def dist ( self , src , tar , word_approx_min = 0.3 , char_approx_min = 0.73 , tests = 2 ** 12 - 1 , ) : return ( synoname ( src , tar , word_approx_min , char_approx_min , tests , False ) / 14 )
8737	def get ( self , query , responseformat = "geojson" , verbosity = "body" , build = True ) : if build : full_query = self . _construct_ql_query ( query , responseformat = responseformat , verbosity = verbosity ) else : full_query = query if self . debug : logging . getLogger ( ) . info ( query ) r = self . _get_from_overpass ( full_query ) content_type = r . headers . get ( "content-type" ) if self . debug : print ( content_type ) if content_type == "text/csv" : result = [ ] reader = csv . reader ( StringIO ( r . text ) , delimiter = "\t" ) for row in reader : result . append ( row ) return result elif content_type in ( "text/xml" , "application/xml" , "application/osm3s+xml" ) : return r . text elif content_type == "application/json" : response = json . loads ( r . text ) if not build : return response if "elements" not in response : raise UnknownOverpassError ( "Received an invalid answer from Overpass." ) overpass_remark = response . get ( "remark" , None ) if overpass_remark and overpass_remark . startswith ( "runtime error" ) : raise ServerRuntimeError ( overpass_remark ) if responseformat is not "geojson" : return response return self . _as_geojson ( response [ "elements" ] )
5161	def __intermediate_addresses ( self , interface ) : address_list = self . get_copy ( interface , 'addresses' ) if not address_list : return [ { 'proto' : 'none' } ] result = [ ] static = { } dhcp = [ ] for address in address_list : family = address . get ( 'family' ) if address [ 'proto' ] == 'dhcp' : address [ 'proto' ] = 'dhcp' if family == 'ipv4' else 'dhcpv6' dhcp . append ( self . __intermediate_address ( address ) ) continue if 'gateway' in address : uci_key = 'gateway' if family == 'ipv4' else 'ip6gw' interface [ uci_key ] = address [ 'gateway' ] address_key = 'ipaddr' if family == 'ipv4' else 'ip6addr' static . setdefault ( address_key , [ ] ) static [ address_key ] . append ( '{address}/{mask}' . format ( ** address ) ) static . update ( self . __intermediate_address ( address ) ) if static : if len ( static . get ( 'ipaddr' , [ ] ) ) == 1 : network = ip_interface ( six . text_type ( static [ 'ipaddr' ] [ 0 ] ) ) static [ 'ipaddr' ] = str ( network . ip ) static [ 'netmask' ] = str ( network . netmask ) if len ( static . get ( 'ip6addr' , [ ] ) ) == 1 : static [ 'ip6addr' ] = static [ 'ip6addr' ] [ 0 ] result . append ( static ) if dhcp : result += dhcp return result
5166	def __intermediate_dns_servers ( self , uci , address ) : if 'dns' in uci : return uci [ 'dns' ] if address [ 'proto' ] in [ 'dhcp' , 'dhcpv6' , 'none' ] : return None dns = self . netjson . get ( 'dns_servers' , None ) if dns : return ' ' . join ( dns )
6992	def flare_model ( flareparams , times , mags , errs ) : ( amplitude , flare_peak_time , rise_gaussian_stdev , decay_time_constant ) = flareparams zerolevel = np . median ( mags ) modelmags = np . full_like ( times , zerolevel ) modelmags [ times < flare_peak_time ] = ( mags [ times < flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times < flare_peak_time ] - flare_peak_time ) * ( times [ times < flare_peak_time ] - flare_peak_time ) ) / ( 2.0 * rise_gaussian_stdev * rise_gaussian_stdev ) ) ) modelmags [ times > flare_peak_time ] = ( mags [ times > flare_peak_time ] + amplitude * np . exp ( - ( ( times [ times > flare_peak_time ] - flare_peak_time ) ) / ( decay_time_constant ) ) ) return modelmags , times , mags , errs
13268	def _gmlv2_to_geojson ( el ) : tag = el . tag . replace ( '{%s}' % NS_GML , '' ) if tag == 'Point' : coordinates = [ float ( c ) for c in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ',' ) ] elif tag == 'LineString' : coordinates = [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in el . findtext ( '{%s}coordinates' % NS_GML ) . split ( ' ' ) ] elif tag == 'Polygon' : coordinates = [ ] for ring in el . xpath ( 'gml:outerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) + el . xpath ( 'gml:innerBoundaryIs/gml:LinearRing/gml:coordinates' , namespaces = NSMAP ) : coordinates . append ( [ [ float ( x ) for x in pair . split ( ',' ) ] for pair in ring . text . split ( ' ' ) ] ) elif tag in ( 'MultiPoint' , 'MultiLineString' , 'MultiPolygon' , 'MultiCurve' ) : if tag == 'MultiCurve' : single_type = 'LineString' member_tag = 'curveMember' else : single_type = tag [ 5 : ] member_tag = single_type [ 0 ] . lower ( ) + single_type [ 1 : ] + 'Member' coordinates = [ gml_to_geojson ( member ) [ 'coordinates' ] for member in el . xpath ( 'gml:%s/gml:%s' % ( member_tag , single_type ) , namespaces = NSMAP ) ] else : raise NotImplementedError return { 'type' : tag , 'coordinates' : coordinates }
7555	def store_random ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) rand = np . arange ( 0 , n_choose_k ( len ( self . samples ) , 4 ) ) np . random . shuffle ( rand ) rslice = rand [ : self . params . nquartets ] rss = np . sort ( rslice ) riter = iter ( rss ) del rand , rslice print ( self . _chunksize ) rando = riter . next ( ) tmpr = np . zeros ( ( self . params . nquartets , 4 ) , dtype = np . uint16 ) tidx = 0 while 1 : try : for i , j in enumerate ( qiter ) : if i == rando : tmpr [ tidx ] = j tidx += 1 rando = riter . next ( ) if not i % self . _chunksize : print ( min ( i , self . params . nquartets ) ) except StopIteration : break fillsets [ : ] = tmpr del tmpr
8587	def attach_cdrom ( self , datacenter_id , server_id , cdrom_id ) : data = '{ "id": "' + cdrom_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/servers/%s/cdroms' % ( datacenter_id , server_id ) , method = 'POST' , data = data ) return response
4575	def hsv2rgb_360 ( hsv ) : h , s , v = hsv r , g , b = colorsys . hsv_to_rgb ( h / 360.0 , s , v ) return ( int ( r * 255.0 ) , int ( g * 255.0 ) , int ( b * 255.0 ) )
9330	def total_memory ( ) : with file ( '/proc/meminfo' , 'r' ) as f : for line in f : words = line . split ( ) if words [ 0 ] . upper ( ) == 'MEMTOTAL:' : return int ( words [ 1 ] ) * 1024 raise IOError ( 'MemTotal unknown' )
12140	def load_table ( self , table ) : items , data_keys = [ ] , None for key , filename in table . items ( ) : data_dict = self . filetype . data ( filename [ 0 ] ) current_keys = tuple ( sorted ( data_dict . keys ( ) ) ) values = [ data_dict [ k ] for k in current_keys ] if data_keys is None : data_keys = current_keys elif data_keys != current_keys : raise Exception ( "Data keys are inconsistent" ) items . append ( ( key , values ) ) return Table ( items , kdims = table . kdims , vdims = data_keys )
13876	def CopyFilesX ( file_mapping ) : files = [ ] for i_target_path , i_source_path_mask in file_mapping : tree_recurse , flat_recurse , dirname , in_filters , out_filters = ExtendedPathMask . Split ( i_source_path_mask ) _AssertIsLocal ( dirname ) filenames = FindFiles ( dirname , in_filters , out_filters , tree_recurse ) for i_source_filename in filenames : if os . path . isdir ( i_source_filename ) : continue i_target_filename = i_source_filename [ len ( dirname ) + 1 : ] if flat_recurse : i_target_filename = os . path . basename ( i_target_filename ) i_target_filename = os . path . join ( i_target_path , i_target_filename ) files . append ( ( StandardizePath ( i_source_filename ) , StandardizePath ( i_target_filename ) ) ) for i_source_filename , i_target_filename in files : target_dir = os . path . dirname ( i_target_filename ) CreateDirectory ( target_dir ) CopyFile ( i_source_filename , i_target_filename ) return files
8745	def get_floatingips ( context , filters = None , fields = None , sorts = [ 'id' ] , limit = None , marker = None , page_reverse = False ) : LOG . info ( 'get_floatingips for tenant %s filters %s fields %s' % ( context . tenant_id , filters , fields ) ) floating_ips = _get_ips_by_type ( context , ip_types . FLOATING , filters = filters , fields = fields ) return [ v . _make_floating_ip_dict ( flip ) for flip in floating_ips ]
10652	def prepare_to_run ( self , clock , period_count ) : for c in self . components : c . prepare_to_run ( clock , period_count ) for a in self . activities : a . prepare_to_run ( clock , period_count )
9655	def run_commands ( commands , settings ) : sprint = settings [ "sprint" ] quiet = settings [ "quiet" ] error = settings [ "error" ] enhanced_errors = True the_shell = None if settings [ "no_enhanced_errors" ] : enhanced_errors = False if "shell" in settings : the_shell = settings [ "shell" ] windows_p = sys . platform == "win32" STDOUT = None STDERR = None if quiet : STDOUT = PIPE STDERR = PIPE commands = commands . rstrip ( ) sprint ( "About to run commands '{}'" . format ( commands ) , level = "verbose" ) if not quiet : sprint ( commands ) if the_shell : tmp = shlex . split ( the_shell ) the_shell = tmp [ 0 ] tmp = tmp [ 1 : ] if enhanced_errors and not windows_p : tmp . append ( "-e" ) tmp . append ( commands ) commands = tmp else : if enhanced_errors and not windows_p : commands = [ "-e" , commands ] p = Popen ( commands , shell = True , stdout = STDOUT , stderr = STDERR , executable = the_shell ) out , err = p . communicate ( ) if p . returncode : if quiet : error ( err . decode ( locale . getpreferredencoding ( ) ) ) error ( "Command failed to run" ) sys . exit ( 1 )
10475	def _sendKeyWithModifiers ( self , keychr , modifiers , globally = False ) : if not self . _isSingleCharacter ( keychr ) : raise ValueError ( 'Please provide only one character to send' ) if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) modFlags = self . _pressModifiers ( modifiers , globally = globally ) self . _sendKey ( keychr , modFlags , globally = globally ) self . _releaseModifiers ( modifiers , globally = globally ) self . _postQueuedEvents ( )
10090	def rst2node ( doc_name , data ) : if not data : return parser = docutils . parsers . rst . Parser ( ) document = docutils . utils . new_document ( '<%s>' % doc_name ) document . settings = docutils . frontend . OptionParser ( ) . get_default_values ( ) document . settings . tab_width = 4 document . settings . pep_references = False document . settings . rfc_references = False document . settings . env = Env ( ) parser . parse ( data , document ) if len ( document . children ) == 1 : return document . children [ 0 ] else : par = docutils . nodes . paragraph ( ) for child in document . children : par += child return par
10940	def calc_J ( self ) : del self . J self . J = np . zeros ( [ self . param_vals . size , self . data . size ] ) dp = np . zeros_like ( self . param_vals ) f0 = self . model . copy ( ) for a in range ( self . param_vals . size ) : dp *= 0 dp [ a ] = self . dl [ a ] f1 = self . func ( self . param_vals + dp , * self . func_args , ** self . func_kwargs ) grad_func = ( f1 - f0 ) / dp [ a ] self . J [ a ] = - grad_func
5011	def _call_post_with_session ( self , url , payload ) : now = datetime . datetime . utcnow ( ) if now >= self . expires_at : self . session . close ( ) self . _create_session ( ) response = self . session . post ( url , data = payload ) return response . status_code , response . text
5667	def add_walk_distances_to_db_python ( gtfs , osm_path , cutoff_distance_m = 1000 ) : if isinstance ( gtfs , str ) : gtfs = GTFS ( gtfs ) assert ( isinstance ( gtfs , GTFS ) ) print ( "Reading in walk network" ) walk_network = create_walk_network_from_osm ( osm_path ) print ( "Matching stops to the OSM network" ) stop_I_to_nearest_osm_node , stop_I_to_nearest_osm_node_distance = match_stops_to_nodes ( gtfs , walk_network ) transfers = gtfs . get_straight_line_transfer_distances ( ) from_I_to_to_stop_Is = { stop_I : set ( ) for stop_I in stop_I_to_nearest_osm_node } for transfer_tuple in transfers . itertuples ( ) : from_I = transfer_tuple . from_stop_I to_I = transfer_tuple . to_stop_I from_I_to_to_stop_Is [ from_I ] . add ( to_I ) print ( "Computing walking distances" ) for from_I , to_stop_Is in from_I_to_to_stop_Is . items ( ) : from_node = stop_I_to_nearest_osm_node [ from_I ] from_dist = stop_I_to_nearest_osm_node_distance [ from_I ] shortest_paths = networkx . single_source_dijkstra_path_length ( walk_network , from_node , cutoff = cutoff_distance_m - from_dist , weight = "distance" ) for to_I in to_stop_Is : to_distance = stop_I_to_nearest_osm_node_distance [ to_I ] to_node = stop_I_to_nearest_osm_node [ to_I ] osm_distance = shortest_paths . get ( to_node , float ( 'inf' ) ) total_distance = from_dist + osm_distance + to_distance from_stop_I_transfers = transfers [ transfers [ 'from_stop_I' ] == from_I ] straigth_distance = from_stop_I_transfers [ from_stop_I_transfers [ "to_stop_I" ] == to_I ] [ "d" ] . values [ 0 ] assert ( straigth_distance < total_distance + 2 ) if total_distance <= cutoff_distance_m : gtfs . conn . execute ( "UPDATE stop_distances " "SET d_walk = " + str ( int ( total_distance ) ) + " WHERE from_stop_I=" + str ( from_I ) + " AND to_stop_I=" + str ( to_I ) ) gtfs . conn . commit ( )
7748	def _process_iq_response ( self , stanza ) : stanza_id = stanza . stanza_id from_jid = stanza . from_jid if from_jid : ufrom = from_jid . as_unicode ( ) else : ufrom = None res_handler = err_handler = None try : res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , ufrom ) ) except KeyError : logger . debug ( "No response handler for id={0!r} from={1!r}" . format ( stanza_id , ufrom ) ) logger . debug ( " from_jid: {0!r} peer: {1!r} me: {2!r}" . format ( from_jid , self . peer , self . me ) ) if ( ( from_jid == self . peer or from_jid == self . me or self . me and from_jid == self . me . bare ( ) ) ) : try : logger . debug ( " trying id={0!r} from=None" . format ( stanza_id ) ) res_handler , err_handler = self . _iq_response_handlers . pop ( ( stanza_id , None ) ) except KeyError : pass if stanza . stanza_type == "result" : if res_handler : response = res_handler ( stanza ) else : return False else : if err_handler : response = err_handler ( stanza ) else : return False self . _process_handler_result ( response ) return True
9423	def _load_metadata ( self , handle ) : rarinfo = self . _read_header ( handle ) while rarinfo : self . filelist . append ( rarinfo ) self . NameToInfo [ rarinfo . filename ] = rarinfo self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle )
5000	def require_at_least_one_query_parameter ( * query_parameter_names ) : def outer_wrapper ( view ) : @ wraps ( view ) def wrapper ( request , * args , ** kwargs ) : requirement_satisfied = False for query_parameter_name in query_parameter_names : query_parameter_values = request . query_params . getlist ( query_parameter_name ) kwargs [ query_parameter_name ] = query_parameter_values if query_parameter_values : requirement_satisfied = True if not requirement_satisfied : raise ValidationError ( detail = 'You must provide at least one of the following query parameters: {params}.' . format ( params = ', ' . join ( query_parameter_names ) ) ) return view ( request , * args , ** kwargs ) return wrapper return outer_wrapper
11422	def print_rec ( rec , format = 1 , tags = None ) : if tags is None : tags = [ ] if format == 1 : text = record_xml_output ( rec , tags ) else : return '' return text
8040	def is_public ( self ) : for decorator in self . decorators : if re . compile ( r"^{}\." . format ( self . name ) ) . match ( decorator . name ) : return False name_is_public = ( not self . name . startswith ( "_" ) or self . name in VARIADIC_MAGIC_METHODS or self . is_magic ) return self . parent . is_public and name_is_public
6195	def _get_group_randomstate ( rs , seed , group ) : if rs is None : rs = np . random . RandomState ( seed = seed ) if 'last_random_state' in group . _v_attrs : rs . set_state ( group . _v_attrs [ 'last_random_state' ] ) print ( "INFO: Random state set to last saved state in '%s'." % group . _v_name ) else : print ( "INFO: Random state initialized from seed (%d)." % seed ) return rs
9204	def path_to_node ( tree , path ) : if path is None : return None node = tree for key in path : node = child_by_key ( node , key ) return node
1896	def _is_sat ( self ) -> bool : logger . debug ( "Solver.check() " ) start = time . time ( ) self . _send ( '(check-sat)' ) status = self . _recv ( ) logger . debug ( "Check took %s seconds (%s)" , time . time ( ) - start , status ) if status not in ( 'sat' , 'unsat' , 'unknown' ) : raise SolverError ( status ) if consider_unknown_as_unsat : if status == 'unknown' : logger . info ( 'Found an unknown core, probably a solver timeout' ) status = 'unsat' if status == 'unknown' : raise SolverUnknown ( status ) return status == 'sat'
6415	def median ( nums ) : nums = sorted ( nums ) mag = len ( nums ) if mag % 2 : mag = int ( ( mag - 1 ) / 2 ) return nums [ mag ] mag = int ( mag / 2 ) med = ( nums [ mag - 1 ] + nums [ mag ] ) / 2 return med if not med . is_integer ( ) else int ( med )
7346	async def get_oauth_token ( consumer_key , consumer_secret , callback_uri = "oob" ) : client = BasePeonyClient ( consumer_key = consumer_key , consumer_secret = consumer_secret , api_version = "" , suffix = "" ) response = await client . api . oauth . request_token . post ( _suffix = "" , oauth_callback = callback_uri ) return parse_token ( response )
4638	def shared_blockchain_instance ( self ) : if not self . _sharedInstance . instance : klass = self . get_instance_class ( ) self . _sharedInstance . instance = klass ( ** self . _sharedInstance . config ) return self . _sharedInstance . instance
13111	def verbose ( cls , key = False , default = '' ) : if key is False : items = cls . _item_dict . values ( ) return [ ( x . key , x . value ) for x in sorted ( items , key = lambda x : x . sort or x . key ) ] item = cls . _item_dict . get ( key ) return item . value if item else default
3287	def _get_log ( self , limit = None ) : self . ui . pushbuffer ( ) commands . log ( self . ui , self . repo , limit = limit , date = None , rev = None , user = None ) res = self . ui . popbuffer ( ) . strip ( ) logList = [ ] for logentry in res . split ( "\n\n" ) : log = { } logList . append ( log ) for line in logentry . split ( "\n" ) : k , v = line . split ( ":" , 1 ) assert k in ( "changeset" , "tag" , "user" , "date" , "summary" ) log [ k . strip ( ) ] = v . strip ( ) log [ "parsed_date" ] = util . parse_time_string ( log [ "date" ] ) local_id , unid = log [ "changeset" ] . split ( ":" ) log [ "local_id" ] = int ( local_id ) log [ "unid" ] = unid return logList
13387	def make_upstream_request ( self ) : "Return request object for calling the upstream" url = self . upstream_url ( self . request . uri ) return tornado . httpclient . HTTPRequest ( url , method = self . request . method , headers = self . request . headers , body = self . request . body if self . request . body else None )
10844	def sent ( self ) : sent_updates = [ ] url = PATHS [ 'GET_SENT' ] % self . profile_id response = self . api . get ( url = url ) for update in response [ 'updates' ] : sent_updates . append ( Update ( api = self . api , raw_response = update ) ) self . __sent = sent_updates return self . __sent
11665	def _build_indices ( X , flann_args ) : "Builds FLANN indices for each bag." logger . info ( "Building indices..." ) indices = [ None ] * len ( X ) for i , bag in enumerate ( plog ( X , name = "index building" ) ) : indices [ i ] = idx = FLANNIndex ( ** flann_args ) idx . build_index ( bag ) return indices
1908	def forward_events_to ( self , sink , include_source = False ) : assert isinstance ( sink , Eventful ) , f'{sink.__class__.__name__} is not Eventful' self . _forwards [ sink ] = include_source
7578	def _get_evanno_table ( self , kpops , max_var_multiple , quiet ) : kpops = sorted ( kpops ) replnliks = [ ] for kpop in kpops : reps , excluded = _concat_reps ( self , kpop , max_var_multiple , quiet ) if excluded : if not quiet : sys . stderr . write ( "[K{}] {} reps excluded (not converged) see 'max_var_multiple'.\n" . format ( kpop , excluded ) ) if reps : ninds = reps [ 0 ] . inds nreps = len ( reps ) else : ninds = nreps = 0 if not reps : print "no result files found" replnliks . append ( [ i . est_lnlik for i in reps ] ) if len ( replnliks ) > 1 : lnmean = [ np . mean ( i ) for i in replnliks ] lnstds = [ np . std ( i , ddof = 1 ) for i in replnliks ] else : lnmean = replnliks lnstds = np . nan tab = pd . DataFrame ( index = kpops , data = { "Nreps" : [ len ( i ) for i in replnliks ] , "lnPK" : [ 0 ] * len ( kpops ) , "lnPPK" : [ 0 ] * len ( kpops ) , "deltaK" : [ 0 ] * len ( kpops ) , "estLnProbMean" : lnmean , "estLnProbStdev" : lnstds , } ) for kpop in kpops [ 1 : ] : tab . loc [ kpop , "lnPK" ] = tab . loc [ kpop , "estLnProbMean" ] - tab . loc [ kpop - 1 , "estLnProbMean" ] for kpop in kpops [ 1 : - 1 ] : tab . loc [ kpop , "lnPPK" ] = abs ( tab . loc [ kpop + 1 , "lnPK" ] - tab . loc [ kpop , "lnPK" ] ) tab . loc [ kpop , "deltaK" ] = ( abs ( tab . loc [ kpop + 1 , "estLnProbMean" ] - 2.0 * tab . loc [ kpop , "estLnProbMean" ] + tab . loc [ kpop - 1 , "estLnProbMean" ] ) / tab . loc [ kpop , "estLnProbStdev" ] ) return tab
12442	def require_accessibility ( self , user , method ) : if method == 'OPTIONS' : return authz = self . meta . authorization if not authz . is_accessible ( user , method , self ) : authz . unaccessible ( )
3314	def _stream_data ( self , environ , content_length , block_size ) : if content_length == 0 : _logger . info ( "PUT: Content-Length == 0. Creating empty file..." ) else : assert content_length > 0 contentremain = content_length while contentremain > 0 : n = min ( contentremain , block_size ) readbuffer = environ [ "wsgi.input" ] . read ( n ) if not len ( readbuffer ) > 0 : _logger . error ( "input.read({}) returned 0 bytes" . format ( n ) ) break environ [ "wsgidav.some_input_read" ] = 1 yield readbuffer contentremain -= len ( readbuffer ) if contentremain == 0 : environ [ "wsgidav.all_input_read" ] = 1
1567	def invoke_hook_bolt_execute ( self , heron_tuple , execute_latency_ns ) : if len ( self . task_hooks ) > 0 : bolt_execute_info = BoltExecuteInfo ( heron_tuple = heron_tuple , executing_task_id = self . get_task_id ( ) , execute_latency_ms = execute_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . bolt_execute ( bolt_execute_info )
4253	def org_by_name ( self , hostname ) : addr = self . _gethostbyname ( hostname ) return self . org_by_addr ( addr )
1481	def start_process_monitor ( self ) : Log . info ( "Start process monitor" ) while True : if len ( self . processes_to_monitor ) > 0 : ( pid , status ) = os . wait ( ) with self . process_lock : if pid in self . processes_to_monitor . keys ( ) : old_process_info = self . processes_to_monitor [ pid ] name = old_process_info . name command = old_process_info . command Log . info ( "%s (pid=%s) exited with status %d. command=%s" % ( name , pid , status , command ) ) self . _wait_process_std_out_err ( name , old_process_info . process ) if os . path . isfile ( "core.%d" % pid ) : os . system ( "chmod a+r core.%d" % pid ) if old_process_info . attempts >= self . max_runs : Log . info ( "%s exited too many times" % name ) sys . exit ( 1 ) time . sleep ( self . interval_between_runs ) p = self . _run_process ( name , command ) del self . processes_to_monitor [ pid ] self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) log_pid_for_process ( name , p . pid )
10873	def get_K ( rho , z , alpha = 1.0 , zint = 100.0 , n2n1 = 0.95 , get_hdet = False , K = 1 , Kprefactor = None , return_Kprefactor = False , npts = 20 , ** kwargs ) : if type ( rho ) != np . ndarray or type ( z ) != np . ndarray or ( rho . shape != z . shape ) : raise ValueError ( 'rho and z must be np.arrays of same shape.' ) pts , wts = np . polynomial . legendre . leggauss ( npts ) n1n2 = 1.0 / n2n1 rr = np . ravel ( rho ) zr = np . ravel ( z ) cos_theta = 0.5 * ( 1 - np . cos ( alpha ) ) * pts + 0.5 * ( 1 + np . cos ( alpha ) ) if Kprefactor is None : Kprefactor = get_Kprefactor ( z , cos_theta , zint = zint , n2n1 = n2n1 , get_hdet = get_hdet , ** kwargs ) if K == 1 : part_1 = j0 ( np . outer ( rr , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , 0.5 * ( get_taus ( cos_theta , n2n1 = n2n1 ) + get_taup ( cos_theta , n2n1 = n2n1 ) * csqrt ( 1 - n1n2 ** 2 * ( 1 - cos_theta ** 2 ) ) ) ) integrand = Kprefactor * part_1 elif K == 2 : part_2 = j2 ( np . outer ( rr , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , 0.5 * ( get_taus ( cos_theta , n2n1 = n2n1 ) - get_taup ( cos_theta , n2n1 = n2n1 ) * csqrt ( 1 - n1n2 ** 2 * ( 1 - cos_theta ** 2 ) ) ) ) integrand = Kprefactor * part_2 elif K == 3 : part_3 = j1 ( np . outer ( rho , np . sqrt ( 1 - cos_theta ** 2 ) ) ) * np . outer ( np . ones_like ( rr ) , n1n2 * get_taup ( cos_theta , n2n1 = n2n1 ) * np . sqrt ( 1 - cos_theta ** 2 ) ) integrand = Kprefactor * part_3 else : raise ValueError ( 'K=1,2,3 only...' ) big_wts = np . outer ( np . ones_like ( rr ) , wts ) kint = ( big_wts * integrand ) . sum ( axis = 1 ) * 0.5 * ( 1 - np . cos ( alpha ) ) if return_Kprefactor : return kint . reshape ( rho . shape ) , Kprefactor else : return kint . reshape ( rho . shape )
11181	def acquire ( self , * args , ** kwargs ) : with self . _stat_lock : self . _waiting += 1 self . _lock . acquire ( * args , ** kwargs ) with self . _stat_lock : self . _locked = True self . _waiting -= 1
4481	def storage ( self , provider = 'osfstorage' ) : stores = self . _json ( self . _get ( self . _storages_url ) , 200 ) stores = stores [ 'data' ] for store in stores : provides = self . _get_attribute ( store , 'attributes' , 'provider' ) if provides == provider : return Storage ( store , self . session ) raise RuntimeError ( "Project has no storage " "provider '{}'" . format ( provider ) )
11798	def suppose ( self , var , value ) : "Start accumulating inferences from assuming var=value." self . support_pruning ( ) removals = [ ( var , a ) for a in self . curr_domains [ var ] if a != value ] self . curr_domains [ var ] = [ value ] return removals
2677	def _install_packages ( path , packages ) : def _filter_blacklist ( package ) : blacklist = [ '-i' , '#' , 'Python==' , 'python-lambda==' ] return all ( package . startswith ( entry ) is False for entry in blacklist ) filtered_packages = filter ( _filter_blacklist , packages ) for package in filtered_packages : if package . startswith ( '-e ' ) : package = package . replace ( '-e ' , '' ) print ( 'Installing {package}' . format ( package = package ) ) subprocess . check_call ( [ sys . executable , '-m' , 'pip' , 'install' , package , '-t' , path , '--ignore-installed' ] ) print ( 'Install directory contents are now: {directory}' . format ( directory = os . listdir ( path ) ) )
9971	def _get_range ( book , range_ , sheet ) : filename = None if isinstance ( book , str ) : filename = book book = opxl . load_workbook ( book , data_only = True ) elif isinstance ( book , opxl . Workbook ) : pass else : raise TypeError if _is_range_address ( range_ ) : sheet_names = [ name . upper ( ) for name in book . sheetnames ] index = sheet_names . index ( sheet . upper ( ) ) data = book . worksheets [ index ] [ range_ ] else : data = _get_namedrange ( book , range_ , sheet ) if data is None : raise ValueError ( "Named range '%s' not found in %s" % ( range_ , filename or book ) ) return data
13350	def add_file ( self , file , ** kwargs ) : if os . access ( file , os . F_OK ) : if file in self . f_repository : raise DuplicationError ( "file already added." ) self . f_repository . append ( file ) else : raise IOError ( "file not found." )
12284	def rootdir ( self , username , reponame , create = True ) : path = os . path . join ( self . workspace , 'datasets' , username , reponame ) if create : try : os . makedirs ( path ) except : pass return path
1294	def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss
8126	def search ( q , start = 1 , count = 10 , context = None , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_SEARCH return YahooSearch ( q , start , count , service , context , wait , asynchronous , cached )
421	def delete_training_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) self . db . TrainLog . delete_many ( kwargs ) logging . info ( "[Database] Delete TrainLog SUCCESS" )
4578	def set_one ( desc , name , value ) : old_value = desc . get ( name ) if old_value is None : raise KeyError ( 'No section "%s"' % name ) if value is None : value = type ( old_value ) ( ) elif name in CLASS_SECTIONS : if isinstance ( value , str ) : value = { 'typename' : aliases . resolve ( value ) } elif isinstance ( value , type ) : value = { 'typename' : class_name . class_name ( value ) } elif not isinstance ( value , dict ) : raise TypeError ( 'Expected dict, str or type, got "%s"' % value ) typename = value . get ( 'typename' ) if typename : s = 's' if name == 'driver' else '' path = 'bibliopixel.' + name + s importer . import_symbol ( typename , path ) elif name == 'shape' : if not isinstance ( value , ( list , int , tuple , str ) ) : raise TypeError ( 'Expected shape, got "%s"' % value ) elif type ( old_value ) is not type ( value ) : raise TypeError ( 'Expected %s but got "%s" of type %s' % ( type ( old_value ) , value , type ( value ) ) ) desc [ name ] = value
3757	def LFL ( Hc = None , atoms = { } , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in IEC_2010 . index and not np . isnan ( IEC_2010 . at [ CASRN , 'LFL' ] ) : methods . append ( IEC ) if CASRN in NFPA_2008 . index and not np . isnan ( NFPA_2008 . at [ CASRN , 'LFL' ] ) : methods . append ( NFPA ) if Hc : methods . append ( SUZUKI ) if atoms : methods . append ( CROWLLOUVAR ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IEC : return float ( IEC_2010 . at [ CASRN , 'LFL' ] ) elif Method == NFPA : return float ( NFPA_2008 . at [ CASRN , 'LFL' ] ) elif Method == SUZUKI : return Suzuki_LFL ( Hc = Hc ) elif Method == CROWLLOUVAR : return Crowl_Louvar_LFL ( atoms = atoms ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' )
2684	def cached_download ( url , name ) : clean_name = os . path . normpath ( name ) if clean_name != name : raise ValueError ( "{} is not normalized." . format ( name ) ) for dir_ in iter_data_dirs ( ) : path = os . path . join ( dir_ , name ) if os . path . exists ( path ) : return path dir_ = next ( iter_data_dirs ( True ) ) path = os . path . join ( dir_ , name ) log . info ( "Downloading {} to {}" . format ( url , path ) ) response = urlopen ( url ) if response . getcode ( ) != 200 : raise ValueError ( "HTTP {}" . format ( response . getcode ( ) ) ) dir_ = os . path . dirname ( path ) try : os . makedirs ( dir_ ) except OSError as e : if e . errno != errno . EEXIST : raise tmp_path = path + '.tmp' with open ( tmp_path , 'wb' ) as fh : while True : chunk = response . read ( 8196 ) if chunk : fh . write ( chunk ) else : break os . rename ( tmp_path , path ) return path
6222	def _update_yaw_and_pitch ( self ) : front = Vector3 ( [ 0.0 , 0.0 , 0.0 ] ) front . x = cos ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) front . y = sin ( radians ( self . pitch ) ) front . z = sin ( radians ( self . yaw ) ) * cos ( radians ( self . pitch ) ) self . dir = vector . normalise ( front ) self . right = vector . normalise ( vector3 . cross ( self . dir , self . _up ) ) self . up = vector . normalise ( vector3 . cross ( self . right , self . dir ) )
8055	def trusted_cmd ( f ) : def run_cmd ( self , line ) : if self . trusted : f ( self , line ) else : print ( "Sorry cannot do %s here." % f . __name__ [ 3 : ] ) global trusted_cmds trusted_cmds . add ( f . __name__ ) run_cmd . __doc__ = f . __doc__ return run_cmd
5498	def is_cached ( self , url ) : try : return True if url in self . cache else False except TypeError : return False
4870	def to_internal_value ( self , data ) : if not isinstance ( data , list ) : message = self . error_messages [ 'not_a_list' ] . format ( input_type = type ( data ) . __name__ ) raise serializers . ValidationError ( { api_settings . NON_FIELD_ERRORS_KEY : [ message ] } ) ret = [ ] for item in data : try : validated = self . child . run_validation ( item ) except serializers . ValidationError as exc : ret . append ( exc . detail ) else : ret . append ( validated ) return ret
8757	def _validate_subnet_cidr ( context , network_id , new_subnet_cidr ) : if neutron_cfg . cfg . CONF . allow_overlapping_ips : return try : new_subnet_ipset = netaddr . IPSet ( [ new_subnet_cidr ] ) except TypeError : LOG . exception ( "Invalid or missing cidr: %s" % new_subnet_cidr ) raise n_exc . BadRequest ( resource = "subnet" , msg = "Invalid or missing cidr" ) filters = { 'network_id' : network_id , 'shared' : [ False ] } subnet_list = db_api . subnet_find ( context = context . elevated ( ) , ** filters ) for subnet in subnet_list : if ( netaddr . IPSet ( [ subnet . cidr ] ) & new_subnet_ipset ) : err_msg = ( _ ( "Requested subnet with cidr: %(cidr)s for " "network: %(network_id)s overlaps with another " "subnet" ) % { 'cidr' : new_subnet_cidr , 'network_id' : network_id } ) LOG . error ( _ ( "Validation for CIDR: %(new_cidr)s failed - " "overlaps with subnet %(subnet_id)s " "(CIDR: %(cidr)s)" ) , { 'new_cidr' : new_subnet_cidr , 'subnet_id' : subnet . id , 'cidr' : subnet . cidr } ) raise n_exc . InvalidInput ( error_message = err_msg )
6686	def is_installed ( pkg_name ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' , 'stderr' , 'warnings' ) , warn_only = True ) : res = run ( "rpm --query %(pkg_name)s" % locals ( ) ) if res . succeeded : return True return False
10799	def _eval_firstorder ( self , rvecs , data , sigma ) : if not self . blocksize : dist_between_points = self . _distance_matrix ( rvecs , self . x ) gaussian_weights = self . _weight ( dist_between_points , sigma = sigma ) return gaussian_weights . dot ( data ) / gaussian_weights . sum ( axis = 1 ) else : ans = np . zeros ( rvecs . shape [ 0 ] , dtype = 'float' ) bs = self . blocksize for a in range ( 0 , rvecs . shape [ 0 ] , bs ) : dist = self . _distance_matrix ( rvecs [ a : a + bs ] , self . x ) weights = self . _weight ( dist , sigma = sigma ) ans [ a : a + bs ] += weights . dot ( data ) / weights . sum ( axis = 1 ) return ans
5158	def _add_uninstall ( self , context ) : contents = self . _render_template ( 'uninstall.sh' , context ) self . config . setdefault ( 'files' , [ ] ) self . _add_unique_file ( { "path" : "/uninstall.sh" , "contents" : contents , "mode" : "755" } )
5750	def downloadURL ( url , filename ) : path_temp_bviewfile = os . path . join ( c . raw_data , c . bview_dir , 'tmp' , filename ) path_bviewfile = os . path . join ( c . raw_data , c . bview_dir , filename ) try : f = urlopen ( url ) except : return False if f . getcode ( ) != 200 : publisher . warning ( '{} unavailable, code: {}' . format ( url , f . getcode ( ) ) ) return False try : with open ( path_temp_bviewfile , 'w' ) as outfile : outfile . write ( f . read ( ) ) os . rename ( path_temp_bviewfile , path_bviewfile ) except : os . remove ( path_temp_bviewfile ) return False return True
13248	def get_bibliography ( lsst_bib_names = None , bibtex = None ) : bibtex_data = get_lsst_bibtex ( bibtex_filenames = lsst_bib_names ) pybtex_data = [ pybtex . database . parse_string ( _bibtex , 'bibtex' ) for _bibtex in bibtex_data . values ( ) ] if bibtex is not None : pybtex_data . append ( pybtex . database . parse_string ( bibtex , 'bibtex' ) ) bib = pybtex_data [ 0 ] if len ( pybtex_data ) > 1 : for other_bib in pybtex_data [ 1 : ] : for key , entry in other_bib . entries . items ( ) : bib . add_entry ( key , entry ) return bib
13373	def unipath ( * paths ) : return os . path . normpath ( expandpath ( os . path . join ( * paths ) ) )
6954	def make_combined_periodogram ( pflist , outfile , addmethods = False ) : import matplotlib . pyplot as plt for pf in pflist : if pf [ 'method' ] == 'pdm' : plt . plot ( pf [ 'periods' ] , np . max ( pf [ 'lspvals' ] ) / pf [ 'lspvals' ] - 1.0 , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) else : plt . plot ( pf [ 'periods' ] , pf [ 'lspvals' ] / np . max ( pf [ 'lspvals' ] ) , label = '%s P=%.5f' % ( pf [ 'method' ] , pf [ 'bestperiod' ] ) , alpha = 0.5 ) plt . xlabel ( 'period [days]' ) plt . ylabel ( 'normalized periodogram power' ) plt . xscale ( 'log' ) plt . legend ( ) plt . tight_layout ( ) plt . savefig ( outfile ) plt . close ( 'all' ) return outfile
11148	def create_package ( self , path = None , name = None , mode = None ) : assert mode in ( None , 'w' , 'w:' , 'w:gz' , 'w:bz2' ) , 'unkown archive mode %s' % str ( mode ) if mode is None : mode = 'w:' if path is None : root = os . path . split ( self . __path ) [ 0 ] elif path . strip ( ) in ( '' , '.' ) : root = os . getcwd ( ) else : root = os . path . realpath ( os . path . expanduser ( path ) ) assert os . path . isdir ( root ) , 'absolute path %s is not a valid directory' % path if name is None : ext = mode . split ( ":" ) if len ( ext ) == 2 : if len ( ext [ 1 ] ) : ext = "." + ext [ 1 ] else : ext = '.tar' else : ext = '.tar' name = os . path . split ( self . __path ) [ 1 ] + ext tarfilePath = os . path . join ( root , name ) try : tarHandler = tarfile . TarFile . open ( tarfilePath , mode = mode ) except Exception as e : raise Exception ( "Unable to create package (%s)" % e ) for dpath in sorted ( list ( self . walk_directories_path ( recursive = True ) ) ) : t = tarfile . TarInfo ( dpath ) t . type = tarfile . DIRTYPE tarHandler . addfile ( t ) tarHandler . add ( os . path . join ( self . __path , dpath , self . __dirInfo ) , arcname = self . __dirInfo ) for fpath in self . walk_files_path ( recursive = True ) : relaPath , fname = os . path . split ( fpath ) tarHandler . add ( os . path . join ( self . __path , fpath ) , arcname = fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileInfo % fname ) , arcname = self . __fileInfo % fname ) tarHandler . add ( os . path . join ( self . __path , relaPath , self . __fileClass % fname ) , arcname = self . __fileClass % fname ) tarHandler . add ( os . path . join ( self . __path , self . __repoFile ) , arcname = ".pyrepinfo" ) tarHandler . close ( )
9983	def replace_funcname ( source : str , name : str ) : lines = source . splitlines ( ) atok = asttokens . ASTTokens ( source , parse = True ) for node in ast . walk ( atok . tree ) : if isinstance ( node , ast . FunctionDef ) : break i = node . first_token . index for i in range ( node . first_token . index , node . last_token . index ) : if ( atok . tokens [ i ] . type == token . NAME and atok . tokens [ i ] . string == "def" ) : break lineno , col_begin = atok . tokens [ i + 1 ] . start lineno_end , col_end = atok . tokens [ i + 1 ] . end assert lineno == lineno_end lines [ lineno - 1 ] = ( lines [ lineno - 1 ] [ : col_begin ] + name + lines [ lineno - 1 ] [ col_end : ] ) return "\n" . join ( lines ) + "\n"
4135	def check_md5sum_change ( src_file ) : src_md5 = get_md5sum ( src_file ) src_md5_file = src_file + '.md5' src_file_changed = True if os . path . exists ( src_md5_file ) : with open ( src_md5_file , 'r' ) as file_checksum : ref_md5 = file_checksum . read ( ) if src_md5 == ref_md5 : src_file_changed = False if src_file_changed : with open ( src_md5_file , 'w' ) as file_checksum : file_checksum . write ( src_md5 ) return src_file_changed
9669	def is_valid_sound ( sound , ts ) : if isinstance ( sound , ( Marker , UnknownSound ) ) : return False s1 = ts [ sound . name ] s2 = ts [ sound . s ] return s1 . name == s2 . name and s1 . s == s2 . s
6234	def start ( self ) : if self . initialized : mixer . music . unpause ( ) else : mixer . music . play ( ) mixer . music . play ( ) self . initialized = True self . paused = False
12383	def create_project ( self ) : if os . path . exists ( self . _py ) : prj_dir = os . path . join ( self . _app_dir , self . _project_name ) if os . path . exists ( prj_dir ) : if self . _force : logging . warn ( 'Removing existing project' ) shutil . rmtree ( prj_dir ) else : logging . warn ( 'Found existing project; not creating (use --force to overwrite)' ) return logging . info ( 'Creating project' ) p = subprocess . Popen ( 'cd {0} ; {1} startproject {2} > /dev/null' . format ( self . _app_dir , self . _ve_dir + os . sep + self . _project_name + os . sep + 'bin' + os . sep + 'django-admin.py' , self . _project_name ) , shell = True ) os . waitpid ( p . pid , 0 ) else : logging . error ( 'Unable to find Python interpreter in virtualenv' ) return
6807	def create_raspbian_vagrant_box ( self ) : r = self . local_renderer r . sudo ( 'adduser --disabled-password --gecos "" vagrant' ) r . sudo ( 'echo "vagrant ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/vagrant' ) r . sudo ( 'chmod 0440 /etc/sudoers.d/vagrant' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install -y openssh-server' ) r . sudo ( 'mkdir -p /home/vagrant/.ssh' ) r . sudo ( 'chmod 0700 /home/vagrant/.ssh' ) r . sudo ( 'wget --no-check-certificate https://raw.github.com/mitchellh/vagrant/master/keys/vagrant.pub -O /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chmod 0600 /home/vagrant/.ssh/authorized_keys' ) r . sudo ( 'chown -R vagrant /home/vagrant/.ssh' ) r . sudo ( "sed -i '/AuthorizedKeysFile/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i '/PasswordAuthentication/s/^#//g' /etc/ssh/sshd_config" ) r . sudo ( "sed -i 's/PasswordAuthentication yes/PasswordAuthentication no/g' /etc/ssh/sshd_config" ) r . sudo ( 'apt-get upgrade' ) r . sudo ( 'apt-get install -y gcc build-essential' ) r . sudo ( 'mkdir /tmp/test' ) r . sudo ( 'cp {libvirt_images_dir}/{raspbian_image} /tmp/test' ) r . sudo ( 'cp {libvirt_boot_dir}/{raspbian_kernel} /tmp/test' ) r . render_to_file ( 'rpi/metadata.json' , '/tmp/test/metadata.json' ) r . render_to_file ( 'rpi/Vagrantfile' , '/tmp/test/Vagrantfile' ) r . sudo ( 'qemu-img convert -f raw -O qcow2 {libvirt_images_dir}/{raspbian_image} {libvirt_images_dir}/{raspbian_image}.qcow2' ) r . sudo ( 'mv {libvirt_images_dir}/{raspbian_image}.qcow2 {libvirt_images_dir}/box.img' ) r . sudo ( 'cd /tmp/test; tar cvzf custom_box.box ./metadata.json ./Vagrantfile ./{raspbian_kernel} ./box.img' )
11684	def _readxml ( self ) : block = re . sub ( r'<(/?)s>' , r'&lt;\1s&gt;' , self . _readblock ( ) ) try : xml = XML ( block ) except ParseError : xml = None return xml
8602	def get_user ( self , user_id , depth = 1 ) : response = self . _perform_request ( '/um/users/%s?depth=%s' % ( user_id , str ( depth ) ) ) return response
3104	def oauth_enabled ( decorated_function = None , scopes = None , ** decorator_kwargs ) : def curry_wrapper ( wrapped_function ) : @ wraps ( wrapped_function ) def enabled_wrapper ( request , * args , ** kwargs ) : return_url = decorator_kwargs . pop ( 'return_url' , request . get_full_path ( ) ) user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) setattr ( request , django_util . oauth2_settings . request_prefix , user_oauth ) return wrapped_function ( request , * args , ** kwargs ) return enabled_wrapper if decorated_function : return curry_wrapper ( decorated_function ) else : return curry_wrapper
10649	def get_component ( self , name ) : return [ c for c in self . components if c . name == name ] [ 0 ]
1865	def PSHUFW ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) arg3 = Operators . ZEXTEND ( op3 . read ( ) , size ) assert size == 64 arg0 |= ( ( arg1 >> ( ( arg3 >> 0 ) & 3 * 16 ) ) & 0xffff ) arg0 |= ( ( arg1 >> ( ( arg3 >> 2 ) & 3 * 16 ) ) & 0xffff ) << 16 arg0 |= ( ( arg1 >> ( ( arg3 >> 4 ) & 3 * 16 ) ) & 0xffff ) << 32 arg0 |= ( ( arg1 >> ( ( arg3 >> 6 ) & 3 * 16 ) ) & 0xffff ) << 48 op0 . write ( arg0 )
443	def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( "The length of the inspection stack is shorter than the requested start position." ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] if inspect . isfunction ( val ) : params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } elif arg . endswith ( 'init' ) : continue else : params [ arg ] = val return params
4637	def claim ( self , account = None , ** kwargs ) : if not account : if "default_account" in self . blockchain . config : account = self . blockchain . config [ "default_account" ] if not account : raise ValueError ( "You need to provide an account" ) account = self . account_class ( account , blockchain_instance = self . blockchain ) pubkeys = self . blockchain . wallet . getPublicKeys ( ) addresses = dict ( ) for p in pubkeys : if p [ : len ( self . blockchain . prefix ) ] != self . blockchain . prefix : continue pubkey = self . publickey_class ( p , prefix = self . blockchain . prefix ) addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 0 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = False , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey addresses [ str ( self . address_class . from_pubkey ( pubkey , compressed = True , version = 56 , prefix = self . blockchain . prefix , ) ) ] = pubkey if self [ "owner" ] not in addresses . keys ( ) : raise MissingKeyError ( "Need key for address {}" . format ( self [ "owner" ] ) ) op = self . operations . Balance_claim ( ** { "fee" : { "amount" : 0 , "asset_id" : "1.3.0" } , "deposit_to_account" : account [ "id" ] , "balance_to_claim" : self [ "id" ] , "balance_owner_key" : addresses [ self [ "owner" ] ] , "total_claimed" : self [ "balance" ] , "prefix" : self . blockchain . prefix , } ) signers = [ account [ "name" ] , addresses . get ( self [ "owner" ] ) , ] return self . blockchain . finalizeOp ( op , signers , "active" , ** kwargs )
3942	async def _longpoll_request ( self ) : params = { 'VER' : 8 , 'gsessionid' : self . _gsessionid_param , 'RID' : 'rpc' , 't' : 1 , 'SID' : self . _sid_param , 'CI' : 0 , 'ctype' : 'hangouts' , 'TYPE' : 'xmlhttp' , } logger . info ( 'Opening new long-polling request' ) try : async with self . _session . fetch_raw ( 'GET' , CHANNEL_URL , params = params ) as res : if res . status != 200 : if res . status == 400 and res . reason == 'Unknown SID' : raise ChannelSessionError ( 'SID became invalid' ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) while True : async with async_timeout . timeout ( PUSH_TIMEOUT ) : chunk = await res . content . read ( MAX_READ_BYTES ) if not chunk : break await self . _on_push_data ( chunk ) except asyncio . TimeoutError : raise exceptions . NetworkError ( 'Request timed out' ) except aiohttp . ServerDisconnectedError as err : raise exceptions . NetworkError ( 'Server disconnected error: %s' % err ) except aiohttp . ClientPayloadError : raise ChannelSessionError ( 'SID is about to expire' ) except aiohttp . ClientError as err : raise exceptions . NetworkError ( 'Request connection error: %s' % err )
1182	def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False
5698	def get_centroid_of_stops ( gtfs ) : stops = gtfs . get_table ( "stops" ) mean_lat = numpy . mean ( stops [ 'lat' ] . values ) mean_lon = numpy . mean ( stops [ 'lon' ] . values ) return mean_lat , mean_lon
10583	def create_account ( self , name , number = None , description = None ) : new_account = GeneralLedgerAccount ( name , description , number , self . account_type ) new_account . set_parent_path ( self . path ) self . accounts . append ( new_account ) return new_account
7236	def read ( self , bands = None , ** kwargs ) : arr = self if bands is not None : arr = self [ bands , ... ] return arr . compute ( scheduler = threaded_get )
8812	def get_used_ips ( session , ** kwargs ) : LOG . debug ( "Getting used IPs..." ) with session . begin ( ) : query = session . query ( models . Subnet . segment_id , func . count ( models . IPAddress . address ) ) query = query . group_by ( models . Subnet . segment_id ) query = _filter ( query , ** kwargs ) reuse_window = timeutils . utcnow ( ) - datetime . timedelta ( seconds = cfg . CONF . QUARK . ipam_reuse_after ) query = query . outerjoin ( models . IPAddress , and_ ( models . Subnet . id == models . IPAddress . subnet_id , or_ ( not_ ( models . IPAddress . lock_id . is_ ( None ) ) , models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPAddress . deallocated_at > reuse_window ) ) ) query = query . outerjoin ( models . IPPolicyCIDR , and_ ( models . Subnet . ip_policy_id == models . IPPolicyCIDR . ip_policy_id , models . IPAddress . address >= models . IPPolicyCIDR . first_ip , models . IPAddress . address <= models . IPPolicyCIDR . last_ip ) ) query = query . filter ( or_ ( models . IPAddress . _deallocated . is_ ( None ) , models . IPAddress . _deallocated == 0 , models . IPPolicyCIDR . id . is_ ( None ) ) ) ret = ( ( segment_id , address_count ) for segment_id , address_count in query . all ( ) ) return dict ( ret )
8376	def parse ( svg , cached = False , _copy = True ) : if not cached : dom = parser . parseString ( svg ) paths = parse_node ( dom , [ ] ) else : id = _cache . id ( svg ) if not _cache . has_key ( id ) : dom = parser . parseString ( svg ) _cache . save ( id , parse_node ( dom , [ ] ) ) paths = _cache . load ( id , _copy ) return paths
6784	def unlock ( self ) : self . init ( ) r = self . local_renderer if self . file_exists ( r . env . lockfile_path ) : self . vprint ( 'Unlocking %s.' % r . env . lockfile_path ) r . run_or_local ( 'rm -f {lockfile_path}' )
2442	def add_annotation_comment ( self , doc , comment ) : if len ( doc . annotations ) != 0 : if not self . annotation_comment_set : self . annotation_comment_set = True if validations . validate_annotation_comment ( comment ) : doc . annotations [ - 1 ] . comment = str_from_text ( comment ) return True else : raise SPDXValueError ( 'AnnotationComment::Comment' ) else : raise CardinalityError ( 'AnnotationComment::Comment' ) else : raise OrderError ( 'AnnotationComment::Comment' )
4470	def _get_param_names ( cls ) : init = cls . __init__ args , varargs = inspect . getargspec ( init ) [ : 2 ] if varargs is not None : raise RuntimeError ( 'BaseTransformer objects cannot have varargs' ) args . pop ( 0 ) args . sort ( ) return args
4315	def validate_input_file_list ( input_filepath_list ) : if not isinstance ( input_filepath_list , list ) : raise TypeError ( "input_filepath_list must be a list." ) elif len ( input_filepath_list ) < 2 : raise ValueError ( "input_filepath_list must have at least 2 files." ) for input_filepath in input_filepath_list : validate_input_file ( input_filepath )
2784	def get_timeout ( self ) : timeout_str = os . environ . get ( REQUEST_TIMEOUT_ENV_VAR ) if timeout_str : try : return float ( timeout_str ) except : self . _log . error ( 'Failed parsing the request read timeout of ' '"%s". Please use a valid float number!' % timeout_str ) return None
795	def getActiveJobCountForClientKey ( self , clientKey ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_key = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientKey , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount
6342	def docs_of_words ( self ) : r return [ [ words for sents in doc for words in sents ] for doc in self . corpus ]
11939	def broadcast_message ( level , message_text , extra_tags = '' , date = None , url = None , fail_silently = False ) : from django . contrib . auth import get_user_model users = get_user_model ( ) . objects . all ( ) add_message_for ( users , level , message_text , extra_tags = extra_tags , date = date , url = url , fail_silently = fail_silently )
2264	def dict_union ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict return dictclass ( it . chain . from_iterable ( d . items ( ) for d in args ) )
5823	def to_dict ( self ) : return { "type" : self . type , "name" : self . name , "group_by_key" : self . group_by_key , "role" : self . role , "units" : self . units , "options" : self . build_options ( ) }
12164	def add_listener ( self , event , listener ) : self . emit ( 'new_listener' , event , listener ) self . _listeners [ event ] . append ( listener ) self . _check_limit ( event ) return self
6804	def init_raspbian_disk ( self , yes = 0 ) : self . assume_localhost ( ) yes = int ( yes ) device_question = 'SD card present at %s? ' % self . env . sd_device if not yes and not raw_input ( device_question ) . lower ( ) . startswith ( 'y' ) : return r = self . local_renderer r . local_if_missing ( fn = '{raspbian_image_zip}' , cmd = 'wget {raspbian_download_url} -O raspbian_lite_latest.zip' ) r . lenv . img_fn = r . local ( "unzip -l {raspbian_image_zip} | sed -n 4p | awk '{{print $4}}'" , capture = True ) or '$IMG_FN' r . local ( 'echo {img_fn}' ) r . local ( '[ ! -f {img_fn} ] && unzip {raspbian_image_zip} {img_fn} || true' ) r . lenv . img_fn = r . local ( 'readlink -f {img_fn}' , capture = True ) r . local ( 'echo {img_fn}' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir}" ] && umount {sd_media_mount_dir} || true' ) with self . settings ( warn_only = True ) : r . sudo ( '[ -d "{sd_media_mount_dir2}" ] && umount {sd_media_mount_dir2} || true' ) r . pc ( 'Writing the image onto the card.' ) r . sudo ( 'time dd bs=4M if={img_fn} of={sd_device}' ) r . run ( 'sync' )
6726	def delete ( name = None , group = None , release = None , except_release = None , dryrun = 1 , verbose = 1 ) : verbose = int ( verbose ) if env . vm_type == EC2 : conn = get_ec2_connection ( ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , ) for instance_name , instance_data in instances . items ( ) : public_dns_name = instance_data [ 'public_dns_name' ] print ( '\nDeleting %s (%s)...' % ( instance_name , instance_data [ 'id' ] ) ) if not get_dryrun ( ) : conn . terminate_instances ( instance_ids = [ instance_data [ 'id' ] ] ) known_hosts = os . path . expanduser ( '~/.ssh/known_hosts' ) cmd = 'ssh-keygen -f "%s" -R %s' % ( known_hosts , public_dns_name ) local_or_dryrun ( cmd ) else : raise NotImplementedError
6395	def fingerprint ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = '' . join ( c for c in word if c in self . _letters ) key = '' for char in self . _consonants : if char in word : key += char for char in word : if char not in self . _consonants and char not in key : key += char return key
1442	def next_tuple ( self , latency_in_ns ) : self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) self . update_count ( self . NEXT_TUPLE_COUNT )
3115	def _get_flow_for_token ( csrf_token , request ) : flow_pickle = request . session . get ( _FLOW_KEY . format ( csrf_token ) , None ) return None if flow_pickle is None else jsonpickle . decode ( flow_pickle )
5570	def profile ( self ) : with rasterio . open ( self . path , "r" ) as src : return deepcopy ( src . meta )
13873	def CopyFile ( source_filename , target_filename , override = True , md5_check = False , copy_symlink = True ) : from . _exceptions import FileNotFoundError if not override and Exists ( target_filename ) : from . _exceptions import FileAlreadyExistsError raise FileAlreadyExistsError ( target_filename ) md5_check = md5_check and not target_filename . endswith ( '.md5' ) if md5_check : source_md5_filename = source_filename + '.md5' target_md5_filename = target_filename + '.md5' try : source_md5_contents = GetFileContents ( source_md5_filename ) except FileNotFoundError : source_md5_contents = None try : target_md5_contents = GetFileContents ( target_md5_filename ) except FileNotFoundError : target_md5_contents = None if source_md5_contents is not None and source_md5_contents == target_md5_contents and Exists ( target_filename ) : return MD5_SKIP _DoCopyFile ( source_filename , target_filename , copy_symlink = copy_symlink ) if md5_check and source_md5_contents is not None and source_md5_contents != target_md5_contents : CreateFile ( target_md5_filename , source_md5_contents )
3340	def make_complete_url ( environ , localUri = None ) : url = environ [ "wsgi.url_scheme" ] + "://" if environ . get ( "HTTP_HOST" ) : url += environ [ "HTTP_HOST" ] else : url += environ [ "SERVER_NAME" ] if environ [ "wsgi.url_scheme" ] == "https" : if environ [ "SERVER_PORT" ] != "443" : url += ":" + environ [ "SERVER_PORT" ] else : if environ [ "SERVER_PORT" ] != "80" : url += ":" + environ [ "SERVER_PORT" ] url += compat . quote ( environ . get ( "SCRIPT_NAME" , "" ) ) if localUri is None : url += compat . quote ( environ . get ( "PATH_INFO" , "" ) ) if environ . get ( "QUERY_STRING" ) : url += "?" + environ [ "QUERY_STRING" ] else : url += localUri return url
11345	def handle_starttag ( self , tag , attrs ) : if tag in self . mathml_elements : final_attr = "" for key , value in attrs : final_attr += ' {0}="{1}"' . format ( key , value ) self . fed . append ( "<{0}{1}>" . format ( tag , final_attr ) )
9624	def autodiscover ( ) : from django . conf import settings for application in settings . INSTALLED_APPS : module = import_module ( application ) if module_has_submodule ( module , 'emails' ) : emails = import_module ( '%s.emails' % application ) try : import_module ( '%s.emails.previews' % application ) except ImportError : if module_has_submodule ( emails , 'previews' ) : raise
12979	def deleteMultiple ( self , objs ) : conn = self . _get_connection ( ) pipeline = conn . pipeline ( ) numDeleted = 0 for obj in objs : numDeleted += self . deleteOne ( obj , pipeline ) pipeline . execute ( ) return numDeleted
3203	def delete ( self , store_id , cart_id , line_id ) : self . store_id = store_id self . cart_id = cart_id self . line_id = line_id return self . _mc_client . _delete ( url = self . _build_path ( store_id , 'carts' , cart_id , 'lines' , line_id ) )
8311	def draw_math ( str , x , y , alpha = 1.0 ) : try : from web import _ctx except : pass str = re . sub ( "</{0,1}math>" , "" , str . strip ( ) ) img = mimetex . gif ( str ) w , h = _ctx . imagesize ( img ) _ctx . image ( img , x , y , alpha = alpha ) return w , h
13730	def value_to_bool ( config_val , evar ) : if not config_val : return False if config_val . strip ( ) . lower ( ) == 'true' : return True else : return False
89	def new_random_state ( seed = None , fully_random = False ) : if seed is None : if not fully_random : seed = CURRENT_RANDOM_STATE . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return np . random . RandomState ( seed )
10709	def _authenticate ( self ) : auth_url = BASE_URL + "/auth/token" payload = { 'username' : self . email , 'password' : self . password , 'grant_type' : 'password' } arequest = requests . post ( auth_url , data = payload , headers = BASIC_HEADERS ) status = arequest . status_code if status != 200 : _LOGGER . error ( "Authentication request failed, please check credintials. " + str ( status ) ) return False response = arequest . json ( ) _LOGGER . debug ( str ( response ) ) self . token = response . get ( "access_token" ) self . refresh_token = response . get ( "refresh_token" ) _auth = HEADERS . get ( "Authorization" ) _auth = _auth % self . token HEADERS [ "Authorization" ] = _auth _LOGGER . info ( "Authentication was successful, token set." ) return True
1670	def ProcessFileData ( filename , file_extension , lines , error , extra_check_functions = None ) : lines = ( [ '// marker so line numbers and indices both start at 1' ] + lines + [ '// marker so line numbers end in a known way' ] ) include_state = _IncludeState ( ) function_state = _FunctionState ( ) nesting_state = NestingState ( ) ResetNolintSuppressions ( ) CheckForCopyright ( filename , lines , error ) ProcessGlobalSuppresions ( lines ) RemoveMultiLineComments ( filename , lines , error ) clean_lines = CleansedLines ( lines ) if file_extension in GetHeaderExtensions ( ) : CheckForHeaderGuard ( filename , clean_lines , error ) for line in range ( clean_lines . NumLines ( ) ) : ProcessLine ( filename , file_extension , clean_lines , line , include_state , function_state , nesting_state , error , extra_check_functions ) FlagCxx11Features ( filename , clean_lines , line , error ) nesting_state . CheckCompletedBlocks ( filename , error ) CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) if _IsSourceExtension ( file_extension ) : CheckHeaderFileIncluded ( filename , include_state , error ) CheckForBadCharacters ( filename , lines , error ) CheckForNewlineAtEOF ( filename , lines , error )
4448	def delete_document ( self , doc_id , conn = None ) : if conn is None : conn = self . redis return conn . execute_command ( self . DEL_CMD , self . index_name , doc_id )
2181	def parse_authorization_response ( self , url ) : log . debug ( "Parsing token from query part of url %s" , url ) token = dict ( urldecode ( urlparse ( url ) . query ) ) log . debug ( "Updating internal client token attribute." ) self . _populate_attributes ( token ) self . token = token return token
13273	def default ( self , obj ) : if isinstance ( obj , np . ndarray ) : return obj . tolist ( ) elif isinstance ( obj , np . generic ) : return np . asscalar ( obj ) return json . JSONEncoder ( self , obj )
6527	def get_tools ( ) : if not hasattr ( get_tools , '_CACHE' ) : get_tools . _CACHE = dict ( ) for entry in pkg_resources . iter_entry_points ( 'tidypy.tools' ) : try : get_tools . _CACHE [ entry . name ] = entry . load ( ) except ImportError as exc : output_error ( 'Could not load tool "%s" defined by "%s": %s' % ( entry , entry . dist , exc , ) , ) return get_tools . _CACHE
7938	def _connect ( self , addr , port , service ) : self . _dst_name = addr self . _dst_port = port family = None try : res = socket . getaddrinfo ( addr , port , socket . AF_UNSPEC , socket . SOCK_STREAM , 0 , socket . AI_NUMERICHOST ) family = res [ 0 ] [ 0 ] sockaddr = res [ 0 ] [ 4 ] except socket . gaierror : family = None sockaddr = None if family is not None : if not port : raise ValueError ( "No port number given with literal IP address" ) self . _dst_service = None self . _family = family self . _dst_addrs = [ ( family , sockaddr ) ] self . _set_state ( "connect" ) elif service is not None : self . _dst_service = service self . _set_state ( "resolve-srv" ) self . _dst_name = addr elif port : self . _dst_nameports = [ ( self . _dst_name , self . _dst_port ) ] self . _dst_service = None self . _set_state ( "resolve-hostname" ) else : raise ValueError ( "No port number and no SRV service name given" )
13154	def cursor ( func ) : @ wraps ( func ) def wrapper ( cls , * args , ** kwargs ) : with ( yield from cls . get_cursor ( ) ) as c : return ( yield from func ( cls , c , * args , ** kwargs ) ) return wrapper
3590	def get_provider ( ) : global _provider if _provider is None : if sys . platform . startswith ( 'linux' ) : from . bluez_dbus . provider import BluezProvider _provider = BluezProvider ( ) elif sys . platform == 'darwin' : from . corebluetooth . provider import CoreBluetoothProvider _provider = CoreBluetoothProvider ( ) else : raise RuntimeError ( 'Sorry the {0} platform is not supported by the BLE library!' . format ( sys . platform ) ) return _provider
10465	def _getRunningApps ( cls ) : def runLoopAndExit ( ) : AppHelper . stopEventLoop ( ) AppHelper . callLater ( 1 , runLoopAndExit ) AppHelper . runConsoleEventLoop ( ) ws = AppKit . NSWorkspace . sharedWorkspace ( ) apps = ws . runningApplications ( ) return apps
901	def mmPrettyPrintMetrics ( metrics , sigFigs = 5 ) : assert len ( metrics ) > 0 , "No metrics found" table = PrettyTable ( [ "Metric" , "mean" , "standard deviation" , "min" , "max" , "sum" , ] ) for metric in metrics : table . add_row ( [ metric . prettyPrintTitle ( ) ] + metric . getStats ( ) ) return table . get_string ( ) . encode ( "utf-8" )
5741	def result ( self , timeout = None ) : start = time . time ( ) while True : task = self . get_task ( ) if not task or task . status not in ( FINISHED , FAILED ) : if not timeout : continue elif time . time ( ) - start < timeout : continue else : raise TimeoutError ( ) if task . status == FAILED : raise task . result return task . result
886	def punishPredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells ) : self . _punishPredictedColumn ( self . connections , columnMatchingSegments , prevActiveCells , self . predictedSegmentDecrement )
10701	def set_state ( _id , body ) : url = DEVICE_URL % _id if "mode" in body : url = MODES_URL % _id arequest = requests . put ( url , headers = HEADERS , data = json . dumps ( body ) ) status_code = str ( arequest . status_code ) if status_code != '202' : _LOGGER . error ( "State not accepted. " + status_code ) return False
1747	def _in_range ( self , index ) : if isinstance ( index , slice ) : in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end else : in_range = index >= self . start and index <= self . end return in_range
3342	def send_status_response ( environ , start_response , e , add_headers = None , is_head = False ) : status = get_http_status_string ( e ) headers = [ ] if add_headers : headers . extend ( add_headers ) if e in ( HTTP_NOT_MODIFIED , HTTP_NO_CONTENT ) : start_response ( status , [ ( "Content-Length" , "0" ) , ( "Date" , get_rfc1123_time ( ) ) ] + headers ) return [ b"" ] if e in ( HTTP_OK , HTTP_CREATED ) : e = DAVError ( e ) assert isinstance ( e , DAVError ) content_type , body = e . get_response_page ( ) if is_head : body = compat . b_empty assert compat . is_bytes ( body ) , body start_response ( status , [ ( "Content-Type" , content_type ) , ( "Date" , get_rfc1123_time ( ) ) , ( "Content-Length" , str ( len ( body ) ) ) , ] + headers , ) return [ body ]
12332	def get_diffs ( history ) : mgr = plugins_get_mgr ( ) keys = mgr . search ( 'representation' ) [ 'representation' ] representations = [ mgr . get_by_key ( 'representation' , k ) for k in keys ] for i in range ( len ( history ) ) : if i + 1 > len ( history ) - 1 : continue prev = history [ i ] curr = history [ i + 1 ] for c in curr [ 'changes' ] : path = c [ 'path' ] if c [ 'path' ] . endswith ( 'datapackage.json' ) : continue handler = None for r in representations : if r . can_process ( path ) : handler = r break if handler is None : continue v1_hex = prev [ 'commit' ] v2_hex = curr [ 'commit' ] temp1 = tempfile . mkdtemp ( prefix = "dgit-diff-" ) try : for h in [ v1_hex , v2_hex ] : filename = '{}/{}/checkout.tar' . format ( temp1 , h ) try : os . makedirs ( os . path . dirname ( filename ) ) except : pass extractcmd = [ 'git' , 'archive' , '-o' , filename , h , path ] output = run ( extractcmd ) if 'fatal' in output : raise Exception ( "File not present in commit" ) with cd ( os . path . dirname ( filename ) ) : cmd = [ 'tar' , 'xvf' , 'checkout.tar' ] output = run ( cmd ) if 'fatal' in output : print ( "Cleaning up - fatal 1" , temp1 ) shutil . rmtree ( temp1 ) continue path1 = os . path . join ( temp1 , v1_hex , path ) path2 = os . path . join ( temp1 , v2_hex , path ) if not os . path . exists ( path1 ) or not os . path . exists ( path2 ) : shutil . rmtree ( temp1 ) continue diff = handler . get_diff ( path1 , path2 ) c [ 'diff' ] = diff except Exception as e : shutil . rmtree ( temp1 )
10716	def main ( argv = None ) : if len ( argv ) : commands = ' ' . join ( argv ) comPort , commands = commands . split ( None , 1 ) sendCommands ( comPort , commands ) return 0
6697	def install ( packages , update = False , options = None , version = None ) : manager = MANAGER if update : update_index ( ) if options is None : options = [ ] if version is None : version = '' if version and not isinstance ( packages , list ) : version = '=' + version if not isinstance ( packages , six . string_types ) : packages = " " . join ( packages ) options . append ( "--quiet" ) options . append ( "--assume-yes" ) options = " " . join ( options ) cmd = '%(manager)s install %(options)s %(packages)s%(version)s' % locals ( ) run_as_root ( cmd , pty = False )
13442	def cmd_init_pull_from_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-pull-from-cloud]: %s => %s" % ( ccat , lcat ) ) if isfile ( lcat ) : args . error ( "[init-pull-from-cloud] The local catalog already exist: %s" % lcat ) if not isfile ( ccat ) : args . error ( "[init-pull-from-cloud] The cloud catalog does not exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-pull-from-cloud] The local meta-data already exist: %s" % lmeta ) if not isfile ( cmeta ) : args . error ( "[init-pull-from-cloud] The cloud meta-data does not exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( ccat , lcat ) cloudDAG = ChangesetDAG ( ccat ) path = cloudDAG . path ( cloudDAG . root . hash , cloudDAG . leafs [ 0 ] . hash ) util . apply_changesets ( args , path , lcat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'filename' ] mfile [ 'last_push' ] [ 'hash' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'hash' ] mfile [ 'last_push' ] [ 'modification_utc' ] = cloudDAG . leafs [ 0 ] . mfile [ 'changeset' ] [ 'modification_utc' ] mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = False ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-pull-from-cloud]: Success!" )
5905	def _mdp_include_string ( dirs ) : include_paths = [ os . path . expanduser ( p ) for p in dirs ] return ' -I' . join ( [ '' ] + include_paths )
424	def run_top_task ( self , task_name = None , sort = None , ** kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( "task_name should be string" ) self . _fill_project_info ( kwargs ) kwargs . update ( { 'status' : 'pending' } ) task = self . db . Task . find_one_and_update ( kwargs , { '$set' : { 'status' : 'running' } } , sort = sort ) try : if task is None : logging . info ( "[Database] Find Task FAIL: key: {} sort: {}" . format ( task_name , sort ) ) return False else : logging . info ( "[Database] Find Task SUCCESS: key: {} sort: {}" . format ( task_name , sort ) ) _datetime = task [ 'time' ] _script = task [ 'script' ] _id = task [ '_id' ] _hyper_parameters = task [ 'hyper_parameters' ] _saved_result_keys = task [ 'saved_result_keys' ] logging . info ( " hyper parameters:" ) for key in _hyper_parameters : globals ( ) [ key ] = _hyper_parameters [ key ] logging . info ( " {}: {}" . format ( key , _hyper_parameters [ key ] ) ) s = time . time ( ) logging . info ( "[Database] Start Task: key: {} sort: {} push time: {}" . format ( task_name , sort , _datetime ) ) _script = _script . decode ( 'utf-8' ) with tf . Graph ( ) . as_default ( ) : exec ( _script , globals ( ) ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'finished' } } ) __result = { } for _key in _saved_result_keys : logging . info ( " result: {}={} {}" . format ( _key , globals ( ) [ _key ] , type ( globals ( ) [ _key ] ) ) ) __result . update ( { "%s" % _key : globals ( ) [ _key ] } ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'result' : __result } } , return_document = pymongo . ReturnDocument . AFTER ) logging . info ( "[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s" . format ( task_name , sort , _datetime , time . time ( ) - s ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) logging . info ( "[Database] Fail to run task" ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'pending' } } ) return False
321	def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )
8183	def edge ( self , id1 , id2 ) : if id1 in self and id2 in self and self [ id2 ] in self [ id1 ] . links : return self [ id1 ] . links . edge ( id2 ) return None
11887	def receive ( self ) : try : buffer = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout as error : _LOGGER . error ( "Error receiving: %s" , error ) return "" buffering = True response = '' while buffering : if '\n' in buffer . decode ( "utf8" ) : response = buffer . decode ( "utf8" ) . split ( '\n' ) [ 0 ] buffering = False else : try : more = self . _socket . recv ( BUFFER_SIZE ) except socket . timeout : more = None if not more : buffering = False response = buffer . decode ( "utf8" ) else : buffer += more return response
11483	def _has_only_files ( local_folder ) : return not any ( os . path . isdir ( os . path . join ( local_folder , entry ) ) for entry in os . listdir ( local_folder ) )
11717	def create ( self , config ) : assert config [ "name" ] == self . name , "Given config is not for this template" data = self . _json_encode ( config ) headers = self . _default_headers ( ) return self . _request ( "" , ok_status = None , data = data , headers = headers )
6108	def yticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 0 ] ) , np . amax ( self . grid_stack . regular [ : , 0 ] ) , 4 )
2759	def get_all_load_balancers ( self ) : data = self . get_data ( "load_balancers" ) load_balancers = list ( ) for jsoned in data [ 'load_balancers' ] : load_balancer = LoadBalancer ( ** jsoned ) load_balancer . token = self . token load_balancer . health_check = HealthCheck ( ** jsoned [ 'health_check' ] ) load_balancer . sticky_sessions = StickySesions ( ** jsoned [ 'sticky_sessions' ] ) forwarding_rules = list ( ) for rule in jsoned [ 'forwarding_rules' ] : forwarding_rules . append ( ForwardingRule ( ** rule ) ) load_balancer . forwarding_rules = forwarding_rules load_balancers . append ( load_balancer ) return load_balancers
1306	def GetConsoleTitle ( ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleTitleW ( values , MAX_PATH ) return values . value
13038	def main ( ) : cred_search = CredentialSearch ( ) arg = argparse . ArgumentParser ( parents = [ cred_search . argparser ] , conflict_handler = 'resolve' ) arg . add_argument ( '-c' , '--count' , help = "Only show the number of results" , action = "store_true" ) arguments = arg . parse_args ( ) if arguments . count : print_line ( "Number of credentials: {}" . format ( cred_search . argument_count ( ) ) ) else : response = cred_search . get_credentials ( ) for hit in response : print_json ( hit . to_dict ( include_meta = True ) )
9395	def _set_scores ( self ) : anom_scores = { } self . _compute_derivatives ( ) derivatives_ema = utils . compute_ema ( self . smoothing_factor , self . derivatives ) for i , ( timestamp , value ) in enumerate ( self . time_series_items ) : anom_scores [ timestamp ] = abs ( self . derivatives [ i ] - derivatives_ema [ i ] ) stdev = numpy . std ( anom_scores . values ( ) ) if stdev : for timestamp in anom_scores . keys ( ) : anom_scores [ timestamp ] /= stdev self . anom_scores = TimeSeries ( self . _denoise_scores ( anom_scores ) )
9530	def encrypt ( base_field , key = None , ttl = None ) : if not isinstance ( base_field , models . Field ) : assert key is None assert ttl is None return get_encrypted_field ( base_field ) name , path , args , kwargs = base_field . deconstruct ( ) kwargs . update ( { 'key' : key , 'ttl' : ttl } ) return get_encrypted_field ( base_field . __class__ ) ( * args , ** kwargs )
13162	def raw_sql ( cls , cur , query : str , values : tuple ) : yield from cur . execute ( query , values ) return ( yield from cur . fetchall ( ) )
6318	def _find_last_of ( self , path , finders ) : found_path = None for finder in finders : result = finder . find ( path ) if result : found_path = result return found_path
3335	def byte_number_string ( number , thousandsSep = True , partition = False , base1024 = True , appendBytes = True ) : magsuffix = "" bytesuffix = "" if partition : magnitude = 0 if base1024 : while number >= 1024 : magnitude += 1 number = number >> 10 else : while number >= 1000 : magnitude += 1 number /= 1000.0 magsuffix = [ "" , "K" , "M" , "G" , "T" , "P" ] [ magnitude ] if appendBytes : if number == 1 : bytesuffix = " Byte" else : bytesuffix = " Bytes" if thousandsSep and ( number >= 1000 or magsuffix ) : snum = "{:,d}" . format ( number ) else : snum = str ( number ) return "{}{}{}" . format ( snum , magsuffix , bytesuffix )
7018	def parallel_concat_lcdir ( lcbasedir , objectidlist , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True , nworkers = 32 , maxworkertasks = 1000 ) : if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) tasks = [ ( lcbasedir , x , { 'aperture' : aperture , 'postfix' : postfix , 'sortby' : sortby , 'normalize' : normalize , 'outdir' : outdir , 'recursive' : recursive } ) for x in objectidlist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( parallel_concat_worker , tasks ) pool . close ( ) pool . join ( ) return { x : y for ( x , y ) in zip ( objectidlist , results ) }
4016	def get_app_volume_mounts ( app_name , assembled_specs , test = False ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] volumes = [ get_command_files_volume_mount ( app_name , test = test ) ] volumes . append ( get_asset_volume_mount ( app_name ) ) repo_mount = _get_app_repo_volume_mount ( app_spec ) if repo_mount : volumes . append ( repo_mount ) volumes += _get_app_libs_volume_mounts ( app_name , assembled_specs ) return volumes
13311	def site_path ( self ) : if platform == 'win' : return unipath ( self . path , 'Lib' , 'site-packages' ) py_ver = 'python{0}' . format ( sys . version [ : 3 ] ) return unipath ( self . path , 'lib' , py_ver , 'site-packages' )
11491	def download ( server_path , local_path = '.' ) : session . token = verify_credentials ( ) is_item , resource_id = _find_resource_id_from_path ( server_path ) if resource_id == - 1 : print ( 'Unable to locate {0}' . format ( server_path ) ) else : if is_item : _download_item ( resource_id , local_path ) else : _download_folder_recursive ( resource_id , local_path )
3918	def _handle_event ( self , conv_event ) : if not self . _is_scrolling : self . set_focus ( conv_event . id_ ) else : self . _modified ( )
3928	def _replace_words ( replacements , string ) : output_lines = [ ] for line in string . split ( '\n' ) : output_words = [ ] for word in line . split ( ' ' ) : new_word = replacements . get ( word , word ) output_words . append ( new_word ) output_lines . append ( output_words ) return '\n' . join ( ' ' . join ( output_words ) for output_words in output_lines )
7607	def search_tournaments ( self , name : str , ** params : keys ) : url = self . api . TOURNAMENT params [ 'name' ] = name return self . _get_model ( url , PartialTournament , ** params )
7989	def event ( self , event ) : event . stream = self logger . debug ( u"Stream event: {0}" . format ( event ) ) self . settings [ "event_queue" ] . put ( event ) return False
2087	def _convert_pagenum ( self , kwargs ) : for key in ( 'next' , 'previous' ) : if not kwargs . get ( key ) : continue match = re . search ( r'page=(?P<num>[\d]+)' , kwargs [ key ] ) if match is None and key == 'previous' : kwargs [ key ] = 1 continue kwargs [ key ] = int ( match . groupdict ( ) [ 'num' ] )
1895	def _recv ( self ) -> str : buf , left , right = self . __readline_and_count ( ) bufl = [ buf ] while left != right : buf , l , r = self . __readline_and_count ( ) bufl . append ( buf ) left += l right += r buf = '' . join ( bufl ) . strip ( ) logger . debug ( '<%s' , buf ) if '(error' in bufl [ 0 ] : raise Exception ( f"Error in smtlib: {bufl[0]}" ) return buf
9990	def get_object ( self , name ) : parts = name . split ( "." ) child = parts . pop ( 0 ) if parts : return self . spaces [ child ] . get_object ( "." . join ( parts ) ) else : return self . _namespace_impl [ child ]
7865	def handle_authorized ( self , event ) : request_software_version ( self . client , self . target_jid , self . success , self . failure )
5814	def _try_decode ( byte_string ) : try : return str_cls ( byte_string , _encoding ) except ( UnicodeDecodeError ) : for encoding in _fallback_encodings : try : return str_cls ( byte_string , encoding , errors = 'strict' ) except ( UnicodeDecodeError ) : pass return str_cls ( byte_string , errors = 'replace' )
10337	def build_spia_matrices ( nodes : Set [ str ] ) -> Dict [ str , pd . DataFrame ] : nodes = list ( sorted ( nodes ) ) matrices = OrderedDict ( ) for relation in KEGG_RELATIONS : matrices [ relation ] = pd . DataFrame ( 0 , index = nodes , columns = nodes ) return matrices
2790	def get_snapshots ( self ) : data = self . get_data ( "volumes/%s/snapshots/" % self . id ) snapshots = list ( ) for jsond in data [ u'snapshots' ] : snapshot = Snapshot ( ** jsond ) snapshot . token = self . token snapshots . append ( snapshot ) return snapshots
12627	def iter_recursive_find ( folder_path , * regex ) : for root , dirs , files in os . walk ( folder_path ) : if len ( files ) > 0 : outlist = [ ] for f in files : for reg in regex : if re . search ( reg , f ) : outlist . append ( op . join ( root , f ) ) if len ( outlist ) == len ( regex ) : yield outlist
7716	def remove_item ( self , jid , callback = None , error_callback = None ) : item = self . roster [ jid ] if jid not in self . roster : raise KeyError ( jid ) item = RosterItem ( jid , subscription = "remove" ) self . _roster_set ( item , callback , error_callback )
4489	def might_need_auth ( f ) : @ wraps ( f ) def wrapper ( cli_args ) : try : return_value = f ( cli_args ) except UnauthorizedException as e : config = config_from_env ( config_from_file ( ) ) username = _get_username ( cli_args , config ) if username is None : sys . exit ( "Please set a username (run `osf -h` for details)." ) else : sys . exit ( "You are not authorized to access this project." ) return return_value return wrapper
7771	def payload_class_for_element_name ( element_name ) : logger . debug ( " looking up payload class for element: {0!r}" . format ( element_name ) ) logger . debug ( " known: {0!r}" . format ( STANZA_PAYLOAD_CLASSES ) ) if element_name in STANZA_PAYLOAD_CLASSES : return STANZA_PAYLOAD_CLASSES [ element_name ] else : return XMLPayload
12425	def reverse ( self ) : if not self . test_drive and self . bumps : map ( lambda b : b . reverse ( ) , self . bumpers )
11923	def parse ( self ) : if exists ( self . filepath ) : content = open ( self . filepath ) . read ( ) . decode ( charset ) else : content = "" try : config = toml . loads ( content ) except toml . TomlSyntaxError : raise ConfigSyntaxError return config
4218	def _get_env ( self , env_var ) : value = os . environ . get ( env_var ) if not value : raise ValueError ( 'Missing environment variable:%s' % env_var ) return value
4235	def _convert ( value , to_type , default = None ) : try : return default if value is None else to_type ( value ) except ValueError : return default
9968	def copy ( self , space = None , name = None ) : return Cells ( space = space , name = name , formula = self . formula )
13416	def linkcode_resolve ( domain , info ) : if domain != 'py' : return None modname = info [ 'module' ] fullname = info [ 'fullname' ] submod = sys . modules . get ( modname ) if submod is None : return None obj = submod for part in fullname . split ( '.' ) : try : obj = getattr ( obj , part ) except : return None try : fn = inspect . getsourcefile ( obj ) except : fn = None if not fn : return None try : source , lineno = inspect . getsourcelines ( obj ) except : lineno = None if lineno : linespec = "#L%d-L%d" % ( lineno , lineno + len ( source ) - 1 ) else : linespec = "" fn = relpath ( fn , start = dirname ( scisalt . __file__ ) ) if 'dev' in scisalt . __version__ : return "http://github.com/joelfrederico/SciSalt/blob/master/scisalt/%s%s" % ( fn , linespec ) else : return "http://github.com/joelfrederico/SciSalt/blob/v%s/scisalt/%s%s" % ( scisalt . __version__ , fn , linespec )
11271	def to_str ( prev , encoding = None ) : first = next ( prev ) if isinstance ( first , str ) : if encoding is None : yield first for s in prev : yield s else : yield first . encode ( encoding ) for s in prev : yield s . encode ( encoding ) else : if encoding is None : encoding = sys . stdout . encoding or 'utf-8' yield first . decode ( encoding ) for s in prev : yield s . decode ( encoding )
12582	def get_3D_from_4D ( filename , vol_idx = 0 ) : def remove_4th_element_from_hdr_string ( hdr , fieldname ) : if fieldname in hdr : hdr [ fieldname ] = ' ' . join ( hdr [ fieldname ] . split ( ) [ : 3 ] ) vol , hdr = load_raw_data_with_mhd ( filename ) if vol . ndim != 4 : raise ValueError ( 'Volume in {} does not have 4 dimensions.' . format ( op . join ( op . dirname ( filename ) , hdr [ 'ElementDataFile' ] ) ) ) if not 0 <= vol_idx < vol . shape [ 3 ] : raise IndexError ( 'IndexError: 4th dimension in volume {} has {} volumes, not {}.' . format ( filename , vol . shape [ 3 ] , vol_idx ) ) new_vol = vol [ : , : , : , vol_idx ] . copy ( ) hdr [ 'NDims' ] = 3 remove_4th_element_from_hdr_string ( hdr , 'ElementSpacing' ) remove_4th_element_from_hdr_string ( hdr , 'DimSize' ) return new_vol , hdr
2162	def update ( self , inventory_source , monitor = False , wait = False , timeout = None , ** kwargs ) : debug . log ( 'Asking whether the inventory source can be updated.' , header = 'details' ) r = client . get ( '%s%d/update/' % ( self . endpoint , inventory_source ) ) if not r . json ( ) [ 'can_update' ] : raise exc . BadRequest ( 'Tower says it cannot run an update against this inventory source.' ) debug . log ( 'Updating the inventory source.' , header = 'details' ) r = client . post ( '%s%d/update/' % ( self . endpoint , inventory_source ) , data = { } ) inventory_update_id = r . json ( ) [ 'inventory_update' ] if monitor or wait : if monitor : result = self . monitor ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) elif wait : result = self . wait ( inventory_update_id , parent_pk = inventory_source , timeout = timeout ) inventory = client . get ( '/inventory_sources/%d/' % result [ 'inventory_source' ] ) . json ( ) [ 'inventory' ] result [ 'inventory' ] = int ( inventory ) return result return { 'id' : inventory_update_id , 'status' : 'ok' }
3062	def string_to_scopes ( scopes ) : if not scopes : return [ ] elif isinstance ( scopes , six . string_types ) : return scopes . split ( ' ' ) else : return scopes
6679	def getmtime ( self , path , use_sudo = False ) : func = use_sudo and run_as_root or self . run with self . settings ( hide ( 'running' , 'stdout' ) ) : return int ( func ( 'stat -c %%Y "%(path)s" ' % locals ( ) ) . strip ( ) )
3793	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . kappa , self . Tc = self . ais [ i ] , self . kappas [ i ] , self . Tcs [ i ]
9841	def __field ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'component' ) : component = self . __consume ( ) . value ( ) if not self . __consume ( ) . equals ( 'value' ) : raise DXParseError ( 'field: "value" expected' ) classid = self . __consume ( ) . value ( ) try : self . currentobject [ 'components' ] [ component ] = classid except KeyError : self . currentobject [ 'components' ] = { component : classid } else : raise DXParseError ( 'field: ' + str ( tok ) + ' not recognized.' )
7626	def transcription ( ref , est , ** kwargs ) : r namespace = 'pitch_contour' ref = coerce_annotation ( ref , namespace ) est = coerce_annotation ( est , namespace ) ref_intervals , ref_p = ref . to_interval_values ( ) est_intervals , est_p = est . to_interval_values ( ) ref_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in ref_p ] ) est_pitches = np . asarray ( [ p [ 'frequency' ] * ( - 1 ) ** ( ~ p [ 'voiced' ] ) for p in est_p ] ) return mir_eval . transcription . evaluate ( ref_intervals , ref_pitches , est_intervals , est_pitches , ** kwargs )
7415	def copy ( self ) : cp = copy . deepcopy ( self ) cp . genotypes = allel . GenotypeArray ( self . genotypes , copy = True ) return cp
2831	def set_training ( model , mode ) : if mode is None : yield return old_mode = model . training if old_mode != mode : model . train ( mode ) try : yield finally : if old_mode != mode : model . train ( old_mode )
11530	def __get_rev ( self , key , version , ** kwa ) : if '_doc' in kwa : doc = kwa [ '_doc' ] else : if type ( version ) is int : if version == 0 : order = pymongo . ASCENDING elif version == - 1 : order = pymongo . DESCENDING doc = self . _collection . find_one ( { 'k' : key } , sort = [ [ 'd' , order ] ] ) elif type ( version ) is datetime : ver = self . __round_time ( version ) doc = self . _collection . find_one ( { 'k' : key , 'd' : ver } ) if doc is None : raise KeyError ( 'Supplied key `{0}` or version `{1}` does not exist' . format ( key , str ( version ) ) ) coded_val = doc [ 'v' ] return pickle . loads ( coded_val )
7485	def concat_reads ( data , subsamples , ipyclient ) : if any ( [ len ( i . files . fastqs ) > 1 for i in subsamples ] ) : start = time . time ( ) printstr = " concatenating inputs | {} | s2 |" finished = 0 catjobs = { } for sample in subsamples : if len ( sample . files . fastqs ) > 1 : catjobs [ sample . name ] = ipyclient [ 0 ] . apply ( concat_multiple_inputs , * ( data , sample ) ) else : sample . files . concat = sample . files . fastqs while 1 : finished = sum ( [ i . ready ( ) for i in catjobs . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( catjobs ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( catjobs ) : print ( "" ) break for async in catjobs : if catjobs [ async ] . successful ( ) : data . samples [ async ] . files . concat = catjobs [ async ] . result ( ) else : error = catjobs [ async ] . result ( ) LOGGER . error ( "error in step2 concat %s" , error ) raise IPyradWarningExit ( "error in step2 concat: {}" . format ( error ) ) else : for sample in subsamples : sample . files . concat = sample . files . fastqs return subsamples
11372	def get_temporary_file ( prefix = "tmp_" , suffix = "" , directory = None ) : try : file_fd , filepath = mkstemp ( prefix = prefix , suffix = suffix , dir = directory ) os . close ( file_fd ) except IOError , e : try : os . remove ( filepath ) except Exception : pass raise e return filepath
6361	def sim_matrix ( src , tar , mat = None , mismatch_cost = 0 , match_cost = 1 , symmetric = True , alphabet = None , ) : if alphabet : alphabet = tuple ( alphabet ) for i in src : if i not in alphabet : raise ValueError ( 'src value not in alphabet' ) for i in tar : if i not in alphabet : raise ValueError ( 'tar value not in alphabet' ) if src == tar : if mat and ( src , src ) in mat : return mat [ ( src , src ) ] return match_cost if mat and ( src , tar ) in mat : return mat [ ( src , tar ) ] elif symmetric and mat and ( tar , src ) in mat : return mat [ ( tar , src ) ] return mismatch_cost
3672	def bubble_at_P ( P , zs , vapor_pressure_eqns , fugacities = None , gammas = None ) : def bubble_P_error ( T ) : Psats = [ VP ( T ) for VP in vapor_pressure_eqns ] Pcalc = bubble_at_T ( zs , Psats , fugacities , gammas ) return P - Pcalc T_bubble = newton ( bubble_P_error , 300 ) return T_bubble
7772	def _unquote ( data ) : if not data . startswith ( b'"' ) or not data . endswith ( b'"' ) : return data return QUOTE_RE . sub ( b"\\1" , data [ 1 : - 1 ] )
12671	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) : return self . __do_aggregate ( clazz , new_col , * args )
5352	def __studies ( self , retention_time ) : cfg = self . config . get_conf ( ) if 'studies' not in cfg [ self . backend_section ] or not cfg [ self . backend_section ] [ 'studies' ] : logger . debug ( 'No studies for %s' % self . backend_section ) return studies = [ study for study in cfg [ self . backend_section ] [ 'studies' ] if study . strip ( ) != "" ] if not studies : logger . debug ( 'No studies for %s' % self . backend_section ) return logger . debug ( "Executing studies for %s: %s" % ( self . backend_section , studies ) ) time . sleep ( 2 ) enrich_backend = self . _get_enrich_backend ( ) ocean_backend = self . _get_ocean_backend ( enrich_backend ) active_studies = [ ] all_studies = enrich_backend . studies all_studies_names = [ study . __name__ for study in enrich_backend . studies ] logger . debug ( "All studies in %s: %s" , self . backend_section , all_studies_names ) logger . debug ( "Configured studies %s" , studies ) cfg_studies_types = [ study . split ( ":" ) [ 0 ] for study in studies ] if not set ( cfg_studies_types ) . issubset ( set ( all_studies_names ) ) : logger . error ( 'Wrong studies names for %s: %s' , self . backend_section , studies ) raise RuntimeError ( 'Wrong studies names ' , self . backend_section , studies ) for study in enrich_backend . studies : if study . __name__ in cfg_studies_types : active_studies . append ( study ) enrich_backend . studies = active_studies print ( "Executing for %s the studies %s" % ( self . backend_section , [ study for study in studies ] ) ) studies_args = self . __load_studies ( ) do_studies ( ocean_backend , enrich_backend , studies_args , retention_time = retention_time ) enrich_backend . studies = all_studies
12753	def indices_for_body ( self , name , step = 3 ) : for j , body in enumerate ( self . bodies ) : if body . name == name : return list ( range ( j * step , ( j + 1 ) * step ) ) return [ ]
1565	def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : if len ( self . task_hooks ) > 0 : spout_ack_info = SpoutAckInfo ( message_id = message_id , spout_task_id = self . get_task_id ( ) , complete_latency_ms = complete_latency_ns * system_constants . NS_TO_MS ) for task_hook in self . task_hooks : task_hook . spout_ack ( spout_ack_info )
8409	def _extend_breaks ( self , major ) : trans = self . trans trans = trans if isinstance ( trans , type ) else trans . __class__ is_log = trans . __name__ . startswith ( 'log' ) diff = np . diff ( major ) step = diff [ 0 ] if is_log and all ( diff == step ) : major = np . hstack ( [ major [ 0 ] - step , major , major [ - 1 ] + step ] ) return major
6201	def simulate_timestamps_mix ( self , max_rates , populations , bg_rate , rs = None , seed = 1 , chunksize = 2 ** 16 , comp_filter = None , overwrite = False , skip_existing = False , scale = 10 , path = None , t_chunksize = None , timeslice = None ) : self . open_store_timestamp ( chunksize = chunksize , path = path ) rs = self . _get_group_randomstate ( rs , seed , self . ts_group ) if t_chunksize is None : t_chunksize = self . emission . chunkshape [ 1 ] timeslice_size = self . n_samples if timeslice is not None : timeslice_size = timeslice // self . t_step name = self . _get_ts_name_mix ( max_rates , populations , bg_rate , rs = rs ) kw = dict ( name = name , clk_p = self . t_step / scale , max_rates = max_rates , bg_rate = bg_rate , populations = populations , num_particles = self . num_particles , bg_particle = self . num_particles , overwrite = overwrite , chunksize = chunksize ) if comp_filter is not None : kw . update ( comp_filter = comp_filter ) try : self . _timestamps , self . _tparticles = ( self . ts_store . add_timestamps ( ** kw ) ) except ExistingArrayError as e : if skip_existing : print ( ' - Skipping already present timestamps array.' ) return else : raise e self . ts_group . _v_attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'init_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'PyBroMo' ] = __version__ ts_list , part_list = [ ] , [ ] bg_rates = [ None ] * ( len ( max_rates ) - 1 ) + [ bg_rate ] prev_time = 0 for i_start , i_end in iter_chunk_index ( timeslice_size , t_chunksize ) : curr_time = np . around ( i_start * self . t_step , decimals = 0 ) if curr_time > prev_time : print ( ' %.1fs' % curr_time , end = '' , flush = True ) prev_time = curr_time em_chunk = self . emission [ : , i_start : i_end ] times_chunk_s , par_index_chunk_s = self . _sim_timestamps_populations ( em_chunk , max_rates , populations , bg_rates , i_start , rs , scale ) ts_list . append ( times_chunk_s ) part_list . append ( par_index_chunk_s ) for ts , part in zip ( ts_list , part_list ) : self . _timestamps . append ( ts ) self . _tparticles . append ( part ) self . ts_group . _v_attrs [ 'last_random_state' ] = rs . get_state ( ) self . _timestamps . attrs [ 'last_random_state' ] = rs . get_state ( ) self . ts_store . h5file . flush ( )
6188	def print_summary ( string = 'Repository' , git_path = None ) : if git_path is None : git_path = GIT_PATH if not git_path_valid ( ) : print ( '\n%s revision unknown (git not found).' % string ) else : last_commit = get_last_commit_line ( ) print ( '\n{} revision:\n {}\n' . format ( string , last_commit ) ) if not check_clean_status ( ) : print ( '\nWARNING -> Uncommitted changes:' ) print ( get_status ( ) )
6751	def unregister ( self ) : for k in list ( env . keys ( ) ) : if k . startswith ( self . env_prefix ) : del env [ k ] try : del all_satchels [ self . name . upper ( ) ] except KeyError : pass try : del manifest_recorder [ self . name ] except KeyError : pass try : del manifest_deployers [ self . name . upper ( ) ] except KeyError : pass try : del manifest_deployers_befores [ self . name . upper ( ) ] except KeyError : pass try : del required_system_packages [ self . name . upper ( ) ] except KeyError : pass
8221	def do_toggle_fullscreen ( self , action ) : is_fullscreen = action . get_active ( ) if is_fullscreen : self . fullscreen ( ) else : self . unfullscreen ( )
12937	def depricated_name ( newmethod ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'always' , DeprecationWarning ) warnings . warn ( "Function {} is depricated, please use {} instead." . format ( func . __name__ , newmethod ) , category = DeprecationWarning , stacklevel = 2 ) warnings . simplefilter ( 'default' , DeprecationWarning ) return func ( * args , ** kwargs ) return wrapper return decorator
7007	def _fourier_func ( fourierparams , phase , mags ) : order = int ( len ( fourierparams ) / 2 ) f_amp = fourierparams [ : order ] f_pha = fourierparams [ order : ] f_orders = [ f_amp [ x ] * npcos ( 2.0 * pi_value * x * phase + f_pha [ x ] ) for x in range ( order ) ] total_f = npmedian ( mags ) for fo in f_orders : total_f += fo return total_f
71	def clip_out_of_image ( self ) : bbs_cut = [ bb . clip_out_of_image ( self . shape ) for bb in self . bounding_boxes if bb . is_partly_within_image ( self . shape ) ] return BoundingBoxesOnImage ( bbs_cut , shape = self . shape )
8800	def run_migrations_offline ( ) : context . configure ( url = neutron_config . database . connection ) with context . begin_transaction ( ) : context . run_migrations ( )
5781	def _create_buffers ( self , number ) : buffers = new ( secur32 , 'SecBuffer[%d]' % number ) for index in range ( 0 , number ) : buffers [ index ] . cbBuffer = 0 buffers [ index ] . BufferType = Secur32Const . SECBUFFER_EMPTY buffers [ index ] . pvBuffer = null ( ) sec_buffer_desc_pointer = struct ( secur32 , 'SecBufferDesc' ) sec_buffer_desc = unwrap ( sec_buffer_desc_pointer ) sec_buffer_desc . ulVersion = Secur32Const . SECBUFFER_VERSION sec_buffer_desc . cBuffers = number sec_buffer_desc . pBuffers = buffers return ( sec_buffer_desc_pointer , buffers )
1572	def setup ( self , context ) : myindex = context . get_partition_index ( ) self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] self . logger . info ( "TextFileSpout files to consume %s" % self . _files_to_consume ) self . _lines_to_consume = self . _get_next_lines ( ) self . _emit_count = 0
1791	def IMUL ( cpu , * operands ) : dest = operands [ 0 ] OperandSize = dest . size reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] arg0 = dest . read ( ) arg1 = None arg2 = None res = None if len ( operands ) == 1 : arg1 = cpu . read_register ( reg_name_l ) temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) res = Operators . EXTRACT ( temp , 0 , OperandSize ) elif len ( operands ) == 2 : arg1 = operands [ 1 ] . read ( ) arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) else : arg1 = operands [ 1 ] . read ( ) arg2 = operands [ 2 ] . read ( ) temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) cpu . OF = cpu . CF
7854	def add_identity ( self , item_name , item_category = None , item_type = None ) : return DiscoIdentity ( self , item_name , item_category , item_type )
2953	def container_id ( self , name ) : container = self . _containers . get ( name , None ) if not container is None : return container . get ( 'id' , None ) return None
10278	def get_neurommsig_score ( graph : BELGraph , genes : List [ Gene ] , ora_weight : Optional [ float ] = None , hub_weight : Optional [ float ] = None , top_percent : Optional [ float ] = None , topology_weight : Optional [ float ] = None ) -> float : ora_weight = ora_weight or 1.0 hub_weight = hub_weight or 1.0 topology_weight = topology_weight or 1.0 total_weight = ora_weight + hub_weight + topology_weight genes = list ( genes ) ora_score = neurommsig_gene_ora ( graph , genes ) hub_score = neurommsig_hubs ( graph , genes , top_percent = top_percent ) topology_score = neurommsig_topology ( graph , genes ) weighted_sum = ( ora_weight * ora_score + hub_weight * hub_score + topology_weight * topology_score ) return weighted_sum / total_weight
2265	def dict_isect ( * args ) : if not args : return { } else : dictclass = OrderedDict if isinstance ( args [ 0 ] , OrderedDict ) else dict common_keys = set . intersection ( * map ( set , args ) ) first_dict = args [ 0 ] return dictclass ( ( k , first_dict [ k ] ) for k in common_keys )
7798	def sasl_mechanism ( name , secure , preference = 50 ) : def decorator ( klass ) : klass . _pyxmpp_sasl_secure = secure klass . _pyxmpp_sasl_preference = preference if issubclass ( klass , ClientAuthenticator ) : _register_client_authenticator ( klass , name ) elif issubclass ( klass , ServerAuthenticator ) : _register_server_authenticator ( klass , name ) else : raise TypeError ( "Not a ClientAuthenticator" " or ServerAuthenticator class" ) return klass return decorator
7895	def set_subject ( self , subject ) : m = Message ( to_jid = self . room_jid . bare ( ) , stanza_type = "groupchat" , subject = subject ) self . manager . stream . send ( m )
2401	def gen_length_feats ( self , e_set ) : text = e_set . _text lengths = [ len ( e ) for e in text ] word_counts = [ max ( len ( t ) , 1 ) for t in e_set . _tokens ] comma_count = [ e . count ( "," ) for e in text ] ap_count = [ e . count ( "'" ) for e in text ] punc_count = [ e . count ( "." ) + e . count ( "?" ) + e . count ( "!" ) for e in text ] chars_per_word = [ lengths [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] good_pos_tags , bad_pos_positions = self . _get_grammar_errors ( e_set . _pos , e_set . _text , e_set . _tokens ) good_pos_tag_prop = [ good_pos_tags [ m ] / float ( word_counts [ m ] ) for m in xrange ( 0 , len ( text ) ) ] length_arr = numpy . array ( ( lengths , word_counts , comma_count , ap_count , punc_count , chars_per_word , good_pos_tags , good_pos_tag_prop ) ) . transpose ( ) return length_arr . copy ( )
1392	def synch_topologies ( self ) : self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) try : for state_manager in self . state_managers : state_manager . start ( ) except Exception as ex : Log . error ( "Found exception while initializing state managers: %s. Bailing out..." % ex ) traceback . print_exc ( ) sys . exit ( 1 ) def on_topologies_watch ( state_manager , topologies ) : Log . info ( "State watch triggered for topologies." ) Log . debug ( "Topologies: " + str ( topologies ) ) existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) existingTopNames = map ( lambda t : t . name , existingTopologies ) Log . debug ( "Existing topologies: " + str ( existingTopNames ) ) for name in existingTopNames : if name not in topologies : Log . info ( "Removing topology: %s in rootpath: %s" , name , state_manager . rootpath ) self . removeTopology ( name , state_manager . name ) for name in topologies : if name not in existingTopNames : self . addNewTopology ( state_manager , name ) for state_manager in self . state_managers : onTopologiesWatch = partial ( on_topologies_watch , state_manager ) state_manager . get_topologies ( onTopologiesWatch )
6104	def luminosities_of_galaxies_within_ellipses_in_units ( self , major_axis : dim . Length , unit_luminosity = 'eps' , exposure_time = None ) : return list ( map ( lambda galaxy : galaxy . luminosity_within_ellipse_in_units ( major_axis = major_axis , unit_luminosity = unit_luminosity , kpc_per_arcsec = self . kpc_per_arcsec , exposure_time = exposure_time ) , self . galaxies ) )
9842	def use_parser ( self , parsername ) : self . __parser = self . parsers [ parsername ] self . __parser ( )
3391	def prune_unused_metabolites ( cobra_model ) : output_model = cobra_model . copy ( ) inactive_metabolites = [ m for m in output_model . metabolites if len ( m . reactions ) == 0 ] output_model . remove_metabolites ( inactive_metabolites ) return output_model , inactive_metabolites
4786	def ends_with ( self , suffix ) : if suffix is None : raise TypeError ( 'given suffix arg must not be none' ) if isinstance ( self . val , str_types ) : if not isinstance ( suffix , str_types ) : raise TypeError ( 'given suffix arg must be a string' ) if len ( suffix ) == 0 : raise ValueError ( 'given suffix arg must not be empty' ) if not self . val . endswith ( suffix ) : self . _err ( 'Expected <%s> to end with <%s>, but did not.' % ( self . val , suffix ) ) elif isinstance ( self . val , Iterable ) : if len ( self . val ) == 0 : raise ValueError ( 'val must not be empty' ) last = None for last in self . val : pass if last != suffix : self . _err ( 'Expected %s to end with <%s>, but did not.' % ( self . val , suffix ) ) else : raise TypeError ( 'val is not a string or iterable' ) return self
13372	def expandpath ( path ) : return os . path . abspath ( os . path . expandvars ( os . path . expanduser ( path ) ) )
5480	def cancel ( batch_fn , cancel_fn , ops ) : canceled_ops = [ ] error_messages = [ ] max_batch = 256 total_ops = len ( ops ) for first_op in range ( 0 , total_ops , max_batch ) : batch_canceled , batch_messages = _cancel_batch ( batch_fn , cancel_fn , ops [ first_op : first_op + max_batch ] ) canceled_ops . extend ( batch_canceled ) error_messages . extend ( batch_messages ) return canceled_ops , error_messages
11707	def generate_gamete ( self , egg_or_sperm_word ) : p_rate_of_mutation = [ 0.9 , 0.1 ] should_use_mutant_pool = ( npchoice ( [ 0 , 1 ] , 1 , p = p_rate_of_mutation ) [ 0 ] == 1 ) if should_use_mutant_pool : pool = tokens . secondary_tokens else : pool = tokens . primary_tokens return get_matches ( egg_or_sperm_word , pool , 23 )
9534	def get_version ( version = None ) : version = get_complete_version ( version ) main = get_main_version ( version ) sub = '' if version [ 3 ] == 'alpha' and version [ 4 ] == 0 : git_changeset = get_git_changeset ( ) if git_changeset : sub = '.dev%s' % git_changeset elif version [ 3 ] != 'final' : mapping = { 'alpha' : 'a' , 'beta' : 'b' , 'rc' : 'c' } sub = mapping [ version [ 3 ] ] + str ( version [ 4 ] ) return str ( main + sub )
5681	def get_spreading_trips ( self , start_time_ut , lat , lon , max_duration_ut = 4 * 3600 , min_transfer_time = 30 , use_shapes = False ) : from gtfspy . spreading . spreader import Spreader spreader = Spreader ( self , start_time_ut , lat , lon , max_duration_ut , min_transfer_time , use_shapes ) return spreader . spread ( )
11211	def normalized ( self ) : days = int ( self . days ) hours_f = round ( self . hours + 24 * ( self . days - days ) , 11 ) hours = int ( hours_f ) minutes_f = round ( self . minutes + 60 * ( hours_f - hours ) , 10 ) minutes = int ( minutes_f ) seconds_f = round ( self . seconds + 60 * ( minutes_f - minutes ) , 8 ) seconds = int ( seconds_f ) microseconds = round ( self . microseconds + 1e6 * ( seconds_f - seconds ) ) return self . __class__ ( years = self . years , months = self . months , days = days , hours = hours , minutes = minutes , seconds = seconds , microseconds = microseconds , leapdays = self . leapdays , year = self . year , month = self . month , day = self . day , weekday = self . weekday , hour = self . hour , minute = self . minute , second = self . second , microsecond = self . microsecond )
12670	def create ( _ ) : endpoint = client_endpoint ( ) if not endpoint : raise CLIError ( "Connection endpoint not found. " "Before running sfctl commands, connect to a cluster using " "the 'sfctl cluster select' command." ) no_verify = no_verify_setting ( ) if security_type ( ) == 'aad' : auth = AdalAuthentication ( no_verify ) else : cert = cert_info ( ) ca_cert = ca_cert_info ( ) auth = ClientCertAuthentication ( cert , ca_cert , no_verify ) return ServiceFabricClientAPIs ( auth , base_url = endpoint )
12163	def _check_limit ( self , event ) : if self . count ( event ) > self . max_listeners : warnings . warn ( 'Too many listeners for event {}' . format ( event ) , ResourceWarning , )
11769	def printf ( format , * args ) : sys . stdout . write ( str ( format ) % args ) return if_ ( args , lambda : args [ - 1 ] , lambda : format )
2749	def get_droplet ( self , droplet_id ) : return Droplet . get_object ( api_token = self . token , droplet_id = droplet_id )
13392	def paginate_update ( update ) : from happenings . models import Update time = update . pub_time event = update . event try : next = Update . objects . filter ( event = event , pub_time__gt = time ) . order_by ( 'pub_time' ) . only ( 'title' ) [ 0 ] except : next = None try : previous = Update . objects . filter ( event = event , pub_time__lt = time ) . order_by ( '-pub_time' ) . only ( 'title' ) [ 0 ] except : previous = None return { 'next' : next , 'previous' : previous , 'event' : event }
8947	def run_elective ( self , cmd , * args , ** kwargs ) : if self . _commit : return self . run ( cmd , * args , ** kwargs ) else : notify . warning ( "WOULD RUN: {}" . format ( cmd ) ) kwargs = kwargs . copy ( ) kwargs [ 'echo' ] = False return self . run ( 'true' , * args , ** kwargs )
5298	def get_queryset ( self ) : qs = super ( BaseCalendarMonthView , self ) . get_queryset ( ) year = self . get_year ( ) month = self . get_month ( ) date_field = self . get_date_field ( ) end_date_field = self . get_end_date_field ( ) date = _date_from_string ( year , self . get_year_format ( ) , month , self . get_month_format ( ) ) since = date until = self . get_next_month ( date ) if since . weekday ( ) != self . get_first_of_week ( ) : diff = math . fabs ( since . weekday ( ) - self . get_first_of_week ( ) ) since = since - datetime . timedelta ( days = diff ) if until . weekday ( ) != ( ( self . get_first_of_week ( ) + 6 ) % 7 ) : diff = math . fabs ( ( ( self . get_first_of_week ( ) + 6 ) % 7 ) - until . weekday ( ) ) until = until + datetime . timedelta ( days = diff ) if end_date_field : predicate1 = Q ( ** { '%s__gte' % date_field : since , end_date_field : None } ) predicate2 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % end_date_field : until } ) predicate3 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : since , '%s__lt' % end_date_field : until } ) predicate4 = Q ( ** { '%s__gte' % date_field : since , '%s__lt' % date_field : until , '%s__gte' % end_date_field : until } ) predicate5 = Q ( ** { '%s__lt' % date_field : since , '%s__gte' % end_date_field : until } ) return qs . filter ( predicate1 | predicate2 | predicate3 | predicate4 | predicate5 ) return qs . filter ( ** { '%s__gte' % date_field : since } )
2146	def _separate ( self , kwargs ) : self . _pop_none ( kwargs ) result = { } for field in Resource . config_fields : if field in kwargs : result [ field ] = kwargs . pop ( field ) if field in Resource . json_fields : if not isinstance ( result [ field ] , six . string_types ) : continue try : data = json . loads ( result [ field ] ) result [ field ] = data except ValueError : raise exc . TowerCLIError ( 'Provided json file format ' 'invalid. Please recheck.' ) return result
5588	def calculate_slope_aspect ( elevation , xres , yres , z = 1.0 , scale = 1.0 ) : z = float ( z ) scale = float ( scale ) height , width = elevation . shape [ 0 ] - 2 , elevation . shape [ 1 ] - 2 window = [ z * elevation [ row : ( row + height ) , col : ( col + width ) ] for ( row , col ) in product ( range ( 3 ) , range ( 3 ) ) ] x = ( ( window [ 0 ] + window [ 3 ] + window [ 3 ] + window [ 6 ] ) - ( window [ 2 ] + window [ 5 ] + window [ 5 ] + window [ 8 ] ) ) / ( 8.0 * xres * scale ) y = ( ( window [ 6 ] + window [ 7 ] + window [ 7 ] + window [ 8 ] ) - ( window [ 0 ] + window [ 1 ] + window [ 1 ] + window [ 2 ] ) ) / ( 8.0 * yres * scale ) slope = math . pi / 2 - np . arctan ( np . sqrt ( x * x + y * y ) ) aspect = np . arctan2 ( x , y ) return slope , aspect
6738	def get_last_modified_timestamp ( path , ignore = None ) : ignore = ignore or [ ] if not isinstance ( path , six . string_types ) : return ignore_str = '' if ignore : assert isinstance ( ignore , ( tuple , list ) ) ignore_str = ' ' . join ( "! -name '%s'" % _ for _ in ignore ) cmd = 'find "' + path + '" ' + ignore_str + ' -type f -printf "%T@ %p\n" | sort -n | tail -1 | cut -f 1 -d " "' ret = subprocess . check_output ( cmd , shell = True ) try : ret = round ( float ( ret ) , 2 ) except ValueError : return return ret
8438	def _patched_run_hook ( hook_name , project_dir , context ) : if hook_name == 'post_gen_project' : with temple . utils . cd ( project_dir ) : temple . utils . write_temple_config ( context [ 'cookiecutter' ] , context [ 'template' ] , context [ 'version' ] ) return cc_hooks . run_hook ( hook_name , project_dir , context )
9660	def merge_from_store_and_in_mems ( from_store , in_mem_shas , dont_update_shas_of ) : if not from_store : for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas for key in from_store [ 'files' ] : if key not in in_mem_shas [ 'files' ] and key not in dont_update_shas_of : in_mem_shas [ 'files' ] [ key ] = from_store [ 'files' ] [ key ] for item in dont_update_shas_of : if item in in_mem_shas [ 'files' ] : del in_mem_shas [ 'files' ] [ item ] return in_mem_shas
2052	def LDRD ( cpu , dest1 , dest2 , src , offset = None ) : assert dest1 . type == 'register' assert dest2 . type == 'register' assert src . type == 'memory' mem1 = cpu . read_int ( src . address ( ) , 32 ) mem2 = cpu . read_int ( src . address ( ) + 4 , 32 ) writeback = cpu . _compute_writeback ( src , offset ) dest1 . write ( mem1 ) dest2 . write ( mem2 ) cpu . _cs_hack_ldr_str_writeback ( src , offset , writeback )
9697	def check_nonce ( self , request , oauth_request ) : oauth_nonce = oauth_request [ 'oauth_nonce' ] oauth_timestamp = oauth_request [ 'oauth_timestamp' ] return check_nonce ( request , oauth_request , oauth_nonce , oauth_timestamp )
8546	def delete_datacenter ( self , datacenter_id ) : response = self . _perform_request ( url = '/datacenters/%s' % ( datacenter_id ) , method = 'DELETE' ) return response
3250	def get_short_version ( self ) : gs_version = self . get_version ( ) match = re . compile ( r'[^\d.]+' ) return match . sub ( '' , gs_version ) . strip ( '.' )
8174	def goal ( self , x , y , z , d = 50.0 ) : return ( x - self . x ) / d , ( y - self . y ) / d , ( z - self . z ) / d
4417	async def play_now ( self , requester : int , track : dict ) : self . add_next ( requester , track ) await self . play ( ignore_shuffle = True )
2403	def gen_feats ( self , e_set ) : bag_feats = self . gen_bag_feats ( e_set ) length_feats = self . gen_length_feats ( e_set ) prompt_feats = self . gen_prompt_feats ( e_set ) overall_feats = numpy . concatenate ( ( length_feats , prompt_feats , bag_feats ) , axis = 1 ) overall_feats = overall_feats . copy ( ) return overall_feats
11018	def balance ( ctx ) : backend = plugins_registry . get_backends_by_class ( ZebraBackend ) [ 0 ] timesheet_collection = get_timesheet_collection_for_context ( ctx , None ) hours_to_be_pushed = timesheet_collection . get_hours ( pushed = False , ignored = False , unmapped = False ) today = datetime . date . today ( ) user_info = backend . get_user_info ( ) timesheets = backend . get_timesheets ( get_first_dow ( today ) , get_last_dow ( today ) ) total_duration = sum ( [ float ( timesheet [ 'time' ] ) for timesheet in timesheets ] ) vacation = hours_to_days ( user_info [ 'vacation' ] [ 'difference' ] ) vacation_balance = '{} days, {:.2f} hours' . format ( * vacation ) hours_balance = user_info [ 'hours' ] [ 'hours' ] [ 'balance' ] click . echo ( "Hours balance: {}" . format ( signed_number ( hours_balance ) ) ) click . echo ( "Hours balance after push: {}" . format ( signed_number ( hours_balance + hours_to_be_pushed ) ) ) click . echo ( "Hours done this week: {:.2f}" . format ( total_duration ) ) click . echo ( "Vacation left: {}" . format ( vacation_balance ) )
7342	def set_debug ( ) : logging . basicConfig ( level = logging . WARNING ) peony . logger . setLevel ( logging . DEBUG )
10386	def match_simple_metapath ( graph , node , simple_metapath ) : if 0 == len ( simple_metapath ) : yield node , else : for neighbor in graph . edges [ node ] : if graph . nodes [ neighbor ] [ FUNCTION ] == simple_metapath [ 0 ] : for path in match_simple_metapath ( graph , neighbor , simple_metapath [ 1 : ] ) : if node not in path : yield ( node , ) + path
7121	def filter_config ( config , deploy_config ) : if not os . path . isfile ( deploy_config ) : return DotDict ( ) config_module = get_config_module ( deploy_config ) return config_module . filter ( config )
5105	def poisson_random_measure ( t , rate , rate_max ) : scale = 1.0 / rate_max t = t + exponential ( scale ) while rate_max * uniform ( ) > rate ( t ) : t = t + exponential ( scale ) return t
10986	def get_particles_featuring ( feature_rad , state_name = None , im_name = None , use_full_path = False , actual_rad = None , invert = True , featuring_params = { } , ** kwargs ) : state_name , im_name = _pick_state_im_name ( state_name , im_name , use_full_path = use_full_path ) s = states . load ( state_name ) if actual_rad is None : actual_rad = np . median ( s . obj_get_radii ( ) ) im = util . RawImage ( im_name , tile = s . image . tile ) pos = locate_spheres ( im , feature_rad , invert = invert , ** featuring_params ) _ = s . obj_remove_particle ( np . arange ( s . obj_get_radii ( ) . size ) ) s . obj_add_particle ( pos , np . ones ( pos . shape [ 0 ] ) * actual_rad ) s . set_image ( im ) _translate_particles ( s , invert = invert , ** kwargs ) return s
318	def calc_bootstrap ( func , returns , * args , ** kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , ** kwargs ) else : out [ i ] = func ( returns_i , * args , ** kwargs ) return out
4318	def _stat_call ( filepath ) : validate_input_file ( filepath ) args = [ 'sox' , filepath , '-n' , 'stat' ] _ , _ , stat_output = sox ( args ) return stat_output
775	def _abbreviate ( text , threshold ) : if text is not None and len ( text ) > threshold : text = text [ : threshold ] + "..." return text
12397	def gen_method_keys ( self , * args , ** kwargs ) : token = args [ 0 ] for mro_type in type ( token ) . __mro__ [ : - 1 ] : name = mro_type . __name__ yield name
203	def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices
3366	def _process_flux_dataframe ( flux_dataframe , fva , threshold , floatfmt ) : abs_flux = flux_dataframe [ 'flux' ] . abs ( ) flux_threshold = threshold * abs_flux . max ( ) if fva is None : flux_dataframe = flux_dataframe . loc [ abs_flux >= flux_threshold , : ] . copy ( ) else : flux_dataframe = flux_dataframe . loc [ ( abs_flux >= flux_threshold ) | ( flux_dataframe [ 'fmin' ] . abs ( ) >= flux_threshold ) | ( flux_dataframe [ 'fmax' ] . abs ( ) >= flux_threshold ) , : ] . copy ( ) if fva is None : flux_dataframe [ 'is_input' ] = ( flux_dataframe [ 'flux' ] >= 0 ) flux_dataframe [ 'flux' ] = flux_dataframe [ 'flux' ] . abs ( ) else : def get_direction ( flux , fmin , fmax ) : if flux < 0 : return - 1 elif flux > 0 : return 1 elif ( fmax > 0 ) & ( fmin <= 0 ) : return 1 elif ( fmax < 0 ) & ( fmin >= 0 ) : return - 1 elif ( ( fmax + fmin ) / 2 ) < 0 : return - 1 else : return 1 sign = flux_dataframe . apply ( lambda x : get_direction ( x . flux , x . fmin , x . fmax ) , 1 ) flux_dataframe [ 'is_input' ] = sign == 1 flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . multiply ( sign , 0 ) . astype ( 'float' ) . round ( 6 ) flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] = flux_dataframe . loc [ : , [ 'flux' , 'fmin' , 'fmax' ] ] . applymap ( lambda x : x if abs ( x ) > 1E-6 else 0 ) if fva is not None : flux_dataframe [ 'fva_fmt' ] = flux_dataframe . apply ( lambda x : ( "[{0.fmin:" + floatfmt + "}, {0.fmax:" + floatfmt + "}]" ) . format ( x ) , 1 ) flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'fmax' , 'fmin' , 'id' ] , ascending = [ False , False , False , True ] ) else : flux_dataframe = flux_dataframe . sort_values ( by = [ 'flux' , 'id' ] , ascending = [ False , True ] ) return flux_dataframe
8078	def arrow ( self , x , y , width , type = NORMAL , draw = True , ** kwargs ) : path = self . BezierPath ( ** kwargs ) if type == self . NORMAL : head = width * .4 tail = width * .2 path . moveto ( x , y ) path . lineto ( x - head , y + head ) path . lineto ( x - head , y + tail ) path . lineto ( x - width , y + tail ) path . lineto ( x - width , y - tail ) path . lineto ( x - head , y - tail ) path . lineto ( x - head , y - head ) path . lineto ( x , y ) elif type == self . FORTYFIVE : head = .3 tail = 1 + head path . moveto ( x , y ) path . lineto ( x , y + width * ( 1 - head ) ) path . lineto ( x - width * head , y + width ) path . lineto ( x - width * head , y + width * tail * .4 ) path . lineto ( x - width * tail * .6 , y + width ) path . lineto ( x - width , y + width * tail * .6 ) path . lineto ( x - width * tail * .4 , y + width * head ) path . lineto ( x - width , y + width * head ) path . lineto ( x - width * ( 1 - head ) , y ) path . lineto ( x , y ) else : raise NameError ( _ ( "arrow: available types for arrow() are NORMAL and FORTYFIVE\n" ) ) if draw : path . draw ( ) return path
1742	def make_grid ( tensor , nrow = 8 , padding = 2 , pad_value = 0 ) : if not ( isinstance ( tensor , np . ndarray ) or ( isinstance ( tensor , list ) and all ( isinstance ( t , np . ndarray ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) if isinstance ( tensor , list ) : tensor = np . stack ( tensor , 0 ) if tensor . ndim == 2 : tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] ) ) if tensor . ndim == 3 : if tensor . shape [ 0 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . reshape ( ( 1 , tensor . shape [ 0 ] , tensor . shape [ 1 ] , tensor . shape [ 2 ] ) ) if tensor . ndim == 4 and tensor . shape [ 1 ] == 1 : tensor = np . concatenate ( ( tensor , tensor , tensor ) , 1 ) if tensor . shape [ 0 ] == 1 : return np . squeeze ( tensor ) nmaps = tensor . shape [ 0 ] xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . shape [ 2 ] + padding ) , int ( tensor . shape [ 3 ] + padding ) grid = np . ones ( ( 3 , height * ymaps + padding , width * xmaps + padding ) ) * pad_value k = 0 for y in range ( ymaps ) : for x in range ( xmaps ) : if k >= nmaps : break grid [ : , y * height + padding : ( y + 1 ) * height , x * width + padding : ( x + 1 ) * width ] = tensor [ k ] k = k + 1 return grid
7144	def transfer ( self , address , amount , priority = prio . NORMAL , payment_id = None , unlock_time = 0 , relay = True ) : return self . _backend . transfer ( [ ( address , amount ) ] , priority , payment_id , unlock_time , account = self . index , relay = relay )
5080	def parse_course_key ( course_identifier ) : try : course_run_key = CourseKey . from_string ( course_identifier ) except InvalidKeyError : return course_identifier return quote_plus ( ' ' . join ( [ course_run_key . org , course_run_key . course ] ) )
10916	def calc_particle_group_region_size ( s , region_size = 40 , max_mem = 1e9 , ** kwargs ) : region_size = np . array ( region_size ) . astype ( 'int' ) def calc_mem_usage ( region_size ) : rs = np . array ( region_size ) particle_groups = separate_particles_into_groups ( s , region_size = rs . tolist ( ) , ** kwargs ) numpart = [ np . size ( g ) for g in particle_groups ] biggroups = [ particle_groups [ i ] for i in np . argsort ( numpart ) [ - 5 : ] ] def get_tile_jsize ( group ) : nms = s . param_particle ( group ) tile = s . get_update_io_tiles ( nms , s . get_values ( nms ) ) [ 2 ] return tile . shape . prod ( ) * len ( nms ) mems = [ 8 * get_tile_jsize ( g ) for g in biggroups ] return np . max ( mems ) im_shape = s . oshape . shape if calc_mem_usage ( region_size ) > max_mem : while ( ( calc_mem_usage ( region_size ) > max_mem ) and np . any ( region_size > 2 ) ) : region_size = np . clip ( region_size - 1 , 2 , im_shape ) else : while ( ( calc_mem_usage ( region_size ) < max_mem ) and np . any ( region_size < im_shape ) ) : region_size = np . clip ( region_size + 1 , 2 , im_shape ) region_size -= 1 return region_size
3232	def get_user_agent_default ( pkg_name = 'cloudaux' ) : version = '0.0.1' try : import pkg_resources version = pkg_resources . get_distribution ( pkg_name ) . version except pkg_resources . DistributionNotFound : pass except ImportError : pass return 'cloudaux/%s' % ( version )
9150	def to_indra_statements ( self , * args , ** kwargs ) : graph = self . to_bel ( * args , ** kwargs ) return to_indra_statements ( graph )
4096	def KIC ( N , rho , k ) : r from numpy import log , array res = log ( rho ) + 3. * ( k + 1. ) / float ( N ) return res
3849	async def fetch ( self , method , url , params = None , headers = None , data = None ) : logger . debug ( 'Sending request %s %s:\n%r' , method , url , data ) for retry_num in range ( MAX_RETRIES ) : try : async with self . fetch_raw ( method , url , params = params , headers = headers , data = data ) as res : async with async_timeout . timeout ( REQUEST_TIMEOUT ) : body = await res . read ( ) logger . debug ( 'Received response %d %s:\n%r' , res . status , res . reason , body ) except asyncio . TimeoutError : error_msg = 'Request timed out' except aiohttp . ServerDisconnectedError as err : error_msg = 'Server disconnected error: {}' . format ( err ) except ( aiohttp . ClientError , ValueError ) as err : error_msg = 'Request connection error: {}' . format ( err ) else : break logger . info ( 'Request attempt %d failed: %s' , retry_num , error_msg ) else : logger . info ( 'Request failed after %d attempts' , MAX_RETRIES ) raise exceptions . NetworkError ( error_msg ) if res . status != 200 : logger . info ( 'Request returned unexpected status: %d %s' , res . status , res . reason ) raise exceptions . NetworkError ( 'Request return unexpected status: {}: {}' . format ( res . status , res . reason ) ) return FetchResponse ( res . status , body )
7035	def submit_post_searchquery ( url , data , apikey ) : postdata = { } for key in data : if key == 'columns' : postdata [ 'columns[]' ] = data [ key ] elif key == 'collections' : postdata [ 'collections[]' ] = data [ key ] else : postdata [ key ] = data [ key ] encoded_postdata = urlencode ( postdata , doseq = True ) . encode ( ) if apikey : headers = { 'Authorization' : 'Bearer: %s' % apikey } else : headers = { } LOGINFO ( 'submitting search query to LCC-Server API URL: %s' % url ) try : req = Request ( url , data = encoded_postdata , headers = headers ) resp = urlopen ( req ) if resp . code == 200 : for line in resp : data = json . loads ( line ) msg = data [ 'message' ] status = data [ 'status' ] if status != 'failed' : LOGINFO ( 'status: %s, %s' % ( status , msg ) ) else : LOGERROR ( 'status: %s, %s' % ( status , msg ) ) if status in ( 'ok' , 'background' ) : setid = data [ 'result' ] [ 'setid' ] outpickle = os . path . join ( os . path . expanduser ( '~' ) , '.astrobase' , 'lccs' , 'query-%s.pkl' % setid ) if not os . path . exists ( os . path . dirname ( outpickle ) ) : os . makedirs ( os . path . dirname ( outpickle ) ) with open ( outpickle , 'wb' ) as outfd : pickle . dump ( data , outfd , pickle . HIGHEST_PROTOCOL ) LOGINFO ( 'saved query info to %s, use this to ' 'download results later with ' 'retrieve_dataset_files' % outpickle ) return status , data , data [ 'result' ] [ 'setid' ] elif status == 'failed' : return status , data , None else : try : data = json . load ( resp ) msg = data [ 'message' ] LOGERROR ( msg ) return 'failed' , None , None except Exception as e : LOGEXCEPTION ( 'failed to submit query to %s' % url ) return 'failed' , None , None except HTTPError as e : LOGERROR ( 'could not submit query to LCC API at: %s' % url ) LOGERROR ( 'HTTP status code was %s, reason: %s' % ( e . code , e . reason ) ) return 'failed' , None , None
6351	def _phonetic_numbers ( self , phonetic ) : phonetic_array = phonetic . split ( '-' ) result = ' ' . join ( [ self . _pnums_with_leading_space ( i ) [ 1 : ] for i in phonetic_array ] ) return result
7479	def build_clustbits ( data , ipyclient , force ) : if os . path . exists ( data . tmpdir ) : shutil . rmtree ( data . tmpdir ) os . mkdir ( data . tmpdir ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " building clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) uhandle = os . path . join ( data . dirs . across , data . name + ".utemp" ) usort = os . path . join ( data . dirs . across , data . name + ".utemp.sort" ) async1 = "" if not os . path . exists ( usort ) or force : LOGGER . info ( "building reads file -- loading utemp file into mem" ) async1 = lbview . apply ( sort_seeds , * ( uhandle , usort ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async1 . ready ( ) : break else : time . sleep ( 0.1 ) async2 = lbview . apply ( count_seeds , usort ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 1 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async2 . ready ( ) : break else : time . sleep ( 0.1 ) nseeds = async2 . result ( ) async3 = lbview . apply ( sub_build_clustbits , * ( data , usort , nseeds ) ) while 1 : elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 2 , printstr . format ( elapsed ) , spacer = data . _spacer ) if async3 . ready ( ) : break else : time . sleep ( 0.1 ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 3 , 3 , printstr . format ( elapsed ) , spacer = data . _spacer ) print ( "" ) for job in [ async1 , async2 , async3 ] : try : if not job . successful ( ) : raise IPyradWarningExit ( job . result ( ) ) except AttributeError : pass
5969	def em_schedule ( ** kwargs ) : mdrunner = kwargs . pop ( 'mdrunner' , None ) integrators = kwargs . pop ( 'integrators' , [ 'l-bfgs' , 'steep' ] ) kwargs . pop ( 'integrator' , None ) nsteps = kwargs . pop ( 'nsteps' , [ 100 , 1000 ] ) outputs = [ 'em{0:03d}_{1!s}.pdb' . format ( i , integrator ) for i , integrator in enumerate ( integrators ) ] outputs [ - 1 ] = kwargs . pop ( 'output' , 'em.pdb' ) files = { 'struct' : kwargs . pop ( 'struct' , None ) } for i , integrator in enumerate ( integrators ) : struct = files [ 'struct' ] logger . info ( "[em %d] energy minimize with %s for maximum %d steps" , i , integrator , nsteps [ i ] ) kwargs . update ( { 'struct' : struct , 'output' : outputs [ i ] , 'integrator' : integrator , 'nsteps' : nsteps [ i ] } ) if not integrator == 'l-bfgs' : kwargs [ 'mdrunner' ] = mdrunner else : kwargs [ 'mdrunner' ] = None logger . warning ( "[em %d] Not using mdrunner for L-BFGS because it cannot " "do parallel runs." , i ) files = energy_minimize ( ** kwargs ) return files
9940	def set_options ( self , ** options ) : self . interactive = options [ 'interactive' ] self . verbosity = options [ 'verbosity' ] self . symlink = options [ 'link' ] self . clear = options [ 'clear' ] self . dry_run = options [ 'dry_run' ] ignore_patterns = options [ 'ignore_patterns' ] if options [ 'use_default_ignore_patterns' ] : ignore_patterns += [ 'CVS' , '.*' , '*~' ] self . ignore_patterns = list ( set ( ignore_patterns ) ) self . post_process = options [ 'post_process' ]
13007	def _from_parts ( cls , args , init = True ) : if args : args = list ( args ) if isinstance ( args [ 0 ] , WindowsPath2 ) : args [ 0 ] = args [ 0 ] . path elif args [ 0 ] . startswith ( "\\\\?\\" ) : args [ 0 ] = args [ 0 ] [ 4 : ] args = tuple ( args ) return super ( WindowsPath2 , cls ) . _from_parts ( args , init )
11217	def _pop_claims_from_payload ( self ) : claims_in_payload = [ k for k in self . payload . keys ( ) if k in registered_claims . values ( ) ] for name in claims_in_payload : self . registered_claims [ name ] = self . payload . pop ( name )
8597	def delete_group ( self , group_id ) : response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'DELETE' ) return response
12149	def analyzeAll ( self ) : searchableData = str ( self . files2 ) self . log . debug ( "considering analysis for %d ABFs" , len ( self . IDs ) ) for ID in self . IDs : if not ID + "_" in searchableData : self . log . debug ( "%s needs analysis" , ID ) try : self . analyzeABF ( ID ) except : print ( "EXCEPTION! " * 100 ) else : self . log . debug ( "%s has existing analysis, not overwriting" , ID ) self . log . debug ( "verified analysis of %d ABFs" , len ( self . IDs ) )
459	def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result
1928	def load_overrides ( path = None ) : if path is not None : names = [ path ] else : possible_names = [ 'mcore.yml' , 'manticore.yml' ] names = [ os . path . join ( '.' , '' . join ( x ) ) for x in product ( [ '' , '.' ] , possible_names ) ] for name in names : try : with open ( name , 'r' ) as yml_f : logger . info ( f'Reading configuration from {name}' ) parse_config ( yml_f ) break except FileNotFoundError : pass else : if path is not None : raise FileNotFoundError ( f"'{path}' not found for config overrides" )
5853	def get_pif ( self , dataset_id , uid , dataset_version = None ) : failure_message = "An error occurred retrieving PIF {}" . format ( uid ) if dataset_version == None : response = self . _get ( routes . pif_dataset_uid ( dataset_id , uid ) , failure_message = failure_message ) else : response = self . _get ( routes . pif_dataset_version_uid ( dataset_id , uid , dataset_version ) , failure_message = failure_message ) return pif . loads ( response . content . decode ( "utf-8" ) )
559	def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True
5183	def node ( self , name ) : nodes = self . nodes ( path = name ) return next ( node for node in nodes )
12699	def _parse_data_fields ( self , fields , tag_id = "tag" , sub_id = "code" ) : for field in fields : params = field . params if tag_id not in params : continue field_repr = OrderedDict ( [ [ self . i1_name , params . get ( self . i1_name , " " ) ] , [ self . i2_name , params . get ( self . i2_name , " " ) ] , ] ) for subfield in field . find ( "subfield" ) : if sub_id not in subfield . params : continue content = MARCSubrecord ( val = subfield . getContent ( ) . strip ( ) , i1 = field_repr [ self . i1_name ] , i2 = field_repr [ self . i2_name ] , other_subfields = field_repr ) code = subfield . params [ sub_id ] if code in field_repr : field_repr [ code ] . append ( content ) else : field_repr [ code ] = [ content ] tag = params [ tag_id ] if tag in self . datafields : self . datafields [ tag ] . append ( field_repr ) else : self . datafields [ tag ] = [ field_repr ]
8727	def daily_at ( cls , at , target ) : daily = datetime . timedelta ( days = 1 ) when = datetime . datetime . combine ( datetime . date . today ( ) , at ) if when < now ( ) : when += daily return cls . at_time ( cls . _localize ( when ) , daily , target )
3753	def Ceiling ( CASRN , AvailableMethods = False , Method = None ) : def list_methods ( ) : methods = [ ] if CASRN in _OntarioExposureLimits and ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] or _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] ) : methods . append ( ONTARIO ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == ONTARIO : if _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (ppm)" ] , 'ppm' ) elif _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] : _Ceiling = ( _OntarioExposureLimits [ CASRN ] [ "Ceiling (mg/m^3)" ] , 'mg/m^3' ) elif Method == NONE : _Ceiling = None else : raise Exception ( 'Failure in in function' ) return _Ceiling
4236	def login ( self ) : if not self . force_login_v2 : v1_result = self . login_v1 ( ) if v1_result : return v1_result return self . login_v2 ( )
12439	def serialize ( self , data , response = None , request = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Serializer = None if format : Serializer = self . meta . serializers [ format ] if not Serializer : media_ranges = ( request . get ( 'Accept' ) or '*/*' ) . strip ( ) if not media_ranges : media_ranges = '*/*' if media_ranges != '*/*' : media_types = six . iterkeys ( self . _serializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _serializer_map [ media_type ] Serializer = self . meta . serializers [ format ] else : default = self . meta . default_serializer Serializer = self . meta . serializers [ default ] if Serializer : try : serializer = Serializer ( request , response ) return serializer . serialize ( data ) , serializer except ValueError : pass available = { } for name in self . meta . allowed_serializers : Serializer = self . meta . serializers [ name ] instance = Serializer ( request , None ) if instance . can_serialize ( data ) : available [ name ] = Serializer . media_types [ 0 ] raise http . exceptions . NotAcceptable ( available )
12559	def largest_connected_component ( volume ) : volume = np . asarray ( volume ) labels , num_labels = scn . label ( volume ) if not num_labels : raise ValueError ( 'No non-zero values: no connected components found.' ) if num_labels == 1 : return volume . astype ( np . bool ) label_count = np . bincount ( labels . ravel ( ) . astype ( np . int ) ) label_count [ 0 ] = 0 return labels == label_count . argmax ( )
3824	async def get_entity_by_id ( self , get_entity_by_id_request ) : response = hangouts_pb2 . GetEntityByIdResponse ( ) await self . _pb_request ( 'contacts/getentitybyid' , get_entity_by_id_request , response ) return response
13112	def get_configured_dns ( ) : ips = [ ] try : output = subprocess . check_output ( [ 'nmcli' , 'device' , 'show' ] ) output = output . decode ( 'utf-8' ) for line in output . split ( '\n' ) : if 'DNS' in line : pattern = r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}" for hit in re . findall ( pattern , line ) : ips . append ( hit ) except FileNotFoundError : pass return ips
241	def create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , trade_daily_vol_limit = 0.05 , last_n_days = utils . APPROX_BDAYS_PER_MONTH * 6 , days_to_liquidate_limit = 1 , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) print ( "Max days to liquidation is computed for each traded name " "assuming a 20% limit on daily bar consumption \n" "and trailing 5 day mean volume as the available bar volume.\n\n" "Tickers with >1 day liquidation time at a" " constant $1m capital base:" ) max_days_by_ticker = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 ) max_days_by_ticker . index = ( max_days_by_ticker . index . map ( utils . format_asset ) ) print ( "Whole backtest:" ) utils . print_table ( max_days_by_ticker [ max_days_by_ticker . days_to_liquidate > days_to_liquidate_limit ] ) max_days_by_ticker_lnd = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 , last_n_days = last_n_days ) max_days_by_ticker_lnd . index = ( max_days_by_ticker_lnd . index . map ( utils . format_asset ) ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( max_days_by_ticker_lnd [ max_days_by_ticker_lnd . days_to_liquidate > 1 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data ) llt . index = llt . index . map ( utils . format_asset ) print ( 'Tickers with daily transactions consuming >{}% of daily bar \n' 'all backtest:' . format ( trade_daily_vol_limit * 100 ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data , last_n_days = last_n_days ) print ( "Last {} trading days:" . format ( last_n_days ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) bt_starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns . iloc [ 0 ] ) fig , ax_capacity_sweep = plt . subplots ( figsize = ( 14 , 6 ) ) plotting . plot_capacity_sweep ( returns , transactions , market_data , bt_starting_capital , min_pv = 100000 , max_pv = 300000000 , step_size = 1000000 , ax = ax_capacity_sweep )
9110	def _create_archive ( self ) : self . status = u'270 creating final encrypted backup of cleansed attachments' return self . _create_encrypted_zip ( source = 'clean' , fs_target_dir = self . container . fs_archive_cleansed )
13313	def _activate ( self ) : old_syspath = set ( sys . path ) site . addsitedir ( self . site_path ) site . addsitedir ( self . bin_path ) new_syspaths = set ( sys . path ) - old_syspath for path in new_syspaths : sys . path . remove ( path ) sys . path . insert ( 1 , path ) if not hasattr ( sys , 'real_prefix' ) : sys . real_prefix = sys . prefix sys . prefix = self . path
1398	def extract_scheduler_location ( self , topology ) : schedulerLocation = { "name" : None , "http_endpoint" : None , "job_page_link" : None , } if topology . scheduler_location : schedulerLocation [ "name" ] = topology . scheduler_location . topology_name schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" return schedulerLocation
11678	def run ( self ) : logger . info ( u'Started listening' ) while not self . _stop : xml = self . _readxml ( ) if xml is None : break if not self . modelize : logger . info ( u'Raw xml: %s' % xml ) self . results . put ( xml ) continue if xml . tag == 'RECOGOUT' : sentence = Sentence . from_shypo ( xml . find ( 'SHYPO' ) , self . encoding ) logger . info ( u'Modelized recognition: %r' % sentence ) self . results . put ( sentence ) else : logger . info ( u'Unmodelized xml: %s' % xml ) self . results . put ( xml ) logger . info ( u'Stopped listening' )
5743	def update_running_pids ( old_procs ) : new_procs = [ ] for proc in old_procs : if proc . poll ( ) is None and check_pid ( proc . pid ) : publisher . debug ( str ( proc . pid ) + ' is alive' ) new_procs . append ( proc ) else : try : publisher . debug ( str ( proc . pid ) + ' is gone' ) os . kill ( proc . pid , signal . SIGKILL ) except : pass return new_procs
2834	def platform_detect ( ) : pi = pi_version ( ) if pi is not None : return RASPBERRY_PI plat = platform . platform ( ) if plat . lower ( ) . find ( 'armv7l-with-debian' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-ubuntu' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'armv7l-with-glibc2.4' ) > - 1 : return BEAGLEBONE_BLACK elif plat . lower ( ) . find ( 'tegra-aarch64-with-ubuntu' ) > - 1 : return JETSON_NANO try : import mraa if mraa . getPlatformName ( ) == 'MinnowBoard MAX' : return MINNOWBOARD except ImportError : pass return UNKNOWN
6053	def unmasked_sparse_to_sparse_from_mask_and_pixel_centres ( mask , unmasked_sparse_grid_pixel_centres , total_sparse_pixels ) : total_unmasked_sparse_pixels = unmasked_sparse_grid_pixel_centres . shape [ 0 ] unmasked_sparse_to_sparse = np . zeros ( total_unmasked_sparse_pixels ) pixel_index = 0 for unmasked_sparse_pixel_index in range ( total_unmasked_sparse_pixels ) : y = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 0 ] x = unmasked_sparse_grid_pixel_centres [ unmasked_sparse_pixel_index , 1 ] unmasked_sparse_to_sparse [ unmasked_sparse_pixel_index ] = pixel_index if not mask [ y , x ] : if pixel_index < total_sparse_pixels - 1 : pixel_index += 1 return unmasked_sparse_to_sparse
11908	def to_bipartite_matrix ( A ) : m , n = A . shape return four_blocks ( zeros ( m , m ) , A , A . T , zeros ( n , n ) )
3499	def assess ( model , reaction , flux_coefficient_cutoff = 0.001 , solver = None ) : reaction = model . reactions . get_by_any ( reaction ) [ 0 ] with model as m : m . objective = reaction if _optimize_or_value ( m , solver = solver ) >= flux_coefficient_cutoff : return True else : results = dict ( ) results [ 'precursors' ] = assess_component ( model , reaction , 'reactants' , flux_coefficient_cutoff ) results [ 'products' ] = assess_component ( model , reaction , 'products' , flux_coefficient_cutoff ) return results
5748	def history ( self , ip , days_limit = None ) : all_dates = sorted ( self . routing_db . smembers ( 'imported_dates' ) , reverse = True ) if days_limit is not None : all_dates = all_dates [ : days_limit ] return [ self . date_asn_block ( ip , date ) for date in all_dates ]
8070	def replace_entities ( ustring , placeholder = " " ) : def _repl_func ( match ) : try : if match . group ( 1 ) : return unichr ( int ( match . group ( 2 ) ) ) else : try : return cp1252 [ unichr ( int ( match . group ( 3 ) ) ) ] . strip ( ) except : return unichr ( name2codepoint [ match . group ( 3 ) ] ) except : return placeholder if not isinstance ( ustring , unicode ) : ustring = UnicodeDammit ( ustring ) . unicode ustring = ustring . replace ( "&nbsp;" , " " ) _entity_re = re . compile ( r'&(?:(#)(\d+)|([^;^> ]+));' ) return _entity_re . sub ( _repl_func , ustring )
2929	def write_manifest ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'Manifest' ) for f in sorted ( self . manifest . keys ( ) ) : config . set ( 'Manifest' , f . replace ( '\\' , '/' ) . lower ( ) , self . manifest [ f ] ) ini = StringIO ( ) config . write ( ini ) self . manifest_data = ini . getvalue ( ) self . package_zip . writestr ( self . MANIFEST_FILE , self . manifest_data )
1982	def sync ( f ) : def new_function ( self , * args , ** kw ) : self . _lock . acquire ( ) try : return f ( self , * args , ** kw ) finally : self . _lock . release ( ) return new_function
47	def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )
12505	def signed_session ( self , session = None ) : if session : session = super ( ClientCertAuthentication , self ) . signed_session ( session ) else : session = super ( ClientCertAuthentication , self ) . signed_session ( ) if self . cert is not None : session . cert = self . cert if self . ca_cert is not None : session . verify = self . ca_cert if self . no_verify : session . verify = False return session
12822	def _path_root ( draw , result_type ) : def tp ( s = '' ) : return _str_to_path ( s , result_type ) if os . name != 'nt' : return tp ( os . sep ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) name = _filename ( result_type ) char = characters ( min_codepoint = ord ( "A" ) , max_codepoint = ord ( "z" ) ) . map ( lambda c : tp ( str ( c ) ) ) relative = sep drive = builds ( lambda * x : tp ( ) . join ( x ) , char , just ( tp ( ':' ) ) , sep ) extended = builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , drive ) network = one_of ( [ builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '?' ) ) , sep , just ( tp ( 'UNC' ) ) , sep , name , sep , name , sep ) , builds ( lambda * x : tp ( ) . join ( x ) , sep , sep , just ( tp ( '.' ) ) , sep , name , sep ) , ] ) final = one_of ( relative , drive , extended , network ) return draw ( final )
6855	def ismounted ( device ) : with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'mount' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True with settings ( hide ( 'running' , 'stdout' ) ) : res = run_as_root ( 'swapon -s' ) for line in res . splitlines ( ) : fields = line . split ( ) if fields [ 0 ] == device : return True return False
7725	def __from_xmlnode ( self , xmlnode ) : actor = None reason = None n = xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != MUC_USER_NS : continue if n . name == "actor" : actor = n . getContent ( ) if n . name == "reason" : reason = n . getContent ( ) n = n . next self . __init ( from_utf8 ( xmlnode . prop ( "affiliation" ) ) , from_utf8 ( xmlnode . prop ( "role" ) ) , from_utf8 ( xmlnode . prop ( "jid" ) ) , from_utf8 ( xmlnode . prop ( "nick" ) ) , from_utf8 ( actor ) , from_utf8 ( reason ) , )
12834	def on_update_stage ( self , dt ) : for actor in self . actors : actor . on_update_game ( dt ) self . forum . on_update_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_update_game ( dt ) if self . world . has_game_ended ( ) : self . exit_stage ( )
6899	def parallel_periodicfeatures ( pfpkl_list , lcbasedir , outdir , starfeaturesdir = None , fourierorder = 5 , transitparams = ( - 0.01 , 0.1 , 0.1 ) , ebparams = ( - 0.2 , 0.3 , 0.7 , 0.5 ) , pdiff_threshold = 1.0e-4 , sidereal_threshold = 1.0e-4 , sampling_peak_multiplier = 5.0 , sampling_startp = None , sampling_endp = None , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , sigclip = 10.0 , verbose = False , maxobjects = None , nworkers = NCPUS ) : if not os . path . exists ( outdir ) : os . makedirs ( outdir ) if maxobjects : pfpkl_list = pfpkl_list [ : maxobjects ] LOGINFO ( '%s periodfinding pickles to process' % len ( pfpkl_list ) ) if starfeaturesdir and os . path . exists ( starfeaturesdir ) : starfeatures_list = [ ] LOGINFO ( 'collecting starfeatures pickles...' ) for pfpkl in pfpkl_list : sfpkl1 = os . path . basename ( pfpkl ) . replace ( 'periodfinding' , 'starfeatures' ) sfpkl2 = sfpkl1 . replace ( '.gz' , '' ) sfpath1 = os . path . join ( starfeaturesdir , sfpkl1 ) sfpath2 = os . path . join ( starfeaturesdir , sfpkl2 ) if os . path . exists ( sfpath1 ) : starfeatures_list . append ( sfpkl1 ) elif os . path . exists ( sfpath2 ) : starfeatures_list . append ( sfpkl2 ) else : starfeatures_list . append ( None ) else : starfeatures_list = [ None for x in pfpkl_list ] kwargs = { 'fourierorder' : fourierorder , 'transitparams' : transitparams , 'ebparams' : ebparams , 'pdiff_threshold' : pdiff_threshold , 'sidereal_threshold' : sidereal_threshold , 'sampling_peak_multiplier' : sampling_peak_multiplier , 'sampling_startp' : sampling_startp , 'sampling_endp' : sampling_endp , 'timecols' : timecols , 'magcols' : magcols , 'errcols' : errcols , 'lcformat' : lcformat , 'lcformatdir' : lcformat , 'sigclip' : sigclip , 'verbose' : verbose } tasks = [ ( x , lcbasedir , outdir , y , kwargs ) for ( x , y ) in zip ( pfpkl_list , starfeatures_list ) ] LOGINFO ( 'processing periodfinding pickles...' ) with ProcessPoolExecutor ( max_workers = nworkers ) as executor : resultfutures = executor . map ( _periodicfeatures_worker , tasks ) results = [ x for x in resultfutures ] resdict = { os . path . basename ( x ) : y for ( x , y ) in zip ( pfpkl_list , results ) } return resdict
10115	def parse ( grid_str , mode = MODE_ZINC , charset = 'utf-8' ) : if isinstance ( grid_str , six . binary_type ) : grid_str = grid_str . decode ( encoding = charset ) _parse = functools . partial ( parse_grid , mode = mode , charset = charset ) if mode == MODE_JSON : if isinstance ( grid_str , six . string_types ) : grid_data = json . loads ( grid_str ) else : grid_data = grid_str if isinstance ( grid_data , dict ) : return _parse ( grid_data ) else : return list ( map ( _parse , grid_data ) ) else : return list ( map ( _parse , GRID_SEP . split ( grid_str . rstrip ( ) ) ) )
268	def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) rows = '' for name , value in header_rows . items ( ) : rows += ( '\n <tr style="text-align: right;"><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )
1983	def save_value ( self , key , value ) : with self . save_stream ( key ) as s : s . write ( value )
5417	def _format_task_uri ( fmt , job_metadata , task_metadata ) : values = { 'job-id' : None , 'task-id' : 'task' , 'job-name' : None , 'user-id' : None , 'task-attempt' : None } for key in values : values [ key ] = task_metadata . get ( key ) or job_metadata . get ( key ) or values [ key ] return fmt . format ( ** values )
2410	def create_essay_set_and_dump_model ( text , score , prompt , model_path , additional_array = None ) : essay_set = create_essay_set ( text , score , prompt ) feature_ext , clf = extract_features_and_generate_model ( essay_set , additional_array ) dump_model_to_file ( prompt , feature_ext , clf , model_path )
10484	def _matchOther ( self , obj , ** kwargs ) : if obj is not None : if self . _findFirstR ( ** kwargs ) : return obj . _match ( ** kwargs ) return False
7286	def has_delete_permission ( self , request ) : return request . user . is_authenticated and request . user . is_active and request . user . is_superuser
12405	def reverse ( self ) : if self . _original_target_content : with open ( self . target , 'w' ) as fp : fp . write ( self . _original_target_content )
4053	def all_collections ( self , collid = None ) : all_collections = [ ] def subcoll ( clct ) : all_collections . append ( clct ) if clct [ "meta" ] . get ( "numCollections" , 0 ) > 0 : [ subcoll ( c ) for c in self . everything ( self . collections_sub ( clct [ "data" ] [ "key" ] ) ) ] if collid : toplevel = [ self . collection ( collid ) ] else : toplevel = self . everything ( self . collections_top ( ) ) [ subcoll ( collection ) for collection in toplevel ] return all_collections
13017	def addHook ( self , name , callable ) : if name not in self . _hooks : self . _hooks [ name ] = [ ] self . _hooks [ name ] . append ( callable )
3473	def build_reaction_string ( self , use_metabolite_names = False ) : def format ( number ) : return "" if number == 1 else str ( number ) . rstrip ( "." ) + " " id_type = 'id' if use_metabolite_names : id_type = 'name' reactant_bits = [ ] product_bits = [ ] for met in sorted ( self . _metabolites , key = attrgetter ( "id" ) ) : coefficient = self . _metabolites [ met ] name = str ( getattr ( met , id_type ) ) if coefficient >= 0 : product_bits . append ( format ( coefficient ) + name ) else : reactant_bits . append ( format ( abs ( coefficient ) ) + name ) reaction_string = ' + ' . join ( reactant_bits ) if not self . reversibility : if self . lower_bound < 0 and self . upper_bound <= 0 : reaction_string += ' <-- ' else : reaction_string += ' else : reaction_string += ' <=> ' reaction_string += ' + ' . join ( product_bits ) return reaction_string
11365	def create_logger ( name , filename = None , logging_level = logging . DEBUG ) : logger = logging . getLogger ( name ) formatter = logging . Formatter ( ( '%(asctime)s - %(name)s - ' '%(levelname)-8s - %(message)s' ) ) if filename : fh = logging . FileHandler ( filename = filename ) fh . setFormatter ( formatter ) logger . addHandler ( fh ) ch = logging . StreamHandler ( ) ch . setFormatter ( formatter ) logger . addHandler ( ch ) logger . setLevel ( logging_level ) return logger
5740	def main ( path , pid , queue ) : setup_logging ( ) if pid : with open ( os . path . expanduser ( pid ) , "w" ) as f : f . write ( str ( os . getpid ( ) ) ) if not path : path = os . getcwd ( ) sys . path . insert ( 0 , path ) queue = import_queue ( queue ) import psq worker = psq . Worker ( queue = queue ) worker . listen ( )
9918	def validate_key ( self , key ) : try : confirmation = models . EmailConfirmation . objects . select_related ( "email__user" ) . get ( key = key ) except models . EmailConfirmation . DoesNotExist : raise serializers . ValidationError ( _ ( "The provided verification key is invalid." ) ) if confirmation . is_expired : raise serializers . ValidationError ( _ ( "That verification code has expired." ) ) self . _confirmation = confirmation return key
13110	def lookup ( cls , key , get = False ) : if get : item = cls . _item_dict . get ( key ) return item . name if item else key return cls . _item_dict [ key ] . name
8217	def trigger_fullscreen_action ( self , fullscreen ) : action = self . action_group . get_action ( 'fullscreen' ) action . set_active ( fullscreen )
4789	def is_digit ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if not self . val . isdigit ( ) : self . _err ( 'Expected <%s> to contain only digits, but did not.' % self . val ) return self
3497	def total_components_flux ( flux , components , consumption = True ) : direction = 1 if consumption else - 1 c_flux = [ elem * flux * direction for elem in components ] return sum ( [ flux for flux in c_flux if flux > 0 ] )
11840	def score ( self ) : "The total score for the words found, according to the rules." return sum ( [ self . scores [ len ( w ) ] for w in self . words ( ) ] )
2883	def to_html_string ( self ) : html = ET . Element ( 'html' ) head = ET . SubElement ( html , 'head' ) title = ET . SubElement ( head , 'title' ) title . text = self . description body = ET . SubElement ( html , 'body' ) h1 = ET . SubElement ( body , 'h1' ) h1 . text = self . description span = ET . SubElement ( body , 'span' ) span . text = ' CONTENT ' html_text = ET . tostring ( html ) svg_content = '' svg_done = set ( ) for spec in self . get_specs_depth_first ( ) : if spec . svg and spec . svg not in svg_done : svg_content += '<p>' + spec . svg + "</p>" svg_done . add ( spec . svg ) return html_text . replace ( ' CONTENT ' , svg_content )
9849	def _load_plt ( self , filename ) : g = gOpenMol . Plt ( ) g . read ( filename ) grid , edges = g . histogramdd ( ) self . __init__ ( grid = grid , edges = edges , metadata = self . metadata )
7504	def _parse_names ( self ) : self . samples = [ ] with iter ( open ( self . files . data , 'r' ) ) as infile : infile . next ( ) . strip ( ) . split ( ) while 1 : try : self . samples . append ( infile . next ( ) . split ( ) [ 0 ] ) except StopIteration : break
6620	def wait ( self ) : finished_pids = [ ] while self . running_procs : finished_pids . extend ( self . poll ( ) ) return finished_pids
10507	def server_bind ( self , * args , ** kwargs ) : self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) SimpleXMLRPCServer . server_bind ( self , * args , ** kwargs )
4108	def chirp ( t , f0 = 0. , t1 = 1. , f1 = 100. , form = 'linear' , phase = 0 ) : r valid_forms = [ 'linear' , 'quadratic' , 'logarithmic' ] if form not in valid_forms : raise ValueError ( "Invalid form. Valid form are %s" % valid_forms ) t = numpy . array ( t ) phase = 2. * pi * phase / 360. if form == "linear" : a = pi * ( f1 - f0 ) / t1 b = 2. * pi * f0 y = numpy . cos ( a * t ** 2 + b * t + phase ) elif form == "quadratic" : a = ( 2 / 3. * pi * ( f1 - f0 ) / t1 / t1 ) b = 2. * pi * f0 y = numpy . cos ( a * t ** 3 + b * t + phase ) elif form == "logarithmic" : a = 2. * pi * t1 / numpy . log ( f1 - f0 ) b = 2. * pi * f0 x = ( f1 - f0 ) ** ( 1. / t1 ) y = numpy . cos ( a * x ** t + b * t + phase ) return y
8127	def search_images ( q , start = 1 , count = 10 , wait = 10 , asynchronous = False , cached = False ) : service = YAHOO_IMAGES return YahooSearch ( q , start , count , service , None , wait , asynchronous , cached )
6786	def get_component_funcs ( self , components = None ) : current_tp = self . get_current_thumbprint ( components = components ) or { } previous_tp = self . get_previous_thumbprint ( components = components ) or { } if self . verbose : print ( 'Current thumbprint:' ) pprint ( current_tp , indent = 4 ) print ( 'Previous thumbprint:' ) pprint ( previous_tp , indent = 4 ) differences = list ( iter_dict_differences ( current_tp , previous_tp ) ) if self . verbose : print ( 'Differences:' ) pprint ( differences , indent = 4 ) component_order = get_component_order ( [ k for k , ( _ , _ ) in differences ] ) if self . verbose : print ( 'component_order:' ) pprint ( component_order , indent = 4 ) plan_funcs = list ( get_deploy_funcs ( component_order , current_tp , previous_tp ) ) return component_order , plan_funcs
2841	def write_iodir ( self , iodir = None ) : if iodir is not None : self . iodir = iodir self . _device . writeList ( self . IODIR , self . iodir )
6706	def expire_password ( self , username ) : r = self . local_renderer r . env . username = username r . sudo ( 'chage -d 0 {username}' )
1994	def save_state ( self , state , state_id = None ) : assert isinstance ( state , StateBase ) if state_id is None : state_id = self . _get_id ( ) else : self . rm_state ( state_id ) self . _store . save_state ( state , f'{self._prefix}{state_id:08x}{self._suffix}' ) return state_id
5795	def cf_dictionary_to_dict ( dictionary ) : dict_length = CoreFoundation . CFDictionaryGetCount ( dictionary ) keys = ( CFTypeRef * dict_length ) ( ) values = ( CFTypeRef * dict_length ) ( ) CoreFoundation . CFDictionaryGetKeysAndValues ( dictionary , _cast_pointer_p ( keys ) , _cast_pointer_p ( values ) ) output = { } for index in range ( 0 , dict_length ) : output [ CFHelpers . native ( keys [ index ] ) ] = CFHelpers . native ( values [ index ] ) return output
9099	def write_bel_annotation ( self , file : TextIO ) -> None : if not self . is_populated ( ) : self . populate ( ) values = self . _get_namespace_name_to_encoding ( desc = 'writing names' ) write_annotation ( keyword = self . _get_namespace_keyword ( ) , citation_name = self . _get_namespace_name ( ) , description = '' , values = values , file = file , )
13312	def _pre_activate ( self ) : if 'CPENV_CLEAN_ENV' not in os . environ : if platform == 'win' : os . environ [ 'PROMPT' ] = '$P$G' else : os . environ [ 'PS1' ] = '\\u@\\h:\\w\\$' clean_env_path = utils . get_store_env_tmp ( ) os . environ [ 'CPENV_CLEAN_ENV' ] = clean_env_path utils . store_env ( path = clean_env_path ) else : utils . restore_env_from_file ( os . environ [ 'CPENV_CLEAN_ENV' ] )
11626	def build ( self , pre = None , shortest = False ) : res = super ( Q , self ) . build ( pre , shortest = shortest ) if self . escape : return repr ( res ) elif self . html_js_escape : return ( "'" + res . encode ( "string_escape" ) . replace ( "<" , "\\x3c" ) . replace ( ">" , "\\x3e" ) + "'" ) else : return "" . join ( [ self . quote , res , self . quote ] )
11324	def fix_name_capitalization ( lastname , givennames ) : lastnames = lastname . split ( ) if len ( lastnames ) == 1 : if '-' in lastname : names = lastname . split ( '-' ) names = map ( lambda a : a [ 0 ] + a [ 1 : ] . lower ( ) , names ) lastname = '-' . join ( names ) else : lastname = lastname [ 0 ] + lastname [ 1 : ] . lower ( ) else : names = [ ] for name in lastnames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) lastname = ' ' . join ( names ) lastname = collapse_initials ( lastname ) names = [ ] for name in givennames : if re . search ( r'[A-Z]\.' , name ) : names . append ( name ) else : names . append ( name [ 0 ] + name [ 1 : ] . lower ( ) ) givennames = ' ' . join ( names ) return lastname , givennames
6080	def convergence_of_galaxies_from_grid ( grid , galaxies ) : if galaxies : return sum ( map ( lambda g : g . convergence_from_grid ( grid ) , galaxies ) ) else : return np . full ( ( grid . shape [ 0 ] ) , 0.0 )
13899	def PushPopItem ( obj , key , value ) : if key in obj : old_value = obj [ key ] obj [ key ] = value yield value obj [ key ] = old_value else : obj [ key ] = value yield value del obj [ key ]
13077	def main_collections ( self , lang = None ) : return sorted ( [ { "id" : member . id , "label" : str ( member . get_label ( lang = lang ) ) , "model" : str ( member . model ) , "type" : str ( member . type ) , "size" : member . size } for member in self . resolver . getMetadata ( ) . members ] , key = itemgetter ( "label" ) )
1586	def _handle_state_change_msg ( self , new_helper ) : assert self . my_pplan_helper is not None assert self . my_instance is not None and self . my_instance . py_class is not None if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : self . my_pplan_helper = new_helper if new_helper . is_topology_running ( ) : if not self . is_instance_started : self . start_instance_if_possible ( ) self . my_instance . py_class . invoke_activate ( ) elif new_helper . is_topology_paused ( ) : self . my_instance . py_class . invoke_deactivate ( ) else : raise RuntimeError ( "Unexpected TopologyState update: %s" % new_helper . get_topology_state ( ) ) else : Log . info ( "Topology state remains the same." )
13208	def _parse_abstract ( self ) : command = LatexCommand ( 'setDocAbstract' , { 'name' : 'abstract' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return try : content = parsed [ 'abstract' ] except KeyError : self . _logger . warning ( 'lsstdoc has no abstract' ) self . _abstract = None return content = content . strip ( ) self . _abstract = content
10770	def shapely_formatter ( _ , vertices , codes = None ) : elements = [ ] if codes is None : for vertices_ in vertices : if np . all ( vertices_ [ 0 , : ] == vertices_ [ - 1 , : ] ) : if len ( vertices ) < 3 : elements . append ( Point ( vertices_ [ 0 , : ] ) ) else : elements . append ( LinearRing ( vertices_ ) ) else : elements . append ( LineString ( vertices_ ) ) else : for vertices_ , codes_ in zip ( vertices , codes ) : starts = np . nonzero ( codes_ == MPLPATHCODE . MOVETO ) [ 0 ] stops = np . nonzero ( codes_ == MPLPATHCODE . CLOSEPOLY ) [ 0 ] try : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) except ValueError as err : if np . any ( stop - start - 1 == 0 ) : if stops [ 0 ] < starts [ 0 ] + 2 : pass else : rings = [ LinearRing ( vertices_ [ start : stop + 1 , : ] ) for start , stop in zip ( starts , stops ) if stop >= start + 2 ] elements . append ( Polygon ( rings [ 0 ] , rings [ 1 : ] ) ) else : raise ( err ) return elements
12377	def make_response ( self , data = None ) : if data is not None : data = self . prepare ( data ) self . response . write ( data , serialize = True )
7731	def make_join_request ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : self . clear_muc_child ( ) self . muc_child = MucX ( parent = self . xmlnode ) if ( history_maxchars is not None or history_maxstanzas is not None or history_seconds is not None or history_since is not None ) : history = HistoryParameters ( history_maxchars , history_maxstanzas , history_seconds , history_since ) self . muc_child . set_history ( history ) if password is not None : self . muc_child . set_password ( password )
9979	def extract_params ( source ) : funcdef = find_funcdef ( source ) params = [ ] for node in ast . walk ( funcdef . args ) : if isinstance ( node , ast . arg ) : if node . arg not in params : params . append ( node . arg ) return params
8870	def create_metafile ( bgen_filepath , metafile_filepath , verbose = True ) : r if verbose : verbose = 1 else : verbose = 0 bgen_filepath = make_sure_bytes ( bgen_filepath ) metafile_filepath = make_sure_bytes ( metafile_filepath ) assert_file_exist ( bgen_filepath ) assert_file_readable ( bgen_filepath ) if exists ( metafile_filepath ) : raise ValueError ( f"The file {metafile_filepath} already exists." ) with bgen_file ( bgen_filepath ) as bgen : nparts = _estimate_best_npartitions ( lib . bgen_nvariants ( bgen ) ) metafile = lib . bgen_create_metafile ( bgen , metafile_filepath , nparts , verbose ) if metafile == ffi . NULL : raise RuntimeError ( f"Error while creating metafile: {metafile_filepath}." ) if lib . bgen_close_metafile ( metafile ) != 0 : raise RuntimeError ( f"Error while closing metafile: {metafile_filepath}." )
4916	def entitlements ( self , request , pk = None ) : enterprise_customer_user = self . get_object ( ) instance = { "entitlements" : enterprise_customer_user . entitlements } serializer = serializers . EnterpriseCustomerUserEntitlementSerializer ( instance , context = { 'request' : request } ) return Response ( serializer . data )
1542	def queries_map ( ) : qs = _all_metric_queries ( ) return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) )
13790	def MessageSetItemDecoder ( extensions_by_number ) : type_id_tag_bytes = encoder . TagBytes ( 2 , wire_format . WIRETYPE_VARINT ) message_tag_bytes = encoder . TagBytes ( 3 , wire_format . WIRETYPE_LENGTH_DELIMITED ) item_end_tag_bytes = encoder . TagBytes ( 1 , wire_format . WIRETYPE_END_GROUP ) local_ReadTag = ReadTag local_DecodeVarint = _DecodeVarint local_SkipField = SkipField def DecodeItem ( buffer , pos , end , message , field_dict ) : message_set_item_start = pos type_id = - 1 message_start = - 1 message_end = - 1 while 1 : ( tag_bytes , pos ) = local_ReadTag ( buffer , pos ) if tag_bytes == type_id_tag_bytes : ( type_id , pos ) = local_DecodeVarint ( buffer , pos ) elif tag_bytes == message_tag_bytes : ( size , message_start ) = local_DecodeVarint ( buffer , pos ) pos = message_end = message_start + size elif tag_bytes == item_end_tag_bytes : break else : pos = SkipField ( buffer , pos , end , tag_bytes ) if pos == - 1 : raise _DecodeError ( 'Missing group end tag.' ) if pos > end : raise _DecodeError ( 'Truncated message.' ) if type_id == - 1 : raise _DecodeError ( 'MessageSet item missing type_id.' ) if message_start == - 1 : raise _DecodeError ( 'MessageSet item missing message.' ) extension = extensions_by_number . get ( type_id ) if extension is not None : value = field_dict . get ( extension ) if value is None : value = field_dict . setdefault ( extension , extension . message_type . _concrete_class ( ) ) if value . _InternalParse ( buffer , message_start , message_end ) != message_end : raise _DecodeError ( 'Unexpected end-group tag.' ) else : if not message . _unknown_fields : message . _unknown_fields = [ ] message . _unknown_fields . append ( ( MESSAGE_SET_ITEM_TAG , buffer [ message_set_item_start : pos ] ) ) return pos return DecodeItem
2126	def data_endpoint ( cls , in_data , ignore = [ ] ) : obj , obj_type , res , res_type = cls . obj_res ( in_data , fail_on = [ ] ) data = { } if 'obj' in ignore : obj = None if 'res' in ignore : res = None if obj and obj_type == 'user' : data [ 'members__in' ] = obj if obj and obj_type == 'team' : endpoint = '%s/%s/roles/' % ( grammar . pluralize ( obj_type ) , obj ) if res is not None : data [ 'object_id' ] = res elif res : endpoint = '%s/%s/object_roles/' % ( grammar . pluralize ( res_type ) , res ) else : endpoint = '/roles/' if in_data . get ( 'type' , False ) : data [ 'role_field' ] = '%s_role' % in_data [ 'type' ] . lower ( ) for key , value in in_data . items ( ) : if key not in RESOURCE_FIELDS and key not in [ 'type' , 'user' , 'team' ] : data [ key ] = value return data , endpoint
13908	def create_subparsers ( self , parser ) : subparsers = parser . add_subparsers ( ) for name in self . config [ 'subparsers' ] : subparser = subparsers . add_parser ( name ) self . create_commands ( self . config [ 'subparsers' ] [ name ] , subparser )
6672	def is_dir ( self , path , use_sudo = False ) : if self . is_local and not use_sudo : return os . path . isdir ( path ) else : func = use_sudo and _sudo or _run with self . settings ( hide ( 'running' , 'warnings' ) , warn_only = True ) : return func ( '[ -d "%(path)s" ]' % locals ( ) ) . succeeded
11814	def score ( self , plaintext ) : "Return a score for text based on how common letters pairs are." s = 1.0 for bi in bigrams ( plaintext ) : s = s * self . P2 [ bi ] return s
6050	def run ( self , data , results = None , mask = None , positions = None ) : model_image = results . last . unmasked_model_image galaxy_tuples = results . last . constant . name_instance_tuples_for_class ( g . Galaxy ) results_copy = copy . copy ( results . last ) for name , galaxy in galaxy_tuples : optimizer = self . optimizer . copy_with_name_extension ( name ) optimizer . variable . hyper_galaxy = g . HyperGalaxy galaxy_image = results . last . unmasked_image_for_galaxy ( galaxy ) optimizer . fit ( self . __class__ . Analysis ( data , model_image , galaxy_image ) ) getattr ( results_copy . variable , name ) . hyper_galaxy = optimizer . variable . hyper_galaxy getattr ( results_copy . constant , name ) . hyper_galaxy = optimizer . constant . hyper_galaxy return results_copy
6489	def _process_field_queries ( field_dictionary ) : def field_item ( field ) : return { "match" : { field : field_dictionary [ field ] } } return [ field_item ( field ) for field in field_dictionary ]
8282	def _linelength ( self , x0 , y0 , x1 , y1 ) : a = pow ( abs ( x0 - x1 ) , 2 ) b = pow ( abs ( y0 - y1 ) , 2 ) return sqrt ( a + b )
3182	def create ( self , data ) : if 'id' not in data : raise KeyError ( 'The store must have an id' ) if 'list_id' not in data : raise KeyError ( 'The store must have a list_id' ) if 'name' not in data : raise KeyError ( 'The store must have a name' ) if 'currency_code' not in data : raise KeyError ( 'The store must have a currency_code' ) if not re . match ( r"^[A-Z]{3}$" , data [ 'currency_code' ] ) : raise ValueError ( 'The currency_code must be a valid 3-letter ISO 4217 currency code' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . store_id = response [ 'id' ] else : self . store_id = None return response
13614	def apply_orientation ( im ) : try : kOrientationEXIFTag = 0x0112 if hasattr ( im , '_getexif' ) : e = im . _getexif ( ) if e is not None : orientation = e [ kOrientationEXIFTag ] f = orientation_funcs [ orientation ] return f ( im ) except : pass return im
1219	def restore ( self , sess , save_path ) : if self . _saver is None : raise TensorForceError ( "register_saver_ops should be called before restore" ) self . _saver . restore ( sess = sess , save_path = save_path )
7114	def fit ( self , X , y ) : word_vector_transformer = WordVectorTransformer ( padding = 'max' ) X = word_vector_transformer . fit_transform ( X ) X = LongTensor ( X ) self . word_vector_transformer = word_vector_transformer y_transformer = LabelEncoder ( ) y = y_transformer . fit_transform ( y ) y = torch . from_numpy ( y ) self . y_transformer = y_transformer dataset = CategorizedDataset ( X , y ) dataloader = DataLoader ( dataset , batch_size = self . batch_size , shuffle = True , num_workers = 4 ) KERNEL_SIZES = self . kernel_sizes NUM_KERNEL = self . num_kernel EMBEDDING_DIM = self . embedding_dim model = TextCNN ( vocab_size = word_vector_transformer . get_vocab_size ( ) , embedding_dim = EMBEDDING_DIM , output_size = len ( self . y_transformer . classes_ ) , kernel_sizes = KERNEL_SIZES , num_kernel = NUM_KERNEL ) if USE_CUDA : model = model . cuda ( ) EPOCH = self . epoch LR = self . lr loss_function = nn . CrossEntropyLoss ( ) optimizer = optim . Adam ( model . parameters ( ) , lr = LR ) for epoch in range ( EPOCH ) : losses = [ ] for i , data in enumerate ( dataloader ) : X , y = data X , y = Variable ( X ) , Variable ( y ) optimizer . zero_grad ( ) model . train ( ) output = model ( X ) loss = loss_function ( output , y ) losses . append ( loss . data . tolist ( ) [ 0 ] ) loss . backward ( ) optimizer . step ( ) if i % 100 == 0 : print ( "[%d/%d] mean_loss : %0.2f" % ( epoch , EPOCH , np . mean ( losses ) ) ) losses = [ ] self . model = model
13828	def read ( readme ) : extend = os . path . splitext ( readme ) [ 1 ] if ( extend == '.rst' ) : import codecs return codecs . open ( readme , 'r' , 'utf-8' ) . read ( ) elif ( extend == '.md' ) : import pypandoc return pypandoc . convert ( readme , 'rst' )
9891	def _uptime_amiga ( ) : global __boottime try : __boottime = os . stat ( 'RAM:' ) . st_ctime return time . time ( ) - __boottime except ( NameError , OSError ) : return None
8327	def extract ( self ) : if self . parent : try : self . parent . contents . remove ( self ) except ValueError : pass lastChild = self . _lastRecursiveChild ( ) nextElement = lastChild . next if self . previous : self . previous . next = nextElement if nextElement : nextElement . previous = self . previous self . previous = None lastChild . next = None self . parent = None if self . previousSibling : self . previousSibling . nextSibling = self . nextSibling if self . nextSibling : self . nextSibling . previousSibling = self . previousSibling self . previousSibling = self . nextSibling = None return self
4802	def is_child_of ( self , parent ) : self . is_file ( ) if not isinstance ( parent , str_types ) : raise TypeError ( 'given parent directory arg must be a path' ) val_abspath = os . path . abspath ( self . val ) parent_abspath = os . path . abspath ( parent ) if not val_abspath . startswith ( parent_abspath ) : self . _err ( 'Expected file <%s> to be a child of <%s>, but was not.' % ( val_abspath , parent_abspath ) ) return self
13584	def _obj_display ( obj , display = '' ) : result = '' if not display : result = str ( obj ) else : template = Template ( display ) context = Context ( { 'obj' : obj } ) result = template . render ( context ) return result
6682	def remove ( self , path , recursive = False , use_sudo = False ) : func = use_sudo and run_as_root or self . run options = '-r ' if recursive else '' func ( '/bin/rm {0}{1}' . format ( options , quote ( path ) ) )
3359	def insert ( self , index , object ) : self . _check ( object . id ) list . insert ( self , index , object ) _dict = self . _dict for i , j in iteritems ( _dict ) : if j >= index : _dict [ i ] = j + 1 _dict [ object . id ] = index
7077	def parallel_periodicvar_recovery ( simbasedir , period_tolerance = 1.0e-3 , liststartind = None , listmaxobjects = None , nworkers = None ) : pfpkldir = os . path . join ( simbasedir , 'periodfinding' ) if not os . path . exists ( pfpkldir ) : LOGERROR ( 'no "periodfinding" subdirectory in %s, can\'t continue' % simbasedir ) return None pfpkl_list = glob . glob ( os . path . join ( pfpkldir , '*periodfinding*pkl*' ) ) if len ( pfpkl_list ) > 0 : if liststartind : pfpkl_list = pfpkl_list [ liststartind : ] if listmaxobjects : pfpkl_list = pfpkl_list [ : listmaxobjects ] tasks = [ ( x , simbasedir , period_tolerance ) for x in pfpkl_list ] pool = mp . Pool ( nworkers ) results = pool . map ( periodrec_worker , tasks ) pool . close ( ) pool . join ( ) resdict = { x [ 'objectid' ] : x for x in results if x is not None } actual_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and x [ 'actual_vartype' ] in PERIODIC_VARTYPES ) ] , dtype = np . unicode_ ) recovered_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'actual' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_twice_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'twice' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) alias_half_periodicvars = np . array ( [ x [ 'objectid' ] for x in results if ( x is not None and 'half' in x [ 'best_recovered_status' ] ) ] , dtype = np . unicode_ ) all_objectids = [ x [ 'objectid' ] for x in results ] outdict = { 'simbasedir' : os . path . abspath ( simbasedir ) , 'objectids' : all_objectids , 'period_tolerance' : period_tolerance , 'actual_periodicvars' : actual_periodicvars , 'recovered_periodicvars' : recovered_periodicvars , 'alias_twice_periodicvars' : alias_twice_periodicvars , 'alias_half_periodicvars' : alias_half_periodicvars , 'details' : resdict } outfile = os . path . join ( simbasedir , 'periodicvar-recovery.pkl' ) with open ( outfile , 'wb' ) as outfd : pickle . dump ( outdict , outfd , pickle . HIGHEST_PROTOCOL ) return outdict else : LOGERROR ( 'no periodfinding result pickles found in %s, can\'t continue' % pfpkldir ) return None
11121	def get_file_relative_path_by_name ( self , name , skip = 0 ) : if skip is None : paths = [ ] else : paths = None for path , info in self . walk_files_info ( ) : _ , n = os . path . split ( path ) if n == name : if skip is None : paths . append ( path ) elif skip > 0 : skip -= 1 else : paths = path break return paths
4716	def tcase_setup ( trun , parent , tcase_fname ) : case = copy . deepcopy ( TESTCASE ) case [ "fname" ] = tcase_fname case [ "fpath_orig" ] = os . sep . join ( [ trun [ "conf" ] [ "TESTCASES" ] , case [ "fname" ] ] ) if not os . path . exists ( case [ "fpath_orig" ] ) : cij . err ( 'rnr:tcase_setup: !case["fpath_orig"]: %r' % case [ "fpath_orig" ] ) return None case [ "name" ] = os . path . splitext ( case [ "fname" ] ) [ 0 ] case [ "ident" ] = "/" . join ( [ parent [ "ident" ] , case [ "fname" ] ] ) case [ "res_root" ] = os . sep . join ( [ parent [ "res_root" ] , case [ "fname" ] ] ) case [ "aux_root" ] = os . sep . join ( [ case [ "res_root" ] , "_aux" ] ) case [ "log_fpath" ] = os . sep . join ( [ case [ "res_root" ] , "run.log" ] ) case [ "fpath" ] = os . sep . join ( [ case [ "res_root" ] , case [ "fname" ] ] ) case [ "evars" ] . update ( copy . deepcopy ( parent [ "evars" ] ) ) os . makedirs ( case [ "res_root" ] ) os . makedirs ( case [ "aux_root" ] ) shutil . copyfile ( case [ "fpath_orig" ] , case [ "fpath" ] ) case [ "hooks" ] = hooks_setup ( trun , case , parent . get ( "hooks_pr_tcase" ) ) return case
138	def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )
13241	def daily_periods ( self , range_start = datetime . date . min , range_end = datetime . date . max , exclude_dates = tuple ( ) ) : tz = self . timezone period = self . period weekdays = self . weekdays current_date = max ( range_start , self . start_date ) end_date = range_end if self . end_date : end_date = min ( end_date , self . end_date ) while current_date <= end_date : if current_date . weekday ( ) in weekdays and current_date not in exclude_dates : yield Period ( tz . localize ( datetime . datetime . combine ( current_date , period . start ) ) , tz . localize ( datetime . datetime . combine ( current_date , period . end ) ) ) current_date += datetime . timedelta ( days = 1 )
10042	def admin_permission_factory ( ) : try : pkg_resources . get_distribution ( 'invenio-access' ) from invenio_access . permissions import DynamicPermission as Permission except pkg_resources . DistributionNotFound : from flask_principal import Permission return Permission ( action_admin_access )
6231	def apply_mesh_programs ( self , mesh_programs = None ) : if not mesh_programs : mesh_programs = [ ColorProgram ( ) , TextureProgram ( ) , FallbackProgram ( ) ] for mesh in self . meshes : for mp in mesh_programs : instance = mp . apply ( mesh ) if instance is not None : if isinstance ( instance , MeshProgram ) : mesh . mesh_program = mp break else : raise ValueError ( "apply() must return a MeshProgram instance, not {}" . format ( type ( instance ) ) ) if not mesh . mesh_program : print ( "WARING: No mesh program applied to '{}'" . format ( mesh . name ) )
1659	def CheckCasts ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = Search ( r'(\bnew\s+(?:const\s+)?|\S<\s*(?:const\s+)?)?\b' r'(int|float|double|bool|char|int32|uint32|int64|uint64)' r'(\([^)].*)' , line ) expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) if match and not expecting_function : matched_type = match . group ( 2 ) matched_new_or_template = match . group ( 1 ) if Match ( r'\([^()]+\)\s*\[' , match . group ( 3 ) ) : return matched_funcptr = match . group ( 3 ) if ( matched_new_or_template is None and not ( matched_funcptr and ( Match ( r'\((?:[^() ]+::\s*\*\s*)?[^() ]+\)\s*\(' , matched_funcptr ) or matched_funcptr . startswith ( '(*)' ) ) ) and not Match ( r'\s*using\s+\S+\s*=\s*' + matched_type , line ) and not Search ( r'new\(\S+\)\s*' + matched_type , line ) ) : error ( filename , linenum , 'readability/casting' , 4 , 'Using deprecated casting style. ' 'Use static_cast<%s>(...) instead' % matched_type ) if not expecting_function : CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , r'\((int|float|double|bool|char|u?int(16|32|64))\)' , error ) if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , r'\((char\s?\*+\s?)\)\s*"' , error ) : pass else : CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , r'\((\w+\s?\*+\s?)\)' , error ) match = Search ( r'(?:[^\w]&\(([^)*][^)]*)\)[\w(])|' r'(?:[^\w]&(static|dynamic|down|reinterpret)_cast\b)' , line ) if match : parenthesis_error = False match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\b)<' , line ) if match : _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) if x2 >= 0 : extended_line = clean_lines . elided [ y2 ] [ x2 : ] if y2 < clean_lines . NumLines ( ) - 1 : extended_line += clean_lines . elided [ y2 + 1 ] if Match ( r'\s*(?:->|\[)' , extended_line ) : parenthesis_error = True if parenthesis_error : error ( filename , linenum , 'readability/casting' , 4 , ( 'Are you taking an address of something dereferenced ' 'from a cast? Wrapping the dereferenced expression in ' 'parentheses will make the binding more obvious' ) ) else : error ( filename , linenum , 'runtime/casting' , 4 , ( 'Are you taking an address of a cast? ' 'This is dangerous: could be a temp var. ' 'Take the address before doing the cast, rather than after' ) )
10621	def get_element_mass_dictionary ( self ) : element_symbols = self . material . elements element_masses = self . get_element_masses ( ) return { s : m for s , m in zip ( element_symbols , element_masses ) }
4443	def delete ( self , string ) : return self . redis . execute_command ( AutoCompleter . SUGDEL_COMMAND , self . key , string )
5360	def execute_batch_tasks ( self , tasks_cls , big_delay = 0 , small_delay = 0 , wait_for_threads = True ) : def _split_tasks ( tasks_cls ) : backend_t = [ ] global_t = [ ] for t in tasks_cls : if t . is_backend_task ( t ) : backend_t . append ( t ) else : global_t . append ( t ) return backend_t , global_t backend_tasks , global_tasks = _split_tasks ( tasks_cls ) logger . debug ( 'backend_tasks = %s' % ( backend_tasks ) ) logger . debug ( 'global_tasks = %s' % ( global_tasks ) ) threads = [ ] stopper = threading . Event ( ) if len ( backend_tasks ) > 0 : repos_backend = self . _get_repos_by_backend ( ) for backend in repos_backend : t = TasksManager ( backend_tasks , backend , stopper , self . config , small_delay ) threads . append ( t ) t . start ( ) if len ( global_tasks ) > 0 : gt = TasksManager ( global_tasks , "Global tasks" , stopper , self . config , big_delay ) threads . append ( gt ) gt . start ( ) if big_delay > 0 : when = datetime . now ( ) + timedelta ( seconds = big_delay ) when_str = when . strftime ( '%a, %d %b %Y %H:%M:%S %Z' ) logger . info ( "%s will be executed on %s" % ( global_tasks , when_str ) ) if wait_for_threads : time . sleep ( 1 ) stopper . set ( ) for t in threads : t . join ( ) self . __check_queue_for_errors ( ) logger . debug ( "[thread:main] All threads (and their tasks) are finished" )
1832	def JC ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF , target . read ( ) , cpu . PC )
5102	def get_edge_type ( self , edge_type ) : edges = [ ] for e in self . edges ( ) : if self . adj [ e [ 0 ] ] [ e [ 1 ] ] . get ( 'edge_type' ) == edge_type : edges . append ( e ) return edges
9964	def update_lazyevals ( self ) : if self . lazy_evals is None : return elif isinstance ( self . lazy_evals , LazyEval ) : self . lazy_evals . get_updated ( ) else : for lz in self . lazy_evals : lz . get_updated ( )
2650	def send ( self , message_type , task_id , message ) : x = 0 try : buffer = pickle . dumps ( ( self . source_id , int ( time . time ( ) ) , message_type , message ) ) except Exception as e : print ( "Exception during pickling {}" . format ( e ) ) return try : x = self . sock . sendto ( buffer , ( self . ip , self . port ) ) except socket . timeout : print ( "Could not send message within timeout limit" ) return False return x
2043	def current_human_transaction ( self ) : try : tx , _ , _ , _ , _ = self . _callstack [ 0 ] if tx . result is not None : return None assert tx . depth == 0 return tx except IndexError : return None
6532	def get_local_config ( project_path , use_cache = True ) : pyproject_path = os . path . join ( project_path , 'pyproject.toml' ) if os . path . exists ( pyproject_path ) : with open ( pyproject_path , 'r' ) as config_file : config = pytoml . load ( config_file ) config = config . get ( 'tool' , { } ) . get ( 'tidypy' , { } ) config = merge_dict ( get_default_config ( ) , config ) config = process_extensions ( config , project_path , use_cache = use_cache ) return config return None
5984	def image_psf_shape_tag_from_image_psf_shape ( image_psf_shape ) : if image_psf_shape is None : return '' else : y = str ( image_psf_shape [ 0 ] ) x = str ( image_psf_shape [ 1 ] ) return ( '_image_psf_' + y + 'x' + x )
3669	def Rachford_Rice_flash_error ( V_over_F , zs , Ks ) : r return sum ( [ zi * ( Ki - 1. ) / ( 1. + V_over_F * ( Ki - 1. ) ) for Ki , zi in zip ( Ks , zs ) ] )
10835	def all ( self ) : response = self . api . get ( url = PATHS [ 'GET_PROFILES' ] ) for raw_profile in response : self . append ( Profile ( self . api , raw_profile ) ) return self
10100	def get_snippet ( self , snippet_id , timeout = None ) : return self . _api_request ( self . SNIPPET_ENDPOINT % ( snippet_id ) , self . HTTP_GET , timeout = timeout )
11836	def h ( self , node ) : "h function is straight-line distance from a node's state to goal." locs = getattr ( self . graph , 'locations' , None ) if locs : return int ( distance ( locs [ node . state ] , locs [ self . goal ] ) ) else : return infinity
13587	def add_formatted_field ( cls , field , format_string , title = '' ) : global klass_count klass_count += 1 fn_name = 'dyn_fn_%d' % klass_count cls . list_display . append ( fn_name ) if not title : title = field . capitalize ( ) _format_string = format_string def _ref ( self , obj ) : return _format_string % getattr ( obj , field ) _ref . short_description = title _ref . allow_tags = True _ref . admin_order_field = field setattr ( cls , fn_name , _ref )
7387	def get_idx ( self , node ) : group = self . find_node_group_membership ( node ) return self . nodes [ group ] . index ( node )
213	def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps
6204	def populations_diff_coeff ( particles , populations ) : D_counts = particles . diffusion_coeff_counts if len ( D_counts ) == 1 : pop_sizes = [ pop . stop - pop . start for pop in populations ] assert D_counts [ 0 ] [ 1 ] >= sum ( pop_sizes ) D_counts = [ ( D_counts [ 0 ] [ 0 ] , ps ) for ps in pop_sizes ] D_list = [ ] D_pop_start = 0 for pop , ( D , counts ) in zip ( populations , D_counts ) : D_list . append ( D ) assert pop . start >= D_pop_start assert pop . stop <= D_pop_start + counts D_pop_start += counts return D_list
3267	def md_dimension_info ( name , node ) : def _get_value ( child_name ) : return getattr ( node . find ( child_name ) , 'text' , None ) resolution = _get_value ( 'resolution' ) defaultValue = node . find ( "defaultValue" ) strategy = defaultValue . find ( "strategy" ) if defaultValue is not None else None strategy = strategy . text if strategy is not None else None return DimensionInfo ( name , _get_value ( 'enabled' ) == 'true' , _get_value ( 'presentation' ) , int ( resolution ) if resolution else None , _get_value ( 'units' ) , _get_value ( 'unitSymbol' ) , strategy , _get_value ( 'attribute' ) , _get_value ( 'endAttribute' ) , _get_value ( 'referenceValue' ) , _get_value ( 'nearestMatchEnabled' ) )
6407	def hmean ( nums ) : r if len ( nums ) < 1 : raise AttributeError ( 'hmean requires at least one value' ) elif len ( nums ) == 1 : return nums [ 0 ] else : for i in range ( 1 , len ( nums ) ) : if nums [ 0 ] != nums [ i ] : break else : return nums [ 0 ] if 0 in nums : if nums . count ( 0 ) > 1 : return float ( 'nan' ) return 0 return len ( nums ) / sum ( 1 / i for i in nums )
13173	def parents ( self , name = None ) : p = self . parent while p is not None : if name is None or p . tagname == name : yield p p = p . parent
5953	def stop_logging ( ) : from . import log logger = logging . getLogger ( "gromacs" ) logger . info ( "GromacsWrapper %s STOPPED logging" , get_version ( ) ) log . clear_handlers ( logger )
5632	def unindent ( lines ) : try : indent = min ( len ( line ) - len ( line . lstrip ( ) ) for line in lines if line ) except ValueError : return lines else : return [ line [ indent : ] for line in lines ]
6927	def newcursor ( self , dictcursor = False ) : handle = hashlib . sha256 ( os . urandom ( 12 ) ) . hexdigest ( ) if dictcursor : self . cursors [ handle ] = self . connection . cursor ( cursor_factory = psycopg2 . extras . DictCursor ) else : self . cursors [ handle ] = self . connection . cursor ( ) return ( self . cursors [ handle ] , handle )
2231	def lookup ( self , data ) : query_hash_type = data . __class__ key = ( query_hash_type . __module__ , query_hash_type . __name__ ) try : hash_type , hash_func = self . keyed_extensions [ key ] except KeyError : raise TypeError ( 'No registered hash func for hashable type=%r' % ( query_hash_type ) ) return hash_func
16	def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value
10933	def check_completion ( self ) : terminate = False term_dict = self . get_termination_stats ( get_cos = self . costol is not None ) terminate |= np . all ( np . abs ( term_dict [ 'delta_vals' ] ) < self . paramtol ) terminate |= ( term_dict [ 'delta_err' ] < self . errtol ) terminate |= ( term_dict [ 'exp_err' ] < self . exptol ) terminate |= ( term_dict [ 'frac_err' ] < self . fractol ) if self . costol is not None : terminate |= ( curcos < term_dict [ 'model_cosine' ] ) return terminate
6950	def jhk_to_sdssi ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , SDSSI_JHK , SDSSI_JH , SDSSI_JK , SDSSI_HK , SDSSI_J , SDSSI_H , SDSSI_K )
12305	def get_module_class ( class_path ) : mod_name , cls_name = class_path . rsplit ( '.' , 1 ) try : mod = import_module ( mod_name ) except ImportError as ex : raise EvoStreamException ( 'Error importing module %s: ' '"%s"' % ( mod_name , ex ) ) return getattr ( mod , cls_name )
8433	def cubehelix_pal ( start = 0 , rot = .4 , gamma = 1.0 , hue = 0.8 , light = .85 , dark = .15 , reverse = False ) : cdict = mpl . _cm . cubehelix ( gamma , start , rot , hue ) cubehelix_cmap = mpl . colors . LinearSegmentedColormap ( 'cubehelix' , cdict ) def cubehelix_palette ( n ) : values = np . linspace ( light , dark , n ) return [ mcolors . rgb2hex ( cubehelix_cmap ( x ) ) for x in values ] return cubehelix_palette
9302	def regenerate_signing_key ( self , secret_key = None , region = None , service = None , date = None ) : if secret_key is None and ( self . signing_key is None or self . signing_key . secret_key is None ) : raise NoSecretKeyError secret_key = secret_key or self . signing_key . secret_key region = region or self . region service = service or self . service date = date or self . date if self . signing_key is None : store_secret_key = True else : store_secret_key = self . signing_key . store_secret_key self . signing_key = AWS4SigningKey ( secret_key , region , service , date , store_secret_key ) self . region = region self . service = service self . date = self . signing_key . date
8900	def max_parameter_substitution ( ) : if os . path . isfile ( SQLITE_VARIABLE_FILE_CACHE ) : return conn = sqlite3 . connect ( ':memory:' ) low = 1 high = 1000 conn . execute ( 'CREATE TABLE T1 (id C1)' ) while low < high - 1 : guess = ( low + high ) // 2 try : statement = 'select * from T1 where id in (%s)' % ',' . join ( [ '?' for _ in range ( guess ) ] ) values = [ i for i in range ( guess ) ] conn . execute ( statement , values ) except sqlite3 . DatabaseError as ex : if 'too many SQL variables' in str ( ex ) : high = guess else : raise else : low = guess conn . close ( ) with open ( SQLITE_VARIABLE_FILE_CACHE , 'w' ) as file : file . write ( str ( low ) )
4405	def parse_line ( line , document = None ) : result = re . match ( line_pattern , line ) if result : _ , lineno , offset , severity , msg = result . groups ( ) lineno = int ( lineno or 1 ) offset = int ( offset or 0 ) errno = 2 if severity == 'error' : errno = 1 diag = { 'source' : 'mypy' , 'range' : { 'start' : { 'line' : lineno - 1 , 'character' : offset } , 'end' : { 'line' : lineno - 1 , 'character' : offset + 1 } } , 'message' : msg , 'severity' : errno } if document : word = document . word_at_position ( diag [ 'range' ] [ 'start' ] ) if word : diag [ 'range' ] [ 'end' ] [ 'character' ] = ( diag [ 'range' ] [ 'start' ] [ 'character' ] + len ( word ) ) return diag
12347	def field_metadata ( self , well_row = 0 , well_column = 0 , field_row = 0 , field_column = 0 ) : def condition ( path ) : attrs = attributes ( path ) return ( attrs . u == well_column and attrs . v == well_row and attrs . x == field_column and attrs . y == field_row ) field = [ f for f in self . fields if condition ( f ) ] if field : field = field [ 0 ] filename = _pattern ( field , 'metadata' , _image , extension = '*.ome.xml' ) filename = glob ( filename ) [ 0 ] return objectify . parse ( filename ) . getroot ( )
2443	def add_annotation_type ( self , doc , annotation_type ) : if len ( doc . annotations ) != 0 : if not self . annotation_type_set : self . annotation_type_set = True if validations . validate_annotation_type ( annotation_type ) : doc . annotations [ - 1 ] . annotation_type = annotation_type return True else : raise SPDXValueError ( 'Annotation::AnnotationType' ) else : raise CardinalityError ( 'Annotation::AnnotationType' ) else : raise OrderError ( 'Annotation::AnnotationType' )
13101	def main ( ) : config = Config ( ) core = HostSearch ( ) hosts = core . get_hosts ( tags = [ '!nessus' ] , up = True ) hosts = [ host for host in hosts ] host_ips = "," . join ( [ str ( host . address ) for host in hosts ] ) url = config . get ( 'nessus' , 'host' ) access = config . get ( 'nessus' , 'access_key' ) secret = config . get ( 'nessus' , 'secret_key' ) template_name = config . get ( 'nessus' , 'template_name' ) nessus = Nessus ( access , secret , url , template_name ) scan_id = nessus . create_scan ( host_ips ) nessus . start_scan ( scan_id ) for host in hosts : host . add_tag ( 'nessus' ) host . save ( ) Logger ( ) . log ( "nessus" , "Nessus scan started on {} hosts" . format ( len ( hosts ) ) , { 'scanned_hosts' : len ( hosts ) } )
9609	def find_exception_by_code ( code ) : errorName = None for error in WebDriverError : if error . value . code == code : errorName = error break return errorName
3968	def get_compose_dict ( assembled_specs , port_specs ) : compose_dict = _compose_dict_for_nginx ( port_specs ) for app_name in assembled_specs [ 'apps' ] . keys ( ) : compose_dict [ app_name ] = _composed_app_dict ( app_name , assembled_specs , port_specs ) for service_spec in assembled_specs [ 'services' ] . values ( ) : compose_dict [ service_spec . name ] = _composed_service_dict ( service_spec ) return compose_dict
2575	def launch_task ( self , task_id , executable , * args , ** kwargs ) : self . tasks [ task_id ] [ 'time_submitted' ] = datetime . datetime . now ( ) hit , memo_fu = self . memoizer . check_memo ( task_id , self . tasks [ task_id ] ) if hit : logger . info ( "Reusing cached result for task {}" . format ( task_id ) ) return memo_fu executor_label = self . tasks [ task_id ] [ "executor" ] try : executor = self . executors [ executor_label ] except Exception : logger . exception ( "Task {} requested invalid executor {}: config is\n{}" . format ( task_id , executor_label , self . _config ) ) if self . monitoring is not None and self . monitoring . resource_monitoring_enabled : executable = self . monitoring . monitor_wrapper ( executable , task_id , self . monitoring . monitoring_hub_url , self . run_id , self . monitoring . resource_monitoring_interval ) with self . submitter_lock : exec_fu = executor . submit ( executable , * args , ** kwargs ) self . tasks [ task_id ] [ 'status' ] = States . launched if self . monitoring is not None : task_log_info = self . _create_task_log_info ( task_id , 'lazy' ) self . monitoring . send ( MessageType . TASK_INFO , task_log_info ) exec_fu . retries_left = self . _config . retries - self . tasks [ task_id ] [ 'fail_count' ] logger . info ( "Task {} launched on executor {}" . format ( task_id , executor . label ) ) return exec_fu
12737	def parse_amc ( source ) : lines = 0 frames = 1 frame = { } degrees = False for line in source : lines += 1 line = line . split ( '#' ) [ 0 ] . strip ( ) if not line : continue if line . startswith ( ':' ) : if line . lower ( ) . startswith ( ':deg' ) : degrees = True continue if line . isdigit ( ) : if int ( line ) != frames : raise RuntimeError ( 'frame mismatch on line {}: ' 'produced {} but file claims {}' . format ( lines , frames , line ) ) yield frame frames += 1 frame = { } continue fields = line . split ( ) frame [ fields [ 0 ] ] = list ( map ( float , fields [ 1 : ] ) )
10358	def shuffle_node_data ( graph : BELGraph , key : str , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.3 assert 0 < percentage <= 1 n = graph . number_of_nodes ( ) swaps = int ( percentage * n * ( n - 1 ) / 2 ) result : BELGraph = graph . copy ( ) for _ in range ( swaps ) : s , t = random . sample ( result . node , 2 ) result . nodes [ s ] [ key ] , result . nodes [ t ] [ key ] = result . nodes [ t ] [ key ] , result . nodes [ s ] [ key ] return result
4545	def draw_circle ( setter , x0 , y0 , r , color = None ) : f = 1 - r ddF_x = 1 ddF_y = - 2 * r x = 0 y = r setter ( x0 , y0 + r , color ) setter ( x0 , y0 - r , color ) setter ( x0 + r , y0 , color ) setter ( x0 - r , y0 , color ) while x < y : if f >= 0 : y -= 1 ddF_y += 2 f += ddF_y x += 1 ddF_x += 2 f += ddF_x setter ( x0 + x , y0 + y , color ) setter ( x0 - x , y0 + y , color ) setter ( x0 + x , y0 - y , color ) setter ( x0 - x , y0 - y , color ) setter ( x0 + y , y0 + x , color ) setter ( x0 - y , y0 + x , color ) setter ( x0 + y , y0 - x , color ) setter ( x0 - y , y0 - x , color )
4049	def fulltext_item ( self , itemkey , ** kwargs ) : query_string = "/{t}/{u}/items/{itemkey}/fulltext" . format ( t = self . library_type , u = self . library_id , itemkey = itemkey ) return self . _build_query ( query_string )
5913	def cat ( self , out_ndx = None ) : if out_ndx is None : out_ndx = self . output self . make_ndx ( o = out_ndx , input = [ 'q' ] ) return out_ndx
12352	def rebuild ( self , image , wait = True ) : return self . _action ( 'rebuild' , image = image , wait = wait )
165	def compute_distance ( self , other , default = None ) : distances = self . compute_pointwise_distances ( other , default = [ ] ) if len ( distances ) == 0 : return default return min ( distances )
10896	def load_image ( self ) : try : image = initializers . load_tiff ( self . filename ) image = initializers . normalize ( image , invert = self . invert , scale = self . exposure , dtype = self . float_precision ) except IOError as e : log . error ( "Could not find image '%s'" % self . filename ) raise e return image
3059	def _get_backend ( filename ) : filename = os . path . abspath ( filename ) with _backends_lock : if filename not in _backends : _backends [ filename ] = _MultiprocessStorageBackend ( filename ) return _backends [ filename ]
856	def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )
436	def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
4969	def get_catalog_options ( self ) : if hasattr ( self . instance , 'site' ) : catalog_api = CourseCatalogApiClient ( self . user , self . instance . site ) else : catalog_api = CourseCatalogApiClient ( self . user ) catalogs = catalog_api . get_all_catalogs ( ) catalogs = sorted ( catalogs , key = lambda catalog : catalog . get ( 'name' , '' ) . lower ( ) ) return BLANK_CHOICE_DASH + [ ( catalog [ 'id' ] , catalog [ 'name' ] , ) for catalog in catalogs ]
8163	def _set_mode ( self , mode ) : if mode == CENTER : self . _call_transform_mode = self . _center_transform elif mode == CORNER : self . _call_transform_mode = self . _corner_transform else : raise ValueError ( 'mode must be CENTER or CORNER' )
12564	def _partition_data ( datavol , roivol , roivalue , maskvol = None , zeroe = True ) : if maskvol is not None : indices = ( roivol == roivalue ) * ( maskvol > 0 ) else : indices = roivol == roivalue if datavol . ndim == 4 : ts = datavol [ indices , : ] else : ts = datavol [ indices ] if zeroe : if datavol . ndim == 4 : ts = ts [ ts . sum ( axis = 1 ) != 0 , : ] return ts
7889	def update_presence ( self , presence ) : self . presence = MucPresence ( presence ) t = presence . get_type ( ) if t == "unavailable" : self . role = "none" self . affiliation = "none" self . room_jid = self . presence . get_from ( ) self . nick = self . room_jid . resource mc = self . presence . get_muc_child ( ) if isinstance ( mc , MucUserX ) : items = mc . get_items ( ) for item in items : if not isinstance ( item , MucItem ) : continue if item . role : self . role = item . role if item . affiliation : self . affiliation = item . affiliation if item . jid : self . real_jid = item . jid if item . nick : self . new_nick = item . nick break
2593	def get_last_checkpoint ( rundir = "runinfo" ) : if not os . path . isdir ( rundir ) : return [ ] dirs = sorted ( os . listdir ( rundir ) ) if len ( dirs ) == 0 : return [ ] last_runid = dirs [ - 1 ] last_checkpoint = os . path . abspath ( '{}/{}/checkpoint' . format ( rundir , last_runid ) ) if ( not ( os . path . isdir ( last_checkpoint ) ) ) : return [ ] return [ last_checkpoint ]
2843	def disable_FTDI_driver ( ) : logger . debug ( 'Disabling FTDI driver.' ) if sys . platform == 'darwin' : logger . debug ( 'Detected Mac OSX' ) _check_running_as_root ( ) subprocess . call ( 'kextunload -b com.apple.driver.AppleUSBFTDI' , shell = True ) subprocess . call ( 'kextunload /System/Library/Extensions/FTDIUSBSerialDriver.kext' , shell = True ) elif sys . platform . startswith ( 'linux' ) : logger . debug ( 'Detected Linux' ) _check_running_as_root ( ) subprocess . call ( 'modprobe -r -q ftdi_sio' , shell = True ) subprocess . call ( 'modprobe -r -q usbserial' , shell = True )
4701	def env ( ) : if cij . ssh . env ( ) : cij . err ( "cij.lnvm.env: invalid SSH environment" ) return 1 lnvm = cij . env_to_dict ( PREFIX , REQUIRED ) nvme = cij . env_to_dict ( "NVME" , [ "DEV_NAME" ] ) if "BGN" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_BGN" ) return 1 if "END" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_END" ) return 1 if "DEV_TYPE" not in lnvm . keys ( ) : cij . err ( "cij.lnvm.env: invalid LNVM_DEV_TYPE" ) return 1 lnvm [ "DEV_NAME" ] = "%sb%03de%03d" % ( nvme [ "DEV_NAME" ] , int ( lnvm [ "BGN" ] ) , int ( lnvm [ "END" ] ) ) lnvm [ "DEV_PATH" ] = "/dev/%s" % lnvm [ "DEV_NAME" ] cij . env_export ( PREFIX , EXPORTED , lnvm ) return 0
8585	def get_attached_cdroms ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/cdroms?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
1039	def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column
4429	async def _queue ( self , ctx , page : int = 1 ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) if not player . queue : return await ctx . send ( 'There\'s nothing in the queue! Why not queue something?' ) items_per_page = 10 pages = math . ceil ( len ( player . queue ) / items_per_page ) start = ( page - 1 ) * items_per_page end = start + items_per_page queue_list = '' for index , track in enumerate ( player . queue [ start : end ] , start = start ) : queue_list += f'`{index + 1}.` [**{track.title}**]({track.uri})\n' embed = discord . Embed ( colour = discord . Color . blurple ( ) , description = f'**{len(player.queue)} tracks**\n\n{queue_list}' ) embed . set_footer ( text = f'Viewing page {page}/{pages}' ) await ctx . send ( embed = embed )
12194	def _validate_first_message ( cls , msg ) : data = cls . _unpack_message ( msg ) logger . debug ( data ) if data != cls . RTM_HANDSHAKE : raise SlackApiError ( 'Unexpected response: {!r}' . format ( data ) ) logger . info ( 'Joined real-time messaging.' )
9419	def format_docstring ( * args , ** kwargs ) : def decorator ( func ) : func . __doc__ = getdoc ( func ) . format ( * args , ** kwargs ) return func return decorator
11470	def get_filesize ( self , filename ) : result = [ ] def dir_callback ( val ) : result . append ( val . split ( ) [ 4 ] ) self . _ftp . dir ( filename , dir_callback ) return result [ 0 ]
4182	def window_blackman_nuttall ( N ) : r a0 = 0.3635819 a1 = 0.4891775 a2 = 0.1365995 a3 = 0.0106411 return _coeff4 ( N , a0 , a1 , a2 , a3 )
11276	def disown ( debug ) : pid = os . getpid ( ) cgroup_file = "/proc/" + str ( pid ) + "/cgroup" try : infile = open ( cgroup_file , "r" ) except IOError : print ( "Could not open cgroup file: " , cgroup_file ) return False for line in infile : if line . find ( "ardexa.service" ) == - 1 : continue line = line . replace ( "name=" , "" ) items_list = line . split ( ':' ) accounts = items_list [ 1 ] dir_str = accounts + "/ardexa.disown" if not accounts : continue full_dir = "/sys/fs/cgroup/" + dir_str if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) if debug >= 1 : print ( "Making directory: " , full_dir ) else : if debug >= 1 : print ( "Directory already exists: " , full_dir ) full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) if accounts . find ( "," ) != - 1 : acct_list = accounts . split ( ',' ) accounts = acct_list [ 1 ] + "," + acct_list [ 0 ] dir_str = accounts + "/ardexa.disown" full_dir = "/sys/fs/cgroup/" + dir_str try : if not os . path . exists ( full_dir ) : os . makedirs ( full_dir ) except : continue full_path = full_dir + "/cgroup.procs" prog_list = [ "echo" , str ( pid ) , ">" , full_path ] run_program ( prog_list , debug , True ) infile . close ( ) if debug >= 1 : prog_list = [ "cat" , cgroup_file ] run_program ( prog_list , debug , False ) prog_list = [ "grep" , "-q" , "ardexa.service" , cgroup_file ] if run_program ( prog_list , debug , False ) : return False return True
963	def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal
3949	def execute ( self , using = None ) : if not using : using = self . get_connection ( ) inserted_entities = { } for klass in self . orders : number = self . quantities [ klass ] if klass not in inserted_entities : inserted_entities [ klass ] = [ ] for i in range ( 0 , number ) : entity = self . entities [ klass ] . execute ( using , inserted_entities ) inserted_entities [ klass ] . append ( entity ) return inserted_entities
3254	def delete_granule ( self , coverage , store , granule_id , workspace = None ) : params = dict ( ) workspace_name = workspace if isinstance ( store , basestring ) : store_name = store else : store_name = store . name workspace_name = store . workspace . name if workspace_name is None : raise ValueError ( "Must specify workspace" ) url = build_url ( self . service_url , [ "workspaces" , workspace_name , "coveragestores" , store_name , "coverages" , coverage , "index/granules" , granule_id , ".json" ] , params ) headers = { "Content-type" : "application/json" , "Accept" : "application/json" } resp = self . http_request ( url , method = 'delete' , headers = headers ) if resp . status_code != 200 : FailedRequestError ( 'Failed to delete granule from mosaic {} : {}, {}' . format ( store , resp . status_code , resp . text ) ) self . _cache . clear ( ) return None
1283	def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href="%s">%s</a>' % ( link , text )
4466	def __reconstruct ( params ) : if isinstance ( params , dict ) : if '__class__' in params : cls = params [ '__class__' ] data = __reconstruct ( params [ 'params' ] ) return cls ( ** data ) else : data = dict ( ) for key , value in six . iteritems ( params ) : data [ key ] = __reconstruct ( value ) return data elif isinstance ( params , ( list , tuple ) ) : return [ __reconstruct ( v ) for v in params ] else : return params
8764	def get_public_net_id ( self ) : for id , net_params in self . strategy . iteritems ( ) : if id == CONF . QUARK . public_net_id : return id return None
4516	def bresenham_line ( self , x0 , y0 , x1 , y1 , color = None , colorFunc = None ) : md . bresenham_line ( self . set , x0 , y0 , x1 , y1 , color , colorFunc )
9216	def file ( self , filename ) : with open ( filename ) as f : self . lexer . input ( f . read ( ) ) return self
11041	def get_single_header ( headers , key ) : raw_headers = headers . getRawHeaders ( key ) if raw_headers is None : return None header , _ = cgi . parse_header ( raw_headers [ - 1 ] ) return header
10936	def update_J ( self ) : self . calc_J ( ) step = np . ceil ( 1e-2 * self . J . shape [ 1 ] ) . astype ( 'int' ) self . JTJ = low_mem_sq ( self . J , step = step ) self . _fresh_JTJ = True self . _J_update_counter = 0 if np . any ( np . isnan ( self . JTJ ) ) : raise FloatingPointError ( 'J, JTJ have nans.' ) self . _exp_err = self . error - self . find_expected_error ( delta_params = 'perfect' )
12712	def add_torque ( self , torque , relative = False ) : op = self . ode_body . addRelTorque if relative else self . ode_body . addTorque op ( torque )
9600	def wait_for ( self , timeout = 10000 , interval = 1000 , asserter = lambda x : x ) : if not callable ( asserter ) : raise TypeError ( 'Asserter must be callable.' ) @ retry ( retry_on_exception = lambda ex : isinstance ( ex , WebDriverException ) , stop_max_delay = timeout , wait_fixed = interval ) def _wait_for ( driver ) : asserter ( driver ) return driver return _wait_for ( self )
444	def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output
13095	def callback ( self , event ) : if event . mask == 0x00000008 : if event . name . endswith ( '.json' ) : print_success ( "Ldapdomaindump file found" ) if event . name in [ 'domain_groups.json' , 'domain_users.json' ] : if event . name == 'domain_groups.json' : self . domain_groups_file = event . pathname if event . name == 'domain_users.json' : self . domain_users_file = event . pathname if self . domain_groups_file and self . domain_users_file : print_success ( "Importing users" ) subprocess . Popen ( [ 'jk-import-domaindump' , self . domain_groups_file , self . domain_users_file ] ) elif event . name == 'domain_computers.json' : print_success ( "Importing computers" ) subprocess . Popen ( [ 'jk-import-domaindump' , event . pathname ] ) self . ldap_strings = [ ] self . write_targets ( ) if event . name . endswith ( '_samhashes.sam' ) : host = event . name . replace ( '_samhashes.sam' , '' ) print_success ( "Secretsdump file, host ip: {}" . format ( host ) ) subprocess . Popen ( [ 'jk-import-secretsdump' , event . pathname ] ) self . ips . remove ( host ) self . write_targets ( )
10628	def T ( self , T ) : self . _T = T self . _Hfr = self . _calculate_Hfr ( T )
5400	def _map ( self , event ) : description = event . get ( 'description' , '' ) start_time = google_base . parse_rfc3339_utc_string ( event . get ( 'timestamp' , '' ) ) for name , regex in _EVENT_REGEX_MAP . items ( ) : match = regex . match ( description ) if match : return { 'name' : name , 'start-time' : start_time } , match return { 'name' : description , 'start-time' : start_time } , None
9128	def store_populate ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate ( resource ) _store_helper ( action , session = session ) return action
5413	def lookup_job_tasks ( self , statuses , user_ids = None , job_ids = None , job_names = None , task_ids = None , task_attempts = None , labels = None , create_time_min = None , create_time_max = None , max_tasks = 0 ) : statuses = None if statuses == { '*' } else statuses user_ids = None if user_ids == { '*' } else user_ids job_ids = None if job_ids == { '*' } else job_ids job_names = None if job_names == { '*' } else job_names task_ids = None if task_ids == { '*' } else task_ids task_attempts = None if task_attempts == { '*' } else task_attempts if labels or create_time_min or create_time_max : raise NotImplementedError ( 'Lookup by labels and create_time not yet supported by stub.' ) operations = [ x for x in self . _operations if ( ( not statuses or x . get_field ( 'status' , ( None , None ) ) [ 0 ] in statuses ) and ( not user_ids or x . get_field ( 'user' , None ) in user_ids ) and ( not job_ids or x . get_field ( 'job-id' , None ) in job_ids ) and ( not job_names or x . get_field ( 'job-name' , None ) in job_names ) and ( not task_ids or x . get_field ( 'task-id' , None ) in task_ids ) and ( not task_attempts or x . get_field ( 'task-attempt' , None ) in task_attempts ) ) ] if max_tasks > 0 : operations = operations [ : max_tasks ] return operations
1170	def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = " " * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )
4691	def decode_memo ( priv , pub , nonce , message ) : shared_secret = get_shared_secret ( priv , pub ) aes = init_aes ( shared_secret , nonce ) " Encryption " raw = bytes ( message , "ascii" ) cleartext = aes . decrypt ( unhexlify ( raw ) ) " Checksum " checksum = cleartext [ 0 : 4 ] message = cleartext [ 4 : ] message = _unpad ( message , 16 ) " Verify checksum " check = hashlib . sha256 ( message ) . digest ( ) [ 0 : 4 ] if check != checksum : raise ValueError ( "checksum verification failure" ) return message . decode ( "utf8" )
13030	def make_server ( host , port , app = None , server_class = AsyncWsgiServer , handler_class = AsyncWsgiHandler , ws_handler_class = None , ws_path = '/ws' ) : handler_class . ws_handler_class = ws_handler_class handler_class . ws_path = ws_path httpd = server_class ( ( host , port ) , RequestHandlerClass = handler_class ) httpd . set_app ( app ) return httpd
10215	def rank_subgraph_by_node_filter ( graph : BELGraph , node_predicates : Union [ NodePredicate , Iterable [ NodePredicate ] ] , annotation : str = 'Subgraph' , reverse : bool = True , ) -> List [ Tuple [ str , int ] ] : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) r2 = count_dict_values ( r1 ) return sorted ( r2 . items ( ) , key = itemgetter ( 1 ) , reverse = reverse )
4376	def write ( self , data ) : args = parse_qs ( self . handler . environ . get ( "QUERY_STRING" ) ) if "i" in args : i = args [ "i" ] else : i = "0" super ( JSONPolling , self ) . write ( "io.j[%s]('%s');" % ( i , data ) )
11486	def _descend_folder_for_id ( parsed_path , folder_id ) : if len ( parsed_path ) == 0 : return folder_id session . token = verify_credentials ( ) base_folder = session . communicator . folder_get ( session . token , folder_id ) cur_folder_id = - 1 for path_part in parsed_path : cur_folder_id = base_folder [ 'folder_id' ] cur_children = session . communicator . folder_children ( session . token , cur_folder_id ) for inner_folder in cur_children [ 'folders' ] : if inner_folder [ 'name' ] == path_part : base_folder = session . communicator . folder_get ( session . token , inner_folder [ 'folder_id' ] ) cur_folder_id = base_folder [ 'folder_id' ] break else : return - 1 return cur_folder_id
1011	def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print "Cells, all segments:" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved
8964	def _get_registered_executable ( exe_name ) : registered = None if sys . platform . startswith ( 'win' ) : if os . path . splitext ( exe_name ) [ 1 ] . lower ( ) != '.exe' : exe_name += '.exe' import _winreg try : key = "SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\" + exe_name value = _winreg . QueryValue ( _winreg . HKEY_LOCAL_MACHINE , key ) registered = ( value , "from HKLM\\" + key ) except _winreg . error : pass if registered and not os . path . exists ( registered [ 0 ] ) : registered = None return registered
1118	def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark
472	def build_reverse_dictionary ( word_to_id ) : reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) return reverse_dictionary
359	def load_file_list ( path = None , regx = '\.jpg' , printable = True , keep_prefix = False ) : r if path is None : path = os . getcwd ( ) file_list = os . listdir ( path ) return_list = [ ] for _ , f in enumerate ( file_list ) : if re . search ( regx , f ) : return_list . append ( f ) if keep_prefix : for i , f in enumerate ( return_list ) : return_list [ i ] = os . path . join ( path , f ) if printable : logging . info ( 'Match file list = %s' % return_list ) logging . info ( 'Number of files = %d' % len ( return_list ) ) return return_list
6923	def aovhm_theta ( times , mags , errs , frequency , nharmonics , magvariance ) : period = 1.0 / frequency ndet = times . size two_nharmonics = nharmonics + nharmonics phasedseries = phase_magseries_with_errs ( times , mags , errs , period , times [ 0 ] , sort = True , wrap = False ) phase = phasedseries [ 'phase' ] pmags = phasedseries [ 'mags' ] perrs = phasedseries [ 'errs' ] pweights = 1.0 / perrs phase = phase * 2.0 * pi_value z = npcos ( phase ) + 1.0j * npsin ( phase ) phase = nharmonics * phase psi = pmags * pweights * ( npcos ( phase ) + 1j * npsin ( phase ) ) zn = 1.0 + 0.0j phi = pweights + 0.0j theta_aov = 0.0 for _ in range ( two_nharmonics ) : phi_dot_phi = npsum ( phi * phi . conjugate ( ) ) alpha = npsum ( pweights * z * phi ) phi_dot_psi = npvdot ( phi , psi ) phi_dot_phi = npmax ( [ phi_dot_phi , 10.0e-9 ] ) alpha = alpha / phi_dot_phi theta_aov = ( theta_aov + npabs ( phi_dot_psi ) * npabs ( phi_dot_psi ) / phi_dot_phi ) phi = phi * z - alpha * zn * phi . conjugate ( ) zn = zn * z theta_aov = ( ( ndet - two_nharmonics - 1.0 ) * theta_aov / ( two_nharmonics * npmax ( [ magvariance - theta_aov , 1.0e-9 ] ) ) ) return theta_aov
5384	def _operation_status_message ( self ) : metadata = self . _op [ 'metadata' ] if not self . _op [ 'done' ] : if 'events' in metadata and metadata [ 'events' ] : last_event = metadata [ 'events' ] [ - 1 ] msg = last_event [ 'description' ] ds = last_event [ 'startTime' ] else : msg = 'Pending' ds = metadata [ 'createTime' ] else : ds = metadata [ 'endTime' ] if 'error' in self . _op : msg = self . _op [ 'error' ] [ 'message' ] else : msg = 'Success' return ( msg , google_base . parse_rfc3339_utc_string ( ds ) )
1861	def MOVS ( cpu , dest , src ) : base , size , ty = cpu . get_descriptor ( cpu . DS ) src_addr = src . address ( ) + base dest_addr = dest . address ( ) + base src_reg = src . mem . base dest_reg = dest . mem . base size = dest . size dest . write ( src . read ( ) ) increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment )
11355	def record_xml_output ( rec , pretty = True ) : from . html_utils import MathMLParser ret = etree . tostring ( rec , xml_declaration = False ) ret = re . sub ( "(&lt;)(([\/]?{0}))" . format ( "|[\/]?" . join ( MathMLParser . mathml_elements ) ) , '<\g<2>' , ret ) ret = re . sub ( "&gt;" , '>' , ret ) if pretty : ret = ret . replace ( '</datafield>' , ' </datafield>\n' ) ret = re . sub ( r'<datafield(.*?)>' , r' <datafield\1>\n' , ret ) ret = ret . replace ( '</subfield>' , '</subfield>\n' ) ret = ret . replace ( '<subfield' , ' <subfield' ) ret = ret . replace ( 'record>' , 'record>\n' ) return ret
8547	def get_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule_id ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/nics/%s/firewallrules/%s' % ( datacenter_id , server_id , nic_id , firewall_rule_id ) ) return response
10356	def random_by_nodes ( graph : BELGraph , percentage : Optional [ float ] = None ) -> BELGraph : percentage = percentage or 0.9 assert 0 < percentage <= 1 nodes = graph . nodes ( ) n = int ( len ( nodes ) * percentage ) subnodes = random . sample ( nodes , n ) result = graph . subgraph ( subnodes ) update_node_helper ( graph , result ) return result
4378	def add_parent ( self , parent ) : parent . children . add ( self ) self . parents . add ( parent )
10371	def build_pmid_exclusion_filter ( pmids : Strings ) -> EdgePredicate : if isinstance ( pmids , str ) : @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] != pmids elif isinstance ( pmids , Iterable ) : pmids = set ( pmids ) @ edge_predicate def pmid_exclusion_filter ( data : EdgeData ) -> bool : return has_pubmed ( data ) and data [ CITATION ] [ CITATION_REFERENCE ] not in pmids else : raise TypeError return pmid_exclusion_filter
3775	def solve_prop ( self , goal , reset_method = True ) : r if self . Tmin is None or self . Tmax is None : raise Exception ( 'Both a minimum and a maximum value are not present indicating there is not enough data for temperature dependency.' ) if not self . test_property_validity ( goal ) : raise Exception ( 'Input property is not considered plausible; no method would calculate it.' ) def error ( T ) : if reset_method : self . method = None return self . T_dependent_property ( T ) - goal try : return brenth ( error , self . Tmin , self . Tmax ) except ValueError : raise Exception ( 'To within the implemented temperature range, it is not possible to calculate the desired value.' )
8453	def clean ( ) : temple . check . in_git_repo ( ) current_branch = _get_current_branch ( ) update_branch = temple . constants . UPDATE_BRANCH_NAME temp_update_branch = temple . constants . TEMP_UPDATE_BRANCH_NAME if current_branch in ( update_branch , temp_update_branch ) : err_msg = ( 'You must change from the "{}" branch since it will be deleted during cleanup' ) . format ( current_branch ) raise temple . exceptions . InvalidCurrentBranchError ( err_msg ) if temple . check . _has_branch ( update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( update_branch ) ) if temple . check . _has_branch ( temp_update_branch ) : temple . utils . shell ( 'git branch -D {}' . format ( temp_update_branch ) )
6051	def map_2d_array_to_masked_1d_array_from_array_2d_and_mask ( mask , array_2d ) : total_image_pixels = mask_util . total_regular_pixels_from_mask ( mask ) array_1d = np . zeros ( shape = total_image_pixels ) index = 0 for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : array_1d [ index ] = array_2d [ y , x ] index += 1 return array_1d
7330	def stream_request ( self , method , url , headers = None , _session = None , * args , ** kwargs ) : return StreamResponse ( method = method , url = url , client = self , headers = headers , session = _session , proxy = self . proxy , ** kwargs )
7482	def assembly_cleanup ( data ) : data . stats_dfs . s2 = data . _build_stat ( "s2" ) data . stats_files . s2 = os . path . join ( data . dirs . edits , 's2_rawedit_stats.txt' ) with io . open ( data . stats_files . s2 , 'w' , encoding = 'utf-8' ) as outfile : data . stats_dfs . s2 . fillna ( value = 0 ) . astype ( np . int ) . to_string ( outfile )
11233	def get_inner_template ( self , language , template_type , indentation , key , val ) : inner_templates = { 'php' : { 'iterable' : '%s%s => array \n%s( \n%s%s),\n' % ( indentation , key , indentation , val , indentation ) , 'singular' : '%s%s => %s, \n' % ( indentation , key , val ) } , 'javascript' : { 'iterable' : '%s%s : {\n%s\n%s},\n' % ( indentation , key , val , indentation ) , 'singular' : '%s%s: %s,\n' % ( indentation , key , val ) } , 'ocaml' : { 'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % ( indentation , key , val , indentation ) , 'singular' : '%s(%s, %s);\n' % ( indentation , key , val ) } } return inner_templates [ language ] [ template_type ]
1173	def insort_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 a . insert ( lo , x )
10084	def discard ( self , pid = None ) : pid = pid or self . pid with db . session . begin_nested ( ) : before_record_update . send ( current_app . _get_current_object ( ) , record = self ) _ , record = self . fetch_published ( ) self . model . json = deepcopy ( record . model . json ) self . model . json [ '$schema' ] = self . build_deposit_schema ( record ) flag_modified ( self . model , 'json' ) db . session . merge ( self . model ) after_record_update . send ( current_app . _get_current_object ( ) , record = self ) return self . __class__ ( self . model . json , model = self . model )
9593	def switch_to_frame ( self , frame_reference = None ) : if frame_reference is not None and type ( frame_reference ) not in [ int , WebElement ] : raise TypeError ( 'Type of frame_reference must be None or int or WebElement' ) self . _execute ( Command . SWITCH_TO_FRAME , { 'id' : frame_reference } )
10543	def create_task ( project_id , info , n_answers = 30 , priority_0 = 0 , quorum = 0 ) : try : task = dict ( project_id = project_id , info = info , calibration = 0 , priority_0 = priority_0 , n_answers = n_answers , quorum = quorum ) res = _pybossa_req ( 'post' , 'task' , payload = task ) if res . get ( 'id' ) : return Task ( res ) else : return res except : raise
5245	def current_missing ( ** kwargs ) -> int : data_path = os . environ . get ( BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return 0 return len ( files . all_files ( f'{data_path}/Logs/{missing_info(**kwargs)}' ) )
12940	def getRedisPool ( params ) : global RedisPools global _defaultRedisConnectionParams global _redisManagedConnectionParams if not params : params = _defaultRedisConnectionParams isDefaultParams = True else : isDefaultParams = bool ( params is _defaultRedisConnectionParams ) if 'connection_pool' in params : return params [ 'connection_pool' ] hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] if not isDefaultParams : origParams = params params = copy . copy ( params ) else : origParams = params checkAgain = False if 'host' not in params : if not isDefaultParams and 'host' in _defaultRedisConnectionParams : params [ 'host' ] = _defaultRedisConnectionParams [ 'host' ] else : params [ 'host' ] = '127.0.0.1' checkAgain = True if 'port' not in params : if not isDefaultParams and 'port' in _defaultRedisConnectionParams : params [ 'port' ] = _defaultRedisConnectionParams [ 'port' ] else : params [ 'port' ] = 6379 checkAgain = True if 'db' not in params : if not isDefaultParams and 'db' in _defaultRedisConnectionParams : params [ 'db' ] = _defaultRedisConnectionParams [ 'db' ] else : params [ 'db' ] = 0 checkAgain = True if not isDefaultParams : otherGlobalKeys = set ( _defaultRedisConnectionParams . keys ( ) ) - set ( params . keys ( ) ) for otherKey in otherGlobalKeys : if otherKey == 'connection_pool' : continue params [ otherKey ] = _defaultRedisConnectionParams [ otherKey ] checkAgain = True if checkAgain : hashValue = hashDictOneLevel ( params ) if hashValue in RedisPools : params [ 'connection_pool' ] = RedisPools [ hashValue ] return RedisPools [ hashValue ] connectionPool = redis . ConnectionPool ( ** params ) origParams [ 'connection_pool' ] = params [ 'connection_pool' ] = connectionPool RedisPools [ hashValue ] = connectionPool origParamsHash = hashDictOneLevel ( origParams ) if origParamsHash not in _redisManagedConnectionParams : _redisManagedConnectionParams [ origParamsHash ] = [ origParams ] elif origParams not in _redisManagedConnectionParams [ origParamsHash ] : _redisManagedConnectionParams [ origParamsHash ] . append ( origParams ) return connectionPool
12995	def round_arr_teff_luminosity ( arr ) : arr [ 'temp' ] = np . around ( arr [ 'temp' ] , - 1 ) arr [ 'lum' ] = np . around ( arr [ 'lum' ] , 3 ) return arr
10724	def xformer ( signature ) : funcs = [ f for ( f , _ ) in xformers ( signature ) ] def the_func ( objects ) : if len ( objects ) != len ( funcs ) : raise IntoDPValueError ( objects , "objects" , "must have exactly %u items, has %u" % ( len ( funcs ) , len ( objects ) ) ) return [ x for ( x , _ ) in ( f ( a ) for ( f , a ) in zip ( funcs , objects ) ) ] return the_func
3673	def draw_2d ( self , width = 300 , height = 300 , Hs = False ) : r try : from rdkit . Chem import Draw from rdkit . Chem . Draw import IPythonConsole if Hs : mol = self . rdkitmol_Hs else : mol = self . rdkitmol return Draw . MolToImage ( mol , size = ( width , height ) ) except : return 'Rdkit is required for this feature.'
13378	def preprocess_dict ( d ) : out_env = { } for k , v in d . items ( ) : if not type ( v ) in PREPROCESSORS : raise KeyError ( 'Invalid type in dict: {}' . format ( type ( v ) ) ) out_env [ k ] = PREPROCESSORS [ type ( v ) ] ( v ) return out_env
11446	def create_deleted_record ( self , record ) : identifier = record_get_field_value ( record , tag = "037" , code = "a" ) recid = identifier . split ( ":" ) [ - 1 ] try : source = identifier . split ( ":" ) [ 1 ] except IndexError : source = "Unknown" record_add_field ( record , "035" , subfields = [ ( "9" , source ) , ( "a" , recid ) ] ) record_add_field ( record , "980" , subfields = [ ( "c" , "DELETED" ) ] ) return record
773	def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( "nupic.datafiles" , filename ) print "*" * 40 print "Collecting statistics for file:'%s'" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats
2380	def _load_rule_file ( self , filename ) : if not ( os . path . exists ( filename ) ) : sys . stderr . write ( "rflint: %s: No such file or directory\n" % filename ) return try : basename = os . path . basename ( filename ) ( name , ext ) = os . path . splitext ( basename ) imp . load_source ( name , filename ) except Exception as e : sys . stderr . write ( "rflint: %s: exception while loading: %s\n" % ( filename , str ( e ) ) )
12382	def link ( self , request , response ) : from armet . resources . managed . request import read if self . slug is None : raise http . exceptions . NotImplemented ( ) target = self . read ( ) links = self . _parse_link_headers ( request [ 'Link' ] ) for link in links : self . relate ( target , read ( self , link [ 'uri' ] ) ) self . response . status = http . client . NO_CONTENT self . make_response ( )
8941	def _to_webdav ( self , docs_base , release ) : try : git_path = subprocess . check_output ( 'git remote get-url origin 2>/dev/null' , shell = True ) except subprocess . CalledProcessError : git_path = '' else : git_path = git_path . decode ( 'ascii' ) . strip ( ) git_path = git_path . replace ( 'http://' , '' ) . replace ( 'https://' , '' ) . replace ( 'ssh://' , '' ) git_path = re . search ( r'[^:/]+?[:/](.+)' , git_path ) git_path = git_path . group ( 1 ) . replace ( '.git' , '' ) if git_path else '' url = None with self . _zipped ( docs_base ) as handle : url_ns = dict ( name = self . cfg . project . name , version = release , git_path = git_path ) reply = requests . put ( self . params [ 'url' ] . format ( ** url_ns ) , data = handle . read ( ) , headers = { 'Accept' : 'application/json' } ) if reply . status_code in range ( 200 , 300 ) : notify . info ( "{status_code} {reason}" . format ( ** vars ( reply ) ) ) try : data = reply . json ( ) except ValueError as exc : notify . warning ( "Didn't get a JSON response! ({})" . format ( exc ) ) else : if 'downloadUri' in data : url = data [ 'downloadUri' ] + '!/index.html' elif reply . status_code == 301 : url = reply . headers [ 'location' ] else : data = self . cfg . copy ( ) data . update ( self . params ) data . update ( vars ( reply ) ) notify . error ( "{status_code} {reason} for PUT to {url}" . format ( ** data ) ) if not url : notify . warning ( "Couldn't get URL from upload response!" ) return url
4375	def encode_payload ( self , messages ) : if not messages or messages [ 0 ] is None : return '' if len ( messages ) == 1 : return messages [ 0 ] . encode ( 'utf-8' ) payload = u'' . join ( [ ( u'\ufffd%d\ufffd%s' % ( len ( p ) , p ) ) for p in messages if p is not None ] ) return payload . encode ( 'utf-8' )
8317	def parse_balanced_image ( self , markup ) : opened = 0 closed = 0 for i in range ( len ( markup ) ) : if markup [ i ] == "[" : opened += 1 if markup [ i ] == "]" : closed += 1 if opened == closed : return markup [ : i + 1 ] return markup
4381	def exempt ( self , resource ) : if resource not in self . _exempt : self . _exempt . append ( resource )
8170	def separation ( self , r = 10 ) : vx = vy = vz = 0 for b in self . boids : if b != self : if abs ( self . x - b . x ) < r : vx += ( self . x - b . x ) if abs ( self . y - b . y ) < r : vy += ( self . y - b . y ) if abs ( self . z - b . z ) < r : vz += ( self . z - b . z ) return vx , vy , vz
9914	def create ( self , validated_data ) : email_query = models . EmailAddress . objects . filter ( email = self . validated_data [ "email" ] ) if email_query . exists ( ) : email = email_query . get ( ) email . send_duplicate_notification ( ) else : email = super ( EmailSerializer , self ) . create ( validated_data ) email . send_confirmation ( ) user = validated_data . get ( "user" ) query = models . EmailAddress . objects . filter ( is_primary = True , user = user ) if not query . exists ( ) : email . set_primary ( ) return email
11818	def expected_utility ( a , s , U , mdp ) : "The expected utility of doing a in state s, according to the MDP and U." return sum ( [ p * U [ s1 ] for ( p , s1 ) in mdp . T ( s , a ) ] )
9626	def detail_view ( self , request , module , preview ) : try : preview = self . __previews [ module ] [ preview ] except KeyError : raise Http404 return preview . detail_view ( request )
12182	async def execute_method ( self , method , ** params ) : url = self . url_builder ( method , url_params = params ) logger . info ( 'Executing method %r' , method ) response = await aiohttp . get ( url ) logger . info ( 'Status: %r' , response . status ) if response . status == 200 : json = await response . json ( ) logger . debug ( '...with JSON %r' , json ) if json . get ( 'ok' ) : return json raise SlackApiError ( json [ 'error' ] ) else : raise_for_status ( response )
8057	def do_restart ( self , line ) : self . bot . _frame = 0 self . bot . _namespace . clear ( ) self . bot . _namespace . update ( self . bot . _initial_namespace )
52	def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )
10419	def group_dict_set ( iterator : Iterable [ Tuple [ A , B ] ] ) -> Mapping [ A , Set [ B ] ] : d = defaultdict ( set ) for key , value in iterator : d [ key ] . add ( value ) return dict ( d )
11461	def add_control_number ( self , tag , value ) : record_add_field ( self . record , tag , controlfield_value = value )
4790	def is_lower ( self ) : if not isinstance ( self . val , str_types ) : raise TypeError ( 'val is not a string' ) if len ( self . val ) == 0 : raise ValueError ( 'val is empty' ) if self . val != self . val . lower ( ) : self . _err ( 'Expected <%s> to contain only lowercase chars, but did not.' % self . val ) return self
13352	def monitor ( self , sleep = 5 ) : manager = FileModificationObjectManager ( ) timestamps = { } filebodies = { } for file in self . f_repository : timestamps [ file ] = self . _get_mtime ( file ) filebodies [ file ] = open ( file ) . read ( ) while True : for file in self . f_repository : mtime = timestamps [ file ] fbody = filebodies [ file ] modified = self . _check_modify ( file , mtime , fbody ) if not modified : continue new_mtime = self . _get_mtime ( file ) new_fbody = open ( file ) . read ( ) obj = FileModificationObject ( file , ( mtime , new_mtime ) , ( fbody , new_fbody ) ) timestamps [ file ] = new_mtime filebodies [ file ] = new_fbody manager . add_object ( obj ) yield obj time . sleep ( sleep )
2502	def to_special_value ( self , value ) : if value == self . spdx_namespace . none : return utils . SPDXNone ( ) elif value == self . spdx_namespace . noassertion : return utils . NoAssert ( ) elif value == self . spdx_namespace . unknown : return utils . UnKnown ( ) else : return value
905	def write ( self , proto ) : proto . iteration = self . _iteration pHistScores = proto . init ( 'historicalScores' , len ( self . _historicalScores ) ) for i , score in enumerate ( list ( self . _historicalScores ) ) : _ , value , anomalyScore = score record = pHistScores [ i ] record . value = float ( value ) record . anomalyScore = float ( anomalyScore ) if self . _distribution : proto . distribution . name = self . _distribution [ "distribution" ] [ "name" ] proto . distribution . mean = float ( self . _distribution [ "distribution" ] [ "mean" ] ) proto . distribution . variance = float ( self . _distribution [ "distribution" ] [ "variance" ] ) proto . distribution . stdev = float ( self . _distribution [ "distribution" ] [ "stdev" ] ) proto . distribution . movingAverage . windowSize = float ( self . _distribution [ "movingAverage" ] [ "windowSize" ] ) historicalValues = self . _distribution [ "movingAverage" ] [ "historicalValues" ] pHistValues = proto . distribution . movingAverage . init ( "historicalValues" , len ( historicalValues ) ) for i , value in enumerate ( historicalValues ) : pHistValues [ i ] = float ( value ) proto . distribution . movingAverage . total = float ( self . _distribution [ "movingAverage" ] [ "total" ] ) historicalLikelihoods = self . _distribution [ "historicalLikelihoods" ] pHistLikelihoods = proto . distribution . init ( "historicalLikelihoods" , len ( historicalLikelihoods ) ) for i , likelihood in enumerate ( historicalLikelihoods ) : pHistLikelihoods [ i ] = float ( likelihood ) proto . probationaryPeriod = self . _probationaryPeriod proto . learningPeriod = self . _learningPeriod proto . reestimationPeriod = self . _reestimationPeriod proto . historicWindowSize = self . _historicalScores . maxlen
9540	def number_range_exclusive ( min , max , type = float ) : def checker ( v ) : if type ( v ) <= min or type ( v ) >= max : raise ValueError ( v ) return checker
1940	def get_func_signature ( self , hsh : bytes ) -> Optional [ str ] : if not isinstance ( hsh , ( bytes , bytearray ) ) : raise TypeError ( 'The selector argument must be a concrete byte array' ) return self . _function_signatures_by_selector . get ( hsh )
7405	def top ( self ) : o = self . get_ordering_queryset ( ) . aggregate ( Min ( 'order' ) ) . get ( 'order__min' ) self . to ( o )
6110	def unmasked_blurred_image_of_galaxies_from_psf ( self , padded_grid_stack , psf ) : return [ padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf , image ) if not galaxy . has_pixelization else None for galaxy , image in zip ( self . galaxies , self . image_plane_image_1d_of_galaxies ) ]
11653	def __get_live_version ( self ) : try : import versiontools except ImportError : return None else : return str ( versiontools . Version . from_expression ( self . name ) )
4777	def is_empty ( self ) : if len ( self . val ) != 0 : if isinstance ( self . val , str_types ) : self . _err ( 'Expected <%s> to be empty string, but was not.' % self . val ) else : self . _err ( 'Expected <%s> to be empty, but was not.' % self . val ) return self
6749	def capture_bash ( self ) : class Capture ( object ) : def __init__ ( self , satchel ) : self . satchel = satchel self . _dryrun = self . satchel . dryrun self . satchel . dryrun = 1 begincap ( ) self . _stdout = sys . stdout self . _stderr = sys . stderr self . stdout = sys . stdout = StringIO ( ) self . stderr = sys . stderr = StringIO ( ) def __enter__ ( self ) : return self def __exit__ ( self , type , value , traceback ) : endcap ( ) self . satchel . dryrun = self . _dryrun sys . stdout = self . _stdout sys . stderr = self . _stderr return Capture ( self )
8538	def run ( self , * args , ** kwargs ) : while True : try : timestamp , ip_p = self . _queue . popleft ( ) src_ip = get_ip ( ip_p , ip_p . src ) dst_ip = get_ip ( ip_p , ip_p . dst ) src = intern ( '%s:%s' % ( src_ip , ip_p . data . sport ) ) dst = intern ( '%s:%s' % ( dst_ip , ip_p . data . dport ) ) key = intern ( '%s<->%s' % ( src , dst ) ) stream = self . _streams . get ( key ) if stream is None : stream = Stream ( src , dst ) self . _streams [ key ] = stream setattr ( ip_p , 'timestamp' , timestamp ) pushed = stream . push ( ip_p ) if not pushed : continue for handler in self . _handlers : try : handler ( stream ) except Exception as ex : print ( 'handler exception: %s' % ex ) except Exception : time . sleep ( 0.00001 )
10007	def get_object ( self , name ) : parts = name . split ( "." ) space = self . spaces [ parts . pop ( 0 ) ] if parts : return space . get_object ( "." . join ( parts ) ) else : return space
634	def destroySynapse ( self , synapse ) : self . _numSynapses -= 1 self . _removeSynapseFromPresynapticMap ( synapse ) synapse . segment . _synapses . remove ( synapse )
4179	def window_bartlett_hann ( N ) : r if N == 1 : return ones ( 1 ) n = arange ( 0 , N ) a0 = 0.62 a1 = 0.48 a2 = 0.38 win = a0 - a1 * abs ( n / ( N - 1. ) - 0.5 ) - a2 * cos ( 2 * pi * n / ( N - 1. ) ) return win
5121	def reset_colors ( self ) : for k , e in enumerate ( self . g . edges ( ) ) : self . g . set_ep ( e , 'edge_color' , self . edge2queue [ k ] . colors [ 'edge_color' ] ) for v in self . g . nodes ( ) : self . g . set_vp ( v , 'vertex_fill_color' , self . colors [ 'vertex_fill_color' ] )
10906	def sim_crb_diff ( std0 , std1 , N = 10000 ) : a = std0 * np . random . randn ( N , len ( std0 ) ) b = std1 * np . random . randn ( N , len ( std1 ) ) return a - b
1607	def spec ( cls , name = None , inputs = None , par = 1 , config = None , optional_outputs = None ) : python_class_path = "%s.%s" % ( cls . __module__ , cls . __name__ ) if hasattr ( cls , 'outputs' ) : _outputs = copy . copy ( cls . outputs ) else : _outputs = [ ] if optional_outputs is not None : assert isinstance ( optional_outputs , ( list , tuple ) ) for out in optional_outputs : assert isinstance ( out , ( str , Stream ) ) _outputs . append ( out ) return HeronComponentSpec ( name , python_class_path , is_spout = False , par = par , inputs = inputs , outputs = _outputs , config = config )
2799	def convert_convtranspose ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting transposed convolution ...' ) if names == 'short' : tf_name = 'C' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) bias_name = '{0}.bias' . format ( w_name ) weights_name = '{0}.weight' . format ( w_name ) if len ( weights [ weights_name ] . numpy ( ) . shape ) == 4 : W = weights [ weights_name ] . numpy ( ) . transpose ( 2 , 3 , 1 , 0 ) height , width , n_filters , channels = W . shape n_groups = params [ 'group' ] if n_groups > 1 : raise AssertionError ( 'Cannot convert conv1d with groups != 1' ) if params [ 'dilations' ] [ 0 ] > 1 : raise AssertionError ( 'Cannot convert conv1d with dilation_rate != 1' ) if bias_name in weights : biases = weights [ bias_name ] . numpy ( ) has_bias = True else : biases = None has_bias = False input_name = inputs [ 0 ] if has_bias : weights = [ W , biases ] else : weights = [ W ] conv = keras . layers . Conv2DTranspose ( filters = n_filters , kernel_size = ( height , width ) , strides = ( params [ 'strides' ] [ 0 ] , params [ 'strides' ] [ 1 ] ) , padding = 'valid' , output_padding = 0 , weights = weights , use_bias = has_bias , activation = None , dilation_rate = params [ 'dilations' ] [ 0 ] , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , name = tf_name ) layers [ scope_name ] = conv ( layers [ input_name ] ) layers [ scope_name ] . set_shape ( layers [ scope_name ] . _keras_shape ) pads = params [ 'pads' ] if pads [ 0 ] > 0 : assert ( len ( pads ) == 2 or ( pads [ 2 ] == pads [ 0 ] and pads [ 3 ] == pads [ 1 ] ) ) crop = keras . layers . Cropping2D ( pads [ : 2 ] , name = tf_name + '_crop' ) layers [ scope_name ] = crop ( layers [ scope_name ] ) else : raise AssertionError ( 'Layer is not supported for now' )
7037	def xmatch_search ( lcc_server , file_to_upload , xmatch_dist_arcsec = 3.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , limitspec = None , samplespec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : with open ( file_to_upload ) as infd : xmq = infd . read ( ) xmqlines = len ( xmq . split ( '\n' ) [ : - 1 ] ) if xmqlines > 5000 : LOGERROR ( 'you have more than 5000 lines in the file to upload: %s' % file_to_upload ) return None , None , None params = { 'xmq' : xmq , 'xmd' : xmatch_dist_arcsec } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/xmatch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
5600	def for_web ( self , data ) : rgba = self . _prepare_array_for_png ( data ) data = ma . masked_where ( rgba == self . nodata , rgba ) return memory_file ( data , self . profile ( ) ) , 'image/png'
12902	def _set_range ( self , start , stop , value , value_len ) : assert stop >= start and value_len >= 0 range_len = stop - start if range_len < value_len : self . _insert_zeros ( stop , stop + value_len - range_len ) self . _copy_to_range ( start , value , value_len ) elif range_len > value_len : self . _del_range ( stop - ( range_len - value_len ) , stop ) self . _copy_to_range ( start , value , value_len ) else : self . _copy_to_range ( start , value , value_len )
12617	def get_data ( img ) : if hasattr ( img , '_data_cache' ) and img . _data_cache is None : img = copy . deepcopy ( img ) gc . collect ( ) return img . get_data ( )
9700	def worker ( wrapped , dkwargs , hash_value = None , * args , ** kwargs ) : if "event" not in dkwargs : msg = "djwebhooks.decorators.redis_hook requires an 'event' argument in the decorator." raise TypeError ( msg ) event = dkwargs [ 'event' ] if "owner" not in kwargs : msg = "djwebhooks.senders.redis_callable requires an 'owner' argument in the decorated function." raise TypeError ( msg ) owner = kwargs [ 'owner' ] if "identifier" not in kwargs : msg = "djwebhooks.senders.orm_callable requires an 'identifier' argument in the decorated function." raise TypeError ( msg ) identifier = kwargs [ 'identifier' ] senderobj = DjangoRQSenderable ( wrapped , dkwargs , hash_value , WEBHOOK_ATTEMPTS , * args , ** kwargs ) senderobj . webhook_target = WebhookTarget . objects . get ( event = event , owner = owner , identifier = identifier ) senderobj . url = senderobj . webhook_target . target_url senderobj . payload = senderobj . get_payload ( ) senderobj . payload [ 'owner' ] = getattr ( kwargs [ 'owner' ] , WEBHOOK_OWNER_FIELD ) senderobj . payload [ 'event' ] = dkwargs [ 'event' ] return senderobj . send ( )
13624	def Integer ( value , base = 10 , encoding = None ) : try : return int ( Text ( value , encoding ) , base ) except ( TypeError , ValueError ) : return None
6538	def compile_masks ( masks ) : if not masks : masks = [ ] elif not isinstance ( masks , ( list , tuple ) ) : masks = [ masks ] return [ re . compile ( mask ) for mask in masks ]
5100	def _dict2dict ( adj_dict ) : item = adj_dict . popitem ( ) adj_dict [ item [ 0 ] ] = item [ 1 ] if not isinstance ( item [ 1 ] , dict ) : new_dict = { } for key , value in adj_dict . items ( ) : new_dict [ key ] = { v : { } for v in value } adj_dict = new_dict return adj_dict
13655	def _matchRoute ( components , request , segments , partialMatching ) : if len ( components ) == 1 and isinstance ( components [ 0 ] , bytes ) : components = components [ 0 ] if components [ : 1 ] == '/' : components = components [ 1 : ] components = components . split ( '/' ) results = OrderedDict ( ) NO_MATCH = None , segments remaining = list ( segments ) if len ( segments ) == len ( components ) == 0 : return results , remaining for us , them in izip_longest ( components , segments ) : if us is None : if partialMatching : break else : return NO_MATCH elif them is None : return NO_MATCH if callable ( us ) : name , match = us ( request , them ) if match is None : return NO_MATCH results [ name ] = match elif us != them : return NO_MATCH remaining . pop ( 0 ) return results , remaining
12654	def convert_dcm2nii ( input_dir , output_dir , filename ) : if not op . exists ( input_dir ) : raise IOError ( 'Expected an existing folder in {}.' . format ( input_dir ) ) if not op . exists ( output_dir ) : raise IOError ( 'Expected an existing output folder in {}.' . format ( output_dir ) ) tmpdir = tempfile . TemporaryDirectory ( prefix = 'dcm2nii_' ) arguments = '-o "{}" -i y' . format ( tmpdir . name ) try : call_out = call_dcm2nii ( input_dir , arguments ) except : raise else : log . info ( 'Converted "{}" to nifti.' . format ( input_dir ) ) filenames = glob ( op . join ( tmpdir . name , '*.nii*' ) ) cleaned_filenames = remove_dcm2nii_underprocessed ( filenames ) filepaths = [ ] for srcpath in cleaned_filenames : dstpath = op . join ( output_dir , filename ) realpath = copy_w_plus ( srcpath , dstpath ) filepaths . append ( realpath ) basename = op . basename ( remove_ext ( srcpath ) ) aux_files = set ( glob ( op . join ( tmpdir . name , '{}.*' . format ( basename ) ) ) ) - set ( glob ( op . join ( tmpdir . name , '{}.nii*' . format ( basename ) ) ) ) for aux_file in aux_files : aux_dstpath = copy_w_ext ( aux_file , output_dir , remove_ext ( op . basename ( realpath ) ) ) filepaths . append ( aux_dstpath ) return filepaths
11241	def get_line_count ( fname ) : i = 0 with open ( fname ) as f : for i , l in enumerate ( f ) : pass return i + 1
492	def acquireConnection ( self ) : self . _logger . debug ( "Acquiring connection" ) dbConn = self . _pool . connection ( shareable = False ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap
8406	def zero_range ( x , tol = np . finfo ( float ) . eps * 100 ) : try : if len ( x ) == 1 : return True except TypeError : return True if len ( x ) != 2 : raise ValueError ( 'x must be length 1 or 2' ) x = tuple ( x ) if isinstance ( x [ 0 ] , ( pd . Timestamp , datetime . datetime ) ) : x = date2num ( x ) elif isinstance ( x [ 0 ] , np . datetime64 ) : return x [ 0 ] == x [ 1 ] elif isinstance ( x [ 0 ] , ( pd . Timedelta , datetime . timedelta ) ) : x = x [ 0 ] . total_seconds ( ) , x [ 1 ] . total_seconds ( ) elif isinstance ( x [ 0 ] , np . timedelta64 ) : return x [ 0 ] == x [ 1 ] elif not isinstance ( x [ 0 ] , ( float , int , np . number ) ) : raise TypeError ( "zero_range objects cannot work with objects " "of type '{}'" . format ( type ( x [ 0 ] ) ) ) if any ( np . isnan ( x ) ) : return np . nan if x [ 0 ] == x [ 1 ] : return True if all ( np . isinf ( x ) ) : return False m = np . abs ( x ) . min ( ) if m == 0 : return False return np . abs ( ( x [ 0 ] - x [ 1 ] ) / m ) < tol
2326	def orient_graph ( self , df_data , graph , nb_runs = 6 , printout = None , ** kwargs ) : if type ( graph ) == nx . DiGraph : edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) ] oriented_edges = [ a for a in list ( graph . edges ( ) ) if ( a [ 1 ] , a [ 0 ] ) not in list ( graph . edges ( ) ) ] for a in edges : if ( a [ 1 ] , a [ 0 ] ) in list ( graph . edges ( ) ) : edges . remove ( a ) output = nx . DiGraph ( ) for i in oriented_edges : output . add_edge ( * i ) elif type ( graph ) == nx . Graph : edges = list ( graph . edges ( ) ) output = nx . DiGraph ( ) else : raise TypeError ( "Data type not understood." ) res = [ ] for idx , ( a , b ) in enumerate ( edges ) : weight = self . predict_proba ( df_data [ a ] . values . reshape ( ( - 1 , 1 ) ) , df_data [ b ] . values . reshape ( ( - 1 , 1 ) ) , idx = idx , nb_runs = nb_runs , ** kwargs ) if weight > 0 : output . add_edge ( a , b , weight = weight ) else : output . add_edge ( b , a , weight = abs ( weight ) ) if printout is not None : res . append ( [ str ( a ) + '-' + str ( b ) , weight ] ) DataFrame ( res , columns = [ 'SampleID' , 'Predictions' ] ) . to_csv ( printout , index = False ) for node in list ( df_data . columns . values ) : if node not in output . nodes ( ) : output . add_node ( node ) return output
3299	def xml_to_bytes ( element , pretty_print = False ) : if use_lxml : xml = etree . tostring ( element , encoding = "UTF-8" , xml_declaration = True , pretty_print = pretty_print ) else : xml = etree . tostring ( element , encoding = "UTF-8" ) if not xml . startswith ( b"<?xml " ) : xml = b'<?xml version="1.0" encoding="utf-8" ?>\n' + xml assert xml . startswith ( b"<?xml " ) return xml
1619	def CleanseRawStrings ( raw_lines ) : delimiter = None lines_without_raw_strings = [ ] for line in raw_lines : if delimiter : end = line . find ( delimiter ) if end >= 0 : leading_space = Match ( r'^(\s*)\S' , line ) line = leading_space . group ( 1 ) + '""' + line [ end + len ( delimiter ) : ] delimiter = None else : line = '""' while delimiter is None : matched = Match ( r'^(.*?)\b(?:R|u8R|uR|UR|LR)"([^\s\\()]*)\((.*)$' , line ) if ( matched and not Match ( r'^([^\'"]|\'(\\.|[^\'])*\'|"(\\.|[^"])*")*//' , matched . group ( 1 ) ) ) : delimiter = ')' + matched . group ( 2 ) + '"' end = matched . group ( 3 ) . find ( delimiter ) if end >= 0 : line = ( matched . group ( 1 ) + '""' + matched . group ( 3 ) [ end + len ( delimiter ) : ] ) delimiter = None else : line = matched . group ( 1 ) + '""' else : break lines_without_raw_strings . append ( line ) return lines_without_raw_strings
7268	def operator ( name = None , operators = None , aliases = None , kind = None ) : def delegator ( assertion , subject , expected , * args , ** kw ) : return assertion . test ( subject , expected , * args , ** kw ) def decorator ( fn ) : operator = Operator ( fn = fn , aliases = aliases , kind = kind ) _name = name if isinstance ( name , six . string_types ) else fn . __name__ operator . operators = ( _name , ) _operators = operators if isinstance ( _operators , list ) : _operators = tuple ( _operators ) if isinstance ( _operators , tuple ) : operator . operators += _operators Engine . register ( operator ) return functools . partial ( delegator , operator ) return decorator ( name ) if inspect . isfunction ( name ) else decorator
2816	def convert_maxpool3 ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting pooling ...' ) if names == 'short' : tf_name = 'P' + random_string ( 7 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if 'kernel_shape' in params : height , width , depth = params [ 'kernel_shape' ] else : height , width , depth = params [ 'kernel_size' ] if 'strides' in params : stride_height , stride_width , stride_depth = params [ 'strides' ] else : stride_height , stride_width , stride_depth = params [ 'stride' ] if 'pads' in params : padding_h , padding_w , padding_d , _ , _ = params [ 'pads' ] else : padding_h , padding_w , padding_d = params [ 'padding' ] input_name = inputs [ 0 ] if padding_h > 0 and padding_w > 0 and padding_d > 0 : padding_name = tf_name + '_pad' padding_layer = keras . layers . ZeroPadding3D ( padding = ( padding_h , padding_w , padding_d ) , name = padding_name ) layers [ padding_name ] = padding_layer ( layers [ inputs [ 0 ] ] ) input_name = padding_name pooling = keras . layers . MaxPooling3D ( pool_size = ( height , width , depth ) , strides = ( stride_height , stride_width , stride_depth ) , padding = 'valid' , name = tf_name ) layers [ scope_name ] = pooling ( layers [ input_name ] )
13596	def get ( f , key , default = None ) : if key in f . keys ( ) : val = f [ key ] . value if default is None : return val else : if _np . shape ( val ) == _np . shape ( default ) : return val return default
1794	def NEG ( cpu , dest ) : source = dest . read ( ) res = dest . write ( - source ) cpu . _calculate_logic_flags ( dest . size , res ) cpu . CF = source != 0 cpu . AF = ( res & 0x0f ) != 0x00
4321	def biquad ( self , b , a ) : if not isinstance ( b , list ) : raise ValueError ( 'b must be a list.' ) if not isinstance ( a , list ) : raise ValueError ( 'a must be a list.' ) if len ( b ) != 3 : raise ValueError ( 'b must be a length 3 list.' ) if len ( a ) != 3 : raise ValueError ( 'a must be a length 3 list.' ) if not all ( [ is_number ( b_val ) for b_val in b ] ) : raise ValueError ( 'all elements of b must be numbers.' ) if not all ( [ is_number ( a_val ) for a_val in a ] ) : raise ValueError ( 'all elements of a must be numbers.' ) effect_args = [ 'biquad' , '{:f}' . format ( b [ 0 ] ) , '{:f}' . format ( b [ 1 ] ) , '{:f}' . format ( b [ 2 ] ) , '{:f}' . format ( a [ 0 ] ) , '{:f}' . format ( a [ 1 ] ) , '{:f}' . format ( a [ 2 ] ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'biquad' ) return self
642	def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]
10058	def records ( ) : import pkg_resources from dojson . contrib . marc21 import marc21 from dojson . contrib . marc21 . utils import create_record , split_blob from flask_login import login_user , logout_user from invenio_accounts . models import User from invenio_deposit . api import Deposit users = User . query . all ( ) data_path = pkg_resources . resource_filename ( 'invenio_records' , 'data/marc21/bibliographic.xml' ) with open ( data_path ) as source : with current_app . test_request_context ( ) : indexer = RecordIndexer ( ) with db . session . begin_nested ( ) : for index , data in enumerate ( split_blob ( source . read ( ) ) , start = 1 ) : login_user ( users [ index % len ( users ) ] ) record = marc21 . do ( create_record ( data ) ) indexer . index ( Deposit . create ( record ) ) logout_user ( ) db . session . commit ( )
4132	def split_code_and_text_blocks ( source_file ) : docstring , rest_of_content = get_docstring_and_rest ( source_file ) blocks = [ ( 'text' , docstring ) ] pattern = re . compile ( r'(?P<header_line>^#{20,}.*)\s(?P<text_content>(?:^#.*\s)*)' , flags = re . M ) pos_so_far = 0 for match in re . finditer ( pattern , rest_of_content ) : match_start_pos , match_end_pos = match . span ( ) code_block_content = rest_of_content [ pos_so_far : match_start_pos ] text_content = match . group ( 'text_content' ) sub_pat = re . compile ( '^#' , flags = re . M ) text_block_content = dedent ( re . sub ( sub_pat , '' , text_content ) ) if code_block_content . strip ( ) : blocks . append ( ( 'code' , code_block_content ) ) if text_block_content . strip ( ) : blocks . append ( ( 'text' , text_block_content ) ) pos_so_far = match_end_pos remaining_content = rest_of_content [ pos_so_far : ] if remaining_content . strip ( ) : blocks . append ( ( 'code' , remaining_content ) ) return blocks
5256	def disassemble_all ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) while True : instr = disassemble_one ( bytecode , pc = pc , fork = fork ) if not instr : return pc += instr . size yield instr
27	def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input
7033	def get_new_apikey ( lcc_server ) : USERHOME = os . path . expanduser ( '~' ) APIKEYFILE = os . path . join ( USERHOME , '.astrobase' , 'lccs' , 'apikey-%s' % lcc_server . replace ( 'https://' , 'https-' ) . replace ( 'http://' , 'http-' ) ) url = '%s/api/key' % lcc_server resp = urlopen ( url ) if resp . code == 200 : respdict = json . loads ( resp . read ( ) ) else : LOGERROR ( 'could not fetch the API key from LCC-Server at: %s' % lcc_server ) LOGERROR ( 'the HTTP status code was: %s' % resp . status_code ) return None apikey = respdict [ 'result' ] [ 'apikey' ] expires = respdict [ 'result' ] [ 'expires' ] if not os . path . exists ( os . path . dirname ( APIKEYFILE ) ) : os . makedirs ( os . path . dirname ( APIKEYFILE ) ) with open ( APIKEYFILE , 'w' ) as outfd : outfd . write ( '%s %s\n' % ( apikey , expires ) ) os . chmod ( APIKEYFILE , 0o100600 ) LOGINFO ( 'key fetched successfully from: %s. expires on: %s' % ( lcc_server , expires ) ) LOGINFO ( 'written to: %s' % APIKEYFILE ) return apikey , expires
2751	def get_images ( self , private = False , type = None ) : params = { } if private : params [ 'private' ] = 'true' if type : params [ 'type' ] = type data = self . get_data ( "images/" , params = params ) images = list ( ) for jsoned in data [ 'images' ] : image = Image ( ** jsoned ) image . token = self . token images . append ( image ) return images
1920	def decree ( cls , path , concrete_start = '' , ** kwargs ) : try : return cls ( _make_decree ( path , concrete_start ) , ** kwargs ) except KeyError : raise Exception ( f'Invalid binary: {path}' )
2965	def _sm_start ( self , * args , ** kwargs ) : millisec = random . randint ( self . _start_min_delay , self . _start_max_delay ) self . _timer = threading . Timer ( millisec / 1000.0 , self . event_timeout ) self . _timer . start ( )
12196	def get_tasks ( ) : task_classes = [ ] for task_path in TASKS : try : module , classname = task_path . rsplit ( '.' , 1 ) except ValueError : raise ImproperlyConfigured ( '%s isn\'t a task module' % task_path ) try : mod = import_module ( module ) except ImportError as e : raise ImproperlyConfigured ( 'Error importing task %s: "%s"' % ( module , e ) ) try : task_class = getattr ( mod , classname ) except AttributeError : raise ImproperlyConfigured ( 'Task module "%s" does not define a ' '"%s" class' % ( module , classname ) ) task_classes . append ( task_class ) return task_classes
12354	def change_kernel ( self , kernel_id , wait = True ) : return self . _action ( 'change_kernel' , kernel = kernel_id , wait = wait )
13009	def format ( ) : argparser = argparse . ArgumentParser ( description = 'Formats a json object in a certain way. Use with pipes.' ) argparser . add_argument ( 'format' , metavar = 'format' , help = 'How to format the json for example "{address}:{port}".' , nargs = '?' ) arguments = argparser . parse_args ( ) service_style = "{address:15} {port:7} {protocol:5} {service:15} {state:10} {banner} {tags}" host_style = "{address:15} {tags}" ranges_style = "{range:18} {tags}" users_style = "{username}" if arguments . format : format_input ( arguments . format ) else : doc_mapper = DocMapper ( ) if doc_mapper . is_pipe : for obj in doc_mapper . get_pipe ( ) : style = '' if isinstance ( obj , Range ) : style = ranges_style elif isinstance ( obj , Host ) : style = host_style elif isinstance ( obj , Service ) : style = service_style elif isinstance ( obj , User ) : style = users_style print_line ( fmt . format ( style , ** obj . to_dict ( include_meta = True ) ) ) else : print_error ( "Please use this script with pipes" )
3139	def create ( self , store_id , data ) : self . store_id = store_id if 'id' not in data : raise KeyError ( 'The promo rule must have an id' ) if 'description' not in data : raise KeyError ( 'This promo rule must have a description' ) if 'amount' not in data : raise KeyError ( 'This promo rule must have an amount' ) if 'target' not in data : raise KeyError ( 'This promo rule must apply to a target (example per_item, total, or shipping' ) response = self . _mc_client . _post ( url = self . _build_path ( store_id , 'promo-rules' ) , data = data ) if response is not None : return response
11613	def report_depths ( self , filename , tpm = True , grp_wise = False , reorder = 'as-is' , notes = None ) : if grp_wise : lname = self . probability . gname depths = self . allelic_expression * self . grp_conv_mat else : lname = self . probability . lname depths = self . allelic_expression if tpm : depths *= ( 1000000.0 / depths . sum ( ) ) total_depths = depths . sum ( axis = 0 ) if reorder == 'decreasing' : report_order = np . argsort ( total_depths . flatten ( ) ) report_order = report_order [ : : - 1 ] elif reorder == 'increasing' : report_order = np . argsort ( total_depths . flatten ( ) ) elif reorder == 'as-is' : report_order = np . arange ( len ( lname ) ) cntdata = np . vstack ( ( depths , total_depths ) ) fhout = open ( filename , 'w' ) fhout . write ( "locus\t" + "\t" . join ( self . probability . hname ) + "\ttotal" ) if notes is not None : fhout . write ( "\tnotes" ) fhout . write ( "\n" ) for locus_id in report_order : lname_cur = lname [ locus_id ] fhout . write ( "\t" . join ( [ lname_cur ] + map ( str , cntdata [ : , locus_id ] . ravel ( ) ) ) ) if notes is not None : fhout . write ( "\t%s" % notes [ lname_cur ] ) fhout . write ( "\n" ) fhout . close ( )
4744	def dev_get_rprt ( dev_name , pugrp = None , punit = None ) : cmd = [ "nvm_cmd" , "rprt_all" , dev_name ] if not ( pugrp is None and punit is None ) : cmd = [ "nvm_cmd" , "rprt_lun" , dev_name , str ( pugrp ) , str ( punit ) ] _ , _ , _ , struct = cij . test . command_to_struct ( cmd ) if not struct : return None return struct [ "rprt_descr" ]
3740	def omega ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ 'LK' , 'DEFINITION' ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) : methods . append ( 'PSRK' ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) : methods . append ( 'PD' ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'omega' ] ) : methods . append ( 'YAWS' ) Tcrit , Pcrit = Tc ( CASRN ) , Pc ( CASRN ) if Tcrit and Pcrit : if Tb ( CASRN ) : methods . append ( 'LK' ) if VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tcrit * 0.7 ) : methods . append ( 'DEFINITION' ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( 'NONE' ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == 'PSRK' : _omega = float ( _crit_PSRKR4 . at [ CASRN , 'omega' ] ) elif Method == 'PD' : _omega = float ( _crit_PassutDanner . at [ CASRN , 'omega' ] ) elif Method == 'YAWS' : _omega = float ( _crit_Yaws . at [ CASRN , 'omega' ] ) elif Method == 'LK' : _omega = LK_omega ( Tb ( CASRN ) , Tc ( CASRN ) , Pc ( CASRN ) ) elif Method == 'DEFINITION' : P = VaporPressure ( CASRN = CASRN ) . T_dependent_property ( Tc ( CASRN ) * 0.7 ) _omega = - log10 ( P / Pc ( CASRN ) ) - 1.0 elif Method == 'NONE' : _omega = None else : raise Exception ( 'Failure in in function' ) return _omega
12008	def _generate_key ( pass_id , passphrases , salt , algorithm ) : if pass_id not in passphrases : raise Exception ( 'Passphrase not defined for id: %d' % pass_id ) passphrase = passphrases [ pass_id ] if len ( passphrase ) < 32 : raise Exception ( 'Passphrase less than 32 characters long' ) digestmod = EncryptedPickle . _get_hashlib ( algorithm [ 'pbkdf2_algorithm' ] ) encoder = PBKDF2 ( passphrase , salt , iterations = algorithm [ 'pbkdf2_iterations' ] , digestmodule = digestmod ) return encoder . read ( algorithm [ 'key_size' ] )
6579	def _send_cmd ( self , cmd ) : self . _process . stdin . write ( "{}\n" . format ( cmd ) . encode ( "utf-8" ) ) self . _process . stdin . flush ( )
13123	def argparser ( self ) : core_parser = self . core_parser core_parser . add_argument ( '-r' , '--range' , type = str , help = "The range to search for use" ) return core_parser
3708	def COSTALD_mixture ( xs , T , Tcs , Vcs , omegas ) : r cmps = range ( len ( xs ) ) if not none_and_length_check ( [ xs , Tcs , Vcs , omegas ] ) : raise Exception ( 'Function inputs are incorrect format' ) sum1 = sum ( [ xi * Vci for xi , Vci in zip ( xs , Vcs ) ] ) sum2 = sum ( [ xi * Vci ** ( 2 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) sum3 = sum ( [ xi * Vci ** ( 1 / 3. ) for xi , Vci in zip ( xs , Vcs ) ] ) Vm = 0.25 * ( sum1 + 3. * sum2 * sum3 ) VijTcij = [ [ ( Tcs [ i ] * Tcs [ j ] * Vcs [ i ] * Vcs [ j ] ) ** 0.5 for j in cmps ] for i in cmps ] omega = mixing_simple ( xs , omegas ) Tcm = sum ( [ xs [ i ] * xs [ j ] * VijTcij [ i ] [ j ] / Vm for j in cmps for i in cmps ] ) return COSTALD ( T , Tcm , Vm , omega )
5355	def convert_from_eclipse ( self , eclipse_projects ) : projects = { } projects [ 'unknown' ] = { "gerrit" : [ "git.eclipse.org" ] , "bugzilla" : [ "https://bugs.eclipse.org/bugs/" ] } projects = compose_title ( projects , eclipse_projects ) projects = compose_projects_json ( projects , eclipse_projects ) return projects
12263	def _load_meta ( self , size , md5 ) : if not hasattr ( self , 'local_hashes' ) : self . local_hashes = { } self . size = int ( size ) if ( re . match ( '^[a-fA-F0-9]{32}$' , md5 ) ) : self . md5 = md5
1842	def JNP ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . PF , target . read ( ) , cpu . PC )
5642	def compute_pseudo_connections ( transit_connections , start_time_dep , end_time_dep , transfer_margin , walk_network , walk_speed ) : pseudo_connection_set = set ( ) for c in transit_connections : if start_time_dep <= c . departure_time <= end_time_dep : walk_arr_stop = c . departure_stop walk_arr_time = c . departure_time - transfer_margin for _ , walk_dep_stop , data in walk_network . edges ( nbunch = [ walk_arr_stop ] , data = True ) : walk_dep_time = walk_arr_time - data [ 'd_walk' ] / float ( walk_speed ) if walk_dep_time > end_time_dep or walk_dep_time < start_time_dep : continue pseudo_connection = Connection ( walk_dep_stop , walk_arr_stop , walk_dep_time , walk_arr_time , Connection . WALK_TRIP_ID , Connection . WALK_SEQ , is_walk = True ) pseudo_connection_set . add ( pseudo_connection ) return pseudo_connection_set
10299	def group_errors ( graph : BELGraph ) -> Mapping [ str , List [ int ] ] : warning_summary = defaultdict ( list ) for _ , exc , _ in graph . warnings : warning_summary [ str ( exc ) ] . append ( exc . line_number ) return dict ( warning_summary )
12665	def apply_mask ( image , mask_img ) : img = check_img ( image ) mask = check_img ( mask_img ) check_img_compatibility ( img , mask ) vol = img . get_data ( ) mask_data , _ = load_mask_data ( mask ) return vol [ mask_data ] , mask_data
8944	def pushd ( path ) : saved = os . getcwd ( ) os . chdir ( path ) try : yield saved finally : os . chdir ( saved )
3830	async def rename_conversation ( self , rename_conversation_request ) : response = hangouts_pb2 . RenameConversationResponse ( ) await self . _pb_request ( 'conversations/renameconversation' , rename_conversation_request , response ) return response
8780	def delete_locks ( context , network_ids , addresses ) : addresses_no_longer_null_routed = _find_addresses_to_be_unlocked ( context , network_ids , addresses ) LOG . info ( "Deleting %s lock holders on IPAddress with ids: %s" , len ( addresses_no_longer_null_routed ) , [ addr . id for addr in addresses_no_longer_null_routed ] ) for address in addresses_no_longer_null_routed : lock_holder = None try : lock_holder = db_api . lock_holder_find ( context , lock_id = address . lock_id , name = LOCK_NAME , scope = db_api . ONE ) if lock_holder : db_api . lock_holder_delete ( context , address , lock_holder ) except Exception : LOG . exception ( "Failed to delete lock holder %s" , lock_holder ) continue context . session . flush ( )
5782	def select_read ( self , timeout = None ) : if len ( self . _decrypted_bytes ) > 0 : return True read_ready , _ , _ = select . select ( [ self . _socket ] , [ ] , [ ] , timeout ) return len ( read_ready ) > 0
7539	def get_binom ( base1 , base2 , estE , estH ) : prior_homo = ( 1. - estH ) / 2. prior_hete = estH bsum = base1 + base2 hetprob = scipy . misc . comb ( bsum , base1 ) / ( 2. ** ( bsum ) ) homoa = scipy . stats . binom . pmf ( base2 , bsum , estE ) homob = scipy . stats . binom . pmf ( base1 , bsum , estE ) hetprob *= prior_hete homoa *= prior_homo homob *= prior_homo probabilities = [ homoa , homob , hetprob ] bestprob = max ( probabilities ) / float ( sum ( probabilities ) ) if hetprob > homoa : return True , bestprob else : return False , bestprob
12464	def print_message ( message = None ) : kwargs = { 'stdout' : sys . stdout , 'stderr' : sys . stderr , 'shell' : True } return subprocess . call ( 'echo "{0}"' . format ( message or '' ) , ** kwargs )
7537	def branch_assembly ( args , parsedict ) : data = getassembly ( args , parsedict ) bargs = args . branch newname = bargs [ 0 ] if newname . endswith ( ".txt" ) : newname = newname [ : - 4 ] if len ( bargs ) > 1 : if any ( [ x . stats . state == 6 for x in data . samples . values ( ) ] ) : pass subsamples = bargs [ 1 : ] if bargs [ 1 ] == "-" : fails = [ i for i in subsamples [ 1 : ] if i not in data . samples . keys ( ) ] if any ( fails ) : raise IPyradWarningExit ( "\ \n Failed: unrecognized names requested, check spelling:\n {}" . format ( "\n " . join ( [ i for i in fails ] ) ) ) print ( " dropping {} samples" . format ( len ( subsamples ) - 1 ) ) subsamples = list ( set ( data . samples . keys ( ) ) - set ( subsamples ) ) if os . path . exists ( bargs [ 1 ] ) : new_data = data . branch ( newname , infile = bargs [ 1 ] ) else : new_data = data . branch ( newname , subsamples ) else : new_data = data . branch ( newname , None ) print ( " creating a new branch called '{}' with {} Samples" . format ( new_data . name , len ( new_data . samples ) ) ) print ( " writing new params file to {}" . format ( "params-" + new_data . name + ".txt\n" ) ) new_data . write_params ( "params-" + new_data . name + ".txt" , force = args . force )
11993	def set_encryption_passphrases ( self , encryption_passphrases ) : self . encryption_passphrases = self . _update_dict ( encryption_passphrases , { } , replace_data = True )
1853	def SHR ( cpu , dest , src ) : OperandSize = dest . size count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) value = dest . read ( ) res = dest . write ( value >> count ) MASK = ( 1 << OperandSize ) - 1 SIGN_MASK = 1 << ( OperandSize - 1 ) if issymbolic ( count ) : cpu . CF = Operators . ITE ( count != 0 , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) else : if count != 0 : cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF )
4884	def handle_transmission_error ( self , learner_data , request_exception ) : try : sys_msg = request_exception . response . content except AttributeError : pass else : if 'user account is inactive' in sys_msg : ecu = EnterpriseCustomerUser . objects . get ( enterprise_enrollments__id = learner_data . enterprise_course_enrollment_id ) ecu . active = False ecu . save ( ) LOGGER . warning ( 'User %s with ID %s and email %s is a former employee of %s ' 'and has been marked inactive in SAPSF. Now marking inactive internally.' , ecu . username , ecu . user_id , ecu . user_email , ecu . enterprise_customer ) return super ( SapSuccessFactorsLearnerTransmitter , self ) . handle_transmission_error ( learner_data , request_exception )
4462	def transpose ( label , n_semitones ) : match = re . match ( six . text_type ( '(?P<note>[A-G][b#]*)(?P<mod>.*)' ) , six . text_type ( label ) ) if not match : return label note = match . group ( 'note' ) new_note = librosa . midi_to_note ( librosa . note_to_midi ( note ) + n_semitones , octave = False ) return new_note + match . group ( 'mod' )
1915	def put ( self , state_id ) : self . _states . append ( state_id ) self . _lock . notify_all ( ) return state_id
7689	def piano_roll ( annotation , sr = 22050 , length = None , ** kwargs ) : intervals , pitches = annotation . to_interval_values ( ) pitch_map = { f : idx for idx , f in enumerate ( np . unique ( pitches ) ) } gram = np . zeros ( ( len ( pitch_map ) , len ( intervals ) ) ) for col , f in enumerate ( pitches ) : gram [ pitch_map [ f ] , col ] = 1 return filter_kwargs ( mir_eval . sonify . time_frequency , gram , pitches , intervals , sr , length = length , ** kwargs )
6746	def topological_sort ( source ) : if isinstance ( source , dict ) : source = source . items ( ) pending = sorted ( [ ( name , set ( deps ) ) for name , deps in source ] ) emitted = [ ] while pending : next_pending = [ ] next_emitted = [ ] for entry in pending : name , deps = entry deps . difference_update ( emitted ) if deps : next_pending . append ( entry ) else : yield name emitted . append ( name ) next_emitted . append ( name ) if not next_emitted : raise ValueError ( "cyclic or missing dependancy detected: %r" % ( next_pending , ) ) pending = next_pending emitted = next_emitted
2282	def launch_R_script ( template , arguments , output_function = None , verbose = True , debug = False ) : id = str ( uuid . uuid4 ( ) ) os . makedirs ( '/tmp/cdt_R_script_' + id + '/' ) try : scriptpath = '/tmp/cdt_R_script_' + id + '/instance_{}' . format ( os . path . basename ( template ) ) copy ( template , scriptpath ) with fileinput . FileInput ( scriptpath , inplace = True ) as file : for line in file : mline = line for elt in arguments : mline = mline . replace ( elt , arguments [ elt ] ) print ( mline , end = '' ) if output_function is None : output = subprocess . call ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) else : if verbose : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True ) else : process = subprocess . Popen ( "Rscript --vanilla {}" . format ( scriptpath ) , shell = True , stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) process . wait ( ) output = output_function ( ) except Exception as e : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise e except KeyboardInterrupt : if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) raise KeyboardInterrupt if not debug : rmtree ( '/tmp/cdt_R_script_' + id + '/' ) return output
10378	def calculate_concordance_probability ( graph : BELGraph , key : str , cutoff : Optional [ float ] = None , permutations : Optional [ int ] = None , percentage : Optional [ float ] = None , use_ambiguous : bool = False , permute_type : str = 'shuffle_node_data' , ) -> Tuple [ float , List [ float ] , float ] : if permute_type == 'random_by_edges' : permute_func = partial ( random_by_edges , percentage = percentage ) elif permute_type == 'shuffle_node_data' : permute_func = partial ( shuffle_node_data , key = key , percentage = percentage ) elif permute_type == 'shuffle_relations' : permute_func = partial ( shuffle_relations , percentage = percentage ) else : raise ValueError ( 'Invalid permute_type: {}' . format ( permute_type ) ) graph : BELGraph = graph . copy ( ) collapse_to_genes ( graph ) collapse_all_variants ( graph ) score = calculate_concordance ( graph , key , cutoff = cutoff ) distribution = [ ] for _ in range ( permutations or 500 ) : permuted_graph = permute_func ( graph ) permuted_graph_scores = calculate_concordance ( permuted_graph , key , cutoff = cutoff , use_ambiguous = use_ambiguous ) distribution . append ( permuted_graph_scores ) return score , distribution , one_sided ( score , distribution )
10624	def _calculate_Hfr ( self , T ) : if self . isCoal : return self . _calculate_Hfr_coal ( T ) Hfr = 0.0 for compound in self . material . compounds : index = self . material . get_compound_index ( compound ) dHfr = thermo . H ( compound , T , self . _compound_mfrs [ index ] ) Hfr = Hfr + dHfr return Hfr
6102	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . multiply ( self . intensity_prime , np . power ( np . add ( 1 , np . power ( np . divide ( self . radius_break , grid_radii ) , self . alpha ) ) , ( self . gamma / self . alpha ) ) ) , np . exp ( np . multiply ( - self . sersic_constant , ( np . power ( np . divide ( np . add ( np . power ( grid_radii , self . alpha ) , ( self . radius_break ** self . alpha ) ) , ( self . effective_radius ** self . alpha ) ) , ( 1.0 / ( self . alpha * self . sersic_index ) ) ) ) ) ) )
8032	def pruneUI ( dupeList , mainPos = 1 , mainLen = 1 ) : dupeList = sorted ( dupeList ) print for pos , val in enumerate ( dupeList ) : print "%d) %s" % ( pos + 1 , val ) while True : choice = raw_input ( "[%s/%s] Keepers: " % ( mainPos , mainLen ) ) . strip ( ) if not choice : print ( "Please enter a space/comma-separated list of numbers or " "'all'." ) continue elif choice . lower ( ) == 'all' : return [ ] try : out = [ int ( x ) - 1 for x in choice . replace ( ',' , ' ' ) . split ( ) ] return [ val for pos , val in enumerate ( dupeList ) if pos not in out ] except ValueError : print ( "Invalid choice. Please enter a space/comma-separated list" "of numbers or 'all'." )
12221	def dispatch ( self , func ) : self . callees . append ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
6537	def mod_sys_path ( paths ) : old_path = sys . path sys . path = paths + sys . path try : yield finally : sys . path = old_path
11811	def score ( self , word , docid ) : "Compute a score for this word on this docid." return ( math . log ( 1 + self . index [ word ] [ docid ] ) / math . log ( 1 + self . documents [ docid ] . nwords ) )
12285	def add ( self , repo ) : key = self . key ( repo . username , repo . reponame ) repo . key = key self . repos [ key ] = repo return key
10587	def get_account_descendants ( self , account ) : result = [ ] for child in account . accounts : self . _get_account_and_descendants_ ( child , result ) return result
5735	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-{}-worker' . format ( queue . PUBSUB_OBJECT_PREFIX , self . name , uuid4 ( ) . hex ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating worker subscription {}" . format ( subscription_name ) ) self . subscriber_client . create_subscription ( subscription_path , topic_path ) return subscription_path
4046	def num_tagitems ( self , tag ) : query = "/{t}/{u}/tags/{ta}/items" . format ( u = self . library_id , t = self . library_type , ta = tag ) return self . _totals ( query )
2935	def parse_node ( self , node ) : if node . get ( 'id' ) in self . parsed_nodes : return self . parsed_nodes [ node . get ( 'id' ) ] ( node_parser , spec_class ) = self . parser . _get_parser_class ( node . tag ) if not node_parser or not spec_class : raise ValidationException ( "There is no support implemented for this task type." , node = node , filename = self . filename ) np = node_parser ( self , spec_class , node ) task_spec = np . parse_node ( ) return task_spec
10257	def get_causal_sink_nodes ( graph : BELGraph , func ) -> Set [ BaseEntity ] : return { node for node in graph if node . function == func and is_causal_sink ( graph , node ) }
9291	def db_value ( self , value ) : if not isinstance ( value , UUID ) : value = UUID ( value ) parts = str ( value ) . split ( "-" ) reordered = '' . join ( [ parts [ 2 ] , parts [ 1 ] , parts [ 0 ] , parts [ 3 ] , parts [ 4 ] ] ) value = binascii . unhexlify ( reordered ) return super ( OrderedUUIDField , self ) . db_value ( value )
11318	def update_isbn ( self ) : isbns = record_get_field_instances ( self . record , '020' ) for field in isbns : for idx , ( key , value ) in enumerate ( field [ 0 ] ) : if key == 'a' : field [ 0 ] [ idx ] = ( 'a' , value . replace ( "-" , "" ) . strip ( ) )
6733	def str_to_list ( s ) : if s is None : return [ ] elif isinstance ( s , ( tuple , list ) ) : return s elif not isinstance ( s , six . string_types ) : raise NotImplementedError ( 'Unknown type: %s' % type ( s ) ) return [ _ . strip ( ) . lower ( ) for _ in ( s or '' ) . split ( ',' ) if _ . strip ( ) ]
10408	def bond_reduce ( row_a , row_b ) : spanning_cluster = ( 'percolation_probability_mean' in row_a . dtype . names and 'percolation_probability_mean' in row_b . dtype . names and 'percolation_probability_m2' in row_a . dtype . names and 'percolation_probability_m2' in row_b . dtype . names ) ret = np . empty_like ( row_a ) def _reducer ( key , transpose = False ) : mean_key = '{}_mean' . format ( key ) m2_key = '{}_m2' . format ( key ) res = simoa . stats . online_variance ( * [ ( row [ 'number_of_runs' ] , row [ mean_key ] . T if transpose else row [ mean_key ] , row [ m2_key ] . T if transpose else row [ m2_key ] , ) for row in [ row_a , row_b ] ] ) ( ret [ mean_key ] , ret [ m2_key ] , ) = ( res [ 1 ] . T , res [ 2 ] . T , ) if transpose else res [ 1 : ] if spanning_cluster : _reducer ( 'percolation_probability' ) _reducer ( 'max_cluster_size' ) _reducer ( 'moments' , transpose = True ) ret [ 'number_of_runs' ] = row_a [ 'number_of_runs' ] + row_b [ 'number_of_runs' ] return ret
12639	def move_to_folder ( self , folder_path , groupby_field_name = None ) : try : copy_groups_to_folder ( self . dicom_groups , folder_path , groupby_field_name ) except IOError as ioe : raise IOError ( 'Error moving dicom groups to {}.' . format ( folder_path ) ) from ioe
8532	def can_diff ( msg_a , msg_b ) : if msg_a . method != msg_b . method : return False , 'method name of messages do not match' if len ( msg_a . args ) != len ( msg_b . args ) or not msg_a . args . is_isomorphic_to ( msg_b . args ) : return False , 'argument signature of methods do not match' return True , None
5550	def get_zoom_levels ( process_zoom_levels = None , init_zoom_levels = None ) : process_zoom_levels = _validate_zooms ( process_zoom_levels ) if init_zoom_levels is None : return process_zoom_levels else : init_zoom_levels = _validate_zooms ( init_zoom_levels ) if not set ( init_zoom_levels ) . issubset ( set ( process_zoom_levels ) ) : raise MapcheteConfigError ( "init zooms must be a subset of process zoom" ) return init_zoom_levels
7944	def _connected ( self ) : self . _auth_properties [ 'remote-ip' ] = self . _dst_addr [ 0 ] if self . _dst_service : self . _auth_properties [ 'service-domain' ] = self . _dst_name if self . _dst_hostname is not None : self . _auth_properties [ 'service-hostname' ] = self . _dst_hostname else : self . _auth_properties [ 'service-hostname' ] = self . _dst_addr [ 0 ] self . _auth_properties [ 'security-layer' ] = None self . event ( ConnectedEvent ( self . _dst_addr ) ) self . _set_state ( "connected" ) self . _stream . transport_connected ( )
10226	def get_correlation_triangles ( graph : BELGraph ) -> SetOfNodeTriples : return { tuple ( sorted ( [ n , u , v ] , key = str ) ) for n in graph for u , v in itt . combinations ( graph [ n ] , 2 ) if graph . has_edge ( u , v ) }
1629	def GetHeaderGuardCPPVariable ( filename ) : filename = re . sub ( r'_flymake\.h$' , '.h' , filename ) filename = re . sub ( r'/\.flymake/([^/]*)$' , r'/\1' , filename ) filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) fileinfo = FileInfo ( filename ) file_path_from_root = fileinfo . RepositoryName ( ) if _root : suffix = os . sep if suffix == '\\' : suffix += '\\' file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_'
10613	def H ( self , H ) : self . _H = H self . _T = self . _calculate_T ( H )
8688	def delete ( self , key_name ) : self . db . remove ( Query ( ) . name == key_name ) return self . get ( key_name ) == { }
3788	def set_user_method ( self , user_methods , forced = False ) : r if isinstance ( user_methods , str ) : user_methods = [ user_methods ] self . user_methods = user_methods self . forced = forced if set ( self . user_methods ) . difference ( self . all_methods ) : raise Exception ( "One of the given methods is not available for this mixture" ) if not self . user_methods and self . forced : raise Exception ( 'Only user specified methods are considered when forced is True, but no methods were provided' ) self . method = None self . sorted_valid_methods = [ ] self . TP_zs_ws_cached = ( None , None , None , None )
3022	def _detect_gce_environment ( ) : http = transport . get_http_object ( timeout = GCE_METADATA_TIMEOUT ) try : response , _ = transport . request ( http , _GCE_METADATA_URI , headers = _GCE_HEADERS ) return ( response . status == http_client . OK and response . get ( _METADATA_FLAVOR_HEADER ) == _DESIRED_METADATA_FLAVOR ) except socket . error : logger . info ( 'Timeout attempting to reach GCE metadata service.' ) return False
10592	def get_path_relative_to_module ( module_file_path , relative_target_path ) : module_path = os . path . dirname ( module_file_path ) path = os . path . join ( module_path , relative_target_path ) path = os . path . abspath ( path ) return path
11023	def gen_key ( self , key ) : b_key = self . _hash_digest ( key ) return self . _hash_val ( b_key , lambda x : x )
2417	def write_file ( spdx_file , out ) : out . write ( '# File\n\n' ) write_value ( 'FileName' , spdx_file . name , out ) write_value ( 'SPDXID' , spdx_file . spdx_id , out ) if spdx_file . has_optional_field ( 'type' ) : write_file_type ( spdx_file . type , out ) write_value ( 'FileChecksum' , spdx_file . chk_sum . to_tv ( ) , out ) if isinstance ( spdx_file . conc_lics , ( document . LicenseConjunction , document . LicenseDisjunction ) ) : write_value ( 'LicenseConcluded' , u'({0})' . format ( spdx_file . conc_lics ) , out ) else : write_value ( 'LicenseConcluded' , spdx_file . conc_lics , out ) for lics in sorted ( spdx_file . licenses_in_file ) : write_value ( 'LicenseInfoInFile' , lics , out ) if isinstance ( spdx_file . copyright , six . string_types ) : write_text_value ( 'FileCopyrightText' , spdx_file . copyright , out ) else : write_value ( 'FileCopyrightText' , spdx_file . copyright , out ) if spdx_file . has_optional_field ( 'license_comment' ) : write_text_value ( 'LicenseComments' , spdx_file . license_comment , out ) if spdx_file . has_optional_field ( 'comment' ) : write_text_value ( 'FileComment' , spdx_file . comment , out ) if spdx_file . has_optional_field ( 'notice' ) : write_text_value ( 'FileNotice' , spdx_file . notice , out ) for contributor in sorted ( spdx_file . contributors ) : write_value ( 'FileContributor' , contributor , out ) for dependency in sorted ( spdx_file . dependencies ) : write_value ( 'FileDependency' , dependency , out ) names = spdx_file . artifact_of_project_name homepages = spdx_file . artifact_of_project_home uris = spdx_file . artifact_of_project_uri for name , homepage , uri in sorted ( zip_longest ( names , homepages , uris ) ) : write_value ( 'ArtifactOfProjectName' , name , out ) if homepage is not None : write_value ( 'ArtifactOfProjectHomePage' , homepage , out ) if uri is not None : write_value ( 'ArtifactOfProjectURI' , uri , out )
13535	def prune ( self ) : targets = self . descendents_root ( ) try : targets . remove ( self . graph . root ) except ValueError : pass results = [ n . data for n in targets ] results . append ( self . data ) for node in targets : node . delete ( ) for parent in self . parents . all ( ) : parent . children . remove ( self ) self . delete ( ) return results
201	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap
2578	def submit ( self , func , * args , executors = 'all' , fn_hash = None , cache = False , ** kwargs ) : if self . cleanup_called : raise ValueError ( "Cannot submit to a DFK that has been cleaned up" ) task_id = self . task_count self . task_count += 1 if isinstance ( executors , str ) and executors . lower ( ) == 'all' : choices = list ( e for e in self . executors if e != 'data_manager' ) elif isinstance ( executors , list ) : choices = executors executor = random . choice ( choices ) args , kwargs = self . _add_input_deps ( executor , args , kwargs ) task_def = { 'depends' : None , 'executor' : executor , 'func' : func , 'func_name' : func . __name__ , 'args' : args , 'kwargs' : kwargs , 'fn_hash' : fn_hash , 'memoize' : cache , 'callback' : None , 'exec_fu' : None , 'checkpoint' : None , 'fail_count' : 0 , 'fail_history' : [ ] , 'env' : None , 'status' : States . unsched , 'id' : task_id , 'time_submitted' : None , 'time_returned' : None , 'app_fu' : None } if task_id in self . tasks : raise DuplicateTaskError ( "internal consistency error: Task {0} already exists in task list" . format ( task_id ) ) else : self . tasks [ task_id ] = task_def dep_cnt , depends = self . _gather_all_deps ( args , kwargs ) self . tasks [ task_id ] [ 'depends' ] = depends task_stdout = kwargs . get ( 'stdout' ) task_stderr = kwargs . get ( 'stderr' ) logger . info ( "Task {} submitted for App {}, waiting on tasks {}" . format ( task_id , task_def [ 'func_name' ] , [ fu . tid for fu in depends ] ) ) self . tasks [ task_id ] [ 'task_launch_lock' ] = threading . Lock ( ) app_fu = AppFuture ( tid = task_id , stdout = task_stdout , stderr = task_stderr ) self . tasks [ task_id ] [ 'app_fu' ] = app_fu app_fu . add_done_callback ( partial ( self . handle_app_update , task_id ) ) self . tasks [ task_id ] [ 'status' ] = States . pending logger . debug ( "Task {} set to pending state with AppFuture: {}" . format ( task_id , task_def [ 'app_fu' ] ) ) for d in depends : def callback_adapter ( dep_fut ) : self . launch_if_ready ( task_id ) try : d . add_done_callback ( callback_adapter ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) ) self . launch_if_ready ( task_id ) return task_def [ 'app_fu' ]
7596	def search_tournaments ( self , ** params : keys ) : url = self . api . TOURNAMENT + '/search' return self . _get_model ( url , PartialClan , ** params )
11399	def update_keywords ( self ) : for field in record_get_field_instances ( self . record , '653' , ind1 = '1' ) : subs = field_get_subfields ( field ) new_subs = [ ] if 'a' in subs : for val in subs [ 'a' ] : new_subs . extend ( [ ( '9' , 'author' ) , ( 'a' , val ) ] ) new_field = create_field ( subfields = new_subs , ind1 = '1' ) record_replace_field ( self . record , '653' , new_field , field_position_global = field [ 4 ] )
7309	def with_tz ( request ) : dt = datetime . now ( ) t = Template ( '{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}' ) c = RequestContext ( request ) response = t . render ( c ) return HttpResponse ( response )
4214	def get_all_keyring ( ) : _load_plugins ( ) viable_classes = KeyringBackend . get_viable_backends ( ) rings = util . suppress_exceptions ( viable_classes , exceptions = TypeError ) return list ( rings )
4483	def create_file ( self , path , fp , force = False , update = False ) : if 'b' not in fp . mode : raise ValueError ( "File has to be opened in binary mode." ) path = norm_remote_path ( path ) directory , fname = os . path . split ( path ) directories = directory . split ( os . path . sep ) parent = self for directory in directories : if directory : parent = parent . create_folder ( directory , exist_ok = True ) url = parent . _new_file_url connection_error = False if file_empty ( fp ) : response = self . _put ( url , params = { 'name' : fname } , data = b'' ) else : try : response = self . _put ( url , params = { 'name' : fname } , data = fp ) except ConnectionError : connection_error = True if connection_error or response . status_code == 409 : if not force and not update : file_size_bytes = get_local_file_size ( fp ) large_file_cutoff = 2 ** 20 if connection_error and file_size_bytes < large_file_cutoff : msg = ( "There was a connection error which might mean {} " + "already exists. Try again with the `--force` flag " + "specified." ) . format ( path ) raise RuntimeError ( msg ) else : raise FileExistsError ( path ) else : for file_ in self . files : if norm_remote_path ( file_ . path ) == path : if not force : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : break fp . seek ( 0 ) file_ . update ( fp ) break else : raise RuntimeError ( "Could not create a new file at " "({}) nor update it." . format ( path ) )
172	def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( "Expected (H,W) or (H,W,1) as image_shape, got %s." % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0
11589	def _rc_rpoplpush ( self , src , dst ) : rpop = self . rpop ( src ) if rpop is not None : self . lpush ( dst , rpop ) return rpop return None
3427	def add_metabolites ( self , metabolite_list ) : if not hasattr ( metabolite_list , '__iter__' ) : metabolite_list = [ metabolite_list ] if len ( metabolite_list ) == 0 : return None metabolite_list = [ x for x in metabolite_list if x . id not in self . metabolites ] bad_ids = [ m for m in metabolite_list if not isinstance ( m . id , string_types ) or len ( m . id ) < 1 ] if len ( bad_ids ) != 0 : raise ValueError ( 'invalid identifiers in {}' . format ( repr ( bad_ids ) ) ) for x in metabolite_list : x . _model = self self . metabolites += metabolite_list to_add = [ ] for met in metabolite_list : if met . id not in self . constraints : constraint = self . problem . Constraint ( Zero , name = met . id , lb = 0 , ub = 0 ) to_add += [ constraint ] self . add_cons_vars ( to_add ) context = get_context ( self ) if context : context ( partial ( self . metabolites . __isub__ , metabolite_list ) ) for x in metabolite_list : context ( partial ( setattr , x , '_model' , None ) )
13236	def to_timezone ( self , dt ) : if timezone . is_aware ( dt ) : return dt . astimezone ( self . timezone ) else : return timezone . make_aware ( dt , self . timezone )
2515	def p_file_contributor ( self , f_term , predicate ) : for _ , _ , contributor in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . add_file_contribution ( self . doc , six . text_type ( contributor ) )
6808	def configure_hdmi ( self ) : r = self . local_renderer r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_force_hotplug' , value = 1 , use_sudo = True , ) r . enable_attr ( filename = '/boot/config.txt' , key = 'hdmi_drive' , value = 2 , use_sudo = True , )
1617	def _ShouldPrintError ( category , confidence , linenum ) : if IsErrorSuppressedByNolint ( category , linenum ) : return False if confidence < _cpplint_state . verbose_level : return False is_filtered = False for one_filter in _Filters ( ) : if one_filter . startswith ( '-' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = True elif one_filter . startswith ( '+' ) : if category . startswith ( one_filter [ 1 : ] ) : is_filtered = False else : assert False if is_filtered : return False return True
7563	def _insert_to_array ( self , chunk , results ) : chunksize = self . _chunksize qrts , invs = results with h5py . File ( self . database . output , 'r+' ) as io5 : io5 [ 'quartets' ] [ chunk : chunk + chunksize ] = qrts if self . params . save_invariants : if self . checkpoint . boots : key = "invariants/boot{}" . format ( self . checkpoint . boots ) io5 [ key ] [ chunk : chunk + chunksize ] = invs else : io5 [ "invariants/boot0" ] [ chunk : chunk + chunksize ] = invs
3999	def copy_to_local ( local_path , remote_name , remote_path , demote = True ) : if not container_path_exists ( remote_name , remote_path ) : raise RuntimeError ( 'ERROR: Path {} does not exist inside container {}.' . format ( remote_path , remote_name ) ) temp_identifier = str ( uuid . uuid1 ( ) ) copy_path_inside_container ( remote_name , remote_path , os . path . join ( constants . CONTAINER_CP_DIR , temp_identifier ) ) vm_path = os . path . join ( vm_cp_path ( remote_name ) , temp_identifier ) is_dir = vm_path_is_directory ( vm_path ) sync_local_path_from_vm ( local_path , vm_path , demote = demote , is_dir = is_dir )
11804	def assign ( self , var , val , assignment ) : "Assign var, and keep track of conflicts." oldval = assignment . get ( var , None ) if val != oldval : if oldval is not None : self . record_conflict ( assignment , var , oldval , - 1 ) self . record_conflict ( assignment , var , val , + 1 ) CSP . assign ( self , var , val , assignment )
9800	def bookmark ( ctx ) : user , project_name , _group = get_project_group_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'group' ) ) try : PolyaxonClient ( ) . experiment_group . bookmark ( user , project_name , _group ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not bookmark group `{}`.' . format ( _group ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Experiments group is bookmarked." )
6454	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) word = word . translate ( self . _accents ) wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'ern' : word = word [ : - 3 ] elif wlen > 3 and word [ - 2 : ] in { 'em' , 'en' , 'er' , 'es' } : word = word [ : - 2 ] elif wlen > 2 and ( word [ - 1 ] == 'e' or ( word [ - 1 ] == 's' and word [ - 2 ] in self . _st_ending ) ) : word = word [ : - 1 ] wlen = len ( word ) - 1 if wlen > 4 and word [ - 3 : ] == 'est' : word = word [ : - 3 ] elif wlen > 3 and ( word [ - 2 : ] in { 'er' , 'en' } or ( word [ - 2 : ] == 'st' and word [ - 3 ] in self . _st_ending ) ) : word = word [ : - 2 ] return word
7591	def run ( self , force = False , ipyclient = None , name_fields = 30 , name_separator = "_" , dry_run = False ) : try : if not os . path . exists ( self . workdir ) : os . makedirs ( self . workdir ) self . _set_vdbconfig_path ( ) if ipyclient : self . _ipcluster [ "pids" ] = { } for eid in ipyclient . ids : engine = ipyclient [ eid ] if not engine . outstanding : pid = engine . apply ( os . getpid ) . get ( ) self . _ipcluster [ "pids" ] [ eid ] = pid self . _submit_jobs ( force = force , ipyclient = ipyclient , name_fields = name_fields , name_separator = name_separator , dry_run = dry_run , ) except IPyradWarningExit as inst : print ( inst ) except KeyboardInterrupt : print ( "keyboard interrupt..." ) except Exception as inst : print ( "Exception in run() - {}" . format ( inst ) ) finally : self . _restore_vdbconfig_path ( ) sradir = os . path . join ( self . workdir , "sra" ) if os . path . exists ( sradir ) and ( not os . listdir ( sradir ) ) : shutil . rmtree ( sradir ) else : try : print ( FAILED_DOWNLOAD . format ( os . listdir ( sradir ) ) ) except OSError as inst : raise IPyradWarningExit ( "Download failed. Exiting." ) for srr in os . listdir ( sradir ) : isrr = srr . split ( "." ) [ 0 ] ipath = os . path . join ( self . workdir , "*_{}*.gz" . format ( isrr ) ) ifile = glob . glob ( ipath ) [ 0 ] if os . path . exists ( ifile ) : os . remove ( ifile ) shutil . rmtree ( sradir ) if ipyclient : try : ipyclient . abort ( ) time . sleep ( 0.5 ) for engine_id , pid in self . _ipcluster [ "pids" ] . items ( ) : if ipyclient . queue_status ( ) [ engine_id ] [ "tasks" ] : os . kill ( pid , 2 ) time . sleep ( 0.1 ) except ipp . NoEnginesRegistered : pass if not ipyclient . outstanding : ipyclient . purge_everything ( ) else : ipyclient . shutdown ( hub = True , block = False ) ipyclient . close ( ) print ( "\nwarning: ipcluster shutdown and must be restarted" )
646	def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors
1778	def OR ( cpu , dest , src ) : res = dest . write ( dest . read ( ) | src . read ( ) ) cpu . _calculate_logic_flags ( dest . size , res )
12376	def assert_operations ( self , * args ) : if not set ( args ) . issubset ( self . allowed_operations ) : raise http . exceptions . Forbidden ( )
4591	def to_triplets ( colors ) : try : colors [ 0 ] [ 0 ] return colors except : pass extra = len ( colors ) % 3 if extra : colors = colors [ : - extra ] return list ( zip ( * [ iter ( colors ) ] * 3 ) )
2998	def marketYesterdayDF ( token = '' , version = '' ) : x = marketYesterday ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . DataFrame ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
2941	def deserialize_workflow_spec ( self , s_state , filename = None ) : dom = minidom . parseString ( s_state ) node = dom . getElementsByTagName ( 'process-definition' ) [ 0 ] name = node . getAttribute ( 'name' ) if name == '' : _exc ( '%s without a name attribute' % node . nodeName ) workflow_spec = specs . WorkflowSpec ( name , filename ) del workflow_spec . task_specs [ 'Start' ] end = specs . Simple ( workflow_spec , 'End' ) , [ ] read_specs = dict ( end = end ) for child_node in node . childNodes : if child_node . nodeType != minidom . Node . ELEMENT_NODE : continue if child_node . nodeName == 'name' : workflow_spec . name = child_node . firstChild . nodeValue elif child_node . nodeName == 'description' : workflow_spec . description = child_node . firstChild . nodeValue elif child_node . nodeName . lower ( ) in _spec_map : self . deserialize_task_spec ( workflow_spec , child_node , read_specs ) else : _exc ( 'Unknown node: %s' % child_node . nodeName ) workflow_spec . start = read_specs [ 'start' ] [ 0 ] for name in read_specs : spec , successors = read_specs [ name ] for condition , successor_name in successors : if successor_name not in read_specs : _exc ( 'Unknown successor: "%s"' % successor_name ) successor , foo = read_specs [ successor_name ] if condition is None : spec . connect ( successor ) else : spec . connect_if ( condition , successor ) return workflow_spec
196	def Clouds ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return meta . SomeOf ( ( 1 , 2 ) , children = [ CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.5 , - 2.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.25 , 0.75 ) , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 2.5 , - 2.0 ) , sparsity = ( 0.8 , 1.0 ) , density_multiplier = ( 0.5 , 1.0 ) ) , CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.5 , 1.0 ) , alpha_size_px_max = ( 64 , 128 ) , alpha_freq_exponent = ( - 2.0 , - 1.0 ) , sparsity = ( 1.0 , 1.4 ) , density_multiplier = ( 0.8 , 1.5 ) ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )
10904	def compare_data_model_residuals ( s , tile , data_vmin = 'calc' , data_vmax = 'calc' , res_vmin = - 0.1 , res_vmax = 0.1 , edgepts = 'calc' , do_imshow = True , data_cmap = plt . cm . bone , res_cmap = plt . cm . RdBu ) : residuals = s . residuals [ tile . slicer ] . squeeze ( ) data = s . data [ tile . slicer ] . squeeze ( ) model = s . model [ tile . slicer ] . squeeze ( ) if data . ndim != 2 : raise ValueError ( 'tile does not give a 2D slice' ) im = np . zeros ( [ data . shape [ 0 ] , data . shape [ 1 ] , 4 ] ) if data_vmin == 'calc' : data_vmin = 0.5 * ( data . min ( ) + model . min ( ) ) if data_vmax == 'calc' : data_vmax = 0.5 * ( data . max ( ) + model . max ( ) ) upper_mask , center_mask , lower_mask = trisect_image ( im . shape , edgepts ) gm = data_cmap ( center_data ( model , data_vmin , data_vmax ) ) dt = data_cmap ( center_data ( data , data_vmin , data_vmax ) ) rs = res_cmap ( center_data ( residuals , res_vmin , res_vmax ) ) for a in range ( 4 ) : im [ : , : , a ] [ upper_mask ] = rs [ : , : , a ] [ upper_mask ] im [ : , : , a ] [ center_mask ] = gm [ : , : , a ] [ center_mask ] im [ : , : , a ] [ lower_mask ] = dt [ : , : , a ] [ lower_mask ] if do_imshow : return plt . imshow ( im ) else : return im
10030	def execute ( helper , config , args ) : old_env_name = args . old_environment new_env_name = args . new_environment out ( "Assuming that {} is the currently active environment..." . format ( old_env_name ) ) out ( "Swapping environment cnames: {} will become active, {} will become inactive." . format ( new_env_name , old_env_name ) ) helper . swap_environment_cnames ( old_env_name , new_env_name ) helper . wait_for_environments ( [ old_env_name , new_env_name ] , status = 'Ready' , include_deleted = False )
10397	def remove_random_edge_until_has_leaves ( self ) -> None : while True : leaves = set ( self . iter_leaves ( ) ) if leaves : return self . remove_random_edge ( )
1559	def component_id ( self ) : if isinstance ( self . _component_id , HeronComponentSpec ) : if self . _component_id . name is None : return "<No name available for HeronComponentSpec yet, uuid: %s>" % self . _component_id . uuid return self . _component_id . name elif isinstance ( self . _component_id , str ) : return self . _component_id else : raise ValueError ( "Component Id for this GlobalStreamId is not properly set: <%s:%s>" % ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) )
5960	def _tcorrel ( self , nstep = 100 , ** kwargs ) : t = self . array [ 0 , : : nstep ] r = gromacs . collections . Collection ( [ numkit . timeseries . tcorrel ( t , Y , nstep = 1 , ** kwargs ) for Y in self . array [ 1 : , : : nstep ] ] ) return r
13769	def get_minifier ( self ) : if self . minifier is None : if not self . has_bundles ( ) : raise Exception ( "Unable to get default minifier, no bundles in build group" ) minifier = self . get_first_bundle ( ) . get_default_minifier ( ) else : minifier = self . minifier if minifier : minifier . init_asset ( self ) return minifier
10990	def finish_state ( st , desc = 'finish-state' , invert = 'guess' ) : for minmass in [ None , 0 ] : for _ in range ( 3 ) : npart , poses = addsub . add_subtract_locally ( st , region_depth = 7 , minmass = minmass , invert = invert ) if npart == 0 : break opt . finish ( st , n_loop = 1 , separate_psf = True , desc = desc , dowarn = False ) opt . burn ( st , mode = 'polish' , desc = desc , n_loop = 2 , dowarn = False ) d = opt . finish ( st , desc = desc , n_loop = 4 , dowarn = False ) if not d [ 'converged' ] : RLOG . warn ( 'Optimization did not converge; consider re-running' )
13683	def get ( self , url , params = { } ) : params . update ( { 'api_key' : self . api_key } ) try : response = requests . get ( self . host + url , params = params ) except RequestException as e : response = e . args return self . json_parse ( response . content )
3712	def calculate_P ( self , T , P , method ) : r if method == EOS : self . eos [ 0 ] = self . eos [ 0 ] . to_TP ( T = T , P = P ) Vm = self . eos [ 0 ] . V_g elif method == TSONOPOULOS_EXTENDED : B = BVirial_Tsonopoulos_extended ( T , self . Tc , self . Pc , self . omega , dipole = self . dipole ) Vm = ideal_gas ( T , P ) + B elif method == TSONOPOULOS : B = BVirial_Tsonopoulos ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == ABBOTT : B = BVirial_Abbott ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == PITZER_CURL : B = BVirial_Pitzer_Curl ( T , self . Tc , self . Pc , self . omega ) Vm = ideal_gas ( T , P ) + B elif method == CRC_VIRIAL : a1 , a2 , a3 , a4 , a5 = self . CRC_VIRIAL_coeffs t = 298.15 / T - 1. B = ( a1 + a2 * t + a3 * t ** 2 + a4 * t ** 3 + a5 * t ** 4 ) / 1E6 Vm = ideal_gas ( T , P ) + B elif method == IDEAL : Vm = ideal_gas ( T , P ) elif method == COOLPROP : Vm = 1. / PropsSI ( 'DMOLAR' , 'T' , T , 'P' , P , self . CASRN ) elif method in self . tabular_data : Vm = self . interpolate_P ( T , P , method ) return Vm
3914	def _on_typing ( self , typing_message ) : self . _typing_statuses [ typing_message . user_id ] = typing_message . status self . _update ( )
1839	def JNE ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . ZF , target . read ( ) , cpu . PC )
3842	async def sync_recent_conversations ( self , sync_recent_conversations_request ) : response = hangouts_pb2 . SyncRecentConversationsResponse ( ) await self . _pb_request ( 'conversations/syncrecentconversations' , sync_recent_conversations_request , response ) return response
9947	def new_space ( self , name = None , bases = None , formula = None , refs = None ) : space = self . _impl . model . currentspace = self . _impl . new_space ( name = name , bases = get_impls ( bases ) , formula = formula , refs = refs ) return space . interface
9041	def as_instruction ( self , specification ) : instruction = self . _instruction_class ( specification ) type_ = instruction . type if type_ in self . _type_to_instruction : instruction . inherit_from ( self . _type_to_instruction [ type_ ] ) return instruction
11768	def weighted_sampler ( seq , weights ) : "Return a random-sample function that picks from seq weighted by weights." totals = [ ] for w in weights : totals . append ( w + totals [ - 1 ] if totals else w ) return lambda : seq [ bisect . bisect ( totals , random . uniform ( 0 , totals [ - 1 ] ) ) ]
811	def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : if not isinstance ( statsInfo , dict ) : raise RuntimeError ( "statsInfo must be a dict -- " "found '%s' instead" % type ( statsInfo ) ) filename = resource_filename ( "nupic.datafiles" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , "rb" ) ) except : print "Warning: unable to load stats for %s -- " "will regenerate" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print "generateStats: re-generating stats file %s because " "keys %s are not available" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print "Generating statistics for file '%s' with filters '%s'" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters stats = [ ] for field in statsInfo : if statsInfo [ field ] == "number" : statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == "category" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( "Unknown stats type '%s' for field '%s'" % ( statsInfo [ field ] , field ) ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , "wb" ) pickle . dump ( r , f ) f . close ( ) r [ "_filename" ] = statsFilename return r
3330	def acquire_write ( self , timeout = None ) : if timeout is not None : endtime = time ( ) + timeout me , upgradewriter = currentThread ( ) , False self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount += 1 return elif me in self . __readers : if self . __upgradewritercount : raise ValueError ( "Inevitable dead lock, denying write lock" ) upgradewriter = True self . __upgradewritercount = self . __readers . pop ( me ) else : self . __pendingwriters . append ( me ) while True : if not self . __readers and self . __writer is None : if self . __upgradewritercount : if upgradewriter : self . __writer = me self . __writercount = self . __upgradewritercount + 1 self . __upgradewritercount = 0 return elif self . __pendingwriters [ 0 ] is me : self . __writer = me self . __writercount = 1 self . __pendingwriters = self . __pendingwriters [ 1 : ] return if timeout is not None : remaining = endtime - time ( ) if remaining <= 0 : if upgradewriter : self . __readers [ me ] = self . __upgradewritercount self . __upgradewritercount = 0 else : self . __pendingwriters . remove ( me ) raise RuntimeError ( "Acquiring write lock timed out" ) self . __condition . wait ( remaining ) else : self . __condition . wait ( ) finally : self . __condition . release ( )
4838	def get_course_and_course_run ( self , course_run_id ) : course_id = parse_course_key ( course_run_id ) course = self . get_course_details ( course_id ) course_run = None if course : course_run = None course_runs = [ course_run for course_run in course [ 'course_runs' ] if course_run [ 'key' ] == course_run_id ] if course_runs : course_run = course_runs [ 0 ] return course , course_run
12294	def annotate_metadata_platform ( repo ) : print ( "Added platform information" ) package = repo . package mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'instrumentation' , name = 'platform' ) package [ 'platform' ] = repomgr . get_metadata ( )
6219	def interleaves ( self , info ) : return info . byte_offset == self . component_type . size * self . components
12752	def indices_for_joint ( self , name ) : j = 0 for joint in self . joints : if joint . name == name : return list ( range ( j , j + joint . ADOF ) ) j += joint . ADOF return [ ]
110	def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ "matplotlib" , "cv2" ] , "Expected backend 'matplotlib' or 'cv2', got %s." % ( backend , ) ) if backend == "cv2" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = "imgaug-default-window" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( "imgaug.imshow(%s)" % ( image . shape , ) ) ax . imshow ( image , cmap = "gray" ) plt . show ( )
12453	def config_to_args ( config ) : result = [ ] for key , value in iteritems ( config ) : if value is False : continue key = '--{0}' . format ( key . replace ( '_' , '-' ) ) if isinstance ( value , ( list , set , tuple ) ) : for item in value : result . extend ( ( key , smart_str ( item ) ) ) elif value is not True : result . extend ( ( key , smart_str ( value ) ) ) else : result . append ( key ) return tuple ( result )
5945	def convert_aa_code ( x ) : if len ( x ) == 1 : return amino_acid_codes [ x . upper ( ) ] elif len ( x ) == 3 : return inverse_aa_codes [ x . upper ( ) ] else : raise ValueError ( "Can only convert 1-letter or 3-letter amino acid codes, " "not %r" % x )
2776	def remove_droplets ( self , droplet_ids ) : return self . get_data ( "load_balancers/%s/droplets/" % self . id , type = DELETE , params = { "droplet_ids" : droplet_ids } )
1585	def send_buffered_messages ( self ) : while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : tuple_set = self . out_stream . poll ( ) if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : tuple_set . src_task_id = self . my_pplan_helper . my_task_id self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) self . _stmgr_client . send_message ( tuple_set )
4085	def get_common_prefix ( z ) : name_list = z . namelist ( ) if name_list and all ( n . startswith ( name_list [ 0 ] ) for n in name_list [ 1 : ] ) : return name_list [ 0 ] return None
8837	def merge ( * args ) : ret = [ ] for arg in args : if isinstance ( arg , list ) or isinstance ( arg , tuple ) : ret += list ( arg ) else : ret . append ( arg ) return ret
1130	def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )
10964	def trigger_update ( self , params , values ) : if self . _parent : self . _parent . trigger_update ( params , values ) else : self . update ( params , values )
6622	def configure ( self , component , all_dependencies ) : r = { } builddir = self . buildroot available_dependencies = OrderedDict ( ( k , v ) for k , v in all_dependencies . items ( ) if v ) self . set_toplevel_definitions = '' if self . build_info_include_file is None : self . build_info_include_file , build_info_definitions = self . getBuildInfo ( component . path , builddir ) self . set_toplevel_definitions += build_info_definitions if self . config_include_file is None : self . config_include_file , config_definitions , self . config_json_file = self . _getConfigData ( available_dependencies , component , builddir , self . build_info_include_file ) self . set_toplevel_definitions += config_definitions self . configured = True return { 'merged_config_include' : self . config_include_file , 'merged_config_json' : self . config_json_file , 'build_info_include' : self . build_info_include_file }
12292	def annotate_metadata_code ( repo , files ) : package = repo . package package [ 'code' ] = [ ] for p in files : matching_files = glob2 . glob ( "**/{}" . format ( p ) ) for f in matching_files : absf = os . path . abspath ( f ) print ( "Add commit data for {}" . format ( f ) ) package [ 'code' ] . append ( OrderedDict ( [ ( 'script' , f ) , ( 'permalink' , repo . manager . permalink ( repo , absf ) ) , ( 'mimetypes' , mimetypes . guess_type ( absf ) [ 0 ] ) , ( 'sha256' , compute_sha256 ( absf ) ) ] ) )
5257	def block_to_fork ( block_number ) : forks_by_block = { 0 : "frontier" , 1150000 : "homestead" , 2463000 : "tangerine_whistle" , 2675000 : "spurious_dragon" , 4370000 : "byzantium" , 7280000 : "petersburg" , 9999999 : "serenity" } fork_names = list ( forks_by_block . values ( ) ) fork_blocks = list ( forks_by_block . keys ( ) ) return fork_names [ bisect ( fork_blocks , block_number ) - 1 ]
2277	def parse_generator_doubling ( config ) : start = 1 if 'start' in config : start = int ( config [ 'start' ] ) def generator ( ) : val = start while ( True ) : yield val val = val * 2 return generator ( )
733	def inferSingleStep ( self , patternNZ , weightMatrix ) : outputActivation = weightMatrix [ patternNZ ] . sum ( axis = 0 ) outputActivation = outputActivation - numpy . max ( outputActivation ) expOutputActivation = numpy . exp ( outputActivation ) predictDist = expOutputActivation / numpy . sum ( expOutputActivation ) return predictDist
7399	def swap ( self , qs ) : try : replacement = qs [ 0 ] except IndexError : return if not self . _valid_ordering_reference ( replacement ) : raise ValueError ( "%r can only be swapped with instances of %r which %s equals %r." % ( self , self . __class__ , self . order_with_respect_to , self . _get_order_with_respect_to ( ) ) ) self . order , replacement . order = replacement . order , self . order self . save ( ) replacement . save ( )
692	def _initializeEncoders ( self , encoderSpec ) : if self . encoderType in [ 'adaptiveScalar' , 'scalar' ] : if 'minval' in encoderSpec : self . minval = encoderSpec . pop ( 'minval' ) else : self . minval = None if 'maxval' in encoderSpec : self . maxval = encoderSpec . pop ( 'maxval' ) else : self . maxval = None self . encoder = adaptive_scalar . AdaptiveScalarEncoder ( name = 'AdaptiveScalarEncoder' , w = self . w , n = self . n , minval = self . minval , maxval = self . maxval , periodic = False , forced = True ) elif self . encoderType == 'category' : self . encoder = sdr_category . SDRCategoryEncoder ( name = 'categoryEncoder' , w = self . w , n = self . n ) elif self . encoderType in [ 'date' , 'datetime' ] : self . encoder = date . DateEncoder ( name = 'dateEncoder' ) else : raise RuntimeError ( 'Error in constructing class object. Either encoder type' 'or dataType must be specified' )
7224	def save ( self , project ) : if 'id' in project and project [ 'id' ] is not None : self . logger . debug ( 'Updating existing project: ' + json . dumps ( project ) ) url = '%(base_url)s/%(project_id)s' % { 'base_url' : self . base_url , 'project_id' : project [ 'id' ] } r = self . gbdx_connection . put ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise return project [ 'id' ] else : self . logger . debug ( 'Creating new project: ' + json . dumps ( project ) ) url = self . base_url r = self . gbdx_connection . post ( url , json = project ) try : r . raise_for_status ( ) except : print ( r . text ) raise project_json = r . json ( ) return project_json [ 'id' ]
4346	def swap ( self ) : effect_args = [ 'swap' ] self . effects . extend ( effect_args ) self . effects_log . append ( 'swap' ) return self
12765	def reposition ( self , frame_no ) : for label , j in self . channels . items ( ) : body = self . bodies [ label ] body . position = self . positions [ frame_no , j ] body . linear_velocity = self . velocities [ frame_no , j ]
7599	def get_popular_players ( self , ** params : keys ) : url = self . api . POPULAR + '/players' return self . _get_model ( url , PartialPlayerClan , ** params )
8981	def _set_pixel_and_convert_color ( self , x , y , color ) : if color is None : return color = self . _convert_color_to_rrggbb ( color ) self . _set_pixel ( x , y , color )
3866	async def rename ( self , name ) : await self . _client . rename_conversation ( hangouts_pb2 . RenameConversationRequest ( request_header = self . _client . get_request_header ( ) , new_name = name , event_request_header = self . _get_event_request_header ( ) , ) )
8893	def calculate_uuid ( self ) : if self . uuid_input_fields is None : raise NotImplementedError ( ) if self . uuid_input_fields == "RANDOM" : return uuid . uuid4 ( ) . hex assert isinstance ( self . uuid_input_fields , tuple ) , "'uuid_input_fields' must either be a tuple or the string 'RANDOM'" hashable_input_vals = [ ] for field in self . uuid_input_fields : new_value = getattr ( self , field ) if new_value : hashable_input_vals . append ( str ( new_value ) ) hashable_input = ":" . join ( hashable_input_vals ) if not hashable_input : return uuid . uuid4 ( ) . hex return sha2_uuid ( hashable_input )
8811	def filter_factory ( global_conf , ** local_conf ) : conf = global_conf . copy ( ) conf . update ( local_conf ) def wrapper ( app ) : return ResponseAsyncIdAdder ( app , conf ) return wrapper
9719	async def take_control ( self , password ) : cmd = "takecontrol %s" % password return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
2432	def add_creator ( self , doc , creator ) : if validations . validate_creator ( creator ) : doc . creation_info . add_creator ( creator ) return True else : raise SPDXValueError ( 'CreationInfo::Creator' )
13266	def xml_to_json ( root ) : j = { } if len ( root ) == 0 : return _maybe_intify ( root . text ) if len ( root ) == 1 and root [ 0 ] . tag . startswith ( '{' + NS_GML ) : return gml_to_geojson ( root [ 0 ] ) if root . tag == 'open511' : j [ 'meta' ] = { 'version' : root . get ( 'version' ) } for elem in root : name = elem . tag if name == 'link' and elem . get ( 'rel' ) : name = elem . get ( 'rel' ) + '_url' if name == 'self_url' : name = 'url' if root . tag == 'open511' : j [ 'meta' ] [ name ] = elem . get ( 'href' ) continue elif name . startswith ( '{' + NS_PROTECTED ) : name = '!' + name [ name . index ( '}' ) + 1 : ] elif name [ 0 ] == '{' : name = '+' + name [ name . index ( '}' ) + 1 : ] if name in j : continue elif elem . tag == 'link' and not elem . text : j [ name ] = elem . get ( 'href' ) elif len ( elem ) : if name == 'grouped_events' : j [ name ] = [ xml_link_to_json ( child , to_dict = False ) for child in elem ] elif name in ( 'attachments' , 'media_files' ) : j [ name ] = [ xml_link_to_json ( child , to_dict = True ) for child in elem ] elif all ( ( name == pluralize ( child . tag ) for child in elem ) ) : j [ name ] = [ xml_to_json ( child ) for child in elem ] else : j [ name ] = xml_to_json ( elem ) else : if root . tag == 'open511' and name . endswith ( 's' ) and not elem . text : j [ name ] = [ ] else : j [ name ] = _maybe_intify ( elem . text ) return j
7373	def get_error ( data ) : if isinstance ( data , dict ) : if 'errors' in data : error = data [ 'errors' ] [ 0 ] else : error = data . get ( 'error' , None ) if isinstance ( error , dict ) : if error . get ( 'code' ) in errors : return error
9283	def set_login ( self , callsign , passwd = "-1" , skip_login = False ) : self . __dict__ . update ( locals ( ) )
9588	def _execute ( self , command , data = None , unpack = True ) : if not data : data = { } if self . session_id is not None : data . setdefault ( 'session_id' , self . session_id ) data = self . _wrap_el ( data ) res = self . remote_invoker . execute ( command , data ) ret = WebDriverResult . from_object ( res ) ret . raise_for_status ( ) ret . value = self . _unwrap_el ( ret . value ) if not unpack : return ret return ret . value
8881	def fit ( self , X , y = None ) : X = check_array ( X ) self . inverse_influence_matrix = self . __make_inverse_matrix ( X ) if self . threshold == 'auto' : self . threshold_value = 3 * ( 1 + X . shape [ 1 ] ) / X . shape [ 0 ] elif self . threshold == 'cv' : if y is None : raise ValueError ( "Y must be specified to find the optimal threshold." ) y = check_array ( y , accept_sparse = 'csc' , ensure_2d = False , dtype = None ) self . threshold_value = 0 score = 0 Y_pred , Y_true , AD = [ ] , [ ] , [ ] cv = KFold ( n_splits = 5 , random_state = 1 , shuffle = True ) for train_index , test_index in cv . split ( X ) : x_train = safe_indexing ( X , train_index ) x_test = safe_indexing ( X , test_index ) y_train = safe_indexing ( y , train_index ) y_test = safe_indexing ( y , test_index ) if self . reg_model is None : reg_model = RandomForestRegressor ( n_estimators = 500 , random_state = 1 ) . fit ( x_train , y_train ) else : reg_model = clone ( self . reg_model ) . fit ( x_train , y_train ) Y_pred . append ( reg_model . predict ( x_test ) ) Y_true . append ( y_test ) ad_model = self . __make_inverse_matrix ( x_train ) AD . append ( self . __find_leverages ( x_test , ad_model ) ) AD_ = unique ( hstack ( AD ) ) for z in AD_ : AD_new = hstack ( AD ) <= z if self . score == 'ba_ad' : val = balanced_accuracy_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) elif self . score == 'rmse_ad' : val = rmse_score_with_ad ( Y_true = hstack ( Y_true ) , Y_pred = hstack ( Y_pred ) , AD = AD_new ) if val >= score : score = val self . threshold_value = z else : self . threshold_value = self . threshold return self
4344	def stat ( self , input_filepath , scale = None , rms = False ) : effect_args = [ 'channels' , '1' , 'stat' ] if scale is not None : if not is_number ( scale ) or scale <= 0 : raise ValueError ( "scale must be a positive number." ) effect_args . extend ( [ '-s' , '{:f}' . format ( scale ) ] ) if rms : effect_args . append ( '-rms' ) _ , _ , stat_output = self . build ( input_filepath , None , extra_args = effect_args , return_output = True ) stat_dict = { } lines = stat_output . split ( '\n' ) for line in lines : split_line = line . split ( ) if len ( split_line ) == 0 : continue value = split_line [ - 1 ] key = ' ' . join ( split_line [ : - 1 ] ) stat_dict [ key . strip ( ':' ) ] = value return stat_dict
1679	def CheckNextIncludeOrder ( self , header_type ) : error_message = ( 'Found %s after %s' % ( self . _TYPE_NAMES [ header_type ] , self . _SECTION_NAMES [ self . _section ] ) ) last_section = self . _section if header_type == _C_SYS_HEADER : if self . _section <= self . _C_SECTION : self . _section = self . _C_SECTION else : self . _last_header = '' return error_message elif header_type == _CPP_SYS_HEADER : if self . _section <= self . _CPP_SECTION : self . _section = self . _CPP_SECTION else : self . _last_header = '' return error_message elif header_type == _LIKELY_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION elif header_type == _POSSIBLE_MY_HEADER : if self . _section <= self . _MY_H_SECTION : self . _section = self . _MY_H_SECTION else : self . _section = self . _OTHER_H_SECTION else : assert header_type == _OTHER_HEADER self . _section = self . _OTHER_H_SECTION if last_section != self . _section : self . _last_header = '' return ''
6366	def population ( self ) : return self . _tp + self . _tn + self . _fp + self . _fn
1258	def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )
8752	def is_isonet_vif ( vif ) : nicira_iface_id = vif . record . get ( 'other_config' ) . get ( 'nicira-iface-id' ) if nicira_iface_id : return True return False
11194	def freeze ( proto_dataset_uri ) : proto_dataset = dtoolcore . ProtoDataSet . from_uri ( uri = proto_dataset_uri , config_path = CONFIG_PATH ) num_items = len ( list ( proto_dataset . _identifiers ( ) ) ) max_files_limit = int ( dtoolcore . utils . get_config_value ( "DTOOL_MAX_FILES_LIMIT" , CONFIG_PATH , 10000 ) ) assert isinstance ( max_files_limit , int ) if num_items > max_files_limit : click . secho ( "Too many items ({} > {}) in proto dataset" . format ( num_items , max_files_limit ) , fg = "red" ) click . secho ( "1. Consider splitting the dataset into smaller datasets" ) click . secho ( "2. Consider packaging small files using tar" ) click . secho ( "3. Increase the limit using the DTOOL_MAX_FILES_LIMIT" ) click . secho ( " environment variable" ) sys . exit ( 2 ) handles = [ h for h in proto_dataset . _storage_broker . iter_item_handles ( ) ] for h in handles : if not valid_handle ( h ) : click . secho ( "Invalid item name: {}" . format ( h ) , fg = "red" ) click . secho ( "1. Consider renaming the item" ) click . secho ( "2. Consider removing the item" ) sys . exit ( 3 ) with click . progressbar ( length = len ( list ( proto_dataset . _identifiers ( ) ) ) , label = "Generating manifest" ) as progressbar : try : proto_dataset . freeze ( progressbar = progressbar ) except dtoolcore . storagebroker . DiskStorageBrokerValidationWarning as e : click . secho ( "" ) click . secho ( str ( e ) , fg = "red" , nl = False ) sys . exit ( 4 ) click . secho ( "Dataset frozen " , nl = False , fg = "green" ) click . secho ( proto_dataset_uri )
4776	def does_not_contain_duplicates ( self ) : try : if len ( self . val ) == len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to not contain duplicates, but did.' % self . val )
7629	def namespace_array ( ns_key ) : obs_sch = namespace ( ns_key ) obs_sch [ 'title' ] = 'Observation' sch = copy . deepcopy ( JAMS_SCHEMA [ 'definitions' ] [ 'SparseObservationList' ] ) sch [ 'items' ] = obs_sch return sch
5453	def _remove_empty_items ( d , required ) : new_dict = { } for k , v in d . items ( ) : if k in required : new_dict [ k ] = v elif isinstance ( v , int ) or v : new_dict [ k ] = v return new_dict
3117	def oauth2_authorize ( request ) : return_url = request . GET . get ( 'return_url' , None ) if not return_url : return_url = request . META . get ( 'HTTP_REFERER' , '/' ) scopes = request . GET . getlist ( 'scopes' , django_util . oauth2_settings . scopes ) if django_util . oauth2_settings . storage_model : if not request . user . is_authenticated ( ) : return redirect ( '{0}?next={1}' . format ( settings . LOGIN_URL , parse . quote ( request . get_full_path ( ) ) ) ) else : user_oauth = django_util . UserOAuth2 ( request , scopes , return_url ) if user_oauth . has_credentials ( ) : return redirect ( return_url ) flow = _make_flow ( request = request , scopes = scopes , return_url = return_url ) auth_url = flow . step1_get_authorize_url ( ) return shortcuts . redirect ( auth_url )
1898	def _getvalue ( self , expression ) : if not issymbolic ( expression ) : return expression assert isinstance ( expression , Variable ) if isinstance ( expression , Array ) : result = bytearray ( ) for c in expression : expression_str = translate_to_smtlib ( c ) self . _send ( '(get-value (%s))' % expression_str ) response = self . _recv ( ) result . append ( int ( '0x{:s}' . format ( response . split ( expression_str ) [ 1 ] [ 3 : - 2 ] ) , 16 ) ) return bytes ( result ) else : self . _send ( '(get-value (%s))' % expression . name ) ret = self . _recv ( ) assert ret . startswith ( '((' ) and ret . endswith ( '))' ) , ret if isinstance ( expression , Bool ) : return { 'true' : True , 'false' : False } [ ret [ 2 : - 2 ] . split ( ' ' ) [ 1 ] ] elif isinstance ( expression , BitVec ) : pattern , base = self . _get_value_fmt m = pattern . match ( ret ) expr , value = m . group ( 'expr' ) , m . group ( 'value' ) return int ( value , base ) raise NotImplementedError ( "_getvalue only implemented for Bool and BitVec" )
9194	def publish ( request ) : if 'epub' not in request . POST : raise httpexceptions . HTTPBadRequest ( "Missing EPUB in POST body." ) is_pre_publication = asbool ( request . POST . get ( 'pre-publication' ) ) epub_upload = request . POST [ 'epub' ] . file try : epub = cnxepub . EPUB . from_file ( epub_upload ) except : raise httpexceptions . HTTPBadRequest ( 'Format not recognized.' ) with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : epub_upload . seek ( 0 ) publication_id , publications = add_publication ( cursor , epub , epub_upload , is_pre_publication ) state , messages = poke_publication_state ( publication_id ) response_data = { 'publication' : publication_id , 'mapping' : publications , 'state' : state , 'messages' : messages , } return response_data
5493	def validate_config_key ( ctx , param , value ) : if not value : return value try : section , item = value . split ( "." , 1 ) except ValueError : raise click . BadArgumentUsage ( "Given key does not contain a section name." ) else : return section , item
674	def _loadDummyModelParameters ( self , params ) : for key , value in params . iteritems ( ) : if type ( value ) == list : index = self . modelIndex % len ( params [ key ] ) self . _params [ key ] = params [ key ] [ index ] else : self . _params [ key ] = params [ key ]
9638	def emit ( self , record ) : try : self . redis_client . publish ( self . channel , self . format ( record ) ) except redis . RedisError : pass
8623	def get_self_user_id ( session ) : response = make_get_request ( session , 'self' ) if response . status_code == 200 : return response . json ( ) [ 'result' ] [ 'id' ] else : raise UserIdNotRetrievedException ( 'Error retrieving user id: %s' % response . text , response . text )
13207	def _parse_author ( self ) : r command = LatexCommand ( 'author' , { 'name' : 'authors' , 'required' : True , 'bracket' : '{' } ) try : parsed = next ( command . parse ( self . _tex ) ) except StopIteration : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return try : content = parsed [ 'authors' ] except KeyError : self . _logger . warning ( 'lsstdoc has no author' ) self . _authors = [ ] return content = content . replace ( '\n' , ' ' ) content = content . replace ( '~' , ' ' ) content = content . strip ( ) authors = [ ] for part in content . split ( ',' ) : part = part . strip ( ) for split_part in part . split ( 'and ' ) : split_part = split_part . strip ( ) if len ( split_part ) > 0 : authors . append ( split_part ) self . _authors = authors
7350	def predict ( self , sequences ) : with tempfile . NamedTemporaryFile ( suffix = ".fsa" , mode = "w" ) as input_fd : for ( i , sequence ) in enumerate ( sequences ) : input_fd . write ( "> %d\n" % i ) input_fd . write ( sequence ) input_fd . write ( "\n" ) input_fd . flush ( ) try : output = subprocess . check_output ( [ "netChop" , input_fd . name ] ) except subprocess . CalledProcessError as e : logging . error ( "Error calling netChop: %s:\n%s" % ( e , e . output ) ) raise parsed = self . parse_netchop ( output ) assert len ( parsed ) == len ( sequences ) , "Expected %d results but got %d" % ( len ( sequences ) , len ( parsed ) ) assert [ len ( x ) for x in parsed ] == [ len ( x ) for x in sequences ] return parsed
13238	def _daily_periods ( self , range_start , range_end ) : specific = set ( self . exceptions . keys ( ) ) return heapq . merge ( self . exception_periods ( range_start , range_end ) , * [ sched . daily_periods ( range_start = range_start , range_end = range_end , exclude_dates = specific ) for sched in self . _recurring_schedules ] )
5280	def sklearn2pmml ( pipeline , pmml , user_classpath = [ ] , with_repr = False , debug = False , java_encoding = "UTF-8" ) : if debug : java_version = _java_version ( java_encoding ) if java_version is None : java_version = ( "java" , "N/A" ) print ( "python: {0}" . format ( platform . python_version ( ) ) ) print ( "sklearn: {0}" . format ( sklearn . __version__ ) ) print ( "sklearn.externals.joblib: {0}" . format ( joblib . __version__ ) ) print ( "pandas: {0}" . format ( pandas . __version__ ) ) print ( "sklearn_pandas: {0}" . format ( sklearn_pandas . __version__ ) ) print ( "sklearn2pmml: {0}" . format ( __version__ ) ) print ( "{0}: {1}" . format ( java_version [ 0 ] , java_version [ 1 ] ) ) if not isinstance ( pipeline , PMMLPipeline ) : raise TypeError ( "The pipeline object is not an instance of " + PMMLPipeline . __name__ + ". Use the 'sklearn2pmml.make_pmml_pipeline(obj)' utility function to translate a regular Scikit-Learn estimator or pipeline to a PMML pipeline" ) estimator = pipeline . _final_estimator cmd = [ "java" , "-cp" , os . pathsep . join ( _classpath ( user_classpath ) ) , "org.jpmml.sklearn.Main" ] dumps = [ ] try : if with_repr : pipeline . repr_ = repr ( pipeline ) if hasattr ( estimator , "download_mojo" ) : estimator_mojo = estimator . download_mojo ( ) dumps . append ( estimator_mojo ) estimator . _mojo_path = estimator_mojo pipeline_pkl = _dump ( pipeline , "pipeline" ) cmd . extend ( [ "--pkl-pipeline-input" , pipeline_pkl ] ) dumps . append ( pipeline_pkl ) cmd . extend ( [ "--pmml-output" , pmml ] ) if debug : print ( "Executing command:\n{0}" . format ( " " . join ( cmd ) ) ) try : process = Popen ( cmd , stdout = PIPE , stderr = PIPE , bufsize = 1 ) except OSError : raise RuntimeError ( "Java is not installed, or the Java executable is not on system path" ) output , error = process . communicate ( ) retcode = process . poll ( ) if debug or retcode : if ( len ( output ) > 0 ) : print ( "Standard output:\n{0}" . format ( _decode ( output , java_encoding ) ) ) else : print ( "Standard output is empty" ) if ( len ( error ) > 0 ) : print ( "Standard error:\n{0}" . format ( _decode ( error , java_encoding ) ) ) else : print ( "Standard error is empty" ) if retcode : raise RuntimeError ( "The JPMML-SkLearn conversion application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams" ) finally : if debug : print ( "Preserved joblib dump file(s): {0}" . format ( " " . join ( dumps ) ) ) else : for dump in dumps : os . remove ( dump )
4168	def ss2zpk ( a , b , c , d , input = 0 ) : import scipy . signal z , p , k = scipy . signal . ss2zpk ( a , b , c , d , input = input ) return z , p , k
10527	def cast_to_list ( position ) : @ wrapt . decorator def wrapper ( function , instance , args , kwargs ) : if not isinstance ( args [ position ] , list ) : args = list ( args ) args [ position ] = [ args [ position ] ] args = tuple ( args ) return function ( * args , ** kwargs ) return wrapper
12088	def proto_01_01_HP010 ( abf = exampleABF ) : swhlab . memtest . memtest ( abf ) swhlab . memtest . checkSweep ( abf ) swhlab . plot . save ( abf , tag = "tau" )
12638	def merge_groups ( self , indices ) : try : merged = merge_dict_of_lists ( self . dicom_groups , indices , pop_later = True , copy = True ) self . dicom_groups = merged except IndexError : raise IndexError ( 'Index out of range to merge DICOM groups.' )
1521	def get_hostname ( ip_addr , cl_args ) : if is_self ( ip_addr ) : return get_self_hostname ( ) cmd = "hostname" ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) pid = subprocess . Popen ( ssh_cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) if return_code != 0 : Log . error ( "Failed to get hostname for remote host %s with output:\n%s" % ( ip_addr , output ) ) sys . exit ( - 1 ) return output [ 0 ] . strip ( "\n" )
4069	def _validate ( self , conditions ) : allowed_keys = set ( self . searchkeys ) operators_set = set ( self . operators . keys ( ) ) for condition in conditions : if set ( condition . keys ( ) ) != allowed_keys : raise ze . ParamNotPassed ( "Keys must be all of: %s" % ", " . join ( self . searchkeys ) ) if condition . get ( "operator" ) not in operators_set : raise ze . ParamNotPassed ( "You have specified an unknown operator: %s" % condition . get ( "operator" ) ) permitted_operators = self . conditions_operators . get ( condition . get ( "condition" ) ) permitted_operators_list = set ( [ self . operators . get ( op ) for op in permitted_operators ] ) if condition . get ( "operator" ) not in permitted_operators_list : raise ze . ParamNotPassed ( "You may not use the '%s' operator when selecting the '%s' condition. \nAllowed operators: %s" % ( condition . get ( "operator" ) , condition . get ( "condition" ) , ", " . join ( list ( permitted_operators_list ) ) , ) )
3592	def encryptPassword ( self , login , passwd ) : binaryKey = b64decode ( config . GOOGLE_PUBKEY ) i = utils . readInt ( binaryKey , 0 ) modulus = utils . toBigInt ( binaryKey [ 4 : ] [ 0 : i ] ) j = utils . readInt ( binaryKey , i + 4 ) exponent = utils . toBigInt ( binaryKey [ i + 8 : ] [ 0 : j ] ) digest = hashes . Hash ( hashes . SHA1 ( ) , backend = default_backend ( ) ) digest . update ( binaryKey ) h = b'\x00' + digest . finalize ( ) [ 0 : 4 ] der_data = encode_dss_signature ( modulus , exponent ) publicKey = load_der_public_key ( der_data , backend = default_backend ( ) ) to_be_encrypted = login . encode ( ) + b'\x00' + passwd . encode ( ) ciphertext = publicKey . encrypt ( to_be_encrypted , padding . OAEP ( mgf = padding . MGF1 ( algorithm = hashes . SHA1 ( ) ) , algorithm = hashes . SHA1 ( ) , label = None ) ) return urlsafe_b64encode ( h + ciphertext )
9109	def _create_encrypted_zip ( self , source = 'dirty' , fs_target_dir = None ) : backup_recipients = [ r for r in self . editors if checkRecipient ( self . gpg_context , r ) ] if not backup_recipients : self . status = u'500 no valid keys at all' return self . status fs_backup = join ( self . fs_path , '%s.zip' % source ) if fs_target_dir is None : fs_backup_pgp = join ( self . fs_path , '%s.zip.pgp' % source ) else : fs_backup_pgp = join ( fs_target_dir , '%s.zip.pgp' % self . drop_id ) fs_source = dict ( dirty = self . fs_dirty_attachments , clean = self . fs_cleansed_attachments ) with ZipFile ( fs_backup , 'w' , ZIP_STORED ) as backup : if exists ( join ( self . fs_path , 'message' ) ) : backup . write ( join ( self . fs_path , 'message' ) , arcname = 'message' ) for fs_attachment in fs_source [ source ] : backup . write ( fs_attachment , arcname = split ( fs_attachment ) [ - 1 ] ) with open ( fs_backup , "rb" ) as backup : self . gpg_context . encrypt_file ( backup , backup_recipients , always_trust = True , output = fs_backup_pgp ) remove ( fs_backup ) return fs_backup_pgp
2558	def clean_pair ( cls , attribute , value ) : attribute = cls . clean_attribute ( attribute ) if value is True : value = attribute if value is False : value = "false" return ( attribute , value )
11019	def show_response_messages ( response_json ) : message_type_kwargs = { 'warning' : { 'fg' : 'yellow' } , 'error' : { 'fg' : 'red' } , } for message in response_json . get ( 'messages' , [ ] ) : click . secho ( message [ 'text' ] , ** message_type_kwargs . get ( message [ 'type' ] , { } ) )
3384	def generate_fva_warmup ( self ) : self . n_warmup = 0 reactions = self . model . reactions self . warmup = np . zeros ( ( 2 * len ( reactions ) , len ( self . model . variables ) ) ) self . model . objective = Zero for sense in ( "min" , "max" ) : self . model . objective_direction = sense for i , r in enumerate ( reactions ) : variables = ( self . model . variables [ self . fwd_idx [ i ] ] , self . model . variables [ self . rev_idx [ i ] ] ) if r . upper_bound - r . lower_bound < self . bounds_tol : LOGGER . info ( "skipping fixed reaction %s" % r . id ) continue self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 1 , variables [ 1 ] : - 1 } ) self . model . slim_optimize ( ) if not self . model . solver . status == OPTIMAL : LOGGER . info ( "can not maximize reaction %s, skipping it" % r . id ) continue primals = self . model . solver . primal_values sol = [ primals [ v . name ] for v in self . model . variables ] self . warmup [ self . n_warmup , ] = sol self . n_warmup += 1 self . model . objective . set_linear_coefficients ( { variables [ 0 ] : 0 , variables [ 1 ] : 0 } ) self . warmup = self . warmup [ 0 : self . n_warmup , : ] keep = np . logical_not ( self . _is_redundant ( self . warmup ) ) self . warmup = self . warmup [ keep , : ] self . n_warmup = self . warmup . shape [ 0 ] if len ( self . warmup . shape ) == 1 or self . warmup . shape [ 0 ] == 1 : raise ValueError ( "Your flux cone consists only of a single point!" ) elif self . n_warmup == 2 : if not self . problem . homogeneous : raise ValueError ( "Can not sample from an inhomogenous problem" " with only 2 search directions :(" ) LOGGER . info ( "All search directions on a line, adding another one." ) newdir = self . warmup . T . dot ( [ 0.25 , 0.25 ] ) self . warmup = np . vstack ( [ self . warmup , newdir ] ) self . n_warmup += 1 self . warmup = shared_np_array ( ( self . n_warmup , len ( self . model . variables ) ) , self . warmup )
11901	def _run_server ( ) : port = _get_server_port ( ) SocketServer . TCPServer . allow_reuse_address = True server = SocketServer . TCPServer ( ( '' , port ) , SimpleHTTPServer . SimpleHTTPRequestHandler ) print ( 'Your images are at http://127.0.0.1:%d/%s' % ( port , INDEX_FILE_NAME ) ) try : server . serve_forever ( ) except KeyboardInterrupt : print ( 'User interrupted, stopping' ) except Exception as exptn : print ( exptn ) print ( 'Unhandled exception in server, stopping' )
6486	def course_discovery ( request ) : results = { "error" : _ ( "Nothing to search" ) } status_code = 500 search_term = request . POST . get ( "search_string" , None ) try : size , from_ , page = _process_pagination_values ( request ) field_dictionary = _process_field_values ( request ) track . emit ( 'edx.course_discovery.search.initiated' , { "search_term" : search_term , "page_size" : size , "page_number" : page , } ) results = course_discovery_search ( search_term = search_term , size = size , from_ = from_ , field_dictionary = field_dictionary , ) track . emit ( 'edx.course_discovery.search.results_displayed' , { "search_term" : search_term , "page_size" : size , "page_number" : page , "results_count" : results [ "total" ] , } ) status_code = 200 except ValueError as invalid_err : results = { "error" : six . text_type ( invalid_err ) } log . debug ( six . text_type ( invalid_err ) ) except QueryParseError : results = { "error" : _ ( 'Your query seems malformed. Check for unmatched quotes.' ) } except Exception as err : results = { "error" : _ ( 'An error occurred when searching for "{search_string}"' ) . format ( search_string = search_term ) } log . exception ( 'Search view exception when searching for %s for user %s: %r' , search_term , request . user . id , err ) return JsonResponse ( results , status = status_code )
4223	def init_backend ( limit = None ) : backend . _limit = limit keyrings = filter ( limit , backend . get_all_keyring ( ) ) set_keyring ( load_env ( ) or load_config ( ) or max ( keyrings , default = fail . Keyring ( ) , key = backend . by_priority ) )
9715	async def qtm_version ( self ) : return await asyncio . wait_for ( self . _protocol . send_command ( "qtmversion" ) , timeout = self . _timeout )
11150	def get_text_fingerprint ( text , hash_meth , encoding = "utf-8" ) : m = hash_meth ( ) m . update ( text . encode ( encoding ) ) return m . hexdigest ( )
6277	def resize ( self , width , height ) : if not self . fbo : return self . width = width // self . widget . devicePixelRatio ( ) self . height = height // self . widget . devicePixelRatio ( ) self . buffer_width = width self . buffer_height = height super ( ) . resize ( width , height )
3735	def CoolProp_T_dependent_property ( T , CASRN , prop , phase ) : r if not has_CoolProp : raise Exception ( 'CoolProp library is not installed' ) if CASRN not in coolprop_dict : raise Exception ( 'CASRN not in list of supported fluids' ) Tc = coolprop_fluids [ CASRN ] . Tc T = float ( T ) if phase == 'l' : if T > Tc : raise Exception ( 'For liquid properties, must be under the critical temperature.' ) if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) in [ u'liquid' , u'supercritical_liquid' ] : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'Q' , 0 , CASRN ) elif phase == 'g' : if PhaseSI ( 'T' , T , 'P' , 101325 , CASRN ) == 'gas' : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : if T < Tc : return PropsSI ( prop , 'T' , T , 'Q' , 1 , CASRN ) else : return PropsSI ( prop , 'T' , T , 'P' , 101325 , CASRN ) else : raise Exception ( 'Error in CoolProp property function' )
4905	def create_course_completion ( self , user_id , payload ) : return self . _post ( urljoin ( self . enterprise_configuration . degreed_base_url , self . global_degreed_config . completion_status_api_path ) , payload , self . COMPLETION_PROVIDER_SCOPE )
6943	def jhk_to_bmag ( jmag , hmag , kmag ) : return convert_constants ( jmag , hmag , kmag , BJHK , BJH , BJK , BHK , BJ , BH , BK )
1238	def put ( self , item , priority = None ) : if not self . _isfull ( ) : self . _memory . append ( None ) position = self . _next_position_then_increment ( ) old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) row = _SumRow ( item , priority ) self . _memory [ position ] = row self . _update_internal_nodes ( position , ( row . priority or 0 ) - old_priority )
2933	def write_meta_data ( self ) : config = configparser . ConfigParser ( ) config . add_section ( 'MetaData' ) config . set ( 'MetaData' , 'entry_point_process' , self . wf_spec . name ) if self . editor : config . set ( 'MetaData' , 'editor' , self . editor ) for k , v in self . meta_data : config . set ( 'MetaData' , k , v ) if not self . PARSER_CLASS == BpmnParser : config . set ( 'MetaData' , 'parser_class_module' , inspect . getmodule ( self . PARSER_CLASS ) . __name__ ) config . set ( 'MetaData' , 'parser_class' , self . PARSER_CLASS . __name__ ) ini = StringIO ( ) config . write ( ini ) self . write_to_package_zip ( self . METADATA_FILE , ini . getvalue ( ) )
10430	def multiselect ( self , window_name , object_name , row_text_list , partial_match = False ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) object_handle . activate ( ) selected = False try : window = self . _get_front_most_window ( ) except ( IndexError , ) : window = self . _get_any_window ( ) for row_text in row_text_list : selected = False for cell in object_handle . AXRows : parent_cell = cell cell = self . _getfirstmatchingchild ( cell , "(AXTextField|AXStaticText)" ) if not cell : continue if re . match ( row_text , cell . AXValue ) : selected = True if not parent_cell . AXSelected : x , y , width , height = self . _getobjectsize ( parent_cell ) window . clickMouseButtonLeftWithMods ( ( x + width / 2 , y + height / 2 ) , [ '<command_l>' ] ) self . wait ( 0.5 ) else : pass break if not selected : raise LdtpServerException ( u"Unable to select row: %s" % row_text ) if not selected : raise LdtpServerException ( u"Unable to select any row" ) return 1
2695	def parse_doc ( json_iter ) : global DEBUG for meta in json_iter : base_idx = 0 for graf_text in filter_quotes ( meta [ "text" ] , is_email = False ) : if DEBUG : print ( "graf_text:" , graf_text ) grafs , new_base_idx = parse_graf ( meta [ "id" ] , graf_text , base_idx ) base_idx = new_base_idx for graf in grafs : yield graf
4939	def get_link_by_email ( self , user_email ) : try : user = User . objects . get ( email = user_email ) try : return self . get ( user_id = user . id ) except EnterpriseCustomerUser . DoesNotExist : pass except User . DoesNotExist : pass try : return PendingEnterpriseCustomerUser . objects . get ( user_email = user_email ) except PendingEnterpriseCustomerUser . DoesNotExist : pass return None
9263	def get_filtered_pull_requests ( self , pull_requests ) : pull_requests = self . filter_by_labels ( pull_requests , "pull requests" ) pull_requests = self . filter_merged_pull_requests ( pull_requests ) if self . options . verbose > 1 : print ( "\tremaining pull requests: {}" . format ( len ( pull_requests ) ) ) return pull_requests
442	def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params
9303	def get_request_date ( cls , req ) : date = None for header in [ 'x-amz-date' , 'date' ] : if header not in req . headers : continue try : date_str = cls . parse_date ( req . headers [ header ] ) except DateFormatError : continue try : date = datetime . datetime . strptime ( date_str , '%Y-%m-%d' ) . date ( ) except ValueError : continue else : break return date
9173	def _formatter_callback_factory ( ) : includes = [ ] exercise_url_template = '{baseUrl}/api/exercises?q={field}:"{{itemCode}}"' settings = get_current_registry ( ) . settings exercise_base_url = settings . get ( 'embeddables.exercise.base_url' , None ) exercise_matches = [ match . split ( ',' , 1 ) for match in aslist ( settings . get ( 'embeddables.exercise.match' , '' ) , flatten = False ) ] exercise_token = settings . get ( 'embeddables.exercise.token' , None ) mathml_url = settings . get ( 'mathmlcloud.url' , None ) memcache_servers = settings . get ( 'memcache_servers' ) if memcache_servers : memcache_servers = memcache_servers . split ( ) else : memcache_servers = None if exercise_base_url and exercise_matches : mc_client = None if memcache_servers : mc_client = memcache . Client ( memcache_servers , debug = 0 ) for ( exercise_match , exercise_field ) in exercise_matches : template = exercise_url_template . format ( baseUrl = exercise_base_url , field = exercise_field ) includes . append ( exercise_callback_factory ( exercise_match , template , mc_client , exercise_token , mathml_url ) ) return includes
12111	def save ( self , filename , metadata = { } , ** data ) : intersection = set ( metadata . keys ( ) ) & set ( data . keys ( ) ) if intersection : msg = 'Key(s) overlap between data and metadata: %s' raise Exception ( msg % ',' . join ( intersection ) )
13353	def status_job ( self , fn = None , name = None , timeout = 3 ) : if fn is None : def decorator ( fn ) : self . add_status_job ( fn , name , timeout ) return decorator else : self . add_status_job ( fn , name , timeout )
13725	def register_credentials ( self , credentials = None , user = None , user_file = None , password = None , password_file = None ) : if credentials is not None : self . credentials = credentials else : self . credentials = { } if user : self . credentials [ "user" ] = user elif user_file : with open ( user_file , "r" ) as of : pattern = re . compile ( "^user: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "user" ] = re . sub ( pattern , "" , l ) if self . credentials [ "user" ] [ 0 : 1 ] == '"' and self . credentials [ "user" ] [ - 1 : ] == '"' : self . credentials [ "user" ] = self . credentials [ "user" ] [ 1 : - 1 ] if password : self . credentials [ "password" ] = password elif password_file : with open ( password_file , "r" ) as of : pattern = re . compile ( "^password: " ) for l in of : if re . match ( pattern , l ) : l = l [ 0 : - 1 ] self . credentials [ "password" ] = re . sub ( pattern , "" , l ) if self . credentials [ "password" ] [ 0 : 1 ] == '"' and self . credentials [ "password" ] [ - 1 : ] == '"' : self . credentials [ "password" ] = self . credentials [ "password" ] [ 1 : - 1 ] if "user" in self . credentials and "password" in self . credentials : c = self . credentials [ "user" ] + ":" + self . credentials [ "password" ] self . credentials [ "base64" ] = b64encode ( c . encode ( ) ) . decode ( "ascii" )
2095	def monitor ( self , pk , parent_pk = None , timeout = None , interval = 0.5 , outfile = sys . stdout , ** kwargs ) : if pk is None : pk = self . last_job_data ( parent_pk , ** kwargs ) [ 'id' ] job_endpoint = '%s%s/' % ( self . unified_job_type , pk ) self . wait ( pk , exit_on = [ 'running' , 'successful' ] , outfile = outfile ) start = time . time ( ) start_line = 0 result = client . get ( job_endpoint ) . json ( ) click . echo ( '\033[0;91m------Starting Standard Out Stream------\033[0m' , nl = 2 , file = outfile ) while not result [ 'failed' ] and result [ 'status' ] != 'successful' : result = client . get ( job_endpoint ) . json ( ) time . sleep ( interval ) content = self . lookup_stdout ( pk , start_line , full = False ) if not content . startswith ( "Waiting for results" ) : line_count = len ( content . splitlines ( ) ) start_line += line_count click . echo ( content , nl = 0 , file = outfile ) if timeout and time . time ( ) - start > timeout : raise exc . Timeout ( 'Monitoring aborted due to timeout.' ) if self . endpoint == '/workflow_jobs/' : click . echo ( self . lookup_stdout ( pk , start_line , full = True ) , nl = 1 ) click . echo ( '\033[0;91m------End of Standard Out Stream--------\033[0m' , nl = 2 , file = outfile ) if result [ 'failed' ] : raise exc . JobFailure ( 'Job failed.' ) answer = OrderedDict ( ( ( 'changed' , True ) , ( 'id' , pk ) ) ) answer . update ( result ) if parent_pk : answer [ 'id' ] = parent_pk else : answer [ 'id' ] = pk return answer
8991	def rows_before ( self ) : rows_before = [ ] for mesh in self . consumed_meshes : if mesh . is_produced ( ) : row = mesh . producing_row if rows_before not in rows_before : rows_before . append ( row ) return rows_before
10473	def _sendKey ( self , keychr , modFlags = 0 , globally = False ) : escapedChrs = { '\n' : AXKeyCodeConstants . RETURN , '\r' : AXKeyCodeConstants . RETURN , '\t' : AXKeyCodeConstants . TAB , } if keychr in escapedChrs : keychr = escapedChrs [ keychr ] self . _addKeyToQueue ( keychr , modFlags , globally = globally ) self . _postQueuedEvents ( )
1471	def setup ( executor ) : def signal_handler ( signal_to_handle , frame ) : Log . info ( 'signal_handler invoked with signal %s' , signal_to_handle ) executor . stop_state_manager_watches ( ) sys . exit ( signal_to_handle ) def cleanup ( ) : Log . info ( 'Executor terminated; exiting all process in executor.' ) for pid in executor . processes_to_monitor . keys ( ) : os . kill ( pid , signal . SIGTERM ) time . sleep ( 5 ) os . killpg ( 0 , signal . SIGTERM ) shardid = executor . shard log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) pid = os . getpid ( ) sid = os . getsid ( pid ) if pid <> sid : Log . info ( 'Set up process group; executor becomes leader' ) os . setpgrp ( ) Log . info ( 'Register the SIGTERM signal handler' ) signal . signal ( signal . SIGTERM , signal_handler ) Log . info ( 'Register the atexit clean up' ) atexit . register ( cleanup )
844	def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices
9352	def street_number ( ) : length = int ( random . choice ( string . digits [ 1 : 6 ] ) ) return '' . join ( random . sample ( string . digits , length ) )
12428	def check_directories ( self ) : self . log . debug ( 'Checking directories' ) if not os . path . exists ( self . _ve_dir ) : os . makedirs ( self . _ve_dir ) if not os . path . exists ( self . _app_dir ) : os . makedirs ( self . _app_dir ) if not os . path . exists ( self . _conf_dir ) : os . makedirs ( self . _conf_dir ) if not os . path . exists ( self . _var_dir ) : os . makedirs ( self . _var_dir ) if not os . path . exists ( self . _log_dir ) : os . makedirs ( self . _log_dir ) if not os . path . exists ( self . _script_dir ) : os . makedirs ( self . _script_dir ) uwsgi_params = '/etc/nginx/uwsgi_params' if os . path . exists ( uwsgi_params ) : shutil . copy ( uwsgi_params , self . _conf_dir ) else : logging . warning ( 'Unable to find Nginx uwsgi_params. You must manually copy this to {0}.' . format ( self . _conf_dir ) ) mime_types = '/etc/nginx/mime.types' if os . path . exists ( mime_types ) : shutil . copy ( mime_types , self . _conf_dir ) self . _include_mimetypes = True else : logging . warn ( 'Unable to find mime.types for Nginx. You must manually copy this to {0}.' . format ( self . _conf_dir ) )
11303	def invalidate_stored_oembeds ( self , sender , instance , created , ** kwargs ) : ctype = ContentType . objects . get_for_model ( instance ) StoredOEmbed . objects . filter ( object_id = instance . pk , content_type = ctype ) . delete ( )
11803	def nconflicts ( self , var , val , assignment ) : n = len ( self . vars ) c = self . rows [ val ] + self . downs [ var + val ] + self . ups [ var - val + n - 1 ] if assignment . get ( var , None ) == val : c -= 3 return c
4571	def adapt_animation_layout ( animation ) : layout = animation . layout required = getattr ( animation , 'LAYOUT_CLASS' , None ) if not required or isinstance ( layout , required ) : return msg = LAYOUT_WARNING % ( type ( animation ) . __name__ , required . __name__ , type ( layout ) . __name__ ) setter = layout . set adaptor = None if required is strip . Strip : if isinstance ( layout , matrix . Matrix ) : width = layout . width def adaptor ( pixel , color = None ) : y , x = divmod ( pixel , width ) setter ( x , y , color or BLACK ) elif isinstance ( layout , cube . Cube ) : lx , ly = layout . x , layout . y def adaptor ( pixel , color = None ) : yz , x = divmod ( pixel , lx ) z , y = divmod ( yz , ly ) setter ( x , y , z , color or BLACK ) elif isinstance ( layout , circle . Circle ) : def adaptor ( pixel , color = None ) : layout . _set_base ( pixel , color or BLACK ) elif required is matrix . Matrix : if isinstance ( layout , strip . Strip ) : width = animation . width def adaptor ( x , y , color = None ) : setter ( x + y * width , color or BLACK ) if not adaptor : raise ValueError ( msg ) log . warning ( msg ) animation . layout . set = adaptor
2342	def run ( self , x , y , lr = 0.01 , train_epochs = 1000 , test_epochs = 1000 , idx = 0 , verbose = None , ** kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) running_loss = 0 teloss = 0 for i in range ( train_epochs + test_epochs ) : optim . zero_grad ( ) pred = self . forward ( x ) loss = self . criterion ( pred , y ) running_loss += loss . item ( ) if i < train_epochs : loss . backward ( ) optim . step ( ) else : teloss += running_loss if verbose and not i % 300 : print ( 'Idx:{}; epoch:{}; score:{}' . format ( idx , i , running_loss / 300 ) ) running_loss = 0.0 return teloss / test_epochs
3358	def index ( self , id , * args ) : if isinstance ( id , string_types ) : try : return self . _dict [ id ] except KeyError : raise ValueError ( "%s not found" % id ) try : i = self . _dict [ id . id ] if self [ i ] is not id : raise ValueError ( "Another object with the identical id (%s) found" % id . id ) return i except KeyError : raise ValueError ( "%s not found" % str ( id ) )
4970	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerAdminForm , self ) . clean ( ) if 'catalog' in cleaned_data and not cleaned_data [ 'catalog' ] : cleaned_data [ 'catalog' ] = None return cleaned_data
11450	def get_date ( self , filename ) : try : self . document = parse ( filename ) return self . _get_date ( ) except DateNotFoundException : print ( "Date problem found in {0}" . format ( filename ) ) return datetime . datetime . strftime ( datetime . datetime . now ( ) , "%Y-%m-%d" )
5790	def peek_openssl_error ( ) : error = libcrypto . ERR_peek_error ( ) lib = int ( ( error >> 24 ) & 0xff ) func = int ( ( error >> 12 ) & 0xfff ) reason = int ( error & 0xfff ) return ( lib , func , reason )
1685	def RepositoryName ( self ) : r fullname = self . FullName ( ) if os . path . exists ( fullname ) : project_dir = os . path . dirname ( fullname ) if _repository : repo = FileInfo ( _repository ) . FullName ( ) root_dir = project_dir while os . path . exists ( root_dir ) : if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : return os . path . relpath ( fullname , root_dir ) . replace ( '\\' , '/' ) one_up_dir = os . path . dirname ( root_dir ) if one_up_dir == root_dir : break root_dir = one_up_dir if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : root_dir = project_dir one_up_dir = os . path . dirname ( root_dir ) while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : root_dir = os . path . dirname ( root_dir ) one_up_dir = os . path . dirname ( one_up_dir ) prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] root_dir = current_dir = os . path . dirname ( fullname ) while current_dir != os . path . dirname ( current_dir ) : if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : root_dir = current_dir current_dir = os . path . dirname ( current_dir ) if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) return fullname [ len ( prefix ) + 1 : ] return fullname
7031	def specwindow_lsp ( times , mags , errs , magsarefluxes = False , startp = None , endp = None , stepsize = 1.0e-4 , autofreq = True , nbestpeaks = 5 , periodepsilon = 0.1 , sigclip = 10.0 , nworkers = None , glspfunc = _glsp_worker_specwindow , verbose = True ) : lspres = pgen_lsp ( times , mags , errs , magsarefluxes = magsarefluxes , startp = startp , endp = endp , autofreq = autofreq , nbestpeaks = nbestpeaks , periodepsilon = periodepsilon , stepsize = stepsize , nworkers = nworkers , sigclip = sigclip , glspfunc = glspfunc , verbose = verbose ) lspres [ 'method' ] = 'win' if lspres [ 'lspvals' ] is not None : lspmax = npnanmax ( lspres [ 'lspvals' ] ) if npisfinite ( lspmax ) : lspres [ 'lspvals' ] = lspres [ 'lspvals' ] / lspmax lspres [ 'nbestlspvals' ] = [ x / lspmax for x in lspres [ 'nbestlspvals' ] ] lspres [ 'bestlspval' ] = lspres [ 'bestlspval' ] / lspmax return lspres
1508	def add_additional_args ( parsers ) : for parser in parsers : cli_args . add_verbose ( parser ) cli_args . add_config ( parser ) parser . add_argument ( '--heron-dir' , default = config . get_heron_dir ( ) , help = 'Path to Heron home directory' )
4326	def delay ( self , positions ) : if not isinstance ( positions , list ) : raise ValueError ( "positions must be a a list of numbers" ) if not all ( ( is_number ( p ) and p >= 0 ) for p in positions ) : raise ValueError ( "positions must be positive nubmers" ) effect_args = [ 'delay' ] effect_args . extend ( [ '{:f}' . format ( p ) for p in positions ] ) self . effects . extend ( effect_args ) self . effects_log . append ( 'delay' ) return self
4333	def noisered ( self , profile_path , amount = 0.5 ) : if not os . path . exists ( profile_path ) : raise IOError ( "profile_path {} does not exist." . format ( profile_path ) ) if not is_number ( amount ) or amount < 0 or amount > 1 : raise ValueError ( "amount must be a number between 0 and 1." ) effect_args = [ 'noisered' , profile_path , '{:f}' . format ( amount ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'noisered' ) return self
10641	def Re ( L : float , v : float , nu : float ) -> float : return v * L / nu
9846	def resample_factor ( self , factor ) : newlengths = [ ( N - 1 ) * float ( factor ) + 1 for N in self . _len_edges ( ) ] edges = [ numpy . linspace ( start , stop , num = int ( N ) , endpoint = True ) for ( start , stop , N ) in zip ( self . _min_edges ( ) , self . _max_edges ( ) , newlengths ) ] return self . resample ( edges )
7884	def _make_ns_declarations ( declarations , declared_prefixes ) : result = [ ] for namespace , prefix in declarations . items ( ) : if prefix : result . append ( u' xmlns:{0}={1}' . format ( prefix , quoteattr ( namespace ) ) ) else : result . append ( u' xmlns={1}' . format ( prefix , quoteattr ( namespace ) ) ) for d_namespace , d_prefix in declared_prefixes . items ( ) : if ( not prefix and not d_prefix ) or d_prefix == prefix : if namespace != d_namespace : del declared_prefixes [ d_namespace ] return u" " . join ( result )
9197	def get ( self , key , default = _sentinel ) : tup = self . _data . get ( key . lower ( ) ) if tup is not None : return tup [ 1 ] elif default is not _sentinel : return default else : return None
3481	def read_sbml_model ( filename , number = float , f_replace = F_REPLACE , set_missing_bounds = False , ** kwargs ) : try : doc = _get_doc_from_filename ( filename ) return _sbml_to_model ( doc , number = number , f_replace = f_replace , set_missing_bounds = set_missing_bounds , ** kwargs ) except IOError as e : raise e except Exception : LOGGER . error ( traceback . print_exc ( ) ) raise CobraSBMLError ( "Something went wrong reading the SBML model. Most likely the SBML" " model is not valid. Please check that your model is valid using " "the `cobra.io.sbml.validate_sbml_model` function or via the " "online validator at http://sbml.org/validator .\n" "\t`(model, errors) = validate_sbml_model(filename)`" "\nIf the model is valid and cannot be read please open an issue " "at https://github.com/opencobra/cobrapy/issues ." )
7452	def writetofastq ( data , dsort , read ) : if read == 1 : rrr = "R1" else : rrr = "R2" for sname in dsort : handle = os . path . join ( data . dirs . fastqs , "{}_{}_.fastq" . format ( sname , rrr ) ) with open ( handle , 'a' ) as out : out . write ( "" . join ( dsort [ sname ] ) )
11749	def attach_bundle ( self , bundle ) : if not isinstance ( bundle , BlueprintBundle ) : raise IncompatibleBundle ( 'BlueprintBundle object passed to attach_bundle must be of type {0}' . format ( BlueprintBundle ) ) elif len ( bundle . blueprints ) == 0 : raise MissingBlueprints ( "Bundles must contain at least one flask.Blueprint" ) elif self . _bundle_exists ( bundle . path ) : raise ConflictingPath ( "Duplicate bundle path {0}" . format ( bundle . path ) ) elif self . _journey_path == bundle . path == '/' : raise ConflictingPath ( "Bundle path and Journey path cannot both be {0}" . format ( bundle . path ) ) self . _attached_bundles . append ( bundle )
262	def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax
4807	def create_char_dataframe ( words ) : char_dict = [ ] for word in words : for i , char in enumerate ( word ) : if i == 0 : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : True } ) else : char_dict . append ( { 'char' : char , 'type' : CHAR_TYPE_FLATTEN . get ( char , 'o' ) , 'target' : False } ) return pd . DataFrame ( char_dict )
4510	def get ( name = None ) : if name is None or name == 'default' : return _DEFAULT_PALETTE if isinstance ( name , str ) : return PROJECT_PALETTES . get ( name ) or BUILT_IN_PALETTES . get ( name )
1051	def format_exception_only ( etype , value ) : if ( isinstance ( etype , BaseException ) or etype is None or type ( etype ) is str ) : return [ _format_final_exc_line ( etype , value ) ] stype = etype . __name__ if not issubclass ( etype , SyntaxError ) : return [ _format_final_exc_line ( stype , value ) ] lines = [ ] try : msg , ( filename , lineno , offset , badline ) = value . args except Exception : pass else : filename = filename or "<string>" lines . append ( ' File "%s", line %d\n' % ( filename , lineno ) ) if badline is not None : lines . append ( ' %s\n' % badline . strip ( ) ) if offset is not None : caretspace = badline . rstrip ( '\n' ) offset = min ( len ( caretspace ) , offset ) - 1 caretspace = caretspace [ : offset ] . lstrip ( ) caretspace = ( ( c . isspace ( ) and c or ' ' ) for c in caretspace ) lines . append ( ' %s^\n' % '' . join ( caretspace ) ) value = msg lines . append ( _format_final_exc_line ( stype , value ) ) return lines
4334	def norm ( self , db_level = - 3.0 ) : if not is_number ( db_level ) : raise ValueError ( 'db_level must be a number.' ) effect_args = [ 'norm' , '{:f}' . format ( db_level ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'norm' ) return self
12222	def dispatch_first ( self , func ) : self . callees . appendleft ( self . _make_dispatch ( func ) ) return self . _make_wrapper ( func )
13000	def calculate_diagram_ranges ( data ) : data = round_arr_teff_luminosity ( data ) temps = data [ 'temp' ] x_range = [ 1.05 * np . amax ( temps ) , .95 * np . amin ( temps ) ] lums = data [ 'lum' ] y_range = [ .50 * np . amin ( lums ) , 2 * np . amax ( lums ) ] return ( x_range , y_range )
4093	def addBorrowers ( self , * borrowers ) : self . _borrowers . extend ( borrowers ) debug . logger & debug . flagCompiler and debug . logger ( 'current MIB borrower(s): %s' % ', ' . join ( [ str ( x ) for x in self . _borrowers ] ) ) return self
12826	def flush_buffer ( self ) : self . code_builder . add_line ( '{0}.extend([{1}])' , self . result_var , ',' . join ( self . buffered ) ) self . buffered = [ ]
6089	def for_data_and_tracer ( cls , lens_data , tracer , padded_tracer = None ) : if tracer . has_light_profile and not tracer . has_pixelization : return LensProfileFit ( lens_data = lens_data , tracer = tracer , padded_tracer = padded_tracer ) elif not tracer . has_light_profile and tracer . has_pixelization : return LensInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) elif tracer . has_light_profile and tracer . has_pixelization : return LensProfileInversionFit ( lens_data = lens_data , tracer = tracer , padded_tracer = None ) else : raise exc . FittingException ( 'The fit routine did not call a Fit class - check the ' 'properties of the tracer' )
13636	def contentEncoding ( requestHeaders , encoding = None ) : if encoding is None : encoding = b'utf-8' headers = _splitHeaders ( requestHeaders . getRawHeaders ( b'Content-Type' , [ ] ) ) if headers : return headers [ 0 ] [ 1 ] . get ( b'charset' , encoding ) return encoding
8508	def _get_dataset ( self , X , y = None ) : from pylearn2 . datasets import DenseDesignMatrix X = np . asarray ( X ) assert X . ndim > 1 if y is not None : y = self . _get_labels ( y ) if X . ndim == 2 : return DenseDesignMatrix ( X = X , y = y ) return DenseDesignMatrix ( topo_view = X , y = y )
5608	def bounds_to_ranges ( out_bounds = None , in_affine = None , in_shape = None ) : return itertools . chain ( * from_bounds ( * out_bounds , transform = in_affine , height = in_shape [ - 2 ] , width = in_shape [ - 1 ] ) . round_lengths ( pixel_precision = 0 ) . round_offsets ( pixel_precision = 0 ) . toranges ( ) )
1421	def load ( file_object ) : marshaller = JavaObjectUnmarshaller ( file_object ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
3320	def refresh ( self , token , timeout ) : assert token in self . _dict , "Lock must exist" assert timeout == - 1 or timeout > 0 if timeout < 0 or timeout > LockStorageDict . LOCK_TIME_OUT_MAX : timeout = LockStorageDict . LOCK_TIME_OUT_MAX self . _lock . acquire_write ( ) try : lock = self . _dict [ token ] lock [ "timeout" ] = timeout lock [ "expire" ] = time . time ( ) + timeout self . _dict [ token ] = lock self . _flush ( ) finally : self . _lock . release ( ) return lock
1004	def _inferPhase2 ( self ) : self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . colConfidence [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : for i in xrange ( self . cellsPerColumn ) : for s in self . cells [ c ] [ i ] : numActiveSyns = self . _getSegmentActivityLevel ( s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) if numActiveSyns < self . activationThreshold : continue if self . verbosity >= 6 : print "incorporating DC from cell[%d,%d]: " % ( c , i ) , s . debugPrint ( ) dc = s . dutyCycle ( ) self . cellConfidence [ 't' ] [ c , i ] += dc self . colConfidence [ 't' ] [ c ] += dc if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : self . infPredictedState [ 't' ] [ c , i ] = 1 sumConfidences = self . colConfidence [ 't' ] . sum ( ) if sumConfidences > 0 : self . colConfidence [ 't' ] /= sumConfidences self . cellConfidence [ 't' ] /= sumConfidences numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) if numPredictedCols >= 0.5 * self . avgInputDensity : return True else : return False
434	def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" ) else : raise Exception ( "Unknown n_color" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )
9430	def _extract_members ( self , members , targetpath , pwd ) : archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename in members : self . _process_current ( handle , constants . RAR_EXTRACT , targetpath ) else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : raise RuntimeError ( "File CRC Error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle )
3483	def write_sbml_model ( cobra_model , filename , f_replace = F_REPLACE , ** kwargs ) : doc = _model_to_sbml ( cobra_model , f_replace = f_replace , ** kwargs ) if isinstance ( filename , string_types ) : libsbml . writeSBMLToFile ( doc , filename ) elif hasattr ( filename , "write" ) : sbml_str = libsbml . writeSBMLToString ( doc ) filename . write ( sbml_str )
7634	def __load_jams_schema ( ) : schema_file = os . path . join ( SCHEMA_DIR , 'jams_schema.json' ) jams_schema = None with open ( resource_filename ( __name__ , schema_file ) , mode = 'r' ) as fdesc : jams_schema = json . load ( fdesc ) if jams_schema is None : raise JamsError ( 'Unable to load JAMS schema' ) return jams_schema
573	def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )
8298	def readLong ( data ) : high , low = struct . unpack ( ">ll" , data [ 0 : 8 ] ) big = ( long ( high ) << 32 ) + low rest = data [ 8 : ] return ( big , rest )
247	def map_transaction ( txn ) : if isinstance ( txn [ 'sid' ] , dict ) : sid = txn [ 'sid' ] [ 'sid' ] symbol = txn [ 'sid' ] [ 'symbol' ] else : sid = txn [ 'sid' ] symbol = txn [ 'sid' ] return { 'sid' : sid , 'symbol' : symbol , 'price' : txn [ 'price' ] , 'order_id' : txn [ 'order_id' ] , 'amount' : txn [ 'amount' ] , 'commission' : txn [ 'commission' ] , 'dt' : txn [ 'dt' ] }
13145	def remove_near_duplicate_relation ( triples , threshold = 0.97 ) : logging . debug ( "remove duplicate" ) _assert_threshold ( threshold ) duplicate_rel_counter = defaultdict ( list ) relations = set ( ) for t in triples : duplicate_rel_counter [ t . relation ] . append ( f"{t.head} {t.tail}" ) relations . add ( t . relation ) relations = list ( relations ) num_triples = len ( triples ) removal_relation_set = set ( ) for rel , values in duplicate_rel_counter . items ( ) : duplicate_rel_counter [ rel ] = Superminhash ( values ) for i in relations : for j in relations : if i == j or i in removal_relation_set or j in removal_relation_set : continue close_relations = [ i ] if _set_close_to ( duplicate_rel_counter [ i ] , duplicate_rel_counter [ j ] , threshold ) : close_relations . append ( j ) if len ( close_relations ) > 1 : close_relations . pop ( np . random . randint ( len ( close_relations ) ) ) removal_relation_set |= set ( close_relations ) logging . info ( "Removing {} relations: {}" . format ( len ( removal_relation_set ) , str ( removal_relation_set ) ) ) return list ( filterfalse ( lambda x : x . relation in removal_relation_set , triples ) )
11269	def substitute ( prev , * args , ** kw ) : template_obj = string . Template ( * args , ** kw ) for data in prev : yield template_obj . substitute ( data )
3007	def _redirect_with_params ( url_name , * args , ** kwargs ) : url = urlresolvers . reverse ( url_name , args = args ) params = parse . urlencode ( kwargs , True ) return "{0}?{1}" . format ( url , params )
9542	def add_header_check ( self , code = HEADER_CHECK_FAILED , message = MESSAGES [ HEADER_CHECK_FAILED ] ) : t = code , message self . _header_checks . append ( t )
11832	def mate ( self , other ) : "Return a new individual crossing self and other." c = random . randrange ( len ( self . genes ) ) return self . __class__ ( self . genes [ : c ] + other . genes [ c : ] )
10040	def deposit_fetcher ( record_uuid , data ) : return FetchedPID ( provider = DepositProvider , pid_type = DepositProvider . pid_type , pid_value = str ( data [ '_deposit' ] [ 'id' ] ) , )
4707	def power_on ( self , interval = 200 ) : if self . __power_on_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_ON" ) return 1 return self . __press ( self . __power_on_port , interval = interval )
12756	def set_target_angles ( self , angles ) : j = 0 for joint in self . joints : velocities = [ ctrl ( tgt - cur , self . world . dt ) for cur , tgt , ctrl in zip ( joint . angles , angles [ j : j + joint . ADOF ] , joint . controllers ) ] joint . velocities = velocities j += joint . ADOF
12434	def redirect ( cls , request , response ) : if cls . meta . legacy_redirect : if request . method in ( 'GET' , 'HEAD' , ) : response . status = http . client . MOVED_PERMANENTLY else : response . status = http . client . TEMPORARY_REDIRECT else : response . status = http . client . PERMANENT_REDIRECT response . close ( )
7234	def tilemap ( self , query , styles = { } , bbox = [ - 180 , - 90 , 180 , 90 ] , zoom = 16 , api_key = os . environ . get ( 'MAPBOX_API_KEY' , None ) , image = None , image_bounds = None , index = "vector-user-provided" , name = "GBDX_Task_Output" , ** kwargs ) : try : from IPython . display import display except : print ( "IPython is required to produce maps." ) return assert api_key is not None , "No Mapbox API Key found. You can either pass in a token or set the MAPBOX_API_KEY environment variable." wkt = box ( * bbox ) . wkt features = self . query ( wkt , query , index = index ) union = cascaded_union ( [ shape ( f [ 'geometry' ] ) for f in features ] ) lon , lat = union . centroid . coords [ 0 ] url = 'https://vector.geobigdata.io/insight-vector/api/mvt/{z}/{x}/{y}?' url += 'q={}&index={}' . format ( query , index ) if styles is not None and not isinstance ( styles , list ) : styles = [ styles ] map_id = "map_{}" . format ( str ( int ( time . time ( ) ) ) ) map_data = VectorTileLayer ( url , source_name = name , styles = styles , ** kwargs ) image_layer = self . _build_image_layer ( image , image_bounds ) template = BaseTemplate ( map_id , ** { "lat" : lat , "lon" : lon , "zoom" : zoom , "datasource" : json . dumps ( map_data . datasource ) , "layers" : json . dumps ( map_data . layers ) , "image_layer" : image_layer , "mbkey" : api_key , "token" : self . gbdx_connection . access_token } ) template . inject ( )
7085	def precess_coordinates ( ra , dec , epoch_one , epoch_two , jd = None , mu_ra = 0.0 , mu_dec = 0.0 , outscalar = False ) : raproc , decproc = np . radians ( ra ) , np . radians ( dec ) if ( ( mu_ra != 0.0 ) and ( mu_dec != 0.0 ) and jd ) : jd_epoch_one = JD2000 + ( epoch_one - epoch_two ) * 365.25 raproc = ( raproc + ( jd - jd_epoch_one ) * mu_ra * MAS_P_YR_TO_RAD_P_DAY / np . cos ( decproc ) ) decproc = decproc + ( jd - jd_epoch_one ) * mu_dec * MAS_P_YR_TO_RAD_P_DAY ca = np . cos ( raproc ) cd = np . cos ( decproc ) sa = np . sin ( raproc ) sd = np . sin ( decproc ) if epoch_one != epoch_two : t1 = 1.0e-3 * ( epoch_two - epoch_one ) t2 = 1.0e-3 * ( epoch_one - 2000.0 ) a = ( t1 * ARCSEC_TO_RADIANS * ( 23062.181 + t2 * ( 139.656 + 0.0139 * t2 ) + t1 * ( 30.188 - 0.344 * t2 + 17.998 * t1 ) ) ) b = t1 * t1 * ARCSEC_TO_RADIANS * ( 79.280 + 0.410 * t2 + 0.205 * t1 ) + a c = ( ARCSEC_TO_RADIANS * t1 * ( 20043.109 - t2 * ( 85.33 + 0.217 * t2 ) + t1 * ( - 42.665 - 0.217 * t2 - 41.833 * t2 ) ) ) sina , sinb , sinc = np . sin ( a ) , np . sin ( b ) , np . sin ( c ) cosa , cosb , cosc = np . cos ( a ) , np . cos ( b ) , np . cos ( c ) precmatrix = np . matrix ( [ [ cosa * cosb * cosc - sina * sinb , sina * cosb + cosa * sinb * cosc , cosa * sinc ] , [ - cosa * sinb - sina * cosb * cosc , cosa * cosb - sina * sinb * cosc , - sina * sinc ] , [ - cosb * sinc , - sinb * sinc , cosc ] ] ) precmatrix = precmatrix . transpose ( ) x = ( np . matrix ( [ cd * ca , cd * sa , sd ] ) ) . transpose ( ) x2 = precmatrix * x outra = np . arctan2 ( x2 [ 1 ] , x2 [ 0 ] ) outdec = np . arcsin ( x2 [ 2 ] ) outradeg = np . rad2deg ( outra ) outdecdeg = np . rad2deg ( outdec ) if outradeg < 0.0 : outradeg = outradeg + 360.0 if outscalar : return float ( outradeg ) , float ( outdecdeg ) else : return outradeg , outdecdeg else : return np . degrees ( raproc ) , np . degrees ( decproc )
3611	def delete_async ( self , url , name , callback = None , params = None , headers = None ) : if not name : name = '' params = params or { } headers = headers or { } endpoint = self . _build_endpoint_url ( url , name ) self . _authenticate ( params , headers ) process_pool . apply_async ( make_delete_request , args = ( endpoint , params , headers ) , callback = callback )
4596	def pid_context ( pid_filename = None ) : pid_filename = pid_filename or DEFAULT_PID_FILENAME if os . path . exists ( pid_filename ) : contents = open ( pid_filename ) . read ( 16 ) log . warning ( 'pid_filename %s already exists with contents %s' , pid_filename , contents ) with open ( pid_filename , 'w' ) as fp : fp . write ( str ( os . getpid ( ) ) ) fp . write ( '\n' ) try : yield finally : try : os . remove ( pid_filename ) except Exception as e : log . error ( 'Got an exception %s deleting the pid_filename %s' , e , pid_filename )
11434	def _tag_matches_pattern ( tag , pattern ) : for char1 , char2 in zip ( tag , pattern ) : if char2 not in ( '%' , char1 ) : return False return True
12664	def union_mask ( filelist ) : firstimg = check_img ( filelist [ 0 ] ) mask = np . zeros_like ( firstimg . get_data ( ) ) try : for volf in filelist : roiimg = check_img ( volf ) check_img_compatibility ( firstimg , roiimg ) mask += get_img_data ( roiimg ) except Exception as exc : raise ValueError ( 'Error joining mask {} and {}.' . format ( repr_imgs ( firstimg ) , repr_imgs ( volf ) ) ) from exc else : return as_ndarray ( mask > 0 , dtype = bool )
9460	def conference_unmute ( self , call_params ) : path = '/' + self . api_version + '/ConferenceUnmute/' method = 'POST' return self . request ( path , method , call_params )
9960	def get_object ( self , name ) : parts = name . split ( "." ) model_name = parts . pop ( 0 ) return self . models [ model_name ] . get_object ( "." . join ( parts ) )
9843	def __tokenize ( self , string ) : for m in self . dx_regex . finditer ( string . strip ( ) ) : code = m . lastgroup text = m . group ( m . lastgroup ) tok = Token ( code , text ) if not tok . iscode ( 'WHITESPACE' ) : self . tokens . append ( tok )
4824	def get_course_modes ( self , course_id ) : details = self . get_course_details ( course_id ) modes = details . get ( 'course_modes' , [ ] ) return self . _sort_course_modes ( [ mode for mode in modes if mode [ 'slug' ] not in EXCLUDED_COURSE_MODES ] )
2567	def check_tracking_enabled ( self ) : track = True test = False testvar = str ( os . environ . get ( "PARSL_TESTING" , 'None' ) ) . lower ( ) if testvar == 'true' : test = True if not self . config . usage_tracking : track = False envvar = str ( os . environ . get ( "PARSL_TRACKING" , True ) ) . lower ( ) if envvar == "false" : track = False return test , track
5	def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )
3332	def init_logging ( config ) : verbose = config . get ( "verbose" , 3 ) enable_loggers = config . get ( "enable_loggers" , [ ] ) if enable_loggers is None : enable_loggers = [ ] logger_date_format = config . get ( "logger_date_format" , "%Y-%m-%d %H:%M:%S" ) logger_format = config . get ( "logger_format" , "%(asctime)s.%(msecs)03d - <%(thread)d> %(name)-27s %(levelname)-8s: %(message)s" , ) formatter = logging . Formatter ( logger_format , logger_date_format ) consoleHandler = logging . StreamHandler ( sys . stdout ) consoleHandler . setFormatter ( formatter ) logger = logging . getLogger ( BASE_LOGGER_NAME ) if verbose >= 4 : logger . setLevel ( logging . DEBUG ) elif verbose == 3 : logger . setLevel ( logging . INFO ) elif verbose == 2 : logger . setLevel ( logging . WARN ) elif verbose == 1 : logger . setLevel ( logging . ERROR ) else : logger . setLevel ( logging . CRITICAL ) logger . propagate = False for hdlr in logger . handlers [ : ] : try : hdlr . flush ( ) hdlr . close ( ) except Exception : pass logger . removeHandler ( hdlr ) logger . addHandler ( consoleHandler ) if verbose >= 3 : for e in enable_loggers : if not e . startswith ( BASE_LOGGER_NAME + "." ) : e = BASE_LOGGER_NAME + "." + e lg = logging . getLogger ( e . strip ( ) ) lg . setLevel ( logging . DEBUG )
8132	def export ( self , filename ) : self . flatten ( ) self . layers [ 1 ] . img . save ( filename ) return filename
1448	def parse ( version ) : match = _REGEX . match ( version ) if match is None : raise ValueError ( '%s is not valid SemVer string' % version ) verinfo = match . groupdict ( ) verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) return verinfo
8344	def renderContents ( self , encoding = DEFAULT_OUTPUT_ENCODING , prettyPrint = False , indentLevel = 0 ) : s = [ ] for c in self : text = None if isinstance ( c , NavigableString ) : text = c . __str__ ( encoding ) elif isinstance ( c , Tag ) : s . append ( c . __str__ ( encoding , prettyPrint , indentLevel ) ) if text and prettyPrint : text = text . strip ( ) if text : if prettyPrint : s . append ( " " * ( indentLevel - 1 ) ) s . append ( text ) if prettyPrint : s . append ( "\n" ) return '' . join ( s )
3160	def update ( self , campaign_id , data ) : self . campaign_id = campaign_id return self . _mc_client . _put ( url = self . _build_path ( campaign_id , 'content' ) , data = data )
7545	def calculate_depths ( data , samples , lbview ) : start = time . time ( ) printstr = " calculating depths | {} | s5 |" recaljobs = { } maxlens = [ ] for sample in samples : recaljobs [ sample . name ] = lbview . apply ( recal_hidepth , * ( data , sample ) ) while 1 : ready = [ i . ready ( ) for i in recaljobs . values ( ) ] elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( ready ) , sum ( ready ) , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if len ( ready ) == sum ( ready ) : print ( "" ) break modsamples = [ ] for sample in samples : if not recaljobs [ sample . name ] . successful ( ) : LOGGER . error ( " sample %s failed: %s" , sample . name , recaljobs [ sample . name ] . exception ( ) ) else : modsample , _ , maxlen , _ , _ = recaljobs [ sample . name ] . result ( ) modsamples . append ( modsample ) maxlens . append ( maxlen ) data . _hackersonly [ "max_fragment_length" ] = int ( max ( maxlens ) ) + 4 return samples
7272	def register_operators ( * operators ) : def validate ( operator ) : if isoperator ( operator ) : return True raise NotImplementedError ( 'invalid operator: {}' . format ( operator ) ) def register ( operator ) : for name in operator . operators : if name in Engine . operators : raise ValueError ( 'operator name "{}" from {} is already ' 'in use by other operator' . format ( name , operator . __name__ ) ) Engine . operators [ name ] = operator [ register ( operator ) for operator in operators if validate ( operator ) ]
5368	def _get_storage_service ( credentials ) : if credentials is None : credentials = oauth2client . client . GoogleCredentials . get_application_default ( ) return discovery . build ( 'storage' , 'v1' , credentials = credentials )
3726	def Pc ( CASRN , AvailableMethods = False , Method = None , IgnoreMethods = [ SURF ] ) : r def list_methods ( ) : methods = [ ] if CASRN in _crit_IUPAC . index and not np . isnan ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) : methods . append ( IUPAC ) if CASRN in _crit_Matthews . index and not np . isnan ( _crit_Matthews . at [ CASRN , 'Pc' ] ) : methods . append ( MATTHEWS ) if CASRN in _crit_CRC . index and not np . isnan ( _crit_CRC . at [ CASRN , 'Pc' ] ) : methods . append ( CRC ) if CASRN in _crit_PSRKR4 . index and not np . isnan ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) : methods . append ( PSRK ) if CASRN in _crit_PassutDanner . index and not np . isnan ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) : methods . append ( PD ) if CASRN in _crit_Yaws . index and not np . isnan ( _crit_Yaws . at [ CASRN , 'Pc' ] ) : methods . append ( YAWS ) if CASRN : methods . append ( SURF ) if IgnoreMethods : for Method in IgnoreMethods : if Method in methods : methods . remove ( Method ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == IUPAC : _Pc = float ( _crit_IUPAC . at [ CASRN , 'Pc' ] ) elif Method == MATTHEWS : _Pc = float ( _crit_Matthews . at [ CASRN , 'Pc' ] ) elif Method == CRC : _Pc = float ( _crit_CRC . at [ CASRN , 'Pc' ] ) elif Method == PSRK : _Pc = float ( _crit_PSRKR4 . at [ CASRN , 'Pc' ] ) elif Method == PD : _Pc = float ( _crit_PassutDanner . at [ CASRN , 'Pc' ] ) elif Method == YAWS : _Pc = float ( _crit_Yaws . at [ CASRN , 'Pc' ] ) elif Method == SURF : _Pc = third_property ( CASRN = CASRN , P = True ) elif Method == NONE : return None else : raise Exception ( 'Failure in in function' ) return _Pc
8613	def delete_volume ( self , datacenter_id , volume_id ) : response = self . _perform_request ( url = '/datacenters/%s/volumes/%s' % ( datacenter_id , volume_id ) , method = 'DELETE' ) return response
971	def _copyAllocatedStates ( self ) : if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )
5602	def create_app ( mapchete_files = None , zoom = None , bounds = None , single_input_file = None , mode = "continue" , debug = None ) : from flask import Flask , render_template_string app = Flask ( __name__ ) mapchete_processes = { os . path . splitext ( os . path . basename ( mapchete_file ) ) [ 0 ] : mapchete . open ( mapchete_file , zoom = zoom , bounds = bounds , single_input_file = single_input_file , mode = mode , with_cache = True , debug = debug ) for mapchete_file in mapchete_files } mp = next ( iter ( mapchete_processes . values ( ) ) ) pyramid_type = mp . config . process_pyramid . grid pyramid_srid = mp . config . process_pyramid . crs . to_epsg ( ) process_bounds = "," . join ( [ str ( i ) for i in mp . config . bounds_at_zoom ( ) ] ) grid = "g" if pyramid_srid == 3857 else "WGS84" web_pyramid = BufferedTilePyramid ( pyramid_type ) @ app . route ( '/' , methods = [ 'GET' ] ) def index ( ) : return render_template_string ( pkgutil . get_data ( 'mapchete.static' , 'index.html' ) . decode ( "utf-8" ) , srid = pyramid_srid , process_bounds = process_bounds , is_mercator = ( pyramid_srid == 3857 ) , process_names = mapchete_processes . keys ( ) ) @ app . route ( "/" . join ( [ "" , "wmts_simple" , "1.0.0" , "<string:mp_name>" , "default" , grid , "<int:zoom>" , "<int:row>" , "<int:col>.<string:file_ext>" ] ) , methods = [ 'GET' ] ) def get ( mp_name , zoom , row , col , file_ext ) : logger . debug ( "received tile (%s, %s, %s) for process %s" , zoom , row , col , mp_name ) return _tile_response ( mapchete_processes [ mp_name ] , web_pyramid . tile ( zoom , row , col ) , debug ) return app
3874	def _add_conversation ( self , conversation , events = [ ] , event_cont_token = None ) : conv_id = conversation . conversation_id . id logger . debug ( 'Adding new conversation: {}' . format ( conv_id ) ) conv = Conversation ( self . _client , self . _user_list , conversation , events , event_cont_token ) self . _conv_dict [ conv_id ] = conv return conv
9057	def economic_qs_zeros ( n ) : Q0 = empty ( ( n , 0 ) ) Q1 = eye ( n ) S0 = empty ( 0 ) return ( ( Q0 , Q1 ) , S0 )
7036	def cone_search ( lcc_server , center_ra , center_decl , radiusarcmin = 5.0 , result_visibility = 'unlisted' , email_when_done = False , collections = None , columns = None , filters = None , sortspec = None , samplespec = None , limitspec = None , download_data = True , outdir = None , maxtimeout = 300.0 , refresh = 15.0 ) : coords = '%.5f %.5f %.1f' % ( center_ra , center_decl , radiusarcmin ) params = { 'coords' : coords } if collections : params [ 'collections' ] = collections if columns : params [ 'columns' ] = columns if filters : params [ 'filters' ] = filters if sortspec : params [ 'sortspec' ] = json . dumps ( [ sortspec ] ) if samplespec : params [ 'samplespec' ] = int ( samplespec ) if limitspec : params [ 'limitspec' ] = int ( limitspec ) params [ 'visibility' ] = result_visibility params [ 'emailwhendone' ] = email_when_done if email_when_done : download_data = False have_apikey , apikey , expires = check_existing_apikey ( lcc_server ) if not have_apikey : apikey , expires = get_new_apikey ( lcc_server ) api_url = '%s/api/conesearch' % lcc_server searchresult = submit_post_searchquery ( api_url , params , apikey ) status = searchresult [ 0 ] if download_data : if status == 'ok' : LOGINFO ( 'query complete, downloading associated data...' ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if pkl : return searchresult [ 1 ] , csv , lczip , pkl else : return searchresult [ 1 ] , csv , lczip elif status == 'background' : LOGINFO ( 'query is not yet complete, ' 'waiting up to %.1f minutes, ' 'updates every %s seconds (hit Ctrl+C to cancel)...' % ( maxtimeout / 60.0 , refresh ) ) timewaited = 0.0 while timewaited < maxtimeout : try : time . sleep ( refresh ) csv , lczip , pkl = retrieve_dataset_files ( searchresult , outdir = outdir , apikey = apikey ) if ( csv and os . path . exists ( csv ) and lczip and os . path . exists ( lczip ) ) : LOGINFO ( 'all dataset products collected' ) return searchresult [ 1 ] , csv , lczip timewaited = timewaited + refresh except KeyboardInterrupt : LOGWARNING ( 'abandoned wait for downloading data' ) return searchresult [ 1 ] , None , None LOGERROR ( 'wait timed out.' ) return searchresult [ 1 ] , None , None else : LOGERROR ( 'could not download the data for this query result' ) return searchresult [ 1 ] , None , None else : return searchresult [ 1 ] , None , None
11622	def _unrecognised ( chr ) : if options [ 'handleUnrecognised' ] == UNRECOGNISED_ECHO : return chr elif options [ 'handleUnrecognised' ] == UNRECOGNISED_SUBSTITUTE : return options [ 'substituteChar' ] else : raise ( KeyError , chr )
7801	def handle_authorized ( self , event ) : stream = event . stream if not stream : return if not stream . initiator : return if stream . features is None : return element = stream . features . find ( SESSION_TAG ) if element is None : return logger . debug ( "Establishing IM session" ) stanza = Iq ( stanza_type = "set" ) payload = XMLPayload ( ElementTree . Element ( SESSION_TAG ) ) stanza . set_payload ( payload ) self . stanza_processor . set_response_handlers ( stanza , self . _session_success , self . _session_error ) stream . send ( stanza )
41	def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )
3686	def solve_T ( self , P , V , quick = True ) : def to_solve ( T ) : a_alpha = self . a_alpha_and_derivatives ( T , full = False ) P_calc = R * T / ( V - self . b ) - a_alpha / ( V * V + self . delta * V + self . epsilon ) return P_calc - P return newton ( to_solve , self . Tc * 0.5 )
287	def show_perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' , live_start_date = None , bootstrap = False , header_rows = None ) : if bootstrap : perf_func = timeseries . perf_stats_bootstrap else : perf_func = timeseries . perf_stats perf_stats_all = perf_func ( returns , factor_returns = factor_returns , positions = positions , transactions = transactions , turnover_denom = turnover_denom ) date_rows = OrderedDict ( ) if len ( returns . index ) > 0 : date_rows [ 'Start date' ] = returns . index [ 0 ] . strftime ( '%Y-%m-%d' ) date_rows [ 'End date' ] = returns . index [ - 1 ] . strftime ( '%Y-%m-%d' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) returns_is = returns [ returns . index < live_start_date ] returns_oos = returns [ returns . index >= live_start_date ] positions_is = None positions_oos = None transactions_is = None transactions_oos = None if positions is not None : positions_is = positions [ positions . index < live_start_date ] positions_oos = positions [ positions . index >= live_start_date ] if transactions is not None : transactions_is = transactions [ ( transactions . index < live_start_date ) ] transactions_oos = transactions [ ( transactions . index > live_start_date ) ] perf_stats_is = perf_func ( returns_is , factor_returns = factor_returns , positions = positions_is , transactions = transactions_is , turnover_denom = turnover_denom ) perf_stats_oos = perf_func ( returns_oos , factor_returns = factor_returns , positions = positions_oos , transactions = transactions_oos , turnover_denom = turnover_denom ) if len ( returns . index ) > 0 : date_rows [ 'In-sample months' ] = int ( len ( returns_is ) / APPROX_BDAYS_PER_MONTH ) date_rows [ 'Out-of-sample months' ] = int ( len ( returns_oos ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . concat ( OrderedDict ( [ ( 'In-sample' , perf_stats_is ) , ( 'Out-of-sample' , perf_stats_oos ) , ( 'All' , perf_stats_all ) , ] ) , axis = 1 ) else : if len ( returns . index ) > 0 : date_rows [ 'Total months' ] = int ( len ( returns ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . DataFrame ( perf_stats_all , columns = [ 'Backtest' ] ) for column in perf_stats . columns : for stat , value in perf_stats [ column ] . iteritems ( ) : if stat in STAT_FUNCS_PCT : perf_stats . loc [ stat , column ] = str ( np . round ( value * 100 , 1 ) ) + '%' if header_rows is None : header_rows = date_rows else : header_rows = OrderedDict ( header_rows ) header_rows . update ( date_rows ) utils . print_table ( perf_stats , float_format = '{0:.2f}' . format , header_rows = header_rows , )
2634	def status ( self ) : if self . provider : status = self . provider . status ( self . engines ) else : status = [ ] return status
8588	def start_server ( self , datacenter_id , server_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/start' % ( datacenter_id , server_id ) , method = 'POST-ACTION' ) return response
9000	def unique ( iterables ) : included_elements = set ( ) def included ( element ) : result = element in included_elements included_elements . add ( element ) return result return [ element for elements in iterables for element in elements if not included ( element ) ]
10729	def _handle_base_case ( klass , symbol ) : def the_func ( value , variant = 0 ) : ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( 0 , variant ) return ( klass ( value , variant_level = obj_level ) , func_level ) return lambda : ( the_func , symbol )
1556	def get_out_streamids ( self ) : if self . outputs is None : return set ( ) if not isinstance ( self . outputs , ( list , tuple ) ) : raise TypeError ( "Argument to outputs must be either list or tuple, given: %s" % str ( type ( self . outputs ) ) ) ret_lst = [ ] for output in self . outputs : if not isinstance ( output , ( str , Stream ) ) : raise TypeError ( "Outputs must be a list of strings or Streams, given: %s" % str ( output ) ) ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) return set ( ret_lst )
1325	def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : alphas = a . gradient ( image , target ) * mask if fast : betas = - np . ones_like ( alphas ) else : betas = np . sum ( [ a . gradient ( image , label ) * mask - alphas for label in labels ] , 0 ) salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) idx = np . argmin ( salmap ) idx = np . unravel_index ( idx , mask . shape ) pix_sign = np . sign ( alphas ) [ idx ] return idx , pix_sign
4775	def contains_duplicates ( self ) : try : if len ( self . val ) != len ( set ( self . val ) ) : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain duplicates, but did not.' % self . val )
2289	def run ( self , data , train_epochs = 1000 , test_epochs = 1000 , verbose = None , idx = 0 , lr = 0.01 , ** kwargs ) : verbose = SETTINGS . get_default ( verbose = verbose ) optim = th . optim . Adam ( self . parameters ( ) , lr = lr ) self . score . zero_ ( ) with trange ( train_epochs + test_epochs , disable = not verbose ) as t : for epoch in t : optim . zero_grad ( ) generated_data = self . forward ( ) mmd = self . criterion ( generated_data , data ) if not epoch % 200 : t . set_postfix ( idx = idx , epoch = epoch , loss = mmd . item ( ) ) mmd . backward ( ) optim . step ( ) if epoch >= test_epochs : self . score . add_ ( mmd . data ) return self . score . cpu ( ) . numpy ( ) / test_epochs
6430	def sim ( self , src , tar ) : def _lcsstr_stl ( src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , src_longest , tar_longest = 0 , 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] src_longest = i tar_longest = j else : lengths [ i , j ] = 0 return src_longest - longest , tar_longest - longest , longest def _sstr_matches ( src , tar ) : src_start , tar_start , length = _lcsstr_stl ( src , tar ) if length == 0 : return 0 return ( _sstr_matches ( src [ : src_start ] , tar [ : tar_start ] ) + length + _sstr_matches ( src [ src_start + length : ] , tar [ tar_start + length : ] ) ) if src == tar : return 1.0 elif not src or not tar : return 0.0 return 2 * _sstr_matches ( src , tar ) / ( len ( src ) + len ( tar ) )
6121	def elliptical_annular ( cls , shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre = ( 0.0 , 0.0 ) , invert = False ) : mask = mask_util . mask_elliptical_annular_from_shape_pixel_scale_and_radius ( shape , pixel_scale , inner_major_axis_radius_arcsec , inner_axis_ratio , inner_phi , outer_major_axis_radius_arcsec , outer_axis_ratio , outer_phi , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
7287	def set_form_fields ( self , form_field_dict , parent_key = None , field_type = None ) : for form_key , field_value in form_field_dict . items ( ) : form_key = make_key ( parent_key , form_key ) if parent_key is not None else form_key if isinstance ( field_value , tuple ) : set_list_class = False base_key = form_key if ListField in ( field_value . field_type , field_type ) : if parent_key is None or ListField == field_value . field_type : if field_type != EmbeddedDocumentField : field_value . widget . attrs [ 'class' ] += ' listField {0}' . format ( form_key ) set_list_class = True else : field_value . widget . attrs [ 'class' ] += ' listField' list_keys = [ field_key for field_key in self . form . fields . keys ( ) if has_digit ( field_key ) ] key_int = 0 while form_key in list_keys : key_int += 1 form_key = make_key ( form_key , key_int ) if parent_key is not None : valid_base_keys = [ model_key for model_key in self . model_map_dict . keys ( ) if not model_key . startswith ( "_" ) ] while base_key not in valid_base_keys and base_key : base_key = make_key ( base_key , exclude_last_string = True ) embedded_key_class = None if set_list_class : field_value . widget . attrs [ 'class' ] += " listField" . format ( base_key ) embedded_key_class = make_key ( field_key , exclude_last_string = True ) field_value . widget . attrs [ 'class' ] += " embeddedField" if base_key == parent_key : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( base_key ) else : field_value . widget . attrs [ 'class' ] += ' {0} {1}' . format ( base_key , parent_key ) if embedded_key_class is not None : field_value . widget . attrs [ 'class' ] += ' {0}' . format ( embedded_key_class ) default_value = self . get_field_value ( form_key ) if isinstance ( default_value , list ) and len ( default_value ) > 0 : key_index = int ( form_key . split ( "_" ) [ - 1 ] ) new_base_key = make_key ( form_key , exclude_last_string = True ) for list_value in default_value : list_widget = deepcopy ( field_value . widget ) new_key = make_key ( new_base_key , six . text_type ( key_index ) ) list_widget . attrs [ 'class' ] += " {0}" . format ( make_key ( base_key , key_index ) ) self . set_form_field ( list_widget , field_value . document_field , new_key , list_value ) key_index += 1 else : self . set_form_field ( field_value . widget , field_value . document_field , form_key , default_value ) elif isinstance ( field_value , dict ) : self . set_form_fields ( field_value , form_key , field_value . get ( "_field_type" , None ) )
8189	def eigenvector_centrality ( self , normalized = True , reversed = True , rating = { } , start = None , iterations = 100 , tolerance = 0.0001 ) : ec = proximity . eigenvector_centrality ( self , normalized , reversed , rating , start , iterations , tolerance ) for id , w in ec . iteritems ( ) : self [ id ] . _eigenvalue = w return ec
7375	def code ( self , code ) : def decorator ( exception ) : self [ code ] = exception return exception return decorator
2579	def cleanup ( self ) : logger . info ( "DFK cleanup initiated" ) if self . cleanup_called : raise Exception ( "attempt to clean up DFK when it has already been cleaned-up" ) self . cleanup_called = True self . log_task_states ( ) if self . checkpoint_mode is not None : self . checkpoint ( ) if self . _checkpoint_timer : logger . info ( "Stopping checkpoint timer" ) self . _checkpoint_timer . close ( ) self . usage_tracker . send_message ( ) self . usage_tracker . close ( ) logger . info ( "Terminating flow_control and strategy threads" ) self . flowcontrol . close ( ) for executor in self . executors . values ( ) : if executor . managed : if executor . scaling_enabled : job_ids = executor . provider . resources . keys ( ) executor . scale_in ( len ( job_ids ) ) executor . shutdown ( ) self . time_completed = datetime . datetime . now ( ) if self . monitoring : self . monitoring . send ( MessageType . WORKFLOW_INFO , { 'tasks_failed_count' : self . tasks_failed_count , 'tasks_completed_count' : self . tasks_completed_count , "time_began" : self . time_began , 'time_completed' : self . time_completed , 'workflow_duration' : ( self . time_completed - self . time_began ) . total_seconds ( ) , 'run_id' : self . run_id , 'rundir' : self . run_dir } ) self . monitoring . close ( ) logger . info ( "DFK cleanup complete" )
8475	def _getClassInstance ( path , args = None ) : if not path . endswith ( ".py" ) : return None if args is None : args = { } classname = AtomShieldsScanner . _getClassName ( path ) basename = os . path . basename ( path ) . replace ( ".py" , "" ) sys . path . append ( os . path . dirname ( path ) ) try : mod = __import__ ( basename , globals ( ) , locals ( ) , [ classname ] , - 1 ) class_ = getattr ( mod , classname ) instance = class_ ( ** args ) except Exception as e : AtomShieldsScanner . _debug ( "[!] %s" % e ) return None finally : sys . path . remove ( os . path . dirname ( path ) ) return instance
1475	def _get_streaming_processes ( self ) : retval = { } instance_plans = self . _get_instance_plans ( self . packing_plan , self . shard ) instance_info = [ ] for instance_plan in instance_plans : global_task_id = instance_plan . task_id component_index = instance_plan . component_index component_name = instance_plan . component_name instance_id = "container_%s_%s_%d" % ( str ( self . shard ) , component_name , global_task_id ) instance_info . append ( ( instance_id , component_name , global_task_id , component_index ) ) stmgr_cmd_lst = [ self . stmgr_binary , '--topology_name=%s' % self . topology_name , '--topology_id=%s' % self . topology_id , '--topologydefn_file=%s' % self . topology_defn_file , '--zkhostportlist=%s' % self . state_manager_connection , '--zkroot=%s' % self . state_manager_root , '--stmgr_id=%s' % self . stmgr_ids [ self . shard ] , '--instance_ids=%s' % ',' . join ( map ( lambda x : x [ 0 ] , instance_info ) ) , '--myhost=%s' % self . master_host , '--data_port=%s' % str ( self . master_port ) , '--local_data_port=%s' % str ( self . tmaster_controller_port ) , '--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , '--shell_port=%s' % str ( self . shell_port ) , '--config_file=%s' % self . heron_internals_config_file , '--override_config_file=%s' % self . override_config_file , '--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) , '--ckptmgr_id=%s' % self . ckptmgr_ids [ self . shard ] , '--metricscachemgr_mode=%s' % self . metricscache_manager_mode . lower ( ) ] stmgr_env = self . shell_env . copy ( ) if self . shell_env is not None else { } stmgr_cmd = Command ( stmgr_cmd_lst , stmgr_env ) if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : stmgr_cmd . env . update ( { 'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 'HEAPCHECK' : "normal" } ) retval [ self . stmgr_ids [ self . shard ] ] = stmgr_cmd retval [ self . metricsmgr_ids [ self . shard ] ] = self . _get_metricsmgr_cmd ( self . metricsmgr_ids [ self . shard ] , self . metrics_sinks_config_file , self . metrics_manager_port ) if self . is_stateful_topology : retval . update ( self . _get_ckptmgr_process ( ) ) if self . pkg_type == 'jar' or self . pkg_type == 'tar' : retval . update ( self . _get_java_instance_cmd ( instance_info ) ) elif self . pkg_type == 'pex' : retval . update ( self . _get_python_instance_cmd ( instance_info ) ) elif self . pkg_type == 'so' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) elif self . pkg_type == 'dylib' : retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) else : raise ValueError ( "Unrecognized package type: %s" % self . pkg_type ) return retval
1805	def SETA ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) == False , 1 , 0 ) )
10437	def getapplist ( self ) : app_list = [ ] self . _update_apps ( ) for gui in self . _running_apps : name = gui . localizedName ( ) try : name = unicode ( name ) except NameError : name = str ( name ) except UnicodeEncodeError : pass app_list . append ( name ) return list ( set ( app_list ) )
467	def sample ( a = None , temperature = 1.0 ) : if a is None : raise Exception ( "a : list of float" ) b = np . copy ( a ) try : if temperature == 1 : return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) if temperature is None : return np . argmax ( a ) else : a = np . log ( a ) / temperature a = np . exp ( a ) / np . sum ( np . exp ( a ) ) return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) except Exception : message = "For large vocabulary_size, choice a higher temperature\ to avoid log error. Hint : use ``sample_top``. " warnings . warn ( message , Warning ) return np . argmax ( np . random . multinomial ( 1 , b , 1 ) )
6322	def resolve_loader ( self , meta : ProgramDescription ) : if not meta . loader : meta . loader = 'single' if meta . path else 'separate' for loader_cls in self . _loaders : if loader_cls . name == meta . loader : meta . loader_cls = loader_cls break else : raise ImproperlyConfigured ( ( "Program {} has no loader class registered." "Check PROGRAM_LOADERS or PROGRAM_DIRS" ) . format ( meta . path ) )
12929	def as_dict ( self ) : self_as_dict = { 'chrom' : self . chrom , 'start' : self . start , 'ref_allele' : self . ref_allele , 'alt_alleles' : self . alt_alleles , 'alleles' : [ x . as_dict ( ) for x in self . alleles ] } try : self_as_dict [ 'info' ] = self . info except AttributeError : pass return self_as_dict
13581	def apply_types ( use_types , guess_type , line ) : new_line = { } for k , v in line . items ( ) : if use_types . has_key ( k ) : new_line [ k ] = force_type ( use_types [ k ] , v ) elif guess_type : new_line [ k ] = determine_type ( v ) else : new_line [ k ] = v return new_line
1664	def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) if match : error ( filename , linenum , 'build/explicit_make_pair' , 4 , 'For C++11-compatibility, omit template arguments from make_pair' ' OR use pair directly OR if appropriate, construct a pair directly' )
5202	def create_connection ( port = _PORT_ , timeout = _TIMEOUT_ , restart = False ) : if _CON_SYM_ in globals ( ) : if not isinstance ( globals ( ) [ _CON_SYM_ ] , pdblp . BCon ) : del globals ( ) [ _CON_SYM_ ] if ( _CON_SYM_ in globals ( ) ) and ( not restart ) : con = globals ( ) [ _CON_SYM_ ] if getattr ( con , '_session' ) . start ( ) : con . start ( ) return con , False else : con = pdblp . BCon ( port = port , timeout = timeout ) globals ( ) [ _CON_SYM_ ] = con con . start ( ) return con , True
13622	def many ( func ) : def _many ( result ) : if _isSequenceTypeNotText ( result ) : return map ( func , result ) return [ ] return maybe ( _many , default = [ ] )
3423	def resettable ( f ) : def wrapper ( self , new_value ) : context = get_context ( self ) if context : old_value = getattr ( self , f . __name__ ) if old_value == new_value : return context ( partial ( f , self , old_value ) ) f ( self , new_value ) return wrapper
3079	def get ( http , path , root = METADATA_ROOT , recursive = None ) : url = urlparse . urljoin ( root , path ) url = _helpers . _add_query_parameter ( url , 'recursive' , recursive ) response , content = transport . request ( http , url , headers = METADATA_HEADERS ) if response . status == http_client . OK : decoded = _helpers . _from_bytes ( content ) if response [ 'content-type' ] == 'application/json' : return json . loads ( decoded ) else : return decoded else : raise http_client . HTTPException ( 'Failed to retrieve {0} from the Google Compute Engine' 'metadata service. Response:\n{1}' . format ( url , response ) )
3696	def Watson ( T , Hvap_ref , T_Ref , Tc , exponent = 0.38 ) : Tr = T / Tc Trefr = T_Ref / Tc H2 = Hvap_ref * ( ( 1 - Tr ) / ( 1 - Trefr ) ) ** exponent return H2
10267	def collapse_nodes_with_same_names ( graph : BELGraph ) -> None : survivor_mapping = defaultdict ( set ) victims = set ( ) it = tqdm ( itt . combinations ( graph , r = 2 ) , total = graph . number_of_nodes ( ) * ( graph . number_of_nodes ( ) - 1 ) / 2 ) for a , b in it : if b in victims : continue a_name , b_name = a . get ( NAME ) , b . get ( NAME ) if not a_name or not b_name or a_name . lower ( ) != b_name . lower ( ) : continue if a . keys ( ) != b . keys ( ) : continue for k in set ( a . keys ( ) ) - { NAME , NAMESPACE } : if a [ k ] != b [ k ] : continue survivor_mapping [ a ] . add ( b ) victims . add ( b ) collapse_nodes ( graph , survivor_mapping )
5943	def autoconvert ( s ) : if type ( s ) is not str : return s for converter in int , float , str : try : s = [ converter ( i ) for i in s . split ( ) ] if len ( s ) == 1 : return s [ 0 ] else : return numpy . array ( s ) except ( ValueError , AttributeError ) : pass raise ValueError ( "Failed to autoconvert {0!r}" . format ( s ) )
1574	def add_tracker_url ( parser ) : parser . add_argument ( '--tracker_url' , metavar = '(tracker url; default: "' + DEFAULT_TRACKER_URL + '")' , type = str , default = DEFAULT_TRACKER_URL ) return parser
10124	def flip_x ( self , center = None ) : if center is None : self . poly . flip ( ) else : self . poly . flip ( center [ 0 ] )
3555	def power_on ( self , timeout_sec = TIMEOUT_SEC ) : self . _powered_on . clear ( ) IOBluetoothPreferenceSetControllerPowerState ( 1 ) if not self . _powered_on . wait ( timeout_sec ) : raise RuntimeError ( 'Exceeded timeout waiting for adapter to power on!' )
2324	def predict ( self , x , * args , ** kwargs ) : if len ( args ) > 0 : if type ( args [ 0 ] ) == nx . Graph or type ( args [ 0 ] ) == nx . DiGraph : return self . orient_graph ( x , * args , ** kwargs ) else : return self . predict_proba ( x , * args , ** kwargs ) elif type ( x ) == DataFrame : return self . predict_dataset ( x , * args , ** kwargs ) elif type ( x ) == Series : return self . predict_proba ( x . iloc [ 0 ] , x . iloc [ 1 ] , * args , ** kwargs )
7650	def load ( path_or_file , validate = True , strict = True , fmt = 'auto' ) : r with _open ( path_or_file , mode = 'r' , fmt = fmt ) as fdesc : jam = JAMS ( ** json . load ( fdesc ) ) if validate : jam . validate ( strict = strict ) return jam
4201	def modcovar ( x , order ) : from spectrum import corrmtx import scipy . linalg X = corrmtx ( x , order , 'modified' ) Xc = np . matrix ( X [ : , 1 : ] ) X1 = np . array ( X [ : , 0 ] ) a , residues , rank , singular_values = scipy . linalg . lstsq ( - Xc , X1 ) Cz = np . dot ( X1 . conj ( ) . transpose ( ) , Xc ) e = np . dot ( X1 . conj ( ) . transpose ( ) , X1 ) + np . dot ( Cz , a ) assert e . imag < 1e-4 , 'wierd behaviour' e = float ( e . real ) return a , e
2517	def p_file_comment ( self , f_term , predicate ) : try : for _ , _ , comment in self . graph . triples ( ( f_term , predicate , None ) ) : self . builder . set_file_comment ( self . doc , six . text_type ( comment ) ) except CardinalityError : self . more_than_one_error ( 'file comment' )
2285	def predict ( self , df_data , graph = None , ** kwargs ) : if graph is None : return self . create_graph_from_data ( df_data , ** kwargs ) elif isinstance ( graph , nx . DiGraph ) : return self . orient_directed_graph ( df_data , graph , ** kwargs ) elif isinstance ( graph , nx . Graph ) : return self . orient_undirected_graph ( df_data , graph , ** kwargs ) else : print ( 'Unknown Graph type' ) raise ValueError
1666	def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : line = clean_lines . elided [ linenum ] declarator_end = line . rfind ( ')' ) if declarator_end >= 0 : fragment = line [ declarator_end : ] else : if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : fragment = line else : return if Search ( r'\boverride\b' , fragment ) and Search ( r'\bfinal\b' , fragment ) : error ( filename , linenum , 'readability/inheritance' , 4 , ( '"override" is redundant since function is ' 'already declared as "final"' ) )
11338	def schedule_mode ( self , mode ) : modes = [ config . SCHEDULE_RUN , config . SCHEDULE_TEMPORARY_HOLD , config . SCHEDULE_HOLD ] if mode not in modes : raise Exception ( "Invalid mode. Please use one of: {}" . format ( modes ) ) self . set_data ( { "ScheduleMode" : mode } )
12365	def get ( self , id ) : info = super ( Images , self ) . get ( id ) return ImageActions ( self . api , parent = self , ** info )
11614	def export_posterior_probability ( self , filename , title = "Posterior Probability" ) : self . probability . save ( h5file = filename , title = title )
7172	def calc_intents ( self , query ) : if self . must_train : self . train ( ) intents = { } if self . train_thread and self . train_thread . is_alive ( ) else { i . name : i for i in self . intents . calc_intents ( query , self . entities ) } sent = tokenize ( query ) for perfect_match in self . padaos . calc_intents ( query ) : name = perfect_match [ 'name' ] intents [ name ] = MatchData ( name , sent , matches = perfect_match [ 'entities' ] , conf = 1.0 ) return list ( intents . values ( ) )
5193	def send_select_and_operate_command_set ( self , command_set , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command_set , callback , config )
1549	def set_logging_level ( cl_args ) : if 'verbose' in cl_args and cl_args [ 'verbose' ] : configure ( logging . DEBUG ) else : configure ( logging . INFO )
11471	def upload ( self , filename , location = '' ) : current_folder = self . _ftp . pwd ( ) self . mkdir ( location ) self . cd ( location ) fl = open ( filename , 'rb' ) filename = filename . split ( '/' ) [ - 1 ] self . _ftp . storbinary ( 'STOR %s' % filename , fl ) fl . close ( ) self . cd ( current_folder )
2787	def attach ( self , droplet_id , region ) : return self . get_data ( "volumes/%s/actions/" % self . id , type = POST , params = { "type" : "attach" , "droplet_id" : droplet_id , "region" : region } )
12086	def html_index ( self , launch = False , showChildren = False ) : self . makePics ( ) html = '<a href="index_splash.html" target="content">./%s/</a><br>' % os . path . basename ( self . abfFolder ) for ID in smartSort ( self . fnamesByCell . keys ( ) ) : link = '' if ID + ".html" in self . fnames2 : link = 'href="%s.html" target="content"' % ID html += ( '<a %s>%s</a><br>' % ( link , ID ) ) if showChildren : for fname in self . fnamesByCell [ ID ] : thisID = os . path . splitext ( fname ) [ 0 ] files2 = [ x for x in self . fnames2 if x . startswith ( thisID ) and not x . endswith ( ".html" ) ] html += '<i>%s</i>' % thisID if len ( files2 ) : html += ' (%s)' % len ( files2 ) html += '<br>' html += "<br>" style . save ( html , self . abfFolder2 + "/index_menu.html" ) self . html_index_splash ( ) style . frames ( self . abfFolder2 + "/index.html" , launch = launch )
12387	def parse_segment ( text ) : "we expect foo=bar" if not len ( text ) : return NoopQuerySegment ( ) q = QuerySegment ( ) equalities = zip ( constants . OPERATOR_EQUALITIES , itertools . repeat ( text ) ) equalities = map ( lambda x : ( x [ 0 ] , x [ 1 ] . split ( x [ 0 ] , 1 ) ) , equalities ) equalities = list ( filter ( lambda x : len ( x [ 1 ] ) > 1 , equalities ) ) key_len = len ( min ( ( x [ 1 ] [ 0 ] for x in equalities ) , key = len ) ) equalities = filter ( lambda x : len ( x [ 1 ] [ 0 ] ) == key_len , equalities ) op , ( key , value ) = min ( equalities , key = lambda x : len ( x [ 1 ] [ 1 ] ) ) key , directive = parse_directive ( key ) if directive : op = constants . OPERATOR_EQUALITY_FALLBACK q . directive = directive path = key . split ( constants . SEP_PATH ) last = path [ - 1 ] if last . endswith ( constants . OPERATOR_NEGATION ) : last = last [ : - 1 ] q . negated = not q . negated if last == constants . PATH_NEGATION : path . pop ( - 1 ) q . negated = not q . negated q . values = value . split ( constants . SEP_VALUE ) if path [ - 1 ] in constants . OPERATOR_SUFFIXES : if op not in constants . OPERATOR_FALLBACK : raise ValueError ( 'Both path-style operator and equality style operator ' 'provided. Please provide only a single style operator.' ) q . operator = constants . OPERATOR_SUFFIX_MAP [ path [ - 1 ] ] path . pop ( - 1 ) else : q . operator = constants . OPERATOR_EQUALITY_MAP [ op ] if not len ( path ) : raise ValueError ( 'No attribute navigation path provided.' ) q . path = path return q
13510	def pyflakes ( ) : packages = [ x for x in options . setup . packages if '.' not in x ] sh ( 'pyflakes {param} {files}' . format ( param = options . paved . pycheck . pyflakes . param , files = ' ' . join ( packages ) ) )
4047	def _totals ( self , query ) : self . add_parameters ( limit = 1 ) query = self . _build_query ( query ) self . _retrieve_data ( query ) self . url_params = None return int ( self . request . headers [ "Total-Results" ] )
5303	def parse_json_color_file ( path ) : with open ( path , "r" ) as color_file : color_list = json . load ( color_file ) color_dict = { c [ "name" ] : c [ "hex" ] for c in color_list } return color_dict
5962	def parse ( self , stride = None ) : if stride is None : stride = self . stride self . corrupted_lineno = [ ] irow = 0 with utilities . openany ( self . real_filename ) as xvg : rows = [ ] ncol = None for lineno , line in enumerate ( xvg ) : line = line . strip ( ) if len ( line ) == 0 : continue if "label" in line and "xaxis" in line : self . xaxis = line . split ( '"' ) [ - 2 ] if "label" in line and "yaxis" in line : self . yaxis = line . split ( '"' ) [ - 2 ] if line . startswith ( "@ legend" ) : if not "legend" in self . metadata : self . metadata [ "legend" ] = [ ] self . metadata [ "legend" ] . append ( line . split ( "legend " ) [ - 1 ] ) if line . startswith ( "@ s" ) and "subtitle" not in line : name = line . split ( "legend " ) [ - 1 ] . replace ( '"' , '' ) . strip ( ) self . names . append ( name ) if line . startswith ( ( '#' , '@' ) ) : continue if line . startswith ( '&' ) : raise NotImplementedError ( '{0!s}: Multi-data not supported, only simple NXY format.' . format ( self . real_filename ) ) try : row = [ float ( el ) for el in line . split ( ) ] except : if self . permissive : self . logger . warn ( "%s: SKIPPING unparsable line %d: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue self . logger . error ( "%s: Cannot parse line %d: %r" , self . real_filename , lineno + 1 , line ) raise if ncol is not None and len ( row ) != ncol : if self . permissive : self . logger . warn ( "%s: SKIPPING line %d with wrong number of columns: %r" , self . real_filename , lineno + 1 , line ) self . corrupted_lineno . append ( lineno + 1 ) continue errmsg = "{0!s}: Wrong number of columns in line {1:d}: {2!r}" . format ( self . real_filename , lineno + 1 , line ) self . logger . error ( errmsg ) raise IOError ( errno . ENODATA , errmsg , self . real_filename ) if irow % stride == 0 : ncol = len ( row ) rows . append ( row ) irow += 1 try : self . __array = numpy . array ( rows ) . transpose ( ) except : self . logger . error ( "%s: Failed reading XVG file, possibly data corrupted. " "Check the last line of the file..." , self . real_filename ) raise finally : del rows
12760	def load_c3d ( self , filename , start_frame = 0 , max_frames = int ( 1e300 ) ) : import c3d with open ( filename , 'rb' ) as handle : reader = c3d . Reader ( handle ) logging . info ( 'world frame rate %s, marker frame rate %s' , 1 / self . world . dt , reader . point_rate ) self . channels = self . _map_labels_to_channels ( [ s . strip ( ) for s in reader . point_labels ] ) data = [ ] for i , ( _ , frame , _ ) in enumerate ( reader . read_frames ( ) ) : if i >= start_frame : data . append ( frame [ : , [ 0 , 1 , 2 , 4 ] ] ) if len ( data ) > max_frames : break self . data = np . array ( data ) if reader . get ( 'POINT:UNITS' ) . string_value . strip ( ) . lower ( ) == 'mm' : logging . info ( 'scaling point data from mm to m' ) self . data [ : , : , : 3 ] /= 1000. logging . info ( '%s: loaded marker data %s' , filename , self . data . shape ) self . process_data ( ) self . create_bodies ( )
9139	def label ( labels = [ ] , language = 'any' , sortLabel = False ) : if not labels : return None if not language : language = 'und' labels = [ dict_to_label ( l ) for l in labels ] l = False if sortLabel : l = find_best_label_for_type ( labels , language , 'sortLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'prefLabel' ) if not l : l = find_best_label_for_type ( labels , language , 'altLabel' ) if l : return l else : return label ( labels , 'any' , sortLabel ) if language != 'any' else None
9717	async def get_current_frame ( self , components = None ) -> QRTPacket : if components is None : components = [ "all" ] else : _validate_components ( components ) cmd = "getcurrentframe %s" % " " . join ( components ) return await asyncio . wait_for ( self . _protocol . send_command ( cmd ) , timeout = self . _timeout )
4106	def MINEIGVAL ( T0 , T , TOL ) : M = len ( T ) eigval = 10 eigvalold = 1 eigvec = numpy . zeros ( M + 1 , dtype = complex ) for k in range ( 0 , M + 1 ) : eigvec [ k ] = 1 + 0j it = 0 maxit = 15 while abs ( eigvalold - eigval ) > TOL * eigvalold and it < maxit : it = it + 1 eigvalold = eigval eig = toeplitz . HERMTOEP ( T0 , T , eigvec ) SUM = 0 save = 0. + 0j for k in range ( 0 , M + 1 ) : SUM = SUM + eig [ k ] . real ** 2 + eig [ k ] . imag ** 2 save = save + eig [ k ] * eigvec [ k ] . conjugate ( ) SUM = 1. / SUM eigval = save . real * SUM for k in range ( 0 , M + 1 ) : eigvec [ k ] = SUM * eig [ k ] if it == maxit : print ( 'warning reached max number of iteration (%s)' % maxit ) return eigval , eigvec
7092	def handle_change ( self , change ) : op = change [ 'operation' ] if op in 'append' : self . add ( len ( change [ 'value' ] ) , LatLng ( * change [ 'item' ] ) ) elif op == 'insert' : self . add ( change [ 'index' ] , LatLng ( * change [ 'item' ] ) ) elif op == 'extend' : points = [ LatLng ( * p ) for p in change [ 'items' ] ] self . addAll ( [ bridge . encode ( c ) for c in points ] ) elif op == '__setitem__' : self . set ( change [ 'index' ] , LatLng ( * change [ 'newitem' ] ) ) elif op == 'pop' : self . remove ( change [ 'index' ] ) else : raise NotImplementedError ( "Unsupported change operation {}" . format ( op ) )
8884	def fit ( self , X , y = None ) : X = check_array ( X ) self . _x_min = X . min ( axis = 0 ) self . _x_max = X . max ( axis = 0 ) return self
6847	def purge_keys ( self ) : r = self . local_renderer r . env . default_ip = self . hostname_to_ip ( self . env . default_hostname ) r . env . home_dir = '/home/%s' % getpass . getuser ( ) r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {host_string}' ) if self . env . default_hostname : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_hostname}' ) if r . env . default_ip : r . local ( 'ssh-keygen -f "{home_dir}/.ssh/known_hosts" -R {default_ip}' )
801	def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : assert len ( fields ) >= 1 , "fields is empty" with ConnectionFactory . get ( ) as conn : dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] dbFieldStr = ", " . join ( dbFields ) query = 'SELECT model_id, {fields} from {models}' ' WHERE job_id=%s AND model_checkpoint_id IS NOT NULL' . format ( fields = dbFieldStr , models = self . modelsTableName ) conn . cursor . execute ( query , [ jobID ] ) rows = conn . cursor . fetchall ( ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]
1228	def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss
4772	def does_not_contain ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) elif len ( items ) == 1 : if items [ 0 ] in self . val : self . _err ( 'Expected <%s> to not contain item <%s>, but did.' % ( self . val , items [ 0 ] ) ) else : found = [ ] for i in items : if i in self . val : found . append ( i ) if found : self . _err ( 'Expected <%s> to not contain items %s, but did contain %s.' % ( self . val , self . _fmt_items ( items ) , self . _fmt_items ( found ) ) ) return self
409	def _tf_batch_map_coordinates ( self , inputs , coords ) : input_shape = inputs . get_shape ( ) coords_shape = coords . get_shape ( ) batch_channel = tf . shape ( inputs ) [ 0 ] input_h = int ( input_shape [ 1 ] ) input_w = int ( input_shape [ 2 ] ) kernel_n = int ( coords_shape [ 3 ] ) n_coords = input_h * input_w * kernel_n coords_lt = tf . cast ( tf . floor ( coords ) , 'int32' ) coords_rb = tf . cast ( tf . ceil ( coords ) , 'int32' ) coords_lb = tf . stack ( [ coords_lt [ : , : , : , : , 0 ] , coords_rb [ : , : , : , : , 1 ] ] , axis = - 1 ) coords_rt = tf . stack ( [ coords_rb [ : , : , : , : , 0 ] , coords_lt [ : , : , : , : , 1 ] ] , axis = - 1 ) idx = self . _tf_repeat ( tf . range ( batch_channel ) , n_coords ) vals_lt = self . _get_vals_by_coords ( inputs , coords_lt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rb = self . _get_vals_by_coords ( inputs , coords_rb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_lb = self . _get_vals_by_coords ( inputs , coords_lb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rt = self . _get_vals_by_coords ( inputs , coords_rt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) coords_offset_lt = coords - tf . cast ( coords_lt , 'float32' ) vals_t = vals_lt + ( vals_rt - vals_lt ) * coords_offset_lt [ : , : , : , : , 0 ] vals_b = vals_lb + ( vals_rb - vals_lb ) * coords_offset_lt [ : , : , : , : , 0 ] mapped_vals = vals_t + ( vals_b - vals_t ) * coords_offset_lt [ : , : , : , : , 1 ] return mapped_vals
9928	def authenticate ( self , request , email = None , password = None , username = None ) : email = email or username try : email_instance = models . EmailAddress . objects . get ( is_verified = True , email = email ) except models . EmailAddress . DoesNotExist : return None user = email_instance . user if user . check_password ( password ) : return user return None
2294	def integral_approx_estimator ( x , y ) : a , b = ( 0. , 0. ) x = np . array ( x ) y = np . array ( y ) idx , idy = ( np . argsort ( x ) , np . argsort ( y ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idx ] ] [ : - 1 ] , x [ [ idx ] ] [ 1 : ] , y [ [ idx ] ] [ : - 1 ] , y [ [ idx ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : a = a + np . log ( np . abs ( ( y2 - y1 ) / ( x2 - x1 ) ) ) for x1 , x2 , y1 , y2 in zip ( x [ [ idy ] ] [ : - 1 ] , x [ [ idy ] ] [ 1 : ] , y [ [ idy ] ] [ : - 1 ] , y [ [ idy ] ] [ 1 : ] ) : if x1 != x2 and y1 != y2 : b = b + np . log ( np . abs ( ( x2 - x1 ) / ( y2 - y1 ) ) ) return ( a - b ) / len ( x )
2689	def _CCompiler_spawn_silent ( cmd , dry_run = None ) : proc = Popen ( cmd , stdout = PIPE , stderr = PIPE ) out , err = proc . communicate ( ) if proc . returncode : raise DistutilsExecError ( err )
7814	def from_file ( cls , filename ) : with open ( filename , "r" ) as pem_file : data = pem . readPemFromFile ( pem_file ) return cls . from_der_data ( data )
13008	def path ( self ) : path = super ( WindowsPath2 , self ) . path if path . startswith ( "\\\\?\\" ) : return path [ 4 : ] return path
2801	def convert_reduce_sum ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting reduce_sum ...' ) keepdims = params [ 'keepdims' ] > 0 axis = params [ 'axes' ] def target_layer ( x , keepdims = keepdims , axis = axis ) : import keras . backend as K return K . sum ( x , keepdims = keepdims , axis = axis ) lambda_layer = keras . layers . Lambda ( target_layer ) layers [ scope_name ] = lambda_layer ( layers [ inputs [ 0 ] ] )
8135	def down ( self ) : i = self . index ( ) if i != None : del self . canvas . layers [ i ] i = max ( 0 , i - 1 ) self . canvas . layers . insert ( i , self )
7000	def _runpf_worker ( task ) : ( lcfile , outdir , timecols , magcols , errcols , lcformat , lcformatdir , pfmethods , pfkwargs , getblssnr , sigclip , nworkers , minobservations , excludeprocessed ) = task if os . path . exists ( lcfile ) : pfresult = runpf ( lcfile , outdir , timecols = timecols , magcols = magcols , errcols = errcols , lcformat = lcformat , lcformatdir = lcformatdir , pfmethods = pfmethods , pfkwargs = pfkwargs , getblssnr = getblssnr , sigclip = sigclip , nworkers = nworkers , minobservations = minobservations , excludeprocessed = excludeprocessed ) return pfresult else : LOGERROR ( 'LC does not exist for requested file %s' % lcfile ) return None
7416	def loci2migrate ( name , locifile , popdict , mindict = 1 ) : outfile = open ( name + ".migrate" , 'w' ) infile = open ( locifile , 'r' ) if isinstance ( mindict , int ) : mindict = { pop : mindict for pop in popdict } else : mindict = mindict keep = [ ] MINS = zip ( taxa . keys ( ) , minhits ) loci = infile . read ( ) . strip ( ) . split ( "|" ) [ : - 1 ] for loc in loci : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] GG = [ ] for group , mins in MINS : GG . append ( sum ( [ i in samps for i in taxa [ group ] ] ) >= int ( mins ) ) if all ( GG ) : keep . append ( loc ) print >> outfile , len ( taxa ) , len ( keep ) , "( npops nloci for data set" , data . name + ".loci" , ")" done = 0 for group in taxa : if not done : loclens = [ len ( loc . split ( "\n" ) [ 1 ] . split ( ) [ - 1 ] . replace ( "x" , "n" ) . replace ( "n" , "" ) ) for loc in keep ] print >> outfile , " " . join ( map ( str , loclens ) ) done += 1 indslist = [ ] for loc in keep : samps = [ i . split ( ) [ 0 ] . replace ( ">" , "" ) for i in loc . split ( "\n" ) if ">" in i ] inds = sum ( [ i in samps for i in taxa [ group ] ] ) indslist . append ( inds ) print >> outfile , " " . join ( map ( str , indslist ) ) , group for loc in range ( len ( keep ) ) : seqs = [ i . split ( ) [ - 1 ] for i in keep [ loc ] . split ( "\n" ) if i . split ( ) [ 0 ] . replace ( ">" , "" ) in taxa [ group ] ] for i in range ( len ( seqs ) ) : print >> outfile , group [ 0 : 8 ] + "_" + str ( i ) + ( " " * ( 10 - len ( group [ 0 : 8 ] + "_" + str ( i ) ) ) ) + seqs [ i ] . replace ( "x" , "n" ) . replace ( "n" , "" ) outfile . close ( )
4874	def validate_lms_user_id ( self , value ) : enterprise_customer = self . context . get ( 'enterprise_customer' ) try : return models . EnterpriseCustomerUser . objects . get ( user_id = value , enterprise_customer = enterprise_customer ) except models . EnterpriseCustomerUser . DoesNotExist : pass return None
7794	def register_fetcher ( self , object_class , fetcher_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : cache = Cache ( self . max_items , self . default_freshness_period , self . default_expiration_period , self . default_purge_period ) self . _caches [ object_class ] = cache cache . set_fetcher ( fetcher_class ) finally : self . _lock . release ( )
5542	def contours ( self , elevation , interval = 100 , field = 'elev' , base = 0 ) : return commons_contours . extract_contours ( elevation , self . tile , interval = interval , field = field , base = base )
13079	def render ( self , template , ** kwargs ) : kwargs [ "cache_key" ] = "%s" % kwargs [ "url" ] . values ( ) kwargs [ "lang" ] = self . get_locale ( ) kwargs [ "assets" ] = self . assets kwargs [ "main_collections" ] = self . main_collections ( kwargs [ "lang" ] ) kwargs [ "cache_active" ] = self . cache is not None kwargs [ "cache_time" ] = 0 kwargs [ "cache_key" ] , kwargs [ "cache_key_i18n" ] = self . make_cache_keys ( request . endpoint , kwargs [ "url" ] ) kwargs [ "template" ] = template for plugin in self . __plugins_render_views__ : kwargs . update ( plugin . render ( ** kwargs ) ) return render_template ( kwargs [ "template" ] , ** kwargs )
9054	def posteriori_covariance ( self ) : r K = GLMM . covariance ( self ) tau = self . _ep . _posterior . tau return pinv ( pinv ( K ) + diag ( 1 / tau ) )
2152	def get ( self , pk = None , ** kwargs ) : self . _separate ( kwargs ) return super ( Resource , self ) . get ( pk = pk , ** kwargs )
4948	def export ( self ) : content_metadata_export = { } content_metadata_items = self . enterprise_api . get_content_metadata ( self . enterprise_customer ) LOGGER . info ( 'Retrieved content metadata for enterprise [%s]' , self . enterprise_customer . name ) for item in content_metadata_items : transformed = self . _transform_item ( item ) LOGGER . info ( 'Exporting content metadata item with plugin configuration [%s]: [%s]' , self . enterprise_configuration , json . dumps ( transformed , indent = 4 ) , ) content_metadata_item_export = ContentMetadataItemExport ( item , transformed ) content_metadata_export [ content_metadata_item_export . content_id ] = content_metadata_item_export return OrderedDict ( sorted ( content_metadata_export . items ( ) ) )
12599	def get_sheet_list ( xl_path : str ) -> List : wb = read_xl ( xl_path ) if hasattr ( wb , 'sheetnames' ) : return wb . sheetnames else : return wb . sheet_names ( )
8108	def search ( q , start = 0 , wait = 10 , asynchronous = False , cached = False ) : service = GOOGLE_SEARCH return GoogleSearch ( q , start , service , "" , wait , asynchronous , cached )
12619	def check_img_compatibility ( one_img , another_img , only_check_3d = False ) : nd_to_check = None if only_check_3d : nd_to_check = 3 if hasattr ( one_img , 'shape' ) and hasattr ( another_img , 'shape' ) : if not have_same_shape ( one_img , another_img , nd_to_check = nd_to_check ) : msg = 'Shape of the first image: \n{}\n is different from second one: \n{}' . format ( one_img . shape , another_img . shape ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg ) if hasattr ( one_img , 'get_affine' ) and hasattr ( another_img , 'get_affine' ) : if not have_same_affine ( one_img , another_img , only_check_3d = only_check_3d ) : msg = 'Affine matrix of the first image: \n{}\n is different ' 'from second one:\n{}' . format ( one_img . get_affine ( ) , another_img . get_affine ( ) ) raise NiftiFilesNotCompatible ( repr_imgs ( one_img ) , repr_imgs ( another_img ) , message = msg )
12242	def bohachevsky1 ( theta ) : x , y = theta obj = x ** 2 + 2 * y ** 2 - 0.3 * np . cos ( 3 * np . pi * x ) - 0.4 * np . cos ( 4 * np . pi * y ) + 0.7 grad = np . array ( [ 2 * x + 0.3 * np . sin ( 3 * np . pi * x ) * 3 * np . pi , 4 * y + 0.4 * np . sin ( 4 * np . pi * y ) * 4 * np . pi , ] ) return obj , grad
1691	def InnermostClass ( self ) : for i in range ( len ( self . stack ) , 0 , - 1 ) : classinfo = self . stack [ i - 1 ] if isinstance ( classinfo , _ClassInfo ) : return classinfo return None
197	def Fog ( name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return CloudLayer ( intensity_mean = ( 220 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.5 ) , intensity_coarse_scale = 2 , alpha_min = ( 0.7 , 0.9 ) , alpha_multiplier = 0.3 , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 4.0 , - 2.0 ) , sparsity = 0.9 , density_multiplier = ( 0.4 , 0.9 ) , name = name , deterministic = deterministic , random_state = random_state )
1505	def template_statemgr_yaml ( cl_args , zookeepers ) : statemgr_config_file_template = "%s/standalone/templates/statemgr.template.yaml" % cl_args [ "config_path" ] statemgr_config_file_actual = "%s/standalone/statemgr.yaml" % cl_args [ "config_path" ] template_file ( statemgr_config_file_template , statemgr_config_file_actual , { "<zookeeper_host:zookeeper_port>" : "," . join ( [ '"%s"' % zk if ":" in zk else '"%s:2181"' % zk for zk in zookeepers ] ) } )
6203	def em_rates_from_E_DA_mix ( em_rates_tot , E_values ) : em_rates_d , em_rates_a = [ ] , [ ] for em_rate_tot , E_value in zip ( em_rates_tot , E_values ) : em_rate_di , em_rate_ai = em_rates_from_E_DA ( em_rate_tot , E_value ) em_rates_d . append ( em_rate_di ) em_rates_a . append ( em_rate_ai ) return em_rates_d , em_rates_a
3781	def calculate ( self , T , method ) : r if method == TEST_METHOD_1 : prop = self . TEST_METHOD_1_coeffs [ 0 ] + self . TEST_METHOD_1_coeffs [ 1 ] * T elif method == TEST_METHOD_2 : prop = self . TEST_METHOD_2_coeffs [ 0 ] + self . TEST_METHOD_2_coeffs [ 1 ] * T elif method in self . tabular_data : prop = self . interpolate ( T , method ) return prop
13904	def parse_hub_key ( key ) : if key is None : raise ValueError ( 'Not a valid key' ) match = re . match ( PATTERN , key ) if not match : match = re . match ( PATTERN_S0 , key ) if not match : raise ValueError ( 'Not a valid key' ) return dict ( map ( normalise_part , zip ( [ p for p in PARTS_S0 . keys ( ) ] , match . groups ( ) ) ) ) return dict ( zip ( PARTS . keys ( ) , match . groups ( ) ) )
9855	def _read_header ( self , ccp4file ) : bsaflag = self . _detect_byteorder ( ccp4file ) nheader = struct . calcsize ( self . _headerfmt ) names = [ r . key for r in self . _header_struct ] bintopheader = ccp4file . read ( 25 * 4 ) def decode_header ( header , bsaflag = '@' ) : h = dict ( zip ( names , struct . unpack ( bsaflag + self . _headerfmt , header ) ) ) h [ 'bsaflag' ] = bsaflag return h header = decode_header ( bintopheader , bsaflag ) for rec in self . _header_struct : if not rec . is_legal_dict ( header ) : warnings . warn ( "Key %s: Illegal value %r" % ( rec . key , header [ rec . key ] ) ) if ( header [ 'lskflg' ] ) : skewmatrix = np . fromfile ( ccp4file , dtype = np . float32 , count = 9 ) header [ 'skwmat' ] = skewmatrix . reshape ( ( 3 , 3 ) ) header [ 'skwtrn' ] = np . fromfile ( ccp4file , dtype = np . float32 , count = 3 ) else : header [ 'skwmat' ] = header [ 'skwtrn' ] = None ccp4file . seek ( 12 * 4 , 1 ) ccp4file . seek ( 15 * 4 , 1 ) ccp4file . seek ( 4 , 1 ) endiancode = struct . unpack ( bsaflag + '4b' , ccp4file . read ( 4 ) ) header [ 'endianness' ] = 'little' if endiancode == ( 0x44 , 0x41 , 0 , 0 ) else 'big' header [ 'arms' ] = struct . unpack ( bsaflag + 'f' , ccp4file . read ( 4 ) ) [ 0 ] header [ 'nlabl' ] = struct . unpack ( bsaflag + 'I' , ccp4file . read ( 4 ) ) [ 0 ] if header [ 'nlabl' ] : binlabel = ccp4file . read ( 80 * header [ 'nlabl' ] ) flag = bsaflag + str ( 80 * header [ 'nlabl' ] ) + 's' label = struct . unpack ( flag , binlabel ) [ 0 ] header [ 'label' ] = label . decode ( 'utf-8' ) . rstrip ( '\x00' ) else : header [ 'label' ] = None ccp4file . seek ( 256 * 4 ) return header
9900	def data ( self , data ) : if self . is_caching : self . cache = data else : fcontents = self . file_contents with open ( self . path , "w" ) as f : try : indent = self . indent if self . pretty else None json . dump ( data , f , sort_keys = self . sort_keys , indent = indent ) except Exception as e : f . seek ( 0 ) f . truncate ( ) f . write ( fcontents ) raise e self . _updateType ( )
12549	def spatial_map ( icc , thr , mode = '+' ) : return thr_img ( icc_img_to_zscore ( icc ) , thr = thr , mode = mode ) . get_data ( )
699	def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if swarmId is not None : assert ( not entry [ 'hidden' ] ) modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )
9581	def eof ( fd ) : b = fd . read ( 1 ) end = len ( b ) == 0 if not end : curpos = fd . tell ( ) fd . seek ( curpos - 1 ) return end
7557	def random_product ( iter1 , iter2 ) : iter4 = np . concatenate ( [ np . random . choice ( iter1 , 2 , replace = False ) , np . random . choice ( iter2 , 2 , replace = False ) ] ) return iter4
558	def bestModelInSprint ( self , sprintIdx ) : swarms = self . getAllSwarms ( sprintIdx ) bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )
11507	def item_get ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.get' , parameters ) return response
13862	def totz ( when , tz = None ) : if when is None : return None when = to_datetime ( when ) if when . tzinfo is None : when = when . replace ( tzinfo = localtz ) return when . astimezone ( tz or utc )
3588	def cbuuid_to_uuid ( cbuuid ) : data = cbuuid . data ( ) . bytes ( ) template = '{:0>8}-0000-1000-8000-00805f9b34fb' if len ( data ) <= 4 else '{:0>32}' value = template . format ( hexlify ( data . tobytes ( ) [ : 16 ] ) . decode ( 'ascii' ) ) return uuid . UUID ( hex = value )
3663	def calculate_integral_over_T ( self , T1 , T2 , method ) : r if method == ZABRANSKY_SPLINE : return self . Zabransky_spline . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_C : return self . Zabransky_spline_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_SPLINE_SAT : return self . Zabransky_spline_sat . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL : return self . Zabransky_quasipolynomial . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_C : return self . Zabransky_quasipolynomial_iso . calculate_integral_over_T ( T1 , T2 ) elif method == ZABRANSKY_QUASIPOLYNOMIAL_SAT : return self . Zabransky_quasipolynomial_sat . calculate_integral_over_T ( T1 , T2 ) elif method == POLING_CONST : return self . POLING_constant * log ( T2 / T1 ) elif method == CRCSTD : return self . CRCSTD_constant * log ( T2 / T1 ) elif method == DADGOSTAR_SHAW : dS = ( Dadgostar_Shaw_integral_over_T ( T2 , self . similarity_variable ) - Dadgostar_Shaw_integral_over_T ( T1 , self . similarity_variable ) ) return property_mass_to_molar ( dS , self . MW ) elif method in self . tabular_data or method == COOLPROP or method in [ ROWLINSON_POLING , ROWLINSON_BONDI ] : return float ( quad ( lambda T : self . calculate ( T , method ) / T , T1 , T2 ) [ 0 ] ) else : raise Exception ( 'Method not valid' )
3339	def is_equal_or_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and ( childUri . rstrip ( "/" ) + "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
7538	def getassembly ( args , parsedict ) : project_dir = ip . core . assembly . _expander ( parsedict [ 'project_dir' ] ) assembly_name = parsedict [ 'assembly_name' ] assembly_file = os . path . join ( project_dir , assembly_name ) if not os . path . exists ( project_dir ) : os . mkdir ( project_dir ) try : if ( '1' in args . steps ) and args . force : data = ip . Assembly ( assembly_name , cli = True ) else : data = ip . load_json ( assembly_file , cli = True ) data . _cli = True except IPyradWarningExit as _ : if '1' not in args . steps : raise IPyradWarningExit ( " Error: You must first run step 1 on the assembly: {}" . format ( assembly_file ) ) else : data = ip . Assembly ( assembly_name , cli = True ) for param in parsedict : if param == "assembly_name" : if parsedict [ param ] != data . name : data . set_params ( param , parsedict [ param ] ) else : try : data . set_params ( param , parsedict [ param ] ) except IndexError as _ : print ( " Malformed params file: {}" . format ( args . params ) ) print ( " Bad parameter {} - {}" . format ( param , parsedict [ param ] ) ) sys . exit ( - 1 ) return data
7429	def draw ( self , axes ) : tre = toytree . tree ( newick = self . results . tree ) tre . draw ( axes = axes , use_edge_lengths = True , tree_style = 'c' , tip_labels_align = True , edge_align_style = { "stroke-width" : 1 } ) for admix in self . results . admixture : pidx , pdist , cidx , cdist , weight = admix a = _get_admix_point ( tre , pidx , pdist ) b = _get_admix_point ( tre , cidx , cdist ) mark = axes . plot ( a = ( a [ 0 ] , b [ 0 ] ) , b = ( a [ 1 ] , b [ 1 ] ) , style = { "stroke-width" : 10 * weight , "stroke-opacity" : 0.95 , "stroke-linecap" : "round" } ) axes . scatterplot ( a = ( b [ 0 ] ) , b = ( b [ 1 ] ) , size = 8 , title = "weight: {}" . format ( weight ) , ) axes . y . show = False axes . x . ticks . show = True axes . x . label . text = "Drift parameter" return axes
8825	def update_sg ( self , context , sg , rule_id , action ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None with context . session . begin ( ) : job_body = dict ( action = "%s sg rule %s" % ( action , rule_id ) , resource_id = rule_id , tenant_id = db_sg [ 'tenant_id' ] ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_client = QuarkSGAsyncProducerClient ( ) try : rpc_client . populate_subtasks ( context , sg , job [ 'id' ] ) except om_exc . MessagingTimeout : LOG . error ( "Failed to create subtasks. Rabbit running?" ) return None return { "job_id" : job [ 'id' ] }
8266	def _cache ( self ) : n = self . steps if len ( self . _colors ) == 1 : ColorList . __init__ ( self , [ self . _colors [ 0 ] for i in _range ( n ) ] ) return colors = self . _interpolate ( self . _colors , 40 ) left = colors [ : len ( colors ) / 2 ] right = colors [ len ( colors ) / 2 : ] left . append ( right [ 0 ] ) right . insert ( 0 , left [ - 1 ] ) gradient = self . _interpolate ( left , int ( n * self . spread ) ) [ : - 1 ] gradient . extend ( self . _interpolate ( right , n - int ( n * self . spread ) ) [ 1 : ] ) if self . spread > 1 : gradient = gradient [ : n ] if self . spread < 0 : gradient = gradient [ - n : ] ColorList . __init__ ( self , gradient )
4090	def _process_event ( self , key , mask ) : self . _logger . debug ( 'Processing event with key {} and mask {}' . format ( key , mask ) ) fileobj , ( reader , writer ) = key . fileobj , key . data if mask & selectors . EVENT_READ and reader is not None : if reader . _cancelled : self . remove_reader ( fileobj ) else : self . _logger . debug ( 'Invoking reader callback: {}' . format ( reader ) ) reader . _run ( ) if mask & selectors . EVENT_WRITE and writer is not None : if writer . _cancelled : self . remove_writer ( fileobj ) else : self . _logger . debug ( 'Invoking writer callback: {}' . format ( writer ) ) writer . _run ( )
2690	def new_compiler ( * args , ** kwargs ) : make_silent = kwargs . pop ( 'silent' , True ) cc = _new_compiler ( * args , ** kwargs ) if is_msvc ( cc ) : from distutils . msvc9compiler import get_build_version if get_build_version ( ) == 10 : cc . initialize ( ) for ldflags in [ cc . ldflags_shared , cc . ldflags_shared_debug ] : unique_extend ( ldflags , [ '/MANIFEST' ] ) elif get_build_version ( ) == 14 : make_silent = False if make_silent : cc . spawn = _CCompiler_spawn_silent return cc
1064	def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None
3911	def _on_event ( self , _ ) : self . sort ( key = lambda conv_button : conv_button . last_modified , reverse = True )
6438	def dist ( self , src , tar , weights = 'exponential' , max_length = 8 ) : return self . dist_abs ( src , tar , weights , max_length , True )
11444	def parse ( self , path_to_xml = None ) : if not path_to_xml : if not self . path : self . logger . error ( "No path defined!" ) return path_to_xml = self . path root = self . _clean_xml ( path_to_xml ) if root . tag . lower ( ) == 'collection' : tree = ET . ElementTree ( root ) self . records = element_tree_collection_to_records ( tree ) elif root . tag . lower ( ) == 'record' : new_root = ET . Element ( 'collection' ) new_root . append ( root ) tree = ET . ElementTree ( new_root ) self . records = element_tree_collection_to_records ( tree ) else : header_subs = get_request_subfields ( root ) records = root . find ( 'ListRecords' ) if records is None : records = root . find ( 'GetRecord' ) if records is None : raise ValueError ( "Cannot find ListRecords or GetRecord!" ) tree = ET . ElementTree ( records ) for record , is_deleted in element_tree_oai_records ( tree , header_subs ) : if is_deleted : self . deleted_records . append ( self . create_deleted_record ( record ) ) else : self . records . append ( record )
7916	def get_arg_parser ( cls , settings = None , option_prefix = u'--' , add_help = False ) : parser = argparse . ArgumentParser ( add_help = add_help , prefix_chars = option_prefix [ 0 ] ) if settings is None : settings = cls . list_all ( basic = True ) if sys . version_info . major < 3 : from locale import getpreferredencoding encoding = getpreferredencoding ( ) def decode_string_option ( value ) : return value . decode ( encoding ) for name in settings : if name not in cls . _defs : logger . debug ( "get_arg_parser: ignoring unknown option {0}" . format ( name ) ) return setting = cls . _defs [ name ] if not setting . cmdline_help : logger . debug ( "get_arg_parser: option {0} has no cmdline" . format ( name ) ) return if sys . version_info . major < 3 : name = name . encode ( encoding , "replace" ) option = option_prefix + name . replace ( "_" , "-" ) dest = "pyxmpp2_" + name if setting . validator : opt_type = setting . validator elif setting . type is unicode and sys . version_info . major < 3 : opt_type = decode_string_option else : opt_type = setting . type if setting . default_d : default_s = setting . default_d if sys . version_info . major < 3 : default_s = default_s . encode ( encoding , "replace" ) elif setting . default is not None : default_s = repr ( setting . default ) else : default_s = None opt_help = setting . cmdline_help if sys . version_info . major < 3 : opt_help = opt_help . encode ( encoding , "replace" ) if default_s : opt_help += " (Default: {0})" . format ( default_s ) if opt_type is bool : opt_action = _YesNoAction else : opt_action = "store" parser . add_argument ( option , action = opt_action , default = setting . default , type = opt_type , help = opt_help , metavar = name . upper ( ) , dest = dest ) return parser
7951	def wait_for_readability ( self ) : with self . lock : while True : if self . _socket is None or self . _eof : return False if self . _state in ( "connected" , "closing" ) : return True if self . _state == "tls-handshake" and self . _tls_state == "want_read" : return True self . _state_cond . wait ( )
5780	def _obtain_credentials ( self ) : protocol_values = { 'SSLv3' : Secur32Const . SP_PROT_SSL3_CLIENT , 'TLSv1' : Secur32Const . SP_PROT_TLS1_CLIENT , 'TLSv1.1' : Secur32Const . SP_PROT_TLS1_1_CLIENT , 'TLSv1.2' : Secur32Const . SP_PROT_TLS1_2_CLIENT , } protocol_bit_mask = 0 for key , value in protocol_values . items ( ) : if key in self . _protocols : protocol_bit_mask |= value algs = [ Secur32Const . CALG_AES_128 , Secur32Const . CALG_AES_256 , Secur32Const . CALG_3DES , Secur32Const . CALG_SHA1 , Secur32Const . CALG_ECDHE , Secur32Const . CALG_DH_EPHEM , Secur32Const . CALG_RSA_KEYX , Secur32Const . CALG_RSA_SIGN , Secur32Const . CALG_ECDSA , Secur32Const . CALG_DSS_SIGN , ] if 'TLSv1.2' in self . _protocols : algs . extend ( [ Secur32Const . CALG_SHA512 , Secur32Const . CALG_SHA384 , Secur32Const . CALG_SHA256 , ] ) alg_array = new ( secur32 , 'ALG_ID[%s]' % len ( algs ) ) for index , alg in enumerate ( algs ) : alg_array [ index ] = alg flags = Secur32Const . SCH_USE_STRONG_CRYPTO | Secur32Const . SCH_CRED_NO_DEFAULT_CREDS if not self . _manual_validation and not self . _extra_trust_roots : flags |= Secur32Const . SCH_CRED_AUTO_CRED_VALIDATION else : flags |= Secur32Const . SCH_CRED_MANUAL_CRED_VALIDATION schannel_cred_pointer = struct ( secur32 , 'SCHANNEL_CRED' ) schannel_cred = unwrap ( schannel_cred_pointer ) schannel_cred . dwVersion = Secur32Const . SCHANNEL_CRED_VERSION schannel_cred . cCreds = 0 schannel_cred . paCred = null ( ) schannel_cred . hRootStore = null ( ) schannel_cred . cMappers = 0 schannel_cred . aphMappers = null ( ) schannel_cred . cSupportedAlgs = len ( alg_array ) schannel_cred . palgSupportedAlgs = alg_array schannel_cred . grbitEnabledProtocols = protocol_bit_mask schannel_cred . dwMinimumCipherStrength = 0 schannel_cred . dwMaximumCipherStrength = 0 schannel_cred . dwSessionLifespan = 0 schannel_cred . dwFlags = flags schannel_cred . dwCredFormat = 0 cred_handle_pointer = new ( secur32 , 'CredHandle *' ) result = secur32 . AcquireCredentialsHandleW ( null ( ) , Secur32Const . UNISP_NAME , Secur32Const . SECPKG_CRED_OUTBOUND , null ( ) , schannel_cred_pointer , null ( ) , null ( ) , cred_handle_pointer , null ( ) ) handle_error ( result ) self . _credentials_handle = cred_handle_pointer
284	def plot_drawdown_periods ( returns , top = 10 , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) df_drawdowns = timeseries . gen_drawdown_table ( returns , top = top ) df_cum_rets . plot ( ax = ax , ** kwargs ) lim = ax . get_ylim ( ) colors = sns . cubehelix_palette ( len ( df_drawdowns ) ) [ : : - 1 ] for i , ( peak , recovery ) in df_drawdowns [ [ 'Peak date' , 'Recovery date' ] ] . iterrows ( ) : if pd . isnull ( recovery ) : recovery = returns . index [ - 1 ] ax . fill_between ( ( peak , recovery ) , lim [ 0 ] , lim [ 1 ] , alpha = .4 , color = colors [ i ] ) ax . set_ylim ( lim ) ax . set_title ( 'Top %i drawdown periods' % top ) ax . set_ylabel ( 'Cumulative returns' ) ax . legend ( [ 'Portfolio' ] , loc = 'upper left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax
8948	def info ( msg ) : _flush ( ) sys . stdout . write ( msg + '\n' ) sys . stdout . flush ( )
1037	def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )
13814	def _MessageToJsonObject ( message , including_default_value_fields ) : message_descriptor = message . DESCRIPTOR full_name = message_descriptor . full_name if _IsWrapperMessage ( message_descriptor ) : return _WrapperMessageToJsonObject ( message ) if full_name in _WKTJSONMETHODS : return _WKTJSONMETHODS [ full_name ] [ 0 ] ( message , including_default_value_fields ) js = { } return _RegularMessageToJsonObject ( message , js , including_default_value_fields )
11529	def upload_json_results ( self , token , filepath , community_id , producer_display_name , metric_name , producer_revision , submit_time , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'parent_keys' : parameters [ 'parentKeys' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] file_payload = open ( filepath , 'rb' ) response = self . request ( 'midas.tracker.results.upload.json' , parameters , file_payload ) return response
13321	def add_active_module ( module ) : modules = set ( get_active_modules ( ) ) modules . add ( module ) new_modules_path = os . pathsep . join ( [ m . path for m in modules ] ) os . environ [ 'CPENV_ACTIVE_MODULES' ] = str ( new_modules_path )
3178	def get ( self , list_id , merge_id ) : self . list_id = list_id self . merge_id = merge_id return self . _mc_client . _get ( url = self . _build_path ( list_id , 'merge-fields' , merge_id ) )
13597	def get_state ( self ) : return [ os . path . join ( dp , f ) for dp , _ , fn in os . walk ( self . dir ) for f in fn ]
5206	def format_earning ( data : pd . DataFrame , header : pd . DataFrame ) -> pd . DataFrame : if data . dropna ( subset = [ 'value' ] ) . empty : return pd . DataFrame ( ) res = pd . concat ( [ grp . loc [ : , [ 'value' ] ] . set_index ( header . value ) for _ , grp in data . groupby ( data . position ) ] , axis = 1 ) res . index . name = None res . columns = res . iloc [ 0 ] res = res . iloc [ 1 : ] . transpose ( ) . reset_index ( ) . apply ( pd . to_numeric , downcast = 'float' , errors = 'ignore' ) res . rename ( columns = lambda vv : '_' . join ( vv . lower ( ) . split ( ) ) . replace ( 'fy_' , 'fy' ) , inplace = True , ) years = res . columns [ res . columns . str . startswith ( 'fy' ) ] lvl_1 = res . level == 1 for yr in years : res . loc [ : , yr ] = res . loc [ : , yr ] . round ( 1 ) pct = f'{yr}_pct' res . loc [ : , pct ] = 0. res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , pct ] . astype ( float ) . round ( 1 ) res . loc [ lvl_1 , pct ] = res . loc [ lvl_1 , yr ] / res . loc [ lvl_1 , yr ] . sum ( ) * 100 sub_pct = [ ] for _ , snap in res [ : : - 1 ] . iterrows ( ) : if snap . level > 2 : continue if snap . level == 1 : if len ( sub_pct ) == 0 : continue sub = pd . concat ( sub_pct , axis = 1 ) . transpose ( ) res . loc [ sub . index , pct ] = res . loc [ sub . index , yr ] / res . loc [ sub . index , yr ] . sum ( ) * 100 sub_pct = [ ] if snap . level == 2 : sub_pct . append ( snap ) res . set_index ( 'segment_name' , inplace = True ) res . index . name = None return res
4752	def extract_hook_names ( ent ) : hnames = [ ] for hook in ent [ "hooks" ] [ "enter" ] + ent [ "hooks" ] [ "exit" ] : hname = os . path . basename ( hook [ "fpath_orig" ] ) hname = os . path . splitext ( hname ) [ 0 ] hname = hname . strip ( ) hname = hname . replace ( "_enter" , "" ) hname = hname . replace ( "_exit" , "" ) if hname in hnames : continue hnames . append ( hname ) hnames . sort ( ) return hnames
12996	def main ( ) : services = ServiceSearch ( ) argparse = services . argparser argparse . add_argument ( '-f' , '--file' , type = str , help = "File" ) arguments = argparse . parse_args ( ) if not arguments . file : print_error ( "Please provide a file with credentials seperated by ':'" ) sys . exit ( ) services = services . get_services ( search = [ "Tomcat" ] , up = True , tags = [ '!tomcat_brute' ] ) credentials = [ ] with open ( arguments . file , 'r' ) as f : credentials = f . readlines ( ) for service in services : print_notification ( "Checking ip:{} port {}" . format ( service . address , service . port ) ) url = 'http://{}:{}/manager/html' gevent . spawn ( brutefore_passwords , service . address , url . format ( service . address , service . port ) , credentials , service ) service . add_tag ( 'tomcat_brute' ) service . update ( tags = service . tags ) gevent . wait ( ) Logger ( ) . log ( "tomcat_brute" , "Performed tomcat bruteforce scan" , { 'scanned_services' : len ( services ) } )
7927	def reorder_srv ( records ) : records = list ( records ) records . sort ( ) ret = [ ] tmp = [ ] for rrecord in records : if not tmp or rrecord . priority == tmp [ 0 ] . priority : tmp . append ( rrecord ) continue ret += shuffle_srv ( tmp ) tmp = [ rrecord ] if tmp : ret += shuffle_srv ( tmp ) return ret
10244	def get_citation_years ( graph : BELGraph ) -> List [ Tuple [ int , int ] ] : return create_timeline ( count_citation_years ( graph ) )
998	def printConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var , i ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += ' ' s += ' %5.3f' % var [ c , i ] s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatFPRow ( aState , i )
9962	def get_interfaces ( impls ) : if impls is None : return None elif isinstance ( impls , OrderMixin ) : result = OrderedDict ( ) for name in impls . order : result [ name ] = impls [ name ] . interface return result elif isinstance ( impls , Mapping ) : return { name : impls [ name ] . interface for name in impls } elif isinstance ( impls , Sequence ) : return [ impl . interface for impl in impls ] else : return impls . interface
1718	def fix_js_args ( func ) : fcode = six . get_function_code ( func ) fargs = fcode . co_varnames [ fcode . co_argcount - 2 : fcode . co_argcount ] if fargs == ( 'this' , 'arguments' ) or fargs == ( 'arguments' , 'var' ) : return func code = append_arguments ( six . get_function_code ( func ) , ( 'this' , 'arguments' ) ) return types . FunctionType ( code , six . get_function_globals ( func ) , func . __name__ , closure = six . get_function_closure ( func ) )
12836	def render_vars ( self ) : return { 'records' : [ { 'message' : record . getMessage ( ) , 'time' : dt . datetime . fromtimestamp ( record . created ) . strftime ( '%H:%M:%S' ) , } for record in self . handler . records ] }
13714	def upload ( self ) : success = False batch = self . next ( ) if len ( batch ) == 0 : return False try : self . request ( batch ) success = True except Exception as e : self . log . error ( 'error uploading: %s' , e ) success = False if self . on_error : self . on_error ( e , batch ) finally : for item in batch : self . queue . task_done ( ) return success
4124	def data_cosine ( N = 1024 , A = 0.1 , sampling = 1024. , freq = 200 ) : r t = arange ( 0 , float ( N ) / sampling , 1. / sampling ) x = cos ( 2. * pi * t * freq ) + A * randn ( t . size ) return x
686	def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding
2632	def scale_out ( self , blocks = 1 ) : r = [ ] for i in range ( blocks ) : if self . provider : block = self . provider . submit ( self . launch_cmd , 1 , self . workers_per_node ) logger . debug ( "Launched block {}:{}" . format ( i , block ) ) if not block : raise ( ScalingFailed ( self . provider . label , "Attempts to provision nodes via provider has failed" ) ) self . engines . extend ( [ block ] ) r . extend ( [ block ] ) else : logger . error ( "No execution provider available" ) r = None return r
3837	async def set_focus ( self , set_focus_request ) : response = hangouts_pb2 . SetFocusResponse ( ) await self . _pb_request ( 'conversations/setfocus' , set_focus_request , response ) return response
6693	def get_or_create_bucket ( self , name ) : from boto . s3 import connection if self . dryrun : print ( 'boto.connect_s3().create_bucket(%s)' % repr ( name ) ) else : conn = connection . S3Connection ( self . genv . aws_access_key_id , self . genv . aws_secret_access_key ) bucket = conn . create_bucket ( name ) return bucket
4389	def adsGetLocalAddressEx ( port ) : get_local_address_ex = _adsDLL . AdsGetLocalAddressEx ams_address_struct = SAmsAddr ( ) error_code = get_local_address_ex ( port , ctypes . pointer ( ams_address_struct ) ) if error_code : raise ADSError ( error_code ) local_ams_address = AmsAddr ( ) local_ams_address . _ams_addr = ams_address_struct return local_ams_address
5813	def detect_other_protocol ( server_handshake_bytes ) : if server_handshake_bytes [ 0 : 5 ] == b'HTTP/' : return 'HTTP' if server_handshake_bytes [ 0 : 4 ] == b'220 ' : if re . match ( b'^[^\r\n]*ftp' , server_handshake_bytes , re . I ) : return 'FTP' else : return 'SMTP' if server_handshake_bytes [ 0 : 4 ] == b'220-' : return 'FTP' if server_handshake_bytes [ 0 : 4 ] == b'+OK ' : return 'POP3' if server_handshake_bytes [ 0 : 4 ] == b'* OK' or server_handshake_bytes [ 0 : 9 ] == b'* PREAUTH' : return 'IMAP' return None
3039	def has_scopes ( self , scopes ) : scopes = _helpers . string_to_scopes ( scopes ) return set ( scopes ) . issubset ( self . scopes )
9317	def refresh ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response )
8655	def search_messages ( session , thread_id , query , limit = 20 , offset = 0 , message_context_details = None , window_above = None , window_below = None ) : query = { 'thread_id' : thread_id , 'query' : query , 'limit' : limit , 'offset' : offset } if message_context_details : query [ 'message_context_details' ] = message_context_details if window_above : query [ 'window_above' ] = window_above if window_below : query [ 'window_below' ] = window_below response = make_get_request ( session , 'messages/search' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
1329	def has_gradient ( self ) : try : self . __model . gradient self . __model . predictions_and_gradient except AttributeError : return False else : return True
4868	def to_representation ( self , instance ) : updated_course_run = copy . deepcopy ( instance ) enterprise_customer_catalog = self . context [ 'enterprise_customer_catalog' ] updated_course_run [ 'enrollment_url' ] = enterprise_customer_catalog . get_course_run_enrollment_url ( updated_course_run [ 'key' ] ) return updated_course_run
415	def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( "dataset_name is None, please give a dataset name" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( "[Database] Save dataset: SUCCESS, took: {}s" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( "{} {} {} {} {}" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( "[Database] Save dataset: FAIL" ) return False
12171	def count ( self , event ) : return len ( self . _listeners [ event ] ) + len ( self . _once [ event ] )
6100	def intensities_from_grid_radii ( self , grid_radii ) : return np . multiply ( np . divide ( self . intensity , self . sigma * np . sqrt ( 2.0 * np . pi ) ) , np . exp ( - 0.5 * np . square ( np . divide ( grid_radii , self . sigma ) ) ) )
1622	def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : for i in range ( begin , end ) : lines [ i ] = '/**/'
7983	def registration_error ( self , stanza ) : self . lock . acquire ( ) try : err = stanza . get_error ( ) ae = err . xpath_eval ( "e:*" , { "e" : "jabber:iq:auth:error" } ) if ae : ae = ae [ 0 ] . name else : ae = err . get_condition ( ) . name raise RegistrationError ( "Authentication error condition: %s" % ( ae , ) ) finally : self . lock . release ( )
7735	def set_stringprep_cache_size ( size ) : global _stringprep_cache_size _stringprep_cache_size = size if len ( Profile . cache_items ) > size : remove = Profile . cache_items [ : - size ] for profile , key in remove : try : del profile . cache [ key ] except KeyError : pass Profile . cache_items = Profile . cache_items [ - size : ]
13189	async def _upload_to_mongodb ( collection , jsonld ) : document = { 'data' : jsonld } query = { 'data.reportNumber' : jsonld [ 'reportNumber' ] } await collection . update ( query , document , upsert = True , multi = False )
7302	def get_mongoadmins ( self ) : apps = [ ] for app_name in settings . INSTALLED_APPS : mongoadmin = "{0}.mongoadmin" . format ( app_name ) try : module = import_module ( mongoadmin ) except ImportError as e : if str ( e ) . startswith ( "No module named" ) : continue raise e app_store = AppStore ( module ) apps . append ( dict ( app_name = app_name , obj = app_store ) ) return apps
8980	def temporary_path ( self , extension = "" ) : path = NamedTemporaryFile ( delete = False , suffix = extension ) . name self . path ( path ) return path
6690	def repolist ( status = '' , media = None ) : manager = MANAGER with settings ( hide ( 'running' , 'stdout' ) ) : if media : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) else : repos = run_as_root ( "%(manager)s repolist %(status)s | sed '/Media\\|Debug/d' | sed '$d' | sed -n '/repo id/,$p'" % locals ( ) ) return [ line . split ( ' ' ) [ 0 ] for line in repos . splitlines ( ) [ 1 : ] ]
11512	def share_item ( self , token , item_id , dest_folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id parameters [ 'dstfolderid' ] = dest_folder_id response = self . request ( 'midas.item.share' , parameters ) return response
1415	def create_pplan ( self , topologyName , pplan ) : if not pplan or not pplan . IsInitialized ( ) : raise_ ( StateException ( "Physical Plan protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_pplan_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) pplanString = pplan . SerializeToString ( ) try : self . client . create ( path , value = pplanString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating pplan" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating pplan" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating pplan" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
9270	def get_temp_tag_for_repo_creation ( self ) : tag_date = self . tag_times_dict . get ( REPO_CREATED_TAG_NAME , None ) if not tag_date : tag_name , tag_date = self . fetcher . fetch_repo_creation_date ( ) self . tag_times_dict [ tag_name ] = timestring_to_datetime ( tag_date ) return REPO_CREATED_TAG_NAME
5285	def get ( self , request , * args , ** kwargs ) : formset = self . construct_formset ( ) return self . render_to_response ( self . get_context_data ( formset = formset ) )
5163	def __intermediate_address ( self , address ) : for key in self . _address_keys : if key in address : del address [ key ] return address
4626	def decrypt ( self , wif ) : if not self . unlocked ( ) : raise WalletLocked return format ( bip38 . decrypt ( wif , self . masterkey ) , "wif" )
13260	def main ( argv = None , white_list = None , load_yaz_extension = True ) : assert argv is None or isinstance ( argv , list ) , type ( argv ) assert white_list is None or isinstance ( white_list , list ) , type ( white_list ) assert isinstance ( load_yaz_extension , bool ) , type ( load_yaz_extension ) argv = sys . argv if argv is None else argv assert len ( argv ) > 0 , len ( argv ) if load_yaz_extension : load ( "~/.yaz" , "yaz_extension" ) parser = Parser ( prog = argv [ 0 ] ) parser . add_task_tree ( get_task_tree ( white_list ) ) task , kwargs = parser . parse_arguments ( argv ) if task : try : result = task ( ** kwargs ) if isinstance ( result , bool ) : code = 0 if result else 1 output = None elif isinstance ( result , int ) : code = result % 256 output = None else : code = 0 output = result except Error as error : code = error . return_code output = error else : code = 1 output = parser . format_help ( ) . rstrip ( ) if output is not None : print ( output ) sys . exit ( code )
937	def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( "(%s) Creating local checkpoint in %r..." , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete (not a directory)" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( "Existing filesystem entry <%s> is not a model" " checkpoint -- refusing to delete" " (%s missing or not a file)" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( "(%s) Pickling Model instance..." , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( "(%s) Finished pickling Model instance" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( "(%s) Finished creating local checkpoint" , self ) return
4457	def get_args ( self ) : args = [ self . _query_string ] if self . _no_content : args . append ( 'NOCONTENT' ) if self . _fields : args . append ( 'INFIELDS' ) args . append ( len ( self . _fields ) ) args += self . _fields if self . _verbatim : args . append ( 'VERBATIM' ) if self . _no_stopwords : args . append ( 'NOSTOPWORDS' ) if self . _filters : for flt in self . _filters : assert isinstance ( flt , Filter ) args += flt . args if self . _with_payloads : args . append ( 'WITHPAYLOADS' ) if self . _ids : args . append ( 'INKEYS' ) args . append ( len ( self . _ids ) ) args += self . _ids if self . _slop >= 0 : args += [ 'SLOP' , self . _slop ] if self . _in_order : args . append ( 'INORDER' ) if self . _return_fields : args . append ( 'RETURN' ) args . append ( len ( self . _return_fields ) ) args += self . _return_fields if self . _sortby : assert isinstance ( self . _sortby , SortbyField ) args . append ( 'SORTBY' ) args += self . _sortby . args if self . _language : args += [ 'LANGUAGE' , self . _language ] args += self . _summarize_fields + self . _highlight_fields args += [ "LIMIT" , self . _offset , self . _num ] return args
6163	def QPSK_rx ( fc , N_symb , Rs , EsN0 = 100 , fs = 125 , lfsr_len = 10 , phase = 0 , pulse = 'src' ) : Ns = int ( np . round ( fs / Rs ) ) print ( 'Ns = ' , Ns ) print ( 'Rs = ' , fs / float ( Ns ) ) print ( 'EsN0 = ' , EsN0 , 'dB' ) print ( 'phase = ' , phase , 'degrees' ) print ( 'pulse = ' , pulse ) x , b , data = QPSK_bb ( N_symb , Ns , lfsr_len , pulse ) x = cpx_AWGN ( x , EsN0 , Ns ) n = np . arange ( len ( x ) ) xc = x * np . exp ( 1j * 2 * np . pi * fc / float ( fs ) * n ) * np . exp ( 1j * phase ) return xc , b , data
8439	def _generate_files ( repo_dir , config , template , version ) : with unittest . mock . patch ( 'cookiecutter.generate.run_hook' , side_effect = _patched_run_hook ) : cc_generate . generate_files ( repo_dir = repo_dir , context = { 'cookiecutter' : config , 'template' : template , 'version' : version } , overwrite_if_exists = False , output_dir = '.' )
1560	def register_metric ( self , name , metric , time_bucket_in_sec ) : collector = self . get_metrics_collector ( ) collector . register_metric ( name , metric , time_bucket_in_sec )
4819	def connect ( self ) : if JwtBuilder is None : raise NotConnectedToOpenEdX ( "This package must be installed in an OpenEdX environment." ) now = int ( time ( ) ) jwt = JwtBuilder . create_jwt_for_user ( self . user ) self . client = EdxRestApiClient ( self . API_BASE_URL , append_slash = self . APPEND_SLASH , jwt = jwt , ) self . expires_at = now + self . expires_in
4030	def load ( self ) : con = sqlite3 . connect ( self . tmp_cookie_file ) cur = con . cursor ( ) try : cur . execute ( 'SELECT host_key, path, secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) except sqlite3 . OperationalError : cur . execute ( 'SELECT host_key, path, is_secure, expires_utc, name, value, encrypted_value ' 'FROM cookies WHERE host_key like "%{}%";' . format ( self . domain_name ) ) cj = http . cookiejar . CookieJar ( ) for item in cur . fetchall ( ) : host , path , secure , expires , name = item [ : 5 ] value = self . _decrypt ( item [ 5 ] , item [ 6 ] ) c = create_cookie ( host , path , secure , expires , name , value ) cj . set_cookie ( c ) con . close ( ) return cj
13855	def getTextFromNode ( node ) : t = "" for n in node . childNodes : if n . nodeType == n . TEXT_NODE : t += n . nodeValue else : raise NotTextNodeError return t
7030	def specwindow_lsp_value ( times , mags , errs , omega ) : norm_times = times - times . min ( ) tau = ( ( 1.0 / ( 2.0 * omega ) ) * nparctan ( npsum ( npsin ( 2.0 * omega * norm_times ) ) / npsum ( npcos ( 2.0 * omega * norm_times ) ) ) ) lspval_top_cos = ( npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npcos ( omega * ( norm_times - tau ) ) ) ) lspval_bot_cos = npsum ( ( npcos ( omega * ( norm_times - tau ) ) ) * ( npcos ( omega * ( norm_times - tau ) ) ) ) lspval_top_sin = ( npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) * npsum ( 1.0 * npsin ( omega * ( norm_times - tau ) ) ) ) lspval_bot_sin = npsum ( ( npsin ( omega * ( norm_times - tau ) ) ) * ( npsin ( omega * ( norm_times - tau ) ) ) ) lspval = 0.5 * ( ( lspval_top_cos / lspval_bot_cos ) + ( lspval_top_sin / lspval_bot_sin ) ) return lspval
11842	def ModelBasedVacuumAgent ( ) : "An agent that keeps track of what locations are clean or dirty." model = { loc_A : None , loc_B : None } def program ( ( location , status ) ) : "Same as ReflexVacuumAgent, except if everything is clean, do NoOp." model [ location ] = status if model [ loc_A ] == model [ loc_B ] == 'Clean' : return 'NoOp' elif status == 'Dirty' : return 'Suck' elif location == loc_A : return 'Right' elif location == loc_B : return 'Left' return Agent ( program )
12876	def many ( parser ) : results = [ ] terminate = object ( ) while local_ps . value : result = optional ( parser , terminate ) if result == terminate : break results . append ( result ) return results
1517	def start_master_nodes ( masters , cl_args ) : pids = [ ] for master in masters : Log . info ( "Starting master on %s" % master ) cmd = "%s agent -config %s >> /tmp/nomad_server_log 2>&1 &" % ( get_nomad_path ( cl_args ) , get_nomad_master_config_file ( cl_args ) ) if not is_self ( master ) : cmd = ssh_remote_execute ( cmd , master , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) pids . append ( { "pid" : pid , "dest" : master } ) errors = [ ] for entry in pids : pid = entry [ "pid" ] return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) if return_code != 0 : errors . append ( "Failed to start master on %s with error:\n%s" % ( entry [ "dest" ] , output [ 1 ] ) ) if errors : for error in errors : Log . error ( error ) sys . exit ( - 1 ) Log . info ( "Done starting masters" )
3215	def get_vpc_flow_logs ( vpc , ** conn ) : fl_result = describe_flow_logs ( Filters = [ { "Name" : "resource-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) fl_ids = [ ] for fl in fl_result : fl_ids . append ( fl [ "FlowLogId" ] ) return fl_ids
6013	def load_background_sky_map ( background_sky_map_path , background_sky_map_hdu , pixel_scale ) : if background_sky_map_path is not None : return ScaledSquarePixelArray . from_fits_with_pixel_scale ( file_path = background_sky_map_path , hdu = background_sky_map_hdu , pixel_scale = pixel_scale ) else : return None
12494	def check_X_y ( X , y , accept_sparse = None , dtype = None , order = None , copy = False , force_all_finite = True , ensure_2d = True , allow_nd = False , multi_output = False ) : X = check_array ( X , accept_sparse , dtype , order , copy , force_all_finite , ensure_2d , allow_nd ) if multi_output : y = check_array ( y , 'csr' , force_all_finite = True , ensure_2d = False ) else : y = column_or_1d ( y , warn = True ) _assert_all_finite ( y ) check_consistent_length ( X , y ) return X , y
3850	def fetch_raw ( self , method , url , params = None , headers = None , data = None ) : if not urllib . parse . urlparse ( url ) . hostname . endswith ( '.google.com' ) : raise Exception ( 'expected google.com domain' ) headers = headers or { } headers . update ( self . _authorization_headers ) return self . _session . request ( method , url , params = params , headers = headers , data = data , proxy = self . _proxy )
12575	def apply_mask ( self , mask_img ) : self . set_mask ( mask_img ) return self . get_data ( masked = True , smoothed = True , safe_copy = True )
8284	def _segment_lengths ( self , relative = False , n = 20 ) : lengths = [ ] first = True for el in self . _get_elements ( ) : if first is True : close_x , close_y = el . x , el . y first = False elif el . cmd == MOVETO : close_x , close_y = el . x , el . y lengths . append ( 0.0 ) elif el . cmd == CLOSE : lengths . append ( self . _linelength ( x0 , y0 , close_x , close_y ) ) elif el . cmd == LINETO : lengths . append ( self . _linelength ( x0 , y0 , el . x , el . y ) ) elif el . cmd == CURVETO : x3 , y3 , x1 , y1 , x2 , y2 = el . x , el . y , el . c1x , el . c1y , el . c2x , el . c2y lengths . append ( self . _curvelength ( x0 , y0 , x1 , y1 , x2 , y2 , x3 , y3 , n ) ) if el . cmd != CLOSE : x0 = el . x y0 = el . y if relative : length = sum ( lengths ) try : return map ( lambda l : l / length , lengths ) except ZeroDivisionError : return [ 0.0 ] * len ( lengths ) else : return lengths
10137	def _assert_version ( self , version ) : if self . nearest_version < version : if self . _version_given : raise ValueError ( 'Data type requires version %s' % version ) else : self . _version = version
13044	def main ( ) : config = Config ( ) pipes_dir = config . get ( 'pipes' , 'directory' ) pipes_config = config . get ( 'pipes' , 'config_file' ) pipes_config_path = os . path . join ( config . config_dir , pipes_config ) if not os . path . exists ( pipes_config_path ) : print_error ( "Please configure the named pipes first" ) return workers = create_pipe_workers ( pipes_config_path , pipes_dir ) if workers : for worker in workers : worker . start ( ) try : for worker in workers : worker . join ( ) except KeyboardInterrupt : print_notification ( "Shutting down" ) for worker in workers : worker . terminate ( ) worker . join ( )
8188	def betweenness_centrality ( self , normalized = True ) : bc = proximity . brandes_betweenness_centrality ( self , normalized ) for id , w in bc . iteritems ( ) : self [ id ] . _betweenness = w return bc
11009	def subscribe ( self , event , bet_ids ) : if not self . _subscriptions . get ( event ) : self . _subscriptions [ event ] = set ( ) self . _subscriptions [ event ] = self . _subscriptions [ event ] . union ( bet_ids )
11384	def module ( self ) : if not hasattr ( self , '_module' ) : if "__main__" in sys . modules : mod = sys . modules [ "__main__" ] path = self . normalize_path ( mod . __file__ ) if os . path . splitext ( path ) == os . path . splitext ( self . path ) : self . _module = mod else : self . _module = imp . load_source ( 'captain_script' , self . path ) return self . _module
3851	async def lookup_entities ( client , args ) : lookup_spec = _get_lookup_spec ( args . entity_identifier ) request = hangups . hangouts_pb2 . GetEntityByIdRequest ( request_header = client . get_request_header ( ) , batch_lookup_spec = [ lookup_spec ] , ) res = await client . get_entity_by_id ( request ) for entity_result in res . entity_result : for entity in entity_result . entity : print ( entity )
5601	def serve ( mapchete_file , port = None , internal_cache = None , zoom = None , bounds = None , overwrite = False , readonly = False , memory = False , input_file = None , debug = False , logfile = None ) : app = create_app ( mapchete_files = [ mapchete_file ] , zoom = zoom , bounds = bounds , single_input_file = input_file , mode = _get_mode ( memory , readonly , overwrite ) , debug = debug ) if os . environ . get ( "MAPCHETE_TEST" ) == "TRUE" : logger . debug ( "don't run flask app, MAPCHETE_TEST environment detected" ) else : app . run ( threaded = True , debug = True , port = port , host = '0.0.0.0' , extra_files = [ mapchete_file ] )
13107	def remove_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) - set ( [ tag ] ) )
286	def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax
11443	def _compare_lists ( list1 , list2 , custom_cmp ) : if len ( list1 ) != len ( list2 ) : return False for element1 , element2 in zip ( list1 , list2 ) : if not custom_cmp ( element1 , element2 ) : return False return True
6328	def _add_to_ngcorpus ( self , corpus , words , count ) : if words [ 0 ] not in corpus : corpus [ words [ 0 ] ] = Counter ( ) if len ( words ) == 1 : corpus [ words [ 0 ] ] [ None ] += count else : self . _add_to_ngcorpus ( corpus [ words [ 0 ] ] , words [ 1 : ] , count )
12414	def flush ( self ) : self . require_not_closed ( ) chunk = self . _stream . getvalue ( ) self . _stream . truncate ( 0 ) self . _stream . seek ( 0 ) self . body = chunk if ( self . _body is None ) else ( self . _body + chunk ) if self . asynchronous : self . streaming = True
1523	def log ( self , message , level = None ) : if level is None : _log_level = logging . INFO else : if level == "trace" or level == "debug" : _log_level = logging . DEBUG elif level == "info" : _log_level = logging . INFO elif level == "warn" : _log_level = logging . WARNING elif level == "error" : _log_level = logging . ERROR else : raise ValueError ( "%s is not supported as logging level" % str ( level ) ) self . logger . log ( _log_level , message )
6370	def specificity ( self ) : r if self . _tn + self . _fp == 0 : return float ( 'NaN' ) return self . _tn / ( self . _tn + self . _fp )
7411	def sample_loci ( self ) : idxs = np . random . choice ( self . idxs , self . ntests ) with open ( self . data ) as indata : liter = ( indata . read ( ) . strip ( ) . split ( "|\n" ) ) seqdata = { i : "" for i in self . samples } for idx , loc in enumerate ( liter ) : if idx in idxs : lines = loc . split ( "\n" ) [ : - 1 ] names = [ i . split ( ) [ 0 ] for i in lines ] seqs = [ i . split ( ) [ 1 ] for i in lines ] dd = { i : j for i , j in zip ( names , seqs ) } for name in seqdata : if name in names : seqdata [ name ] += dd [ name ] else : seqdata [ name ] += "N" * len ( seqs [ 0 ] ) return seqdata
4932	def transform_courserun_schedule ( self , content_metadata_item ) : start = content_metadata_item . get ( 'start' ) or UNIX_MIN_DATE_STRING end = content_metadata_item . get ( 'end' ) or UNIX_MAX_DATE_STRING return [ { 'startDate' : parse_datetime_to_epoch_millis ( start ) , 'endDate' : parse_datetime_to_epoch_millis ( end ) , 'active' : current_time_is_in_interval ( start , end ) } ]
5209	def info_qry ( tickers , flds ) -> str : full_list = '\n' . join ( [ f'tickers: {tickers[:8]}' ] + [ f' {tickers[n:(n + 8)]}' for n in range ( 8 , len ( tickers ) , 8 ) ] ) return f'{full_list}\nfields: {flds}'
11583	def image_urls ( self ) : all_image_urls = self . finder_image_urls [ : ] for image_url in self . extender_image_urls : if image_url not in all_image_urls : all_image_urls . append ( image_url ) return all_image_urls
8919	def _get_service ( self ) : if "service" in self . document . attrib : value = self . document . attrib [ "service" ] . lower ( ) if value in allowed_service_types : self . params [ "service" ] = value else : raise OWSInvalidParameterValue ( "Service %s is not supported" % value , value = "service" ) else : raise OWSMissingParameterValue ( 'Parameter "service" is missing' , value = "service" ) return self . params [ "service" ]
4494	def upload ( args ) : osf = _setup_osf ( args ) if osf . username is None or osf . password is None : sys . exit ( 'To upload a file you need to provide a username and' ' password.' ) project = osf . project ( args . project ) storage , remote_path = split_storage ( args . destination ) store = project . storage ( storage ) if args . recursive : if not os . path . isdir ( args . source ) : raise RuntimeError ( "Expected source ({}) to be a directory when " "using recursive mode." . format ( args . source ) ) _ , dir_name = os . path . split ( args . source ) for root , _ , files in os . walk ( args . source ) : subdir_path = os . path . relpath ( root , args . source ) for fname in files : local_path = os . path . join ( root , fname ) with open ( local_path , 'rb' ) as fp : name = os . path . join ( remote_path , dir_name , subdir_path , fname ) store . create_file ( name , fp , force = args . force , update = args . update ) else : with open ( args . source , 'rb' ) as fp : store . create_file ( remote_path , fp , force = args . force , update = args . update )
3188	def create ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'note' not in data : raise KeyError ( 'The list member note must have a note' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'notes' ) , data = data ) if response is not None : self . note_id = response [ 'id' ] else : self . note_id = None return response
1754	def write_register ( self , register , value ) : self . _publish ( 'will_write_register' , register , value ) value = self . _regfile . write ( register , value ) self . _publish ( 'did_write_register' , register , value ) return value
7277	def set_video_pos ( self , x1 , y1 , x2 , y2 ) : position = "%s %s %s %s" % ( str ( x1 ) , str ( y1 ) , str ( x2 ) , str ( y2 ) ) self . _player_interface . VideoPos ( ObjectPath ( '/not/used' ) , String ( position ) )
7377	def _user_headers ( self , headers = None ) : h = self . copy ( ) if headers is not None : keys = set ( headers . keys ( ) ) if h . get ( 'Authorization' , False ) : keys -= { 'Authorization' } for key in keys : h [ key ] = headers [ key ] return h
10416	def data_contains_key_builder ( key : str ) -> NodePredicate : def data_contains_key ( _ : BELGraph , node : BaseEntity ) -> bool : return key in node return data_contains_key
5420	def _get_job_resources ( args ) : logging = param_util . build_logging_param ( args . logging ) if args . logging else None timeout = param_util . timeout_in_seconds ( args . timeout ) log_interval = param_util . log_interval_in_seconds ( args . log_interval ) return job_model . Resources ( min_cores = args . min_cores , min_ram = args . min_ram , machine_type = args . machine_type , disk_size = args . disk_size , disk_type = args . disk_type , boot_disk_size = args . boot_disk_size , preemptible = args . preemptible , image = args . image , regions = args . regions , zones = args . zones , logging = logging , logging_path = None , service_account = args . service_account , scopes = args . scopes , keep_alive = args . keep_alive , cpu_platform = args . cpu_platform , network = args . network , subnetwork = args . subnetwork , use_private_address = args . use_private_address , accelerator_type = args . accelerator_type , accelerator_count = args . accelerator_count , nvidia_driver_version = args . nvidia_driver_version , timeout = timeout , log_interval = log_interval , ssh = args . ssh )
13271	def unique_justseen ( iterable , key = None ) : "List unique elements, preserving order. Remember only the element just seen." try : from itertools import imap as map except ImportError : from builtins import map return map ( next , map ( operator . itemgetter ( 1 ) , itertools . groupby ( iterable , key ) ) )
13413	def addMenu ( self ) : self . parent . multiLogLayout . addLayout ( self . logSelectLayout ) self . getPrograms ( logType , programName )
1337	def softmax ( logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) return e / np . sum ( e )
113	def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )
5818	def get_path ( temp_dir = None , cache_length = 24 , cert_callback = None ) : ca_path , temp = _ca_path ( temp_dir ) if temp and _cached_path_needs_update ( ca_path , cache_length ) : empty_set = set ( ) any_purpose = '2.5.29.37.0' apple_ssl = '1.2.840.113635.100.1.3' win_server_auth = '1.3.6.1.5.5.7.3.1' with path_lock : if _cached_path_needs_update ( ca_path , cache_length ) : with open ( ca_path , 'wb' ) as f : for cert , trust_oids , reject_oids in extract_from_system ( cert_callback , True ) : if sys . platform == 'darwin' : if trust_oids != empty_set and any_purpose not in trust_oids and apple_ssl not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( apple_ssl in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue elif sys . platform == 'win32' : if trust_oids != empty_set and any_purpose not in trust_oids and win_server_auth not in trust_oids : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'implicitly distrusted for TLS' ) continue if reject_oids != empty_set and ( win_server_auth in reject_oids or any_purpose in reject_oids ) : if cert_callback : cert_callback ( Certificate . load ( cert ) , 'explicitly distrusted for TLS' ) continue if cert_callback : cert_callback ( Certificate . load ( cert ) , None ) f . write ( armor ( 'CERTIFICATE' , cert ) ) if not ca_path : raise CACertsError ( 'No CA certs found' ) return ca_path
9424	def _open ( self , archive ) : try : handle = unrarlib . RAROpenArchiveEx ( ctypes . byref ( archive ) ) except unrarlib . UnrarException : raise BadRarFile ( "Invalid RAR file." ) return handle
6480	def null ( self ) : if not self . option . axis : return - 1 else : return self . screen . height - ( - self . minimum * 4.0 / self . extents * self . size . y )
1202	def execute ( self , action ) : adjusted_action = list ( ) for action_spec in self . level . action_spec ( ) : if action_spec [ 'min' ] == - 1 and action_spec [ 'max' ] == 1 : adjusted_action . append ( action [ action_spec [ 'name' ] ] - 1 ) else : adjusted_action . append ( action [ action_spec [ 'name' ] ] ) action = np . array ( adjusted_action , dtype = np . intc ) reward = self . level . step ( action = action , num_steps = self . repeat_action ) state = self . level . observations ( ) [ 'RGB_INTERLACED' ] terminal = not self . level . is_running ( ) return state , terminal , reward
4411	def store ( self , key : object , value : object ) : self . _user_data . update ( { key : value } )
12799	def _url ( self , url = None , parameters = None ) : uri = url or self . _settings [ "url" ] if url and self . _settings [ "base_url" ] : uri = "%s/%s" % ( self . _settings [ "base_url" ] , url ) uri += ".json" if parameters : uri += "?%s" % urllib . urlencode ( parameters ) return uri
2802	def convert_concat ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting concat ...' ) concat_nodes = [ layers [ i ] for i in inputs ] if len ( concat_nodes ) == 1 : layers [ scope_name ] = concat_nodes [ 0 ] return if names == 'short' : tf_name = 'CAT' + random_string ( 5 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) cat = keras . layers . Concatenate ( name = tf_name , axis = params [ 'axis' ] ) layers [ scope_name ] = cat ( concat_nodes )
6445	def _cond_x ( self , word , suffix_len ) : return word [ - suffix_len - 1 ] in { 'i' , 'l' } or ( word [ - suffix_len - 3 : - suffix_len ] == 'u' and word [ - suffix_len - 1 ] == 'e' )
6086	def unmasked_blurred_image_of_planes_and_galaxies_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : return [ plane . unmasked_blurred_image_of_galaxies_from_psf ( padded_grid_stack , psf ) for plane in planes ]
12777	def resorted ( values ) : if not values : return values values = sorted ( values ) first_word = next ( ( cnt for cnt , val in enumerate ( values ) if val and not val [ 0 ] . isdigit ( ) ) , None ) if first_word is None : return values words = values [ first_word : ] numbers = values [ : first_word ] return words + numbers
1504	def template_apiserver_hcl ( cl_args , masters , zookeepers ) : single_master = masters [ 0 ] apiserver_config_template = "%s/standalone/templates/apiserver.template.hcl" % cl_args [ "config_path" ] apiserver_config_actual = "%s/standalone/resources/apiserver.hcl" % cl_args [ "config_path" ] replacements = { "<heron_apiserver_hostname>" : '"%s"' % get_hostname ( single_master , cl_args ) , "<heron_apiserver_executable>" : '"%s/heron-apiserver"' % config . get_heron_bin_dir ( ) if is_self ( single_master ) else '"%s/.heron/bin/heron-apiserver"' % get_remote_home ( single_master , cl_args ) , "<zookeeper_host:zookeeper_port>" : "," . join ( [ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , "<scheduler_uri>" : "http://%s:4646" % single_master } template_file ( apiserver_config_template , apiserver_config_actual , replacements )
9344	def read ( self , n ) : while len ( self . pool ) < n : self . cur = self . files . next ( ) self . pool = numpy . append ( self . pool , self . fetch ( self . cur ) , axis = 0 ) rt = self . pool [ : n ] if n == len ( self . pool ) : self . pool = self . fetch ( None ) else : self . pool = self . pool [ n : ] return rt
7315	def parse_filter ( self , filters ) : for filter_type in filters : if filter_type == 'or' or filter_type == 'and' : conditions = [ ] for field in filters [ filter_type ] : if self . is_field_allowed ( field ) : conditions . append ( self . create_query ( self . parse_field ( field , filters [ filter_type ] [ field ] ) ) ) if filter_type == 'or' : self . model_query = self . model_query . filter ( or_ ( * conditions ) ) elif filter_type == 'and' : self . model_query = self . model_query . filter ( and_ ( * conditions ) ) else : if self . is_field_allowed ( filter_type ) : conditions = self . create_query ( self . parse_field ( filter_type , filters [ filter_type ] ) ) self . model_query = self . model_query . filter ( conditions ) return self . model_query
11566	def stepper_step ( self , motor_speed , number_of_steps ) : if number_of_steps > 0 : direction = 1 else : direction = 0 abs_number_of_steps = abs ( number_of_steps ) data = [ self . STEPPER_STEP , motor_speed & 0x7f , ( motor_speed >> 7 ) & 0x7f , ( motor_speed >> 14 ) & 0x7f , abs_number_of_steps & 0x7f , ( abs_number_of_steps >> 7 ) & 0x7f , direction ] self . _command_handler . send_sysex ( self . _command_handler . STEPPER_DATA , data )
3380	def add_lexicographic_constraints ( model , objectives , objective_direction = 'max' ) : if type ( objective_direction ) is not list : objective_direction = [ objective_direction ] * len ( objectives ) constraints = [ ] for rxn_id , obj_dir in zip ( objectives , objective_direction ) : model . objective = model . reactions . get_by_id ( rxn_id ) model . objective_direction = obj_dir constraints . append ( fix_objective_as_constraint ( model ) ) return pd . Series ( constraints , index = objectives )
4625	def change_password ( self , newpassword ) : if not self . unlocked ( ) : raise WalletLocked self . password = newpassword self . _save_encrypted_masterpassword ( )
8306	def close ( self ) : self . process . stdout . close ( ) self . process . stderr . close ( ) self . running = False
8912	def includeme ( config ) : settings = config . registry . settings if asbool ( settings . get ( 'twitcher.rpcinterface' , True ) ) : LOGGER . debug ( 'Twitcher XML-RPC Interface enabled.' ) config . include ( 'twitcher.config' ) config . include ( 'twitcher.basicauth' ) config . include ( 'pyramid_rpc.xmlrpc' ) config . include ( 'twitcher.db' ) config . add_xmlrpc_endpoint ( 'api' , '/RPC2' ) config . add_xmlrpc_method ( RPCInterface , attr = 'generate_token' , endpoint = 'api' , method = 'generate_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_token' , endpoint = 'api' , method = 'revoke_token' ) config . add_xmlrpc_method ( RPCInterface , attr = 'revoke_all_tokens' , endpoint = 'api' , method = 'revoke_all_tokens' ) config . add_xmlrpc_method ( RPCInterface , attr = 'register_service' , endpoint = 'api' , method = 'register_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'unregister_service' , endpoint = 'api' , method = 'unregister_service' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_name' , endpoint = 'api' , method = 'get_service_by_name' ) config . add_xmlrpc_method ( RPCInterface , attr = 'get_service_by_url' , endpoint = 'api' , method = 'get_service_by_url' ) config . add_xmlrpc_method ( RPCInterface , attr = 'clear_services' , endpoint = 'api' , method = 'clear_services' ) config . add_xmlrpc_method ( RPCInterface , attr = 'list_services' , endpoint = 'api' , method = 'list_services' )
11571	def set_bit_map ( self , shape , color ) : for row in range ( 0 , 8 ) : data = shape [ row ] bit_mask = 0x80 for column in range ( 0 , 8 ) : if data & bit_mask : self . set_pixel ( row , column , color , True ) bit_mask >>= 1 self . output_entire_buffer ( )
6168	def from_bin ( bin_array ) : width = len ( bin_array ) bin_wgts = 2 ** np . arange ( width - 1 , - 1 , - 1 ) return int ( np . dot ( bin_array , bin_wgts ) )
9409	def _extract ( data , session = None ) : if isinstance ( data , list ) : return [ _extract ( d , session ) for d in data ] if not isinstance ( data , np . ndarray ) : return data if isinstance ( data , MatlabObject ) : cls = session . _get_user_class ( data . classname ) return cls . from_value ( data ) if data . dtype . names : if data . size == 1 : return _create_struct ( data , session ) return StructArray ( data , session ) if data . dtype . kind == 'O' : return Cell ( data , session ) if data . size == 1 : return data . item ( ) if data . size == 0 : if data . dtype . kind in 'US' : return '' return [ ] return data
10289	def enrich_reactions ( graph : BELGraph ) : nodes = list ( get_nodes_by_function ( graph , REACTION ) ) for u in nodes : for v in u . reactants : graph . add_has_reactant ( u , v ) for v in u . products : graph . add_has_product ( u , v )
1061	def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '"' ) and s . endswith ( '"' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\"' , '"' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s
5699	def write_stats_as_csv ( gtfs , path_to_csv , re_write = False ) : stats_dict = get_stats ( gtfs ) if re_write : os . remove ( path_to_csv ) is_new = True mode = 'r' if os . path . exists ( path_to_csv ) else 'w+' with open ( path_to_csv , mode ) as csvfile : for line in csvfile : if line : is_new = False else : is_new = True with open ( path_to_csv , 'a' ) as csvfile : if ( sys . version_info > ( 3 , 0 ) ) : delimiter = u"," else : delimiter = b"," statswriter = csv . writer ( csvfile , delimiter = delimiter ) if is_new : statswriter . writerow ( [ key for key in sorted ( stats_dict . keys ( ) ) ] ) row_to_write = [ ] for key in sorted ( stats_dict . keys ( ) ) : row_to_write . append ( stats_dict [ key ] ) statswriter . writerow ( row_to_write )
4368	def error ( self , error_name , error_message , msg_id = None , quiet = False ) : self . socket . error ( error_name , error_message , endpoint = self . ns_name , msg_id = msg_id , quiet = quiet )
12650	def is_fnmatch_regex ( string ) : is_regex = False regex_chars = [ '!' , '*' , '$' ] for c in regex_chars : if string . find ( c ) > - 1 : return True return is_regex
11351	def merge_from_list ( self , list_args ) : def xs ( name , parser_args , list_args ) : for args , kwargs in list_args : if len ( set ( args ) & parser_args ) > 0 : yield args , kwargs else : if 'dest' in kwargs : if kwargs [ 'dest' ] == name : yield args , kwargs for args , kwargs in xs ( self . name , self . parser_args , list_args ) : self . merge_args ( args ) self . merge_kwargs ( kwargs )
9924	def create ( self , * args , ** kwargs ) : is_primary = kwargs . pop ( "is_primary" , False ) with transaction . atomic ( ) : email = super ( EmailAddressManager , self ) . create ( * args , ** kwargs ) if is_primary : email . set_primary ( ) return email
602	def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) plt . draw ( )
12282	def get_resource ( self , p ) : for r in self . package [ 'resources' ] : if r [ 'relativepath' ] == p : r [ 'localfullpath' ] = os . path . join ( self . rootdir , p ) return r raise Exception ( "Invalid path" )
10912	def vectorize_damping ( params , damping = 1.0 , increase_list = [ [ 'psf-' , 1e4 ] ] ) : damp_vec = np . ones ( len ( params ) ) * damping for nm , fctr in increase_list : for a in range ( damp_vec . size ) : if nm in params [ a ] : damp_vec [ a ] *= fctr return damp_vec
13617	def publish ( ) : try : build_site ( dev_mode = False , clean = True ) click . echo ( 'Deploying the site...' ) call ( "rsync -avz -e ssh --progress %s/ %s" % ( BUILD_DIR , CONFIG [ "scp_target" ] , ) , shell = True ) if "cloudflare" in CONFIG and "purge" in CONFIG [ "cloudflare" ] and CONFIG [ "cloudflare" ] [ "purge" ] : do_purge ( ) except ( KeyboardInterrupt , SystemExit ) : raise sys . exit ( 1 )
12797	def build_twisted_request ( self , method , url , extra_headers = { } , body_producer = None , full_url = False ) : uri = url if full_url else self . _url ( url ) raw_headers = self . get_headers ( ) if extra_headers : raw_headers . update ( extra_headers ) headers = http_headers . Headers ( ) for header in raw_headers : headers . addRawHeader ( header , raw_headers [ header ] ) agent = client . Agent ( reactor ) request = agent . request ( method , uri , headers , body_producer ) return ( reactor , request )
9850	def export ( self , filename , file_format = None , type = None , typequote = '"' ) : exporter = self . _get_exporter ( filename , file_format = file_format ) exporter ( filename , type = type , typequote = typequote )
1733	def is_object ( n , last ) : if is_empty_object ( n , last ) : return True if not n . strip ( ) : return False if len ( argsplit ( n , ';' ) ) > 1 : return False cands = argsplit ( n , ',' ) if not cands [ - 1 ] . strip ( ) : return True for cand in cands : cand = cand . strip ( ) kv = argsplit ( cand , ':' ) if len ( kv ) > 2 : kv = kv [ 0 ] , ':' . join ( kv [ 1 : ] ) if len ( kv ) == 2 : k , v = kv if not is_lval ( k . strip ( ) ) : return False v = v . strip ( ) if v . startswith ( 'function' ) : continue if v [ 0 ] == '{' : return False for e in KEYWORD_METHODS : if v . startswith ( e ) and len ( e ) < len ( v ) and v [ len ( e ) ] not in IDENTIFIER_PART : return False elif not ( cand . startswith ( 'set ' ) or cand . startswith ( 'get ' ) ) : return False return True
1649	def GetLineWidth ( line ) : if isinstance ( line , unicode ) : width = 0 for uc in unicodedata . normalize ( 'NFC' , line ) : if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : width += 2 elif not unicodedata . combining ( uc ) : width += 1 return width else : return len ( line )
6314	def load ( self ) : self . create_effect_classes ( ) self . _add_resource_descriptions_to_pools ( self . create_external_resources ( ) ) self . _add_resource_descriptions_to_pools ( self . create_resources ( ) ) for meta , resource in resources . textures . load_pool ( ) : self . _textures [ meta . label ] = resource for meta , resource in resources . programs . load_pool ( ) : self . _programs [ meta . label ] = resource for meta , resource in resources . scenes . load_pool ( ) : self . _scenes [ meta . label ] = resource for meta , resource in resources . data . load_pool ( ) : self . _data [ meta . label ] = resource self . create_effect_instances ( ) self . post_load ( )
10113	def filter_rows_as_dict ( fname , filter_ , ** kw ) : filter_ = DictFilter ( filter_ ) rewrite ( fname , filter_ , ** kw ) return filter_ . removed
6748	def collect_genv ( self , include_local = True , include_global = True ) : e = type ( self . genv ) ( ) if include_global : e . update ( self . genv ) if include_local : for k , v in self . lenv . items ( ) : e [ '%s_%s' % ( self . obj . name . lower ( ) , k ) ] = v return e
12585	def spatialimg_to_hdfpath ( file_path , spatial_img , h5path = None , append = True ) : if h5path is None : h5path = '/img' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' with h5py . File ( file_path , mode ) as f : try : h5img = f . create_group ( h5path ) spatialimg_to_hdfgroup ( h5img , spatial_img ) except ValueError as ve : raise Exception ( 'Error creating group ' + h5path ) from ve
6728	def respawn ( name = None , group = None ) : if name is None : name = get_name ( ) delete ( name = name , group = group ) instance = get_or_create ( name = name , group = group ) env . host_string = instance . public_dns_name
10373	def node_has_namespaces ( node : BaseEntity , namespaces : Set [ str ] ) -> bool : ns = node . get ( NAMESPACE ) return ns is not None and ns in namespaces
8654	def get_messages ( session , query , limit = 10 , offset = 0 ) : query [ 'limit' ] = limit query [ 'offset' ] = offset response = make_get_request ( session , 'messages' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise MessagesNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
8289	def sbot_executable ( ) : gsettings = load_gsettings ( ) venv = gsettings . get_string ( 'current-virtualenv' ) if venv == 'Default' : sbot = which ( 'sbot' ) elif venv == 'System' : env_venv = os . environ . get ( 'VIRTUAL_ENV' ) if not env_venv : return which ( 'sbot' ) for p in os . environ [ 'PATH' ] . split ( os . path . pathsep ) : sbot = '%s/sbot' % p if not p . startswith ( env_venv ) and os . path . isfile ( sbot ) : return sbot else : sbot = os . path . join ( venv , 'bin/sbot' ) if not os . path . isfile ( sbot ) : print ( 'Shoebot not found, reverting to System shoebot' ) sbot = which ( 'sbot' ) return os . path . realpath ( sbot )
7468	def multi_muscle_align ( data , samples , ipyclient ) : LOGGER . info ( "starting alignments" ) lbview = ipyclient . load_balanced_view ( ) start = time . time ( ) printstr = " aligning clusters | {} | s6 |" elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) path = os . path . join ( data . tmpdir , data . name + ".chunk_*" ) clustbits = glob . glob ( path ) jobs = { } for idx in xrange ( len ( clustbits ) ) : args = [ data , samples , clustbits [ idx ] ] jobs [ idx ] = lbview . apply ( persistent_popen_align3 , * args ) allwait = len ( jobs ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( 20 , 0 , printstr . format ( elapsed ) , spacer = data . _spacer ) while 1 : finished = [ i . ready ( ) for i in jobs . values ( ) ] fwait = sum ( finished ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( allwait , fwait , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if all ( finished ) : break keys = jobs . keys ( ) for idx in keys : if not jobs [ idx ] . successful ( ) : LOGGER . error ( "error in persistent_popen_align %s" , jobs [ idx ] . exception ( ) ) raise IPyradWarningExit ( "error in step 6 {}" . format ( jobs [ idx ] . exception ( ) ) ) del jobs [ idx ] print ( "" )
10252	def highlight_edges ( graph : BELGraph , edges = None , color : Optional [ str ] = None ) -> None : color = color or EDGE_HIGHLIGHT_DEFAULT_COLOR for u , v , k , d in edges if edges is not None else graph . edges ( keys = True , data = True ) : graph [ u ] [ v ] [ k ] [ EDGE_HIGHLIGHT ] = color
3993	def _load_ssh_auth_post_yosemite ( mac_username ) : user_id = subprocess . check_output ( [ 'id' , '-u' , mac_username ] ) ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'asuser' , user_id , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) _set_ssh_auth_sock ( ssh_auth_sock )
11158	def mirror_to ( self , dst ) : self . assert_is_dir_and_exists ( ) src = self . abspath dst = os . path . abspath ( dst ) if os . path . exists ( dst ) : raise Exception ( "distination already exist!" ) folder_to_create = list ( ) file_to_create = list ( ) for current_folder , _ , file_list in os . walk ( self . abspath ) : current_folder = current_folder . replace ( src , dst ) try : os . mkdir ( current_folder ) except : pass for basename in file_list : abspath = os . path . join ( current_folder , basename ) with open ( abspath , "wb" ) as _ : pass
12100	def _append_log ( self , specs ) : self . _spec_log += specs log_path = os . path . join ( self . root_directory , ( "%s.log" % self . batch_name ) ) core . Log . write_log ( log_path , [ spec for ( _ , spec ) in specs ] , allow_append = True )
6919	def _autocorr_func1 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 1 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) acorr = ( 1.0 / ( ( maglen - lag ) * magstd ) ) * npsum ( products ) return acorr
2458	def set_pkg_license_comment ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_license_comment_set : self . package_license_comment_set = True if validations . validate_pkg_lics_comment ( text ) : doc . package . license_comment = str_from_text ( text ) return True else : raise SPDXValueError ( 'Package::LicenseComment' ) else : raise CardinalityError ( 'Package::LicenseComment' )
343	def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )
4042	def _extract_links ( self ) : extracted = dict ( ) try : for key , value in self . request . links . items ( ) : parsed = urlparse ( value [ "url" ] ) fragment = "{path}?{query}" . format ( path = parsed [ 2 ] , query = parsed [ 4 ] ) extracted [ key ] = fragment parsed = list ( urlparse ( self . self_link ) ) stripped = "&" . join ( [ "%s=%s" % ( p [ 0 ] , p [ 1 ] ) for p in parse_qsl ( parsed [ 4 ] ) if p [ 0 ] != "format" ] ) extracted [ "self" ] = urlunparse ( [ parsed [ 0 ] , parsed [ 1 ] , parsed [ 2 ] , parsed [ 3 ] , stripped , parsed [ 5 ] ] ) return extracted except KeyError : return None
1351	def make_response ( self , status ) : response = { constants . RESPONSE_KEY_STATUS : status , constants . RESPONSE_KEY_VERSION : constants . API_VERSION , constants . RESPONSE_KEY_EXECUTION_TIME : 0 , constants . RESPONSE_KEY_MESSAGE : "" , } return response
7206	def generate_workflow_description ( self ) : if not self . tasks : raise WorkflowError ( 'Workflow contains no tasks, and cannot be executed.' ) self . definition = self . workflow_skeleton ( ) if self . batch_values : self . definition [ "batch_values" ] = self . batch_values all_input_port_values = [ t . inputs . __getattribute__ ( input_port_name ) . value for t in self . tasks for input_port_name in t . inputs . _portnames ] for task in self . tasks : output_multiplex_ports_to_exclude = [ ] multiplex_output_port_names = [ portname for portname in task . outputs . _portnames if task . outputs . __getattribute__ ( portname ) . is_multiplex ] for p in multiplex_output_port_names : output_port_reference = 'source:' + task . name + ':' + p if output_port_reference not in all_input_port_values : output_multiplex_ports_to_exclude . append ( p ) task_def = task . generate_task_workflow_json ( output_multiplex_ports_to_exclude = output_multiplex_ports_to_exclude ) self . definition [ 'tasks' ] . append ( task_def ) if self . callback : self . definition [ 'callback' ] = self . callback return self . definition
6791	def loaddata ( self , path , site = None ) : site = site or self . genv . SITE r = self . local_renderer r . env . _loaddata_path = path for _site , site_data in self . iter_sites ( site = site , no_secure = True ) : try : self . set_db ( site = _site ) r . env . SITE = _site r . sudo ( 'export SITE={SITE}; export ROLE={ROLE}; ' 'cd {project_dir}; ' '{manage_cmd} loaddata {_loaddata_path}' ) except KeyError : pass
6332	def decode ( self , code , terminator = '\0' ) : r if code : if terminator not in code : raise ValueError ( 'Specified terminator, {}, absent from code.' . format ( terminator if terminator != '\0' else '\\0' ) ) else : wordlist = [ '' ] * len ( code ) for i in range ( len ( code ) ) : wordlist = sorted ( code [ i ] + wordlist [ i ] for i in range ( len ( code ) ) ) rows = [ w for w in wordlist if w [ - 1 ] == terminator ] [ 0 ] return rows . rstrip ( terminator ) else : return ''
5621	def tile_to_zoom_level ( tile , dst_pyramid = None , matching_method = "gdal" , precision = 8 ) : def width_height ( bounds ) : try : l , b , r , t = reproject_geometry ( box ( * bounds ) , src_crs = tile . crs , dst_crs = dst_pyramid . crs ) . bounds except ValueError : raise TopologicalError ( "bounds cannot be translated into target CRS" ) return r - l , t - b if tile . tp . crs == dst_pyramid . crs : return tile . zoom else : if matching_method == "gdal" : transform , width , height = calculate_default_transform ( tile . tp . crs , dst_pyramid . crs , tile . width , tile . height , * tile . bounds ) tile_resolution = round ( transform [ 0 ] , precision ) elif matching_method == "min" : l , b , r , t = tile . bounds x = tile . pixel_x_size y = tile . pixel_y_size res = [ ] for bounds in [ ( l , t - y , l + x , t ) , ( l , b , l + x , b + y ) , ( r - x , b , r , b + y ) , ( r - x , t - y , r , t ) ] : try : w , h = width_height ( bounds ) res . extend ( [ w , h ] ) except TopologicalError : logger . debug ( "pixel outside of destination pyramid" ) if res : tile_resolution = round ( min ( res ) , precision ) else : raise TopologicalError ( "tile outside of destination pyramid" ) else : raise ValueError ( "invalid method given: %s" , matching_method ) logger . debug ( "we are looking for a zoom level interpolating to %s resolution" , tile_resolution ) zoom = 0 while True : td_resolution = round ( dst_pyramid . pixel_x_size ( zoom ) , precision ) if td_resolution <= tile_resolution : break zoom += 1 logger . debug ( "target zoom for %s: %s (%s)" , tile_resolution , zoom , td_resolution ) return zoom
4827	def get_course_enrollment ( self , username , course_id ) : endpoint = getattr ( self . client . enrollment , '{username},{course_id}' . format ( username = username , course_id = course_id ) ) try : result = endpoint . get ( ) except HttpNotFoundError : LOGGER . error ( 'Course enrollment details not found for invalid username or course; username=[%s], course=[%s]' , username , course_id ) return None if not result : LOGGER . info ( 'Failed to find course enrollment details for user [%s] and course [%s]' , username , course_id ) return None return result
2268	def to_dict ( self ) : return self . _base ( ( key , ( value . to_dict ( ) if isinstance ( value , AutoDict ) else value ) ) for key , value in self . items ( ) )
11975	def _add ( self , other ) : if isinstance ( other , self . __class__ ) : sum_ = self . _ip_dec + other . _ip_dec elif isinstance ( other , int ) : sum_ = self . _ip_dec + other else : other = self . __class__ ( other ) sum_ = self . _ip_dec + other . _ip_dec return sum_
13039	def overview ( ) : search = Credential . search ( ) search . aggs . bucket ( 'password_count' , 'terms' , field = 'secret' , order = { '_count' : 'desc' } , size = 20 ) . metric ( 'username_count' , 'cardinality' , field = 'username' ) . metric ( 'host_count' , 'cardinality' , field = 'host_ip' ) . metric ( 'top_hits' , 'top_hits' , docvalue_fields = [ 'username' ] , size = 100 ) response = search . execute ( ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( "Secret" , "Count" , "Hosts" , "Users" , "Usernames" ) ) print_line ( "-" * 100 ) for entry in response . aggregations . password_count . buckets : usernames = [ ] for creds in entry . top_hits : usernames . append ( creds . username [ 0 ] ) usernames = list ( set ( usernames ) ) print_line ( "{0:65} {1:5} {2:5} {3:5} {4}" . format ( entry . key , entry . doc_count , entry . host_count . value , entry . username_count . value , usernames ) )
5040	def get_users_by_email ( cls , emails ) : users = User . objects . filter ( email__in = emails ) present_emails = users . values_list ( 'email' , flat = True ) missing_emails = list ( set ( emails ) - set ( present_emails ) ) return users , missing_emails
7825	def feature_uri ( uri ) : def decorator ( class_ ) : if "_pyxmpp_feature_uris" not in class_ . __dict__ : class_ . _pyxmpp_feature_uris = set ( ) class_ . _pyxmpp_feature_uris . add ( uri ) return class_ return decorator
11199	def _validate_fromutc_inputs ( f ) : @ wraps ( f ) def fromutc ( self , dt ) : if not isinstance ( dt , datetime ) : raise TypeError ( "fromutc() requires a datetime argument" ) if dt . tzinfo is not self : raise ValueError ( "dt.tzinfo is not self" ) return f ( self , dt ) return fromutc
12430	def create_nginx_config ( self ) : cfg = '# nginx config for {0}\n' . format ( self . _project_name ) if not self . _shared_hosting : if self . _user : cfg += 'user {0};\n' . format ( self . _user ) cfg += 'worker_processes 1;\nerror_log {0}-errors.log;\n\pid {1}_ nginx.pid;\n\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) , os . path . join ( self . _var_dir , self . _project_name ) ) cfg += 'events {\n\tworker_connections 32;\n}\n\n' cfg += 'http {\n' if self . _include_mimetypes : cfg += '\tinclude mime.types;\n' cfg += '\tdefault_type application/octet-stream;\n' cfg += '\tclient_max_body_size 1G;\n' cfg += '\tproxy_max_temp_file_size 0;\n' cfg += '\tproxy_buffering off;\n' cfg += '\taccess_log {0}-access.log;\n' . format ( os . path . join ( self . _log_dir , self . _project_name ) ) cfg += '\tsendfile on;\n' cfg += '\tkeepalive_timeout 65;\n' cfg += '\tserver {\n' cfg += '\t\tlisten 0.0.0.0:{0};\n' . format ( self . _port ) if self . _server_name : cfg += '\t\tserver_name {0};\n' . format ( self . _server_name ) cfg += '\t\tlocation / {\n' cfg += '\t\t\tuwsgi_pass unix:///{0}.sock;\n' . format ( os . path . join ( self . _var_dir , self . _project_name ) ) cfg += '\t\t\tinclude uwsgi_params;\n' cfg += '\t\t}\n\n' cfg += '\t\terror_page 500 502 503 504 /50x.html;\n' cfg += '\t\tlocation = /50x.html {\n' cfg += '\t\t\troot html;\n' cfg += '\t\t}\n' cfg += '\t}\n' if not self . _shared_hosting : cfg += '}\n' f = open ( self . _nginx_config , 'w' ) f . write ( cfg ) f . close ( )
3462	def double_reaction_deletion ( model , reaction_list1 = None , reaction_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : reaction_list1 , reaction_list2 = _element_lists ( model . reactions , reaction_list1 , reaction_list2 ) return _multi_deletion ( model , 'reaction' , element_lists = [ reaction_list1 , reaction_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
2313	def predict_proba ( self , a , b , ** kwargs ) : a = scale ( a ) . reshape ( ( - 1 , 1 ) ) b = scale ( b ) . reshape ( ( - 1 , 1 ) ) return self . anm_score ( b , a ) - self . anm_score ( a , b )
1456	def valid_path ( path ) : if path . endswith ( '*' ) : Log . debug ( 'Checking classpath entry suffix as directory: %s' , path [ : - 1 ] ) if os . path . isdir ( path [ : - 1 ] ) : return True return False Log . debug ( 'Checking classpath entry as directory: %s' , path ) if os . path . isdir ( path ) : return True else : Log . debug ( 'Checking classpath entry as file: %s' , path ) if os . path . isfile ( path ) : return True return False
10236	def get_graphs_by_ids ( self , network_ids : Iterable [ int ] ) -> List [ BELGraph ] : return [ self . networks [ network_id ] for network_id in network_ids ]
7511	def select_samples ( dbsamples , samples , pidx = None ) : samples = [ i . name for i in samples ] if pidx : sidx = [ list ( dbsamples [ pidx ] ) . index ( i ) for i in samples ] else : sidx = [ list ( dbsamples ) . index ( i ) for i in samples ] sidx . sort ( ) return sidx
13872	def StandardizePath ( path , strip = False ) : path = path . replace ( SEPARATOR_WINDOWS , SEPARATOR_UNIX ) if strip : path = path . rstrip ( SEPARATOR_UNIX ) return path
449	def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )
9358	def paragraph ( separator = '\n\n' , wrap_start = '' , wrap_end = '' , html = False , sentences_quantity = 3 ) : return paragraphs ( quantity = 1 , separator = separator , wrap_start = wrap_start , wrap_end = wrap_end , html = html , sentences_quantity = sentences_quantity )
7353	def NetMHC ( alleles , default_peptide_lengths = [ 9 ] , program_name = "netMHC" ) : with open ( os . devnull , 'w' ) as devnull : help_output = check_output ( [ program_name , "-h" ] , stderr = devnull ) help_output_str = help_output . decode ( "ascii" , "ignore" ) substring_to_netmhc_class = { "-listMHC" : NetMHC4 , "--Alleles" : NetMHC3 , } successes = [ ] for substring , netmhc_class in substring_to_netmhc_class . items ( ) : if substring in help_output_str : successes . append ( netmhc_class ) if len ( successes ) > 1 : raise SystemError ( "Command %s is valid for multiple NetMHC versions. " "This is likely an mhctools bug." % program_name ) if len ( successes ) == 0 : raise SystemError ( "Command %s is not a valid way of calling any NetMHC software." % program_name ) netmhc_class = successes [ 0 ] return netmhc_class ( alleles = alleles , default_peptide_lengths = default_peptide_lengths , program_name = program_name )
2183	def existing_versions ( self ) : import glob pattern = join ( self . dpath , self . fname + '_*' + self . ext ) for fname in glob . iglob ( pattern ) : data_fpath = join ( self . dpath , fname ) yield data_fpath
1434	def custom_serialized ( cls , serialized , is_java = True ) : if not isinstance ( serialized , bytes ) : raise TypeError ( "Argument to custom_serialized() must be " "a serialized Python class as bytes, given: %s" % str ( serialized ) ) if not is_java : return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , python_serialized = serialized ) else : raise NotImplementedError ( "Custom grouping implemented in Java for Python topology" "is not yet supported." )
3430	def add_reactions ( self , reaction_list ) : def existing_filter ( rxn ) : if rxn . id in self . reactions : LOGGER . warning ( "Ignoring reaction '%s' since it already exists." , rxn . id ) return False return True pruned = DictList ( filter ( existing_filter , reaction_list ) ) context = get_context ( self ) for reaction in pruned : reaction . _model = self for metabolite in list ( reaction . metabolites ) : if metabolite not in self . metabolites : self . add_metabolites ( metabolite ) else : stoichiometry = reaction . _metabolites . pop ( metabolite ) model_metabolite = self . metabolites . get_by_id ( metabolite . id ) reaction . _metabolites [ model_metabolite ] = stoichiometry model_metabolite . _reaction . add ( reaction ) if context : context ( partial ( model_metabolite . _reaction . remove , reaction ) ) for gene in list ( reaction . _genes ) : if not self . genes . has_id ( gene . id ) : self . genes += [ gene ] gene . _model = self if context : context ( partial ( self . genes . __isub__ , [ gene ] ) ) context ( partial ( setattr , gene , '_model' , None ) ) else : model_gene = self . genes . get_by_id ( gene . id ) if model_gene is not gene : reaction . _dissociate_gene ( gene ) reaction . _associate_gene ( model_gene ) self . reactions += pruned if context : context ( partial ( self . reactions . __isub__ , pruned ) ) self . _populate_solver ( pruned )
9662	def get_ties ( G ) : ties = [ ] dep_dict = { } for node in G . nodes ( data = True ) : if 'dependencies' in node [ 1 ] : for item in node [ 1 ] [ 'dependencies' ] : if item not in dep_dict : dep_dict [ item ] = [ ] dep_dict [ item ] . append ( node [ 0 ] ) for item in dep_dict : if len ( list ( set ( dep_dict [ item ] ) ) ) > 1 : ties . append ( list ( set ( dep_dict [ item ] ) ) ) return ties
2726	def create ( self , * args , ** kwargs ) : for attr in kwargs . keys ( ) : setattr ( self , attr , kwargs [ attr ] ) if not self . size_slug and self . size : self . size_slug = self . size ssh_keys_id = Droplet . __get_ssh_keys_id_or_fingerprint ( self . ssh_keys , self . token , self . name ) data = { "name" : self . name , "size" : self . size_slug , "image" : self . image , "region" : self . region , "ssh_keys" : ssh_keys_id , "backups" : bool ( self . backups ) , "ipv6" : bool ( self . ipv6 ) , "private_networking" : bool ( self . private_networking ) , "volumes" : self . volumes , "tags" : self . tags , "monitoring" : bool ( self . monitoring ) , } if self . user_data : data [ "user_data" ] = self . user_data data = self . get_data ( "droplets/" , type = POST , params = data ) if data : self . id = data [ 'droplet' ] [ 'id' ] action_id = data [ 'links' ] [ 'actions' ] [ 0 ] [ 'id' ] self . action_ids = [ ] self . action_ids . append ( action_id )
12962	def exists ( self , pk ) : conn = self . _get_connection ( ) key = self . _get_key_for_id ( pk ) return conn . exists ( key )
11675	def bare ( self ) : "Make a Features object with no metadata; points to the same features." if not self . meta : return self elif self . stacked : return Features ( self . stacked_features , self . n_pts , copy = False ) else : return Features ( self . features , copy = False )
8972	def connect_to ( self , other_mesh ) : other_mesh . disconnect ( ) self . disconnect ( ) self . _connect_to ( other_mesh )
2607	def update_memo ( self , task_id , task , r ) : if not self . memoize or not task [ 'memoize' ] : return if task [ 'hashsum' ] in self . memo_lookup_table : logger . info ( 'Updating appCache entry with latest %s:%s call' % ( task [ 'func_name' ] , task_id ) ) self . memo_lookup_table [ task [ 'hashsum' ] ] = r else : self . memo_lookup_table [ task [ 'hashsum' ] ] = r
3853	def get_conv_name ( conv , truncate = False , show_unread = False ) : num_unread = len ( [ conv_event for conv_event in conv . unread_events if isinstance ( conv_event , hangups . ChatMessageEvent ) and not conv . get_user ( conv_event . user_id ) . is_self ] ) if show_unread and num_unread > 0 : postfix = ' ({})' . format ( num_unread ) else : postfix = '' if conv . name is not None : return conv . name + postfix else : participants = sorted ( ( user for user in conv . users if not user . is_self ) , key = lambda user : user . id_ ) names = [ user . first_name for user in participants ] if not participants : return "Empty Conversation" + postfix if len ( participants ) == 1 : return participants [ 0 ] . full_name + postfix elif truncate and len ( participants ) > 2 : return ( ', ' . join ( names [ : 2 ] + [ '+{}' . format ( len ( names ) - 2 ) ] ) + postfix ) else : return ', ' . join ( names ) + postfix
13396	def settings_and_attributes ( self ) : attrs = self . setting_values ( ) attrs . update ( self . __dict__ ) skip = [ "_instance_settings" , "aliases" ] for a in skip : del attrs [ a ] return attrs
8984	def to_svg ( self , instruction_or_id , i_promise_not_to_change_the_result = False ) : return self . _new_svg_dumper ( lambda : self . instruction_to_svg_dict ( instruction_or_id , not i_promise_not_to_change_the_result ) )
7617	def coerce_annotation ( ann , namespace ) : ann = convert ( ann , namespace ) ann . validate ( strict = True ) return ann
6870	def get_snr_of_dip ( times , mags , modeltimes , modelmags , atol_normalization = 1e-8 , indsforrms = None , magsarefluxes = False , verbose = True , transitdepth = None , npoints_in_transit = None ) : if magsarefluxes : if not np . isclose ( np . nanmedian ( modelmags ) , 1 , atol = atol_normalization ) : raise AssertionError ( 'snr calculation assumes modelmags are ' 'median-normalized' ) else : raise NotImplementedError ( 'need to implement a method for identifying in-transit points when' 'mags are mags, and not fluxes' ) if not transitdepth : transitdepth = np . abs ( np . max ( modelmags ) - np . min ( modelmags ) ) if not len ( mags ) == len ( modelmags ) : from scipy . interpolate import interp1d fn = interp1d ( modeltimes , modelmags , kind = 'cubic' , bounds_error = True , fill_value = np . nan ) modelmags = fn ( times ) if verbose : LOGINFO ( 'interpolated model timeseries onto the data timeseries' ) subtractedmags = mags - modelmags if isinstance ( indsforrms , np . ndarray ) : subtractedrms = np . std ( subtractedmags [ indsforrms ] ) if verbose : LOGINFO ( 'using selected points to measure RMS' ) else : subtractedrms = np . std ( subtractedmags ) if verbose : LOGINFO ( 'using all points to measure RMS' ) def _get_npoints_in_transit ( modelmags ) : if np . nanmedian ( modelmags ) == 1 : return len ( modelmags [ ( modelmags != 1 ) ] ) else : raise NotImplementedError if not npoints_in_transit : npoints_in_transit = _get_npoints_in_transit ( modelmags ) snr = np . sqrt ( npoints_in_transit ) * transitdepth / subtractedrms if verbose : LOGINFO ( '\npoints in transit: {:d}' . format ( npoints_in_transit ) + '\ndepth: {:.2e}' . format ( transitdepth ) + '\nrms in residual: {:.2e}' . format ( subtractedrms ) + '\n\t SNR: {:.2e}' . format ( snr ) ) return snr , transitdepth , subtractedrms
5737	def _get_or_create_subscription ( self ) : topic_path = self . _get_topic_path ( ) subscription_name = '{}-{}-shared' . format ( PUBSUB_OBJECT_PREFIX , self . name ) subscription_path = self . subscriber_client . subscription_path ( self . project , subscription_name ) try : self . subscriber_client . get_subscription ( subscription_path ) except google . cloud . exceptions . NotFound : logger . info ( "Creating shared subscription {}" . format ( subscription_name ) ) try : self . subscriber_client . create_subscription ( subscription_path , topic = topic_path ) except google . cloud . exceptions . Conflict : pass return subscription_path
4815	def create_feature_array ( text , n_pad = 21 ) : n = len ( text ) n_pad_2 = int ( ( n_pad - 1 ) / 2 ) text_pad = [ ' ' ] * n_pad_2 + [ t for t in text ] + [ ' ' ] * n_pad_2 x_char , x_type = [ ] , [ ] for i in range ( n_pad_2 , n_pad_2 + n ) : char_list = text_pad [ i + 1 : i + n_pad_2 + 1 ] + list ( reversed ( text_pad [ i - n_pad_2 : i ] ) ) + [ text_pad [ i ] ] char_map = [ CHARS_MAP . get ( c , 80 ) for c in char_list ] char_type = [ CHAR_TYPES_MAP . get ( CHAR_TYPE_FLATTEN . get ( c , 'o' ) , 4 ) for c in char_list ] x_char . append ( char_map ) x_type . append ( char_type ) x_char = np . array ( x_char ) . astype ( float ) x_type = np . array ( x_type ) . astype ( float ) return x_char , x_type
8168	def run ( self ) : with LiveExecution . lock : if self . edited_source : success , ex = self . run_tenuous ( ) if success : return self . do_exec ( self . known_good , self . ns )
10606	def prepare_to_run ( self ) : self . clock . reset ( ) for e in self . entities : e . prepare_to_run ( self . clock , self . period_count )
4239	def config_finish ( self ) : _LOGGER . info ( "Config finish" ) if not self . config_started : return True success , _ = self . _make_request ( SERVICE_DEVICE_CONFIG , "ConfigurationFinished" , { "NewStatus" : "ChangesApplied" } ) self . config_started = not success return success
6941	def _double_inverted_gaussian ( x , amp1 , loc1 , std1 , amp2 , loc2 , std2 ) : gaussian1 = - _gaussian ( x , amp1 , loc1 , std1 ) gaussian2 = - _gaussian ( x , amp2 , loc2 , std2 ) return gaussian1 + gaussian2
2353	def wait_for_region_to_load ( self ) : self . wait . until ( lambda _ : self . loaded ) self . pm . hook . pypom_after_wait_for_region_to_load ( region = self ) return self
9549	def validate ( self , data , expect_header_row = True , ignore_lines = 0 , summarize = False , limit = 0 , context = None , report_unexpected_exceptions = True ) : problems = list ( ) problem_generator = self . ivalidate ( data , expect_header_row , ignore_lines , summarize , context , report_unexpected_exceptions ) for i , p in enumerate ( problem_generator ) : if not limit or i < limit : problems . append ( p ) return problems
747	def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )
2589	def shutdown ( self , block = False ) : x = self . executor . shutdown ( wait = block ) logger . debug ( "Done with executor shutdown" ) return x
5213	def intraday ( ticker , dt , session = '' , ** kwargs ) -> pd . DataFrame : from xbbg . core import intervals cur_data = bdib ( ticker = ticker , dt = dt , typ = kwargs . get ( 'typ' , 'TRADE' ) ) if cur_data . empty : return pd . DataFrame ( ) fmt = '%H:%M:%S' ss = intervals . SessNA ref = kwargs . get ( 'ref' , None ) exch = pd . Series ( ) if ref is None else const . exch_info ( ticker = ref ) if session : ss = intervals . get_interval ( ticker = kwargs . get ( 'ref' , ticker ) , session = session ) start_time = kwargs . get ( 'start_time' , None ) end_time = kwargs . get ( 'end_time' , None ) if ss != intervals . SessNA : start_time = pd . Timestamp ( ss . start_time ) . strftime ( fmt ) end_time = pd . Timestamp ( ss . end_time ) . strftime ( fmt ) if start_time and end_time : kw = dict ( start_time = start_time , end_time = end_time ) if not exch . empty : cur_tz = cur_data . index . tz res = cur_data . tz_convert ( exch . tz ) . between_time ( ** kw ) if kwargs . get ( 'keep_tz' , False ) : res = res . tz_convert ( cur_tz ) return pd . DataFrame ( res ) return pd . DataFrame ( cur_data . between_time ( ** kw ) ) return cur_data
5752	def strToBool ( val ) : if isinstance ( val , str ) : val = val . lower ( ) return val in [ 'true' , 'on' , 'yes' , True ]
9217	def input ( self , file ) : if isinstance ( file , string_types ) : with open ( file ) as f : self . lexer . input ( f . read ( ) ) else : self . lexer . input ( file . read ( ) )
7481	def cleanup_tempfiles ( data ) : tmps1 = glob . glob ( os . path . join ( data . tmpdir , "*.fa" ) ) tmps2 = glob . glob ( os . path . join ( data . tmpdir , "*.npy" ) ) for tmp in tmps1 + tmps2 : if os . path . exists ( tmp ) : os . remove ( tmp ) removal = [ os . path . join ( data . dirs . across , data . name + ".utemp" ) , os . path . join ( data . dirs . across , data . name + ".htemp" ) , os . path . join ( data . dirs . across , data . name + "_catcons.tmp" ) , os . path . join ( data . dirs . across , data . name + "_cathaps.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catshuf.tmp" ) , os . path . join ( data . dirs . across , data . name + "_catsort.tmp" ) , os . path . join ( data . dirs . across , data . name + ".tmparrs.h5" ) , os . path . join ( data . dirs . across , data . name + ".tmp.indels.hdf5" ) , ] for rfile in removal : if os . path . exists ( rfile ) : os . remove ( rfile ) smpios = glob . glob ( os . path . join ( data . dirs . across , '*.tmp.h5' ) ) for smpio in smpios : if os . path . exists ( smpio ) : os . remove ( smpio )
12317	def delete ( self , repo , args = [ ] ) : result = None with cd ( repo . rootdir ) : try : cmd = [ 'rm' ] + list ( args ) result = { 'status' : 'success' , 'message' : self . _run ( cmd ) } except Exception as e : result = { 'status' : 'error' , 'message' : str ( e ) } return result
10827	def create ( cls , group , user , state = MembershipState . ACTIVE ) : with db . session . begin_nested ( ) : membership = cls ( user_id = user . get_id ( ) , id_group = group . id , state = state , ) db . session . add ( membership ) return membership
7138	def get_type_info ( obj ) : if isinstance ( obj , primitive_types ) : return ( 'primitive' , type ( obj ) . __name__ ) if isinstance ( obj , sequence_types ) : return ( 'sequence' , type ( obj ) . __name__ ) if isinstance ( obj , array_types ) : return ( 'array' , type ( obj ) . __name__ ) if isinstance ( obj , key_value_types ) : return ( 'key-value' , type ( obj ) . __name__ ) if isinstance ( obj , types . ModuleType ) : return ( 'module' , type ( obj ) . __name__ ) if isinstance ( obj , ( types . FunctionType , types . MethodType ) ) : return ( 'function' , type ( obj ) . __name__ ) if isinstance ( obj , type ) : if hasattr ( obj , '__dict__' ) : return ( 'class' , obj . __name__ ) if isinstance ( type ( obj ) , type ) : if hasattr ( obj , '__dict__' ) : cls_name = type ( obj ) . __name__ if cls_name == 'classobj' : cls_name = obj . __name__ return ( 'class' , '{}' . format ( cls_name ) ) if cls_name == 'instance' : cls_name = obj . __class__ . __name__ return ( 'instance' , '{} instance' . format ( cls_name ) ) return ( 'unknown' , type ( obj ) . __name__ )
5289	def forms_invalid ( self , form , inlines ) : return self . render_to_response ( self . get_context_data ( form = form , inlines = inlines ) )
11663	def as_integer_type ( ary ) : ary = np . asanyarray ( ary ) if is_integer_type ( ary ) : return ary rounded = np . rint ( ary ) if np . any ( rounded != ary ) : raise ValueError ( "argument array must contain only integers" ) return rounded . astype ( int )
12160	def parent ( groups , ID ) : if ID in groups . keys ( ) : return ID if not ID in groups . keys ( ) : for actualParent in groups . keys ( ) : if ID in groups [ actualParent ] : return actualParent return None
4888	def update_enterprise_courses ( self , enterprise_customer , course_container_key = 'results' , ** kwargs ) : enterprise_context = { 'tpa_hint' : enterprise_customer and enterprise_customer . identity_provider , 'enterprise_id' : enterprise_customer and str ( enterprise_customer . uuid ) , } enterprise_context . update ( ** kwargs ) courses = [ ] for course in self . data [ course_container_key ] : courses . append ( self . update_course ( course , enterprise_customer , enterprise_context ) ) self . data [ course_container_key ] = courses
11643	def transform ( self , X ) : X = check_array ( X ) X_rbf = np . empty_like ( X ) if self . copy else X X_in = X if not self . squared : np . power ( X_in , 2 , out = X_rbf ) X_in = X_rbf if self . scale_by_median : scale = self . median_ if self . squared else self . median_ ** 2 gamma = self . gamma * scale else : gamma = self . gamma np . multiply ( X_in , - gamma , out = X_rbf ) np . exp ( X_rbf , out = X_rbf ) return X_rbf
5457	def _from_yaml_v0 ( cls , job ) : job_metadata = { } for key in [ 'job-id' , 'job-name' , 'create-time' ] : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( datetime . datetime . strptime ( job [ 'create-time' ] , '%Y-%m-%d %H:%M:%S.%f' ) , tzlocal ( ) ) job_resources = Resources ( ) params = { } labels = job . get ( 'labels' , { } ) if 'dsub-version' in labels : job_metadata [ 'dsub-version' ] = labels [ 'dsub-version' ] del labels [ 'dsub-version' ] params [ 'labels' ] = cls . _label_params_from_dict ( labels ) params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) if job . get ( 'task-id' ) is None : job_params = params task_metadata = { 'task-id' : None } task_params = { } else : job_params = { } task_metadata = { 'task-id' : str ( job . get ( 'task-id' ) ) } task_params = params task_resources = Resources ( logging_path = job . get ( 'logging' ) ) task_descriptors = [ TaskDescriptor . get_complete_descriptor ( task_metadata , task_params , task_resources ) ] return JobDescriptor . get_complete_descriptor ( job_metadata , job_params , job_resources , task_descriptors )
5363	def stdout ( self ) : if self . _streaming : stdout = [ ] while not self . __stdout . empty ( ) : try : line = self . __stdout . get_nowait ( ) stdout . append ( line ) except : pass else : stdout = self . __stdout return stdout
11134	def is_not_exist_or_allow_overwrite ( self , overwrite = False ) : if self . exists ( ) and overwrite is False : return False else : return True
7660	def validate ( self , strict = True ) : ann_schema = schema . namespace_array ( self . namespace ) valid = True try : jsonschema . validate ( self . __json_light__ ( data = False ) , schema . JAMS_SCHEMA ) data_ser = [ serialize_obj ( obs ) for obs in self . data ] jsonschema . validate ( data_ser , ann_schema ) except jsonschema . ValidationError as invalid : if strict : raise SchemaError ( str ( invalid ) ) else : warnings . warn ( str ( invalid ) ) valid = False return valid
2195	def write ( self , msg ) : if self . redirect is not None : self . redirect . write ( msg ) if six . PY2 : from xdoctest . utils . util_str import ensure_unicode msg = ensure_unicode ( msg ) super ( TeeStringIO , self ) . write ( msg )
6610	def getArrays ( self , tree , branchName ) : itsArray = self . _getArray ( tree , branchName ) if itsArray is None : return None , None itsCountArray = self . _getCounterArray ( tree , branchName ) return itsArray , itsCountArray
8440	def setup ( template , version = None ) : temple . check . is_git_ssh_path ( template ) temple . check . not_in_git_repo ( ) repo_path = temple . utils . get_repo_path ( template ) msg = ( 'You will be prompted for the parameters of your new project.' ' Please read the docs at https://github.com/{} before entering parameters.' ) . format ( repo_path ) print ( msg ) cc_repo_dir , config = temple . utils . get_cookiecutter_config ( template , version = version ) if not version : with temple . utils . cd ( cc_repo_dir ) : ret = temple . utils . shell ( 'git rev-parse HEAD' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) _generate_files ( repo_dir = cc_repo_dir , config = config , template = template , version = version )
12449	def _add_method ( self , effect , verb , resource , conditions ) : if verb != '*' and not hasattr ( HttpVerb , verb ) : raise NameError ( 'Invalid HTTP verb ' + verb + '. Allowed verbs in HttpVerb class' ) resource_pattern = re . compile ( self . path_regex ) if not resource_pattern . match ( resource ) : raise NameError ( 'Invalid resource path: ' + resource + '. Path should match ' + self . path_regex ) if resource [ : 1 ] == '/' : resource = resource [ 1 : ] resource_arn = ( 'arn:aws:execute-api:' + self . region + ':' + self . aws_account_id + ':' + self . rest_api_id + '/' + self . stage + '/' + verb + '/' + resource ) if effect . lower ( ) == 'allow' : self . allowMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } ) elif effect . lower ( ) == 'deny' : self . denyMethods . append ( { 'resource_arn' : resource_arn , 'conditions' : conditions } )
7056	def ec2_ssh ( ip_address , keypem_file , username = 'ec2-user' , raiseonfail = False ) : c = paramiko . client . SSHClient ( ) c . load_system_host_keys ( ) c . set_missing_host_key_policy ( paramiko . client . AutoAddPolicy ) privatekey = paramiko . RSAKey . from_private_key_file ( keypem_file ) try : c . connect ( ip_address , pkey = privatekey , username = 'ec2-user' ) return c except Exception as e : LOGEXCEPTION ( 'could not connect to EC2 instance at %s ' 'using keyfile: %s and user: %s' % ( ip_address , keypem_file , username ) ) if raiseonfail : raise return None
10008	def get_dynamic_base ( self , bases : tuple ) : try : return self . _dynamic_bases_inverse [ bases ] except KeyError : name = self . _dynamic_base_namer . get_next ( self . _dynamic_bases ) base = self . _new_space ( name = name ) self . spacegraph . add_space ( base ) self . _dynamic_bases [ name ] = base self . _dynamic_bases_inverse [ bases ] = base base . add_bases ( bases ) return base
1085	def timetz ( self ) : "Return the time part, with same tzinfo." return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )
10683	def S ( self , T ) : result = self . Sref for Tmax in sorted ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) : result += self . _Cp_records [ str ( Tmax ) ] . S ( T ) if T <= Tmax : return result + self . S_mag ( T ) Tmax = max ( [ float ( TT ) for TT in self . _Cp_records . keys ( ) ] ) result += self . Cp ( Tmax ) * math . log ( T / Tmax ) return result + self . S_mag ( T )
12595	def get_aad_token ( endpoint , no_verify ) : from azure . servicefabric . service_fabric_client_ap_is import ( ServiceFabricClientAPIs ) from sfctl . auth import ClientCertAuthentication from sfctl . config import set_aad_metadata auth = ClientCertAuthentication ( None , None , no_verify ) client = ServiceFabricClientAPIs ( auth , base_url = endpoint ) aad_metadata = client . get_aad_metadata ( ) if aad_metadata . type != "aad" : raise CLIError ( "Not AAD cluster" ) aad_resource = aad_metadata . metadata tenant_id = aad_resource . tenant authority_uri = aad_resource . login + '/' + tenant_id context = adal . AuthenticationContext ( authority_uri , api_version = None ) cluster_id = aad_resource . cluster client_id = aad_resource . client set_aad_metadata ( authority_uri , cluster_id , client_id ) code = context . acquire_user_code ( cluster_id , client_id ) print ( code [ 'message' ] ) token = context . acquire_token_with_device_code ( cluster_id , code , client_id ) print ( "Succeed!" ) return token , context . cache
8024	def __button_action ( self , data = None ) : if any ( not x for x in ( self . _ename . value , self . _p1 . value , self . _p2 . value , self . _file . value ) ) : print ( "Missing one of the required fields (event name, player names, file name)" ) return self . __p1chars = [ ] self . __p2chars = [ ] options = Namespace ( ) self . __history . append ( self . __save_form ( ) ) options . ename = self . _ename . value if self . _ename_min . value : options . ename_min = self . _ename_min . value else : options . ename_min = options . ename options . pID = self . _pID . value options . mtype = self . _mtype . value options . mmid = options . mtype options . p1 = self . _p1 . value options . p2 = self . _p2 . value options . p1char = self . _p1char . value options . p2char = self . _p2char . value options . bracket = self . _bracket . value isadir = os . path . isdir ( self . _file . value ) if isadir : options . file = max ( [ os . path . join ( self . _file . value , f ) for f in os . listdir ( self . _file . value ) if os . path . isfile ( os . path . join ( self . _file . value , f ) ) ] , key = os . path . getmtime ) else : options . file = self . _file . value options . tags = self . _tags . value options . msuffix = self . _msuffix . value options . mprefix = self . _mprefix . value options . privacy = self . _privacy . value options . descrip = self . _description . value options . titleformat = self . _titleformat . value if self . _p1sponsor . value : options . p1 = " | " . join ( ( self . _p1sponsor . value , options . p1 ) ) if self . _p2sponsor . value : options . p2 = " | " . join ( ( self . _p2sponsor . value , options . p2 ) ) options . ignore = False self . __reset_match ( False , isadir ) self . __add_to_qview ( options ) self . _queueref . append ( options ) if consts . firstrun : thr = threading . Thread ( target = self . __worker ) thr . daemon = True thr . start ( ) consts . firstrun = False
5251	def _init_services ( self ) : logger = _get_logger ( self . debug ) opened = self . _session . openService ( '//blp/refdata' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/refdata' ) raise ConnectionError ( 'Could not open a //blp/refdata service' ) self . refDataService = self . _session . getService ( '//blp/refdata' ) opened = self . _session . openService ( '//blp/exrsvc' ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SERVICE_STATUS : raise RuntimeError ( 'Expected a "SERVICE_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) if not opened : logger . warning ( 'Failed to open //blp/exrsvc' ) raise ConnectionError ( 'Could not open a //blp/exrsvc service' ) self . exrService = self . _session . getService ( '//blp/exrsvc' ) return self
13858	def curl ( self , url , post ) : try : req = urllib2 . Request ( url ) req . add_header ( "Content-type" , "application/xml" ) data = urllib2 . urlopen ( req , post . encode ( 'utf-8' ) ) . read ( ) except urllib2 . URLError , v : raise AmbientSMSError ( v ) return dictFromXml ( data )
10184	def _aggregations_list_bookmarks ( aggregation_types = None , start_date = None , end_date = None , limit = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) bookmarks = aggregator . list_bookmarks ( start_date , end_date , limit ) click . echo ( '{}:' . format ( a ) ) for b in bookmarks : click . echo ( ' - {}' . format ( b . date ) )
4401	def fetch ( self ) : xml = urllib . request . urlopen ( self . URL ) tree = ET . ElementTree ( file = xml ) records = self . _parse_deputies ( tree . getroot ( ) ) df = pd . DataFrame ( records , columns = ( 'congressperson_id' , 'budget_id' , 'condition' , 'congressperson_document' , 'civil_name' , 'congressperson_name' , 'picture_url' , 'gender' , 'state' , 'party' , 'phone_number' , 'email' ) ) return self . _translate ( df )
13441	def cmd_init_push_to_cloud ( args ) : ( lcat , ccat ) = ( args . local_catalog , args . cloud_catalog ) logging . info ( "[init-push-to-cloud]: %s => %s" % ( lcat , ccat ) ) if not isfile ( lcat ) : args . error ( "[init-push-to-cloud] The local catalog does not exist: %s" % lcat ) if isfile ( ccat ) : args . error ( "[init-push-to-cloud] The cloud catalog already exist: %s" % ccat ) ( lmeta , cmeta ) = ( "%s.lrcloud" % lcat , "%s.lrcloud" % ccat ) if isfile ( lmeta ) : args . error ( "[init-push-to-cloud] The local meta-data already exist: %s" % lmeta ) if isfile ( cmeta ) : args . error ( "[init-push-to-cloud] The cloud meta-data already exist: %s" % cmeta ) logging . info ( "Locking local catalog: %s" % ( lcat ) ) if not lock_file ( lcat ) : raise RuntimeError ( "The catalog %s is locked!" % lcat ) util . copy ( lcat , ccat ) mfile = MetaFile ( lmeta ) utcnow = datetime . utcnow ( ) . strftime ( DATETIME_FORMAT ) [ : - 4 ] mfile [ 'catalog' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'catalog' ] [ 'modification_utc' ] = utcnow mfile [ 'catalog' ] [ 'filename' ] = lcat mfile [ 'last_push' ] [ 'filename' ] = ccat mfile [ 'last_push' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'last_push' ] [ 'modification_utc' ] = utcnow mfile . flush ( ) mfile = MetaFile ( cmeta ) mfile [ 'changeset' ] [ 'is_base' ] = True mfile [ 'changeset' ] [ 'hash' ] = hashsum ( lcat ) mfile [ 'changeset' ] [ 'modification_utc' ] = utcnow mfile [ 'changeset' ] [ 'filename' ] = basename ( ccat ) mfile . flush ( ) if not args . no_smart_previews : copy_smart_previews ( lcat , ccat , local2cloud = True ) logging . info ( "Unlocking local catalog: %s" % ( lcat ) ) unlock_file ( lcat ) logging . info ( "[init-push-to-cloud]: Success!" )
4974	def verify_edx_resources ( ) : required_methods = { 'ProgramDataExtender' : ProgramDataExtender , } for method in required_methods : if required_methods [ method ] is None : raise NotConnectedToOpenEdX ( _ ( "The following method from the Open edX platform is necessary for this view but isn't available." ) + "\nUnavailable: {method}" . format ( method = method ) )
3531	def is_internal_ip ( context , prefix = None ) : try : request = context [ 'request' ] remote_ip = request . META . get ( 'HTTP_X_FORWARDED_FOR' , '' ) if not remote_ip : remote_ip = request . META . get ( 'REMOTE_ADDR' , '' ) if not remote_ip : return False internal_ips = None if prefix is not None : internal_ips = getattr ( settings , '%s_INTERNAL_IPS' % prefix , None ) if internal_ips is None : internal_ips = getattr ( settings , 'ANALYTICAL_INTERNAL_IPS' , None ) if internal_ips is None : internal_ips = getattr ( settings , 'INTERNAL_IPS' , None ) return remote_ip in ( internal_ips or [ ] ) except ( KeyError , AttributeError ) : return False
6604	def result_relpath ( self , package_index ) : dirname = 'task_{:05d}' . format ( package_index ) ret = os . path . join ( 'results' , dirname , 'result.p.gz' ) return ret
3840	async def set_typing ( self , set_typing_request ) : response = hangouts_pb2 . SetTypingResponse ( ) await self . _pb_request ( 'conversations/settyping' , set_typing_request , response ) return response
10078	def _process_files ( self , record_id , data ) : if self . files : assert not self . files . bucket . locked self . files . bucket . locked = True snapshot = self . files . bucket . snapshot ( lock = True ) data [ '_files' ] = self . files . dumps ( bucket = snapshot . id ) yield data db . session . add ( RecordsBuckets ( record_id = record_id , bucket_id = snapshot . id ) ) else : yield data
1751	def scan_mem ( self , data_to_find ) : if isinstance ( data_to_find , bytes ) : data_to_find = [ bytes ( [ c ] ) for c in data_to_find ] for mapping in sorted ( self . maps ) : for ptr in mapping : if ptr + len ( data_to_find ) >= mapping . end : break candidate = mapping [ ptr : ptr + len ( data_to_find ) ] if issymbolic ( candidate [ 0 ] ) : break if candidate == data_to_find : yield ptr
6236	def set_time ( self , value : float ) : if value < 0 : value = 0 mixer . music . set_pos ( value )
11871	def color_from_hex ( value ) : if "#" in value : value = value [ 1 : ] try : unhexed = bytes . fromhex ( value ) except : unhexed = binascii . unhexlify ( value ) return color_from_rgb ( * struct . unpack ( 'BBB' , unhexed ) )
4121	def twosided_2_centerdc ( data ) : N = len ( data ) newpsd = np . concatenate ( ( cshift ( data [ N // 2 : ] , 1 ) , data [ 0 : N // 2 ] ) ) newpsd [ 0 ] = data [ - 1 ] return newpsd
3463	def double_gene_deletion ( model , gene_list1 = None , gene_list2 = None , method = "fba" , solution = None , processes = None , ** kwargs ) : gene_list1 , gene_list2 = _element_lists ( model . genes , gene_list1 , gene_list2 ) return _multi_deletion ( model , 'gene' , element_lists = [ gene_list1 , gene_list2 ] , method = method , solution = solution , processes = processes , ** kwargs )
8653	def create_project_thread ( session , member_ids , project_id , message ) : return create_thread ( session , member_ids , 'project' , project_id , message )
9341	def MetaOrdered ( parallel , done , turnstile ) : class Ordered : def __init__ ( self , iterref ) : if parallel . master : done [ ... ] = 0 self . iterref = iterref parallel . barrier ( ) @ classmethod def abort ( self ) : turnstile . release ( ) def __enter__ ( self ) : while self . iterref != done : pass turnstile . acquire ( ) return self def __exit__ ( self , * args ) : done [ ... ] += 1 turnstile . release ( ) return Ordered
1365	def get_required_arguments_metricnames ( self ) : try : metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) if not metricnames : raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) return metricnames except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
7124	def validate_date ( date_text ) : try : if int ( date_text ) < 0 : return True except ValueError : pass try : datetime . strptime ( date_text , '%Y-%m-%d' ) return True except ValueError : pass raise ValueError ( 'Dates must be negative integers or YYYY-MM-DD in the past.' )
12823	def fspaths ( draw , allow_pathlike = None ) : has_pathlike = hasattr ( os , 'PathLike' ) if allow_pathlike is None : allow_pathlike = has_pathlike if allow_pathlike and not has_pathlike : raise InvalidArgument ( 'allow_pathlike: os.PathLike not supported, use None instead ' 'to enable it only when available' ) result_type = draw ( sampled_from ( [ bytes , text_type ] ) ) def tp ( s = '' ) : return _str_to_path ( s , result_type ) special_component = sampled_from ( [ tp ( os . curdir ) , tp ( os . pardir ) ] ) normal_component = _filename ( result_type ) path_component = one_of ( normal_component , special_component ) extension = normal_component . map ( lambda f : tp ( os . extsep ) + f ) root = _path_root ( result_type ) def optional ( st ) : return one_of ( st , just ( result_type ( ) ) ) sep = sampled_from ( [ os . sep , os . altsep or os . sep ] ) . map ( tp ) path_part = builds ( lambda s , l : s . join ( l ) , sep , lists ( path_component ) ) main_strategy = builds ( lambda * x : tp ( ) . join ( x ) , optional ( root ) , path_part , optional ( extension ) ) if allow_pathlike and hasattr ( os , 'fspath' ) : pathlike_strategy = main_strategy . map ( lambda p : _PathLike ( p ) ) main_strategy = one_of ( main_strategy , pathlike_strategy ) return draw ( main_strategy )
1801	def LAHF ( cpu ) : used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) is_expression = any ( issymbolic ( x ) for x in used_regs ) def make_flag ( val , offset ) : if is_expression : return Operators . ITEBV ( 8 , val , BitVecConstant ( 8 , 1 << offset ) , BitVecConstant ( 8 , 0 ) ) else : return val << offset cpu . AH = ( make_flag ( cpu . SF , 7 ) | make_flag ( cpu . ZF , 6 ) | make_flag ( 0 , 5 ) | make_flag ( cpu . AF , 4 ) | make_flag ( 0 , 3 ) | make_flag ( cpu . PF , 2 ) | make_flag ( 1 , 1 ) | make_flag ( cpu . CF , 0 ) )
7804	def verify_server ( self , server_name , srv_type = 'xmpp-client' ) : server_jid = JID ( server_name ) if "XmppAddr" not in self . alt_names and "DNS" not in self . alt_names and "SRV" not in self . alt_names : return self . verify_jid_against_common_name ( server_jid ) names = [ name for name in self . alt_names . get ( "DNS" , [ ] ) if not name . startswith ( u"*." ) ] names += self . alt_names . get ( "XmppAddr" , [ ] ) for name in names : logger . debug ( "checking {0!r} against {1!r}" . format ( server_jid , name ) ) try : jid = JID ( name ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_jid : logger . debug ( "Match!" ) return True if srv_type and self . verify_jid_against_srv_name ( server_jid , srv_type ) : return True wildcards = [ name [ 2 : ] for name in self . alt_names . get ( "DNS" , [ ] ) if name . startswith ( "*." ) ] if not wildcards or not "." in server_jid . domain : return False logger . debug ( "checking {0!r} against wildcard domains: {1!r}" . format ( server_jid , wildcards ) ) server_domain = JID ( domain = server_jid . domain . split ( "." , 1 ) [ 1 ] ) for domain in wildcards : logger . debug ( "checking {0!r} against {1!r}" . format ( server_domain , domain ) ) try : jid = JID ( domain ) except ValueError : logger . debug ( "Not a valid JID: {0!r}" . format ( name ) ) continue if jid == server_domain : logger . debug ( "Match!" ) return True return False
12610	def _concat_queries ( queries , operators = '__and__' ) : if not queries : raise ValueError ( 'Expected some `queries`, got {}.' . format ( queries ) ) if len ( queries ) == 1 : return queries [ 0 ] if isinstance ( operators , str ) : operators = [ operators ] * ( len ( queries ) - 1 ) if len ( queries ) - 1 != len ( operators ) : raise ValueError ( 'Expected `operators` to be a string or a list with the same' ' length as `field_names` ({}), got {}.' . format ( len ( queries ) , operators ) ) first , rest , end = queries [ 0 ] , queries [ 1 : - 1 ] , queries [ - 1 : ] [ 0 ] bigop = getattr ( first , operators [ 0 ] ) for i , q in enumerate ( rest ) : bigop = getattr ( bigop ( q ) , operators [ i ] ) return bigop ( end )
6745	def iter_sites ( sites = None , site = None , renderer = None , setter = None , no_secure = False , verbose = None ) : if verbose is None : verbose = get_verbose ( ) hostname = get_current_hostname ( ) target_sites = env . available_sites_by_host . get ( hostname , None ) if sites is None : site = site or env . SITE or ALL if site == ALL : sites = list ( six . iteritems ( env . sites ) ) else : sys . stderr . flush ( ) sites = [ ( site , env . sites . get ( site ) ) ] renderer = renderer env_default = save_env ( ) for _site , site_data in sorted ( sites ) : if no_secure and _site . endswith ( '_secure' ) : continue if target_sites is None : pass else : assert isinstance ( target_sites , ( tuple , list ) ) if _site not in target_sites : if verbose : print ( 'Skipping site %s because not in among target sites.' % _site ) continue env . update ( env_default ) env . update ( env . sites . get ( _site , { } ) ) env . SITE = _site if callable ( renderer ) : renderer ( ) if setter : setter ( _site ) yield _site , site_data env . update ( env_default ) added_keys = set ( env ) . difference ( env_default ) for key in added_keys : if key . startswith ( '_' ) : continue del env [ key ]
4005	def streaming_to_client ( ) : for handler in client_logger . handlers : if hasattr ( handler , 'append_newlines' ) : break else : handler = None old_propagate = client_logger . propagate client_logger . propagate = False if handler is not None : old_append = handler . append_newlines handler . append_newlines = False yield client_logger . propagate = old_propagate if handler is not None : handler . append_newlines = old_append
2125	def disassociate_always_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'always' ) , parent , child )
561	def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )
7100	def on_marker ( self , marker ) : mid , pos = marker self . marker = Marker ( __id__ = mid ) mapview = self . parent ( ) mapview . markers [ mid ] = self self . marker . setTag ( mid ) for w in self . child_widgets ( ) : mapview . init_info_window_adapter ( ) break d = self . declaration if d . show_info : self . set_show_info ( d . show_info ) del self . options
3562	def find_characteristic ( self , uuid ) : for char in self . list_characteristics ( ) : if char . uuid == uuid : return char return None
5884	def get_title ( self ) : title = '' if "title" in list ( self . article . opengraph . keys ( ) ) : return self . clean_title ( self . article . opengraph [ 'title' ] ) elif self . article . schema and "headline" in self . article . schema : return self . clean_title ( self . article . schema [ 'headline' ] ) meta_headline = self . parser . getElementsByTag ( self . article . doc , tag = "meta" , attr = "name" , value = "headline" ) if meta_headline is not None and len ( meta_headline ) > 0 : title = self . parser . getAttribute ( meta_headline [ 0 ] , 'content' ) return self . clean_title ( title ) title_element = self . parser . getElementsByTag ( self . article . doc , tag = 'title' ) if title_element is not None and len ( title_element ) > 0 : title = self . parser . getText ( title_element [ 0 ] ) return self . clean_title ( title ) return title
9532	def dumps ( obj , key = None , salt = 'django.core.signing' , serializer = JSONSerializer , compress = False ) : data = serializer ( ) . dumps ( obj ) is_compressed = False if compress : compressed = zlib . compress ( data ) if len ( compressed ) < ( len ( data ) - 1 ) : data = compressed is_compressed = True base64d = b64_encode ( data ) if is_compressed : base64d = b'.' + base64d return TimestampSigner ( key , salt = salt ) . sign ( base64d )
12306	def find_executable_files ( ) : files = glob . glob ( "*" ) + glob . glob ( "*/*" ) + glob . glob ( '*/*/*' ) files = filter ( lambda f : os . path . isfile ( f ) , files ) executable = stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH final = [ ] for filename in files : if os . path . isfile ( filename ) : st = os . stat ( filename ) mode = st . st_mode if mode & executable : final . append ( filename ) if len ( final ) > 5 : break return final
5224	def market_timing ( ticker , dt , timing = 'EOD' , tz = 'local' ) -> str : logger = logs . get_logger ( market_timing ) exch = pd . Series ( exch_info ( ticker = ticker ) ) if any ( req not in exch . index for req in [ 'tz' , 'allday' , 'day' ] ) : logger . error ( f'required exchange info cannot be found in {ticker} ...' ) return '' mkt_time = { 'BOD' : exch . day [ 0 ] , 'FINISHED' : exch . allday [ - 1 ] } . get ( timing , exch . day [ - 1 ] ) cur_dt = pd . Timestamp ( str ( dt ) ) . strftime ( '%Y-%m-%d' ) if tz == 'local' : return f'{cur_dt} {mkt_time}' return timezone . tz_convert ( f'{cur_dt} {mkt_time}' , to_tz = tz , from_tz = exch . tz )
1033	def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a_base64 ( s ) output . write ( line )
12955	def _add_id_to_index ( self , indexedField , pk , val , conn = None ) : if conn is None : conn = self . _get_connection ( ) conn . sadd ( self . _get_key_for_index ( indexedField , val ) , pk )
7729	def get_muc_child ( self ) : if self . muc_child : return self . muc_child if not self . xmlnode . children : return None n = self . xmlnode . children while n : if n . name not in ( "x" , "query" ) : n = n . next continue ns = n . ns ( ) if not ns : n = n . next continue ns_uri = ns . getContent ( ) if ( n . name , ns_uri ) == ( "x" , MUC_NS ) : self . muc_child = MucX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "x" , MUC_USER_NS ) : self . muc_child = MucUserX ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_ADMIN_NS ) : self . muc_child = MucAdminQuery ( n ) return self . muc_child if ( n . name , ns_uri ) == ( "query" , MUC_OWNER_NS ) : self . muc_child = MucOwnerX ( n ) return self . muc_child n = n . next
12507	def voxspace_to_mmspace ( img ) : shape , affine = img . shape [ : 3 ] , img . affine coords = np . array ( np . meshgrid ( * ( range ( i ) for i in shape ) , indexing = 'ij' ) ) coords = np . rollaxis ( coords , 0 , len ( shape ) + 1 ) mm_coords = nib . affines . apply_affine ( affine , coords ) return mm_coords
3217	def get_subnets ( vpc , ** conn ) : subnets = describe_subnets ( Filters = [ { "Name" : "vpc-id" , "Values" : [ vpc [ "id" ] ] } ] , ** conn ) s_ids = [ ] for s in subnets : s_ids . append ( s [ "SubnetId" ] ) return s_ids
11961	def is_wildcard_nm ( nm ) : try : dec = 0xFFFFFFFF - _dot_to_dec ( nm , check = True ) except ValueError : return False if dec in _NETMASKS_VALUES : return True return False
11383	def parser ( self ) : module = self . module subcommands = self . subcommands if subcommands : module_desc = inspect . getdoc ( module ) parser = Parser ( description = module_desc , module = module ) subparsers = parser . add_subparsers ( ) for sc_name , callback in subcommands . items ( ) : sc_name = sc_name . replace ( "_" , "-" ) cb_desc = inspect . getdoc ( callback ) sc_parser = subparsers . add_parser ( sc_name , callback = callback , help = cb_desc ) else : parser = Parser ( callback = self . callbacks [ self . function_name ] , module = module ) return parser
6755	def lenv ( self ) : _env = type ( env ) ( ) for _k , _v in six . iteritems ( env ) : if _k . startswith ( self . name + '_' ) : _env [ _k [ len ( self . name ) + 1 : ] ] = _v return _env
9225	def permutations_with_replacement ( iterable , r = None ) : pool = tuple ( iterable ) n = len ( pool ) r = n if r is None else r for indices in itertools . product ( range ( n ) , repeat = r ) : yield list ( pool [ i ] for i in indices )
3361	def shadow_price ( self ) : try : check_solver_status ( self . _model . solver . status ) return self . _model . constraints [ self . id ] . dual except AttributeError : raise RuntimeError ( "metabolite '{}' is not part of a model" . format ( self . id ) ) except ( RuntimeError , OptimizationError ) as err : raise_with_traceback ( err ) except Exception as err : raise_from ( OptimizationError ( "Likely no solution exists. Original solver message: {}." "" . format ( str ( err ) ) ) , err )
6060	def numpy_array_2d_from_fits ( file_path , hdu ) : hdu_list = fits . open ( file_path ) return np . flipud ( np . array ( hdu_list [ hdu ] . data ) )
7050	def _reform_templatelc_for_tfa ( task ) : try : ( lcfile , lcformat , lcformatdir , tcol , mcol , ecol , timebase , interpolate_type , sigclip ) = task try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None lcdict = readerfunc ( lcfile ) if ( ( isinstance ( lcdict , ( list , tuple ) ) ) and ( isinstance ( lcdict [ 0 ] , dict ) ) ) : lcdict = lcdict [ 0 ] outdict = { } if '.' in tcol : tcolget = tcol . split ( '.' ) else : tcolget = [ tcol ] times = _dict_get ( lcdict , tcolget ) if '.' in mcol : mcolget = mcol . split ( '.' ) else : mcolget = [ mcol ] mags = _dict_get ( lcdict , mcolget ) if '.' in ecol : ecolget = ecol . split ( '.' ) else : ecolget = [ ecol ] errs = _dict_get ( lcdict , ecolget ) if normfunc is None : ntimes , nmags = normalize_magseries ( times , mags , magsarefluxes = magsarefluxes ) times , mags , errs = ntimes , nmags , errs stimes , smags , serrs = sigclip_magseries ( times , mags , errs , sigclip = sigclip ) mags_interpolator = spi . interp1d ( stimes , smags , kind = interpolate_type , fill_value = 'extrapolate' ) errs_interpolator = spi . interp1d ( stimes , serrs , kind = interpolate_type , fill_value = 'extrapolate' ) interpolated_mags = mags_interpolator ( timebase ) interpolated_errs = errs_interpolator ( timebase ) magmedian = np . median ( interpolated_mags ) renormed_mags = interpolated_mags - magmedian outdict = { 'mags' : renormed_mags , 'errs' : interpolated_errs , 'origmags' : interpolated_mags } return outdict except Exception as e : LOGEXCEPTION ( 'reform LC task failed: %s' % repr ( task ) ) return None
1968	def wait ( self , readfds , writefds , timeout ) : logger . debug ( "WAIT:" ) logger . debug ( f"\tProcess {self._current} is going to wait for [ {readfds!r} {writefds!r} {timeout!r} ]" ) logger . debug ( f"\tProcess: {self.procs!r}" ) logger . debug ( f"\tRunning: {self.running!r}" ) logger . debug ( f"\tRWait: {self.rwait!r}" ) logger . debug ( f"\tTWait: {self.twait!r}" ) logger . debug ( f"\tTimers: {self.timers!r}" ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . debug ( f"\tTransfer control from process {procid} to {self._current}" ) logger . debug ( f"\tREMOVING {procid!r} from {self.running!r}. Current: {self._current!r}" ) self . running . remove ( procid ) if self . _current not in self . running : logger . debug ( "\tCurrent not running. Checking for timers..." ) self . _current = None self . check_timers ( )
4524	def save ( self , project_file = '' ) : self . _request_project_file ( project_file ) data_file . dump ( self . desc . as_dict ( ) , self . project_file )
9752	def build_swig ( ) : print ( "Looking for FANN libs..." ) find_fann ( ) print ( "running SWIG..." ) swig_bin = find_swig ( ) swig_cmd = [ swig_bin , '-c++' , '-python' , 'fann2/fann2.i' ] subprocess . Popen ( swig_cmd ) . wait ( )
3771	def mixing_logarithmic ( fracs , props ) : r if not none_and_length_check ( [ fracs , props ] ) : return None return exp ( sum ( frac * log ( prop ) for frac , prop in zip ( fracs , props ) ) )
2489	def create_extracted_license ( self , lic ) : licenses = list ( self . graph . triples ( ( None , self . spdx_namespace . licenseId , lic . identifier ) ) ) if len ( licenses ) != 0 : return licenses [ 0 ] [ 0 ] else : license_node = BNode ( ) type_triple = ( license_node , RDF . type , self . spdx_namespace . ExtractedLicensingInfo ) self . graph . add ( type_triple ) ident_triple = ( license_node , self . spdx_namespace . licenseId , Literal ( lic . identifier ) ) self . graph . add ( ident_triple ) text_triple = ( license_node , self . spdx_namespace . extractedText , Literal ( lic . text ) ) self . graph . add ( text_triple ) if lic . full_name is not None : name_triple = ( license_node , self . spdx_namespace . licenseName , self . to_special_value ( lic . full_name ) ) self . graph . add ( name_triple ) for ref in lic . cross_ref : triple = ( license_node , RDFS . seeAlso , URIRef ( ref ) ) self . graph . add ( triple ) if lic . comment is not None : comment_triple = ( license_node , RDFS . comment , Literal ( lic . comment ) ) self . graph . add ( comment_triple ) return license_node
8663	def generate_passphrase ( size = 12 ) : chars = string . ascii_lowercase + string . ascii_uppercase + string . digits return str ( '' . join ( random . choice ( chars ) for _ in range ( size ) ) )
7529	def setup_dirs ( data ) : pdir = os . path . realpath ( data . paramsdict [ "project_dir" ] ) data . dirs . clusts = os . path . join ( pdir , "{}_clust_{}" . format ( data . name , data . paramsdict [ "clust_threshold" ] ) ) if not os . path . exists ( data . dirs . clusts ) : os . mkdir ( data . dirs . clusts ) data . tmpdir = os . path . abspath ( os . path . expanduser ( os . path . join ( pdir , data . name + '-tmpalign' ) ) ) if not os . path . exists ( data . tmpdir ) : os . mkdir ( data . tmpdir ) if not data . paramsdict [ "assembly_method" ] == "denovo" : data . dirs . refmapping = os . path . join ( pdir , "{}_refmapping" . format ( data . name ) ) if not os . path . exists ( data . dirs . refmapping ) : os . mkdir ( data . dirs . refmapping )
11426	def record_strip_empty_volatile_subfields ( rec ) : for tag in rec . keys ( ) : for field in rec [ tag ] : field [ 0 ] [ : ] = [ subfield for subfield in field [ 0 ] if subfield [ 1 ] [ : 9 ] != "VOLATILE:" ]
8116	def line_line_intersection ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 , infinite = False ) : ua = ( x4 - x3 ) * ( y1 - y3 ) - ( y4 - y3 ) * ( x1 - x3 ) ub = ( x2 - x1 ) * ( y1 - y3 ) - ( y2 - y1 ) * ( x1 - x3 ) d = ( y4 - y3 ) * ( x2 - x1 ) - ( x4 - x3 ) * ( y2 - y1 ) if d == 0 : if ua == ub == 0 : return [ ] else : return [ ] ua /= float ( d ) ub /= float ( d ) if not infinite and not ( 0 <= ua <= 1 and 0 <= ub <= 1 ) : return None , None return [ ( x1 + ua * ( x2 - x1 ) , y1 + ua * ( y2 - y1 ) ) ]
1310	def MouseInput ( dx : int , dy : int , mouseData : int = 0 , dwFlags : int = MouseEventFlag . LeftDown , time_ : int = 0 ) -> INPUT : return _CreateInput ( MOUSEINPUT ( dx , dy , mouseData , dwFlags , time_ , None ) )
5883	def post_cleanup ( self ) : parse_tags = [ 'p' ] if self . config . parse_lists : parse_tags . extend ( [ 'ul' , 'ol' ] ) if self . config . parse_headers : parse_tags . extend ( [ 'h1' , 'h2' , 'h3' , 'h4' , 'h5' , 'h6' ] ) target_node = self . article . top_node node = self . add_siblings ( target_node ) for elm in self . parser . getChildren ( node ) : e_tag = self . parser . getTag ( elm ) if e_tag not in parse_tags : if ( self . is_highlink_density ( elm ) or self . is_table_and_no_para_exist ( elm ) or not self . is_nodescore_threshold_met ( node , elm ) ) : self . parser . remove ( elm ) return node
13519	def prop_power ( self , propulsion_eff = 0.7 , sea_margin = 0.2 ) : PP = ( 1 + sea_margin ) * self . resistance ( ) * self . speed / propulsion_eff return PP
8176	def iterscan ( self , string , idx = 0 , context = None ) : match = self . scanner . scanner ( string , idx ) . match actions = self . actions lastend = idx end = len ( string ) while True : m = match ( ) if m is None : break matchbegin , matchend = m . span ( ) if lastend == matchend : break action = actions [ m . lastindex ] if action is not None : rval , next_pos = action ( m , context ) if next_pos is not None and next_pos != matchend : matchend = next_pos match = self . scanner . scanner ( string , matchend ) . match yield rval , matchend lastend = matchend
7584	def _get_binary ( self ) : backup_binaries = [ "raxmlHPC-PTHREADS" , "raxmlHPC-PTHREADS-SSE3" ] for binary in [ self . params . binary ] + backup_binaries : proc = subprocess . Popen ( [ "which" , self . params . binary ] , stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) . communicate ( ) if proc : self . params . binary = binary if not proc [ 0 ] : raise Exception ( BINARY_ERROR . format ( self . params . binary ) )
12787	def check_file ( self , filename ) : if not exists ( filename ) : return False new_config = ConfigResolverBase ( ) new_config . read ( filename ) if self . version and not new_config . has_option ( 'meta' , 'version' ) : raise NoVersionError ( "The config option 'meta.version' is missing in {}. The " "application expects version {}!" . format ( filename , self . version ) ) elif not self . version and new_config . has_option ( 'meta' , 'version' ) : self . version = StrictVersion ( new_config . get ( 'meta' , 'version' ) ) self . _log . info ( '%r contains a version number, but the config ' 'instance was not created with a version ' 'restriction. Will set version number to "%s" to ' 'prevent accidents!' , filename , self . version ) elif self . version : file_version = new_config . get ( 'meta' , 'version' ) major , minor , _ = StrictVersion ( file_version ) . version expected_major , expected_minor , _ = self . version . version if expected_major != major : self . _log . error ( 'Invalid major version number in %r. Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return False if expected_minor != minor : self . _log . warning ( 'Mismatching minor version number in %r. ' 'Expected %r, got %r!' , abspath ( filename ) , str ( self . version ) , file_version ) return True return True
12967	def random ( self , cascadeFetch = False ) : matchedKeys = list ( self . getPrimaryKeys ( ) ) obj = None while matchedKeys and not obj : key = matchedKeys . pop ( random . randint ( 0 , len ( matchedKeys ) - 1 ) ) obj = self . get ( key , cascadeFetch = cascadeFetch ) return obj
12739	def create_joints ( self ) : stack = [ 'root' ] while stack : parent = stack . pop ( ) for child in self . hierarchy . get ( parent , ( ) ) : stack . append ( child ) if parent not in self . bones : continue bone = self . bones [ parent ] body = [ b for b in self . bodies if b . name == parent ] [ 0 ] for child in self . hierarchy . get ( parent , ( ) ) : child_bone = self . bones [ child ] child_body = [ b for b in self . bodies if b . name == child ] [ 0 ] shape = ( '' , 'hinge' , 'universal' , 'ball' ) [ len ( child_bone . dof ) ] self . joints . append ( self . world . join ( shape , body , child_body ) )
12772	def follow_markers ( self , start = 0 , end = 1e100 , states = None ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( self . markers ) : if frame_no < start : continue if frame_no >= end : break for states in self . _step_to_marker_frame ( frame_no ) : yield states
13772	def _default_json_default ( obj ) : if isinstance ( obj , ( datetime . datetime , datetime . date , datetime . time ) ) : return obj . strftime ( default_date_fmt ) else : return str ( obj )
3953	def get_last_result ( self ) : result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
10281	def get_peripheral_successor_edges ( graph : BELGraph , subgraph : BELGraph ) -> EdgeIterator : for u in subgraph : for _ , v , k in graph . out_edges ( u , keys = True ) : if v not in subgraph : yield u , v , k
11430	def record_order_subfields ( rec , tag = None ) : if rec is None : return rec if tag is None : tags = rec . keys ( ) for tag in tags : record_order_subfields ( rec , tag ) elif tag in rec : for i in xrange ( len ( rec [ tag ] ) ) : field = rec [ tag ] [ i ] ordered_subfields = sorted ( field [ 0 ] , key = lambda subfield : subfield [ 0 ] ) rec [ tag ] [ i ] = ( ordered_subfields , field [ 1 ] , field [ 2 ] , field [ 3 ] , field [ 4 ] )
2808	def convert_matmul ( params , w_name , scope_name , inputs , layers , weights , names ) : print ( 'Converting matmul ...' ) if names == 'short' : tf_name = 'MMUL' + random_string ( 4 ) elif names == 'keep' : tf_name = w_name else : tf_name = w_name + str ( random . random ( ) ) if len ( inputs ) == 1 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) elif len ( inputs ) == 2 : weights_name = '{0}.weight' . format ( w_name ) W = weights [ weights_name ] . numpy ( ) . transpose ( ) input_channels , output_channels = W . shape keras_weights = [ W ] dense = keras . layers . Dense ( output_channels , weights = keras_weights , use_bias = False , name = tf_name , bias_initializer = 'zeros' , kernel_initializer = 'zeros' , ) layers [ scope_name ] = dense ( layers [ inputs [ 0 ] ] ) else : raise AssertionError ( 'Cannot convert matmul layer' )
13805	def merge_ordered ( ordereds : typing . Iterable [ typing . Any ] ) -> typing . Iterable [ typing . Any ] : seen_set = set ( ) add_seen = seen_set . add return reversed ( tuple ( map ( lambda obj : add_seen ( obj ) or obj , filterfalse ( seen_set . __contains__ , chain . from_iterable ( map ( reversed , reversed ( ordereds ) ) ) , ) , ) ) )
13696	def parse_int ( s ) : try : val = int ( s ) except ValueError : print_err ( '\nInvalid integer: {}' . format ( s ) ) sys . exit ( 1 ) return val
5009	def create_course_completion ( self , user_id , payload ) : url = self . enterprise_configuration . sapsf_base_url + self . global_sap_config . completion_status_api_path return self . _call_post_with_user_override ( user_id , url , payload )
3337	def join_uri ( uri , * segments ) : sub = "/" . join ( segments ) if not sub : return uri return uri . rstrip ( "/" ) + "/" + sub
9104	def dropbox_post_factory ( request ) : try : max_age = int ( request . registry . settings . get ( 'post_token_max_age_seconds' ) ) except Exception : max_age = 300 try : drop_id = parse_post_token ( token = request . matchdict [ 'token' ] , secret = request . registry . settings [ 'post_secret' ] , max_age = max_age ) except SignatureExpired : raise HTTPGone ( 'dropbox expired' ) except Exception : raise HTTPNotFound ( 'no such dropbox' ) dropbox = request . registry . settings [ 'dropbox_container' ] . get_dropbox ( drop_id ) if dropbox . status_int >= 20 : raise HTTPGone ( 'dropbox already in processing, no longer accepts data' ) return dropbox
7520	def write_usnps ( data , sidx , pnames ) : tmparrs = os . path . join ( data . dirs . outfiles , "tmp-{}.h5" . format ( data . name ) ) with h5py . File ( tmparrs , 'r' ) as io5 : bisarr = io5 [ "bisarr" ] end = np . where ( np . all ( bisarr [ : ] == "" , axis = 0 ) ) [ 0 ] if np . any ( end ) : end = end . min ( ) else : end = bisarr . shape [ 1 ] with open ( data . outfiles . usnpsphy , 'w' ) as out : out . write ( "{} {}\n" . format ( bisarr . shape [ 0 ] , end ) ) for idx , name in enumerate ( pnames ) : out . write ( "{}{}\n" . format ( name , "" . join ( bisarr [ idx , : end ] ) ) )
12112	def _savepath ( self , filename ) : ( basename , ext ) = os . path . splitext ( filename ) basename = basename if ( ext in self . extensions ) else filename ext = ext if ( ext in self . extensions ) else self . extensions [ 0 ] savepath = os . path . abspath ( os . path . join ( self . directory , '%s%s' % ( basename , ext ) ) ) return ( tempfile . mkstemp ( ext , basename + "_" , self . directory ) [ 1 ] if self . hash_suffix else savepath )
2036	def SSTORE ( self , offset , value ) : storage_address = self . address self . _publish ( 'will_evm_write_storage' , storage_address , offset , value ) if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . world . set_storage_data ( storage_address , offset , value ) self . _publish ( 'did_evm_write_storage' , storage_address , offset , value )
11410	def record_move_fields ( rec , tag , field_positions_local , field_position_local = None ) : fields = record_delete_fields ( rec , tag , field_positions_local = field_positions_local ) return record_add_fields ( rec , tag , fields , field_position_local = field_position_local )
9541	def datetime_range_inclusive ( min , max , format ) : dmin = datetime . strptime ( min , format ) dmax = datetime . strptime ( max , format ) def checker ( v ) : dv = datetime . strptime ( v , format ) if dv < dmin or dv > dmax : raise ValueError ( v ) return checker
6504	def decorate_matches ( match_in , match_word ) : matches = re . finditer ( match_word , match_in , re . IGNORECASE ) for matched_string in set ( [ match . group ( ) for match in matches ] ) : match_in = match_in . replace ( matched_string , getattr ( settings , "SEARCH_MATCH_DECORATION" , u"<b>{}</b>" ) . format ( matched_string ) ) return match_in
12135	def directory ( cls , directory , root = None , extension = None , ** kwargs ) : root = os . getcwd ( ) if root is None else root suffix = '' if extension is None else '.' + extension . rsplit ( '.' ) [ - 1 ] pattern = directory + os . sep + '*' + suffix key = os . path . join ( root , directory , '*' ) . rsplit ( os . sep ) [ - 2 ] format_parse = list ( string . Formatter ( ) . parse ( key ) ) if not all ( [ el is None for el in zip ( * format_parse ) [ 1 ] ] ) : raise Exception ( 'Directory cannot contain format field specifications' ) return cls ( key , pattern , root , ** kwargs )
13890	def CreateLink ( target_path , link_path , override = True ) : _AssertIsLocal ( target_path ) _AssertIsLocal ( link_path ) if override and IsLink ( link_path ) : DeleteLink ( link_path ) dirname = os . path . dirname ( link_path ) if dirname : CreateDirectory ( dirname ) if sys . platform != 'win32' : return os . symlink ( target_path , link_path ) else : import jaraco . windows . filesystem return jaraco . windows . filesystem . symlink ( target_path , link_path ) from . _easyfs_win32 import CreateSymbolicLink try : dw_flags = 0 if target_path and os . path . isdir ( target_path ) : dw_flags = 1 return CreateSymbolicLink ( target_path , link_path , dw_flags ) except Exception as e : reraise ( e , 'Creating link "%(link_path)s" pointing to "%(target_path)s"' % locals ( ) )
7051	def parallel_tfa_lclist ( lclist , templateinfo , timecols = None , magcols = None , errcols = None , lcformat = 'hat-sql' , lcformatdir = None , interp = 'nearest' , sigclip = 5.0 , mintemplatedist_arcmin = 10.0 , nworkers = NCPUS , maxworkertasks = 1000 ) : if isinstance ( templateinfo , str ) and os . path . exists ( templateinfo ) : with open ( templateinfo , 'rb' ) as infd : templateinfo = pickle . load ( infd ) try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if timecols is None : timecols = templateinfo [ 'timecols' ] if magcols is None : magcols = templateinfo [ 'magcols' ] if errcols is None : errcols = templateinfo [ 'errcols' ] outdict = { } for t , m , e in zip ( timecols , magcols , errcols ) : tasks = [ ( x , t , m , e , templateinfo , lcformat , lcformatdir , interp , sigclip ) for x in lclist ] pool = mp . Pool ( nworkers , maxtasksperchild = maxworkertasks ) results = pool . map ( _parallel_tfa_worker , tasks ) pool . close ( ) pool . join ( ) outdict [ m ] = results return outdict
2664	def status ( self ) : status = [ ] if self . provider : status = self . provider . status ( self . blocks . values ( ) ) return status
7250	def launch_batch_workflow ( self , batch_workflow ) : url = '%(base_url)s/batch_workflows' % { 'base_url' : self . base_url } try : r = self . gbdx_connection . post ( url , json = batch_workflow ) batch_workflow_id = r . json ( ) [ 'batch_workflow_id' ] return batch_workflow_id except TypeError as e : self . logger . debug ( 'Batch Workflow not launched, reason: {0}' . format ( e ) )
9678	def read_info_string ( self ) : infostring = [ ] self . cnxn . xfer ( [ 0x3F ] ) sleep ( 9e-3 ) for i in range ( 60 ) : resp = self . cnxn . xfer ( [ 0x00 ] ) [ 0 ] infostring . append ( chr ( resp ) ) sleep ( 0.1 ) return '' . join ( infostring )
7715	def update_item ( self , jid , name = NO_CHANGE , groups = NO_CHANGE , callback = None , error_callback = None ) : item = self . roster [ jid ] if name is NO_CHANGE and groups is NO_CHANGE : return if name is NO_CHANGE : name = item . name if groups is NO_CHANGE : groups = item . groups item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
11262	def sub ( prev , pattern , repl , * args , ** kw ) : count = 0 if 'count' not in kw else kw . pop ( 'count' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . sub ( repl , s , count = count )
5024	def get_enterprise_customer ( uuid ) : if uuid is None : return None try : return EnterpriseCustomer . active_customers . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : raise CommandError ( _ ( 'Enterprise customer {uuid} not found, or not active' ) . format ( uuid = uuid ) )
6556	def projection ( self , variables ) : variables = set ( variables ) if not variables . issubset ( self . variables ) : raise ValueError ( "Cannot project to variables not in the constraint." ) idxs = [ i for i , v in enumerate ( self . variables ) if v in variables ] configurations = frozenset ( tuple ( config [ i ] for i in idxs ) for config in self . configurations ) variables = tuple ( self . variables [ i ] for i in idxs ) return self . from_configurations ( configurations , variables , self . vartype )
8931	def description ( _dummy_ctx , markdown = False ) : cfg = config . load ( ) markup = 'md' if markdown else 'html' description_file = cfg . rootjoin ( "build/project.{}" . format ( markup ) ) notify . banner ( "Creating {} file for Jenkins..." . format ( description_file ) ) long_description = cfg . project . long_description long_description = long_description . replace ( '\n\n' , '</p>\n<p>' ) long_description = re . sub ( r'(\W)``([^`]+)``(\W)' , r'\1<tt>\2</tt>\3' , long_description ) text = DESCRIPTION_TEMPLATES [ markup ] . format ( keywords = ', ' . join ( cfg . project . keywords ) , classifiers = '\n' . join ( cfg . project . classifiers ) , classifiers_indented = ' ' + '\n ' . join ( cfg . project . classifiers ) , packages = ', ' . join ( cfg . project . packages ) , long_description_html = '<p>{}</p>' . format ( long_description ) , ** cfg ) with io . open ( description_file , 'w' , encoding = 'utf-8' ) as handle : handle . write ( text )
8043	def consume ( self , kind ) : next_token = self . stream . move ( ) assert next_token . kind == kind
3402	def is_boundary_type ( reaction , boundary_type , external_compartment ) : sbo_term = reaction . annotation . get ( "sbo" , "" ) if isinstance ( sbo_term , list ) : sbo_term = sbo_term [ 0 ] sbo_term = sbo_term . upper ( ) if sbo_term == sbo_terms [ boundary_type ] : return True if sbo_term in [ sbo_terms [ k ] for k in sbo_terms if k != boundary_type ] : return False correct_compartment = external_compartment in reaction . compartments if boundary_type != "exchange" : correct_compartment = not correct_compartment rev_type = True if boundary_type == "demand" : rev_type = not reaction . reversibility elif boundary_type == "sink" : rev_type = reaction . reversibility return ( reaction . boundary and not any ( ex in reaction . id for ex in excludes [ boundary_type ] ) and correct_compartment and rev_type )
9703	def monitorTUN ( self ) : packet = self . checkTUN ( ) if packet : try : ret = self . _faraday . send ( packet ) return ret except AttributeError as error : print ( "AttributeError" )
7380	async def run ( self , * args , data ) : cmd = self . _get ( data . text ) try : if cmd is not None : command = self [ cmd ] ( * args , data = data ) return await peony . utils . execute ( command ) except : fmt = "Error occurred while running function {cmd}:" peony . utils . log_error ( fmt . format ( cmd = cmd ) )
2386	def create_model_path ( model_path ) : if not model_path . startswith ( "/" ) and not model_path . startswith ( "models/" ) : model_path = "/" + model_path if not model_path . startswith ( "models" ) : model_path = "models" + model_path if not model_path . endswith ( ".p" ) : model_path += ".p" return model_path
7326	def with_continuations ( ** c ) : if len ( c ) : keys , k = zip ( * c . items ( ) ) else : keys , k = tuple ( [ ] ) , tuple ( [ ] ) def d ( f ) : return C ( lambda kself , * conts : lambda * args : f ( * args , self = kself , ** dict ( zip ( keys , conts ) ) ) ) ( * k ) return d
545	def __checkIfBestCompletedModel ( self ) : jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is None : jobResults = { } else : jobResults = json . loads ( jobResultsStr ) isSaved = jobResults . get ( 'saved' , False ) bestMetric = jobResults . get ( 'bestValue' , None ) currentMetric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _isBestModel = ( not isSaved ) or ( currentMetric < bestMetric ) return self . _isBestModel , jobResults , jobResultsStr
6261	def swap_buffers ( self ) : self . frames += 1 glfw . swap_buffers ( self . window ) self . poll_events ( )
7907	def __error_message ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_error_message ( stanza ) return True
12318	def drop ( self , repo , args = [ ] ) : rootdir = repo . rootdir if os . path . exists ( rootdir ) : print ( "Cleaning repo directory: {}" . format ( rootdir ) ) shutil . rmtree ( rootdir ) server_repodir = self . server_rootdir_from_repo ( repo , create = False ) if os . path . exists ( server_repodir ) : print ( "Cleaning data from local git 'server': {}" . format ( server_repodir ) ) shutil . rmtree ( server_repodir ) super ( GitRepoManager , self ) . drop ( repo ) return { 'status' : 'success' , 'message' : "successful cleanup" }
10617	def get_compound_mass ( self , compound ) : if compound in self . material . compounds : return self . _compound_masses [ self . material . get_compound_index ( compound ) ] else : return 0.0
902	def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( "In updateAnomalyLikelihoods." ) print ( "Number of anomaly scores:" , len ( anomalyScores ) ) print ( "First 20:" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( "Params:" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( "Must have at least one anomalyScore" ) if not isValidEstimatorParams ( params ) : raise ValueError ( "'params' is not a valid params structure" ) if "historicalLikelihoods" not in params : params [ "historicalLikelihoods" ] = [ 1.0 ] historicalValues = params [ "movingAverage" ] [ "historicalValues" ] total = params [ "movingAverage" ] [ "total" ] windowSize = params [ "movingAverage" ] [ "windowSize" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] newParams = { "distribution" : params [ "distribution" ] , "movingAverage" : { "historicalValues" : historicalValues , "total" : total , "windowSize" : windowSize , } , "historicalLikelihoods" : historicalLikelihoods , } assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize if verbosity > 3 : print ( "Number of likelihoods:" , len ( likelihoods ) ) print ( "First 20 likelihoods:" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( "Leaving updateAnomalyLikelihoods." ) return ( likelihoods , aggRecordList , newParams )
6469	def consume ( self , istream , ostream , batch = False ) : datapoints = [ ] if batch : sleep = max ( 0.01 , self . option . sleep ) fd = istream . fileno ( ) while True : try : if select . select ( [ fd ] , [ ] , [ ] , sleep ) : try : line = istream . readline ( ) if line == '' : break datapoints . append ( self . consume_line ( line ) ) except ValueError : continue if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) if len ( datapoints ) > 1 : datapoints = datapoints [ - self . maximum_points : ] self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream ) time . sleep ( sleep ) except KeyboardInterrupt : break else : for line in istream : try : datapoints . append ( self . consume_line ( line ) ) except ValueError : pass if self . option . sort_by_column : datapoints = sorted ( datapoints , key = itemgetter ( self . option . sort_by_column - 1 ) ) self . update ( [ dp [ 0 ] for dp in datapoints ] , [ dp [ 1 ] for dp in datapoints ] ) self . render ( ostream )
11388	def parse ( self ) : if self . parsed : return self . callbacks = { } regex = re . compile ( "^{}_?" . format ( self . function_name ) , flags = re . I ) mains = set ( ) body = self . body ast_tree = ast . parse ( self . body , self . path ) for n in ast_tree . body : if hasattr ( n , 'name' ) : if regex . match ( n . name ) : mains . add ( n . name ) if hasattr ( n , 'value' ) : ns = n . value if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'targets' ) : ns = n . targets [ 0 ] if hasattr ( ns , 'id' ) : if regex . match ( ns . id ) : mains . add ( ns . id ) if hasattr ( n , 'names' ) : for ns in n . names : if hasattr ( ns , 'name' ) : if regex . match ( ns . name ) : mains . add ( ns . name ) if getattr ( ns , 'asname' , None ) : if regex . match ( ns . asname ) : mains . add ( ns . asname ) if len ( mains ) > 0 : module = self . module for function_name in mains : cb = getattr ( module , function_name , None ) if cb and callable ( cb ) : self . callbacks [ function_name ] = cb else : raise ParseError ( "no main function found" ) self . parsed = True return len ( self . callbacks ) > 0
2371	def settings ( self ) : for table in self . tables : if isinstance ( table , SettingTable ) : for statement in table . statements : yield statement
10462	def doesmenuitemexist ( self , window_name , object_name ) : try : menu_handle = self . _get_menu_handle ( window_name , object_name , False ) return 1 except LdtpServerException : return 0
1759	def read_int ( self , where , size = None , force = False ) : if size is None : size = self . address_bit_size assert size in SANE_SIZES self . _publish ( 'will_read_memory' , where , size ) data = self . _memory . read ( where , size // 8 , force ) assert ( 8 * len ( data ) ) == size value = Operators . CONCAT ( size , * map ( Operators . ORD , reversed ( data ) ) ) self . _publish ( 'did_read_memory' , where , value , size ) return value
13159	def update ( cls , cur , table : str , values : dict , where_keys : list ) -> tuple : keys = cls . _COMMA . join ( values . keys ( ) ) value_place_holder = cls . _PLACEHOLDER * len ( values ) where_clause , where_values = cls . _get_where_clause_with_values ( where_keys ) query = cls . _update_string . format ( table , keys , value_place_holder [ : - 1 ] , where_clause ) yield from cur . execute ( query , ( tuple ( values . values ( ) ) + where_values ) ) return ( yield from cur . fetchall ( ) )
5749	def aggregate_history ( self , ip , days_limit = None ) : first_date = None last_date = None prec_asn = None prec_block = None for entry in self . history ( ip , days_limit ) : if entry is None : continue date , asn , block = entry if first_date is None : last_date = date first_date = date prec_asn = asn prec_block = block elif prec_asn == asn and prec_block == block : first_date = date else : yield first_date , last_date , prec_asn , prec_block last_date = date first_date = date prec_asn = asn prec_block = block if first_date is not None : yield first_date , last_date , prec_asn , prec_block
2580	def checkpoint ( self , tasks = None ) : with self . checkpoint_lock : checkpoint_queue = None if tasks : checkpoint_queue = tasks else : checkpoint_queue = self . tasks checkpoint_dir = '{0}/checkpoint' . format ( self . run_dir ) checkpoint_dfk = checkpoint_dir + '/dfk.pkl' checkpoint_tasks = checkpoint_dir + '/tasks.pkl' if not os . path . exists ( checkpoint_dir ) : try : os . makedirs ( checkpoint_dir ) except FileExistsError : pass with open ( checkpoint_dfk , 'wb' ) as f : state = { 'rundir' : self . run_dir , 'task_count' : self . task_count } pickle . dump ( state , f ) count = 0 with open ( checkpoint_tasks , 'ab' ) as f : for task_id in checkpoint_queue : if not self . tasks [ task_id ] [ 'checkpoint' ] and self . tasks [ task_id ] [ 'app_fu' ] . done ( ) and self . tasks [ task_id ] [ 'app_fu' ] . exception ( ) is None : hashsum = self . tasks [ task_id ] [ 'hashsum' ] if not hashsum : continue t = { 'hash' : hashsum , 'exception' : None , 'result' : None } try : r = self . memoizer . hash_lookup ( hashsum ) . result ( ) except Exception as e : t [ 'exception' ] = e else : t [ 'result' ] = r pickle . dump ( t , f ) count += 1 self . tasks [ task_id ] [ 'checkpoint' ] = True logger . debug ( "Task {} checkpointed" . format ( task_id ) ) self . checkpointed_tasks += count if count == 0 : if self . checkpointed_tasks == 0 : logger . warn ( "No tasks checkpointed so far in this run. Please ensure caching is enabled" ) else : logger . debug ( "No tasks checkpointed in this pass." ) else : logger . info ( "Done checkpointing {} tasks" . format ( count ) ) return checkpoint_dir
11905	def snoise2d ( size , z = 0.0 , scale = 0.05 , octaves = 1 , persistence = 0.25 , lacunarity = 2.0 ) : import noise data = np . empty ( size , dtype = 'float32' ) for y in range ( size [ 0 ] ) : for x in range ( size [ 1 ] ) : v = noise . snoise3 ( x * scale , y * scale , z , octaves = octaves , persistence = persistence , lacunarity = lacunarity ) data [ x , y ] = v data = data * 0.5 + 0.5 if __debug__ : assert data . min ( ) >= 0. and data . max ( ) <= 1.0 return data
4554	def pointOnCircle ( cx , cy , radius , angle ) : angle = math . radians ( angle ) - ( math . pi / 2 ) x = cx + radius * math . cos ( angle ) if x < cx : x = math . ceil ( x ) else : x = math . floor ( x ) y = cy + radius * math . sin ( angle ) if y < cy : y = math . ceil ( y ) else : y = math . floor ( y ) return ( int ( x ) , int ( y ) )
11936	def display ( self ) : if not self . is_group ( ) : return self . _display return ( ( force_text ( k ) , v ) for k , v in self . _display )
7696	def delayed_call ( self , delay , function ) : main_loop = self handler = [ ] class DelayedCallHandler ( TimeoutHandler ) : @ timeout_handler ( delay , False ) def callback ( self ) : try : function ( ) finally : main_loop . remove_handler ( handler [ 0 ] ) handler . append ( DelayedCallHandler ( ) ) self . add_handler ( handler [ 0 ] )
2630	def scale_in ( self , blocks = 0 , machines = 0 , strategy = None ) : count = 0 instances = self . client . servers . list ( ) for instance in instances [ 0 : machines ] : print ( "Deleting : " , instance ) instance . delete ( ) count += 1 return count
10638	def get_element_mfr ( self , element ) : result = 0.0 for compound in self . material . compounds : formula = compound . split ( '[' ) [ 0 ] result += self . get_compound_mfr ( compound ) * stoich . element_mass_fraction ( formula , element ) return result
1545	def get_topology_info ( * args ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_topology_info ( * args ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
4548	def fill_rect ( setter , x , y , w , h , color = None , aa = False ) : for i in range ( x , x + w ) : _draw_fast_vline ( setter , i , y , h , color , aa )
11097	def select_by_pattern_in_abspath ( self , pattern , recursive = True , case_sensitive = False ) : if case_sensitive : def filters ( p ) : return pattern in p . abspath else : pattern = pattern . lower ( ) def filters ( p ) : return pattern in p . abspath . lower ( ) return self . select_file ( filters , recursive )
4910	def _create_session ( self , scope ) : now = datetime . datetime . utcnow ( ) if self . session is None or self . expires_at is None or now >= self . expires_at : if self . session : self . session . close ( ) oauth_access_token , expires_at = self . _get_oauth_access_token ( self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . degreed_user_id , self . enterprise_configuration . degreed_user_password , scope ) session = requests . Session ( ) session . timeout = self . SESSION_TIMEOUT session . headers [ 'Authorization' ] = 'Bearer {}' . format ( oauth_access_token ) session . headers [ 'content-type' ] = 'application/json' self . session = session self . expires_at = expires_at
12636	def dist_percentile_threshold ( dist_matrix , perc_thr = 0.05 , k = 1 ) : triu_idx = np . triu_indices ( dist_matrix . shape [ 0 ] , k = k ) upper = np . zeros_like ( dist_matrix ) upper [ triu_idx ] = dist_matrix [ triu_idx ] < np . percentile ( dist_matrix [ triu_idx ] , perc_thr ) return upper
7611	def get_top_clans ( self , location_id = 'global' , ** params : keys ) : url = self . api . LOCATIONS + '/' + str ( location_id ) + '/rankings/clans' return self . _get_model ( url , PartialClan , ** params )
2543	def set_file_license_comment ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_license_comment_set : self . file_license_comment_set = True self . file ( doc ) . license_comment = text return True else : raise CardinalityError ( 'File::LicenseComment' ) else : raise OrderError ( 'File::LicenseComment' )
12433	def dasherize ( value ) : value = value . strip ( ) value = re . sub ( r'([A-Z])' , r'-\1' , value ) value = re . sub ( r'[-_\s]+' , r'-' , value ) value = re . sub ( r'^-' , r'' , value ) value = value . lower ( ) return value
1333	def predictions_and_gradient ( self , image = None , label = None , strict = True , return_details = False ) : assert self . has_gradient ( ) if image is None : image = self . __original_image if label is None : label = self . __original_class in_bounds = self . in_bounds ( image ) assert not strict or in_bounds self . _total_prediction_calls += 1 self . _total_gradient_calls += 1 predictions , gradient = self . __model . predictions_and_gradient ( image , label ) is_adversarial , is_best , distance = self . __is_adversarial ( image , predictions , in_bounds ) assert predictions . ndim == 1 assert gradient . shape == image . shape if return_details : return predictions , gradient , is_adversarial , is_best , distance else : return predictions , gradient , is_adversarial
1530	def monitor ( self ) : def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : for topology , callbacks in watchers . items ( ) : file_path = os . path . join ( path , topology ) data = "" if os . path . exists ( file_path ) : with open ( os . path . join ( path , topology ) ) as f : data = f . read ( ) if topology not in directory or data != directory [ topology ] : proto_object = ProtoClass ( ) proto_object . ParseFromString ( data ) for callback in callbacks : callback ( proto_object ) directory [ topology ] = data while not self . monitoring_thread_stop_signal : topologies_path = self . get_topologies_path ( ) topologies = [ ] if os . path . isdir ( topologies_path ) : topologies = list ( filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , os . listdir ( topologies_path ) ) ) if set ( topologies ) != set ( self . topologies_directory ) : for callback in self . topologies_watchers : callback ( topologies ) self . topologies_directory = topologies trigger_watches_based_on_files ( self . topology_watchers , topologies_path , self . topologies_directory , Topology ) execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) trigger_watches_based_on_files ( self . execution_state_watchers , execution_state_path , self . execution_state_directory , ExecutionState ) packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) trigger_watches_based_on_files ( self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) trigger_watches_based_on_files ( self . pplan_watchers , pplan_path , self . pplan_directory , PhysicalPlan ) tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) trigger_watches_based_on_files ( self . tmaster_watchers , tmaster_path , self . tmaster_directory , TMasterLocation ) scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) trigger_watches_based_on_files ( self . scheduler_location_watchers , scheduler_location_path , self . scheduler_location_directory , SchedulerLocation ) self . event . wait ( timeout = 5 )
2644	def push_file ( self , source , dest_dir ) : local_dest = dest_dir + '/' + os . path . basename ( source ) if os . path . dirname ( source ) != dest_dir : try : shutil . copyfile ( source , local_dest ) os . chmod ( local_dest , 0o777 ) except OSError as e : raise FileCopyException ( e , self . hostname ) return local_dest
7398	def parse ( string ) : bib = [ ] if not isinstance ( string , six . text_type ) : string = string . decode ( 'utf-8' ) for key , value in special_chars : string = string . replace ( key , value ) string = re . sub ( r'\\[cuHvs]{?([a-zA-Z])}?' , r'\1' , string ) entries = re . findall ( r'(?u)@(\w+)[ \t]?{[ \t]*([^,\s]*)[ \t]*,?\s*((?:[^=,\s]+\s*\=\s*(?:"[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,}]*),?\s*?)+)\s*}' , string ) for entry in entries : pairs = re . findall ( r'(?u)([^=,\s]+)\s*\=\s*("[^"]*"|{(?:[^{}]*|{[^{}]*})*}|[^,]*)' , entry [ 2 ] ) bib . append ( { 'type' : entry [ 0 ] . lower ( ) , 'key' : entry [ 1 ] } ) for key , value in pairs : key = key . lower ( ) if value and value [ 0 ] == '"' and value [ - 1 ] == '"' : value = value [ 1 : - 1 ] if value and value [ 0 ] == '{' and value [ - 1 ] == '}' : value = value [ 1 : - 1 ] if key not in [ 'booktitle' , 'title' ] : value = value . replace ( '}' , '' ) . replace ( '{' , '' ) else : if value . startswith ( '{' ) and value . endswith ( '}' ) : value = value [ 1 : ] value = value [ : - 1 ] value = value . strip ( ) value = re . sub ( r'\s+' , ' ' , value ) bib [ - 1 ] [ key ] = value return bib
10450	def getobjectsize ( self , window_name , object_name = None ) : if not object_name : handle , name , app = self . _get_window_handle ( window_name ) else : handle = self . _get_object_handle ( window_name , object_name ) return self . _getobjectsize ( handle )
1579	def create_packet ( reqid , message ) : assert message . IsInitialized ( ) packet = '' typename = message . DESCRIPTOR . full_name datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) packet += HeronProtocol . pack_int ( datasize ) packet += HeronProtocol . pack_int ( len ( typename ) ) packet += typename packet += reqid . pack ( ) packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) packet += message . SerializeToString ( ) return OutgoingPacket ( packet )
4587	def stop ( self ) : if self . is_running : log . info ( 'Stopping' ) self . is_running = False self . __class__ . _INSTANCE = None try : self . thread and self . thread . stop ( ) except : log . error ( 'Error stopping thread' ) traceback . print_exc ( ) self . thread = None return True
11080	def get_user ( self , username ) : if hasattr ( self . _bot , 'user_manager' ) : user = self . _bot . user_manager . get_by_username ( username ) if user : return user user = SlackUser . get_user ( self . _bot . sc , username ) self . _bot . user_manager . set ( user ) return user return SlackUser . get_user ( self . _bot . sc , username )
9425	def open ( self , member , pwd = None ) : if isinstance ( member , RarInfo ) : member = member . filename archive = unrarlib . RAROpenArchiveDataEx ( self . filename , mode = constants . RAR_OM_EXTRACT ) handle = self . _open ( archive ) password = pwd or self . pwd if password is not None : unrarlib . RARSetPassword ( handle , b ( password ) ) data = _ReadIntoMemory ( ) c_callback = unrarlib . UNRARCALLBACK ( data . _callback ) unrarlib . RARSetCallback ( handle , c_callback , 0 ) try : rarinfo = self . _read_header ( handle ) while rarinfo is not None : if rarinfo . filename == member : self . _process_current ( handle , constants . RAR_TEST ) break else : self . _process_current ( handle , constants . RAR_SKIP ) rarinfo = self . _read_header ( handle ) if rarinfo is None : data = None except unrarlib . MissingPassword : raise RuntimeError ( "File is encrypted, password required" ) except unrarlib . BadPassword : raise RuntimeError ( "Bad password for File" ) except unrarlib . BadDataError : if password is not None : raise RuntimeError ( "File CRC error or incorrect password" ) else : raise RuntimeError ( "File CRC error" ) except unrarlib . UnrarException as e : raise BadRarFile ( "Bad RAR archive data: %s" % str ( e ) ) finally : self . _close ( handle ) if data is None : raise KeyError ( 'There is no item named %r in the archive' % member ) return data . get_bytes ( )
7677	def hierarchy ( annotation , ** kwargs ) : htimes , hlabels = hierarchy_flatten ( annotation ) htimes = [ np . asarray ( _ ) for _ in htimes ] return mir_eval . display . hierarchy ( htimes , hlabels , ** kwargs )
3604	def _authenticate ( self , params , headers ) : if self . authentication : user = self . authentication . get_user ( ) params . update ( { 'auth' : user . firebase_auth_token } ) headers . update ( self . authentication . authenticator . HEADERS )
5172	def _auto_client_files ( cls , client , ca_path = None , ca_contents = None , cert_path = None , cert_contents = None , key_path = None , key_contents = None ) : files = [ ] if ca_path and ca_contents : client [ 'ca' ] = ca_path files . append ( dict ( path = ca_path , contents = ca_contents , mode = DEFAULT_FILE_MODE ) ) if cert_path and cert_contents : client [ 'cert' ] = cert_path files . append ( dict ( path = cert_path , contents = cert_contents , mode = DEFAULT_FILE_MODE ) ) if key_path and key_contents : client [ 'key' ] = key_path files . append ( dict ( path = key_path , contents = key_contents , mode = DEFAULT_FILE_MODE , ) ) return files
3506	def loopless_fva_iter ( model , reaction , solution = False , zero_cutoff = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) current = model . objective . value sol = get_solution ( model ) objective_dir = model . objective . direction if reaction . boundary : if solution : return sol else : return current with model : _add_cycle_free ( model , sol . fluxes ) model . slim_optimize ( ) if abs ( reaction . flux - current ) < zero_cutoff : if solution : return sol return current ll_sol = get_solution ( model ) . fluxes reaction . bounds = ( current , current ) model . slim_optimize ( ) almost_ll_sol = get_solution ( model ) . fluxes with model : for rxn in model . reactions : rid = rxn . id if ( ( abs ( ll_sol [ rid ] ) < zero_cutoff ) and ( abs ( almost_ll_sol [ rid ] ) > zero_cutoff ) ) : rxn . bounds = max ( 0 , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) if solution : best = model . optimize ( ) else : model . slim_optimize ( ) best = reaction . flux model . objective . direction = objective_dir return best
2549	def include ( f ) : fl = open ( f , 'r' ) data = fl . read ( ) fl . close ( ) return raw ( data )
12402	def require ( self , req ) : reqs = req if isinstance ( req , list ) else [ req ] for req in reqs : if not isinstance ( req , BumpRequirement ) : req = BumpRequirement ( req ) req . required = True req . required_by = self self . requirements . append ( req )
2988	def ensure_iterable ( inst ) : if isinstance ( inst , string_types ) : return [ inst ] elif not isinstance ( inst , collections . Iterable ) : return [ inst ] else : return inst
11404	def filter_field_instances ( field_instances , filter_subcode , filter_value , filter_mode = 'e' ) : matched = [ ] if filter_mode == 'e' : to_match = ( filter_subcode , filter_value ) for instance in field_instances : if to_match in instance [ 0 ] : matched . append ( instance ) elif filter_mode == 's' : for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and subfield [ 1 ] . find ( filter_value ) > - 1 : matched . append ( instance ) break elif filter_mode == 'r' : reg_exp = re . compile ( filter_value ) for instance in field_instances : for subfield in instance [ 0 ] : if subfield [ 0 ] == filter_subcode and reg_exp . match ( subfield [ 1 ] ) is not None : matched . append ( instance ) break return matched
864	def makeDirectoryFromAbsolutePath ( absDirPath ) : assert os . path . isabs ( absDirPath ) try : os . makedirs ( absDirPath ) except OSError , e : if e . errno != os . errno . EEXIST : raise return absDirPath
5877	def get_local_image ( self , src ) : return ImageUtils . store_image ( self . fetcher , self . article . link_hash , src , self . config )
12216	def traverse_local_prefs ( stepback = 0 ) : locals_dict = get_frame_locals ( stepback + 1 ) for k in locals_dict : if not k . startswith ( '_' ) and k . upper ( ) == k : yield k , locals_dict
2957	def _get_blockade_id_from_cwd ( self , cwd = None ) : if not cwd : cwd = os . getcwd ( ) parent_dir = os . path . abspath ( cwd ) basename = os . path . basename ( parent_dir ) . lower ( ) blockade_id = re . sub ( r"[^a-z0-9]" , "" , basename ) if not blockade_id : blockade_id = "default" return blockade_id
11942	def stored_messages_archive ( context , num_elements = 10 ) : if "user" in context : user = context [ "user" ] if user . is_authenticated ( ) : qs = MessageArchive . objects . select_related ( "message" ) . filter ( user = user ) return { "messages" : qs [ : num_elements ] , "count" : qs . count ( ) , }
10340	def spia_matrices_to_tsvs ( spia_matrices : Mapping [ str , pd . DataFrame ] , directory : str ) -> None : os . makedirs ( directory , exist_ok = True ) for relation , df in spia_matrices . items ( ) : df . to_csv ( os . path . join ( directory , f'{relation}.tsv' ) , index = True )
5381	def _datetime_to_utc_int ( date ) : if date is None : return None epoch = dsub_util . replace_timezone ( datetime . utcfromtimestamp ( 0 ) , pytz . utc ) return ( date - epoch ) . total_seconds ( )
11327	def autodiscover ( ) : import imp from django . conf import settings for app in settings . INSTALLED_APPS : try : app_path = __import__ ( app , { } , { } , [ app . split ( '.' ) [ - 1 ] ] ) . __path__ except AttributeError : continue try : imp . find_module ( 'oembed_providers' , app_path ) except ImportError : continue __import__ ( "%s.oembed_providers" % app )
6570	def last_arg_decorator ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : if signature_matches ( func , args , kwargs ) : return func ( * args , ** kwargs ) else : return lambda last : func ( * ( args + ( last , ) ) , ** kwargs ) return decorator
4189	def window_poisson ( N , alpha = 2 ) : r n = linspace ( - N / 2. , ( N ) / 2. , N ) w = exp ( - alpha * abs ( n ) / ( N / 2. ) ) return w
1123	def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule
13118	def argument_search ( self ) : arguments , _ = self . argparser . parse_known_args ( ) return self . search ( ** vars ( arguments ) )
4038	def default_headers ( self ) : _headers = { "User-Agent" : "Pyzotero/%s" % __version__ , "Zotero-API-Version" : "%s" % __api_version__ , } if self . api_key : _headers [ "Authorization" ] = "Bearer %s" % self . api_key return _headers
162	def get_pointwise_inside_image_mask ( self , image ) : if len ( self . coords ) == 0 : return np . zeros ( ( 0 , ) , dtype = bool ) shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] x_within = np . logical_and ( 0 <= self . xx , self . xx < width ) y_within = np . logical_and ( 0 <= self . yy , self . yy < height ) return np . logical_and ( x_within , y_within )
4396	def adsSyncWriteByNameEx ( port , address , data_name , value , data_type ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_VALBYHND , handle , value , data_type ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT )
2021	def EXP_gas ( self , base , exponent ) : EXP_SUPPLEMENTAL_GAS = 10 def nbytes ( e ) : result = 0 for i in range ( 32 ) : result = Operators . ITEBV ( 512 , Operators . EXTRACT ( e , i * 8 , 8 ) != 0 , i + 1 , result ) return result return EXP_SUPPLEMENTAL_GAS * nbytes ( exponent )
6585	def input ( self , input , song ) : try : cmd = getattr ( self , self . CMD_MAP [ input ] [ 1 ] ) except ( IndexError , KeyError ) : return self . screen . print_error ( "Invalid command {!r}!" . format ( input ) ) cmd ( song )
8562	def list_loadbalancers ( self , datacenter_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers?depth=%s' % ( datacenter_id , str ( depth ) ) ) return response
8488	def load ( self , prefix = None , depth = None ) : prefix = prefix or self . prefix prefix = '/' + prefix . strip ( '/' ) + '/' if depth is None : depth = self . inherit_depth if not self . configured : log . debug ( "etcd not available" ) return if self . watching : log . info ( "Starting watcher for %r" , prefix ) self . start_watching ( ) log . info ( "Loading from etcd %r" , prefix ) try : result = self . client . get ( prefix ) except self . module . EtcdKeyNotFound : result = None if not result : log . info ( "No configuration found" ) return { } update = { } for item in result . children : key = item . key value = item . value try : value = pytool . json . from_json ( value ) except : pass if not self . case_sensitive : key = key . lower ( ) if key . startswith ( prefix ) : key = key [ len ( prefix ) : ] update [ key ] = value inherited = Config ( ) . settings . get ( self . inherit_key , update . get ( self . inherit_key , None ) ) if depth > 0 and inherited : log . info ( " ... inheriting ..." ) inherited = self . load ( inherited , depth - 1 ) or { } inherited . update ( update ) update = inherited return update
13869	def _GetNativeEolStyle ( platform = sys . platform ) : _NATIVE_EOL_STYLE_MAP = { 'win32' : EOL_STYLE_WINDOWS , 'linux2' : EOL_STYLE_UNIX , 'linux' : EOL_STYLE_UNIX , 'darwin' : EOL_STYLE_MAC , } result = _NATIVE_EOL_STYLE_MAP . get ( platform ) if result is None : from . _exceptions import UnknownPlatformError raise UnknownPlatformError ( platform ) return result
6721	def get_combined_requirements ( self , requirements = None ) : requirements = requirements or self . env . requirements def iter_lines ( fn ) : with open ( fn , 'r' ) as fin : for line in fin . readlines ( ) : line = line . strip ( ) if not line or line . startswith ( '#' ) : continue yield line content = [ ] if isinstance ( requirements , ( tuple , list ) ) : for f in requirements : f = self . find_template ( f ) content . extend ( list ( iter_lines ( f ) ) ) else : assert isinstance ( requirements , six . string_types ) f = self . find_template ( requirements ) content . extend ( list ( iter_lines ( f ) ) ) return '\n' . join ( content )
4525	def get ( self , position = 0 ) : n = len ( self ) if n == 1 : return self [ 0 ] pos = position if self . length and self . autoscale : pos *= len ( self ) pos /= self . length pos *= self . scale pos += self . offset if not self . continuous : if not self . serpentine : return self [ int ( pos % n ) ] m = ( 2 * n ) - 2 pos %= m if pos < n : return self [ int ( pos ) ] else : return self [ int ( m - pos ) ] if self . serpentine : pos %= ( 2 * n ) if pos > n : pos = ( 2 * n ) - pos else : pos %= n pos *= n - 1 pos /= n index = int ( pos ) fade = pos - index if not fade : return self [ index ] r1 , g1 , b1 = self [ index ] r2 , g2 , b2 = self [ ( index + 1 ) % len ( self ) ] dr , dg , db = r2 - r1 , g2 - g1 , b2 - b1 return r1 + fade * dr , g1 + fade * dg , b1 + fade * db
7369	def doc ( func ) : stripped_chars = " \t" if hasattr ( func , '__doc__' ) : docstring = func . __doc__ . lstrip ( " \n\t" ) if "\n" in docstring : i = docstring . index ( "\n" ) return docstring [ : i ] . rstrip ( stripped_chars ) elif docstring : return docstring . rstrip ( stripped_chars ) return ""
7426	def refmap_stats ( data , sample ) : mapf = os . path . join ( data . dirs . refmapping , sample . name + "-mapped-sorted.bam" ) umapf = os . path . join ( data . dirs . refmapping , sample . name + "-unmapped.bam" ) cmd1 = [ ipyrad . bins . samtools , "flagstat" , umapf ] proc1 = sps . Popen ( cmd1 , stderr = sps . STDOUT , stdout = sps . PIPE ) result1 = proc1 . communicate ( ) [ 0 ] cmd2 = [ ipyrad . bins . samtools , "flagstat" , mapf ] proc2 = sps . Popen ( cmd2 , stderr = sps . STDOUT , stdout = sps . PIPE ) result2 = proc2 . communicate ( ) [ 0 ] if "pair" in data . paramsdict [ "datatype" ] : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) / 2 sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) / 2 else : sample . stats [ "refseq_unmapped_reads" ] = int ( result1 . split ( ) [ 0 ] ) sample . stats [ "refseq_mapped_reads" ] = int ( result2 . split ( ) [ 0 ] ) sample_cleanup ( data , sample )
3820	async def create_conversation ( self , create_conversation_request ) : response = hangouts_pb2 . CreateConversationResponse ( ) await self . _pb_request ( 'conversations/createconversation' , create_conversation_request , response ) return response
2293	def eval_entropy ( x ) : hx = 0. sx = sorted ( x ) for i , j in zip ( sx [ : - 1 ] , sx [ 1 : ] ) : delta = j - i if bool ( delta ) : hx += np . log ( np . abs ( delta ) ) hx = hx / ( len ( x ) - 1 ) + psi ( len ( x ) ) - psi ( 1 ) return hx
12131	def lexsort ( self , * order ) : if order == [ ] : raise Exception ( "Please specify the keys for sorting, use" "'+' prefix for ascending," "'-' for descending.)" ) if not set ( el [ 1 : ] for el in order ) . issubset ( set ( self . varying_keys ) ) : raise Exception ( "Key(s) specified not in the set of varying keys." ) sorted_args = copy . deepcopy ( self ) specs_param = sorted_args . params ( 'specs' ) specs_param . constant = False sorted_args . specs = self . _lexsorted_specs ( order ) specs_param . constant = True sorted_args . _lexorder = order return sorted_args
12919	def save ( self ) : if len ( self ) == 0 : return [ ] mdl = self . getModel ( ) return mdl . saver . save ( self )
1083	def combine ( cls , date , time ) : "Construct a datetime from a given date and a given time." if not isinstance ( date , _date_class ) : raise TypeError ( "date argument must be a date instance" ) if not isinstance ( time , _time_class ) : raise TypeError ( "time argument must be a time instance" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )
11772	def NaiveBayesLearner ( dataset ) : targetvals = dataset . values [ dataset . target ] target_dist = CountingProbDist ( targetvals ) attr_dists = dict ( ( ( gv , attr ) , CountingProbDist ( dataset . values [ attr ] ) ) for gv in targetvals for attr in dataset . inputs ) for example in dataset . examples : targetval = example [ dataset . target ] target_dist . add ( targetval ) for attr in dataset . inputs : attr_dists [ targetval , attr ] . add ( example [ attr ] ) def predict ( example ) : def class_probability ( targetval ) : return ( target_dist [ targetval ] * product ( attr_dists [ targetval , attr ] [ example [ attr ] ] for attr in dataset . inputs ) ) return argmax ( targetvals , class_probability ) return predict
13517	def resistance ( self ) : self . total_resistance_coef = frictional_resistance_coef ( self . length , self . speed ) + residual_resistance_coef ( self . slenderness_coefficient , self . prismatic_coefficient , froude_number ( self . speed , self . length ) ) RT = 1 / 2 * self . total_resistance_coef * 1025 * self . surface_area * self . speed ** 2 return RT
5122	def set_transitions ( self , mat ) : if isinstance ( mat , dict ) : for key , value in mat . items ( ) : probs = list ( value . values ( ) ) if key not in self . g . node : msg = "One of the keys don't correspond to a vertex." raise ValueError ( msg ) elif len ( self . out_edges [ key ] ) > 0 and not np . isclose ( sum ( probs ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( np . array ( probs ) < 0 ) . any ( ) : msg = "Some transition probabilities were negative." raise ValueError ( msg ) for k , e in enumerate ( sorted ( self . g . out_edges ( key ) ) ) : self . _route_probs [ key ] [ k ] = value . get ( e [ 1 ] , 0 ) elif isinstance ( mat , np . ndarray ) : non_terminal = np . array ( [ self . g . out_degree ( v ) > 0 for v in self . g . nodes ( ) ] ) if mat . shape != ( self . nV , self . nV ) : msg = ( "Matrix is the wrong shape, should " "be {0} x {1}." ) . format ( self . nV , self . nV ) raise ValueError ( msg ) elif not np . allclose ( np . sum ( mat [ non_terminal , : ] , axis = 1 ) , 1 ) : msg = "Sum of transition probabilities at a vertex was not 1." raise ValueError ( msg ) elif ( mat < 0 ) . any ( ) : raise ValueError ( "Some transition probabilities were negative." ) for k in range ( self . nV ) : for j , e in enumerate ( sorted ( self . g . out_edges ( k ) ) ) : self . _route_probs [ k ] [ j ] = mat [ k , e [ 1 ] ] else : raise TypeError ( "mat must be a numpy array or a dict." )
979	def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap
2058	def _dict_diff ( d1 , d2 ) : d = { } for key in set ( d1 ) . intersection ( set ( d2 ) ) : if d2 [ key ] != d1 [ key ] : d [ key ] = d2 [ key ] for key in set ( d2 ) . difference ( set ( d1 ) ) : d [ key ] = d2 [ key ] return d
10987	def _pick_state_im_name ( state_name , im_name , use_full_path = False ) : initial_dir = os . getcwd ( ) if ( state_name is None ) or ( im_name is None ) : wid = tk . Tk ( ) wid . withdraw ( ) if state_name is None : state_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select pre-featured state' ) os . chdir ( os . path . dirname ( state_name ) ) if im_name is None : im_name = tkfd . askopenfilename ( initialdir = initial_dir , title = 'Select new image' ) if ( not use_full_path ) and ( os . path . dirname ( im_name ) != '' ) : im_path = os . path . dirname ( im_name ) os . chdir ( im_path ) im_name = os . path . basename ( im_name ) else : os . chdir ( initial_dir ) return state_name , im_name
5436	def parse_pair_args ( labels , argclass ) : label_data = set ( ) for arg in labels : name , value = split_pair ( arg , '=' , nullable_idx = 1 ) label_data . add ( argclass ( name , value ) ) return label_data
6325	def encode ( self , text ) : text = text_type ( text ) if '\x00' in text : text = text . replace ( '\x00' , ' ' ) minval = Fraction ( 0 ) maxval = Fraction ( 1 ) for char in text + '\x00' : prob_range = self . _probs [ char ] delta = maxval - minval maxval = minval + prob_range [ 1 ] * delta minval = minval + prob_range [ 0 ] * delta delta = ( maxval - minval ) / 2 nbits = long ( 0 ) while delta < 1 : nbits += 1 delta *= 2 if nbits == 0 : return 0 , 0 avg = ( maxval + minval ) * 2 ** ( nbits - 1 ) return avg . numerator // avg . denominator , nbits
12082	def clampfit_rename ( path , char ) : assert len ( char ) == 1 and type ( char ) == str , "replacement character must be a single character" assert os . path . exists ( path ) , "path doesn't exist" files = sorted ( os . listdir ( path ) ) files = [ x for x in files if len ( x ) > 18 and x [ 4 ] + x [ 7 ] + x [ 10 ] == ' ' ] for fname in files : fname2 = list ( fname ) fname2 [ 11 ] = char fname2 = "" . join ( fname2 ) if fname == fname2 : print ( fname , "==" , fname2 ) else : print ( fname , "->" , fname2 ) return
13458	def download_s3 ( bucket_name , file_key , file_path , force = False ) : file_path = path ( file_path ) bucket = open_s3 ( bucket_name ) file_dir = file_path . dirname ( ) file_dir . makedirs ( ) s3_key = bucket . get_key ( file_key ) if file_path . exists ( ) : file_data = file_path . bytes ( ) file_md5 , file_md5_64 = s3_key . get_md5_from_hexdigest ( hashlib . md5 ( file_data ) . hexdigest ( ) ) try : s3_md5 = s3_key . etag . replace ( '"' , '' ) except KeyError : pass else : if s3_md5 == file_md5 : info ( 'Hash is the same. Skipping %s' % file_path ) return elif not force : s3_datetime = datetime . datetime ( * time . strptime ( s3_key . last_modified , '%a, %d %b %Y %H:%M:%S %Z' ) [ 0 : 6 ] ) local_datetime = datetime . datetime . utcfromtimestamp ( file_path . stat ( ) . st_mtime ) if s3_datetime < local_datetime : info ( "File at %s is less recent than the local version." % ( file_key ) ) return info ( "Downloading %s..." % ( file_key ) ) try : with open ( file_path , 'w' ) as fo : s3_key . get_contents_to_file ( fo ) except Exception as e : error ( "Failed: %s" % e ) raise
9033	def _walk ( self ) : while self . _todo : args = self . _todo . pop ( 0 ) self . _step ( * args )
10446	def launchapp ( self , cmd , args = [ ] , delay = 0 , env = 1 , lang = "C" ) : try : atomac . NativeUIElement . launchAppByBundleId ( cmd ) return 1 except RuntimeError : if atomac . NativeUIElement . launchAppByBundlePath ( cmd , args ) : try : time . sleep ( int ( delay ) ) except ValueError : time . sleep ( 5 ) return 1 else : raise LdtpServerException ( u"Unable to find app '%s'" % cmd )
347	def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = "imdb.pkl" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( ".gz" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test
13299	def install ( self , package ) : logger . debug ( 'Installing ' + package ) shell . run ( self . pip_path , 'install' , package )
4782	def is_close_to ( self , other , tolerance ) : self . _validate_close_to_args ( self . val , other , tolerance ) if self . val < ( other - tolerance ) or self . val > ( other + tolerance ) : if type ( self . val ) is datetime . datetime : tolerance_seconds = tolerance . days * 86400 + tolerance . seconds + tolerance . microseconds / 1000000 h , rem = divmod ( tolerance_seconds , 3600 ) m , s = divmod ( rem , 60 ) self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%d:%02d:%02d>, but was not.' % ( self . val . strftime ( '%Y-%m-%d %H:%M:%S' ) , other . strftime ( '%Y-%m-%d %H:%M:%S' ) , h , m , s ) ) else : self . _err ( 'Expected <%s> to be close to <%s> within tolerance <%s>, but was not.' % ( self . val , other , tolerance ) ) return self
3666	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Cpsms = [ i ( T ) for i in self . HeatCapacitySolids ] return mixing_simple ( zs , Cpsms ) else : raise Exception ( 'Method not valid' )
12833	def on_enter_stage ( self ) : with self . world . _unlock_temporarily ( ) : self . forum . connect_everyone ( self . world , self . actors ) self . forum . on_start_game ( ) with self . world . _unlock_temporarily ( ) : self . world . on_start_game ( ) num_players = len ( self . actors ) - 1 for actor in self . actors : actor . on_setup_gui ( self . gui ) for actor in self . actors : actor . on_start_game ( num_players )
2085	def format_options ( self , ctx , formatter ) : field_opts = [ ] global_opts = [ ] local_opts = [ ] other_opts = [ ] for param in self . params : if param . name in SETTINGS_PARMS : opts = global_opts elif getattr ( param , 'help' , None ) and param . help . startswith ( '[FIELD]' ) : opts = field_opts param . help = param . help [ len ( '[FIELD]' ) : ] else : opts = local_opts rv = param . get_help_record ( ctx ) if rv is None : continue else : opts . append ( rv ) if self . add_help_option : help_options = self . get_help_option_names ( ctx ) if help_options : other_opts . append ( [ join_options ( help_options ) [ 0 ] , 'Show this message and exit.' ] ) if field_opts : with formatter . section ( 'Field Options' ) : formatter . write_dl ( field_opts ) if local_opts : with formatter . section ( 'Local Options' ) : formatter . write_dl ( local_opts ) if global_opts : with formatter . section ( 'Global Options' ) : formatter . write_dl ( global_opts ) if other_opts : with formatter . section ( 'Other Options' ) : formatter . write_dl ( other_opts )
12003	def _add_header ( self , data , options ) : version_info = self . _get_version_info ( options [ 'version' ] ) flags = options [ 'flags' ] header_flags = dict ( ( i , str ( int ( j ) ) ) for i , j in options [ 'flags' ] . iteritems ( ) ) header_flags = '' . join ( version_info [ 'flags' ] ( ** header_flags ) ) header_flags = int ( header_flags , 2 ) options [ 'flags' ] = header_flags header = version_info [ 'header' ] header = header ( ** options ) header = pack ( version_info [ 'header_format' ] , * header ) if 'timestamp' in flags and flags [ 'timestamp' ] : timestamp = long ( time ( ) ) timestamp = pack ( version_info [ 'timestamp_format' ] , timestamp ) header = header + timestamp return header + data
10548	def delete_taskrun ( taskrun_id ) : try : res = _pybossa_req ( 'delete' , 'taskrun' , taskrun_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
9975	def _alter_code ( code , ** attrs ) : PyCode_New = ctypes . pythonapi . PyCode_New PyCode_New . argtypes = ( ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . c_int , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . py_object , ctypes . c_int , ctypes . py_object ) PyCode_New . restype = ctypes . py_object args = [ [ code . co_argcount , 'co_argcount' ] , [ code . co_kwonlyargcount , 'co_kwonlyargcount' ] , [ code . co_nlocals , 'co_nlocals' ] , [ code . co_stacksize , 'co_stacksize' ] , [ code . co_flags , 'co_flags' ] , [ code . co_code , 'co_code' ] , [ code . co_consts , 'co_consts' ] , [ code . co_names , 'co_names' ] , [ code . co_varnames , 'co_varnames' ] , [ code . co_freevars , 'co_freevars' ] , [ code . co_cellvars , 'co_cellvars' ] , [ code . co_filename , 'co_filename' ] , [ code . co_name , 'co_name' ] , [ code . co_firstlineno , 'co_firstlineno' ] , [ code . co_lnotab , 'co_lnotab' ] ] for arg in args : if arg [ 1 ] in attrs : arg [ 0 ] = attrs [ arg [ 1 ] ] return PyCode_New ( args [ 0 ] [ 0 ] , args [ 1 ] [ 0 ] , args [ 2 ] [ 0 ] , args [ 3 ] [ 0 ] , args [ 4 ] [ 0 ] , args [ 5 ] [ 0 ] , args [ 6 ] [ 0 ] , args [ 7 ] [ 0 ] , args [ 8 ] [ 0 ] , args [ 9 ] [ 0 ] , args [ 10 ] [ 0 ] , args [ 11 ] [ 0 ] , args [ 12 ] [ 0 ] , args [ 13 ] [ 0 ] , args [ 14 ] [ 0 ] )
4169	def zpk2tf ( z , p , k ) : r import scipy . signal b , a = scipy . signal . zpk2tf ( z , p , k ) return b , a
5826	def _validate_search_query ( self , returning_query ) : start_index = returning_query . from_index or 0 size = returning_query . size or 0 if start_index < 0 : raise CitrinationClientError ( "start_index cannot be negative. Please enter a value greater than or equal to zero" ) if size < 0 : raise CitrinationClientError ( "Size cannot be negative. Please enter a value greater than or equal to zero" ) if start_index + size > MAX_QUERY_DEPTH : raise CitrinationClientError ( "Citrination does not support pagination past the {0}th result. Please reduce either the from_index and/or size such that their sum is below {0}" . format ( MAX_QUERY_DEPTH ) )
1728	def to_arr ( this ) : return [ this . get ( str ( e ) ) for e in xrange ( len ( this ) ) ]
9831	def read ( self , file ) : DXfield = self p = DXParser ( file ) p . parse ( DXfield )
4712	def script_run ( trun , script ) : if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { script: %s }" % script ) cij . emph ( "rnr:script:run:evars: %s" % script [ "evars" ] ) launchers = { ".py" : "python" , ".sh" : "source" } ext = os . path . splitext ( script [ "fpath" ] ) [ - 1 ] if not ext in launchers . keys ( ) : cij . err ( "rnr:script:run { invalid script[\"fpath\"]: %r }" % script [ "fpath" ] ) return 1 launch = launchers [ ext ] with open ( script [ "log_fpath" ] , "a" ) as log_fd : log_fd . write ( "# script_fpath: %r\n" % script [ "fpath" ] ) log_fd . flush ( ) bgn = time . time ( ) cmd = [ 'bash' , '-c' , 'CIJ_ROOT=$(cij_root) && ' 'source $CIJ_ROOT/modules/cijoe.sh && ' 'source %s && ' 'CIJ_TEST_RES_ROOT="%s" %s %s ' % ( trun [ "conf" ] [ "ENV_FPATH" ] , script [ "res_root" ] , launch , script [ "fpath" ] ) ] if trun [ "conf" ] [ "VERBOSE" ] > 1 : cij . emph ( "rnr:script:run { cmd: %r }" % " " . join ( cmd ) ) evars = os . environ . copy ( ) evars . update ( { k : str ( script [ "evars" ] [ k ] ) for k in script [ "evars" ] } ) process = Popen ( cmd , stdout = log_fd , stderr = STDOUT , cwd = script [ "res_root" ] , env = evars ) process . wait ( ) script [ "rcode" ] = process . returncode script [ "wallc" ] = time . time ( ) - bgn if trun [ "conf" ] [ "VERBOSE" ] : cij . emph ( "rnr:script:run { wallc: %02f }" % script [ "wallc" ] ) cij . emph ( "rnr:script:run { rcode: %r } " % script [ "rcode" ] , script [ "rcode" ] ) return script [ "rcode" ]
3890	def markdown ( tag ) : return ( MARKDOWN_START . format ( tag = tag ) , MARKDOWN_END . format ( tag = tag ) )
11502	def folder_get ( self , token , folder_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = folder_id response = self . request ( 'midas.folder.get' , parameters ) return response
9368	def legal_inn ( ) : mask = [ 2 , 4 , 10 , 3 , 5 , 9 , 4 , 6 , 8 ] inn = [ random . randint ( 1 , 9 ) for _ in range ( 10 ) ] weighted = [ v * mask [ i ] for i , v in enumerate ( inn [ : - 1 ] ) ] inn [ 9 ] = sum ( weighted ) % 11 % 10 return "" . join ( map ( str , inn ) )
8258	def sort_by_distance ( self , reversed = False ) : if len ( self ) == 0 : return ColorList ( ) root = self [ 0 ] for clr in self [ 1 : ] : if clr . brightness < root . brightness : root = clr stack = [ clr for clr in self ] stack . remove ( root ) sorted = [ root ] while len ( stack ) > 1 : closest , distance = stack [ 0 ] , stack [ 0 ] . distance ( sorted [ - 1 ] ) for clr in stack [ 1 : ] : d = clr . distance ( sorted [ - 1 ] ) if d < distance : closest , distance = clr , d stack . remove ( closest ) sorted . append ( closest ) sorted . append ( stack [ 0 ] ) if reversed : _list . reverse ( sorted ) return ColorList ( sorted )
12759	def load_csv ( self , filename , start_frame = 10 , max_frames = int ( 1e300 ) ) : import pandas as pd compression = None if filename . endswith ( '.gz' ) : compression = 'gzip' df = pd . read_csv ( filename , compression = compression ) . set_index ( 'time' ) . fillna ( - 1 ) assert self . world . dt == pd . Series ( df . index ) . diff ( ) . mean ( ) markers = [ ] for c in df . columns : m = re . match ( r'^marker\d\d-(.*)-c$' , c ) if m : markers . append ( m . group ( 1 ) ) self . channels = self . _map_labels_to_channels ( markers ) cols = [ c for c in df . columns if re . match ( r'^marker\d\d-.*-[xyzc]$' , c ) ] self . data = df [ cols ] . values . reshape ( ( len ( df ) , len ( markers ) , 4 ) ) [ start_frame : ] self . data [ : , : , [ 1 , 2 ] ] = self . data [ : , : , [ 2 , 1 ] ] logging . info ( '%s: loaded marker data %s' , filename , self . data . shape ) self . process_data ( ) self . create_bodies ( )
7945	def _continue_connect ( self ) : try : self . _socket . connect ( self . _dst_addr ) except socket . error , err : logger . debug ( "Connect error: {0}" . format ( err ) ) if err . args [ 0 ] == errno . EISCONN : pass elif err . args [ 0 ] in BLOCKING_ERRORS : return None elif self . _dst_addrs : self . _set_state ( "connect" ) return None elif self . _dst_nameports : self . _set_state ( "resolve-hostname" ) return None else : self . _socket . close ( ) self . _socket = None self . _set_state ( "aborted" ) raise self . _connected ( )
4550	def fill_round_rect ( setter , x , y , w , h , r , color = None , aa = False ) : fill_rect ( setter , x + r , y , w - 2 * r , h , color , aa ) _fill_circle_helper ( setter , x + w - r - 1 , y + r , r , 1 , h - 2 * r - 1 , color , aa ) _fill_circle_helper ( setter , x + r , y + r , r , 2 , h - 2 * r - 1 , color , aa )
11058	def start ( self ) : self . bot_start_time = datetime . now ( ) self . webserver = Webserver ( self . config [ 'webserver' ] [ 'host' ] , self . config [ 'webserver' ] [ 'port' ] ) self . plugins . load ( ) self . plugins . load_state ( ) self . _find_event_handlers ( ) self . sc = ThreadedSlackClient ( self . config [ 'slack_token' ] ) self . always_send_dm = [ '_unauthorized_' ] if 'always_send_dm' in self . config : self . always_send_dm . extend ( map ( lambda x : '!' + x , self . config [ 'always_send_dm' ] ) ) logging . getLogger ( 'Rocket.Errors.ThreadPool' ) . setLevel ( logging . INFO ) self . is_setup = True if self . test_mode : self . metrics [ 'startup_time' ] = ( datetime . now ( ) - self . bot_start_time ) . total_seconds ( ) * 1000.0
10857	def _i2p ( self , ind , coord ) : return '-' . join ( [ self . param_prefix , str ( ind ) , coord ] )
1826	def PUSH ( cpu , src ) : size = src . size v = src . read ( ) if size != 64 and size != cpu . address_bit_size // 2 : v = Operators . SEXTEND ( v , size , cpu . address_bit_size ) size = cpu . address_bit_size cpu . push ( v , size )
4669	def decrypt ( encrypted_privkey , passphrase ) : d = unhexlify ( base58decode ( encrypted_privkey ) ) d = d [ 2 : ] flagbyte = d [ 0 : 1 ] d = d [ 1 : ] assert flagbyte == b"\xc0" , "Flagbyte has to be 0xc0" salt = d [ 0 : 4 ] d = d [ 4 : - 4 ] if SCRYPT_MODULE == "scrypt" : key = scrypt . hash ( passphrase , salt , 16384 , 8 , 8 ) elif SCRYPT_MODULE == "pylibscrypt" : key = scrypt . scrypt ( bytes ( passphrase , "utf-8" ) , salt , 16384 , 8 , 8 ) else : raise ValueError ( "No scrypt module loaded" ) derivedhalf1 = key [ 0 : 32 ] derivedhalf2 = key [ 32 : 64 ] encryptedhalf1 = d [ 0 : 16 ] encryptedhalf2 = d [ 16 : 32 ] aes = AES . new ( derivedhalf2 , AES . MODE_ECB ) decryptedhalf2 = aes . decrypt ( encryptedhalf2 ) decryptedhalf1 = aes . decrypt ( encryptedhalf1 ) privraw = decryptedhalf1 + decryptedhalf2 privraw = "%064x" % ( int ( hexlify ( privraw ) , 16 ) ^ int ( hexlify ( derivedhalf1 ) , 16 ) ) wif = Base58 ( privraw ) privkey = PrivateKey ( format ( wif , "wif" ) ) addr = format ( privkey . bitcoin . address , "BTC" ) a = _bytes ( addr ) saltverify = hashlib . sha256 ( hashlib . sha256 ( a ) . digest ( ) ) . digest ( ) [ 0 : 4 ] if saltverify != salt : raise SaltException ( "checksum verification failed! Password may be incorrect." ) return wif
7676	def intervals ( annotation , ** kwargs ) : times , labels = annotation . to_interval_values ( ) return mir_eval . display . labeled_intervals ( times , labels , ** kwargs )
283	def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
5218	def hist_file ( ticker : str , dt , typ = 'TRADE' ) -> str : data_path = os . environ . get ( assist . BBG_ROOT , '' ) . replace ( '\\' , '/' ) if not data_path : return '' asset = ticker . split ( ) [ - 1 ] proper_ticker = ticker . replace ( '/' , '_' ) cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) return f'{data_path}/{asset}/{proper_ticker}/{typ}/{cur_dt}.parq'
11883	def scanProcessForOpenFile ( pid , searchPortion , isExactMatch = True , ignoreCase = False ) : try : try : pid = int ( pid ) except ValueError as e : sys . stderr . write ( 'Expected an integer, got %s for pid.\n' % ( str ( type ( pid ) ) , ) ) raise e prefixDir = "/proc/%d/fd" % ( pid , ) processFDs = os . listdir ( prefixDir ) matchedFDs = [ ] matchedFilenames = [ ] if isExactMatch is True : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor == totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) == totalPath . lower ( ) ) else : if ignoreCase is False : isMatch = lambda searchFor , totalPath : bool ( searchFor in totalPath ) else : isMatch = lambda searchFor , totalPath : bool ( searchFor . lower ( ) in totalPath . lower ( ) ) for fd in processFDs : fdPath = os . readlink ( prefixDir + '/' + fd ) if isMatch ( searchPortion , fdPath ) : matchedFDs . append ( fd ) matchedFilenames . append ( fdPath ) if len ( matchedFDs ) == 0 : return None cmdline = getProcessCommandLineStr ( pid ) owner = getProcessOwnerStr ( pid ) return { 'searchPortion' : searchPortion , 'pid' : pid , 'owner' : owner , 'cmdline' : cmdline , 'fds' : matchedFDs , 'filenames' : matchedFilenames , } except OSError : return None except IOError : return None except FileNotFoundError : return None except PermissionError : return None
5816	def _read_remaining ( socket ) : output = b'' old_timeout = socket . gettimeout ( ) try : socket . settimeout ( 0.0 ) output += socket . recv ( 8192 ) except ( socket_ . error ) : pass finally : socket . settimeout ( old_timeout ) return output
4219	def get_preferred_collection ( self ) : bus = secretstorage . dbus_init ( ) try : if hasattr ( self , 'preferred_collection' ) : collection = secretstorage . Collection ( bus , self . preferred_collection ) else : collection = secretstorage . get_default_collection ( bus ) except exceptions . SecretStorageException as e : raise InitError ( "Failed to create the collection: %s." % e ) if collection . is_locked ( ) : collection . unlock ( ) if collection . is_locked ( ) : raise KeyringLocked ( "Failed to unlock the collection!" ) return collection
9926	def handle ( self , * args , ** kwargs ) : cutoff = timezone . now ( ) cutoff -= app_settings . CONFIRMATION_EXPIRATION cutoff -= app_settings . CONFIRMATION_SAVE_PERIOD queryset = models . EmailConfirmation . objects . filter ( created_at__lte = cutoff ) count = queryset . count ( ) queryset . delete ( ) if count : self . stdout . write ( self . style . SUCCESS ( "Removed {count} old email confirmation(s)" . format ( count = count ) ) ) else : self . stdout . write ( "No email confirmations to remove." )
6554	def flip_variable ( self , v ) : try : idx = self . variables . index ( v ) except ValueError : raise ValueError ( "variable {} is not a variable in constraint {}" . format ( v , self . name ) ) if self . vartype is dimod . BINARY : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = 1 - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( 1 - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) else : original_func = self . func def func ( * args ) : new_args = list ( args ) new_args [ idx ] = - new_args [ idx ] return original_func ( * new_args ) self . func = func self . configurations = frozenset ( config [ : idx ] + ( - config [ idx ] , ) + config [ idx + 1 : ] for config in self . configurations ) self . name = '{} ({} flipped)' . format ( self . name , v )
12461	def pip_cmd ( env , cmd , ignore_activated = False , ** kwargs ) : r cmd = tuple ( cmd ) dirname = safe_path ( env ) if not ignore_activated : activated_env = os . environ . get ( 'VIRTUAL_ENV' ) if hasattr ( sys , 'real_prefix' ) : dirname = sys . prefix elif activated_env : dirname = activated_env pip_path = os . path . join ( dirname , 'Scripts' if IS_WINDOWS else 'bin' , 'pip' ) if kwargs . pop ( 'return_path' , False ) : return pip_path if not os . path . isfile ( pip_path ) : raise OSError ( 'No pip found at {0!r}' . format ( pip_path ) ) if BOOTSTRAPPER_TEST_KEY in os . environ and cmd [ 0 ] == 'install' : cmd = list ( cmd ) cmd . insert ( 1 , '--disable-pip-version-check' ) cmd = tuple ( cmd ) with disable_error_handler ( ) : return run_cmd ( ( pip_path , ) + cmd , ** kwargs )
8798	def get_security_group_states ( self , interfaces ) : LOG . debug ( "Getting security groups from Redis for {0}" . format ( interfaces ) ) interfaces = tuple ( interfaces ) vif_keys = [ self . vif_key ( vif . device_id , vif . mac_address ) for vif in interfaces ] sec_grp_all = self . get_fields_all ( vif_keys ) ret = { } for vif , group in zip ( interfaces , sec_grp_all ) : if group : ret [ vif ] = { SECURITY_GROUP_ACK : None , SECURITY_GROUP_HASH_ATTR : [ ] } temp_ack = group [ SECURITY_GROUP_ACK ] . lower ( ) temp_rules = group [ SECURITY_GROUP_HASH_ATTR ] if temp_rules : temp_rules = json . loads ( temp_rules ) ret [ vif ] [ SECURITY_GROUP_HASH_ATTR ] = temp_rules [ "rules" ] if "true" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = True elif "false" in temp_ack : ret [ vif ] [ SECURITY_GROUP_ACK ] = False else : ret . pop ( vif , None ) LOG . debug ( "Skipping bad ack value %s" % temp_ack ) return ret
4160	def _get_data ( url ) : if url . startswith ( 'http://' ) : try : resp = urllib . urlopen ( url ) encoding = resp . headers . dict . get ( 'content-encoding' , 'plain' ) except AttributeError : resp = urllib . request . urlopen ( url ) encoding = resp . headers . get ( 'content-encoding' , 'plain' ) data = resp . read ( ) if encoding == 'plain' : pass elif encoding == 'gzip' : data = StringIO ( data ) data = gzip . GzipFile ( fileobj = data ) . read ( ) else : raise RuntimeError ( 'unknown encoding' ) else : with open ( url , 'r' ) as fid : data = fid . read ( ) return data
6376	def dist_abs ( self , src , tar , max_offset = 5 ) : if not src : return len ( tar ) if not tar : return len ( src ) src_len = len ( src ) tar_len = len ( tar ) src_cur = 0 tar_cur = 0 lcss = 0 local_cs = 0 while ( src_cur < src_len ) and ( tar_cur < tar_len ) : if src [ src_cur ] == tar [ tar_cur ] : local_cs += 1 else : lcss += local_cs local_cs = 0 if src_cur != tar_cur : src_cur = tar_cur = max ( src_cur , tar_cur ) for i in range ( max_offset ) : if not ( ( src_cur + i < src_len ) or ( tar_cur + i < tar_len ) ) : break if ( src_cur + i < src_len ) and ( src [ src_cur + i ] == tar [ tar_cur ] ) : src_cur += i local_cs += 1 break if ( tar_cur + i < tar_len ) and ( src [ src_cur ] == tar [ tar_cur + i ] ) : tar_cur += i local_cs += 1 break src_cur += 1 tar_cur += 1 lcss += local_cs return round ( max ( src_len , tar_len ) - lcss )
7749	def process_iq ( self , stanza ) : typ = stanza . stanza_type if typ in ( "result" , "error" ) : return self . _process_iq_response ( stanza ) if typ not in ( "get" , "set" ) : raise BadRequestProtocolError ( "Bad <iq/> type" ) logger . debug ( "Handling <iq type='{0}'> stanza: {1!r}" . format ( stanza , typ ) ) payload = stanza . get_payload ( None ) logger . debug ( " payload: {0!r}" . format ( payload ) ) if not payload : raise BadRequestProtocolError ( "<iq/> stanza with no child element" ) handler = self . _get_iq_handler ( typ , payload ) if not handler : payload = stanza . get_payload ( None , specialize = True ) logger . debug ( " specialized payload: {0!r}" . format ( payload ) ) if not isinstance ( payload , XMLPayload ) : handler = self . _get_iq_handler ( typ , payload ) if handler : response = handler ( stanza ) self . _process_handler_result ( response ) return True else : raise ServiceUnavailableProtocolError ( "Not implemented" )
11989	def on_message ( self , websocket , message ) : waiter = self . _waiter self . _waiter = None encoded = json . loads ( message ) event = encoded . get ( 'event' ) channel = encoded . get ( 'channel' ) data = json . loads ( encoded . get ( 'data' ) ) try : if event == PUSHER_ERROR : raise PusherError ( data [ 'message' ] , data [ 'code' ] ) elif event == PUSHER_CONNECTION : self . socket_id = data . get ( 'socket_id' ) self . logger . info ( 'Succesfully connected on socket %s' , self . socket_id ) waiter . set_result ( self . socket_id ) elif event == PUSHER_SUBSCRIBED : self . logger . info ( 'Succesfully subscribed to %s' , encoded . get ( 'channel' ) ) elif channel : self [ channel ] . _event ( event , data ) except Exception as exc : if waiter : waiter . set_exception ( exc ) else : self . logger . exception ( 'pusher error' )
6024	def convolve ( self , array ) : if self . shape [ 0 ] % 2 == 0 or self . shape [ 1 ] % 2 == 0 : raise exc . KernelException ( "PSF Kernel must be odd" ) return scipy . signal . convolve2d ( array , self , mode = 'same' )
6602	def collect_result ( self , package_index ) : result_fullpath = self . result_fullpath ( package_index ) try : with gzip . open ( result_fullpath , 'rb' ) as f : result = pickle . load ( f ) except Exception as e : logger = logging . getLogger ( __name__ ) logger . warning ( e ) return None return result
7299	def get_qset ( self , queryset , q ) : if self . mongoadmin . search_fields and q : params = { } for field in self . mongoadmin . search_fields : if field == 'id' : if is_valid_object_id ( q ) : return queryset . filter ( pk = q ) continue search_key = "{field}__icontains" . format ( field = field ) params [ search_key ] = q queryset = queryset . filter ( ** params ) return queryset
8802	def do_notify ( context , event_type , payload ) : LOG . debug ( 'IP_BILL: notifying {}' . format ( payload ) ) notifier = n_rpc . get_notifier ( 'network' ) notifier . info ( context , event_type , payload )
3523	def intercom_user_hash ( data ) : if getattr ( settings , 'INTERCOM_HMAC_SECRET_KEY' , None ) : return hmac . new ( key = _hashable_bytes ( settings . INTERCOM_HMAC_SECRET_KEY ) , msg = _hashable_bytes ( data ) , digestmod = hashlib . sha256 , ) . hexdigest ( ) else : return None
5486	def jsonify_status_code ( status_code , * args , ** kw ) : is_batch = kw . pop ( 'is_batch' , False ) if is_batch : response = flask_make_response ( json . dumps ( * args , ** kw ) ) response . mimetype = 'application/json' response . status_code = status_code return response response = jsonify ( * args , ** kw ) response . status_code = status_code return response
12266	def docstring ( docstr ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : return func ( * args , ** kwargs ) wrapper . __doc__ = docstr return wrapper return decorator
2629	def scale_out ( self , blocks = 1 , block_size = 1 ) : self . config [ 'sites.jetstream.{0}' . format ( self . pool ) ] [ 'flavor' ] count = 0 if blocks == 1 : block_id = len ( self . blocks ) self . blocks [ block_id ] = [ ] for instance_id in range ( 0 , block_size ) : instances = self . server_manager . create ( 'parsl-{0}-{1}' . format ( block_id , instance_id ) , self . client . images . get ( '87e08a17-eae2-4ce4-9051-c561d9a54bde' ) , self . client . flavors . list ( ) [ 0 ] , min_count = 1 , max_count = 1 , userdata = setup_script . format ( engine_config = self . engine_config ) , key_name = 'TG-MCB090174-api-key' , security_groups = [ 'global-ssh' ] , nics = [ { "net-id" : '724a50cf-7f11-4b3b-a884-cd7e6850e39e' , "net-name" : 'PARSL-priv-net' , "v4-fixed-ip" : '' } ] ) self . blocks [ block_id ] . extend ( [ instances ] ) count += 1 return count
8200	def set_bot ( self , bot ) : self . bot = bot self . sink . set_bot ( bot )
2187	def ensure ( self , func , * args , ** kwargs ) : r data = self . tryload ( ) if data is None : data = func ( * args , ** kwargs ) self . save ( data ) return data
13656	def routedResource ( f , routerAttribute = 'router' ) : return wraps ( f ) ( lambda * a , ** kw : getattr ( f ( * a , ** kw ) , routerAttribute ) . resource ( ) )
4709	def power_btn ( self , interval = 200 ) : if self . __power_btn_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_BTN" ) return 1 return self . __press ( self . __power_btn_port , interval = interval )
12923	def start_tag ( self ) : direct_attributes = ( attribute . render ( self ) for attribute in self . render_attributes ) attributes = ( ) if hasattr ( self , '_attributes' ) : attributes = ( '{0}="{1}"' . format ( key , value ) for key , value in self . attributes . items ( ) if value ) rendered_attributes = " " . join ( filter ( bool , chain ( direct_attributes , attributes ) ) ) return '<{0}{1}{2}{3}>' . format ( self . tag , ' ' if rendered_attributes else '' , rendered_attributes , ' /' if self . tag_self_closes else "" )
2105	def version ( ) : click . echo ( 'Tower CLI %s' % __version__ ) click . echo ( 'API %s' % CUR_API_VERSION ) try : r = client . get ( '/config/' ) except RequestException as ex : raise exc . TowerCLIError ( 'Could not connect to Ansible Tower.\n%s' % six . text_type ( ex ) ) config = r . json ( ) license = config . get ( 'license_info' , { } ) . get ( 'license_type' , 'open' ) if license == 'open' : server_type = 'AWX' else : server_type = 'Ansible Tower' click . echo ( '%s %s' % ( server_type , config [ 'version' ] ) ) click . echo ( 'Ansible %s' % config [ 'ansible_version' ] )
5889	def smart_unicode ( string , encoding = 'utf-8' , strings_only = False , errors = 'strict' ) : return force_unicode ( string , encoding , strings_only , errors )
5085	def has_implicit_access_to_dashboard ( user , obj ) : request = get_request_or_stub ( ) decoded_jwt = get_decoded_jwt_from_request ( request ) return request_user_has_implicit_access_via_jwt ( decoded_jwt , ENTERPRISE_DASHBOARD_ADMIN_ROLE )
2368	def type ( self ) : robot_tables = [ table for table in self . tables if not isinstance ( table , UnknownTable ) ] if len ( robot_tables ) == 0 : return None for table in self . tables : if isinstance ( table , TestcaseTable ) : return "suite" return "resource"
2880	def get_event_definition ( self ) : messageEventDefinition = first ( self . xpath ( './/bpmn:messageEventDefinition' ) ) if messageEventDefinition is not None : return self . get_message_event_definition ( messageEventDefinition ) timerEventDefinition = first ( self . xpath ( './/bpmn:timerEventDefinition' ) ) if timerEventDefinition is not None : return self . get_timer_event_definition ( timerEventDefinition ) raise NotImplementedError ( 'Unsupported Intermediate Catch Event: %r' , ET . tostring ( self . node ) )
8576	def get_request ( self , request_id , status = False ) : if status : response = self . _perform_request ( '/requests/' + request_id + '/status' ) else : response = self . _perform_request ( '/requests/%s' % request_id ) return response
4947	def send_course_completion_statement ( lrs_configuration , user , course_overview , course_grade ) : user_details = LearnerInfoSerializer ( user ) course_details = CourseInfoSerializer ( course_overview ) statement = LearnerCourseCompletionStatement ( user , course_overview , user_details . data , course_details . data , course_grade , ) EnterpriseXAPIClient ( lrs_configuration ) . save_statement ( statement )
10135	def dump ( grids , mode = MODE_ZINC ) : if isinstance ( grids , Grid ) : return dump_grid ( grids , mode = mode ) _dump = functools . partial ( dump_grid , mode = mode ) if mode == MODE_ZINC : return '\n' . join ( map ( _dump , grids ) ) elif mode == MODE_JSON : return '[%s]' % ',' . join ( map ( _dump , grids ) ) else : raise NotImplementedError ( 'Format not implemented: %s' % mode )
1486	def _modules_to_main ( modList ) : if not modList : return main = sys . modules [ '__main__' ] for modname in modList : if isinstance ( modname , str ) : try : mod = __import__ ( modname ) except Exception : sys . stderr . write ( 'warning: could not import %s\n. ' 'Your function may unexpectedly error due to this import failing;' 'A version mismatch is likely. Specific error was:\n' % modname ) print_exec ( sys . stderr ) else : setattr ( main , mod . __name__ , mod )
4774	def contains_sequence ( self , * items ) : if len ( items ) == 0 : raise ValueError ( 'one or more args must be given' ) else : try : for i in xrange ( len ( self . val ) - len ( items ) + 1 ) : for j in xrange ( len ( items ) ) : if self . val [ i + j ] != items [ j ] : break else : return self except TypeError : raise TypeError ( 'val is not iterable' ) self . _err ( 'Expected <%s> to contain sequence %s, but did not.' % ( self . val , self . _fmt_items ( items ) ) )
10022	def update_environment ( self , environment_name , description = None , option_settings = [ ] , tier_type = None , tier_name = None , tier_version = '1.0' ) : out ( "Updating environment: " + str ( environment_name ) ) messages = self . ebs . validate_configuration_settings ( self . app_name , option_settings , environment_name = environment_name ) messages = messages [ 'ValidateConfigurationSettingsResponse' ] [ 'ValidateConfigurationSettingsResult' ] [ 'Messages' ] ok = True for message in messages : if message [ 'Severity' ] == 'error' : ok = False out ( "[" + message [ 'Severity' ] + "] " + str ( environment_name ) + " - '" + message [ 'Namespace' ] + ":" + message [ 'OptionName' ] + "': " + message [ 'Message' ] ) self . ebs . update_environment ( environment_name = environment_name , description = description , option_settings = option_settings , tier_type = tier_type , tier_name = tier_name , tier_version = tier_version )
12600	def concat_sheets ( xl_path : str , sheetnames = None , add_tab_names = False ) : xl_path , choice = _check_xl_path ( xl_path ) if sheetnames is None : sheetnames = get_sheet_list ( xl_path ) sheets = pd . read_excel ( xl_path , sheetname = sheetnames ) if add_tab_names : for tab in sheets : sheets [ tab ] [ 'Tab' ] = [ tab ] * len ( sheets [ tab ] ) return pd . concat ( [ sheets [ tab ] for tab in sheets ] )
13401	def removeLogbook ( self , menu = None ) : if self . logMenuCount > 1 and menu is not None : menu . removeMenu ( ) self . logMenus . remove ( menu ) self . logMenuCount -= 1
3811	def get_request_header ( self ) : if self . _client_id is not None : self . _request_header . client_identifier . resource = self . _client_id return self . _request_header
8994	def relative_folder ( self , module , folder ) : folder = self . _relative_to_absolute ( module , folder ) return self . folder ( folder )
8573	def delete_nic ( self , datacenter_id , server_id , nic_id ) : response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s' % ( datacenter_id , server_id , nic_id ) , method = 'DELETE' ) return response
7571	def revcomp ( sequence ) : "returns reverse complement of a string" sequence = sequence [ : : - 1 ] . strip ( ) . replace ( "A" , "t" ) . replace ( "T" , "a" ) . replace ( "C" , "g" ) . replace ( "G" , "c" ) . upper ( ) return sequence
13229	def create_jwt ( integration_id , private_key_path ) : integration_id = int ( integration_id ) with open ( private_key_path , 'rb' ) as f : cert_bytes = f . read ( ) now = datetime . datetime . now ( ) expiration_time = now + datetime . timedelta ( minutes = 9 ) payload = { 'iat' : int ( now . timestamp ( ) ) , 'exp' : int ( expiration_time . timestamp ( ) ) , 'iss' : integration_id } return jwt . encode ( payload , cert_bytes , algorithm = 'RS256' )
9995	def del_attr ( self , name ) : if name in self . namespace : if name in self . cells : self . del_cells ( name ) elif name in self . spaces : self . del_space ( name ) elif name in self . refs : self . del_ref ( name ) else : raise RuntimeError ( "Must not happen" ) else : raise KeyError ( "'%s' not found in Space '%s'" % ( name , self . name ) )
5849	def list_files ( self , dataset_id , glob = "." , is_dir = False ) : data = { "list" : { "glob" : glob , "isDir" : is_dir } } return self . _get_success_json ( self . _post_json ( routes . list_files ( dataset_id ) , data , failure_message = "Failed to list files for dataset {}" . format ( dataset_id ) ) ) [ 'files' ]
9641	def _display_details ( var_data ) : meta_keys = ( key for key in list ( var_data . keys ( ) ) if key . startswith ( 'META_' ) ) for key in meta_keys : display_key = key [ 5 : ] . capitalize ( ) pprint ( '{0}: {1}' . format ( display_key , var_data . pop ( key ) ) ) pprint ( var_data )
1359	def get_argument_instance ( self ) : try : instance = self . get_argument ( constants . PARAM_INSTANCE ) return instance except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
2735	def load ( self ) : data = self . get_data ( 'floating_ips/%s' % self . ip , type = GET ) floating_ip = data [ 'floating_ip' ] for attr in floating_ip . keys ( ) : setattr ( self , attr , floating_ip [ attr ] ) return self
1841	def JNO ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC )
13541	def create ( self , server ) : if len ( self . geometries ) == 0 : raise Exception ( 'no geometries' ) return server . post ( 'task_admin' , self . as_payload ( ) , replacements = { 'slug' : self . __challenge__ . slug , 'identifier' : self . identifier } )
13759	def _create_api_uri ( self , * parts ) : return urljoin ( self . API_URI , '/' . join ( map ( quote , parts ) ) )
4985	def get ( self , request , enterprise_uuid , program_uuid ) : verify_edx_resources ( ) enterprise_customer = get_enterprise_customer_or_404 ( enterprise_uuid ) context_data = get_global_context ( request , enterprise_customer ) program_details , error_code = self . get_program_details ( request , program_uuid , enterprise_customer ) if error_code : return render ( request , ENTERPRISE_GENERAL_ERROR_PAGE , context = context_data , status = 404 , ) if program_details [ 'certificate_eligible_for_program' ] : return redirect ( LMS_PROGRAMS_DASHBOARD_URL . format ( uuid = program_uuid ) ) course_run_ids = [ ] for course in program_details [ 'courses' ] : for course_run in course [ 'course_runs' ] : course_run_ids . append ( course_run [ 'key' ] ) embargo_url = EmbargoApiClient . redirect_if_blocked ( course_run_ids , request . user , get_ip ( request ) , request . path ) if embargo_url : return redirect ( embargo_url ) return self . get_enterprise_program_enrollment_page ( request , enterprise_customer , program_details )
8756	def delete_tenant_quota ( context , tenant_id ) : tenant_quotas = context . session . query ( Quota ) tenant_quotas = tenant_quotas . filter_by ( tenant_id = tenant_id ) tenant_quotas . delete ( )
10415	def function_namespace_inclusion_builder ( func : str , namespace : Strings ) -> NodePredicate : if isinstance ( namespace , str ) : def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] == namespace elif isinstance ( namespace , Iterable ) : namespaces = set ( namespace ) def function_namespaces_filter ( _ : BELGraph , node : BaseEntity ) -> bool : if func != node [ FUNCTION ] : return False return NAMESPACE in node and node [ NAMESPACE ] in namespaces else : raise ValueError ( 'Invalid type for argument: {}' . format ( namespace ) ) return function_namespaces_filter
8692	def init ( self ) : self . es . indices . create ( index = self . params [ 'index' ] , ignore = 400 )
11786	def sanitize ( self , example ) : "Return a copy of example, with non-input attributes replaced by None." return [ attr_i if i in self . inputs else None for i , attr_i in enumerate ( example ) ]
10368	def find_activations ( graph : BELGraph ) : for u , v , key , data in graph . edges ( keys = True , data = True ) : if u != v : continue bel = graph . edge_to_bel ( u , v , data ) line = data . get ( LINE ) if line is None : continue elif has_protein_modification_increases_activity ( graph , u , v , key ) : print ( line , '- pmod changes -' , bel ) find_related ( graph , v , data ) elif has_degradation_increases_activity ( data ) : print ( line , '- degradation changes -' , bel ) find_related ( graph , v , data ) elif has_translocation_increases_activity ( data ) : print ( line , '- translocation changes -' , bel ) find_related ( graph , v , data ) elif complex_increases_activity ( graph , u , v , key ) : print ( line , '- complex changes - ' , bel ) find_related ( graph , v , data ) elif has_same_subject_object ( graph , u , v , key ) : print ( line , '- same sub/obj -' , bel ) else : print ( line , '- *** - ' , bel )
7783	def _deactivate ( self ) : self . cache . remove_fetcher ( self ) if self . active : self . _deactivated ( )
6971	def _epd_function ( coeffs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ) : return ( coeffs [ 0 ] * fsv * fsv + coeffs [ 1 ] * fsv + coeffs [ 2 ] * fdv * fdv + coeffs [ 3 ] * fdv + coeffs [ 4 ] * fkv * fkv + coeffs [ 5 ] * fkv + coeffs [ 6 ] + coeffs [ 7 ] * fsv * fdv + coeffs [ 8 ] * fsv * fkv + coeffs [ 9 ] * fdv * fkv + coeffs [ 10 ] * np . sin ( 2 * pi_value * xcc ) + coeffs [ 11 ] * np . cos ( 2 * pi_value * xcc ) + coeffs [ 12 ] * np . sin ( 2 * pi_value * ycc ) + coeffs [ 13 ] * np . cos ( 2 * pi_value * ycc ) + coeffs [ 14 ] * np . sin ( 4 * pi_value * xcc ) + coeffs [ 15 ] * np . cos ( 4 * pi_value * xcc ) + coeffs [ 16 ] * np . sin ( 4 * pi_value * ycc ) + coeffs [ 17 ] * np . cos ( 4 * pi_value * ycc ) + coeffs [ 18 ] * bgv + coeffs [ 19 ] * bge + coeffs [ 20 ] * iha + coeffs [ 21 ] * izd )
5051	def commit ( self ) : if self . _child_consents : consents = [ ] for consent in self . _child_consents : consent . granted = self . granted consents . append ( consent . save ( ) or consent ) return ProxyDataSharingConsent . from_children ( self . program_uuid , * consents ) consent , _ = DataSharingConsent . objects . update_or_create ( enterprise_customer = self . enterprise_customer , username = self . username , course_id = self . course_id , defaults = { 'granted' : self . granted } ) self . _exists = consent . exists return consent
2292	def orient_undirected_graph ( self , data , umg , alg = 'HC' ) : warnings . warn ( "The pairwise GNN model is computed on each edge of the UMG " "to initialize the model and start CGNN with a DAG" ) gnn = GNN ( nh = self . nh , lr = self . lr ) og = gnn . orient_graph ( data , umg , nb_runs = self . nb_runs , nb_max_runs = self . nb_runs , nb_jobs = self . nb_jobs , train_epochs = self . train_epochs , test_epochs = self . test_epochs , verbose = self . verbose , gpu = self . gpu ) dag = dagify_min_edge ( og ) return self . orient_directed_graph ( data , dag , alg = alg )
6710	def shell ( self , gui = 0 , command = '' , dryrun = None , shell_interactive_cmd_str = None ) : from burlap . common import get_hosts_for_site if dryrun is not None : self . dryrun = dryrun r = self . local_renderer if r . genv . SITE != r . genv . default_site : shell_hosts = get_hosts_for_site ( ) if shell_hosts : r . genv . host_string = shell_hosts [ 0 ] r . env . SITE = r . genv . SITE or r . genv . default_site if int ( gui ) : r . env . shell_default_options . append ( '-X' ) if 'host_string' not in self . genv or not self . genv . host_string : if 'available_sites' in self . genv and r . env . SITE not in r . genv . available_sites : raise Exception ( 'No host_string set. Unknown site %s.' % r . env . SITE ) else : raise Exception ( 'No host_string set.' ) if '@' in r . genv . host_string : r . env . shell_host_string = r . genv . host_string else : r . env . shell_host_string = '{user}@{host_string}' if command : r . env . shell_interactive_cmd_str = command else : r . env . shell_interactive_cmd_str = r . format ( shell_interactive_cmd_str or r . env . shell_interactive_cmd ) r . env . shell_default_options_str = ' ' . join ( r . env . shell_default_options ) if self . is_local : self . vprint ( 'Using direct local.' ) cmd = '{shell_interactive_cmd_str}' elif r . genv . key_filename : self . vprint ( 'Using key filename.' ) port = r . env . shell_host_string . split ( ':' ) [ - 1 ] if port . isdigit ( ) : r . env . shell_host_string = r . env . shell_host_string . split ( ':' ) [ 0 ] + ( ' -p %s' % port ) cmd = 'ssh -t {shell_default_options_str} -i {key_filename} {shell_host_string} "{shell_interactive_cmd_str}"' elif r . genv . password : self . vprint ( 'Using password.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' else : self . vprint ( 'Using nothing.' ) cmd = 'ssh -t {shell_default_options_str} {shell_host_string} "{shell_interactive_cmd_str}"' r . local ( cmd )
12584	def spatialimg_to_hdfgroup ( h5group , spatial_img ) : try : h5group [ 'data' ] = spatial_img . get_data ( ) h5group [ 'affine' ] = spatial_img . get_affine ( ) if hasattr ( h5group , 'get_extra' ) : h5group [ 'extra' ] = spatial_img . get_extra ( ) hdr = spatial_img . get_header ( ) for k in list ( hdr . keys ( ) ) : h5group [ 'data' ] . attrs [ k ] = hdr [ k ] except ValueError as ve : raise Exception ( 'Error creating group ' + h5group . name ) from ve
9749	def create_body_index ( xml_string ) : xml = ET . fromstring ( xml_string ) body_to_index = { } for index , body in enumerate ( xml . findall ( "*/Body/Name" ) ) : body_to_index [ body . text . strip ( ) ] = index return body_to_index
1253	def print_state ( self ) : def tile_string ( value ) : if value > 0 : return '% 5d' % ( 2 ** value , ) return " " separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) print ( separator_line )
5839	def submit_predict_request ( self , data_view_id , candidates , prediction_source = 'scalar' , use_prior = True ) : data = { "prediction_source" : prediction_source , "use_prior" : use_prior , "candidates" : candidates } failure_message = "Configuration creation failed" post_url = 'v1/data_views/' + str ( data_view_id ) + '/predict/submit' return self . _get_success_json ( self . _post_json ( post_url , data , failure_message = failure_message ) ) [ 'data' ] [ 'uid' ]
3574	def peripheral_didUpdateValueForDescriptor_error_ ( self , peripheral , descriptor , error ) : logger . debug ( 'peripheral_didUpdateValueForDescriptor_error called' ) if error is not None : return device = device_list ( ) . get ( peripheral ) if device is not None : device . _descriptor_changed ( descriptor )
10152	def _extract_path_from_service ( self , service ) : path_obj = { } path = service . path route_name = getattr ( service , 'pyramid_route' , None ) if route_name : registry = self . pyramid_registry or get_current_registry ( ) route_intr = registry . introspector . get ( 'routes' , route_name ) if route_intr : path = route_intr [ 'pattern' ] else : msg = 'Route `{}` is not found by ' 'pyramid introspector' . format ( route_name ) raise ValueError ( msg ) for subpath_marker in ( '*subpath' , '*traverse' ) : path = path . replace ( subpath_marker , '{subpath}' ) parameters = self . parameters . from_path ( path ) if parameters : path_obj [ 'parameters' ] = parameters return path , path_obj
8659	def filter_by ( zips = _zips , ** kwargs ) : return [ z for z in zips if all ( [ k in z and z [ k ] == v for k , v in kwargs . items ( ) ] ) ]
3684	def solve ( self ) : self . check_sufficient_inputs ( ) if self . V : if self . P : self . T = self . solve_T ( self . P , self . V ) self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) self . P = R * self . T / ( self . V - self . b ) - self . a_alpha / ( self . V * self . V + self . delta * self . V + self . epsilon ) Vs = [ self . V , 1j , 1j ] else : self . a_alpha , self . da_alpha_dT , self . d2a_alpha_dT2 = self . a_alpha_and_derivatives ( self . T ) Vs = self . volume_solutions ( self . T , self . P , self . b , self . delta , self . epsilon , self . a_alpha ) self . set_from_PT ( Vs )
1028	def b64encode ( s , altchars = None ) : encoded = binascii . b2a_base64 ( s ) [ : - 1 ] if altchars is not None : return encoded . translate ( string . maketrans ( b'+/' , altchars [ : 2 ] ) ) return encoded
282	def plot_holdings ( returns , positions , legend_loc = 'best' , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . copy ( ) . drop ( 'cash' , axis = 'columns' ) df_holdings = positions . replace ( 0 , np . nan ) . count ( axis = 1 ) df_holdings_by_month = df_holdings . resample ( '1M' ) . mean ( ) df_holdings . plot ( color = 'steelblue' , alpha = 0.6 , lw = 0.5 , ax = ax , ** kwargs ) df_holdings_by_month . plot ( color = 'orangered' , lw = 2 , ax = ax , ** kwargs ) ax . axhline ( df_holdings . values . mean ( ) , color = 'steelblue' , ls = '--' , lw = 3 ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) leg = ax . legend ( [ 'Daily holdings' , 'Average daily holdings, by month' , 'Average daily holdings, overall' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_title ( 'Total holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax
5544	def clip_array_with_vector ( array , array_affine , geometries , inverted = False , clip_buffer = 0 ) : buffered_geometries = [ ] for feature in geometries : feature_geom = to_shape ( feature [ "geometry" ] ) if feature_geom . is_empty : continue if feature_geom . geom_type == "GeometryCollection" : buffered_geom = unary_union ( [ g . buffer ( clip_buffer ) for g in feature_geom ] ) else : buffered_geom = feature_geom . buffer ( clip_buffer ) if not buffered_geom . is_empty : buffered_geometries . append ( buffered_geom ) if buffered_geometries : if array . ndim == 2 : return ma . masked_array ( array , geometry_mask ( buffered_geometries , array . shape , array_affine , invert = inverted ) ) elif array . ndim == 3 : mask = geometry_mask ( buffered_geometries , ( array . shape [ 1 ] , array . shape [ 2 ] ) , array_affine , invert = inverted ) return ma . masked_array ( array , mask = np . stack ( ( mask for band in array ) ) ) else : fill = False if inverted else True return ma . masked_array ( array , mask = np . full ( array . shape , fill , dtype = bool ) )
5300	def with_setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : colorful = Colorful ( colormode = self . colorful . colormode , colorpalette = copy . copy ( self . colorful . colorpalette ) ) colorful . setup ( colormode = colormode , colorpalette = colorpalette , extend_colors = extend_colors ) yield colorful
7447	def _step6func ( self , samples , noreverse , force , randomseed , ipyclient , ** kwargs ) : samples = _get_samples ( self , samples ) csamples = self . _samples_precheck ( samples , 6 , force ) if self . _headers : print ( "\n Step 6: Clustering at {} similarity across {} samples" . format ( self . paramsdict [ "clust_threshold" ] , len ( csamples ) ) ) if not csamples : raise IPyradError ( FIRST_RUN_5 ) elif not force : if all ( [ i . stats . state >= 6 for i in csamples ] ) : print ( DATABASE_EXISTS . format ( len ( samples ) ) ) return assemble . cluster_across . run ( self , csamples , noreverse , force , randomseed , ipyclient , ** kwargs )
2693	def filter_quotes ( text , is_email = True ) : global DEBUG global PAT_FORWARD , PAT_REPLIED , PAT_UNSUBSC if is_email : text = filter ( lambda x : x in string . printable , text ) if DEBUG : print ( "text:" , text ) m = PAT_FORWARD . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT_REPLIED . split ( text , re . M ) if m and len ( m ) > 1 : text = m [ 0 ] m = PAT_UNSUBSC . split ( text , re . M ) if m : text = m [ 0 ] lines = [ ] for line in text . split ( "\n" ) : if line . startswith ( ">" ) : lines . append ( "" ) else : lines . append ( line ) return list ( split_grafs ( lines ) )
7158	def add ( self , * args , ** kwargs ) : if 'question' in kwargs and isinstance ( kwargs [ 'question' ] , Question ) : question = kwargs [ 'question' ] else : question = Question ( * args , ** kwargs ) self . questions . setdefault ( question . key , [ ] ) . append ( question ) return question
12457	def iteritems ( data , ** kwargs ) : return iter ( data . items ( ** kwargs ) ) if IS_PY3 else data . iteritems ( ** kwargs )
8144	def flip ( self , axis = HORIZONTAL ) : if axis == HORIZONTAL : self . img = self . img . transpose ( Image . FLIP_LEFT_RIGHT ) if axis == VERTICAL : self . img = self . img . transpose ( Image . FLIP_TOP_BOTTOM )
4041	def _retrieve_data ( self , request = None ) : full_url = "%s%s" % ( self . endpoint , request ) self . self_link = request self . request = requests . get ( url = full_url , headers = self . default_headers ( ) ) self . request . encoding = "utf-8" try : self . request . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( self . request ) return self . request
2310	def predict_proba ( self , a , b , ** kwargs ) : return self . b_fit_score ( b , a ) - self . b_fit_score ( a , b )
12606	def search_unique ( table , sample , unique_fields = None ) : if unique_fields is None : unique_fields = list ( sample . keys ( ) ) query = _query_data ( sample , field_names = unique_fields , operators = '__eq__' ) items = table . search ( query ) if len ( items ) == 1 : return items [ 0 ] if len ( items ) == 0 : return None raise MoreThanOneItemError ( 'Expected to find zero or one items, but found ' '{} items.' . format ( len ( items ) ) )
3798	def setup_a_alpha_and_derivatives ( self , i , T = None ) : r self . a , self . Tc , self . S1 , self . S2 = self . ais [ i ] , self . Tcs [ i ] , self . S1s [ i ] , self . S2s [ i ]
6077	def hyper_noise_from_contributions ( self , noise_map , contributions ) : return self . noise_factor * ( noise_map * contributions ) ** self . noise_power
11783	def add_example ( self , example ) : "Add an example to the list of examples, checking it first." self . check_example ( example ) self . examples . append ( example )
9560	def _apply_assert_methods ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for a in dir ( self ) : if a . startswith ( 'assert' ) : rdict = self . _as_dict ( r ) f = getattr ( self , a ) try : f ( rdict ) except AssertionError as e : code = ASSERT_CHECK_FAILED message = MESSAGES [ ASSERT_CHECK_FAILED ] if len ( e . args ) > 0 : custom = e . args [ 0 ] if isinstance ( custom , ( list , tuple ) ) : if len ( custom ) > 0 : code = custom [ 0 ] if len ( custom ) > 1 : message = custom [ 1 ] else : code = custom p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( f . __name__ , f . __doc__ ) if context is not None : p [ 'context' ] = context yield p
7203	def get_matching_multiplex_port ( self , name ) : matching_multiplex_ports = [ self . __getattribute__ ( p ) for p in self . _portnames if name . startswith ( p ) and name != p and hasattr ( self , p ) and self . __getattribute__ ( p ) . is_multiplex ] for port in matching_multiplex_ports : return port return None
10847	def new ( self , text , shorten = None , now = None , top = None , media = None , when = None ) : url = PATHS [ 'CREATE' ] post_data = "text=%s&" % text post_data += "profile_ids[]=%s&" % self . profile_id if shorten : post_data += "shorten=%s&" % shorten if now : post_data += "now=%s&" % now if top : post_data += "top=%s&" % top if when : post_data += "scheduled_at=%s&" % str ( when ) if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) new_update = Update ( api = self . api , raw_response = response [ 'updates' ] [ 0 ] ) self . append ( new_update ) return new_update
5052	def get_course_completions ( self , enterprise_customer , days ) : return PersistentCourseGrade . objects . filter ( passed_timestamp__gt = datetime . datetime . now ( ) - datetime . timedelta ( days = days ) ) . filter ( user_id__in = enterprise_customer . enterprise_customer_users . values_list ( 'user_id' , flat = True ) )
10133	def parse_scalar ( scalar_data , version ) : try : return hs_scalar [ version ] . parseString ( scalar_data , parseAll = True ) [ 0 ] except pp . ParseException as pe : raise ZincParseException ( 'Failed to parse scalar: %s' % reformat_exception ( pe ) , scalar_data , 1 , pe . col ) except : LOG . debug ( 'Failing scalar data: %r (version %r)' , scalar_data , version )
10686	def Cp ( self , phase , T ) : if phase not in self . _phases : raise Exception ( "The phase '%s' was not found in compound '%s'." % ( phase , self . formula ) ) return self . _phases [ phase ] . Cp ( T )
2123	def disassociate_failure_node ( self , parent , child ) : return self . _disassoc ( self . _forward_rel_name ( 'failure' ) , parent , child )
7568	def comp ( seq ) : return seq . replace ( "A" , 't' ) . replace ( 'T' , 'a' ) . replace ( 'C' , 'g' ) . replace ( 'G' , 'c' ) . replace ( 'n' , 'Z' ) . upper ( ) . replace ( "Z" , "n" )
8292	def sorted ( list , cmp = None , reversed = False ) : list = [ x for x in list ] list . sort ( cmp ) if reversed : list . reverse ( ) return list
11000	def _tz ( self , z ) : return ( z - self . param_dict [ 'psf-zslab' ] ) * self . param_dict [ self . zscale ]
984	def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , "# predicted => active cells per column for each sequence" , numCellsPerColumn )
4880	def create_switch ( apps , schema_editor ) : Switch = apps . get_model ( 'waffle' , 'Switch' ) Switch . objects . update_or_create ( name = ENTERPRISE_ROLE_BASED_ACCESS_CONTROL_SWITCH , defaults = { 'active' : False } )
9321	def _validate_api_root ( self ) : if not self . _title : msg = "No 'title' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . _versions : msg = "No 'versions' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . _max_content_length is None : msg = "No 'max_content_length' in API Root for request '{}'" raise ValidationError ( msg . format ( self . url ) )
3958	def resolve ( cls , all_known_repos , name ) : match = None for repo in all_known_repos : if repo . remote_path == name : return repo if name == repo . short_name : if match is None : match = repo else : raise RuntimeError ( 'Short repo name {} is ambiguous. It matches both {} and {}' . format ( name , match . remote_path , repo . remote_path ) ) if match is None : raise RuntimeError ( 'Short repo name {} does not match any known repos' . format ( name ) ) return match
6724	def exists ( name = None , group = None , release = None , except_release = None , verbose = 1 ) : verbose = int ( verbose ) instances = list_instances ( name = name , group = group , release = release , except_release = except_release , verbose = verbose , show = verbose ) ret = bool ( instances ) if verbose : print ( '\ninstance %s exist' % ( 'DOES' if ret else 'does NOT' ) ) return instances
11999	def _decode ( self , data , algorithm , key = None ) : if algorithm [ 'type' ] == 'hmac' : verify_signature = data [ - algorithm [ 'hash_size' ] : ] data = data [ : - algorithm [ 'hash_size' ] ] signature = self . _hmac_generate ( data , algorithm , key ) if not const_equal ( verify_signature , signature ) : raise Exception ( 'Invalid signature' ) return data elif algorithm [ 'type' ] == 'aes' : return self . _aes_decrypt ( data , algorithm , key ) elif algorithm [ 'type' ] == 'no-serialization' : return data elif algorithm [ 'type' ] == 'json' : return json . loads ( data ) elif algorithm [ 'type' ] == 'no-compression' : return data elif algorithm [ 'type' ] == 'gzip' : return self . _zlib_decompress ( data , algorithm ) else : raise Exception ( 'Algorithm not supported: %s' % algorithm [ 'type' ] )
1056	def _slotnames ( cls ) : names = cls . __dict__ . get ( "__slotnames__" ) if names is not None : return names names = [ ] if not hasattr ( cls , "__slots__" ) : pass else : for c in cls . __mro__ : if "__slots__" in c . __dict__ : slots = c . __dict__ [ '__slots__' ] if isinstance ( slots , basestring ) : slots = ( slots , ) for name in slots : if name in ( "__dict__" , "__weakref__" ) : continue elif name . startswith ( '__' ) and not name . endswith ( '__' ) : names . append ( '_%s%s' % ( c . __name__ , name ) ) else : names . append ( name ) try : cls . __slotnames__ = names except : pass return names
4209	def pascal ( n ) : errors . is_positive_integer ( n ) result = numpy . zeros ( ( n , n ) ) for i in range ( 0 , n ) : result [ i , 0 ] = 1 result [ 0 , i ] = 1 if n > 1 : for i in range ( 1 , n ) : for j in range ( 1 , n ) : result [ i , j ] = result [ i - 1 , j ] + result [ i , j - 1 ] return result
11605	def convert_ranges ( cls , ranges , length ) : result = [ ] for start , end in ranges : if end is None : result . append ( ( start , length - 1 ) ) elif start is None : s = length - end result . append ( ( 0 if s < 0 else s , length - 1 ) ) else : result . append ( ( start , end if end < length else length - 1 ) ) return result
210	def pad_to_aspect_ratio ( self , aspect_ratio , mode = "constant" , cval = 0.0 , return_pad_amounts = False ) : arr_0to1_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr_0to1 , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) heatmaps = HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) if return_pad_amounts : return heatmaps , pad_amounts else : return heatmaps
1159	def wait ( self , timeout = None ) : if not self . _is_owned ( ) : raise RuntimeError ( "cannot wait on un-acquired lock" ) waiter = _allocate_lock ( ) waiter . acquire ( ) self . __waiters . append ( waiter ) saved_state = self . _release_save ( ) try : if timeout is None : waiter . acquire ( ) if __debug__ : self . _note ( "%s.wait(): got it" , self ) else : endtime = _time ( ) + timeout delay = 0.0005 while True : gotit = waiter . acquire ( 0 ) if gotit : break remaining = endtime - _time ( ) if remaining <= 0 : break delay = min ( delay * 2 , remaining , .05 ) _sleep ( delay ) if not gotit : if __debug__ : self . _note ( "%s.wait(%s): timed out" , self , timeout ) try : self . __waiters . remove ( waiter ) except ValueError : pass else : if __debug__ : self . _note ( "%s.wait(%s): got it" , self , timeout ) finally : self . _acquire_restore ( saved_state )
5034	def get_enterprise_customer_user_queryset ( self , request , search_keyword , customer_uuid , page_size = PAGE_SIZE ) : page = request . GET . get ( 'page' , 1 ) learners = EnterpriseCustomerUser . objects . filter ( enterprise_customer__uuid = customer_uuid ) user_ids = learners . values_list ( 'user_id' , flat = True ) matching_users = User . objects . filter ( pk__in = user_ids ) if search_keyword is not None : matching_users = matching_users . filter ( Q ( email__icontains = search_keyword ) | Q ( username__icontains = search_keyword ) ) matching_user_ids = matching_users . values_list ( 'pk' , flat = True ) learners = learners . filter ( user_id__in = matching_user_ids ) return paginated_list ( learners , page , page_size )
3598	def reviews ( self , packageName , filterByDevice = False , sort = 2 , nb_results = None , offset = None ) : path = REVIEWS_URL + "?doc={}&sort={}" . format ( requests . utils . quote ( packageName ) , sort ) if nb_results is not None : path += "&n={}" . format ( nb_results ) if offset is not None : path += "&o={}" . format ( offset ) if filterByDevice : path += "&dfil=1" data = self . executeRequestApi2 ( path ) output = [ ] for review in data . payload . reviewResponse . getResponse . review : output . append ( utils . parseProtobufObj ( review ) ) return output
7770	def base_handlers_factory ( self ) : tls_handler = StreamTLSHandler ( self . settings ) sasl_handler = StreamSASLHandler ( self . settings ) session_handler = SessionHandler ( ) binding_handler = ResourceBindingHandler ( self . settings ) return [ tls_handler , sasl_handler , binding_handler , session_handler ]
1984	def load_value ( self , key , binary = False ) : with self . load_stream ( key , binary = binary ) as s : return s . read ( )
9484	def validate_content ( * objs ) : from . main import Collection , Module validator = { Collection : cnxml . validate_collxml , Module : cnxml . validate_cnxml , } [ type ( objs [ 0 ] ) ] return validator ( * [ obj . file for obj in objs ] )
9182	def validate_model ( cursor , model ) : _validate_license ( model ) _validate_roles ( model ) required_metadata = ( 'title' , 'summary' , ) for metadata_key in required_metadata : if model . metadata . get ( metadata_key ) in [ None , '' , [ ] ] : raise exceptions . MissingRequiredMetadata ( metadata_key ) _validate_derived_from ( cursor , model ) _validate_subjects ( cursor , model )
3154	def get ( self , list_id , segment_id ) : return self . _mc_client . _get ( url = self . _build_path ( list_id , 'segments' , segment_id ) )
8096	def edge_label ( s , edge , alpha = 1.0 ) : if s . text and edge . label != "" : s . _ctx . nostroke ( ) s . _ctx . fill ( s . text . r , s . text . g , s . text . b , s . text . a * alpha * 0.75 ) s . _ctx . lineheight ( 1 ) s . _ctx . font ( s . font ) s . _ctx . fontsize ( s . fontsize * 0.75 ) try : p = edge . _textpath except : try : txt = unicode ( edge . label ) except : try : txt = edge . label . decode ( "utf-8" ) except : pass edge . _textpath = s . _ctx . textpath ( txt , s . _ctx . textwidth ( " " ) , 0 , width = s . textwidth ) p = edge . _textpath a = degrees ( atan2 ( edge . node2 . y - edge . node1 . y , edge . node2 . x - edge . node1 . x ) ) d = sqrt ( ( edge . node2 . x - edge . node1 . x ) ** 2 + ( edge . node2 . y - edge . node1 . y ) ** 2 ) d = abs ( d - s . _ctx . textwidth ( edge . label ) ) * 0.5 s . _ctx . push ( ) s . _ctx . transform ( CORNER ) s . _ctx . translate ( edge . node1 . x , edge . node1 . y ) s . _ctx . rotate ( - a ) s . _ctx . translate ( d , s . fontsize * 1.0 ) s . _ctx . scale ( alpha ) if 90 < a % 360 < 270 : s . _ctx . translate ( s . _ctx . textwidth ( edge . label ) , - s . fontsize * 2.0 ) s . _ctx . transform ( CENTER ) s . _ctx . rotate ( 180 ) s . _ctx . transform ( CORNER ) s . _ctx . drawpath ( p . copy ( ) ) s . _ctx . pop ( )
2727	def get_actions ( self ) : answer = self . get_data ( "droplets/%s/actions/" % self . id , type = GET ) actions = [ ] for action_dict in answer [ 'actions' ] : action = Action ( ** action_dict ) action . token = self . token action . droplet_id = self . id action . load ( ) actions . append ( action ) return actions
134	def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( "Polygon must be made up of at least 3 points to extract its area from an image." ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask
11246	def future_value ( present_value , annual_rate , periods_per_year , years ) : rate_per_period = annual_rate / float ( periods_per_year ) periods = periods_per_year * years return present_value * ( 1 + rate_per_period ) ** periods
10958	def update_from_model_change ( self , oldmodel , newmodel , tile ) : self . _loglikelihood -= self . _calc_loglikelihood ( oldmodel , tile = tile ) self . _loglikelihood += self . _calc_loglikelihood ( newmodel , tile = tile ) self . _residuals [ tile . slicer ] = self . _data [ tile . slicer ] - newmodel
13632	def _handleRenderResult ( self , request , result ) : def _requestFinished ( result , cancel ) : cancel ( ) return result if not isinstance ( result , Deferred ) : result = succeed ( result ) def _whenDone ( result ) : render = getattr ( result , 'render' , lambda request : result ) renderResult = render ( request ) if renderResult != NOT_DONE_YET : request . write ( renderResult ) request . finish ( ) return result request . notifyFinish ( ) . addBoth ( _requestFinished , result . cancel ) result . addCallback ( self . _adaptToResource ) result . addCallback ( _whenDone ) result . addErrback ( request . processingFailed ) return NOT_DONE_YET
7069	def get_varfeatures ( simbasedir , mindet = 1000 , nworkers = None ) : with open ( os . path . join ( simbasedir , 'fakelcs-info.pkl' ) , 'rb' ) as infd : siminfo = pickle . load ( infd ) lcfpaths = siminfo [ 'lcfpath' ] varfeaturedir = os . path . join ( simbasedir , 'varfeatures' ) timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] timecols = siminfo [ 'timecols' ] magcols = siminfo [ 'magcols' ] errcols = siminfo [ 'errcols' ] fakelc_formatkey = 'fake-%s' % siminfo [ 'lcformat' ] lcproc . register_lcformat ( fakelc_formatkey , '*-fakelc.pkl' , timecols , magcols , errcols , 'astrobase.lcproc' , '_read_pklc' , magsarefluxes = siminfo [ 'magsarefluxes' ] ) varinfo = lcvfeatures . parallel_varfeatures ( lcfpaths , varfeaturedir , lcformat = fakelc_formatkey , mindet = mindet , nworkers = nworkers ) with open ( os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' ) , 'wb' ) as outfd : pickle . dump ( varinfo , outfd , pickle . HIGHEST_PROTOCOL ) return os . path . join ( simbasedir , 'fakelc-varfeatures.pkl' )
7840	def get_name ( self ) : var = self . xmlnode . prop ( "name" ) if not var : var = "" return var . decode ( "utf-8" )
5560	def bounds ( self ) : if self . _raw [ "bounds" ] is None : return self . process_pyramid . bounds else : return Bounds ( * _validate_bounds ( self . _raw [ "bounds" ] ) )
6355	def _language_index_from_code ( self , code , name_mode ) : if code < 1 or code > sum ( _LANG_DICT [ _ ] for _ in BMDATA [ name_mode ] [ 'languages' ] ) : return L_ANY if ( code & ( code - 1 ) ) != 0 : return L_ANY return code
82	def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
2469	def set_file_copyright ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_copytext_set : self . file_copytext_set = True if validations . validate_file_cpyright ( text ) : if isinstance ( text , string_types ) : self . file ( doc ) . copyright = str_from_text ( text ) else : self . file ( doc ) . copyright = text return True else : raise SPDXValueError ( 'File::CopyRight' ) else : raise CardinalityError ( 'File::CopyRight' ) else : raise OrderError ( 'File::CopyRight' )
1252	def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value
760	def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) numChanged += 1 skipOne = True if numChanged == numNear : break
5700	def _distribution ( gtfs , table , column ) : cur = gtfs . conn . cursor ( ) cur . execute ( 'SELECT {column}, count(*) ' 'FROM {table} GROUP BY {column} ' 'ORDER BY {column}' . format ( column = column , table = table ) ) return ' ' . join ( '%s:%s' % ( t , c ) for t , c in cur )
13256	def as_dict ( self ) : entry_dict = { } entry_dict [ 'UUID' ] = self . uuid entry_dict [ 'Creation Date' ] = self . time entry_dict [ 'Time Zone' ] = self . tz if self . tags : entry_dict [ 'Tags' ] = self . tags entry_dict [ 'Entry Text' ] = self . text entry_dict [ 'Starred' ] = self . starred entry_dict [ 'Location' ] = self . location return entry_dict
7685	def clicks ( annotation , sr = 22050 , length = None , ** kwargs ) : interval , _ = annotation . to_interval_values ( ) return filter_kwargs ( mir_eval . sonify . clicks , interval [ : , 0 ] , fs = sr , length = length , ** kwargs )
1838	def JNB ( cpu , target ) : cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC )
13125	def id_to_object ( self , line ) : user = User . get ( line , ignore = 404 ) if not user : user = User ( username = line ) user . save ( ) return user
4602	def merge ( * projects ) : result = { } for project in projects : for name , section in ( project or { } ) . items ( ) : if name not in PROJECT_SECTIONS : raise ValueError ( UNKNOWN_SECTION_ERROR % name ) if section is None : result [ name ] = type ( result [ name ] ) ( ) continue if name in NOT_MERGEABLE + SPECIAL_CASE : result [ name ] = section continue if section and not isinstance ( section , ( dict , str ) ) : cname = section . __class__ . __name__ raise ValueError ( SECTION_ISNT_DICT_ERROR % ( name , cname ) ) if name == 'animation' : adesc = load . load_if_filename ( section ) if adesc : section = adesc . get ( 'animation' , { } ) section [ 'run' ] = adesc . get ( 'run' , { } ) result_section = result . setdefault ( name , { } ) section = construct . to_type ( section ) for k , v in section . items ( ) : if v is None : result_section . pop ( k , None ) else : result_section [ k ] = v return result
875	def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )
391	def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask
10982	def locate_spheres ( image , feature_rad , dofilter = False , order = ( 3 , 3 , 3 ) , trim_edge = True , ** kwargs ) : m = models . SmoothFieldModel ( ) I = ilms . LegendrePoly2P1D ( order = order , constval = image . get_image ( ) . mean ( ) ) s = states . ImageState ( image , [ I ] , pad = 0 , mdl = m ) if dofilter : opt . do_levmarq ( s , s . params ) pos = addsub . feature_guess ( s , feature_rad , trim_edge = trim_edge , ** kwargs ) [ 0 ] return pos
9024	def write ( self , string ) : bytes_ = string . encode ( self . _encoding ) self . _file . write ( bytes_ )
12484	def get_subdict ( adict , path , sep = os . sep ) : return reduce ( adict . __class__ . get , [ p for p in op . split ( sep ) if p ] , adict )
51	def deepcopy ( self , x = None , y = None ) : x = self . x if x is None else x y = self . y if y is None else y return Keypoint ( x = x , y = y )
1658	def CheckForNonConstReference ( filename , clean_lines , linenum , nesting_state , error ) : line = clean_lines . elided [ linenum ] if '&' not in line : return if IsDerivedFunction ( clean_lines , linenum ) : return if IsOutOfLineMethodDefinition ( clean_lines , linenum ) : return if linenum > 1 : previous = None if Match ( r'\s*::(?:[\w<>]|::)+\s*&\s*\S' , line ) : previous = Search ( r'\b((?:const\s*)?(?:[\w<>]|::)+[\w<>])\s*$' , clean_lines . elided [ linenum - 1 ] ) elif Match ( r'\s*[a-zA-Z_]([\w<>]|::)+\s*&\s*\S' , line ) : previous = Search ( r'\b((?:const\s*)?(?:[\w<>]|::)+::)\s*$' , clean_lines . elided [ linenum - 1 ] ) if previous : line = previous . group ( 1 ) + line . lstrip ( ) else : endpos = line . rfind ( '>' ) if endpos > - 1 : ( _ , startline , startpos ) = ReverseCloseExpression ( clean_lines , linenum , endpos ) if startpos > - 1 and startline < linenum : line = '' for i in xrange ( startline , linenum + 1 ) : line += clean_lines . elided [ i ] . strip ( ) if ( nesting_state . previous_stack_top and not ( isinstance ( nesting_state . previous_stack_top , _ClassInfo ) or isinstance ( nesting_state . previous_stack_top , _NamespaceInfo ) ) ) : return if linenum > 0 : for i in xrange ( linenum - 1 , max ( 0 , linenum - 10 ) , - 1 ) : previous_line = clean_lines . elided [ i ] if not Search ( r'[),]\s*$' , previous_line ) : break if Match ( r'^\s*:\s+\S' , previous_line ) : return if Search ( r'\\\s*$' , line ) : return if IsInitializerList ( clean_lines , linenum ) : return whitelisted_functions = ( r'(?:[sS]wap(?:<\w:+>)?|' r'operator\s*[<>][<>]|' r'static_assert|COMPILE_ASSERT' r')\s*\(' ) if Search ( whitelisted_functions , line ) : return elif not Search ( r'\S+\([^)]*$' , line ) : for i in xrange ( 2 ) : if ( linenum > i and Search ( whitelisted_functions , clean_lines . elided [ linenum - i - 1 ] ) ) : return decls = ReplaceAll ( r'{[^}]*}' , ' ' , line ) for parameter in re . findall ( _RE_PATTERN_REF_PARAM , decls ) : if ( not Match ( _RE_PATTERN_CONST_REF_PARAM , parameter ) and not Match ( _RE_PATTERN_REF_STREAM_PARAM , parameter ) ) : error ( filename , linenum , 'runtime/references' , 2 , 'Is this a non-const reference? ' 'If so, make const or use a pointer: ' + ReplaceAll ( ' *<' , '<' , parameter ) )
8561	def get_loadbalancer ( self , datacenter_id , loadbalancer_id ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s' % ( datacenter_id , loadbalancer_id ) ) return response
1811	def SETNAE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) )
3924	def _on_return ( self , text ) : if not text : return elif text . startswith ( '/image' ) and len ( text . split ( ' ' ) ) == 2 : filename = text . split ( ' ' ) [ 1 ] image_file = open ( filename , 'rb' ) text = '' else : image_file = None text = replace_emoticons ( text ) segments = hangups . ChatMessageSegment . from_str ( text ) self . _coroutine_queue . put ( self . _handle_send_message ( self . _conversation . send_message ( segments , image_file = image_file ) ) )
12832	def validate_xml_name ( name ) : if len ( name ) == 0 : raise RuntimeError ( 'empty XML name' ) if __INVALID_NAME_CHARS & set ( name ) : raise RuntimeError ( 'XML name contains invalid character' ) if name [ 0 ] in __INVALID_NAME_START_CHARS : raise RuntimeError ( 'XML name starts with invalid character' )
2303	def create_graph_from_data ( self , data ) : self . arguments [ '{SCORE}' ] = self . scores [ self . score ] self . arguments [ '{VERBOSE}' ] = str ( self . verbose ) . upper ( ) results = self . _run_gies ( data , verbose = self . verbose ) return nx . relabel_nodes ( nx . DiGraph ( results ) , { idx : i for idx , i in enumerate ( data . columns ) } )
10085	def delete ( self , force = True , pid = None ) : pid = pid or self . pid if self [ '_deposit' ] . get ( 'pid' ) : raise PIDInvalidAction ( ) if pid : pid . delete ( ) return super ( Deposit , self ) . delete ( force = force )
8107	def dumps ( obj , skipkeys = False , ensure_ascii = True , check_circular = True , allow_nan = True , cls = None , indent = None , separators = None , encoding = 'utf-8' , default = None , ** kw ) : if ( skipkeys is False and ensure_ascii is True and check_circular is True and allow_nan is True and cls is None and indent is None and separators is None and encoding == 'utf-8' and default is None and not kw ) : return _default_encoder . encode ( obj ) if cls is None : cls = JSONEncoder return cls ( skipkeys = skipkeys , ensure_ascii = ensure_ascii , check_circular = check_circular , allow_nan = allow_nan , indent = indent , separators = separators , encoding = encoding , default = default , ** kw ) . encode ( obj )
10839	def edit ( self , text , media = None , utc = None , now = None ) : url = PATHS [ 'EDIT' ] % self . id post_data = "text=%s&" % text if now : post_data += "now=%s&" % now if utc : post_data += "utc=%s&" % utc if media : media_format = "media[%s]=%s&" for media_type , media_item in media . iteritems ( ) : post_data += media_format % ( media_type , media_item ) response = self . api . post ( url = url , data = post_data ) return Update ( api = self . api , raw_response = response [ 'update' ] )
7863	def is_certificate_valid ( stream , cert ) : try : logger . debug ( "tls_is_certificate_valid(cert = {0!r})" . format ( cert ) ) if not cert : logger . warning ( "No TLS certificate information received." ) return False if not cert . validated : logger . warning ( "TLS certificate not validated." ) return False srv_type = stream . transport . _dst_service if cert . verify_server ( stream . peer , srv_type ) : logger . debug ( " tls: certificate valid for {0!r}" . format ( stream . peer ) ) return True else : logger . debug ( " tls: certificate not valid for {0!r}" . format ( stream . peer ) ) return False except : logger . exception ( "Exception caught while checking a certificate" ) raise
3033	def credentials_from_clientsecrets_and_code ( filename , scope , code , message = None , redirect_uri = 'postmessage' , http = None , cache = None , device_uri = None ) : flow = flow_from_clientsecrets ( filename , scope , message = message , cache = cache , redirect_uri = redirect_uri , device_uri = device_uri ) credentials = flow . step2_exchange ( code , http = http ) return credentials
13844	def process_macros ( self , content : str ) -> str : def _sub ( macro ) : name = macro . group ( 'body' ) params = self . get_options ( macro . group ( 'options' ) ) return self . options [ 'macros' ] . get ( name , '' ) . format_map ( params ) return self . pattern . sub ( _sub , content )
10972	def requests ( ) : page = request . args . get ( 'page' , 1 , type = int ) per_page = request . args . get ( 'per_page' , 5 , type = int ) memberships = Membership . query_requests ( current_user , eager = True ) . all ( ) return render_template ( 'invenio_groups/pending.html' , memberships = memberships , requests = True , page = page , per_page = per_page , )
11256	def flatten ( prev , depth = sys . maxsize ) : def inner_flatten ( iterable , curr_level , max_levels ) : for i in iterable : if hasattr ( i , '__iter__' ) and curr_level < max_levels : for j in inner_flatten ( i , curr_level + 1 , max_levels ) : yield j else : yield i for d in prev : if hasattr ( d , '__iter__' ) and depth > 0 : for inner_d in inner_flatten ( d , 1 , depth ) : yield inner_d else : yield d
4240	def _make_request ( self , service , method , params = None , body = "" , need_auth = True ) : if need_auth and not self . cookie : if not self . login ( ) : return False , None headers = self . _get_headers ( service , method , need_auth ) if not body : if not params : params = "" if isinstance ( params , dict ) : _map = params params = "" for k in _map : params += "<" + k + ">" + _map [ k ] + "</" + k + ">\n" body = CALL_BODY . format ( service = SERVICE_PREFIX + service , method = method , params = params ) message = SOAP_REQUEST . format ( session_id = SESSION_ID , body = body ) try : response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) if need_auth and _is_unauthorized_response ( response ) : self . cookie = None _LOGGER . warning ( "Unauthorized response, let's login and retry..." ) if self . login ( ) : headers = self . _get_headers ( service , method , need_auth ) response = requests . post ( self . soap_url , headers = headers , data = message , timeout = 30 , verify = False ) success = _is_valid_response ( response ) if not success : _LOGGER . error ( "Invalid response" ) _LOGGER . debug ( "%s\n%s\n%s" , response . status_code , str ( response . headers ) , response . text ) return success , response except requests . exceptions . RequestException : _LOGGER . exception ( "Error talking to API" ) return False , None
13085	def set ( self , section , key , value ) : if not section in self . config : self . config . add_section ( section ) self . config . set ( section , key , value )
1555	def _add_out_streams ( self , spbl ) : if self . outputs is None : return output_map = self . _sanitize_outputs ( ) for stream_id , out_fields in output_map . items ( ) : out_stream = spbl . outputs . add ( ) out_stream . stream . CopyFrom ( self . _get_stream_id ( self . name , stream_id ) ) out_stream . schema . CopyFrom ( self . _get_stream_schema ( out_fields ) )
1962	def sys_rt_sigaction ( self , signum , act , oldact ) : return self . sys_sigaction ( signum , act , oldact )
11844	def run ( self , steps = 1000 ) : "Run the Environment for given number of time steps." for step in range ( steps ) : if self . is_done ( ) : return self . step ( )
2785	def get_object ( cls , api_token , volume_id ) : volume = cls ( token = api_token , id = volume_id ) volume . load ( ) return volume
384	def parse_darknet_ann_str_to_list ( annotations ) : r annotations = annotations . split ( "\n" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann
3131	def merge_results ( x , y ) : z = x . copy ( ) for key , value in y . items ( ) : if isinstance ( value , list ) and isinstance ( z . get ( key ) , list ) : z [ key ] += value else : z [ key ] = value return z
9122	def make_obo_getter ( data_url : str , data_path : str , * , preparsed_path : Optional [ str ] = None , ) -> Callable [ [ Optional [ str ] , bool , bool ] , MultiDiGraph ] : download_function = make_downloader ( data_url , data_path ) def get_obo ( url : Optional [ str ] = None , cache : bool = True , force_download : bool = False ) -> MultiDiGraph : if preparsed_path is not None and os . path . exists ( preparsed_path ) : return read_gpickle ( preparsed_path ) if url is None and cache : url = download_function ( force_download = force_download ) result = obonet . read_obo ( url ) if preparsed_path is not None : write_gpickle ( result , preparsed_path ) return result return get_obo
2467	def set_file_license_in_file ( self , doc , lic ) : if self . has_package ( doc ) and self . has_file ( doc ) : if validations . validate_file_lics_in_file ( lic ) : self . file ( doc ) . add_lics ( lic ) return True else : raise SPDXValueError ( 'File::LicenseInFile' ) else : raise OrderError ( 'File::LicenseInFile' )
3466	def gene_name_reaction_rule ( self ) : names = { i . id : i . name for i in self . _genes } ast = parse_gpr ( self . _gene_reaction_rule ) [ 0 ] return ast2str ( ast , names = names )
7430	def _resolveambig ( subseq ) : N = [ ] for col in subseq : rand = np . random . binomial ( 1 , 0.5 ) N . append ( [ _AMBIGS [ i ] [ rand ] for i in col ] ) return np . array ( N )
2081	def jt_aggregate ( func , is_create = False , has_pk = False ) : def helper ( kwargs , obj ) : unified_job_template = None for item in UNIFIED_JT : if kwargs . get ( item , None ) is not None : jt_id = kwargs . pop ( item ) if unified_job_template is None : unified_job_template = ( item , jt_id ) else : raise exc . UsageError ( 'More than one unified job template fields provided, ' 'please tighten your criteria.' ) if unified_job_template is not None : kwargs [ 'unified_job_template' ] = unified_job_template [ 1 ] obj . identity = tuple ( list ( obj . identity ) + [ 'unified_job_template' ] ) return '/' . join ( [ UNIFIED_JT [ unified_job_template [ 0 ] ] , str ( unified_job_template [ 1 ] ) , 'schedules/' ] ) elif is_create : raise exc . UsageError ( 'You must provide exactly one unified job' ' template field during creation.' ) def decorator_without_pk ( obj , * args , ** kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , * args , ** kwargs ) obj . endpoint = old_endpoint return result def decorator_with_pk ( obj , pk = None , * args , ** kwargs ) : old_endpoint = obj . endpoint new_endpoint = helper ( kwargs , obj ) if is_create : obj . endpoint = new_endpoint result = func ( obj , pk = pk , * args , ** kwargs ) obj . endpoint = old_endpoint return result decorator = decorator_with_pk if has_pk else decorator_without_pk for item in CLICK_ATTRS : setattr ( decorator , item , getattr ( func , item , [ ] ) ) decorator . __doc__ = func . __doc__ return decorator
6707	def run_as_root ( command , * args , ** kwargs ) : from burlap . common import run_or_dryrun , sudo_or_dryrun if env . user == 'root' : func = run_or_dryrun else : func = sudo_or_dryrun return func ( command , * args , ** kwargs )
6653	def start ( self , builddir , program , forward_args ) : child = None try : prog_path = self . findProgram ( builddir , program ) if prog_path is None : return start_env , start_vars = self . buildProgEnvAndVars ( prog_path , builddir ) if self . getScript ( 'start' ) : cmd = [ os . path . expandvars ( string . Template ( x ) . safe_substitute ( ** start_vars ) ) for x in self . getScript ( 'start' ) ] + forward_args else : cmd = shlex . split ( './' + prog_path ) + forward_args logger . debug ( 'starting program: %s' , cmd ) child = subprocess . Popen ( cmd , cwd = builddir , env = start_env ) child . wait ( ) if child . returncode : return "process exited with status %s" % child . returncode child = None except OSError as e : import errno if e . errno == errno . ENOEXEC : return ( "the program %s cannot be run (perhaps your target " + "needs to define a 'start' script to start it on its " "intended execution target?)" ) % prog_path finally : if child is not None : _tryTerminate ( child )
12499	def fwhm2sigma ( fwhm ) : fwhm = np . asarray ( fwhm ) return fwhm / np . sqrt ( 8 * np . log ( 2 ) )
13835	def _ParseOrMerge ( self , lines , message ) : tokenizer = _Tokenizer ( lines ) while not tokenizer . AtEnd ( ) : self . _MergeField ( tokenizer , message )
7583	def run ( self , ipyclient = None , quiet = False , force = False , block = False , ) : if force : for key , oldfile in self . trees : if os . path . exists ( oldfile ) : os . remove ( oldfile ) if os . path . exists ( self . trees . info ) : print ( "Error: set a new name for this job or use Force flag.\nFile exists: {}" . format ( self . trees . info ) ) return if not ipyclient : proc = _call_raxml ( self . _command_list ) self . stdout = proc [ 0 ] self . stderr = proc [ 1 ] else : lbview = ipyclient . load_balanced_view ( ) self . async = lbview . apply ( _call_raxml , self . _command_list ) if not quiet : if not ipyclient : if "Overall execution time" not in self . stdout : print ( "Error in raxml run\n" + self . stdout ) else : print ( "job {} finished successfully" . format ( self . params . n ) ) else : print ( "job {} submitted to cluster" . format ( self . params . n ) )
4657	def clear ( self ) : self . ops = [ ] self . wifs = set ( ) self . signing_accounts = [ ] self [ "expiration" ] = None dict . __init__ ( self , { } )
13520	def configure ( self , url = None , token = None , test = False ) : if url is None : url = Config . get_value ( "url" ) if token is None : token = Config . get_value ( "token" ) self . server_url = url self . auth_header = { "Authorization" : "Basic {0}" . format ( token ) } self . configured = True if test : self . test_connection ( ) Config . set ( "url" , url ) Config . set ( "token" , token )
351	def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , "wb" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = "https://docs.google.com/uc?export=download" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )
6973	def epd_magseries ( times , mags , errs , fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd , magsarefluxes = False , epdsmooth_sigclip = 3.0 , epdsmooth_windowsize = 21 , epdsmooth_func = smooth_magseries_savgol , epdsmooth_extraparams = None ) : finind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes , fmags , ferrs = times [ : : ] [ finind ] , mags [ : : ] [ finind ] , errs [ : : ] [ finind ] ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd = ( fsv [ : : ] [ finind ] , fdv [ : : ] [ finind ] , fkv [ : : ] [ finind ] , xcc [ : : ] [ finind ] , ycc [ : : ] [ finind ] , bgv [ : : ] [ finind ] , bge [ : : ] [ finind ] , iha [ : : ] [ finind ] , izd [ : : ] [ finind ] , ) stimes , smags , serrs , separams = sigclip_magseries_with_extparams ( times , mags , errs , [ fsv , fdv , fkv , xcc , ycc , bgv , bge , iha , izd ] , sigclip = epdsmooth_sigclip , magsarefluxes = magsarefluxes ) sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd = separams if isinstance ( epdsmooth_extraparams , dict ) : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize , ** epdsmooth_extraparams ) else : smoothedmags = epdsmooth_func ( smags , epdsmooth_windowsize ) initcoeffs = np . zeros ( 22 ) leastsqfit = leastsq ( _epd_residual , initcoeffs , args = ( smoothedmags , sfsv , sfdv , sfkv , sxcc , sycc , sbgv , sbge , siha , sizd ) , full_output = True ) if leastsqfit [ - 1 ] in ( 1 , 2 , 3 , 4 ) : fitcoeffs = leastsqfit [ 0 ] epdfit = _epd_function ( fitcoeffs , ffsv , ffdv , ffkv , fxcc , fycc , fbgv , fbge , fiha , fizd ) epdmags = npmedian ( fmags ) + fmags - epdfit retdict = { 'times' : ftimes , 'mags' : epdmags , 'errs' : ferrs , 'fitcoeffs' : fitcoeffs , 'fitinfo' : leastsqfit , 'fitmags' : epdfit , 'mags_median' : npmedian ( epdmags ) , 'mags_mad' : npmedian ( npabs ( epdmags - npmedian ( epdmags ) ) ) } return retdict else : LOGERROR ( 'EPD fit did not converge' ) return None
10981	def accept ( group_id ) : membership = Membership . query . get_or_404 ( ( current_user . get_id ( ) , group_id ) ) try : membership . accept ( ) except Exception as e : flash ( str ( e ) , 'error' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) ) flash ( _ ( 'You are now part of %(name)s group.' , user = membership . user . email , name = membership . group . name ) , 'success' ) return redirect ( url_for ( '.invitations' , group_id = membership . group . id ) )
6206	def _calc_hash_da ( self , rs ) : self . hash_d = hash_ ( rs . get_state ( ) ) [ : 6 ] self . hash_a = self . hash_d
2063	def is_declared ( self , expression_var ) : if not isinstance ( expression_var , Variable ) : raise ValueError ( f'Expression must be a Variable (not a {type(expression_var)})' ) return any ( expression_var is x for x in self . get_declared_variables ( ) )
768	def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )
7727	def get_items ( self ) : if not self . xmlnode . children : return [ ] ret = [ ] n = self . xmlnode . children while n : ns = n . ns ( ) if ns and ns . getContent ( ) != self . ns : pass elif n . name == "item" : ret . append ( MucItem ( n ) ) elif n . name == "status" : ret . append ( MucStatus ( n ) ) n = n . next return ret
8469	def getOSName ( self ) : _system = platform . system ( ) if _system in [ self . __class__ . OS_WINDOWS , self . __class__ . OS_MAC , self . __class__ . OS_LINUX ] : if _system == self . __class__ . OS_LINUX : _dist = platform . linux_distribution ( ) [ 0 ] if _dist . lower ( ) == self . __class__ . OS_UBUNTU . lower ( ) : return self . __class__ . OS_UBUNTU elif _dist . lower ( ) == self . __class__ . OS_DEBIAN . lower ( ) : return self . __class__ . OS_DEBIAN elif _dist . lower ( ) == self . __class__ . OS_CENTOS . lower ( ) : return self . __class__ . OS_CENTOS elif _dist . lower ( ) == self . __class__ . OS_REDHAT . lower ( ) : return self . __class__ . OS_REDHAT elif _dist . lower ( ) == self . __class__ . OS_KALI . lower ( ) : return self . __class__ . OS_KALI return _system else : return None
277	def plotting_context ( context = 'notebook' , font_scale = 1.5 , rc = None ) : if rc is None : rc = { } rc_default = { 'lines.linewidth' : 1.5 } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . plotting_context ( context = context , font_scale = font_scale , rc = rc )
13140	def read ( self ) : if not self . __content__ : self . __retriever__ = self . __resolver__ . resolve ( self . uri ) self . __content__ , self . __mimetype__ = self . __retriever__ . read ( self . uri ) return self . __content__
3857	def is_quiet ( self ) : level = self . _conversation . self_conversation_state . notification_level return level == hangouts_pb2 . NOTIFICATION_LEVEL_QUIET
7421	def fetch_cluster_se ( data , samfile , chrom , rstart , rend ) : overlap_buffer = data . _hackersonly [ "min_SE_refmap_overlap" ] rstart_buff = rstart + overlap_buffer rend_buff = rend - overlap_buffer if rstart_buff > rend_buff : tmp = rstart_buff rstart_buff = rend_buff rend_buff = tmp if rstart_buff == rend_buff : rend_buff += 1 rdict = { } clust = [ ] iterreg = [ ] iterreg = samfile . fetch ( chrom , rstart_buff , rend_buff ) for read in iterreg : if read . qname not in rdict : rdict [ read . qname ] = read sfunc = lambda x : int ( x . split ( ";size=" ) [ 1 ] . split ( ";" ) [ 0 ] ) rkeys = sorted ( rdict . keys ( ) , key = sfunc , reverse = True ) try : read1 = rdict [ rkeys [ 0 ] ] except ValueError : LOGGER . error ( "Found bad cluster, skipping - key:{} rdict:{}" . format ( rkeys [ 0 ] , rdict ) ) return "" poss = read1 . get_reference_positions ( full_length = True ) seed_r1start = min ( poss ) seed_r1end = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( rkeys [ 0 ] ) clust . append ( ">{}:{}:{};size={};*\n{}" . format ( chrom , seed_r1start , seed_r1end , size , seq ) ) if len ( rkeys ) > 1 : for key in rkeys [ 1 : ] : skip = False try : read1 = rdict [ key ] except ValueError : read1 = rdict [ key ] [ 0 ] skip = True if not skip : poss = read1 . get_reference_positions ( full_length = True ) minpos = min ( poss ) maxpos = max ( poss ) if read1 . is_reverse : seq = revcomp ( read1 . seq ) else : seq = read1 . seq size = sfunc ( key ) clust . append ( ">{}:{}:{};size={};+\n{}" . format ( chrom , minpos , maxpos , size , seq ) ) else : pass return clust
3495	def reaction_elements ( reaction ) : c_elements = [ coeff * met . elements . get ( 'C' , 0 ) for met , coeff in iteritems ( reaction . metabolites ) ] return [ elem for elem in c_elements if elem != 0 ]
924	def _aggr_mean ( inList ) : aggrSum = 0 nonNone = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem nonNone += 1 if nonNone != 0 : return aggrSum / nonNone else : return None
8140	def invert ( self ) : alpha = self . img . split ( ) [ 3 ] self . img = self . img . convert ( "RGB" ) self . img = ImageOps . invert ( self . img ) self . img = self . img . convert ( "RGBA" ) self . img . putalpha ( alpha )
132	def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( "Cannot determine whether the polygon is inside the image, because it contains no points." ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )
8704	def read_file ( self , filename , destination = '' ) : if not destination : destination = filename log . info ( 'Transferring %s to %s' , filename , destination ) data = self . download_file ( filename ) log . info ( destination ) if not os . path . exists ( os . path . dirname ( destination ) ) : try : os . makedirs ( os . path . dirname ( destination ) ) except OSError as e : if e . errno != errno . EEXIST : raise with open ( destination , 'w' ) as fil : fil . write ( data )
5928	def getpath ( self , section , option ) : return os . path . expanduser ( os . path . expandvars ( self . get ( section , option ) ) )
8568	def get_loadbalanced_nic ( self , datacenter_id , loadbalancer_id , nic_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/loadbalancers/%s/balancednics/%s?depth=%s' % ( datacenter_id , loadbalancer_id , nic_id , str ( depth ) ) ) return response
608	def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result
3041	def access_token_expired ( self ) : if self . invalid : return True if not self . token_expiry : return False now = _UTCNOW ( ) if now >= self . token_expiry : logger . info ( 'access_token is expired. Now: %s, token_expiry: %s' , now , self . token_expiry ) return True return False
8403	def squish_infinite ( x , range = ( 0 , 1 ) ) : xtype = type ( x ) if not hasattr ( x , 'dtype' ) : x = np . asarray ( x ) x [ x == - np . inf ] = range [ 0 ] x [ x == np . inf ] = range [ 1 ] if not isinstance ( x , xtype ) : x = xtype ( x ) return x
12851	def _unlock_temporarily ( self ) : if not self . _is_locked : yield else : try : self . _is_locked = False yield finally : self . _is_locked = True
10160	def ci ( ctx ) : opts = [ '' ] if os . environ . get ( 'TRAVIS' , '' ) . lower ( ) == 'true' : opts += [ 'test.pytest' ] else : opts += [ 'test.tox' ] ctx . run ( "invoke --echo --pty clean --all build --docs check --reports{}" . format ( ' ' . join ( opts ) ) )
5250	def start ( self ) : logger = _get_logger ( self . debug ) started = self . _session . start ( ) if started : ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) ev = self . _session . nextEvent ( ) ev_name = _EVENT_DICT [ ev . eventType ( ) ] logger . info ( 'Event Type: {!r}' . format ( ev_name ) ) for msg in ev : logger . info ( 'Message Received:\n{}' . format ( msg ) ) if ev . eventType ( ) != blpapi . Event . SESSION_STATUS : raise RuntimeError ( 'Expected a "SESSION_STATUS" event but ' 'received a {!r}' . format ( ev_name ) ) else : ev = self . _session . nextEvent ( self . timeout ) if ev . eventType ( ) == blpapi . Event . SESSION_STATUS : for msg in ev : logger . warning ( 'Message Received:\n{}' . format ( msg ) ) raise ConnectionError ( 'Could not start blpapi.Session' ) self . _init_services ( ) return self
12326	def init ( globalvars = None , show = False ) : global config profileini = getprofileini ( ) if os . path . exists ( profileini ) : config = configparser . ConfigParser ( ) config . read ( profileini ) mgr = plugins_get_mgr ( ) mgr . update_configs ( config ) if show : for source in config : print ( "[%s] :" % ( source ) ) for k in config [ source ] : print ( " %s : %s" % ( k , config [ source ] [ k ] ) ) else : print ( "Profile does not exist. So creating one" ) if not show : update ( globalvars ) print ( "Complete init" )
4858	def ignore_warning ( warning ) : def decorator ( func ) : @ wraps ( func ) def wrapper ( * args , ** kwargs ) : warnings . simplefilter ( 'ignore' , warning ) return func ( * args , ** kwargs ) return wrapper return decorator
5605	def write_raster_window ( in_tile = None , in_data = None , out_profile = None , out_tile = None , out_path = None , tags = None , bucket_resource = None ) : if not isinstance ( out_path , str ) : raise TypeError ( "out_path must be a string" ) logger . debug ( "write %s" , out_path ) if out_path == "memoryfile" : raise DeprecationWarning ( "Writing to memoryfile with write_raster_window() is deprecated. " "Please use RasterWindowMemoryFile." ) out_tile = in_tile if out_tile is None else out_tile _validate_write_window_params ( in_tile , out_tile , in_data , out_profile ) window_data = extract_from_array ( in_raster = in_data , in_affine = in_tile . affine , out_tile = out_tile ) if in_tile != out_tile else in_data if "affine" in out_profile : out_profile [ "transform" ] = out_profile . pop ( "affine" ) if window_data . all ( ) is not ma . masked : try : if out_path . startswith ( "s3://" ) : with RasterWindowMemoryFile ( in_tile = out_tile , in_data = window_data , out_profile = out_profile , out_tile = out_tile , tags = tags ) as memfile : logger . debug ( ( out_tile . id , "upload tile" , out_path ) ) bucket_resource . put_object ( Key = "/" . join ( out_path . split ( "/" ) [ 3 : ] ) , Body = memfile ) else : with rasterio . open ( out_path , 'w' , ** out_profile ) as dst : logger . debug ( ( out_tile . id , "write tile" , out_path ) ) dst . write ( window_data . astype ( out_profile [ "dtype" ] , copy = False ) ) _write_tags ( dst , tags ) except Exception as e : logger . exception ( "error while writing file %s: %s" , out_path , e ) raise else : logger . debug ( ( out_tile . id , "array window empty" , out_path ) )
3972	def _composed_service_dict ( service_spec ) : compose_dict = service_spec . plain_dict ( ) _apply_env_overrides ( env_overrides_for_app_or_service ( service_spec . name ) , compose_dict ) compose_dict . setdefault ( 'volumes' , [ ] ) . append ( _get_cp_volume_mount ( service_spec . name ) ) compose_dict [ 'container_name' ] = "dusty_{}_1" . format ( service_spec . name ) return compose_dict
10626	def _calculate_T ( self , Hfr ) : x = list ( ) x . append ( self . _T ) x . append ( self . _T + 10.0 ) y = list ( ) y . append ( self . _calculate_Hfr ( x [ 0 ] ) - Hfr ) y . append ( self . _calculate_Hfr ( x [ 1 ] ) - Hfr ) for i in range ( 2 , 50 ) : x . append ( x [ i - 1 ] - y [ i - 1 ] * ( ( x [ i - 1 ] - x [ i - 2 ] ) / ( y [ i - 1 ] - y [ i - 2 ] ) ) ) y . append ( self . _calculate_Hfr ( x [ i ] ) - Hfr ) if abs ( y [ i - 1 ] ) < 1.0e-5 : break return x [ len ( x ) - 1 ]
3859	def update_conversation ( self , conversation ) : new_state = conversation . self_conversation_state old_state = self . _conversation . self_conversation_state self . _conversation = conversation if not new_state . delivery_medium_option : new_state . delivery_medium_option . extend ( old_state . delivery_medium_option ) old_timestamp = old_state . self_read_state . latest_read_timestamp new_timestamp = new_state . self_read_state . latest_read_timestamp if new_timestamp == 0 : new_state . self_read_state . latest_read_timestamp = old_timestamp for new_entry in conversation . read_state : tstamp = parsers . from_timestamp ( new_entry . latest_read_timestamp ) if tstamp == 0 : continue uid = parsers . from_participantid ( new_entry . participant_id ) if uid not in self . _watermarks or self . _watermarks [ uid ] < tstamp : self . _watermarks [ uid ] = tstamp
10182	def _aggregations_process ( aggregation_types = None , start_date = None , end_date = None , update_bookmark = False , eager = False ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) if eager : aggregate_events . apply ( ( aggregation_types , ) , dict ( start_date = start_date , end_date = end_date , update_bookmark = update_bookmark ) , throw = True ) click . secho ( 'Aggregations processed successfully.' , fg = 'green' ) else : aggregate_events . delay ( aggregation_types , start_date = start_date , end_date = end_date ) click . secho ( 'Aggregations processing task sent...' , fg = 'yellow' )
13054	def nmap_smb_vulnscan ( ) : service_search = ServiceSearch ( ) services = service_search . get_services ( ports = [ '445' ] , tags = [ '!smb_vulnscan' ] , up = True ) services = [ service for service in services ] service_dict = { } for service in services : service . add_tag ( 'smb_vulnscan' ) service_dict [ str ( service . address ) ] = service nmap_args = "-Pn -n --disable-arp-ping --script smb-security-mode.nse,smb-vuln-ms17-010.nse -p 445" . split ( " " ) if services : result = nmap ( nmap_args , [ str ( s . address ) for s in services ] ) parser = NmapParser ( ) report = parser . parse_fromstring ( result ) smb_signing = 0 ms17 = 0 for nmap_host in report . hosts : for script_result in nmap_host . scripts_results : script_result = script_result . get ( 'elements' , { } ) service = service_dict [ str ( nmap_host . address ) ] if script_result . get ( 'message_signing' , '' ) == 'disabled' : print_success ( "({}) SMB Signing disabled" . format ( nmap_host . address ) ) service . add_tag ( 'smb_signing_disabled' ) smb_signing += 1 if script_result . get ( 'CVE-2017-0143' , { } ) . get ( 'state' , '' ) == 'VULNERABLE' : print_success ( "({}) Vulnerable for MS17-010" . format ( nmap_host . address ) ) service . add_tag ( 'MS17-010' ) ms17 += 1 service . update ( tags = service . tags ) print_notification ( "Completed, 'smb_signing_disabled' tag added to systems with smb signing disabled, 'MS17-010' tag added to systems that did not apply MS17-010." ) stats = { 'smb_signing' : smb_signing , 'MS17_010' : ms17 , 'scanned_services' : len ( services ) } Logger ( ) . log ( 'smb_vulnscan' , 'Scanned {} smb services for vulnerabilities' . format ( len ( services ) ) , stats ) else : print_notification ( "No services found to scan." )
983	def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )
9234	def run ( self ) : if not self . options . project or not self . options . user : print ( "Project and/or user missing. " "For help run:\n pygcgen --help" ) return if not self . options . quiet : print ( "Generating changelog..." ) log = None try : log = self . generator . compound_changelog ( ) except ChangelogGeneratorError as err : print ( "\n\033[91m\033[1m{}\x1b[0m" . format ( err . args [ 0 ] ) ) exit ( 1 ) if not log : if not self . options . quiet : print ( "Empty changelog generated. {} not written." . format ( self . options . output ) ) return if self . options . no_overwrite : out = checkname ( self . options . output ) else : out = self . options . output with codecs . open ( out , "w" , "utf-8" ) as fh : fh . write ( log ) if not self . options . quiet : print ( "Done!" ) print ( "Generated changelog written to {}" . format ( out ) )
1949	def write_back_register ( self , reg , val ) : if self . write_backs_disabled : return if issymbolic ( val ) : logger . warning ( "Skipping Symbolic write-back" ) return if reg in self . flag_registers : self . _emu . reg_write ( self . _to_unicorn_id ( 'EFLAGS' ) , self . _cpu . read_register ( 'EFLAGS' ) ) return self . _emu . reg_write ( self . _to_unicorn_id ( reg ) , val )
11661	def transform ( self , X ) : self . _check_fitted ( ) X = as_features ( X , stack = True ) assignments = self . kmeans_fit_ . predict ( X . stacked_features ) return self . _group_assignments ( X , assignments )
7265	def run ( self , ctx ) : if ctx . reverse : self . engine . reverse ( ) if self . engine . empty : raise AssertionError ( 'grappa: no assertions to run' ) try : return self . run_assertions ( ctx ) except Exception as _err : if getattr ( _err , '__legit__' , False ) : raise _err return self . render_error ( ctx , _err )
3205	def all ( self , get_all = False , ** queryparams ) : self . batch_id = None self . operation_status = None if get_all : return self . _iterate ( url = self . _build_path ( ) , ** queryparams ) else : return self . _mc_client . _get ( url = self . _build_path ( ) , ** queryparams )
3049	def _get_implicit_credentials ( cls ) : environ_checkers = [ cls . _implicit_credentials_from_files , cls . _implicit_credentials_from_gae , cls . _implicit_credentials_from_gce , ] for checker in environ_checkers : credentials = checker ( ) if credentials is not None : return credentials raise ApplicationDefaultCredentialsError ( ADC_HELP_MSG )
11350	def merge_kwargs ( self , kwargs ) : if kwargs : self . parser_kwargs . update ( kwargs ) self . parser_kwargs . setdefault ( 'dest' , self . name ) if 'default' in kwargs : self . parser_kwargs [ "default" ] = kwargs [ "default" ] self . parser_kwargs [ "required" ] = False elif 'action' in kwargs : if kwargs [ 'action' ] in set ( [ 'store_false' , 'store_true' ] ) : self . parser_kwargs [ 'required' ] = False elif kwargs [ 'action' ] in set ( [ 'version' ] ) : self . parser_kwargs . pop ( 'required' , False ) else : self . parser_kwargs . setdefault ( "required" , True )
3189	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash if 'tags' not in data : raise KeyError ( 'The list member tags must have a tag' ) response = self . _mc_client . _post ( url = self . _build_path ( list_id , 'members' , subscriber_hash , 'tags' ) , data = data ) return response
3768	def zs_to_Vfs ( zs , Vms ) : r vol_is = [ zi * Vmi for zi , Vmi in zip ( zs , Vms ) ] tot = sum ( vol_is ) return [ vol_i / tot for vol_i in vol_is ]
2583	def load ( cls , config : Optional [ Config ] = None ) : if cls . _dfk is not None : raise RuntimeError ( 'Config has already been loaded' ) if config is None : cls . _dfk = DataFlowKernel ( Config ( ) ) else : cls . _dfk = DataFlowKernel ( config ) return cls . _dfk
12762	def create_bodies ( self ) : self . bodies = { } for label in self . channels : body = self . world . create_body ( 'sphere' , name = 'marker:{}' . format ( label ) , radius = 0.02 ) body . is_kinematic = True body . color = 0.9 , 0.1 , 0.1 , 0.5 self . bodies [ label ] = body
11321	def update_languages ( self ) : language_fields = record_get_field_instances ( self . record , '041' ) language = "eng" record_delete_fields ( self . record , "041" ) for field in language_fields : subs = field_get_subfields ( field ) if 'a' in subs : language = self . get_config_item ( subs [ 'a' ] [ 0 ] , "languages" ) break new_subs = [ ( 'a' , language ) ] record_add_field ( self . record , "041" , subfields = new_subs )
8084	def scale ( self , x = 1 , y = None ) : if not y : y = x if x == 0 : x = 1 if y == 0 : y = 1 self . _canvas . scale ( x , y )
13591	def n_p ( self ) : return 2 * _sltr . GeV2joule ( self . E ) * _spc . epsilon_0 / ( self . beta * _spc . elementary_charge ) ** 2
13571	def configure ( server = None , username = None , password = None , tid = None , auto = False ) : if not server and not username and not password and not tid : if Config . has ( ) : if not yn_prompt ( "Override old configuration" , False ) : return False reset_db ( ) if not server : while True : server = input ( "Server url [https://tmc.mooc.fi/mooc/]: " ) . strip ( ) if len ( server ) == 0 : server = "https://tmc.mooc.fi/mooc/" if not server . endswith ( '/' ) : server += '/' if not ( server . startswith ( "http://" ) or server . startswith ( "https://" ) ) : ret = custom_prompt ( "Server should start with http:// or https://\n" + "R: Retry, H: Assume http://, S: Assume https://" , [ "r" , "h" , "s" ] , "r" ) if ret == "r" : continue if "://" in server : server = server . split ( "://" ) [ 1 ] if ret == "h" : server = "http://" + server elif ret == "s" : server = "https://" + server break print ( "Using URL: '{0}'" . format ( server ) ) while True : if not username : username = input ( "Username: " ) if not password : password = getpass ( "Password: " ) token = b64encode ( bytes ( "{0}:{1}" . format ( username , password ) , encoding = 'utf-8' ) ) . decode ( "utf-8" ) try : api . configure ( url = server , token = token , test = True ) except APIError as e : print ( e ) if auto is False and yn_prompt ( "Retry authentication" ) : username = password = None continue return False break if tid : select ( course = True , tid = tid , auto = auto ) else : select ( course = True )
4533	def set_color_list ( self , color_list , offset = 0 ) : if not len ( color_list ) : return color_list = make . colors ( color_list ) size = len ( self . _colors ) - offset if len ( color_list ) > size : color_list = color_list [ : size ] self . _colors [ offset : offset + len ( color_list ) ] = color_list
7156	def get_operator ( self , op ) : if op in self . OPERATORS : return self . OPERATORS . get ( op ) try : n_args = len ( inspect . getargspec ( op ) [ 0 ] ) if n_args != 2 : raise TypeError except : eprint ( 'Error: invalid operator function. Operators must accept two args.' ) raise else : return op
10693	def rgb_to_hsv ( rgb ) : r , g , b = rgb [ 0 ] / 255 , rgb [ 1 ] / 255 , rgb [ 2 ] / 255 _min = min ( r , g , b ) _max = max ( r , g , b ) v = _max delta = _max - _min if _max == 0 : return 0 , 0 , v s = delta / _max if delta == 0 : delta = 1 if r == _max : h = 60 * ( ( ( g - b ) / delta ) % 6 ) elif g == _max : h = 60 * ( ( ( b - r ) / delta ) + 2 ) else : h = 60 * ( ( ( r - g ) / delta ) + 4 ) return round ( h , 3 ) , round ( s , 3 ) , round ( v , 3 )
2700	def text_rank ( path ) : graph = build_graph ( json_iter ( path ) ) ranks = nx . pagerank ( graph ) return graph , ranks
1066	def getheader ( self , name , default = None ) : return self . dict . get ( name . lower ( ) , default )
7799	def check_password ( self , username , password , properties ) : logger . debug ( "check_password{0!r}" . format ( ( username , password , properties ) ) ) pwd , pwd_format = self . get_password ( username , ( u"plain" , u"md5:user:realm:password" ) , properties ) if pwd_format == u"plain" : logger . debug ( "got plain password: {0!r}" . format ( pwd ) ) return pwd is not None and password == pwd elif pwd_format in ( u"md5:user:realm:password" ) : logger . debug ( "got md5:user:realm:password password: {0!r}" . format ( pwd ) ) realm = properties . get ( "realm" ) if realm is None : realm = "" else : realm = realm . encode ( "utf-8" ) username = username . encode ( "utf-8" ) password = password . encode ( "utf-8" ) urp_hash = hashlib . md5 ( b"%s:%s:%s" ) . hexdigest ( ) return urp_hash == pwd logger . debug ( "got password in unknown format: {0!r}" . format ( pwd_format ) ) return False
8102	def open_socket ( self ) : self . socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) self . socket . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) self . socket . setblocking ( 0 ) self . socket . bind ( ( self . host , self . port ) )
4535	def fillRGB ( self , r , g , b , start = 0 , end = - 1 ) : self . fill ( ( r , g , b ) , start , end )
850	def setParameter ( self , parameterName , index , parameterValue ) : if parameterName == 'topDownMode' : self . topDownMode = parameterValue elif parameterName == 'predictedField' : self . predictedField = parameterValue else : raise Exception ( 'Unknown parameter: ' + parameterName )
13521	def send_zip ( self , exercise , file , params ) : resp = self . post ( exercise . return_url , params = params , files = { "submission[file]" : ( 'submission.zip' , file ) } , data = { "commit" : "Submit" } ) return self . _to_json ( resp )
2623	def shut_down_instance ( self , instances = None ) : if instances and len ( self . instances ) > 0 : print ( instances ) try : print ( [ i . id for i in instances ] ) except Exception as e : print ( e ) term = self . client . terminate_instances ( InstanceIds = instances ) logger . info ( "Shut down {} instances (ids:{}" . format ( len ( instances ) , str ( instances ) ) ) elif len ( self . instances ) > 0 : instance = self . instances . pop ( ) term = self . client . terminate_instances ( InstanceIds = [ instance ] ) logger . info ( "Shut down 1 instance (id:{})" . format ( instance ) ) else : logger . warn ( "No Instances to shut down.\n" ) return - 1 self . get_instance_state ( ) return term
2164	def list_facts ( self , pk = None , ** kwargs ) : res = self . get ( pk = pk , ** kwargs ) url = self . endpoint + '%d/%s/' % ( res [ 'id' ] , 'ansible_facts' ) return client . get ( url , params = { } ) . json ( )
3680	def T_converter ( T , current , desired ) : r def range_check ( T , Tmin , Tmax ) : if T < Tmin or T > Tmax : raise Exception ( 'Temperature conversion is outside one or both scales' ) try : if current == 'ITS-90' : pass elif current == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T68_to_T90 ( T ) elif current == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T76_to_T90 ( T ) elif current == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T48_to_T90 ( T ) elif current == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T27_to_T90 ( T ) else : raise Exception ( 'Current scale not supported' ) if desired == 'ITS-90' : pass elif desired == 'ITS-68' : range_check ( T , 13.999 , 4300.0001 ) T = T90_to_T68 ( T ) elif desired == 'ITS-76' : range_check ( T , 4.9999 , 27.0001 ) T = T90_to_T76 ( T ) elif desired == 'ITS-48' : range_check ( T , 93.149999 , 4273.15001 ) T = T90_to_T48 ( T ) elif desired == 'ITS-27' : range_check ( T , 903.15 , 4273.15 ) T = T90_to_T27 ( T ) else : raise Exception ( 'Desired scale not supported' ) except ValueError : raise Exception ( 'Temperature could not be converted to desired scale' ) return float ( T )
7580	def get_evanno_table ( self , kvalues , max_var_multiple = 0 , quiet = False ) : if max_var_multiple : if max_var_multiple < 1 : raise ValueError ( 'max_variance_multiplier must be >1' ) table = _get_evanno_table ( self , kvalues , max_var_multiple , quiet ) return table
1964	def sys_chroot ( self , path ) : if path not in self . current . memory : return - errno . EFAULT path_s = self . current . read_string ( path ) if not os . path . exists ( path_s ) : return - errno . ENOENT if not os . path . isdir ( path_s ) : return - errno . ENOTDIR return - errno . EPERM
911	def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore
13194	def remove_comments ( tex_source ) : return re . sub ( r'(?<!\\)%.*$' , r'' , tex_source , flags = re . M )
4526	def run ( self , next_task ) : self . event . wait ( ) self . task ( ) self . event . clear ( ) next_task . event . set ( )
4037	def error_handler ( req ) : error_codes = { 400 : ze . UnsupportedParams , 401 : ze . UserNotAuthorised , 403 : ze . UserNotAuthorised , 404 : ze . ResourceNotFound , 409 : ze . Conflict , 412 : ze . PreConditionFailed , 413 : ze . RequestEntityTooLarge , 428 : ze . PreConditionRequired , 429 : ze . TooManyRequests , } def err_msg ( req ) : return "\nCode: %s\nURL: %s\nMethod: %s\nResponse: %s" % ( req . status_code , req . url , req . request . method , req . text , ) if error_codes . get ( req . status_code ) : if req . status_code == 429 : delay = backoff . delay if delay > 32 : backoff . reset ( ) raise ze . TooManyRetries ( "Continuing to receive HTTP 429 \responses after 62 seconds. You are being rate-limited, try again later" ) time . sleep ( delay ) sess = requests . Session ( ) new_req = sess . send ( req . request ) try : new_req . raise_for_status ( ) except requests . exceptions . HTTPError : error_handler ( new_req ) else : raise error_codes . get ( req . status_code ) ( err_msg ( req ) ) else : raise ze . HTTPError ( err_msg ( req ) )
11724	def init_config ( self , app ) : config_apps = [ 'APP_' , 'RATELIMIT_' ] flask_talisman_debug_mode = [ "'unsafe-inline'" ] for k in dir ( config ) : if any ( [ k . startswith ( prefix ) for prefix in config_apps ] ) : app . config . setdefault ( k , getattr ( config , k ) ) if app . config [ 'DEBUG' ] : app . config . setdefault ( 'APP_DEFAULT_SECURE_HEADERS' , { } ) headers = app . config [ 'APP_DEFAULT_SECURE_HEADERS' ] if headers . get ( 'content_security_policy' ) != { } : headers . setdefault ( 'content_security_policy' , { } ) csp = headers [ 'content_security_policy' ] if csp . get ( 'default-src' ) != [ ] : csp . setdefault ( 'default-src' , [ ] ) csp [ 'default-src' ] += flask_talisman_debug_mode
8621	def get_self ( session , user_details = None ) : if user_details : user_details [ 'compact' ] = True response = make_get_request ( session , 'self' , params_data = user_details ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise SelfNotRetrievedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
12701	def get_subfields ( self , datafield , subfield , i1 = None , i2 = None , exception = False ) : if len ( datafield ) != 3 : raise ValueError ( "`datafield` parameter have to be exactly 3 chars long!" ) if len ( subfield ) != 1 : raise ValueError ( "Bad subfield specification - subfield have to be 1 char long!" ) if datafield not in self . datafields : if exception : raise KeyError ( datafield + " is not in datafields!" ) return [ ] output = [ ] for datafield in self . datafields [ datafield ] : if subfield not in datafield : continue for sfield in datafield [ subfield ] : if i1 and sfield . i1 != i1 : continue if i2 and sfield . i2 != i2 : continue output . append ( sfield ) if not output and exception : raise KeyError ( subfield + " couldn't be found in subfields!" ) return output
1879	def VEXTRACTF128 ( cpu , dest , src , offset ) : offset = offset . read ( ) dest . write ( Operators . EXTRACT ( src . read ( ) , offset * 128 , ( offset + 1 ) * 128 ) )
997	def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )
7875	def element_to_unicode ( element ) : if hasattr ( ElementTree , 'tounicode' ) : return ElementTree . tounicode ( "element" ) elif sys . version_info . major < 3 : return unicode ( ElementTree . tostring ( element ) ) else : return ElementTree . tostring ( element , encoding = "unicode" )
8454	def _cookiecutter_configs_have_changed ( template , old_version , new_version ) : temple . check . is_git_ssh_path ( template ) repo_path = temple . utils . get_repo_path ( template ) github_client = temple . utils . GithubClient ( ) api = '/repos/{}/contents/cookiecutter.json' . format ( repo_path ) old_config_resp = github_client . get ( api , params = { 'ref' : old_version } ) old_config_resp . raise_for_status ( ) new_config_resp = github_client . get ( api , params = { 'ref' : new_version } ) new_config_resp . raise_for_status ( ) return old_config_resp . json ( ) [ 'content' ] != new_config_resp . json ( ) [ 'content' ]
3052	def step1_get_authorize_url ( self , redirect_uri = None , state = None ) : if redirect_uri is not None : logger . warning ( ( 'The redirect_uri parameter for ' 'OAuth2WebServerFlow.step1_get_authorize_url is deprecated. ' 'Please move to passing the redirect_uri in via the ' 'constructor.' ) ) self . redirect_uri = redirect_uri if self . redirect_uri is None : raise ValueError ( 'The value of redirect_uri must not be None.' ) query_params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , 'scope' : self . scope , } if state is not None : query_params [ 'state' ] = state if self . login_hint is not None : query_params [ 'login_hint' ] = self . login_hint if self . _pkce : if not self . code_verifier : self . code_verifier = _pkce . code_verifier ( ) challenge = _pkce . code_challenge ( self . code_verifier ) query_params [ 'code_challenge' ] = challenge query_params [ 'code_challenge_method' ] = 'S256' query_params . update ( self . params ) return _helpers . update_query_params ( self . auth_uri , query_params )
2976	def cmd_status ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) containers = b . status ( ) print_containers ( containers , opts . json )
9884	def _read_all_z_variable_info ( self ) : self . z_variable_info = { } self . z_variable_names_by_num = { } info = fortran_cdf . z_var_all_inquire ( self . fname , self . _num_z_vars , len ( self . fname ) ) status = info [ 0 ] data_types = info [ 1 ] num_elems = info [ 2 ] rec_varys = info [ 3 ] dim_varys = info [ 4 ] num_dims = info [ 5 ] dim_sizes = info [ 6 ] rec_nums = info [ 7 ] var_nums = info [ 8 ] var_names = info [ 9 ] if status == 0 : for i in np . arange ( len ( data_types ) ) : out = { } out [ 'data_type' ] = data_types [ i ] out [ 'num_elems' ] = num_elems [ i ] out [ 'rec_vary' ] = rec_varys [ i ] out [ 'dim_varys' ] = dim_varys [ i ] out [ 'num_dims' ] = num_dims [ i ] out [ 'dim_sizes' ] = dim_sizes [ i , : 1 ] if out [ 'dim_sizes' ] [ 0 ] == 0 : out [ 'dim_sizes' ] [ 0 ] += 1 out [ 'rec_num' ] = rec_nums [ i ] out [ 'var_num' ] = var_nums [ i ] var_name = '' . join ( var_names [ i ] . astype ( 'U' ) ) out [ 'var_name' ] = var_name . rstrip ( ) self . z_variable_info [ out [ 'var_name' ] ] = out self . z_variable_names_by_num [ out [ 'var_num' ] ] = var_name else : raise IOError ( fortran_cdf . statusreporter ( status ) )
3214	def get_stats ( self ) : expired = sum ( [ x [ 'expired' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) miss = sum ( [ x [ 'miss' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) hit = sum ( [ x [ 'hit' ] for _ , x in self . _CACHE_STATS [ 'access_stats' ] . items ( ) ] ) return { 'totals' : { 'keys' : len ( self . _CACHE_STATS [ 'access_stats' ] ) , 'expired' : expired , 'miss' : miss , 'hit' : hit , } }
9272	def filter_due_tag ( self , all_tags ) : filtered_tags = [ ] tag = self . options . due_tag tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "due-tag" ) return copy . deepcopy ( all_tags ) due_tag = all_tags [ idx ] due_date = self . get_time_of_tag ( due_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if tag_date <= due_date : filtered_tags . append ( t ) return filtered_tags
9583	def write_var_header ( fd , header ) : fd . write ( struct . pack ( 'b3xI' , etypes [ 'miUINT32' ] [ 'n' ] , 8 ) ) fd . write ( struct . pack ( 'b3x4x' , mclasses [ header [ 'mclass' ] ] ) ) write_elements ( fd , 'miINT32' , header [ 'dims' ] ) write_elements ( fd , 'miINT8' , asbytes ( header [ 'name' ] ) , is_name = True )
6933	def colormagdiagram_cpdir ( cpdir , outpkl , cpfileglob = 'checkplot*.pkl*' , color_mag1 = [ 'gaiamag' , 'sdssg' ] , color_mag2 = [ 'kmag' , 'kmag' ] , yaxis_mag = [ 'gaia_absmag' , 'rpmj' ] ) : cplist = glob . glob ( os . path . join ( cpdir , cpfileglob ) ) return colormagdiagram_cplist ( cplist , outpkl , color_mag1 = color_mag1 , color_mag2 = color_mag2 , yaxis_mag = yaxis_mag )
2110	def send ( source = None , prevent = None , exclude = None , secret_management = 'default' , no_color = False ) : from tower_cli . cli . transfer . send import Sender sender = Sender ( no_color ) sender . send ( source , prevent , exclude , secret_management )
9536	def enumeration ( * args ) : assert len ( args ) > 0 , 'at least one argument is required' if len ( args ) == 1 : members = args [ 0 ] else : members = args def checker ( value ) : if value not in members : raise ValueError ( value ) return checker
1866	def PSHUFD ( cpu , op0 , op1 , op3 ) : size = op0 . size arg0 = op0 . read ( ) arg1 = op1 . read ( ) order = Operators . ZEXTEND ( op3 . read ( ) , size ) arg0 = arg0 & 0xffffffffffffffffffffffffffffffff00000000000000000000000000000000 arg0 |= ( ( arg1 >> ( ( ( order >> 0 ) & 3 ) * 32 ) ) & 0xffffffff ) arg0 |= ( ( arg1 >> ( ( ( order >> 2 ) & 3 ) * 32 ) ) & 0xffffffff ) << 32 arg0 |= ( ( arg1 >> ( ( ( order >> 4 ) & 3 ) * 32 ) ) & 0xffffffff ) << 64 arg0 |= ( ( arg1 >> ( ( ( order >> 6 ) & 3 ) * 32 ) ) & 0xffffffff ) << 96 op0 . write ( arg0 )
10941	def update_function ( self , param_vals ) : self . model = self . func ( param_vals , * self . func_args , ** self . func_kwargs ) d = self . calc_residuals ( ) return np . dot ( d . flat , d . flat )
11950	def _set_global_verbosity_level ( is_verbose_output = False ) : global verbose_output verbose_output = is_verbose_output if verbose_output : jocker_lgr . setLevel ( logging . DEBUG ) else : jocker_lgr . setLevel ( logging . INFO )
12465	def read_config ( filename , args ) : config = defaultdict ( dict ) splitter = operator . methodcaller ( 'split' , ' ' ) converters = { __script__ : { 'env' : safe_path , 'pre_requirements' : splitter , } , 'pip' : { 'allow_external' : splitter , 'allow_unverified' : splitter , } } default = copy . deepcopy ( CONFIG ) sections = set ( iterkeys ( default ) ) if int ( getattr ( pip , '__version__' , '1.x' ) . split ( '.' ) [ 0 ] ) < 6 : default [ 'pip' ] [ 'download_cache' ] = safe_path ( os . path . expanduser ( os . path . join ( '~' , '.{0}' . format ( __script__ ) , 'pip-cache' ) ) ) is_default = filename == DEFAULT_CONFIG filename = os . path . expandvars ( os . path . expanduser ( filename ) ) if not is_default and not os . path . isfile ( filename ) : print_error ( 'Config file does not exist at {0!r}' . format ( filename ) ) return None parser = ConfigParser ( ) try : parser . read ( filename ) except ConfigParserError : print_error ( 'Cannot parse config file at {0!r}' . format ( filename ) ) return None for section in sections : if not parser . has_section ( section ) : continue items = parser . items ( section ) for key , value in items : try : value = int ( value ) except ( TypeError , ValueError ) : try : value = bool ( strtobool ( value ) ) except ValueError : pass if section in converters and key in converters [ section ] : value = converters [ section ] [ key ] ( value ) config [ section ] [ key ] = value for section , data in iteritems ( default ) : if section not in config : config [ section ] = data else : for key , value in iteritems ( data ) : config [ section ] . setdefault ( key , value ) keys = set ( ( 'env' , 'hook' , 'install_dev_requirements' , 'ignore_activated' , 'pre_requirements' , 'quiet' , 'recreate' , 'requirements' ) ) for key in keys : value = getattr ( args , key ) config [ __script__ ] . setdefault ( key , value ) if key == 'pre_requirements' and not value : continue if value is not None : config [ __script__ ] [ key ] = value return config
368	def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( "The size of cropping should smaller than or equal to the original image" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] else : h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg return x [ h_offset : h_end , w_offset : w_end ]
11992	def set_signature_passphrases ( self , signature_passphrases ) : self . signature_passphrases = self . _update_dict ( signature_passphrases , { } , replace_data = True )
8253	def context_to_rgb ( self , str ) : matches = [ ] for clr in context : tags = context [ clr ] for tag in tags : if tag . startswith ( str ) or str . startswith ( tag ) : matches . append ( clr ) break matches = [ color ( name ) for name in matches ] return matches
13106	def add_tag ( self , tag ) : self . tags = list ( set ( self . tags or [ ] ) | set ( [ tag ] ) )
2270	def _win32_symlink2 ( path , link , allow_fallback = True , verbose = 0 ) : if _win32_can_symlink ( ) : return _win32_symlink ( path , link , verbose ) else : return _win32_junction ( path , link , verbose )
7443	def _step1func ( self , force , ipyclient ) : sfiles = self . paramsdict [ "sorted_fastq_path" ] rfiles = self . paramsdict [ "raw_fastq_path" ] if sfiles and rfiles : raise IPyradWarningExit ( NOT_TWO_PATHS ) if not ( sfiles or rfiles ) : raise IPyradWarningExit ( NO_SEQ_PATH_FOUND ) if self . _headers : if sfiles : print ( "\n{}Step 1: Loading sorted fastq data to Samples" . format ( self . _spacer ) ) else : print ( "\n{}Step 1: Demultiplexing fastq data to Samples" . format ( self . _spacer ) ) if self . samples : if not force : print ( SAMPLES_EXIST . format ( len ( self . samples ) , self . name ) ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient , force = force ) else : assemble . demultiplex . run2 ( self , ipyclient , force ) else : if glob . glob ( sfiles ) : self . _link_fastqs ( ipyclient = ipyclient ) else : assemble . demultiplex . run2 ( self , ipyclient , force )
10536	def get_category ( category_id ) : try : res = _pybossa_req ( 'get' , 'category' , category_id ) if res . get ( 'id' ) : return Category ( res ) else : return res except : raise
4708	def power_off ( self , interval = 200 ) : if self . __power_off_port is None : cij . err ( "cij.usb.relay: Invalid USB_RELAY_POWER_OFF" ) return 1 return self . __press ( self . __power_off_port , interval = interval )
11084	def save ( self , msg , args ) : self . send_message ( msg . channel , "Saving current state..." ) self . _bot . plugins . save_state ( ) self . send_message ( msg . channel , "Done." )
13001	def hr_diagram_from_data ( data , x_range , y_range ) : _ , color_mapper = hr_diagram_color_helper ( [ ] ) data_dict = { 'x' : list ( data [ 'temperature' ] ) , 'y' : list ( data [ 'luminosity' ] ) , 'color' : list ( data [ 'color' ] ) } source = ColumnDataSource ( data = data_dict ) pf = figure ( y_axis_type = 'log' , x_range = x_range , y_range = y_range ) _diagram ( source = source , plot_figure = pf , color = { 'field' : 'color' , 'transform' : color_mapper } , xaxis_label = 'Temperature (Kelvin)' , yaxis_label = 'Luminosity (solar units)' ) show_with_bokeh_server ( pf )
7879	def add_prefix ( self , namespace , prefix ) : if prefix == "xml" and namespace != XML_NS : raise ValueError , "Cannot change 'xml' prefix meaning" self . _prefixes [ namespace ] = prefix
8633	def place_project_bid ( session , project_id , bidder_id , description , amount , period , milestone_percentage ) : bid_data = { 'project_id' : project_id , 'bidder_id' : bidder_id , 'description' : description , 'amount' : amount , 'period' : period , 'milestone_percentage' : milestone_percentage , } response = make_post_request ( session , 'bids' , json_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : bid_data = json_data [ 'result' ] return Bid ( bid_data ) else : raise BidNotPlacedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
9343	def abort ( self ) : self . mutex . release ( ) self . turnstile . release ( ) self . mutex . release ( ) self . turnstile2 . release ( )
3955	def remove_images ( ) : client = get_docker_client ( ) removed = _remove_dangling_images ( ) dusty_images = get_dusty_images ( ) all_images = client . images ( all = True ) for image in all_images : if set ( image [ 'RepoTags' ] ) . intersection ( dusty_images ) : try : client . remove_image ( image [ 'Id' ] ) except Exception as e : logging . info ( "Couldn't remove image {}" . format ( image [ 'RepoTags' ] ) ) else : log_to_client ( "Removed Image {}" . format ( image [ 'RepoTags' ] ) ) removed . append ( image ) return removed
2208	def ensuredir ( dpath , mode = 0o1777 , verbose = None ) : r if verbose is None : verbose = 0 if isinstance ( dpath , ( list , tuple ) ) : dpath = join ( * dpath ) if not exists ( dpath ) : if verbose : print ( 'Ensuring new directory (%r)' % dpath ) if sys . version_info . major == 2 : os . makedirs ( normpath ( dpath ) , mode = mode ) else : os . makedirs ( normpath ( dpath ) , mode = mode , exist_ok = True ) else : if verbose : print ( 'Ensuring existing directory (%r)' % dpath ) return dpath
10075	def merge_with_published ( self ) : pid , first = self . fetch_published ( ) lca = first . revisions [ self [ '_deposit' ] [ 'pid' ] [ 'revision_id' ] ] args = [ lca . dumps ( ) , first . dumps ( ) , self . dumps ( ) ] for arg in args : del arg [ '$schema' ] , arg [ '_deposit' ] args . append ( { } ) m = Merger ( * args ) try : m . run ( ) except UnresolvedConflictsException : raise MergeConflict ( ) return patch ( m . unified_patches , lca )
3450	def flux_variability_analysis ( model , reaction_list = None , loopless = False , fraction_of_optimum = 1.0 , pfba_factor = None , processes = None ) : if reaction_list is None : reaction_ids = [ r . id for r in model . reactions ] else : reaction_ids = [ r . id for r in model . reactions . get_by_any ( reaction_list ) ] if processes is None : processes = CONFIGURATION . processes num_reactions = len ( reaction_ids ) processes = min ( processes , num_reactions ) fva_result = DataFrame ( { "minimum" : zeros ( num_reactions , dtype = float ) , "maximum" : zeros ( num_reactions , dtype = float ) } , index = reaction_ids ) prob = model . problem with model : model . slim_optimize ( error_value = None , message = "There is no optimal solution for the " "chosen objective!" ) if model . solver . objective . direction == "max" : fva_old_objective = prob . Variable ( "fva_old_objective" , lb = fraction_of_optimum * model . solver . objective . value ) else : fva_old_objective = prob . Variable ( "fva_old_objective" , ub = fraction_of_optimum * model . solver . objective . value ) fva_old_obj_constraint = prob . Constraint ( model . solver . objective . expression - fva_old_objective , lb = 0 , ub = 0 , name = "fva_old_objective_constraint" ) model . add_cons_vars ( [ fva_old_objective , fva_old_obj_constraint ] ) if pfba_factor is not None : if pfba_factor < 1. : warn ( "The 'pfba_factor' should be larger or equal to 1." , UserWarning ) with model : add_pfba ( model , fraction_of_optimum = 0 ) ub = model . slim_optimize ( error_value = None ) flux_sum = prob . Variable ( "flux_sum" , ub = pfba_factor * ub ) flux_sum_constraint = prob . Constraint ( model . solver . objective . expression - flux_sum , lb = 0 , ub = 0 , name = "flux_sum_constraint" ) model . add_cons_vars ( [ flux_sum , flux_sum_constraint ] ) model . objective = Zero for what in ( "minimum" , "maximum" ) : if processes > 1 : chunk_size = len ( reaction_ids ) // processes pool = multiprocessing . Pool ( processes , initializer = _init_worker , initargs = ( model , loopless , what [ : 3 ] ) ) for rxn_id , value in pool . imap_unordered ( _fva_step , reaction_ids , chunksize = chunk_size ) : fva_result . at [ rxn_id , what ] = value pool . close ( ) pool . join ( ) else : _init_worker ( model , loopless , what [ : 3 ] ) for rxn_id , value in map ( _fva_step , reaction_ids ) : fva_result . at [ rxn_id , what ] = value return fva_result [ [ "minimum" , "maximum" ] ]
8048	def _parse_from_import_names ( self , is_future_import ) : if self . current . value == "(" : self . consume ( tk . OP ) expected_end_kinds = ( tk . OP , ) else : expected_end_kinds = ( tk . NEWLINE , tk . ENDMARKER ) while self . current . kind not in expected_end_kinds and not ( self . current . kind == tk . OP and self . current . value == ";" ) : if self . current . kind != tk . NAME : self . stream . move ( ) continue self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if is_future_import : self . log . debug ( "found future import: %s" , self . current . value ) self . future_imports . add ( self . current . value ) self . consume ( tk . NAME ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , ) if self . current . kind == tk . NAME and self . current . value == "as" : self . consume ( tk . NAME ) if self . current . kind == tk . NAME : self . consume ( tk . NAME ) if self . current . value == "," : self . consume ( tk . OP ) self . log . debug ( "parsing import, token is %r (%s)" , self . current . kind , self . current . value , )
2600	def unset_logging ( self ) : if self . logger_flag is True : return root_logger = logging . getLogger ( ) for hndlr in root_logger . handlers : if hndlr not in self . prior_loghandlers : hndlr . setLevel ( logging . ERROR ) self . logger_flag = True
2537	def set_pkg_source_info ( self , doc , text ) : self . assert_package_exists ( ) if not self . package_source_info_set : self . package_source_info_set = True doc . package . source_info = text return True else : raise CardinalityError ( 'Package::SourceInfo' )
12438	def deserialize ( self , request = None , text = None , format = None ) : if isinstance ( self , Resource ) : if not request : request = self . _request Deserializer = None if format : Deserializer = self . meta . deserializers [ format ] if not Deserializer : media_ranges = request . get ( 'Content-Type' ) if media_ranges : media_types = six . iterkeys ( self . _deserializer_map ) media_type = mimeparse . best_match ( media_types , media_ranges ) if media_type : format = self . _deserializer_map [ media_type ] Deserializer = self . meta . deserializers [ format ] else : pass if Deserializer : try : deserializer = Deserializer ( ) data = deserializer . deserialize ( request = request , text = text ) return data , deserializer except ValueError : pass raise http . exceptions . UnsupportedMediaType ( )
1005	def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : numPrevPatterns = len ( self . _prevLrnPatterns ) currentTimeStepsOffset = numPrevPatterns - 1 if not readOnly : self . segmentUpdates = { } if self . verbosity >= 3 : if readOnly : print ( "Trying to lock-on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( "Locking on using startCell state from %d steps ago:" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) inSequence = True for offset in range ( startOffset , numPrevPatterns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] inputColumns = self . _prevLrnPatterns [ offset ] if not readOnly : self . _processSegmentUpdates ( inputColumns ) if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) if not inSequence or offset == currentTimeStepsOffset : break if self . verbosity >= 3 : print " backtrack: computing predictions from " , inputColumns self . _learnPhase2 ( readOnly = readOnly ) return inSequence
12351	def restore ( self , image , wait = True ) : return self . _action ( 'restore' , image = image , wait = wait )
9346	def adapt ( cls , source , template ) : if not isinstance ( template , packarray ) : raise TypeError ( 'template must be a packarray' ) return cls ( source , template . start , template . end )
4501	def project ( * descs , root_file = None ) : load . ROOT_FILE = root_file desc = merge . merge ( merge . DEFAULT_PROJECT , * descs ) path = desc . get ( 'path' , '' ) if root_file : project_path = os . path . dirname ( root_file ) if path : path += ':' + project_path else : path = project_path with load . extender ( path ) : desc = recurse . recurse ( desc ) project = construct . construct ( ** desc ) project . desc = desc return project
2948	def evaluate ( self , task , expression ) : if isinstance ( expression , Operator ) : return expression . _matches ( task ) else : return self . _eval ( task , expression , ** task . data )
2217	def _list_itemstrs ( list_ , ** kwargs ) : items = list ( list_ ) kwargs [ '_return_info' ] = True _tups = [ repr2 ( item , ** kwargs ) for item in items ] itemstrs = [ t [ 0 ] for t in _tups ] max_height = max ( [ t [ 1 ] [ 'max_height' ] for t in _tups ] ) if _tups else 0 _leaf_info = { 'max_height' : max_height + 1 , } sort = kwargs . get ( 'sort' , None ) if sort is None : sort = isinstance ( list_ , ( set , frozenset ) ) if sort : itemstrs = _sort_itemstrs ( items , itemstrs ) return itemstrs , _leaf_info
8514	def dict_merge ( base , top ) : out = dict ( top ) for key in base : if key in top : if isinstance ( base [ key ] , dict ) and isinstance ( top [ key ] , dict ) : out [ key ] = dict_merge ( base [ key ] , top [ key ] ) else : out [ key ] = base [ key ] return out
4704	def memcopy ( self , stream , offset = 0 , length = float ( "inf" ) ) : data = [ ord ( i ) for i in list ( stream ) ] size = min ( length , len ( data ) , self . m_size ) buff = cast ( self . m_buf , POINTER ( c_uint8 ) ) for i in range ( size ) : buff [ offset + i ] = data [ i ]
925	def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 if nonNone == 0 : return None sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]
515	def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1
12866	def startup ( self , app ) : self . database . init_async ( app . loop ) if not self . cfg . connection_manual : app . middlewares . insert ( 0 , self . _middleware )
8191	def nodes_by_eigenvalue ( self , treshold = 0.0 ) : nodes = [ ( n . eigenvalue , n ) for n in self . nodes if n . eigenvalue > treshold ] nodes . sort ( ) nodes . reverse ( ) return [ n for w , n in nodes ]
11826	def print_boggle ( board ) : "Print the board in a 2-d array." n2 = len ( board ) n = exact_sqrt ( n2 ) for i in range ( n2 ) : if i % n == 0 and i > 0 : print if board [ i ] == 'Q' : print 'Qu' , else : print str ( board [ i ] ) + ' ' , print
6185	def check_clean_status ( git_path = None ) : output = get_status ( git_path ) is_unmodified = ( len ( output . strip ( ) ) == 0 ) return is_unmodified
10036	def execute ( helper , config , args ) : environment_name = args . environment ( events , next_token ) = helper . describe_events ( environment_name , start_time = datetime . now ( ) . isoformat ( ) ) for event in events : print ( ( "[" + event [ 'Severity' ] + "] " + event [ 'Message' ] ) )
4257	def copy ( src , dst , symlink = False , rellink = False ) : func = os . symlink if symlink else shutil . copy2 if symlink and os . path . lexists ( dst ) : os . remove ( dst ) if rellink : func ( os . path . relpath ( src , os . path . dirname ( dst ) ) , dst ) else : func ( src , dst )
12661	def convert_sav ( inputfile , outputfile = None , method = 'rpy2' , otype = 'csv' ) : assert ( os . path . isfile ( inputfile ) ) assert ( method == 'rpy2' or method == 'savread' ) if method == 'rpy2' : df = sav_to_pandas_rpy2 ( inputfile ) elif method == 'savread' : df = sav_to_pandas_savreader ( inputfile ) otype_exts = { 'csv' : '.csv' , 'hdf' : '.h5' , 'stata' : '.dta' , 'json' : '.json' , 'pickle' : '.pickle' , 'excel' : '.xls' , 'html' : '.html' } if outputfile is None : outputfile = inputfile . replace ( path ( inputfile ) . ext , '' ) outputfile = add_extension_if_needed ( outputfile , otype_exts [ otype ] ) if otype == 'csv' : df . to_csv ( outputfile ) elif otype == 'hdf' : df . to_hdf ( outputfile , os . path . basename ( outputfile ) ) elif otype == 'stata' : df . to_stata ( outputfile ) elif otype == 'json' : df . to_json ( outputfile ) elif otype == 'pickle' : df . to_pickle ( outputfile ) elif otype == 'excel' : df . to_excel ( outputfile ) elif otype == 'html' : df . to_html ( outputfile ) else : df . to_csv ( outputfile )
10526	def get_google_playlist_songs ( self , playlist , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False ) : logger . info ( "Loading Google Music playlist songs..." ) google_playlist = self . get_google_playlist ( playlist ) if not google_playlist : return [ ] , [ ] playlist_song_ids = [ track [ 'trackId' ] for track in google_playlist [ 'tracks' ] ] playlist_songs = [ song for song in self . api . get_all_songs ( ) if song [ 'id' ] in playlist_song_ids ] matched_songs , filtered_songs = filter_google_songs ( playlist_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Filtered {0} Google playlist songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} Google playlist songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs
6487	def _translate_hits ( es_response ) : def translate_result ( result ) : translated_result = copy . copy ( result ) data = translated_result . pop ( "_source" ) translated_result . update ( { "data" : data , "score" : translated_result [ "_score" ] } ) return translated_result def translate_facet ( result ) : terms = { term [ "term" ] : term [ "count" ] for term in result [ "terms" ] } return { "terms" : terms , "total" : result [ "total" ] , "other" : result [ "other" ] , } results = [ translate_result ( hit ) for hit in es_response [ "hits" ] [ "hits" ] ] response = { "took" : es_response [ "took" ] , "total" : es_response [ "hits" ] [ "total" ] , "max_score" : es_response [ "hits" ] [ "max_score" ] , "results" : results , } if "facets" in es_response : response [ "facets" ] = { facet : translate_facet ( es_response [ "facets" ] [ facet ] ) for facet in es_response [ "facets" ] } return response
9386	def parse ( self ) : for infile in self . infile_list : logger . info ( 'Processing : %s' , infile ) status = True file_status = naarad . utils . is_valid_file ( infile ) if not file_status : return False with open ( infile ) as fh : for line in fh : words = line . split ( ) if not words : continue if re . match ( '^\d\d\d\d-\d\d-\d\d$' , line ) : self . ts_date = words [ 0 ] continue prefix_word = words [ 0 ] . strip ( ) if prefix_word == 'top' : self . process_top_line ( words ) self . saw_pid = False elif self . ts_valid_lines : if prefix_word == 'Tasks:' : self . process_tasks_line ( words ) elif prefix_word == 'Cpu(s):' : self . process_cpu_line ( words ) elif prefix_word == 'Mem:' : self . process_mem_line ( words ) elif prefix_word == 'Swap:' : self . process_swap_line ( words ) elif prefix_word == 'PID' : self . saw_pid = True self . process_headers = words else : if self . saw_pid and len ( words ) >= len ( self . process_headers ) : self . process_individual_command ( words ) for out_csv in self . data . keys ( ) : self . csv_files . append ( out_csv ) with open ( out_csv , 'w' ) as fh : fh . write ( '\n' . join ( self . data [ out_csv ] ) ) gc . collect ( ) return status
9902	def with_data ( path , data ) : if isinstance ( data , str ) : data = json . loads ( data ) if os . path . exists ( path ) : raise ValueError ( "File exists, not overwriting data. Set the " "'data' attribute on a normally-initialized " "'livejson.File' instance if you really " "want to do this." ) else : f = File ( path ) f . data = data return f
12068	def new ( ABF , forceNewFigure = False , title = None , xlabel = None , ylabel = None ) : if len ( pylab . get_fignums ( ) ) and forceNewFigure == False : return pylab . figure ( figsize = ( 8 , 6 ) ) pylab . grid ( alpha = .5 ) pylab . title ( ABF . ID ) pylab . ylabel ( ABF . units ) pylab . xlabel ( "seconds" ) if xlabel : pylab . xlabel ( xlabel ) if ylabel : pylab . ylabel ( ylabel ) if title : pylab . title ( title ) annotate ( ABF )
9956	def tracemessage ( self , maxlen = 6 ) : result = "" for i , value in enumerate ( self ) : result += "{0}: {1}\n" . format ( i , get_node_repr ( value ) ) result = result . strip ( "\n" ) lines = result . split ( "\n" ) if maxlen and len ( lines ) > maxlen : i = int ( maxlen / 2 ) lines = lines [ : i ] + [ "..." ] + lines [ - ( maxlen - i ) : ] result = "\n" . join ( lines ) return result
5148	def write ( self , name , path = './' ) : byte_object = self . generate ( ) file_name = '{0}.tar.gz' . format ( name ) if not path . endswith ( '/' ) : path += '/' f = open ( '{0}{1}' . format ( path , file_name ) , 'wb' ) f . write ( byte_object . getvalue ( ) ) f . close ( )
5536	def write ( self , process_tile , data ) : if isinstance ( process_tile , tuple ) : process_tile = self . config . process_pyramid . tile ( * process_tile ) elif not isinstance ( process_tile , BufferedTile ) : raise ValueError ( "invalid process_tile type: %s" % type ( process_tile ) ) if self . config . mode not in [ "continue" , "overwrite" ] : raise ValueError ( "cannot write output in current process mode" ) if self . config . mode == "continue" and ( self . config . output . tiles_exist ( process_tile ) ) : message = "output exists, not overwritten" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) elif data is None : message = "output empty, nothing written" logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = False , write_msg = message ) else : with Timer ( ) as t : self . config . output . write ( process_tile = process_tile , data = data ) message = "output written in %s" % t logger . debug ( ( process_tile . id , message ) ) return ProcessInfo ( tile = process_tile , processed = False , process_msg = None , written = True , write_msg = message )
9792	def is_ignored ( cls , path , patterns ) : status = None for pattern in cls . find_matching ( path , patterns ) : status = pattern . is_exclude return status
13777	def AddEnumDescriptor ( self , enum_desc ) : if not isinstance ( enum_desc , descriptor . EnumDescriptor ) : raise TypeError ( 'Expected instance of descriptor.EnumDescriptor.' ) self . _enum_descriptors [ enum_desc . full_name ] = enum_desc self . AddFileDescriptor ( enum_desc . file )
12071	def update ( self , tids , info ) : outputs_dir = os . path . join ( info [ 'root_directory' ] , 'streams' ) pattern = '%s_*_tid_*{tid}.o.{tid}*' % info [ 'batch_name' ] flist = os . listdir ( outputs_dir ) try : outputs = [ ] for tid in tids : matches = fnmatch . filter ( flist , pattern . format ( tid = tid ) ) if len ( matches ) != 1 : self . warning ( "No unique output file for tid %d" % tid ) contents = open ( os . path . join ( outputs_dir , matches [ 0 ] ) , 'r' ) . read ( ) outputs . append ( self . output_extractor ( contents ) ) self . _next_val = self . _update_state ( outputs ) self . trace . append ( ( outputs , self . _next_val ) ) except : self . warning ( "Cannot load required output files. Cannot continue." ) self . _next_val = StopIteration
5615	def read_vector_window ( input_files , tile , validity_check = True ) : if not isinstance ( input_files , list ) : input_files = [ input_files ] return [ feature for feature in chain . from_iterable ( [ _read_vector_window ( path , tile , validity_check = validity_check ) for path in input_files ] ) ]
59	def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )
9555	def _apply_value_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for field_name , predicate , code , message , modulus in self . _value_predicates : if i % modulus == 0 : fi = self . _field_names . index ( field_name ) if fi < len ( r ) : value = r [ fi ] try : valid = predicate ( value ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'column' ] = fi + 1 p [ 'field' ] = field_name p [ 'value' ] = value p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
12219	def _make_all_matchers ( cls , parameters ) : for name , param in parameters : annotation = param . annotation if annotation is not Parameter . empty : yield name , cls . _make_param_matcher ( annotation , param . kind )
8923	def baseurl ( url ) : parsed_url = urlparse . urlparse ( url ) if not parsed_url . netloc or parsed_url . scheme not in ( "http" , "https" ) : raise ValueError ( 'bad url' ) service_url = "%s://%s%s" % ( parsed_url . scheme , parsed_url . netloc , parsed_url . path . strip ( ) ) return service_url
6631	def set ( self , path , value = None , filename = None ) : if filename is None : config = self . _firstConfig ( ) [ 1 ] else : config = self . configs [ filename ] path = _splitPath ( path ) for el in path [ : - 1 ] : if el in config : config = config [ el ] else : config [ el ] = OrderedDict ( ) config = config [ el ] config [ path [ - 1 ] ] = value
2982	def cmd_add ( opts ) : config = load_config ( opts . config ) b = get_blockade ( config , opts ) b . add_container ( opts . containers )
5954	def tool_factory ( clsname , name , driver , base = GromacsCommand ) : clsdict = { 'command_name' : name , 'driver' : driver , '__doc__' : property ( base . _get_gmx_docs ) } return type ( clsname , ( base , ) , clsdict )
5318	def format ( self , string , * args , ** kwargs ) : return string . format ( c = self , * args , ** kwargs )
6421	def encode ( self , word ) : word = unicode_normalize ( 'NFKD' , text_type ( word . upper ( ) ) ) word = word . translate ( { 198 : 'AE' , 338 : 'OE' } ) word = '' . join ( c for c in word if c in self . _uc_set ) for rule in self . _rule_order : regex , repl = self . _rule_table [ rule ] if isinstance ( regex , text_type ) : word = word . replace ( regex , repl ) else : word = regex . sub ( repl , word ) return word
3093	def _create_flow ( self , request_handler ) : if self . flow is None : redirect_uri = request_handler . request . relative_url ( self . _callback_path ) self . flow = client . OAuth2WebServerFlow ( self . _client_id , self . _client_secret , self . _scope , redirect_uri = redirect_uri , user_agent = self . _user_agent , auth_uri = self . _auth_uri , token_uri = self . _token_uri , revoke_uri = self . _revoke_uri , ** self . _kwargs )
2129	def configure_display ( self , data , kwargs = None , write = False ) : if settings . format != 'human' : return if write : obj , obj_type , res , res_type = self . obj_res ( kwargs ) data [ 'type' ] = kwargs [ 'type' ] data [ obj_type ] = obj data [ res_type ] = res self . set_display_columns ( set_false = [ 'team' if obj_type == 'user' else 'user' ] , set_true = [ 'target_team' if res_type == 'team' else res_type ] ) else : self . set_display_columns ( set_false = [ 'user' , 'team' ] , set_true = [ 'resource_name' , 'resource_type' ] ) if 'results' in data : for i in range ( len ( data [ 'results' ] ) ) : self . populate_resource_columns ( data [ 'results' ] [ i ] ) else : self . populate_resource_columns ( data )
1533	def get_scheduler_location ( self , topologyName , callback = None ) : if callback : self . scheduler_location_watchers [ topologyName ] . append ( callback ) else : scheduler_location_path = self . get_scheduler_location_path ( topologyName ) with open ( scheduler_location_path ) as f : data = f . read ( ) scheduler_location = SchedulerLocation ( ) scheduler_location . ParseFromString ( data ) return scheduler_location
5779	def _advapi32_decrypt ( private_key , ciphertext , rsa_oaep_padding = False ) : flags = 0 if rsa_oaep_padding : flags = Advapi32Const . CRYPT_OAEP ciphertext = ciphertext [ : : - 1 ] buffer = buffer_from_bytes ( ciphertext ) out_len = new ( advapi32 , 'DWORD *' , len ( ciphertext ) ) res = advapi32 . CryptDecrypt ( private_key . ex_key_handle , null ( ) , True , flags , buffer , out_len ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
1698	def union ( self , other_streamlet ) : from heronpy . streamlet . impl . unionbolt import UnionStreamlet union_streamlet = UnionStreamlet ( self , other_streamlet ) self . _add_child ( union_streamlet ) other_streamlet . _add_child ( union_streamlet ) return union_streamlet
539	def _finalize ( self ) : self . _logger . info ( "Finished: modelID=%r; %r records processed. Performing final activities" , self . _modelID , self . _currentRecordIndex + 1 ) self . _updateModelDBResults ( ) if not self . _isKilled : self . __updateJobResults ( ) else : self . __deleteOutputCache ( self . _modelID ) if self . _predictionLogger : self . _predictionLogger . close ( ) if self . _inputSource : self . _inputSource . close ( )
3069	def wrap_http_for_jwt_access ( credentials , http ) : orig_request_method = http . request wrap_http_for_auth ( credentials , http ) authenticated_request_method = http . request def new_request ( uri , method = 'GET' , body = None , headers = None , redirections = httplib2 . DEFAULT_MAX_REDIRECTS , connection_type = None ) : if 'aud' in credentials . _kwargs : if ( credentials . access_token is None or credentials . access_token_expired ) : credentials . refresh ( None ) return request ( authenticated_request_method , uri , method , body , headers , redirections , connection_type ) else : headers = _initialize_headers ( headers ) _apply_user_agent ( headers , credentials . user_agent ) uri_root = uri . split ( '?' , 1 ) [ 0 ] token , unused_expiry = credentials . _create_token ( { 'aud' : uri_root } ) headers [ 'Authorization' ] = 'Bearer ' + token return request ( orig_request_method , uri , method , body , clean_headers ( headers ) , redirections , connection_type ) http . request = new_request http . request . credentials = credentials
1069	def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break
6993	def flare_model_residual ( flareparams , times , mags , errs ) : modelmags , _ , _ , _ = flare_model ( flareparams , times , mags , errs ) return ( mags - modelmags ) / errs
12045	def originFormat ( thing ) : if type ( thing ) is list and type ( thing [ 0 ] ) is dict : return originFormat_listOfDicts ( thing ) if type ( thing ) is list and type ( thing [ 0 ] ) is list : return originFormat_listOfDicts ( dictFlat ( thing ) ) else : print ( " !! I don't know how to format this object!" ) print ( thing )
10837	def zjitter ( jitter = 0.0 , radius = 5 ) : psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) sl = np . s_ [ s0 . pad : - s0 . pad , s0 . pad : - s0 . pad , s0 . pad : - s0 . pad ] finalimage = 0 * s0 . get_model_image ( ) [ sl ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( finalimage . shape [ 0 ] ) : offset = jitter * np . random . randn ( 3 ) * np . array ( [ 1 , 0 , 0 ] ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage [ i ] = s0 . get_model_image ( ) [ sl ] [ i ] position += s0 . obj . pos [ 0 ] position /= float ( finalimage . shape [ 0 ] ) s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) return s , finalimage , position
4221	def set_keyring ( keyring ) : global _keyring_backend if not isinstance ( keyring , backend . KeyringBackend ) : raise TypeError ( "The keyring must be a subclass of KeyringBackend" ) _keyring_backend = keyring
12748	def load_skel ( self , source , ** kwargs ) : logging . info ( '%s: parsing skeleton configuration' , source ) if hasattr ( source , 'read' ) : p = parser . parse ( source , self . world , self . jointgroup , ** kwargs ) else : with open ( source ) as handle : p = parser . parse ( handle , self . world , self . jointgroup , ** kwargs ) self . bodies = p . bodies self . joints = p . joints self . set_pid_params ( kp = 0.999 / self . world . dt )
6563	def load_cnf ( fp ) : fp = iter ( fp ) csp = ConstraintSatisfactionProblem ( dimod . BINARY ) num_clauses = num_variables = 0 problem_pattern = re . compile ( _PROBLEM_REGEX ) for line in fp : matches = problem_pattern . findall ( line ) if matches : if len ( matches ) > 1 : raise ValueError nv , nc = matches [ 0 ] num_variables , num_clauses = int ( nv ) , int ( nc ) break clause_pattern = re . compile ( _CLAUSE_REGEX ) for line in fp : if clause_pattern . match ( line ) is not None : clause = [ int ( v ) for v in line . split ( ' ' ) [ : - 1 ] ] variables = [ abs ( v ) for v in clause ] f = _cnf_or ( clause ) csp . add_constraint ( f , variables ) for v in range ( 1 , num_variables + 1 ) : csp . add_variable ( v ) for v in csp . variables : if v > num_variables : msg = ( "given .cnf file's header defines variables [1, {}] and {} clauses " "but constraints a reference to variable {}" ) . format ( num_variables , num_clauses , v ) raise ValueError ( msg ) if len ( csp ) != num_clauses : msg = ( "given .cnf file's header defines {} " "clauses but the file contains {}" ) . format ( num_clauses , len ( csp ) ) raise ValueError ( msg ) return csp
8105	def callback ( self , * incoming ) : message = incoming [ 0 ] if message : address , command = message [ 0 ] , message [ 2 ] profile = self . get_profile ( address ) if profile is not None : try : getattr ( profile , command ) ( self , message ) except AttributeError : pass
9712	def heappushpop_max ( heap , item ) : if heap and heap [ 0 ] > item : item , heap [ 0 ] = heap [ 0 ] , item _siftup_max ( heap , 0 ) return item
7128	def inv_entry_to_path ( data ) : path_tuple = data [ 2 ] . split ( "#" ) if len ( path_tuple ) > 1 : path_str = "#" . join ( ( path_tuple [ 0 ] , path_tuple [ - 1 ] ) ) else : path_str = data [ 2 ] return path_str
9477	def parse_dom ( dom ) : root = dom . getElementsByTagName ( "graphml" ) [ 0 ] graph = root . getElementsByTagName ( "graph" ) [ 0 ] name = graph . getAttribute ( 'id' ) g = Graph ( name ) for node in graph . getElementsByTagName ( "node" ) : n = g . add_node ( id = node . getAttribute ( 'id' ) ) for attr in node . getElementsByTagName ( "data" ) : if attr . firstChild : n [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : n [ attr . getAttribute ( "key" ) ] = "" for edge in graph . getElementsByTagName ( "edge" ) : source = edge . getAttribute ( 'source' ) dest = edge . getAttribute ( 'target' ) e = g . add_edge_by_id ( source , dest ) for attr in edge . getElementsByTagName ( "data" ) : if attr . firstChild : e [ attr . getAttribute ( "key" ) ] = attr . firstChild . data else : e [ attr . getAttribute ( "key" ) ] = "" return g
526	def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]
6300	def get_dirs ( self ) -> List [ str ] : for package in self . packages : yield os . path . join ( package . path , 'resources' )
13888	def DeleteDirectory ( directory , skip_on_error = False ) : _AssertIsLocal ( directory ) import shutil def OnError ( fn , path , excinfo ) : if IsLink ( path ) : return if fn is os . remove and os . access ( path , os . W_OK ) : raise import stat os . chmod ( path , stat . S_IWRITE ) fn ( path ) try : if not os . path . isdir ( directory ) : if skip_on_error : return from . _exceptions import DirectoryNotFoundError raise DirectoryNotFoundError ( directory ) shutil . rmtree ( directory , onerror = OnError ) except : if not skip_on_error : raise
8859	def calltips ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) signatures = script . call_signatures ( ) for sig in signatures : results = ( str ( sig . module_name ) , str ( sig . name ) , [ p . description for p in sig . params ] , sig . index , sig . bracket_start , column ) return results return [ ]
12840	async def async_connect ( self ) : if self . _waiters is None : raise Exception ( 'Error, database not properly initialized before async connection' ) if self . _waiters or self . max_connections and ( len ( self . _in_use ) >= self . max_connections ) : waiter = asyncio . Future ( loop = self . _loop ) self . _waiters . append ( waiter ) try : logger . debug ( 'Wait for connection.' ) await waiter finally : self . _waiters . remove ( waiter ) self . connect ( ) return self . _state . conn
8529	def report ( self ) : self . _output . write ( '\r' ) sort_by = 'avg' results = { } for key , latencies in self . _latencies_by_method . items ( ) : result = { } result [ 'count' ] = len ( latencies ) result [ 'avg' ] = sum ( latencies ) / len ( latencies ) result [ 'min' ] = min ( latencies ) result [ 'max' ] = max ( latencies ) latencies = sorted ( latencies ) result [ 'p90' ] = percentile ( latencies , 0.90 ) result [ 'p95' ] = percentile ( latencies , 0.95 ) result [ 'p99' ] = percentile ( latencies , 0.99 ) result [ 'p999' ] = percentile ( latencies , 0.999 ) results [ key ] = result headers = [ 'method' , 'count' , 'avg' , 'min' , 'max' , 'p90' , 'p95' , 'p99' , 'p999' ] data = [ ] results = sorted ( results . items ( ) , key = lambda it : it [ 1 ] [ sort_by ] , reverse = True ) def row ( key , res ) : data = [ key ] + [ res [ header ] for header in headers [ 1 : ] ] return tuple ( data ) data = [ row ( key , result ) for key , result in results ] self . _output . write ( '%s\n' % tabulate ( data , headers = headers ) ) self . _output . flush ( )
7693	def _check_authorization ( self , properties , stream ) : authzid = properties . get ( "authzid" ) if not authzid : return True try : jid = JID ( authzid ) except ValueError : return False if "username" not in properties : result = False elif jid . local != properties [ "username" ] : result = False elif jid . domain != stream . me . domain : result = False elif jid . resource : result = False else : result = True return result
5757	def get_homogeneous ( package_descriptors , targets , repos_data ) : homogeneous = { } for package_descriptor in package_descriptors . values ( ) : pkg_name = package_descriptor . pkg_name debian_pkg_name = package_descriptor . debian_pkg_name versions = [ ] for repo_data in repos_data : versions . append ( set ( [ ] ) ) for target in targets : version = _strip_version_suffix ( repo_data . get ( target , { } ) . get ( debian_pkg_name , None ) ) versions [ - 1 ] . add ( version ) homogeneous [ pkg_name ] = max ( [ len ( v ) for v in versions ] ) == 1 return homogeneous
12775	def inverse_dynamics ( self , angles , start = 0 , end = 1e100 , states = None , max_force = 100 ) : if states is not None : self . skeleton . set_body_states ( states ) for frame_no , frame in enumerate ( angles ) : if frame_no < start : continue if frame_no >= end : break self . ode_space . collide ( None , self . on_collision ) states = self . skeleton . get_body_states ( ) self . skeleton . set_body_states ( states ) self . skeleton . enable_motors ( max_force ) self . skeleton . set_target_angles ( angles [ frame_no ] ) self . ode_world . step ( self . dt ) torques = self . skeleton . joint_torques self . skeleton . disable_motors ( ) self . skeleton . set_body_states ( states ) self . skeleton . add_torques ( torques ) yield torques self . ode_world . step ( self . dt ) self . ode_contactgroup . empty ( )
3576	def initialize ( self ) : self . _central_manager = CBCentralManager . alloc ( ) self . _central_manager . initWithDelegate_queue_options_ ( self . _central_delegate , None , None )
1067	def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = "%s\n %s" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( ":" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result
3896	def generate_enum_doc ( enum_descriptor , locations , path , name_prefix = '' ) : print ( make_subsection ( name_prefix + enum_descriptor . name ) ) location = locations [ path ] if location . HasField ( 'leading_comments' ) : print ( textwrap . dedent ( location . leading_comments ) ) row_tuples = [ ] for value_index , value in enumerate ( enum_descriptor . value ) : field_location = locations [ path + ( 2 , value_index ) ] row_tuples . append ( ( make_code ( value . name ) , value . number , textwrap . fill ( get_comment_from_location ( field_location ) , INFINITY ) , ) ) print_table ( ( 'Name' , 'Number' , 'Description' ) , row_tuples )
4065	def fields_types ( self , tname , qstring , itemtype ) : template_name = tname + itemtype query_string = qstring . format ( i = itemtype ) if self . templates . get ( template_name ) and not self . _updated ( query_string , self . templates [ template_name ] , template_name ) : return self . templates [ template_name ] [ "tmplt" ] retrieved = self . _retrieve_data ( query_string ) return self . _cache ( retrieved , template_name )
6414	def aghmean ( nums ) : m_a = amean ( nums ) m_g = gmean ( nums ) m_h = hmean ( nums ) if math . isnan ( m_a ) or math . isnan ( m_g ) or math . isnan ( m_h ) : return float ( 'nan' ) while round ( m_a , 12 ) != round ( m_g , 12 ) and round ( m_g , 12 ) != round ( m_h , 12 ) : m_a , m_g , m_h = ( ( m_a + m_g + m_h ) / 3 , ( m_a * m_g * m_h ) ** ( 1 / 3 ) , 3 / ( 1 / m_a + 1 / m_g + 1 / m_h ) , ) return m_a
255	def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips
11202	def strip_comments ( string , comment_symbols = frozenset ( ( '#' , '//' ) ) ) : lines = string . splitlines ( ) for k in range ( len ( lines ) ) : for symbol in comment_symbols : lines [ k ] = strip_comment_line_with_symbol ( lines [ k ] , start = symbol ) return '\n' . join ( lines )
828	def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( "Field name %s not found in this encoder" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )
304	def show_worst_drawdown_periods ( returns , top = 5 ) : drawdown_df = timeseries . gen_drawdown_table ( returns , top = top ) utils . print_table ( drawdown_df . sort_values ( 'Net drawdown in %' , ascending = False ) , name = 'Worst drawdown periods' , float_format = '{0:.2f}' . format , )
783	def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]
10323	def spanning_2d_grid ( length ) : ret = nx . grid_2d_graph ( length + 2 , length ) for i in range ( length ) : ret . node [ ( 0 , i ) ] [ 'span' ] = 0 ret [ ( 0 , i ) ] [ ( 1 , i ) ] [ 'span' ] = 0 ret . node [ ( length + 1 , i ) ] [ 'span' ] = 1 ret [ ( length + 1 , i ) ] [ ( length , i ) ] [ 'span' ] = 1 return ret
7366	def run_multiple_commands_redirect_stdout ( multiple_args_dict , print_commands = True , process_limit = - 1 , polling_freq = 0.5 , ** kwargs ) : assert len ( multiple_args_dict ) > 0 assert all ( len ( args ) > 0 for args in multiple_args_dict . values ( ) ) assert all ( hasattr ( f , 'name' ) for f in multiple_args_dict . keys ( ) ) if process_limit < 0 : logger . debug ( "Using %d processes" % cpu_count ( ) ) process_limit = cpu_count ( ) start_time = time . time ( ) processes = Queue ( maxsize = process_limit ) def add_to_queue ( process ) : process . start ( ) if print_commands : handler = logging . FileHandler ( process . redirect_stdout_file . name ) handler . setLevel ( logging . DEBUG ) logger . addHandler ( handler ) logger . debug ( " " . join ( process . args ) ) logger . removeHandler ( handler ) processes . put ( process ) for f , args in multiple_args_dict . items ( ) : p = AsyncProcess ( args , redirect_stdout_file = f , ** kwargs ) if not processes . full ( ) : add_to_queue ( p ) else : while processes . full ( ) : to_remove = [ ] for possibly_done in processes . queue : if possibly_done . poll ( ) is not None : possibly_done . wait ( ) to_remove . append ( possibly_done ) if to_remove : for process_to_remove in to_remove : processes . queue . remove ( process_to_remove ) break time . sleep ( polling_freq ) add_to_queue ( p ) while not processes . empty ( ) : processes . get ( ) . wait ( ) elapsed_time = time . time ( ) - start_time logger . info ( "Ran %d commands in %0.4f seconds" , len ( multiple_args_dict ) , elapsed_time )
9736	def get_3d_markers_residual ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionResidual , component_info , data , component_position )
13471	def apply_changesets ( args , changesets , catalog ) : tmpdir = tempfile . mkdtemp ( ) tmp_patch = join ( tmpdir , "tmp.patch" ) tmp_lcat = join ( tmpdir , "tmp.lcat" ) for node in changesets : remove ( tmp_patch ) copy ( node . mfile [ 'changeset' ] [ 'filename' ] , tmp_patch ) logging . info ( "mv %s %s" % ( catalog , tmp_lcat ) ) shutil . move ( catalog , tmp_lcat ) cmd = args . patch_cmd . replace ( "$in1" , tmp_lcat ) . replace ( "$patch" , tmp_patch ) . replace ( "$out" , catalog ) logging . info ( "Patch: %s" % cmd ) subprocess . check_call ( cmd , shell = True ) shutil . rmtree ( tmpdir , ignore_errors = True )
9710	def heapreplace_max ( heap , item ) : returnitem = heap [ 0 ] heap [ 0 ] = item _siftup_max ( heap , 0 ) return returnitem
11379	def extract_oembeds ( text , args = None ) : resource_type = width = height = None if args : dimensions = args . lower ( ) . split ( 'x' ) if len ( dimensions ) in ( 3 , 1 ) : resource_type = dimensions . pop ( ) if len ( dimensions ) == 2 : width , height = map ( lambda x : int ( x ) , dimensions ) client = OEmbedConsumer ( ) return client . extract ( text , width , height , resource_type )
10773	def add_node ( self , node , offset ) : width = self . end [ 0 ] - self . start [ 0 ] height = self . end [ 1 ] - self . start [ 1 ] node . x = self . start [ 0 ] + ( width * offset ) node . y = self . start [ 1 ] + ( height * offset ) self . nodes [ node . ID ] = node
11265	def readline ( prev , filename = None , mode = 'r' , trim = str . rstrip , start = 1 , end = sys . maxsize ) : if prev is None : if filename is None : raise Exception ( 'No input available for readline.' ) elif is_str_type ( filename ) : file_list = [ filename , ] else : file_list = filename else : file_list = prev for fn in file_list : if isinstance ( fn , file_type ) : fd = fn else : fd = open ( fn , mode ) try : if start <= 1 and end == sys . maxsize : for line in fd : yield trim ( line ) else : for line_no , line in enumerate ( fd , 1 ) : if line_no < start : continue yield trim ( line ) if line_no >= end : break finally : if fd != fn : fd . close ( )
7258	def search_address ( self , address , filters = None , startDate = None , endDate = None , types = None ) : lat , lng = self . get_address_coords ( address ) return self . search_point ( lat , lng , filters = filters , startDate = startDate , endDate = endDate , types = types )
1623	def FindEndOfExpressionInLine ( line , startpos , stack ) : for i in xrange ( startpos , len ( line ) ) : char = line [ i ] if char in '([{' : stack . append ( char ) elif char == '<' : if i > 0 and line [ i - 1 ] == '<' : if stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) elif i > 0 and Search ( r'\boperator\s*$' , line [ 0 : i ] ) : continue else : stack . append ( '<' ) elif char in ')]}' : while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) if ( ( stack [ - 1 ] == '(' and char == ')' ) or ( stack [ - 1 ] == '[' and char == ']' ) or ( stack [ - 1 ] == '{' and char == '}' ) ) : stack . pop ( ) if not stack : return ( i + 1 , None ) else : return ( - 1 , None ) elif char == '>' : if ( i > 0 and ( line [ i - 1 ] == '-' or Search ( r'\boperator\s*$' , line [ 0 : i - 1 ] ) ) ) : continue if stack : if stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( i + 1 , None ) elif char == ';' : while stack and stack [ - 1 ] == '<' : stack . pop ( ) if not stack : return ( - 1 , None ) return ( - 1 , stack )
6059	def numpy_array_2d_to_fits ( array_2d , file_path , overwrite = False ) : if overwrite and os . path . exists ( file_path ) : os . remove ( file_path ) new_hdr = fits . Header ( ) hdu = fits . PrimaryHDU ( np . flipud ( array_2d ) , new_hdr ) hdu . writeto ( file_path )
10651	def get_activity ( self , name ) : return [ a for a in self . activities if a . name == name ] [ 0 ]
3429	def add_boundary ( self , metabolite , type = "exchange" , reaction_id = None , lb = None , ub = None , sbo_term = None ) : ub = CONFIGURATION . upper_bound if ub is None else ub lb = CONFIGURATION . lower_bound if lb is None else lb types = { "exchange" : ( "EX" , lb , ub , sbo_terms [ "exchange" ] ) , "demand" : ( "DM" , 0 , ub , sbo_terms [ "demand" ] ) , "sink" : ( "SK" , lb , ub , sbo_terms [ "sink" ] ) } if type == "exchange" : external = find_external_compartment ( self ) if metabolite . compartment != external : raise ValueError ( "The metabolite is not an external metabolite" " (compartment is `%s` but should be `%s`). " "Did you mean to add a demand or sink? " "If not, either change its compartment or " "rename the model compartments to fix this." % ( metabolite . compartment , external ) ) if type in types : prefix , lb , ub , default_term = types [ type ] if reaction_id is None : reaction_id = "{}_{}" . format ( prefix , metabolite . id ) if sbo_term is None : sbo_term = default_term if reaction_id is None : raise ValueError ( "Custom types of boundary reactions require a custom " "identifier. Please set the `reaction_id`." ) if reaction_id in self . reactions : raise ValueError ( "Boundary reaction '{}' already exists." . format ( reaction_id ) ) name = "{} {}" . format ( metabolite . name , type ) rxn = Reaction ( id = reaction_id , name = name , lower_bound = lb , upper_bound = ub ) rxn . add_metabolites ( { metabolite : - 1 } ) if sbo_term : rxn . annotation [ "sbo" ] = sbo_term self . add_reactions ( [ rxn ] ) return rxn
6490	def _process_filters ( filter_dictionary ) : def filter_item ( field ) : if filter_dictionary [ field ] is not None : return { "or" : [ _get_filter_field ( field , filter_dictionary [ field ] ) , { "missing" : { "field" : field } } ] } return { "missing" : { "field" : field } } return [ filter_item ( field ) for field in filter_dictionary ]
11787	def add ( self , o ) : "Add an observation o to the distribution." self . smooth_for ( o ) self . dictionary [ o ] += 1 self . n_obs += 1 self . sampler = None
7154	def prepare_options ( options ) : options_ , verbose_options = [ ] , [ ] for option in options : if is_string ( option ) : options_ . append ( option ) verbose_options . append ( option ) else : options_ . append ( option [ 0 ] ) verbose_options . append ( option [ 1 ] ) return options_ , verbose_options
1353	def make_error_response ( self , message ) : response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) response [ constants . RESPONSE_KEY_MESSAGE ] = message return response
10970	def get_group_name ( id_group ) : group = Group . query . get ( id_group ) if group is not None : return group . name
4278	def process_dir ( self , album , force = False ) : for f in album : if isfile ( f . dst_path ) and not force : self . logger . info ( "%s exists - skipping" , f . filename ) self . stats [ f . type + '_skipped' ] += 1 else : self . stats [ f . type ] += 1 yield ( f . type , f . path , f . filename , f . src_path , album . dst_path , self . settings )
315	def gross_lev ( positions ) : exposure = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) return exposure / positions . sum ( axis = 1 )
9898	def _initfile ( path , data = "dict" ) : data = { } if data . lower ( ) == "dict" else [ ] if not os . path . exists ( path ) : dirname = os . path . dirname ( path ) if dirname and not os . path . exists ( dirname ) : raise IOError ( ( "Could not initialize empty JSON file in non-existant " "directory '{}'" ) . format ( os . path . dirname ( path ) ) ) with open ( path , "w" ) as f : json . dump ( data , f ) return True elif os . path . getsize ( path ) == 0 : with open ( path , "w" ) as f : json . dump ( data , f ) else : return False
615	def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , "Inference Type '%s' needs a predictedField " "specified in the inferenceArgs dictionary" % inferenceType if optionInferenceArgs : if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( "'$REPLACE_ME'" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField
7645	def pitch_hz_to_contour ( annotation ) : annotation . namespace = 'pitch_contour' data = annotation . pop_data ( ) for obs in data : annotation . append ( time = obs . time , duration = obs . duration , confidence = obs . confidence , value = dict ( index = 0 , frequency = np . abs ( obs . value ) , voiced = obs . value > 0 ) ) return annotation
12526	def condor_submit ( cmd ) : is_running = subprocess . call ( 'condor_status' , shell = True ) == 0 if not is_running : raise CalledProcessError ( 'HTCondor is not running.' ) sub_cmd = 'condor_qsub -shell n -b y -r y -N ' + cmd . split ( ) [ 0 ] + ' -m n' log . info ( 'Calling: ' + sub_cmd ) return subprocess . call ( sub_cmd + ' ' + cmd , shell = True )
974	def mapBucketIndexToNonZeroBits ( self , index ) : if index < 0 : index = 0 if index >= self . _maxBuckets : index = self . _maxBuckets - 1 if not self . bucketMap . has_key ( index ) : if self . verbosity >= 2 : print "Adding additional buckets to handle index=" , index self . _createBucket ( index ) return self . bucketMap [ index ]
1551	def _get_bolt ( self ) : bolt = topology_pb2 . Bolt ( ) bolt . comp . CopyFrom ( self . _get_base_component ( ) ) self . _add_in_streams ( bolt ) self . _add_out_streams ( bolt ) return bolt
5418	def format_logging_uri ( uri , job_metadata , task_metadata ) : fmt = str ( uri ) if '{' not in fmt : if uri . endswith ( '.log' ) : fmt = os . path . splitext ( uri ) [ 0 ] else : fmt = os . path . join ( uri , '{job-id}' ) if task_metadata . get ( 'task-id' ) is not None : fmt += '.{task-id}' if task_metadata . get ( 'task-attempt' ) is not None : fmt += '.{task-attempt}' fmt += '.log' return _format_task_uri ( fmt , job_metadata , task_metadata )
1711	def _set_name ( self , name ) : if self . own . get ( 'name' ) : self . func_name = name self . own [ 'name' ] [ 'value' ] = Js ( name )
12057	def version_upload ( fname , username = "nibjb" ) : print ( "popping up pasword window..." ) password = TK_askPassword ( "FTP LOGIN" , "enter password for %s" % username ) if not password : return print ( "username:" , username ) print ( "password:" , "*" * ( len ( password ) ) ) print ( "connecting..." ) ftp = ftplib . FTP ( "swharden.com" ) ftp . login ( username , password ) print ( "successful login!" ) ftp . cwd ( "/software/swhlab/versions" ) print ( "uploading" , os . path . basename ( fname ) ) ftp . storbinary ( "STOR " + os . path . basename ( fname ) , open ( fname , "rb" ) , 1024 ) print ( "disconnecting..." ) ftp . quit ( )
8323	def isList ( l ) : return hasattr ( l , '__iter__' ) or ( type ( l ) in ( types . ListType , types . TupleType ) )
8910	def owsproxy_delegate ( request ) : twitcher_url = request . registry . settings . get ( 'twitcher.url' ) protected_path = request . registry . settings . get ( 'twitcher.ows_proxy_protected_path' , '/ows' ) url = twitcher_url + protected_path + '/proxy' if request . matchdict . get ( 'service_name' ) : url += '/' + request . matchdict . get ( 'service_name' ) if request . matchdict . get ( 'access_token' ) : url += '/' + request . matchdict . get ( 'service_name' ) url += '?' + urlparse . urlencode ( request . params ) LOGGER . debug ( "delegate to owsproxy: %s" , url ) resp = requests . request ( method = request . method . upper ( ) , url = url , data = request . body , headers = request . headers , verify = False ) return Response ( resp . content , status = resp . status_code , headers = resp . headers )
7486	def run_cutadapt ( data , subsamples , lbview ) : start = time . time ( ) printstr = " processing reads | {} | s2 |" finished = 0 rawedits = { } subsamples . sort ( key = lambda x : x . stats . reads_raw , reverse = True ) LOGGER . info ( [ i . stats . reads_raw for i in subsamples ] ) if "pair" in data . paramsdict [ "datatype" ] : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_pairs , * ( data , sample ) ) else : for sample in subsamples : rawedits [ sample . name ] = lbview . apply ( cutadaptit_single , * ( data , sample ) ) while 1 : finished = sum ( [ i . ready ( ) for i in rawedits . values ( ) ] ) elapsed = datetime . timedelta ( seconds = int ( time . time ( ) - start ) ) progressbar ( len ( rawedits ) , finished , printstr . format ( elapsed ) , spacer = data . _spacer ) time . sleep ( 0.1 ) if finished == len ( rawedits ) : print ( "" ) break for async in rawedits : if rawedits [ async ] . successful ( ) : res = rawedits [ async ] . result ( ) if "pair" not in data . paramsdict [ "datatype" ] : parse_single_results ( data , data . samples [ async ] , res ) else : parse_pair_results ( data , data . samples [ async ] , res ) else : print ( " found an error in step2; see ipyrad_log.txt" ) LOGGER . error ( "error in run_cutadapt(): %s" , rawedits [ async ] . exception ( ) )
7910	def __presence_unavailable ( self , stanza ) : fr = stanza . get_from ( ) key = fr . bare ( ) . as_unicode ( ) rs = self . rooms . get ( key ) if not rs : return False rs . process_unavailable_presence ( MucPresence ( stanza ) ) return True
5255	def disassemble_one ( bytecode , pc = 0 , fork = DEFAULT_FORK ) : instruction_table = instruction_tables [ fork ] if isinstance ( bytecode , bytes ) : bytecode = bytearray ( bytecode ) if isinstance ( bytecode , str ) : bytecode = bytearray ( bytecode . encode ( 'latin-1' ) ) bytecode = iter ( bytecode ) try : opcode = next ( bytecode ) except StopIteration : return assert isinstance ( opcode , int ) instruction = copy . copy ( instruction_table . get ( opcode , None ) ) if instruction is None : instruction = Instruction ( opcode , 'INVALID' , 0 , 0 , 0 , 0 , 'Unspecified invalid instruction.' ) instruction . pc = pc try : if instruction . has_operand : instruction . parse_operand ( bytecode ) except ParseError : instruction = None finally : return instruction
4404	def lock ( self , name , timeout = None , sleep = 0.1 ) : return Lock ( self , name , timeout = timeout , sleep = sleep )
6354	def _apply_rule_if_compat ( self , phonetic , target , language_arg ) : candidate = phonetic + target if '[' not in candidate : return candidate candidate = self . _expand_alternates ( candidate ) candidate_array = candidate . split ( '|' ) candidate = '' found = False for i in range ( len ( candidate_array ) ) : this_candidate = candidate_array [ i ] if language_arg != 1 : this_candidate = self . _normalize_lang_attrs ( this_candidate + '[' + str ( language_arg ) + ']' , False ) if this_candidate != '[0]' : found = True if candidate : candidate += '|' candidate += this_candidate if not found : return None if '|' in candidate : candidate = '(' + candidate + ')' return candidate
13667	def execute ( self , command , timeout = None ) : try : self . channel = self . ssh . get_transport ( ) . open_session ( ) except paramiko . SSHException as e : self . unknown ( "Create channel error: %s" % e ) try : self . channel . settimeout ( self . args . timeout if not timeout else timeout ) except socket . timeout as e : self . unknown ( "Settimeout for channel error: %s" % e ) try : self . logger . debug ( "command: {}" . format ( command ) ) self . channel . exec_command ( command ) except paramiko . SSHException as e : self . unknown ( "Execute command error: %s" % e ) try : self . stdin = self . channel . makefile ( 'wb' , - 1 ) self . stderr = map ( string . strip , self . channel . makefile_stderr ( 'rb' , - 1 ) . readlines ( ) ) self . stdout = map ( string . strip , self . channel . makefile ( 'rb' , - 1 ) . readlines ( ) ) except Exception as e : self . unknown ( "Get result error: %s" % e ) try : self . status = self . channel . recv_exit_status ( ) except paramiko . SSHException as e : self . unknown ( "Get return code error: %s" % e ) else : if self . status != 0 : self . unknown ( "Return code: %d , stderr: %s" % ( self . status , self . errors ) ) else : return self . stdout finally : self . logger . debug ( "Execute command finish." )
13146	def remove_direct_link_triples ( train , valid , test ) : pairs = set ( ) merged = valid + test for t in merged : pairs . add ( ( t . head , t . tail ) ) filtered = filterfalse ( lambda t : ( t . head , t . tail ) in pairs or ( t . tail , t . head ) in pairs , train ) return list ( filtered )
5058	def get_notification_subject_line ( course_name , template_configuration = None ) : stock_subject_template = _ ( 'You\'ve been enrolled in {course_name}!' ) default_subject_template = getattr ( settings , 'ENTERPRISE_ENROLLMENT_EMAIL_DEFAULT_SUBJECT_LINE' , stock_subject_template , ) if template_configuration is not None and template_configuration . subject_line : final_subject_template = template_configuration . subject_line else : final_subject_template = default_subject_template try : return final_subject_template . format ( course_name = course_name ) except KeyError : pass try : return default_subject_template . format ( course_name = course_name ) except KeyError : return stock_subject_template . format ( course_name = course_name )
8081	def rellineto ( self , x , y ) : if self . _path is None : raise ShoebotError ( _ ( "No current path. Use beginpath() first." ) ) self . _path . rellineto ( x , y )
10303	def tanimoto_set_similarity ( x : Iterable [ X ] , y : Iterable [ X ] ) -> float : a , b = set ( x ) , set ( y ) union = a | b if not union : return 0.0 return len ( a & b ) / len ( union )
3633	def cardInfo ( self , resource_id ) : base_id = baseId ( resource_id ) if base_id in self . players : return self . players [ base_id ] else : url = '{0}{1}.json' . format ( card_info_url , base_id ) return requests . get ( url , timeout = self . timeout ) . json ( )
11828	def exact_sqrt ( n2 ) : "If n2 is a perfect square, return its square root, else raise error." n = int ( math . sqrt ( n2 ) ) assert n * n == n2 return n
144	def copy ( self , exterior = None , label = None ) : return self . deepcopy ( exterior = exterior , label = label )
8316	def parse_images ( self , markup , treshold = 6 ) : images = [ ] m = re . findall ( self . re [ "image" ] , markup ) for p in m : p = self . parse_balanced_image ( p ) img = p . split ( "|" ) path = img [ 0 ] . replace ( "[[Image:" , "" ) . strip ( ) description = u"" links = { } properties = [ ] if len ( img ) > 1 : img = "|" . join ( img [ 1 : ] ) links = self . parse_links ( img ) properties = self . plain ( img ) . split ( "|" ) description = u"" if len ( properties [ - 1 ] ) > treshold : description = properties [ - 1 ] properties = properties [ : - 1 ] img = WikipediaImage ( path , description , links , properties ) images . append ( img ) markup = markup . replace ( p , "" ) return images , markup . strip ( )
5504	def relative_datetime ( self ) : now = datetime . now ( timezone . utc ) tense = "from now" if self . created_at > now else "ago" return "{0} {1}" . format ( humanize . naturaldelta ( now - self . created_at ) , tense )
7438	def files ( self ) : nameordered = self . samples . keys ( ) nameordered . sort ( ) return pd . DataFrame ( [ self . samples [ i ] . files for i in nameordered ] , index = nameordered ) . dropna ( axis = 1 , how = 'all' )
8339	def _findAll ( self , name , attrs , text , limit , generator , ** kwargs ) : "Iterates over a generator looking for things that match." if isinstance ( name , SoupStrainer ) : strainer = name else : strainer = SoupStrainer ( name , attrs , text , ** kwargs ) results = ResultSet ( strainer ) g = generator ( ) while True : try : i = g . next ( ) except StopIteration : break if i : found = strainer . search ( i ) if found : results . append ( found ) if limit and len ( results ) >= limit : break return results
6806	def init_raspbian_vm ( self ) : r = self . local_renderer r . comment ( 'Installing system packages.' ) r . sudo ( 'add-apt-repository ppa:linaro-maintainers/tools' ) r . sudo ( 'apt-get update' ) r . sudo ( 'apt-get install libsdl-dev qemu-system' ) r . comment ( 'Download image.' ) r . local ( 'wget https://downloads.raspberrypi.org/raspbian_lite_latest' ) r . local ( 'unzip raspbian_lite_latest.zip' ) r . comment ( 'Find start of the Linux ext4 partition.' ) r . local ( "parted -s 2016-03-18-raspbian-jessie-lite.img unit B print | " "awk '/^Number/{{p=1;next}}; p{{gsub(/[^[:digit:]]/, " ", $2); print $2}}' | sed -n 2p" , assign_to = 'START' ) r . local ( 'mkdir -p {raspbian_mount_point}' ) r . sudo ( 'mount -v -o offset=$START -t ext4 {raspbian_image} $MNT' ) r . comment ( 'Comment out everything in ld.so.preload' ) r . local ( "sed -i 's/^/#/g' {raspbian_mount_point}/etc/ld.so.preload" ) r . comment ( 'Comment out entries containing /dev/mmcblk in fstab.' ) r . local ( "sed -i '/mmcblk/ s?^?#?' /etc/fstab" ) r . sudo ( 'umount {raspbian_mount_point}' ) r . comment ( 'Download kernel.' ) r . local ( 'wget https://github.com/dhruvvyas90/qemu-rpi-kernel/blob/master/{raspbian_kernel}?raw=true' ) r . local ( 'mv {raspbian_kernel} {libvirt_images_dir}' ) r . comment ( 'Creating libvirt machine.' ) r . local ( 'virsh define libvirt-raspbian.xml' ) r . comment ( 'You should now be able to boot the VM by running:' ) r . comment ( '' ) r . comment ( ' qemu-system-arm -kernel {libvirt_boot_dir}/{raspbian_kernel} ' '-cpu arm1176 -m 256 -M versatilepb -serial stdio -append "root=/dev/sda2 rootfstype=ext4 rw" ' '-hda {libvirt_images_dir}/{raspbian_image}' ) r . comment ( '' ) r . comment ( 'Or by running virt-manager.' )
4267	def generate_image ( source , outname , settings , options = None ) : logger = logging . getLogger ( __name__ ) if settings [ 'use_orig' ] or source . endswith ( '.gif' ) : utils . copy ( source , outname , symlink = settings [ 'orig_link' ] ) return img = _read_image ( source ) original_format = img . format if settings [ 'copy_exif_data' ] and settings [ 'autorotate_images' ] : logger . warning ( "The 'autorotate_images' and 'copy_exif_data' settings " "are not compatible because Sigal can't save the " "modified Orientation tag." ) if settings [ 'copy_exif_data' ] and _has_exif_tags ( img ) : if options is not None : options = deepcopy ( options ) else : options = { } options [ 'exif' ] = img . info [ 'exif' ] if settings [ 'autorotate_images' ] : try : img = Transpose ( ) . process ( img ) except ( IOError , IndexError ) : pass if settings [ 'img_processor' ] : try : logger . debug ( 'Processor: %s' , settings [ 'img_processor' ] ) processor_cls = getattr ( pilkit . processors , settings [ 'img_processor' ] ) except AttributeError : logger . error ( 'Wrong processor name: %s' , settings [ 'img_processor' ] ) sys . exit ( ) width , height = settings [ 'img_size' ] if img . size [ 0 ] < img . size [ 1 ] : height , width = width , height processor = processor_cls ( width , height , upscale = False ) img = processor . process ( img ) for receiver in signals . img_resized . receivers_for ( img ) : img = receiver ( img , settings = settings ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save resized image to %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
888	def _destroyMinPermanenceSynapses ( cls , connections , random , segment , nDestroy , excludeCells ) : destroyCandidates = sorted ( ( synapse for synapse in connections . synapsesForSegment ( segment ) if synapse . presynapticCell not in excludeCells ) , key = lambda s : s . _ordinal ) for _ in xrange ( nDestroy ) : if len ( destroyCandidates ) == 0 : break minSynapse = None minPermanence = float ( "inf" ) for synapse in destroyCandidates : if synapse . permanence < minPermanence - EPSILON : minSynapse = synapse minPermanence = synapse . permanence connections . destroySynapse ( minSynapse ) destroyCandidates . remove ( minSynapse )
7670	def add ( self , jam , on_conflict = 'fail' ) : if on_conflict not in [ 'overwrite' , 'fail' , 'ignore' ] : raise ParameterError ( "on_conflict='{}' is not in ['fail', " "'overwrite', 'ignore']." . format ( on_conflict ) ) if not self . file_metadata == jam . file_metadata : if on_conflict == 'overwrite' : self . file_metadata = jam . file_metadata elif on_conflict == 'fail' : raise JamsError ( "Metadata conflict! " "Resolve manually or force-overwrite it." ) self . annotations . extend ( jam . annotations ) self . sandbox . update ( ** jam . sandbox )
9479	def node ( self , node ) : if node == self . node1 : return self . node2 elif node == self . node2 : return self . node1 else : return None
3419	def save_matlab_model ( model , file_name , varname = None ) : if not scipy_io : raise ImportError ( 'load_matlab_model requires scipy' ) if varname is None : varname = str ( model . id ) if model . id is not None and len ( model . id ) > 0 else "exported_model" mat = create_mat_dict ( model ) scipy_io . savemat ( file_name , { varname : mat } , appendmat = True , oned_as = "column" )
11640	def json_get_data ( filename ) : with open ( filename ) as fp : json_data = json . load ( fp ) return json_data return False
3020	def create_with_claims ( self , claims ) : new_kwargs = dict ( self . _kwargs ) new_kwargs . update ( claims ) result = self . __class__ ( self . _service_account_email , self . _signer , scopes = self . _scopes , private_key_id = self . _private_key_id , client_id = self . client_id , user_agent = self . _user_agent , ** new_kwargs ) result . token_uri = self . token_uri result . revoke_uri = self . revoke_uri result . _private_key_pkcs8_pem = self . _private_key_pkcs8_pem result . _private_key_pkcs12 = self . _private_key_pkcs12 result . _private_key_password = self . _private_key_password return result
5840	def check_predict_status ( self , view_id , predict_request_id ) : failure_message = "Get status on predict failed" bare_response = self . _get_success_json ( self . _get ( 'v1/data_views/' + str ( view_id ) + '/predict/' + str ( predict_request_id ) + '/status' , None , failure_message = failure_message ) ) result = bare_response [ "data" ] return result
13055	def overview ( ) : search = Service . search ( ) search = search . filter ( "term" , state = 'open' ) search . aggs . bucket ( 'port_count' , 'terms' , field = 'port' , order = { '_count' : 'desc' } , size = 100 ) . metric ( 'unique_count' , 'cardinality' , field = 'address' ) response = search . execute ( ) print_line ( "Port Count" ) print_line ( "---------------" ) for entry in response . aggregations . port_count . buckets : print_line ( "{0:<7} {1}" . format ( entry . key , entry . unique_count . value ) )
10540	def delete_category ( category_id ) : try : res = _pybossa_req ( 'delete' , 'category' , category_id ) if type ( res ) . __name__ == 'bool' : return True else : return res except : raise
6085	def unmasked_blurred_image_of_planes_from_padded_grid_stack_and_psf ( planes , padded_grid_stack , psf ) : unmasked_blurred_image_of_planes = [ ] for plane in planes : if plane . has_pixelization : unmasked_blurred_image_of_plane = None else : unmasked_blurred_image_of_plane = padded_grid_stack . unmasked_blurred_image_from_psf_and_unmasked_image ( psf = psf , unmasked_image_1d = plane . image_plane_image_1d ) unmasked_blurred_image_of_planes . append ( unmasked_blurred_image_of_plane ) return unmasked_blurred_image_of_planes
4422	async def seek ( self , pos : int ) : await self . _lavalink . ws . send ( op = 'seek' , guildId = self . guild_id , position = pos )
8531	def of_messages ( cls , msg_a , msg_b ) : ok_to_diff , reason = cls . can_diff ( msg_a , msg_b ) if not ok_to_diff : raise ValueError ( reason ) return [ cls . of_structs ( x . value , y . value ) for x , y in zip ( msg_a . args , msg_b . args ) if x . field_type == 'struct' ]
9161	def delete_acl_request ( request ) : uuid_ = request . matchdict [ 'uuid' ] posted = request . json permissions = [ ( x [ 'uid' ] , x [ 'permission' ] , ) for x in posted ] with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : remove_acl ( cursor , uuid_ , permissions ) resp = request . response resp . status_int = 200 return resp
730	def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )
8464	def deploy ( target ) : if not os . getenv ( CIRCLECI_ENV_VAR ) : raise EnvironmentError ( 'Must be on CircleCI to run this script' ) current_branch = os . getenv ( 'CIRCLE_BRANCH' ) if ( target == 'PROD' ) and ( current_branch != 'master' ) : raise EnvironmentError ( ( 'Refusing to deploy to production from branch {current_branch!r}. ' 'Production deploys can only be made from master.' ) . format ( current_branch = current_branch ) ) if target in ( 'PROD' , 'TEST' ) : pypi_username = os . getenv ( '{target}_PYPI_USERNAME' . format ( target = target ) ) pypi_password = os . getenv ( '{target}_PYPI_PASSWORD' . format ( target = target ) ) else : raise ValueError ( "Deploy target must be 'PROD' or 'TEST', got {target!r}." . format ( target = target ) ) if not ( pypi_username and pypi_password ) : raise EnvironmentError ( ( "Missing '{target}_PYPI_USERNAME' and/or '{target}_PYPI_PASSWORD' " "environment variables. These are required to push to PyPI." ) . format ( target = target ) ) os . environ [ 'TWINE_USERNAME' ] = pypi_username os . environ [ 'TWINE_PASSWORD' ] = pypi_password _shell ( 'git config --global user.email "oss@cloverhealth.com"' ) _shell ( 'git config --global user.name "Circle CI"' ) _shell ( 'git config push.default current' ) ret = _shell ( 'make version' , stdout = subprocess . PIPE ) version = ret . stdout . decode ( 'utf-8' ) . strip ( ) print ( 'Deploying version {version!r}...' . format ( version = version ) ) _shell ( 'git tag -f -a {version} -m "Version {version}"' . format ( version = version ) ) _shell ( 'sed -i.bak "s/^__version__ = .*/__version__ = {version!r}/" */version.py' . format ( version = version ) ) _shell ( 'python setup.py sdist bdist_wheel' ) _shell ( 'git add ChangeLog AUTHORS */version.py' ) _shell ( 'git commit --no-verify -m "Merge autogenerated files [skip ci]"' ) _pypi_push ( 'dist' ) _shell ( 'git push --follow-tags' ) print ( 'Deployment complete. Latest version is {version}.' . format ( version = version ) )
13801	def revoke_token ( self , token , callback ) : yield Task ( self . data_store . remove , 'tokens' , token = token ) callback ( )
6856	def query ( query , use_sudo = True , ** kwargs ) : func = use_sudo and run_as_root or run user = kwargs . get ( 'mysql_user' ) or env . get ( 'mysql_user' ) password = kwargs . get ( 'mysql_password' ) or env . get ( 'mysql_password' ) options = [ '--batch' , '--raw' , '--skip-column-names' , ] if user : options . append ( '--user=%s' % quote ( user ) ) if password : options . append ( '--password=%s' % quote ( password ) ) options = ' ' . join ( options ) return func ( 'mysql %(options)s --execute=%(query)s' % { 'options' : options , 'query' : quote ( query ) , } )
12678	def unescape ( escaped , escape_char = ESCAPE_CHAR ) : if isinstance ( escaped , bytes ) : escaped = escaped . decode ( 'utf8' ) escape_pat = re . compile ( re . escape ( escape_char ) . encode ( 'utf8' ) + b'([a-z0-9]{2})' , re . IGNORECASE ) buf = escape_pat . subn ( _unescape_char , escaped . encode ( 'utf8' ) ) [ 0 ] return buf . decode ( 'utf8' )
5807	def parse_session_info ( server_handshake_bytes , client_handshake_bytes ) : protocol = None cipher_suite = None compression = False session_id = None session_ticket = None server_session_id = None client_session_id = None for record_type , _ , record_data in parse_tls_records ( server_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x02' : continue protocol = { b'\x03\x00' : "SSLv3" , b'\x03\x01' : "TLSv1" , b'\x03\x02' : "TLSv1.1" , b'\x03\x03' : "TLSv1.2" , b'\x03\x04' : "TLSv1.3" , } [ message_data [ 0 : 2 ] ] session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : server_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_bytes = message_data [ cipher_suite_start : cipher_suite_start + 2 ] cipher_suite = CIPHER_SUITE_MAP [ cipher_suite_bytes ] compression_start = cipher_suite_start + 2 compression = message_data [ compression_start : compression_start + 1 ] != b'\x00' extensions_length_start = compression_start + 1 extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "new" break break for record_type , _ , record_data in parse_tls_records ( client_handshake_bytes ) : if record_type != b'\x16' : continue for message_type , message_data in parse_handshake_messages ( record_data ) : if message_type != b'\x01' : continue session_id_length = int_from_bytes ( message_data [ 34 : 35 ] ) if session_id_length > 0 : client_session_id = message_data [ 35 : 35 + session_id_length ] cipher_suite_start = 35 + session_id_length cipher_suite_length = int_from_bytes ( message_data [ cipher_suite_start : cipher_suite_start + 2 ] ) compression_start = cipher_suite_start + 2 + cipher_suite_length compression_length = int_from_bytes ( message_data [ compression_start : compression_start + 1 ] ) if server_session_id is None and session_ticket is None : extensions_length_start = compression_start + 1 + compression_length extensions_data = message_data [ extensions_length_start : ] for extension_type , extension_data in _parse_hello_extensions ( extensions_data ) : if extension_type == 35 : session_ticket = "reused" break break if server_session_id is not None : if client_session_id is None : session_id = "new" else : if client_session_id != server_session_id : session_id = "new" else : session_id = "reused" return { "protocol" : protocol , "cipher_suite" : cipher_suite , "compression" : compression , "session_id" : session_id , "session_ticket" : session_ticket , }
1349	def write_error_response ( self , message ) : self . set_status ( 404 ) response = self . make_error_response ( str ( message ) ) now = time . time ( ) spent = now - self . basehandler_starttime response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent self . write_json_response ( response )
7043	def lightcurve_moments ( ftimes , fmags , ferrs ) : ndet = len ( fmags ) if ndet > 9 : series_median = npmedian ( fmags ) series_wmean = ( npsum ( fmags * ( 1.0 / ( ferrs * ferrs ) ) ) / npsum ( 1.0 / ( ferrs * ferrs ) ) ) series_mad = npmedian ( npabs ( fmags - series_median ) ) series_stdev = 1.483 * series_mad series_skew = spskew ( fmags ) series_kurtosis = spkurtosis ( fmags ) series_above1std = len ( fmags [ fmags > ( series_median + series_stdev ) ] ) series_below1std = len ( fmags [ fmags < ( series_median - series_stdev ) ] ) series_beyond1std = ( series_above1std + series_below1std ) / float ( ndet ) series_mag_percentiles = nppercentile ( fmags , [ 5.0 , 10 , 17.5 , 25 , 32.5 , 40 , 60 , 67.5 , 75 , 82.5 , 90 , 95 ] ) return { 'median' : series_median , 'wmean' : series_wmean , 'mad' : series_mad , 'stdev' : series_stdev , 'skew' : series_skew , 'kurtosis' : series_kurtosis , 'beyond1std' : series_beyond1std , 'mag_percentiles' : series_mag_percentiles , 'mag_iqr' : series_mag_percentiles [ 8 ] - series_mag_percentiles [ 3 ] , } else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate light curve moments' ) return None
7054	def _check_extmodule ( module , formatkey ) : try : if os . path . exists ( module ) : sys . path . append ( os . path . dirname ( module ) ) importedok = importlib . import_module ( os . path . basename ( module . replace ( '.py' , '' ) ) ) else : importedok = importlib . import_module ( module ) except Exception as e : LOGEXCEPTION ( 'could not import the module: %s for LC format: %s. ' 'check the file path or fully qualified module name?' % ( module , formatkey ) ) importedok = False return importedok
4322	def channels ( self , n_channels ) : if not isinstance ( n_channels , int ) or n_channels <= 0 : raise ValueError ( 'n_channels must be a positive integer.' ) effect_args = [ 'channels' , '{}' . format ( n_channels ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'channels' ) return self
9280	def passcode ( callsign ) : assert isinstance ( callsign , str ) callsign = callsign . split ( '-' ) [ 0 ] . upper ( ) code = 0x73e2 for i , char in enumerate ( callsign ) : code ^= ord ( char ) << ( 8 if not i % 2 else 0 ) return code & 0x7fff
13502	def extra_context ( request ) : host = os . environ . get ( 'DJANGO_LIVE_TEST_SERVER_ADDRESS' , None ) or request . get_host ( ) d = { 'request' : request , 'HOST' : host , 'IN_ADMIN' : request . path . startswith ( '/admin/' ) , } return d
12957	def _get_key_for_index ( self , indexedField , val ) : if hasattr ( indexedField , 'toIndex' ) : val = indexedField . toIndex ( val ) else : val = self . fields [ indexedField ] . toIndex ( val ) return '' . join ( [ INDEXED_REDIS_PREFIX , self . keyName , ':idx:' , indexedField , ':' , val ] )
6260	def calc_global_bbox ( self , view_matrix , bbox_min , bbox_max ) : if self . matrix is not None : view_matrix = matrix44 . multiply ( self . matrix , view_matrix ) if self . mesh : bbox_min , bbox_max = self . mesh . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) for child in self . children : bbox_min , bbox_max = child . calc_global_bbox ( view_matrix , bbox_min , bbox_max ) return bbox_min , bbox_max
5110	def simulate ( self , n = 1 , t = None , nA = None , nD = None ) : if t is None and nD is None and nA is None : for dummy in range ( n ) : self . next_event ( ) elif t is not None : then = self . _current_t + t while self . _current_t < then and self . _time < infty : self . next_event ( ) elif nD is not None : num_departures = self . num_departures + nD while self . num_departures < num_departures and self . _time < infty : self . next_event ( ) elif nA is not None : num_arrivals = self . _oArrivals + nA while self . _oArrivals < num_arrivals and self . _time < infty : self . next_event ( )
4972	def clean ( self ) : cleaned_data = super ( EnterpriseCustomerReportingConfigAdminForm , self ) . clean ( ) report_customer = cleaned_data . get ( 'enterprise_customer' ) invalid_catalogs = [ '{} ({})' . format ( catalog . title , catalog . uuid ) for catalog in cleaned_data . get ( 'enterprise_customer_catalogs' ) if catalog . enterprise_customer != report_customer ] if invalid_catalogs : message = _ ( 'These catalogs for reporting do not match enterprise' 'customer {enterprise_customer}: {invalid_catalogs}' , ) . format ( enterprise_customer = report_customer , invalid_catalogs = invalid_catalogs , ) self . add_error ( 'enterprise_customer_catalogs' , message )
13430	def create_site ( self , params = { } ) : url = "/2/sites/" body = params data = self . _post_resource ( url , body ) return self . site_from_json ( data [ "site" ] )
10952	def build_funcs ( self ) : def m ( inds = None , slicer = None , flat = True ) : return sample ( self . model , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def r ( inds = None , slicer = None , flat = True ) : return sample ( self . residuals , inds = inds , slicer = slicer , flat = flat ) . copy ( ) def l ( ) : return self . loglikelihood def r_e ( ** kwargs ) : return r ( ** kwargs ) , np . copy ( self . error ) def m_e ( ** kwargs ) : return m ( ** kwargs ) , np . copy ( self . error ) self . fisherinformation = partial ( self . _jtj , funct = m ) self . gradloglikelihood = partial ( self . _grad , funct = l ) self . hessloglikelihood = partial ( self . _hess , funct = l ) self . gradmodel = partial ( self . _grad , funct = m ) self . hessmodel = partial ( self . _hess , funct = m ) self . JTJ = partial ( self . _jtj , funct = r ) self . J = partial ( self . _grad , funct = r ) self . J_e = partial ( self . _grad , funct = r_e , nout = 2 ) self . gradmodel_e = partial ( self . _grad , funct = m_e , nout = 2 ) self . fisherinformation . __doc__ = _graddoc + _sampledoc self . gradloglikelihood . __doc__ = _graddoc self . hessloglikelihood . __doc__ = _graddoc self . gradmodel . __doc__ = _graddoc + _sampledoc self . hessmodel . __doc__ = _graddoc + _sampledoc self . JTJ . __doc__ = _graddoc + _sampledoc self . J . __doc__ = _graddoc + _sampledoc self . _dograddoc ( self . _grad_one_param ) self . _dograddoc ( self . _hess_two_param ) self . _dograddoc ( self . _grad ) self . _dograddoc ( self . _hess ) class _Statewrap ( object ) : def __init__ ( self , obj ) : self . obj = obj def __getitem__ ( self , d = None ) : if d is None : d = self . obj . params return util . delistify ( self . obj . get_values ( d ) , d ) self . state = _Statewrap ( self )
6678	def uncommented_lines ( self , filename , use_sudo = False ) : func = run_as_root if use_sudo else self . run res = func ( 'cat %s' % quote ( filename ) , quiet = True ) if res . succeeded : return [ line for line in res . splitlines ( ) if line and not line . startswith ( '#' ) ] return [ ]
7714	def add_item ( self , jid , name = None , groups = None , callback = None , error_callback = None ) : if jid in self . roster : raise ValueError ( "{0!r} already in the roster" . format ( jid ) ) item = RosterItem ( jid , name , groups ) self . _roster_set ( item , callback , error_callback )
2142	def process_extra_vars ( extra_vars_list , force_json = True ) : extra_vars = { } extra_vars_yaml = "" for extra_vars_opt in extra_vars_list : if extra_vars_opt . startswith ( "@" ) : with open ( extra_vars_opt [ 1 : ] , 'r' ) as f : extra_vars_opt = f . read ( ) opt_dict = string_to_dict ( extra_vars_opt , allow_kv = False ) else : opt_dict = string_to_dict ( extra_vars_opt , allow_kv = True ) if any ( line . startswith ( "#" ) for line in extra_vars_opt . split ( '\n' ) ) : extra_vars_yaml += extra_vars_opt + "\n" elif extra_vars_opt != "" : extra_vars_yaml += yaml . dump ( opt_dict , default_flow_style = False ) + "\n" extra_vars . update ( opt_dict ) if not force_json : try : try_dict = yaml . load ( extra_vars_yaml , Loader = yaml . SafeLoader ) assert type ( try_dict ) is dict debug . log ( 'Using unprocessed YAML' , header = 'decision' , nl = 2 ) return extra_vars_yaml . rstrip ( ) except Exception : debug . log ( 'Failed YAML parsing, defaulting to JSON' , header = 'decison' , nl = 2 ) if extra_vars == { } : return "" return json . dumps ( extra_vars , ensure_ascii = False )
784	def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return
4599	def _clean_animation ( desc , parent ) : desc = load . load_if_filename ( desc ) or desc if isinstance ( desc , str ) : animation = { 'typename' : desc } elif not isinstance ( desc , dict ) : raise TypeError ( 'Unexpected type %s in collection' % type ( desc ) ) elif 'typename' in desc or 'animation' not in desc : animation = desc else : animation = desc . pop ( 'animation' , { } ) if isinstance ( animation , str ) : animation = { 'typename' : animation } animation [ 'run' ] = desc . pop ( 'run' , { } ) if desc : raise ValueError ( 'Extra animation fields: ' + ', ' . join ( desc ) ) animation . setdefault ( 'typename' , DEFAULT_ANIMATION ) animation = construct . to_type_constructor ( animation , ANIMATION_PATH ) datatype = animation . setdefault ( 'datatype' , failed . Failed ) animation . setdefault ( 'name' , datatype . __name__ ) run = animation . setdefault ( 'run' , { } ) run_parent = parent . setdefault ( 'run' , { } ) if not ( 'fps' in run or 'sleep_time' in run ) : if 'fps' in run_parent : run . update ( fps = run_parent [ 'fps' ] ) elif 'sleep_time' in run_parent : run . update ( sleep_time = run_parent [ 'sleep_time' ] ) return animation
13556	def all_comments ( self ) : ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'event' ) update_ctype = ContentType . objects . get ( app_label__exact = "happenings" , model__exact = 'update' ) update_ids = self . update_set . values_list ( 'id' , flat = True ) return Comment . objects . filter ( Q ( content_type = ctype . id , object_pk = self . id ) | Q ( content_type = update_ctype . id , object_pk__in = update_ids ) )
11983	async def upload_file ( self , bucket , file , uploadpath = None , key = None , ContentType = None , ** kw ) : is_filename = False if hasattr ( file , 'read' ) : if hasattr ( file , 'seek' ) : file . seek ( 0 ) file = file . read ( ) size = len ( file ) elif key : size = len ( file ) else : is_filename = True size = os . stat ( file ) . st_size key = os . path . basename ( file ) assert key , 'key not available' if not ContentType : ContentType , _ = mimetypes . guess_type ( key ) if uploadpath : if not uploadpath . endswith ( '/' ) : uploadpath = '%s/' % uploadpath key = '%s%s' % ( uploadpath , key ) params = dict ( Bucket = bucket , Key = key ) if not ContentType : ContentType = 'application/octet-stream' params [ 'ContentType' ] = ContentType if size > MULTI_PART_SIZE and is_filename : resp = await _multipart ( self , file , params ) elif is_filename : with open ( file , 'rb' ) as fp : params [ 'Body' ] = fp . read ( ) resp = await self . put_object ( ** params ) else : params [ 'Body' ] = file resp = await self . put_object ( ** params ) if 'Key' not in resp : resp [ 'Key' ] = key if 'Bucket' not in resp : resp [ 'Bucket' ] = bucket return resp
13807	def __get_current_datetime ( self ) : self . wql_time = "SELECT LocalDateTime FROM Win32_OperatingSystem" self . current_time = self . query ( self . wql_time ) self . current_time_string = str ( self . current_time [ 0 ] . get ( 'LocalDateTime' ) . split ( '.' ) [ 0 ] ) self . current_time_format = datetime . datetime . strptime ( self . current_time_string , '%Y%m%d%H%M%S' ) return self . current_time_format
12935	def _parse_allele_data ( self ) : pref_freq , frequencies = self . _parse_frequencies ( ) info_clnvar_single_tags = [ 'ALLELEID' , 'CLNSIG' , 'CLNHGVS' ] cln_data = { x . lower ( ) : self . info [ x ] if x in self . info else None for x in info_clnvar_single_tags } cln_data . update ( { 'clndisdb' : [ x . split ( ',' ) for x in self . info [ 'CLNDISDB' ] . split ( '|' ) ] if 'CLNDISDB' in self . info else [ ] } ) cln_data . update ( { 'clndn' : self . info [ 'CLNDN' ] . split ( '|' ) if 'CLNDN' in self . info else [ ] } ) cln_data . update ( { 'clnvi' : self . info [ 'CLNVI' ] . split ( ',' ) if 'CLNVI' in self . info else [ ] } ) try : sequence = self . alt_alleles [ 0 ] except IndexError : sequence = self . ref_allele allele = ClinVarAllele ( frequency = pref_freq , sequence = sequence , ** cln_data ) if not cln_data [ 'clnsig' ] : return [ ] return [ allele ]
4183	def window_blackman_harris ( N ) : r a0 = 0.35875 a1 = 0.48829 a2 = 0.14128 a3 = 0.01168 return _coeff4 ( N , a0 , a1 , a2 , a3 )
751	def _addAnomalyClassifierRegion ( self , network , params , spEnable , tmEnable ) : allParams = copy . deepcopy ( params ) knnParams = dict ( k = 1 , distanceMethod = 'rawOverlap' , distanceNorm = 1 , doBinarization = 1 , replaceDuplicates = 0 , maxStoredPatterns = 1000 ) allParams . update ( knnParams ) if allParams [ 'trainRecords' ] is None : allParams [ 'trainRecords' ] = DEFAULT_ANOMALY_TRAINRECORDS if allParams [ 'cacheSize' ] is None : allParams [ 'cacheSize' ] = DEFAULT_ANOMALY_CACHESIZE if self . _netInfo is not None and self . _netInfo . net is not None and self . _getAnomalyClassifier ( ) is not None : self . _netInfo . net . removeRegion ( 'AnomalyClassifier' ) network . addRegion ( "AnomalyClassifier" , "py.KNNAnomalyClassifierRegion" , json . dumps ( allParams ) ) if spEnable : network . link ( "SP" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "bottomUpOut" , destInput = "spBottomUpOut" ) else : network . link ( "sensor" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "spBottomUpOut" ) if tmEnable : network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "topDownOut" , destInput = "tpTopDownOut" ) network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , srcOutput = "lrnActiveStateT" , destInput = "tpLrnActiveStateT" ) else : raise RuntimeError ( "TemporalAnomaly models require a TM region." )
7015	def concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , recursive = True ) : LOGINFO ( 'looking for light curves for %s, aperture %s in directory: %s' % ( objectid , aperture , lcbasedir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcbasedir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcbasedir , '**' , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) , recursive = True ) LOGINFO ( 'found %s files: %s' % ( len ( matching ) , repr ( matching ) ) ) else : walker = os . walk ( lcbasedir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , '*%s*%s*%s' % ( objectid , aperture , postfix ) ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) LOGINFO ( 'found %s in dir: %s' % ( repr ( foundfiles ) , os . path . join ( root , sdir ) ) ) if matching and len ( matching ) > 0 : clcdict = concatenate_textlcs ( matching , sortby = sortby , normalize = normalize ) return clcdict else : LOGERROR ( 'did not find any light curves for %s and aperture %s' % ( objectid , aperture ) ) return None
9142	def _sortkey ( self , key = 'uri' , language = 'any' ) : if key == 'uri' : return self . uri else : l = label ( self . labels , language , key == 'sortlabel' ) return l . label . lower ( ) if l else ''
10361	def rewire_targets ( graph , rewiring_probability ) : if not all_edges_consistent ( graph ) : raise ValueError ( '{} is not consistent' . format ( graph ) ) result = graph . copy ( ) nodes = result . nodes ( ) for u , v in result . edges ( ) : if random . random ( ) < rewiring_probability : continue w = random . choice ( nodes ) while w == u or result . has_edge ( u , w ) : w = random . choice ( nodes ) result . add_edge ( w , v ) result . remove_edge ( u , v ) return result
4167	def eqtflength ( b , a ) : d = abs ( len ( b ) - len ( a ) ) if d != 0 : if len ( a ) > len ( b ) : try : b . extend ( [ 0. ] * d ) except : b = np . append ( b , [ 0 ] * d ) elif len ( b ) > len ( a ) : try : a . extend ( [ 0. ] * d ) except : a = np . append ( a , [ 0 ] * d ) return b , a else : return b , a
4343	def silence ( self , location = 0 , silence_threshold = 0.1 , min_silence_duration = 0.1 , buffer_around_silence = False ) : if location not in [ - 1 , 0 , 1 ] : raise ValueError ( "location must be one of -1, 0, 1." ) if not is_number ( silence_threshold ) or silence_threshold < 0 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) elif silence_threshold >= 100 : raise ValueError ( "silence_threshold must be a number between 0 and 100" ) if not is_number ( min_silence_duration ) or min_silence_duration <= 0 : raise ValueError ( "min_silence_duration must be a positive number." ) if not isinstance ( buffer_around_silence , bool ) : raise ValueError ( "buffer_around_silence must be a boolean." ) effect_args = [ ] if location == - 1 : effect_args . append ( 'reverse' ) if buffer_around_silence : effect_args . extend ( [ 'silence' , '-l' ] ) else : effect_args . append ( 'silence' ) effect_args . extend ( [ '1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == 0 : effect_args . extend ( [ '-1' , '{:f}' . format ( min_silence_duration ) , '{:f}%' . format ( silence_threshold ) ] ) if location == - 1 : effect_args . append ( 'reverse' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'silence' ) return self
2281	def to_csv ( self , fname_radical , ** kwargs ) : if self . data is not None : self . data . to_csv ( fname_radical + '_data.csv' , index = False , ** kwargs ) pd . DataFrame ( self . adjacency_matrix ) . to_csv ( fname_radical + '_target.csv' , index = False , ** kwargs ) else : raise ValueError ( "Graph has not yet been generated. \ Use self.generate() to do so." )
10517	def setmax ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) object_handle . AXValue = 1 return 1
8712	def file_list ( self ) : log . info ( 'Listing files' ) res = self . __exchange ( LIST_FILES ) res = res . split ( '\r\n' ) res = res [ 1 : - 1 ] files = [ ] for line in res : files . append ( line . split ( '\t' ) ) return files
3055	def from_string ( cls , key , password = 'notasecret' ) : key = _helpers . _from_bytes ( key ) marker_id , key_bytes = pem . readPemBlocksFromFile ( six . StringIO ( key ) , _PKCS1_MARKER , _PKCS8_MARKER ) if marker_id == 0 : pkey = rsa . key . PrivateKey . load_pkcs1 ( key_bytes , format = 'DER' ) elif marker_id == 1 : key_info , remaining = decoder . decode ( key_bytes , asn1Spec = _PKCS8_SPEC ) if remaining != b'' : raise ValueError ( 'Unused bytes' , remaining ) pkey_info = key_info . getComponentByName ( 'privateKey' ) pkey = rsa . key . PrivateKey . load_pkcs1 ( pkey_info . asOctets ( ) , format = 'DER' ) else : raise ValueError ( 'No key could be detected.' ) return cls ( pkey )
1906	def strlen ( state , s ) : cpu = state . cpu if issymbolic ( s ) : raise ConcretizeArgument ( state . cpu , 1 ) zero_idx = _find_zero ( cpu , state . constraints , s ) ret = zero_idx for offset in range ( zero_idx - 1 , - 1 , - 1 ) : byt = cpu . read_int ( s + offset , 8 ) if issymbolic ( byt ) : ret = ITEBV ( cpu . address_bit_size , byt == 0 , offset , ret ) return ret
3371	def get_solver_name ( mip = False , qp = False ) : if len ( solvers ) == 0 : raise SolverNotFound ( "no solvers installed" ) mip_order = [ "gurobi" , "cplex" , "glpk" ] lp_order = [ "glpk" , "cplex" , "gurobi" ] qp_order = [ "gurobi" , "cplex" ] if mip is False and qp is False : for solver_name in lp_order : if solver_name in solvers : return solver_name return list ( solvers ) [ 0 ] elif qp : for solver_name in qp_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no qp-capable solver found" ) else : for solver_name in mip_order : if solver_name in solvers : return solver_name raise SolverNotFound ( "no mip-capable solver found" )
7190	def new ( n , prefix = None ) : if isinstance ( n , Leaf ) : return Leaf ( n . type , n . value , prefix = n . prefix if prefix is None else prefix ) n . parent = None if prefix is not None : n . prefix = prefix return n
4317	def info ( filepath ) : info_dictionary = { 'channels' : channels ( filepath ) , 'sample_rate' : sample_rate ( filepath ) , 'bitrate' : bitrate ( filepath ) , 'duration' : duration ( filepath ) , 'num_samples' : num_samples ( filepath ) , 'encoding' : encoding ( filepath ) , 'silent' : silent ( filepath ) } return info_dictionary
10636	def get_element_mfrs ( self , elements = None ) : if elements is None : elements = self . material . elements result = numpy . zeros ( len ( elements ) ) for compound in self . material . compounds : result += self . get_compound_mfr ( compound ) * stoich . element_mass_fractions ( compound , elements ) return result
4472	def transform ( self , jam ) : for state in self . states ( jam ) : yield self . _transform ( jam , state )
11245	def clean_strings ( iterable ) : retval = [ ] for val in iterable : try : retval . append ( val . strip ( ) ) except ( AttributeError ) : retval . append ( val ) return retval
1501	def template_slave_hcl ( cl_args , masters ) : slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] masters_in_quotes = [ '"%s"' % master for master in masters ] template_file ( slave_config_template , slave_config_actual , { "<nomad_masters:master_port>" : ", " . join ( masters_in_quotes ) } )
622	def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates
2470	def set_file_notice ( self , doc , text ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_notice_set : self . file_notice_set = True if validations . validate_file_notice ( text ) : self . file ( doc ) . notice = str_from_text ( text ) else : raise SPDXValueError ( 'File::Notice' ) else : raise CardinalityError ( 'File::Notice' ) else : raise OrderError ( 'File::Notice' )
4621	def _decrypt_masterpassword ( self ) : aes = AESCipher ( self . password ) checksum , encrypted_master = self . config [ self . config_key ] . split ( "$" ) try : decrypted_master = aes . decrypt ( encrypted_master ) except Exception : self . _raise_wrongmasterpassexception ( ) if checksum != self . _derive_checksum ( decrypted_master ) : self . _raise_wrongmasterpassexception ( ) self . decrypted_master = decrypted_master
6572	def register_simple_chooser ( self , model , ** kwargs ) : name = '{}Chooser' . format ( model . _meta . object_name ) attrs = { 'model' : model } attrs . update ( kwargs ) chooser = type ( name , ( Chooser , ) , attrs ) self . register_chooser ( chooser ) return model
8630	def get_projects ( session , query ) : response = make_get_request ( session , 'projects' , params_data = query ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'result' ] else : raise ProjectsNotFoundException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11392	def fetch_url ( url , method = 'GET' , user_agent = 'django-oembed' , timeout = SOCKET_TIMEOUT ) : sock = httplib2 . Http ( timeout = timeout ) request_headers = { 'User-Agent' : user_agent , 'Accept-Encoding' : 'gzip' } try : headers , raw = sock . request ( url , headers = request_headers , method = method ) except : raise OEmbedHTTPException ( 'Error fetching %s' % url ) return headers , raw
3014	def _to_json ( self , strip , to_serialize = None ) : if to_serialize is None : to_serialize = copy . copy ( self . __dict__ ) pkcs12_val = to_serialize . get ( _PKCS12_KEY ) if pkcs12_val is not None : to_serialize [ _PKCS12_KEY ] = base64 . b64encode ( pkcs12_val ) return super ( ServiceAccountCredentials , self ) . _to_json ( strip , to_serialize = to_serialize )
2905	def _add_child ( self , task_spec , state = MAYBE ) : if task_spec is None : raise ValueError ( self , '_add_child() requires a TaskSpec' ) if self . _is_predicted ( ) and state & self . PREDICTED_MASK == 0 : msg = 'Attempt to add non-predicted child to predicted task' raise WorkflowException ( self . task_spec , msg ) task = Task ( self . workflow , task_spec , self , state = state ) task . thread_id = self . thread_id if state == self . READY : task . _ready ( ) return task
12186	async def join_rtm ( self , filters = None ) : if filters is None : filters = [ cls ( self ) for cls in self . MESSAGE_FILTERS ] url = await self . _get_socket_url ( ) logger . debug ( 'Connecting to %r' , url ) async with ws_connect ( url ) as socket : first_msg = await socket . receive ( ) self . _validate_first_message ( first_msg ) self . socket = socket async for message in socket : if message . tp == MsgType . text : await self . handle_message ( message , filters ) elif message . tp in ( MsgType . closed , MsgType . error ) : if not socket . closed : await socket . close ( ) self . socket = None break logger . info ( 'Left real-time messaging.' )
3308	def _run_flup ( app , config , mode ) : if mode == "flup-fcgi" : from flup . server . fcgi import WSGIServer , __version__ as flupver elif mode == "flup-fcgi-fork" : from flup . server . fcgi_fork import WSGIServer , __version__ as flupver else : raise ValueError _logger . info ( "Running WsgiDAV/{} {}/{}..." . format ( __version__ , WSGIServer . __module__ , flupver ) ) server = WSGIServer ( app , bindAddress = ( config [ "host" ] , config [ "port" ] ) , ) try : server . run ( ) except KeyboardInterrupt : _logger . warning ( "Caught Ctrl-C, shutting down..." ) return
13546	def get_users ( self , params = { } ) : param_list = [ ( k , params [ k ] ) for k in sorted ( params ) ] url = "/2/users/?%s" % urlencode ( param_list ) data = self . _get_resource ( url ) users = [ ] for entry in data [ "users" ] : users . append ( self . user_from_json ( entry ) ) return users
10780	def diffusion ( diffusion_constant = 0.2 , exposure_time = 0.05 , samples = 200 ) : radius = 5 psfsize = np . array ( [ 2.0 , 1.0 , 3.0 ] ) s0 = init . create_single_particle_state ( imsize = 4 * radius , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) finalimage = 0 * s0 . get_model_image ( ) [ s0 . inner ] position = 0 * s0 . obj . pos [ 0 ] for i in xrange ( samples ) : offset = np . sqrt ( 6 * diffusion_constant * exposure_time ) * np . random . randn ( 3 ) s0 . obj . pos [ 0 ] = np . array ( s0 . image . shape ) / 2 + offset s0 . reset ( ) finalimage += s0 . get_model_image ( ) [ s0 . inner ] position += s0 . obj . pos [ 0 ] finalimage /= float ( samples ) position /= float ( samples ) s = init . create_single_particle_state ( imsize = 4 * radius , sigma = 0.05 , radius = radius , psfargs = { 'params' : psfsize , 'error' : 1e-6 } ) s . reset ( ) return s , finalimage , position
9823	def delete ( ctx ) : user , project_name = get_project_or_local ( ctx . obj . get ( 'project' ) ) if not click . confirm ( "Are sure you want to delete project `{}/{}`" . format ( user , project_name ) ) : click . echo ( 'Existing without deleting project.' ) sys . exit ( 1 ) try : response = PolyaxonClient ( ) . project . delete_project ( user , project_name ) local_project = ProjectManager . get_config ( ) if local_project and ( user , project_name ) == ( local_project . user , local_project . name ) : ProjectManager . purge ( ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not delete project `{}/{}`.' . format ( user , project_name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . status_code == 204 : Printer . print_success ( "Project `{}/{}` was delete successfully" . format ( user , project_name ) )
9375	def get_run_time_period ( run_steps ) : init_ts_start = get_standardized_timestamp ( 'now' , None ) ts_start = init_ts_start ts_end = '0' for run_step in run_steps : if run_step . ts_start and run_step . ts_end : if run_step . ts_start < ts_start : ts_start = run_step . ts_start if run_step . ts_end > ts_end : ts_end = run_step . ts_end if ts_end == '0' : ts_end = None if ts_start == init_ts_start : ts_start = None logger . info ( 'get_run_time_period range returned ' + str ( ts_start ) + ' to ' + str ( ts_end ) ) return ts_start , ts_end
6785	def fake ( self , components = None ) : self . init ( ) if components : current_tp = self . get_previous_thumbprint ( ) or { } current_tp . update ( self . get_current_thumbprint ( components = components ) or { } ) else : current_tp = self . get_current_thumbprint ( components = components ) or { } tp_text = yaml . dump ( current_tp ) r = self . local_renderer r . upload_content ( content = tp_text , fn = self . manifest_filename ) self . reset_all_satchels ( )
5862	def validate ( self , ml_template ) : data = { "ml_template" : ml_template } failure_message = "ML template validation invoke failed" res = self . _get_success_json ( self . _post_json ( 'ml_templates/validate' , data , failure_message = failure_message ) ) [ 'data' ] if res [ 'valid' ] : return 'OK' return res [ 'reason' ]
11072	def _to_primary_key ( self , value ) : if value is None : return None if isinstance ( value , self . base_class ) : if not value . _is_loaded : raise exceptions . DatabaseError ( 'Record must be loaded.' ) return value . _primary_key return self . base_class . _to_primary_key ( value )
8596	def update_group ( self , group_id , ** kwargs ) : properties = { } if 'create_datacenter' in kwargs : kwargs [ 'create_data_center' ] = kwargs . pop ( 'create_datacenter' ) for attr , value in kwargs . items ( ) : properties [ self . _underscore_to_camelcase ( attr ) ] = value data = { "properties" : properties } response = self . _perform_request ( url = '/um/groups/%s' % group_id , method = 'PUT' , data = json . dumps ( data ) ) return response
57	def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )
11234	def translate_array ( self , string , language , level = 3 , retdata = False ) : language = language . lower ( ) assert self . is_built_in ( language ) or language in self . outer_templates , "Sorry, " + language + " is not a supported language." data = phpserialize . loads ( bytes ( string , 'utf-8' ) , array_hook = list , decode_strings = True ) if self . is_built_in ( language ) : self . get_built_in ( language , level , data ) print ( self ) return self . data_structure if retdata else None def loop_print ( iterable , level = 3 ) : retval = '' indentation = ' ' * level if not self . is_iterable ( iterable ) or isinstance ( iterable , str ) : non_iterable = str ( iterable ) return str ( non_iterable ) for item in iterable : if isinstance ( item , tuple ) and len ( item ) == 2 : key = item [ 0 ] val = loop_print ( item [ 1 ] , level = level + 3 ) val = self . translate_val ( language , val ) if language in self . lang_specific_values and val in self . lang_specific_values [ language ] else val key = str ( key ) if isinstance ( key , int ) else '\'' + str ( key ) + '\'' needs_unpacking = hasattr ( item [ 0 ] , '__iter__' ) == False and hasattr ( item [ 1 ] , '__iter__' ) == True if needs_unpacking : retval += self . get_inner_template ( language , 'iterable' , indentation , key , val ) else : val = str ( val ) if val . isdigit ( ) or val in self . lang_specific_values [ language ] . values ( ) else '\'' + str ( val ) + '\'' retval += self . get_inner_template ( language , 'singular' , indentation , key , val ) return retval self . data_structure = self . outer_templates [ language ] % ( loop_print ( data ) ) print ( self ) return self . data_structure if retdata else None
4243	def _get_region ( self , ipnum ) : region_code = None country_code = None seek_country = self . _seek_country ( ipnum ) def get_region_code ( offset ) : region1 = chr ( offset // 26 + 65 ) region2 = chr ( offset % 26 + 65 ) return '' . join ( [ region1 , region2 ] ) if self . _databaseType == const . REGION_EDITION_REV0 : seek_region = seek_country - const . STATE_BEGIN_REV0 if seek_region >= 1000 : country_code = 'US' region_code = get_region_code ( seek_region - 1000 ) else : country_code = const . COUNTRY_CODES [ seek_region ] elif self . _databaseType == const . REGION_EDITION_REV1 : seek_region = seek_country - const . STATE_BEGIN_REV1 if seek_region < const . US_OFFSET : pass elif seek_region < const . CANADA_OFFSET : country_code = 'US' region_code = get_region_code ( seek_region - const . US_OFFSET ) elif seek_region < const . WORLD_OFFSET : country_code = 'CA' region_code = get_region_code ( seek_region - const . CANADA_OFFSET ) else : index = ( seek_region - const . WORLD_OFFSET ) // const . FIPS_RANGE if index < len ( const . COUNTRY_CODES ) : country_code = const . COUNTRY_CODES [ index ] elif self . _databaseType in const . CITY_EDITIONS : rec = self . _get_record ( ipnum ) region_code = rec . get ( 'region_code' ) country_code = rec . get ( 'country_code' ) return { 'country_code' : country_code , 'region_code' : region_code }
5405	def _build_user_environment ( self , envs , inputs , outputs , mounts ) : envs = { env . name : env . value for env in envs } envs . update ( providers_util . get_file_environment_variables ( inputs ) ) envs . update ( providers_util . get_file_environment_variables ( outputs ) ) envs . update ( providers_util . get_file_environment_variables ( mounts ) ) return envs
4694	def regex_find ( pattern , content ) : find = re . findall ( pattern , content ) if not find : cij . err ( "pattern <%r> is invalid, no matches!" % pattern ) cij . err ( "content: %r" % content ) return '' if len ( find ) >= 2 : cij . err ( "pattern <%r> is too simple, matched more than 2!" % pattern ) cij . err ( "content: %r" % content ) return '' return find [ 0 ]
1275	def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )
8286	def _get_elements ( self ) : for index , el in enumerate ( self . _elements ) : if isinstance ( el , tuple ) : el = PathElement ( * el ) self . _elements [ index ] = el yield el
7109	def get_from_cache ( url : str , cache_dir : Path = None ) -> Path : cache_dir . mkdir ( parents = True , exist_ok = True ) filename = re . sub ( r'.+/' , '' , url ) cache_path = cache_dir / filename if cache_path . exists ( ) : return cache_path response = requests . head ( url ) if response . status_code != 200 : if "www.dropbox.com" in url : pass else : raise IOError ( "HEAD request failed for url {}" . format ( url ) ) if not cache_path . exists ( ) : fd , temp_filename = tempfile . mkstemp ( ) logger . info ( "%s not found in cache, downloading to %s" , url , temp_filename ) req = requests . get ( url , stream = True ) content_length = req . headers . get ( 'Content-Length' ) total = int ( content_length ) if content_length is not None else None progress = Tqdm . tqdm ( unit = "B" , total = total ) with open ( temp_filename , 'wb' ) as temp_file : for chunk in req . iter_content ( chunk_size = 1024 ) : if chunk : progress . update ( len ( chunk ) ) temp_file . write ( chunk ) progress . close ( ) logger . info ( "copying %s to cache at %s" , temp_filename , cache_path ) shutil . copyfile ( temp_filename , str ( cache_path ) ) logger . info ( "removing temp file %s" , temp_filename ) os . close ( fd ) os . remove ( temp_filename ) return cache_path
4997	def assign_enterprise_learner_role ( sender , instance , ** kwargs ) : if kwargs [ 'created' ] and instance . user : enterprise_learner_role , __ = SystemWideEnterpriseRole . objects . get_or_create ( name = ENTERPRISE_LEARNER_ROLE ) SystemWideEnterpriseUserRoleAssignment . objects . get_or_create ( user = instance . user , role = enterprise_learner_role )
10889	def kvectors ( self , norm = False , form = 'broadcast' , real = False , shift = False ) : if norm is False : norm = 1 if norm is True : norm = np . array ( self . shape ) norm = aN ( norm , self . dim , dtype = 'float' ) v = list ( np . fft . fftfreq ( self . shape [ i ] ) / norm [ i ] for i in range ( self . dim ) ) if shift : v = list ( np . fft . fftshift ( t ) for t in v ) if real : v [ - 1 ] = v [ - 1 ] [ : ( self . shape [ - 1 ] + 1 ) // 2 ] return self . _format_vector ( v , form = form )
10206	def run ( self , start_date = None , end_date = None , ** kwargs ) : start_date = self . extract_date ( start_date ) if start_date else None end_date = self . extract_date ( end_date ) if end_date else None self . validate_arguments ( start_date , end_date , ** kwargs ) agg_query = self . build_query ( start_date , end_date , ** kwargs ) query_result = agg_query . execute ( ) . to_dict ( ) res = self . process_query_result ( query_result , start_date , end_date ) return res
651	def sameTMParams ( tp1 , tp2 ) : result = True for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , "minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , "permanenceMax" , "globalDecay" , "activationThreshold" , "doPooling" , "segUpdateValidDuration" , "burnIn" , "pamLength" , "maxAge" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , "is different" print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) result = False return result
11918	def get_dataframe ( self ) : assert self . dataframe is not None , ( "'%s' should either include a `dataframe` attribute, " "or override the `get_dataframe()` method." % self . __class__ . __name__ ) dataframe = self . dataframe return dataframe
9821	def create ( ctx , name , description , tags , private , init ) : try : tags = tags . split ( ',' ) if tags else None project_dict = dict ( name = name , description = description , is_public = not private , tags = tags ) project_config = ProjectConfig . from_dict ( project_dict ) except ValidationError : Printer . print_error ( 'Project name should contain only alpha numerical, "-", and "_".' ) sys . exit ( 1 ) try : _project = PolyaxonClient ( ) . project . create_project ( project_config ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not create project `{}`.' . format ( name ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) Printer . print_success ( "Project `{}` was created successfully." . format ( _project . name ) ) if init : ctx . obj = { } ctx . invoke ( init_project , project = name )
12813	def styles ( self ) : styles = get_all_styles ( ) whitelist = self . app . config . get ( 'CSL_STYLES_WHITELIST' ) if whitelist : return { k : v for k , v in styles . items ( ) if k in whitelist } return styles
701	def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]
1147	def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , "__deepcopy__" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , "__reduce_ex__" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , "__reduce__" , None ) if reductor : rv = reductor ( ) else : raise Error ( "un(deep)copyable object of type %s" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) return y
598	def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( "Temporal memory has not been initialized" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )
9089	def _iterate_namespace_models ( self , ** kwargs ) -> Iterable : return tqdm ( self . _get_query ( self . namespace_model ) , total = self . _count_model ( self . namespace_model ) , ** kwargs )
11386	def run ( self , raw_args ) : parser = self . parser args , kwargs = parser . parse_callback_args ( raw_args ) callback = kwargs . pop ( "main_callback" ) if parser . has_injected_quiet ( ) : levels = kwargs . pop ( "quiet_inject" , "" ) logging . inject_quiet ( levels ) try : ret_code = callback ( * args , ** kwargs ) ret_code = int ( ret_code ) if ret_code else 0 except ArgError as e : echo . err ( "{}: error: {}" , parser . prog , str ( e ) ) ret_code = 2 return ret_code
12130	def show ( self , exclude = [ ] ) : ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering if ( k in s ) and ( k not in exclude ) ] ) for s in self . specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) )
1285	def footnote_item ( self , key , text ) : back = ( '<a href="#fnref-%s" class="footnote">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id="fn-%s">%s</li>\n' % ( escape ( key ) , text ) return html
8719	def backup ( self , path ) : log . info ( 'Backing up in ' + path ) files = self . file_list ( ) self . prepare ( ) for f in files : self . read_file ( f [ 0 ] , os . path . join ( path , f [ 0 ] ) )
9754	def get ( ctx , job ) : def get_experiment ( ) : try : response = PolyaxonClient ( ) . experiment . get_experiment ( user , project_name , _experiment ) cache . cache ( config_manager = ExperimentManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not load experiment `{}` info.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) get_experiment_details ( response ) def get_experiment_job ( ) : try : response = PolyaxonClient ( ) . experiment_job . get_job ( user , project_name , _experiment , _job ) cache . cache ( config_manager = ExperimentJobManager , response = response ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) if response . resources : get_resources ( response . resources . to_dict ( ) , header = "Job resources:" ) response = Printer . add_status_color ( response . to_light_dict ( humanize_values = True , exclude_attrs = [ 'uuid' , 'definition' , 'experiment' , 'unique_name' , 'resources' ] ) ) Printer . print_header ( "Job info:" ) dict_tabulate ( response ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job ( ) else : get_experiment ( )
12328	def init_repo ( self , gitdir ) : hooksdir = os . path . join ( gitdir , 'hooks' ) content = postreceive_template % { 'client' : self . client , 'bucket' : self . bucket , 's3cfg' : self . s3cfg , 'prefix' : self . prefix } postrecv_filename = os . path . join ( hooksdir , 'post-receive' ) with open ( postrecv_filename , 'w' ) as fd : fd . write ( content ) self . make_hook_executable ( postrecv_filename ) print ( "Wrote to" , postrecv_filename )
9636	def _getCallingContext ( ) : frames = inspect . stack ( ) if len ( frames ) > 4 : context = frames [ 5 ] else : context = frames [ 0 ] modname = context [ 1 ] lineno = context [ 2 ] if context [ 3 ] : funcname = context [ 3 ] else : funcname = "" del context del frames return modname , funcname , lineno
10301	def count_defaultdict ( dict_of_lists : Mapping [ X , List [ Y ] ] ) -> Mapping [ X , typing . Counter [ Y ] ] : return { k : Counter ( v ) for k , v in dict_of_lists . items ( ) }
6282	def cursor_event ( self , x , y , dx , dy ) : self . sys_camera . rot_state ( x , y )
10183	def _aggregations_delete ( aggregation_types = None , start_date = None , end_date = None ) : aggregation_types = ( aggregation_types or list ( current_stats . enabled_aggregations ) ) for a in aggregation_types : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) aggregator . delete ( start_date , end_date )
8259	def _sorted_copy ( self , comparison , reversed = False ) : sorted = self . copy ( ) _list . sort ( sorted , comparison ) if reversed : _list . reverse ( sorted ) return sorted
1686	def Split ( self ) : googlename = self . RepositoryName ( ) project , rest = os . path . split ( googlename ) return ( project , ) + os . path . splitext ( rest )
12350	def get ( self , id ) : info = self . _get_droplet_info ( id ) return DropletActions ( self . api , self , ** info )
1046	def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]
3715	def calculate ( self , T , P , zs , ws , method ) : r if method == SIMPLE : Vms = [ i ( T , P ) for i in self . VolumeSolids ] return mixing_simple ( zs , Vms ) else : raise Exception ( 'Method not valid' )
13200	def format_title ( self , format = 'html5' , deparagraph = True , mathjax = False , smart = True , extra_args = None ) : if self . title is None : return None output_text = convert_lsstdoc_tex ( self . title , format , deparagraph = deparagraph , mathjax = mathjax , smart = smart , extra_args = extra_args ) return output_text
13243	def weekdays ( self ) : if not self . root . xpath ( 'days' ) : return set ( range ( 7 ) ) return set ( int ( d ) - 1 for d in self . root . xpath ( 'days/day/text()' ) )
2757	def get_all_floating_ips ( self ) : data = self . get_data ( "floating_ips" ) floating_ips = list ( ) for jsoned in data [ 'floating_ips' ] : floating_ip = FloatingIP ( ** jsoned ) floating_ip . token = self . token floating_ips . append ( floating_ip ) return floating_ips
11221	def get ( self , request , hash , filename ) : if _ws_download is True : return HttpResponseForbidden ( ) upload = Upload . objects . uploaded ( ) . get ( hash = hash , name = filename ) return FileResponse ( upload . file , content_type = upload . type )
11065	def acl_show ( self , msg , args ) : name = args [ 0 ] if len ( args ) > 0 else None if name is None : return "%s: The following ACLs are defined: %s" % ( msg . user , ', ' . join ( self . _acl . keys ( ) ) ) if name not in self . _acl : return "Sorry, couldn't find an acl named '%s'" % name return '\n' . join ( [ "%s: ACL '%s' is defined as follows:" % ( msg . user , name ) , "allow: %s" % ', ' . join ( self . _acl [ name ] [ 'allow' ] ) , "deny: %s" % ', ' . join ( self . _acl [ name ] [ 'deny' ] ) ] )
10784	def should_particle_exist ( absent_err , present_err , absent_d , present_d , im_change_frac = 0.2 , min_derr = 0.1 ) : delta_im = np . ravel ( present_d - absent_d ) im_change = np . dot ( delta_im , delta_im ) err_cutoff = max ( [ im_change_frac * im_change , min_derr ] ) return ( absent_err - present_err ) >= err_cutoff
1804	def SAHF ( cpu ) : eflags_size = 32 val = cpu . AH & 0xD5 | 0x02 cpu . EFLAGS = Operators . ZEXTEND ( val , eflags_size )
13377	def walk_up ( start_dir , depth = 20 ) : root = start_dir for i in xrange ( depth ) : contents = os . listdir ( root ) subdirs , files = [ ] , [ ] for f in contents : if os . path . isdir ( os . path . join ( root , f ) ) : subdirs . append ( f ) else : files . append ( f ) yield root , subdirs , files parent = os . path . dirname ( root ) if parent and not parent == root : root = parent else : break
7483	def parse_single_results ( data , sample , res1 ) : sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = 0 sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = 0 sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = 0 sample . stats_dfs . s2 [ "reads_passed_filter" ] = 0 lines = res1 . strip ( ) . split ( "\n" ) for line in lines : if "Total reads processed:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_raw" ] = value if "Reads with adapters:" in line : value = int ( line . split ( ) [ 3 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_adapter_bp_read1" ] = value if "Quality-trimmed" in line : value = int ( line . split ( ) [ 1 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "trim_quality_bp_read1" ] = value if "Reads that were too short" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_minlen" ] = value if "Reads with too many N" in line : value = int ( line . split ( ) [ 5 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_filtered_by_Ns" ] = value if "Reads written (passing filters):" in line : value = int ( line . split ( ) [ 4 ] . replace ( "," , "" ) ) sample . stats_dfs . s2 [ "reads_passed_filter" ] = value if sample . stats_dfs . s2 . reads_passed_filter : sample . stats . state = 2 sample . stats . reads_passed_filter = sample . stats_dfs . s2 . reads_passed_filter sample . files . edits = [ ( OPJ ( data . dirs . edits , sample . name + ".trimmed_R1_.fastq.gz" ) , 0 ) ] LOGGER . info ( res1 ) else : print ( "{}No reads passed filtering in Sample: {}" . format ( data . _spacer , sample . name ) )
13839	def ConsumeFloat ( self ) : try : result = ParseFloat ( self . token ) except ValueError as e : raise self . _ParseError ( str ( e ) ) self . NextToken ( ) return result
12089	def proto_01_12_steps025 ( abf = exampleABF ) : swhlab . ap . detect ( abf ) standard_groupingForInj ( abf , 200 ) for feature in [ 'freq' , 'downslope' ] : swhlab . ap . plot_values ( abf , feature , continuous = False ) swhlab . plot . save ( abf , tag = 'A_' + feature ) swhlab . plot . gain ( abf ) swhlab . plot . save ( abf , tag = '05-gain' )
542	def __getOptimizedMetricLabel ( self ) : matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , self . _getMetricLabels ( ) ) if len ( matchingKeys ) == 0 : raise Exception ( "None of the generated metrics match the specified " "optimization pattern: %s. Available metrics are %s" % ( self . _optimizeKeyPattern , self . _getMetricLabels ( ) ) ) elif len ( matchingKeys ) > 1 : raise Exception ( "The specified optimization pattern '%s' matches more " "than one metric: %s" % ( self . _optimizeKeyPattern , matchingKeys ) ) return matchingKeys [ 0 ]
9728	def get_analog ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . device_count ) : component_position , device = QRTPacket . _get_exact ( RTAnalogDevice , data , component_position ) if device . sample_count > 0 : component_position , sample_number = QRTPacket . _get_exact ( RTSampleNumber , data , component_position ) RTAnalogChannel . format = struct . Struct ( RTAnalogChannel . format_str % device . sample_count ) for _ in range ( device . channel_count ) : component_position , channel = QRTPacket . _get_tuple ( RTAnalogChannel , data , component_position ) append_components ( ( device , sample_number , channel ) ) return components
2553	def set_attribute ( self , key , value ) : if isinstance ( key , int ) : self . children [ key ] = value elif isinstance ( key , basestring ) : self . attributes [ key ] = value else : raise TypeError ( 'Only integer and string types are valid for assigning ' 'child tags and attributes, respectively.' )
1068	def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist
10822	def query_invitations ( cls , user , eager = False ) : if eager : eager = [ Membership . group ] return cls . query_by_user ( user , state = MembershipState . PENDING_USER , eager = eager )
9512	def gaps ( self , min_length = 1 ) : gaps = [ ] regex = re . compile ( 'N+' , re . IGNORECASE ) for m in regex . finditer ( self . seq ) : if m . span ( ) [ 1 ] - m . span ( ) [ 0 ] + 1 >= min_length : gaps . append ( intervals . Interval ( m . span ( ) [ 0 ] , m . span ( ) [ 1 ] - 1 ) ) return gaps
13593	def check_environment ( target , label ) : if not git . exists ( ) : click . secho ( 'You must have git installed to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not os . path . isdir ( '.git' ) : click . secho ( 'You must cd into a git repository to use yld.' , fg = 'red' ) sys . exit ( 1 ) if not git . is_committed ( ) : click . secho ( 'You must commit or stash your work before proceeding.' , fg = 'red' ) sys . exit ( 1 ) if target is None and label is None : click . secho ( 'You must specify either a target or a label.' , fg = 'red' ) sys . exit ( 1 )
2000	def visit_BitVecAdd ( self , expression , * operands ) : left = expression . operands [ 0 ] right = expression . operands [ 1 ] if isinstance ( right , BitVecConstant ) : if right . value == 0 : return left if isinstance ( left , BitVecConstant ) : if left . value == 0 : return right
13232	def get_newcommand_macros ( tex_source ) : r macros = { } command = LatexCommand ( 'newcommand' , { 'name' : 'name' , 'required' : True , 'bracket' : '{' } , { 'name' : 'content' , 'required' : True , 'bracket' : '{' } ) for macro in command . parse ( tex_source ) : macros [ macro [ 'name' ] ] = macro [ 'content' ] return macros
7255	def get ( self , catID , includeRelationships = False ) : url = '%(base_url)s/record/%(catID)s' % { 'base_url' : self . base_url , 'catID' : catID } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
4467	def serialize ( transform , ** kwargs ) : params = transform . get_params ( ) return jsonpickle . encode ( params , ** kwargs )
8246	def morguefile ( query , n = 10 , top = 10 ) : from web import morguefile images = morguefile . search ( query ) [ : top ] path = choice ( images ) . download ( thumbnail = True , wait = 10 ) return ColorList ( path , n , name = query )
10101	def create_snippet ( self , name , body , timeout = None ) : payload = { 'name' : name , 'body' : body } return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_POST , payload = payload , timeout = timeout )
2361	def t_tabbedheredoc ( self , t ) : r'<<-\S+\r?\n' t . lexer . is_tabbed = True self . _init_heredoc ( t ) t . lexer . begin ( 'tabbedheredoc' )
2896	def is_completed ( self ) : mask = Task . NOT_FINISHED_MASK iter = Task . Iterator ( self . task_tree , mask ) try : next ( iter ) except StopIteration : return True return False
7898	def process_configuration_form_success ( self , stanza ) : if stanza . get_query_ns ( ) != MUC_OWNER_NS : raise ValueError ( "Bad result namespace" ) query = stanza . get_query ( ) form = None for el in xml_element_ns_iter ( query . children , DATAFORM_NS ) : form = Form ( el ) break if not form : raise ValueError ( "No form received" ) self . configuration_form = form self . handler . configuration_form_received ( form )
12353	def rename ( self , name , wait = True ) : return self . _action ( 'rename' , name = name , wait = wait )
10722	def _wrapper ( func ) : @ functools . wraps ( func ) def the_func ( expr ) : try : return func ( expr ) except ( TypeError , ValueError ) as err : raise IntoDPValueError ( expr , "expr" , "could not be transformed" ) from err return the_func
9507	def union ( self , i ) : if self . intersects ( i ) or self . end + 1 == i . start or i . end + 1 == self . start : return Interval ( min ( self . start , i . start ) , max ( self . end , i . end ) ) else : return None
1821	def SETPE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) )
7289	def has_digit ( string_or_list , sep = "_" ) : if isinstance ( string_or_list , ( tuple , list ) ) : list_length = len ( string_or_list ) if list_length : return six . text_type ( string_or_list [ - 1 ] ) . isdigit ( ) else : return False else : return has_digit ( string_or_list . split ( sep ) )
1368	def register_on_message ( self , msg_builder ) : message = msg_builder ( ) Log . debug ( "In register_on_message(): %s" % message . DESCRIPTOR . full_name ) self . registered_message_map [ message . DESCRIPTOR . full_name ] = msg_builder
4503	def SPI ( ledtype = None , num = 0 , ** kwargs ) : from . . . project . types . ledtype import make if ledtype is None : raise ValueError ( 'Must provide ledtype value!' ) ledtype = make ( ledtype ) if num == 0 : raise ValueError ( 'Must provide num value >0!' ) if ledtype not in SPI_DRIVERS . keys ( ) : raise ValueError ( '{} is not a valid LED type.' . format ( ledtype ) ) return SPI_DRIVERS [ ledtype ] ( num , ** kwargs )
8355	def _subMSChar ( self , orig ) : sub = self . MS_CHARS . get ( orig ) if type ( sub ) == types . TupleType : if self . smartQuotesTo == 'xml' : sub = '&#x%s;' % sub [ 1 ] else : sub = '&%s;' % sub [ 0 ] return sub
12920	def reload ( self ) : if len ( self ) == 0 : return [ ] ret = [ ] for obj in self : res = None try : res = obj . reload ( ) except Exception as e : res = e ret . append ( res ) return ret
10642	def Ra ( L : float , Ts : float , Tf : float , alpha : float , beta : float , nu : float ) -> float : return g * beta * ( Ts - Tinf ) * L ** 3.0 / ( nu * alpha )
7381	def simplified_edges ( self ) : for group , edgelist in self . edges . items ( ) : for u , v , d in edgelist : yield ( u , v )
12774	def inverse_kinematics ( self , start = 0 , end = 1e100 , states = None , max_force = 20 ) : zeros = None if max_force > 0 : self . skeleton . enable_motors ( max_force ) zeros = np . zeros ( self . skeleton . num_dofs ) for _ in self . follow_markers ( start , end , states ) : if zeros is not None : self . skeleton . set_target_angles ( zeros ) yield self . skeleton . joint_angles
6701	def add_apt_key ( filename = None , url = None , keyid = None , keyserver = 'subkeys.pgp.net' , update = False ) : if keyid is None : if filename is not None : run_as_root ( 'apt-key add %(filename)s' % locals ( ) ) elif url is not None : run_as_root ( 'wget %(url)s -O - | apt-key add -' % locals ( ) ) else : raise ValueError ( 'Either filename, url or keyid must be provided as argument' ) else : if filename is not None : _check_pgp_key ( filename , keyid ) run_as_root ( 'apt-key add %(filename)s' % locals ( ) ) elif url is not None : tmp_key = '/tmp/tmp.burlap.key.%(keyid)s.key' % locals ( ) run_as_root ( 'wget %(url)s -O %(tmp_key)s' % locals ( ) ) _check_pgp_key ( tmp_key , keyid ) run_as_root ( 'apt-key add %(tmp_key)s' % locals ( ) ) else : keyserver_opt = '--keyserver %(keyserver)s' % locals ( ) if keyserver is not None else '' run_as_root ( 'apt-key adv %(keyserver_opt)s --recv-keys %(keyid)s' % locals ( ) ) if update : update_index ( )
1709	def send ( self , str , end = '\n' ) : return self . _process . stdin . write ( str + end )
9273	def filter_between_tags ( self , all_tags ) : tag_names = [ t [ "name" ] for t in all_tags ] between_tags = [ ] for tag in self . options . between_tags : try : idx = tag_names . index ( tag ) except ValueError : raise ChangelogGeneratorError ( "ERROR: can't find tag {0}, specified with " "--between-tags option." . format ( tag ) ) between_tags . append ( all_tags [ idx ] ) between_tags = self . sort_tags_by_date ( between_tags ) if len ( between_tags ) == 1 : between_tags . append ( between_tags [ 0 ] ) older = self . get_time_of_tag ( between_tags [ 1 ] ) newer = self . get_time_of_tag ( between_tags [ 0 ] ) for tag in all_tags : if older < self . get_time_of_tag ( tag ) < newer : between_tags . append ( tag ) if older == newer : between_tags . pop ( 0 ) return between_tags
5824	def add_descriptor ( self , descriptor , role = 'ignore' , group_by_key = False ) : descriptor . validate ( ) if descriptor . key in self . configuration [ "roles" ] : raise ValueError ( "Cannot add a descriptor with the same name twice" ) self . configuration [ 'descriptors' ] . append ( descriptor . as_dict ( ) ) self . configuration [ "roles" ] [ descriptor . key ] = role if group_by_key : self . configuration [ "group_by" ] . append ( descriptor . key )
1140	def fill ( text , width = 70 , ** kwargs ) : w = TextWrapper ( width = width , ** kwargs ) return w . fill ( text )
11132	def _on_file_moved ( self , event : FileSystemMovedEvent ) : if not event . is_directory and self . is_data_file ( event . src_path ) : delete_event = FileSystemEvent ( event . src_path ) delete_event . event_type = EVENT_TYPE_DELETED self . _on_file_deleted ( delete_event ) create_event = FileSystemEvent ( event . dest_path ) create_event . event_type = EVENT_TYPE_CREATED self . _on_file_created ( create_event )
3253	def get_store ( self , name , workspace = None ) : stores = self . get_stores ( workspaces = workspace , names = name ) return self . _return_first_item ( stores )
11216	def valid ( self , time : int = None ) -> bool : if time is None : epoch = datetime ( 1970 , 1 , 1 , 0 , 0 , 0 ) now = datetime . utcnow ( ) time = int ( ( now - epoch ) . total_seconds ( ) ) if isinstance ( self . valid_from , int ) and time < self . valid_from : return False if isinstance ( self . valid_to , int ) and time > self . valid_to : return False return True
10143	def decrypt_files ( file_link ) : if ENCRYPTION_DISABLED : print ( 'For decryption please install gpg' ) exit ( ) try : parsed_link = re . findall ( r'(.*/(.*))#(.{30})' , file_link ) [ 0 ] req = urllib . request . Request ( parsed_link [ 0 ] , data = None , headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) ' ' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36' } ) file_response = urllib . request . urlopen ( req ) file_to_decrypt = file_response . read ( ) decrypt_r , decrypt_w = os . pipe ( ) cmd = 'gpg --batch --decrypt --passphrase-fd {}' . format ( decrypt_r ) decrypt_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE , pass_fds = ( decrypt_r , ) ) os . close ( decrypt_r ) open ( decrypt_w , 'w' ) . write ( parsed_link [ 2 ] ) decrypted_data , stderr = decrypt_output . communicate ( file_to_decrypt ) with open ( parsed_link [ 1 ] , 'wb' ) as decrypted_file : decrypted_file . write ( decrypted_data ) return parsed_link [ 1 ] + ' is decrypted and saved.' except IndexError : return 'Please enter valid link.'
4511	def crop ( image , top_offset = 0 , left_offset = 0 , bottom_offset = 0 , right_offset = 0 ) : if bottom_offset or top_offset or left_offset or right_offset : width , height = image . size box = ( left_offset , top_offset , width - right_offset , height - bottom_offset ) image = image . crop ( box = box ) return image
6920	def _autocorr_func2 ( mags , lag , maglen , magmed , magstd ) : lagindex = nparange ( 0 , maglen - lag ) products = ( mags [ lagindex ] - magmed ) * ( mags [ lagindex + lag ] - magmed ) autocovarfunc = npsum ( products ) / lagindex . size varfunc = npsum ( ( mags [ lagindex ] - magmed ) * ( mags [ lagindex ] - magmed ) ) / mags . size acorr = autocovarfunc / varfunc return acorr
155	def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]
7808	def from_ssl_socket ( cls , ssl_socket ) : cert = cls ( ) try : data = ssl_socket . getpeercert ( ) except AttributeError : return cert logger . debug ( "Certificate data from ssl module: {0!r}" . format ( data ) ) if not data : return cert cert . validated = True cert . subject_name = data . get ( 'subject' ) cert . alt_names = defaultdict ( list ) if 'subjectAltName' in data : for name , value in data [ 'subjectAltName' ] : cert . alt_names [ name ] . append ( value ) if 'notAfter' in data : tstamp = ssl . cert_time_to_seconds ( data [ 'notAfter' ] ) cert . not_after = datetime . utcfromtimestamp ( tstamp ) if sys . version_info . major < 3 : cert . _decode_names ( ) cert . common_names = [ ] if cert . subject_name : for part in cert . subject_name : for name , value in part : if name == 'commonName' : cert . common_names . append ( value ) return cert
1300	def WindowFromPoint ( x : int , y : int ) -> int : return ctypes . windll . user32 . WindowFromPoint ( ctypes . wintypes . POINT ( x , y ) )
6400	def stem ( self , word ) : wlen = len ( word ) - 2 if wlen > 2 and word [ - 1 ] == 's' : word = word [ : - 1 ] wlen -= 1 _endings = { 5 : { 'elser' , 'heten' } , 4 : { 'arne' , 'erna' , 'ande' , 'else' , 'aste' , 'orna' , 'aren' } , 3 : { 'are' , 'ast' , 'het' } , 2 : { 'ar' , 'er' , 'or' , 'en' , 'at' , 'te' , 'et' } , 1 : { 'a' , 'e' , 'n' , 't' } , } for end_len in range ( 5 , 0 , - 1 ) : if wlen > end_len and word [ - end_len : ] in _endings [ end_len ] : return word [ : - end_len ] return word
4808	def generate_best_dataset ( best_path , output_path = 'cleaned_data' , create_val = False ) : if not os . path . isdir ( output_path ) : os . mkdir ( output_path ) if not os . path . isdir ( os . path . join ( output_path , 'train' ) ) : os . makedirs ( os . path . join ( output_path , 'train' ) ) if not os . path . isdir ( os . path . join ( output_path , 'test' ) ) : os . makedirs ( os . path . join ( output_path , 'test' ) ) if not os . path . isdir ( os . path . join ( output_path , 'val' ) ) and create_val : os . makedirs ( os . path . join ( output_path , 'val' ) ) for article_type in article_types : files = glob ( os . path . join ( best_path , article_type , '*.txt' ) ) files_train , files_test = train_test_split ( files , random_state = 0 , test_size = 0.1 ) if create_val : files_train , files_val = train_test_split ( files_train , random_state = 0 , test_size = 0.1 ) val_words = generate_words ( files_val ) val_df = create_char_dataframe ( val_words ) val_df . to_csv ( os . path . join ( output_path , 'val' , 'df_best_{}_val.csv' . format ( article_type ) ) , index = False ) train_words = generate_words ( files_train ) test_words = generate_words ( files_test ) train_df = create_char_dataframe ( train_words ) test_df = create_char_dataframe ( test_words ) train_df . to_csv ( os . path . join ( output_path , 'train' , 'df_best_{}_train.csv' . format ( article_type ) ) , index = False ) test_df . to_csv ( os . path . join ( output_path , 'test' , 'df_best_{}_test.csv' . format ( article_type ) ) , index = False ) print ( "Save {} to CSV file" . format ( article_type ) )
6713	def install_setuptools ( python_cmd = 'python' , use_sudo = True ) : setuptools_version = package_version ( 'setuptools' , python_cmd ) distribute_version = package_version ( 'distribute' , python_cmd ) if setuptools_version is None : _install_from_scratch ( python_cmd , use_sudo ) else : if distribute_version is None : _upgrade_from_setuptools ( python_cmd , use_sudo ) else : _upgrade_from_distribute ( python_cmd , use_sudo )
12863	def quoted ( parser = any_token ) : quote_char = quote ( ) value , _ = many_until ( parser , partial ( one_of , quote_char ) ) return build_string ( value )
3570	def centralManager_didConnectPeripheral_ ( self , manager , peripheral ) : logger . debug ( 'centralManager_didConnectPeripheral called' ) peripheral . setDelegate_ ( self ) peripheral . discoverServices_ ( None ) device = device_list ( ) . get ( peripheral ) if device is not None : device . _set_connected ( )
5388	def _task_sort_function ( task ) : return ( task . get_field ( 'create-time' ) , int ( task . get_field ( 'task-id' , 0 ) ) , int ( task . get_field ( 'task-attempt' , 0 ) ) )
546	def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )
3206	def get ( self , batch_id , ** queryparams ) : self . batch_id = batch_id self . operation_status = None return self . _mc_client . _get ( url = self . _build_path ( batch_id ) , ** queryparams )
11694	def full_analysis ( self ) : self . count ( ) self . verify_words ( ) self . verify_user ( ) if self . review_requested == 'yes' : self . label_suspicious ( 'Review requested' )
5937	def transform_args ( self , * args , ** kwargs ) : options = [ ] for option , value in kwargs . items ( ) : if not option . startswith ( '-' ) : if len ( option ) == 1 : option = '-' + option else : option = '--' + option if value is True : options . append ( option ) continue elif value is False : raise ValueError ( 'A False value is ambiguous for option {0!r}' . format ( option ) ) if option [ : 2 ] == '--' : options . append ( option + '=' + str ( value ) ) else : options . extend ( ( option , str ( value ) ) ) return options + list ( args )
2903	def ref ( function , callback = None ) : try : function . __func__ except AttributeError : return _WeakMethodFree ( function , callback ) return _WeakMethodBound ( function , callback )
2178	def authorization_url ( self , url , request_token = None , ** kwargs ) : kwargs [ "oauth_token" ] = request_token or self . _client . client . resource_owner_key log . debug ( "Adding parameters %s to url %s" , kwargs , url ) return add_params_to_uri ( url , kwargs . items ( ) )
5177	def resources ( self , type_ = None , title = None , ** kwargs ) : if type_ is None : resources = self . __api . resources ( query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) elif type_ is not None and title is None : resources = self . __api . resources ( type_ = type_ , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) else : resources = self . __api . resources ( type_ = type_ , title = title , query = EqualsOperator ( "certname" , self . name ) , ** kwargs ) return resources
13059	def get_inventory ( self ) : if self . _inventory is not None : return self . _inventory self . _inventory = self . resolver . getMetadata ( ) return self . _inventory
7565	def memoize ( func ) : class Memodict ( dict ) : def __getitem__ ( self , * key ) : return dict . __getitem__ ( self , key ) def __missing__ ( self , key ) : ret = self [ key ] = func ( * key ) return ret return Memodict ( ) . __getitem__
12708	def body_to_world ( self , position ) : return np . array ( self . ode_body . getRelPointPos ( tuple ( position ) ) )
7892	def join ( self , password = None , history_maxchars = None , history_maxstanzas = None , history_seconds = None , history_since = None ) : if self . joined : raise RuntimeError ( "Room is already joined" ) p = MucPresence ( to_jid = self . room_jid ) p . make_join_request ( password , history_maxchars , history_maxstanzas , history_seconds , history_since ) self . manager . stream . send ( p )
1904	def _find_zero ( cpu , constrs , ptr ) : offset = 0 while True : byt = cpu . read_int ( ptr + offset , 8 ) if issymbolic ( byt ) : if not solver . can_be_true ( constrs , byt != 0 ) : break else : if byt == 0 : break offset += 1 return offset
5253	def assemble_one ( asmcode , pc = 0 , fork = DEFAULT_FORK ) : try : instruction_table = instruction_tables [ fork ] asmcode = asmcode . strip ( ) . split ( ' ' ) instr = instruction_table [ asmcode [ 0 ] . upper ( ) ] if pc : instr . pc = pc if instr . operand_size > 0 : assert len ( asmcode ) == 2 instr . operand = int ( asmcode [ 1 ] , 0 ) return instr except : raise AssembleError ( "Something wrong at pc %d" % pc )
6915	def collection_worker ( task ) : lcfile , outdir , kwargs = task try : fakelcresults = make_fakelc ( lcfile , outdir , ** kwargs ) return fakelcresults except Exception as e : LOGEXCEPTION ( 'could not process %s into a fakelc' % lcfile ) return None
11139	def get_stats ( self ) : if self . __path is None : return 0 , 0 nfiles = 0 ndirs = 0 for fdict in self . get_repository_state ( ) : fdname = list ( fdict ) [ 0 ] if fdname == '' : continue if fdict [ fdname ] . get ( 'pyrepfileinfo' , False ) : nfiles += 1 elif fdict [ fdname ] . get ( 'pyrepdirinfo' , False ) : ndirs += 1 else : raise Exception ( 'Not sure what to do next. Please report issue' ) return ndirs , nfiles
6267	def set_time ( self , value : float ) : if value < 0 : value = 0 self . offset += self . get_time ( ) - value
1729	def do_statement ( source , start ) : start = pass_white ( source , start ) if not start < len ( source ) : return None , start if any ( startswith_keyword ( source [ start : ] , e ) for e in { 'case' , 'default' } ) : return None , start rest = source [ start : ] for key , meth in KEYWORD_METHODS . iteritems ( ) : if rest . startswith ( key ) : if len ( key ) == len ( rest ) or rest [ len ( key ) ] not in IDENTIFIER_PART : return meth ( source , start ) if rest [ 0 ] == '{' : return do_block ( source , start ) cand = parse_identifier ( source , start , False ) if cand is not None : label , cand_start = cand cand_start = pass_white ( source , cand_start ) if source [ cand_start ] == ':' : return do_label ( source , start ) return do_expression ( source , start )
7395	def get_publication ( context , id ) : pbl = Publication . objects . filter ( pk = int ( id ) ) if len ( pbl ) < 1 : return '' pbl [ 0 ] . links = pbl [ 0 ] . customlink_set . all ( ) pbl [ 0 ] . files = pbl [ 0 ] . customfile_set . all ( ) return render_template ( 'publications/publication.html' , context [ 'request' ] , { 'publication' : pbl [ 0 ] } )
12693	def aggregate ( self , clazz , new_col , * args ) : if is_callable ( clazz ) and not is_none ( new_col ) and has_elements ( * args ) and is_disjoint ( self . __grouping . grouping_colnames , args , __DISJOINT_SETS_ERROR__ ) : return self . __do_aggregate ( clazz , new_col , * args )
11528	def add_scalar_data ( self , token , community_id , producer_display_name , metric_name , producer_revision , submit_time , value , ** kwargs ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'communityId' ] = community_id parameters [ 'producerDisplayName' ] = producer_display_name parameters [ 'metricName' ] = metric_name parameters [ 'producerRevision' ] = producer_revision parameters [ 'submitTime' ] = submit_time parameters [ 'value' ] = value optional_keys = [ 'config_item_id' , 'test_dataset_id' , 'truth_dataset_id' , 'silent' , 'unofficial' , 'build_results_url' , 'branch' , 'extra_urls' , 'params' , 'submission_id' , 'submission_uuid' , 'unit' , 'reproduction_command' ] for key in optional_keys : if key in kwargs : if key == 'config_item_id' : parameters [ 'configItemId' ] = kwargs [ key ] elif key == 'test_dataset_id' : parameters [ 'testDatasetId' ] = kwargs [ key ] elif key == 'truth_dataset_id' : parameters [ 'truthDatasetId' ] = kwargs [ key ] elif key == 'build_results_url' : parameters [ 'buildResultsUrl' ] = kwargs [ key ] elif key == 'extra_urls' : parameters [ 'extraUrls' ] = json . dumps ( kwargs [ key ] ) elif key == 'params' : parameters [ key ] = json . dumps ( kwargs [ key ] ) elif key == 'silent' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'unofficial' : if kwargs [ key ] : parameters [ key ] = kwargs [ key ] elif key == 'submission_id' : parameters [ 'submissionId' ] = kwargs [ key ] elif key == 'submission_uuid' : parameters [ 'submissionUuid' ] = kwargs [ key ] elif key == 'unit' : parameters [ 'unit' ] = kwargs [ key ] elif key == 'reproduction_command' : parameters [ 'reproductionCommand' ] = kwargs [ key ] else : parameters [ key ] = kwargs [ key ] response = self . request ( 'midas.tracker.scalar.add' , parameters ) return response
3331	def release ( self ) : me = currentThread ( ) self . __condition . acquire ( ) try : if self . __writer is me : self . __writercount -= 1 if not self . __writercount : self . __writer = None self . __condition . notifyAll ( ) elif me in self . __readers : self . __readers [ me ] -= 1 if not self . __readers [ me ] : del self . __readers [ me ] if not self . __readers : self . __condition . notifyAll ( ) else : raise ValueError ( "Trying to release unheld lock" ) finally : self . __condition . release ( )
13453	def spawn ( func , * args , ** kwargs ) : return gevent . spawn ( wrap_uncaught_greenlet_exceptions ( func ) , * args , ** kwargs )
10923	def finish ( s , desc = 'finish' , n_loop = 4 , max_mem = 1e9 , separate_psf = True , fractol = 1e-7 , errtol = 1e-3 , dowarn = True ) : values = [ np . copy ( s . state [ s . params ] ) ] remove_params = s . get ( 'psf' ) . params if separate_psf else None global_params = name_globals ( s , remove_params = remove_params ) gs = np . floor ( max_mem / s . residuals . nbytes ) . astype ( 'int' ) groups = [ global_params [ a : a + gs ] for a in range ( 0 , len ( global_params ) , gs ) ] CLOG . info ( 'Start ``finish``:\t{}' . format ( s . error ) ) for a in range ( n_loop ) : start_err = s . error for g in groups : do_levmarq ( s , g , damping = 0.1 , decrease_damp_factor = 20. , max_iter = 1 , max_mem = max_mem , eig_update = False ) if separate_psf : do_levmarq ( s , remove_params , max_mem = max_mem , max_iter = 4 , eig_update = False ) CLOG . info ( 'Globals, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) do_levmarq_all_particle_groups ( s , max_iter = 1 , max_mem = max_mem ) CLOG . info ( 'Particles, loop {}:\t{}' . format ( a , s . error ) ) if desc is not None : states . save ( s , desc = desc ) values . append ( np . copy ( s . state [ s . params ] ) ) new_err = s . error derr = start_err - new_err dobreak = ( derr / new_err < fractol ) or ( derr < errtol ) if dobreak : break if dowarn and ( not dobreak ) : CLOG . warn ( 'finish() did not converge; consider re-running' ) return { 'converged' : dobreak , 'loop_values' : np . array ( values ) }
6890	def read_csv_lightcurve ( lcfile ) : if '.gz' in os . path . basename ( lcfile ) : LOGINFO ( 'reading gzipped K2 LC: %s' % lcfile ) infd = gzip . open ( lcfile , 'rb' ) else : LOGINFO ( 'reading K2 LC: %s' % lcfile ) infd = open ( lcfile , 'rb' ) lctext = infd . read ( ) . decode ( ) infd . close ( ) lcstart = lctext . index ( '# LIGHTCURVE\n' ) lcheader = lctext [ : lcstart + 12 ] lccolumns = lctext [ lcstart + 13 : ] . split ( '\n' ) lccolumns = [ x . split ( ',' ) for x in lccolumns if len ( x ) > 0 ] lcdict = _parse_csv_header ( lcheader ) lccolumns = list ( zip ( * lccolumns ) ) for colind , col in enumerate ( lcdict [ 'columns' ] ) : lcdict [ col . lower ( ) ] = np . array ( [ COLUMNDEFS [ col ] [ 2 ] ( x ) for x in lccolumns [ colind ] ] ) lcdict [ 'columns' ] = [ x . lower ( ) for x in lcdict [ 'columns' ] ] return lcdict
922	def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False return True
4538	def single ( method ) : @ functools . wraps ( method ) def single ( self , address , value = None ) : address = urllib . parse . unquote_plus ( address ) try : error = NO_PROJECT_ERROR if not self . project : raise ValueError error = BAD_ADDRESS_ERROR ed = editor . Editor ( address , self . project ) if value is None : error = BAD_GETTER_ERROR result = method ( self , ed ) else : error = BAD_SETTER_ERROR result = method ( self , ed , value ) result = { 'value' : result } except Exception as e : traceback . print_exc ( ) msg = '%s\n%s' % ( error . format ( ** locals ( ) ) , e ) result = { 'error' : msg } return flask . jsonify ( result ) return single
12520	def _load_images_and_labels ( self , images , labels = None ) : if not isinstance ( images , ( list , tuple ) ) : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects. ' 'Got a {}.' . format ( type ( images ) ) ) if not len ( images ) > 0 : raise ValueError ( 'Expected an iterable (list or tuple) of strings or img-like objects ' 'of size higher than 0. Got {} items.' . format ( len ( images ) ) ) if labels is not None and len ( labels ) != len ( images ) : raise ValueError ( 'Expected the same length for image set ({}) and ' 'labels list ({}).' . format ( len ( images ) , len ( labels ) ) ) first_file = images [ 0 ] if first_file : first_img = NeuroImage ( first_file ) else : raise ( 'Error reading image {}.' . format ( repr_imgs ( first_file ) ) ) for idx , image in enumerate ( images ) : try : img = NeuroImage ( image ) self . check_compatibility ( img , first_img ) except : log . exception ( 'Error reading image {}.' . format ( repr_imgs ( image ) ) ) raise else : self . items . append ( img ) self . set_labels ( labels )
2296	def featurize_row ( self , x , y ) : x = x . ravel ( ) y = y . ravel ( ) b = np . ones ( x . shape ) dx = np . cos ( np . dot ( self . W2 , np . vstack ( ( x , b ) ) ) ) . mean ( 1 ) dy = np . cos ( np . dot ( self . W2 , np . vstack ( ( y , b ) ) ) ) . mean ( 1 ) if ( sum ( dx ) > sum ( dy ) ) : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( x , y , b ) ) ) ) . mean ( 1 ) ) ) else : return np . hstack ( ( dx , dy , np . cos ( np . dot ( self . W , np . vstack ( ( y , x , b ) ) ) ) . mean ( 1 ) ) )
830	def pprint ( self , output , prefix = "" ) : print prefix , description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) - 1 ) : offset = description [ i ] [ 1 ] nextoffset = description [ i + 1 ] [ 1 ] print "%s |" % bitsToString ( output [ offset : nextoffset ] ) , print
8987	def last_produced_mesh ( self ) : for instruction in reversed ( self . instructions ) : if instruction . produces_meshes ( ) : return instruction . last_produced_mesh raise IndexError ( "{} produces no meshes" . format ( self ) )
1769	def concrete_emulate ( self , insn ) : if not self . emu : self . emu = ConcreteUnicornEmulator ( self ) self . emu . _stop_at = self . _break_unicorn_at try : self . emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) )
12471	def get_abspath ( folderpath ) : if not op . exists ( folderpath ) : raise FolderNotFound ( folderpath ) return op . abspath ( folderpath )
445	def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" ) : data_files = [ ] for pattern in file_pattern . split ( "," ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( "Found no input files matching %s" , file_pattern ) else : tl . logging . info ( "Prefetching values from %d files matching %s" , len ( data_files ) , file_pattern ) if is_training : print ( " is_training == True : RandomShuffleQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = "random_" + value_queue_name ) else : print ( " is_training == False : FIFOQueue" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( "queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue
5978	def mask_blurring_from_mask_and_psf_shape ( mask , psf_shape ) : blurring_mask = np . full ( mask . shape , True ) for y in range ( mask . shape [ 0 ] ) : for x in range ( mask . shape [ 1 ] ) : if not mask [ y , x ] : for y1 in range ( ( - psf_shape [ 0 ] + 1 ) // 2 , ( psf_shape [ 0 ] + 1 ) // 2 ) : for x1 in range ( ( - psf_shape [ 1 ] + 1 ) // 2 , ( psf_shape [ 1 ] + 1 ) // 2 ) : if 0 <= x + x1 <= mask . shape [ 1 ] - 1 and 0 <= y + y1 <= mask . shape [ 0 ] - 1 : if mask [ y + y1 , x + x1 ] : blurring_mask [ y + y1 , x + x1 ] = False else : raise exc . MaskException ( "setup_blurring_mask extends beyond the sub_grid_size of the masks - pad the " "datas array before masking" ) return blurring_mask
1357	def get_argument_topology ( self ) : try : topology = self . get_argument ( constants . PARAM_TOPOLOGY ) return topology except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
6925	def autocommit ( self ) : if len ( self . cursors . keys ( ) ) == 0 : self . connection . autocommit = True else : raise AttributeError ( 'database cursors are already active, ' 'cannot switch to autocommit now' )
8721	def operation_download ( uploader , sources ) : sources , destinations = destination_from_source ( sources , False ) print ( 'sources' , sources ) print ( 'destinations' , destinations ) if len ( destinations ) == len ( sources ) : if uploader . prepare ( ) : for filename , dst in zip ( sources , destinations ) : uploader . read_file ( filename , dst ) else : raise Exception ( 'You must specify a destination filename for each file you want to download.' ) log . info ( 'All done!' )
12279	def add ( repo , args , targetdir , execute = False , generator = False , includes = [ ] , script = False , source = None ) : if not execute : files = add_files ( args = args , targetdir = targetdir , source = source , script = script , generator = generator ) else : files = run_executable ( repo , args , includes ) if files is None or len ( files ) == 0 : return repo filtered_files = [ ] package = repo . package for h in files : found = False for i , r in enumerate ( package [ 'resources' ] ) : if h [ 'relativepath' ] == r [ 'relativepath' ] : found = True if h [ 'sha256' ] == r [ 'sha256' ] : change = False for attr in [ 'source' ] : if h [ attr ] != r [ attr ] : r [ attr ] = h [ attr ] change = True if change : filtered_files . append ( h ) continue else : filtered_files . append ( h ) package [ 'resources' ] [ i ] = h break if not found : filtered_files . append ( h ) package [ 'resources' ] . append ( h ) if len ( filtered_files ) == 0 : return 0 repo . manager . add_files ( repo , filtered_files ) rootdir = repo . rootdir with cd ( rootdir ) : datapath = "datapackage.json" with open ( datapath , 'w' ) as fd : fd . write ( json . dumps ( package , indent = 4 ) ) return len ( filtered_files )
7254	def heartbeat ( self ) : url = '%s/heartbeat' % self . base_url r = requests . get ( url ) try : return r . json ( ) == "ok" except : return False
1412	def _get_topologies_with_watch ( self , callback , isWatching ) : path = self . get_topologies_path ( ) if isWatching : LOG . info ( "Adding children watch for path: " + path ) @ self . client . ChildrenWatch ( path ) def watch_topologies ( topologies ) : callback ( topologies ) return isWatching
10921	def do_levmarq_all_particle_groups ( s , region_size = 40 , max_iter = 2 , damping = 1.0 , decrease_damp_factor = 10. , run_length = 4 , collect_stats = False , ** kwargs ) : lp = LMParticleGroupCollection ( s , region_size = region_size , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , get_cos = collect_stats , max_iter = max_iter , ** kwargs ) lp . do_run_2 ( ) if collect_stats : return lp . stats
302	def plot_daily_volume ( returns , transactions , ax = None , ** kwargs ) : if ax is None : ax = plt . gca ( ) daily_txn = txn . get_txn_vol ( transactions ) daily_txn . txn_shares . plot ( alpha = 1.0 , lw = 0.5 , ax = ax , ** kwargs ) ax . axhline ( daily_txn . txn_shares . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . set_title ( 'Daily trading volume' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylabel ( 'Amount of shares traded' ) ax . set_xlabel ( '' ) return ax
9203	def render ( node , strict = False ) : if isinstance ( node , list ) : return render_list ( node ) elif isinstance ( node , dict ) : return render_node ( node , strict = strict ) else : raise NotImplementedError ( "You tried to render a %s. Only list and dicts can be rendered." % node . __class__ . __name__ )
258	def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )
7717	def _roster_set ( self , item , callback , error_callback ) : stanza = Iq ( to_jid = self . server , stanza_type = "set" ) payload = RosterPayload ( [ item ] ) stanza . set_payload ( payload ) def success_cb ( result_stanza ) : if callback : callback ( item ) def error_cb ( error_stanza ) : if error_callback : error_callback ( error_stanza ) else : logger . error ( "Roster change of '{0}' failed" . format ( item . jid ) ) processor = self . stanza_processor processor . set_response_handlers ( stanza , success_cb , error_cb ) processor . send ( stanza )
10006	def clear_obj ( self , obj ) : removed = self . cellgraph . clear_obj ( obj ) for node in removed : del node [ OBJ ] . data [ node [ KEY ] ]
4438	async def _previous ( self , ctx ) : player = self . bot . lavalink . players . get ( ctx . guild . id ) try : await player . play_previous ( ) except lavalink . NoPreviousTrack : await ctx . send ( 'There is no previous song to play.' )
4372	def get_socket ( self , sessid = '' ) : socket = self . sockets . get ( sessid ) if sessid and not socket : return None if socket is None : socket = Socket ( self , self . config ) self . sockets [ socket . sessid ] = socket else : socket . incr_hits ( ) return socket
5344	def compose_gerrit ( projects ) : git_projects = [ project for project in projects if 'git' in projects [ project ] ] for project in git_projects : repos = [ repo for repo in projects [ project ] [ 'git' ] if 'gitroot' in repo ] if len ( repos ) > 0 : projects [ project ] [ 'gerrit' ] = [ ] for repo in repos : gerrit_project = repo . replace ( "http://git.eclipse.org/gitroot/" , "" ) gerrit_project = gerrit_project . replace ( ".git" , "" ) projects [ project ] [ 'gerrit' ] . append ( "git.eclipse.org_" + gerrit_project ) return projects
6854	def getdevice_by_uuid ( uuid ) : with settings ( hide ( 'running' , 'warnings' , 'stdout' ) , warn_only = True ) : res = run_as_root ( 'blkid -U %s' % uuid ) if not res . succeeded : return None return res
10014	def create_archive ( directory , filename , config = { } , ignore_predicate = None , ignored_files = [ '.git' , '.svn' ] ) : with zipfile . ZipFile ( filename , 'w' , compression = zipfile . ZIP_DEFLATED ) as zip_file : root_len = len ( os . path . abspath ( directory ) ) out ( "Creating archive: " + str ( filename ) ) for root , dirs , files in os . walk ( directory , followlinks = True ) : archive_root = os . path . abspath ( root ) [ root_len + 1 : ] for f in files : fullpath = os . path . join ( root , f ) archive_name = os . path . join ( archive_root , f ) if filename in fullpath : continue if ignored_files is not None : for name in ignored_files : if fullpath . endswith ( name ) : out ( "Skipping: " + str ( name ) ) continue if ignore_predicate is not None : if not ignore_predicate ( archive_name ) : out ( "Skipping: " + str ( archive_name ) ) continue out ( "Adding: " + str ( archive_name ) ) zip_file . write ( fullpath , archive_name , zipfile . ZIP_DEFLATED ) return filename
6187	def get_last_commit ( git_path = None ) : if git_path is None : git_path = GIT_PATH line = get_last_commit_line ( git_path ) revision_id = line . split ( ) [ 1 ] return revision_id
11580	def _string_data ( self , data ) : print ( "_string_data:" ) string_to_print = [ ] for i in data [ : : 2 ] : string_to_print . append ( chr ( i ) ) print ( "" . join ( string_to_print ) )
7514	def enter_pairs ( iloc , pnames , snppad , edg , aseqs , asnps , smask , samplecov , locuscov , start ) : LOGGER . info ( "edges in enter_pairs %s" , edg ) seq1 = aseqs [ iloc , : , edg [ 0 ] : edg [ 1 ] + 1 ] snp1 = asnps [ iloc , edg [ 0 ] : edg [ 1 ] + 1 , ] seq2 = aseqs [ iloc , : , edg [ 2 ] : edg [ 3 ] + 1 ] snp2 = asnps [ iloc , edg [ 2 ] : edg [ 3 ] + 1 , ] nalln = np . all ( seq1 == "N" , axis = 1 ) nsidx = nalln + smask LOGGER . info ( "nsidx %s, nalln %s, smask %s" , nsidx , nalln , smask ) samplecov = samplecov + np . invert ( nsidx ) . astype ( np . int32 ) LOGGER . info ( "samplecov %s" , samplecov ) idx = np . sum ( np . invert ( nsidx ) . astype ( np . int32 ) ) LOGGER . info ( "idx %s" , idx ) locuscov [ idx ] += 1 seq1 = seq1 [ ~ nsidx , ] seq2 = seq2 [ ~ nsidx , ] names = pnames [ ~ nsidx ] outstr = "\n" . join ( [ name + s1 . tostring ( ) + "nnnn" + s2 . tostring ( ) for name , s1 , s2 in zip ( names , seq1 , seq2 ) ] ) snpstring1 = [ "-" if snp1 [ i , 0 ] else "*" if snp1 [ i , 1 ] else " " for i in range ( len ( snp1 ) ) ] snpstring2 = [ "-" if snp2 [ i , 0 ] else "*" if snp2 [ i , 1 ] else " " for i in range ( len ( snp2 ) ) ] outstr += "\n" + snppad + "" . join ( snpstring1 ) + " " + "" . join ( snpstring2 ) + "|{}|" . format ( iloc + start ) return outstr , samplecov , locuscov
6698	def preseed_package ( pkg_name , preseed ) : for q_name , _ in preseed . items ( ) : q_type , q_answer = _ run_as_root ( 'echo "%(pkg_name)s %(q_name)s %(q_type)s %(q_answer)s" | debconf-set-selections' % locals ( ) )
4175	def window_blackman ( N , alpha = 0.16 ) : r a0 = ( 1. - alpha ) / 2. a1 = 0.5 a2 = alpha / 2. if ( N == 1 ) : win = array ( [ 1. ] ) else : k = arange ( 0 , N ) / float ( N - 1. ) win = a0 - a1 * cos ( 2 * pi * k ) + a2 * cos ( 4 * pi * k ) return win
3067	def clean_headers ( headers ) : clean = { } try : for k , v in six . iteritems ( headers ) : if not isinstance ( k , six . binary_type ) : k = str ( k ) if not isinstance ( v , six . binary_type ) : v = str ( v ) clean [ _helpers . _to_bytes ( k ) ] = _helpers . _to_bytes ( v ) except UnicodeEncodeError : from oauth2client . client import NonAsciiHeaderError raise NonAsciiHeaderError ( k , ': ' , v ) return clean
11967	def _BYTES_TO_BITS ( ) : the_table = 256 * [ None ] bits_per_byte = list ( range ( 7 , - 1 , - 1 ) ) for n in range ( 256 ) : l = n bits = 8 * [ None ] for i in bits_per_byte : bits [ i ] = '01' [ n & 1 ] n >>= 1 the_table [ l ] = '' . join ( bits ) return the_table
2625	def submit ( self , command = 'sleep 1' , blocksize = 1 , tasks_per_node = 1 , job_name = "parsl.auto" ) : job_name = "parsl.auto.{0}" . format ( time . time ( ) ) wrapped_cmd = self . launcher ( command , tasks_per_node , self . nodes_per_block ) [ instance , * rest ] = self . spin_up_instance ( command = wrapped_cmd , job_name = job_name ) if not instance : logger . error ( "Failed to submit request to EC2" ) return None logger . debug ( "Started instance_id: {0}" . format ( instance . instance_id ) ) state = translate_table . get ( instance . state [ 'Name' ] , "PENDING" ) self . resources [ instance . instance_id ] = { "job_id" : instance . instance_id , "instance" : instance , "status" : state } return instance . instance_id
11710	def request ( self , path , data = None , headers = None , method = None ) : if isinstance ( data , str ) : data = data . encode ( 'utf-8' ) response = urlopen ( self . _request ( path , data = data , headers = headers , method = method ) ) self . _set_session_cookie ( response ) return response
4291	def cleanup_directory ( config_data ) : if os . path . exists ( config_data . project_directory ) : choice = False if config_data . noinput is False and not config_data . verbose : choice = query_yes_no ( 'The installation failed.\n' 'Do you want to clean up by removing {0}?\n' '\tWarning: this will delete all files in:\n' '\t\t{0}\n' 'Do you want to cleanup?' . format ( os . path . abspath ( config_data . project_directory ) ) , 'no' ) else : sys . stdout . write ( 'The installation has failed.\n' ) if config_data . skip_project_dir_check is False and ( choice or ( config_data . noinput and config_data . delete_project_dir ) ) : sys . stdout . write ( 'Removing everything under {0}\n' . format ( os . path . abspath ( config_data . project_directory ) ) ) shutil . rmtree ( config_data . project_directory , True )
10995	def _barnes ( self , pos ) : b_in = self . b_in dist = lambda x : np . sqrt ( np . dot ( x , x ) ) sz = self . npts [ 1 ] coeffs = self . get_values ( self . barnes_params ) b = BarnesInterpolationND ( b_in , coeffs , filter_size = self . filtsize , damp = 0.9 , iterations = 3 , clip = self . local_updates , clipsize = self . barnes_clip_size , blocksize = 100 ) return b ( pos )
13660	def subroute ( self , * components ) : def _factory ( f ) : self . _addRoute ( f , subroute ( * components ) ) return f return _factory
6452	def dist_abs ( self , src , tar ) : if tar == src : return 0 elif not src : return len ( tar ) elif not tar : return len ( src ) src_bag = Counter ( src ) tar_bag = Counter ( tar ) return max ( sum ( ( src_bag - tar_bag ) . values ( ) ) , sum ( ( tar_bag - src_bag ) . values ( ) ) , )
1848	def LJMP ( cpu , cs_selector , target ) : logger . info ( "LJMP: Jumping to: %r:%r" , cs_selector . read ( ) , target . read ( ) ) cpu . CS = cs_selector . read ( ) cpu . PC = target . read ( )
7477	def count_seeds ( usort ) : with open ( usort , 'r' ) as insort : cmd1 = [ "cut" , "-f" , "2" ] cmd2 = [ "uniq" ] cmd3 = [ "wc" ] proc1 = sps . Popen ( cmd1 , stdin = insort , stdout = sps . PIPE , close_fds = True ) proc2 = sps . Popen ( cmd2 , stdin = proc1 . stdout , stdout = sps . PIPE , close_fds = True ) proc3 = sps . Popen ( cmd3 , stdin = proc2 . stdout , stdout = sps . PIPE , close_fds = True ) res = proc3 . communicate ( ) nseeds = int ( res [ 0 ] . split ( ) [ 0 ] ) proc1 . stdout . close ( ) proc2 . stdout . close ( ) proc3 . stdout . close ( ) return nseeds
11994	def set_algorithms ( self , signature = None , encryption = None , serialization = None , compression = None ) : self . signature_algorithms = self . _update_dict ( signature , self . DEFAULT_SIGNATURE ) self . encryption_algorithms = self . _update_dict ( encryption , self . DEFAULT_ENCRYPTION ) self . serialization_algorithms = self . _update_dict ( serialization , self . DEFAULT_SERIALIZATION ) self . compression_algorithms = self . _update_dict ( compression , self . DEFAULT_COMPRESSION )
6606	def run_multiple ( self , workingArea , package_indices ) : if not package_indices : return [ ] job_desc = self . _compose_job_desc ( workingArea , package_indices ) clusterprocids = submit_jobs ( job_desc , cwd = workingArea . path ) clusterids = clusterprocids2clusterids ( clusterprocids ) for clusterid in clusterids : change_job_priority ( [ clusterid ] , 10 ) self . clusterprocids_outstanding . extend ( clusterprocids ) return clusterprocids
5707	def get_lockdown_form ( form_path ) : if not form_path : raise ImproperlyConfigured ( 'No LOCKDOWN_FORM specified.' ) form_path_list = form_path . split ( "." ) new_module = "." . join ( form_path_list [ : - 1 ] ) attr = form_path_list [ - 1 ] try : mod = import_module ( new_module ) except ( ImportError , ValueError ) : raise ImproperlyConfigured ( 'Module configured in LOCKDOWN_FORM (%s) to' ' contain the form class couldn\'t be ' 'found.' % new_module ) try : form = getattr ( mod , attr ) except AttributeError : raise ImproperlyConfigured ( 'The module configured in LOCKDOWN_FORM ' ' (%s) doesn\'t define a "%s" form.' % ( new_module , attr ) ) return form
13501	def update_time ( sender , ** kwargs ) : comment = kwargs [ 'instance' ] if comment . content_type . app_label == "happenings" and comment . content_type . name == "Update" : from . models import Update item = Update . objects . get ( id = comment . object_pk ) item . save ( )
3263	def get_workspace ( self , name ) : workspaces = self . get_workspaces ( names = name ) return self . _return_first_item ( workspaces )
3270	def md_entry ( node ) : key = None value = None if 'key' in node . attrib : key = node . attrib [ 'key' ] else : key = None if key in [ 'time' , 'elevation' ] or key . startswith ( 'custom_dimension' ) : value = md_dimension_info ( key , node . find ( "dimensionInfo" ) ) elif key == 'DynamicDefaultValues' : value = md_dynamic_default_values_info ( key , node . find ( "DynamicDefaultValues" ) ) elif key == 'JDBC_VIRTUAL_TABLE' : value = md_jdbc_virtual_table ( key , node . find ( "virtualTable" ) ) else : value = node . text if None in [ key , value ] : return None else : return ( key , value )
12262	def add ( self , operator , * args ) : if isinstance ( operator , str ) : op = getattr ( proxops , operator ) ( * args ) elif isinstance ( operator , proxops . ProximalOperatorBaseClass ) : op = operator else : raise ValueError ( "operator must be a string or a subclass of ProximalOperator" ) self . operators . append ( op ) return self
10429	def getrowcount ( self , window_name , object_name ) : object_handle = self . _get_object_handle ( window_name , object_name ) if not object_handle . AXEnabled : raise LdtpServerException ( u"Object %s state disabled" % object_name ) return len ( object_handle . AXRows )
7674	def slice ( self , start_time , end_time , strict = False ) : if self . file_metadata . duration is None : raise JamsError ( 'Duration must be set (jam.file_metadata.duration) before ' 'slicing can be performed.' ) if ( start_time < 0 or start_time > float ( self . file_metadata . duration ) or end_time < start_time or end_time > float ( self . file_metadata . duration ) ) : raise ParameterError ( 'start_time and end_time must be within the original file ' 'duration ({:f}) and end_time cannot be smaller than ' 'start_time.' . format ( float ( self . file_metadata . duration ) ) ) jam_sliced = JAMS ( annotations = None , file_metadata = self . file_metadata , sandbox = self . sandbox ) jam_sliced . annotations = self . annotations . slice ( start_time , end_time , strict = strict ) jam_sliced . file_metadata . duration = end_time - start_time if 'slice' not in jam_sliced . sandbox . keys ( ) : jam_sliced . sandbox . update ( slice = [ { 'start_time' : start_time , 'end_time' : end_time } ] ) else : jam_sliced . sandbox . slice . append ( { 'start_time' : start_time , 'end_time' : end_time } ) return jam_sliced
6402	def fingerprint ( self , phrase , joiner = ' ' ) : phrase = unicode_normalize ( 'NFKD' , text_type ( phrase . strip ( ) . lower ( ) ) ) phrase = '' . join ( [ c for c in phrase if c . isalnum ( ) or c . isspace ( ) ] ) phrase = joiner . join ( sorted ( list ( set ( phrase . split ( ) ) ) ) ) return phrase
9113	def message ( self ) : try : with open ( join ( self . fs_path , u'message' ) ) as message_file : return u'' . join ( [ line . decode ( 'utf-8' ) for line in message_file . readlines ( ) ] ) except IOError : return u''
7313	def process_request ( self , request ) : if not request : return if not db_loaded : load_db ( ) tz = request . session . get ( 'django_timezone' ) if not tz : tz = timezone . get_default_timezone ( ) client_ip = get_ip_address_from_request ( request ) ip_addrs = client_ip . split ( ',' ) for ip in ip_addrs : if is_valid_ip ( ip ) and not is_local_ip ( ip ) : if ':' in ip : tz = db_v6 . time_zone_by_addr ( ip ) break else : tz = db . time_zone_by_addr ( ip ) break if tz : timezone . activate ( tz ) request . session [ 'django_timezone' ] = str ( tz ) if getattr ( settings , 'AUTH_USER_MODEL' , None ) and getattr ( request , 'user' , None ) : detected_timezone . send ( sender = get_user_model ( ) , instance = request . user , timezone = tz ) else : timezone . deactivate ( )
4434	async def update_state ( self , data ) : guild_id = int ( data [ 'guildId' ] ) if guild_id in self . players : player = self . players . get ( guild_id ) player . position = data [ 'state' ] . get ( 'position' , 0 ) player . position_timestamp = data [ 'state' ] [ 'time' ]
5584	def prepare_path ( self , tile ) : makedirs ( os . path . dirname ( self . get_path ( tile ) ) )
9461	def conference_kick ( self , call_params ) : path = '/' + self . api_version + '/ConferenceKick/' method = 'POST' return self . request ( path , method , call_params )
2768	def get_volume ( self , volume_id ) : return Volume . get_object ( api_token = self . token , volume_id = volume_id )
8473	def _addConfig ( instance , config , parent_section ) : try : section_name = "{p}/{n}" . format ( p = parent_section , n = instance . NAME . lower ( ) ) config . add_section ( section_name ) for k in instance . CONFIG . keys ( ) : config . set ( section_name , k , instance . CONFIG [ k ] ) except Exception as e : print "[!] %s" % e
1929	def process_config_values ( parser : argparse . ArgumentParser , args : argparse . Namespace ) : load_overrides ( args . config ) defined_vars = list ( get_config_keys ( ) ) command_line_args = vars ( args ) config_cli_args = get_group ( 'cli' ) for k in command_line_args : default = parser . get_default ( k ) set_val = getattr ( args , k ) if default is not set_val : if k not in defined_vars : config_cli_args . update ( k , value = set_val ) else : group_name , key = k . split ( '.' ) group = get_group ( group_name ) setattr ( group , key , set_val ) else : if k in config_cli_args : setattr ( args , k , getattr ( config_cli_args , k ) )
8038	def code_mapping ( level , msg , default = 99 ) : try : return code_mappings_by_level [ level ] [ msg ] except KeyError : pass if msg . count ( '"' ) == 2 and ' "' in msg and msg . endswith ( '".' ) : txt = msg [ : msg . index ( ' "' ) ] return code_mappings_by_level [ level ] . get ( txt , default ) return default
9026	def insert_defs ( self , defs ) : if self . _svg [ "defs" ] is None : self . _svg [ "defs" ] = { } for def_ in defs : for key , value in def_ . items ( ) : if key . startswith ( "@" ) : continue if key not in self . _svg [ "defs" ] : self . _svg [ "defs" ] [ key ] = [ ] if not isinstance ( value , list ) : value = [ value ] self . _svg [ "defs" ] [ key ] . extend ( value )
9494	def compile ( code : list , consts : list , names : list , varnames : list , func_name : str = "<unknown, compiled>" , arg_count : int = 0 , kwarg_defaults : Tuple [ Any ] = ( ) , use_safety_wrapper : bool = True ) : varnames = tuple ( varnames ) consts = tuple ( consts ) names = tuple ( names ) code = util . flatten ( code ) if arg_count > len ( varnames ) : raise CompileError ( "arg_count > len(varnames)" ) if len ( kwarg_defaults ) > len ( varnames ) : raise CompileError ( "len(kwarg_defaults) > len(varnames)" ) bc = compile_bytecode ( code ) dis . dis ( bc ) if PY36 : pass else : if bc [ - 1 ] != tokens . RETURN_VALUE : raise CompileError ( "No default RETURN_VALUE. Add a `pyte.tokens.RETURN_VALUE` to the end of your " "bytecode if you don't need one." ) flags = 1 | 2 | 64 frame_data = inspect . stack ( ) [ 1 ] if sys . version_info [ 0 : 2 ] > ( 3 , 3 ) : stack_size = _simulate_stack ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) else : warnings . warn ( "Cannot check stack for safety." ) stack_size = 99 _optimize_warn_pass ( dis . _get_instructions_bytes ( bc , constants = consts , names = names , varnames = varnames ) ) obb = types . CodeType ( arg_count , 0 , len ( varnames ) , stack_size , flags , bc , consts , names , varnames , frame_data [ 1 ] , func_name , frame_data [ 2 ] , b'' , ( ) , ( ) ) f_globals = frame_data [ 0 ] . f_globals f = types . FunctionType ( obb , f_globals ) f . __name__ = func_name f . __defaults__ = kwarg_defaults if use_safety_wrapper : def __safety_wrapper ( * args , ** kwargs ) : try : return f ( * args , ** kwargs ) except SystemError as e : if 'opcode' not in ' ' . join ( e . args ) : raise msg = "Bytecode exception!" "\nFunction {} returned an invalid opcode." "\nFunction dissection:\n\n" . format ( f . __name__ ) file = io . StringIO ( ) with contextlib . redirect_stdout ( file ) : dis . dis ( f ) msg += file . getvalue ( ) raise SystemError ( msg ) from e returned_func = __safety_wrapper returned_func . wrapped = f else : returned_func = f return returned_func
8205	def flush ( self , frame ) : self . sink . render ( self . size_or_default ( ) , frame , self . _drawqueue ) self . reset_drawqueue ( )
9267	def get_time_of_tag ( self , tag ) : if not tag : raise ChangelogGeneratorError ( "tag is nil" ) name_of_tag = tag [ "name" ] time_for_name = self . tag_times_dict . get ( name_of_tag , None ) if time_for_name : return time_for_name else : time_string = self . fetcher . fetch_date_of_tag ( tag ) try : self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) except UnicodeWarning : print ( "ERROR ERROR:" , tag ) self . tag_times_dict [ name_of_tag ] = timestring_to_datetime ( time_string ) return self . tag_times_dict [ name_of_tag ]
7929	def _start_thread ( self ) : with self . lock : if self . threads and self . queue . empty ( ) : return if len ( self . threads ) >= self . max_threads : return thread_n = self . last_thread_n + 1 self . last_thread_n = thread_n thread = threading . Thread ( target = self . _run , name = "{0!r} #{1}" . format ( self , thread_n ) , args = ( thread_n , ) ) self . threads . append ( thread ) thread . daemon = True thread . start ( )
4297	def dump_config_file ( filename , args , parser = None ) : config = ConfigParser ( ) config . add_section ( SECTION ) if parser is None : for attr in args : config . set ( SECTION , attr , args . attr ) else : keys_empty_values_not_pass = ( '--extra-settings' , '--languages' , '--requirements' , '--template' , '--timezone' ) for action in parser . _actions : if action . dest in ( 'help' , 'config_file' , 'config_dump' , 'project_name' ) : continue keyp = action . option_strings [ 0 ] option_name = keyp . lstrip ( '-' ) option_value = getattr ( args , action . dest ) if any ( [ i for i in keys_empty_values_not_pass if i in action . option_strings ] ) : if action . dest == 'languages' : if len ( option_value ) == 1 and option_value [ 0 ] == 'en' : config . set ( SECTION , option_name , '' ) else : config . set ( SECTION , option_name , ',' . join ( option_value ) ) else : config . set ( SECTION , option_name , option_value if option_value else '' ) elif action . choices == ( 'yes' , 'no' ) : config . set ( SECTION , option_name , 'yes' if option_value else 'no' ) elif action . dest == 'templates' : config . set ( SECTION , option_name , option_value if option_value else 'no' ) elif action . dest == 'cms_version' : version = ( 'stable' if option_value == CMS_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . dest == 'django_version' : version = ( 'stable' if option_value == DJANGO_VERSION_MATRIX [ 'stable' ] else option_value ) config . set ( SECTION , option_name , version ) elif action . const : config . set ( SECTION , option_name , 'true' if option_value else 'false' ) else : config . set ( SECTION , option_name , str ( option_value ) ) with open ( filename , 'w' ) as fp : config . write ( fp )
13363	def echo_via_pager ( text , color = None ) : color = resolve_color_default ( color ) if not isinstance ( text , string_types ) : text = text_type ( text ) from . _termui_impl import pager return pager ( text + '\n' , color )
7926	def shuffle_srv ( records ) : if not records : return [ ] ret = [ ] while len ( records ) > 1 : weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 thres = random . random ( ) * weight_sum weight_sum = 0 for rrecord in records : weight_sum += rrecord . weight + 0.1 if thres < weight_sum : records . remove ( rrecord ) ret . append ( rrecord ) break ret . append ( records [ 0 ] ) return ret
6020	def simulate_as_gaussian ( cls , shape , pixel_scale , sigma , centre = ( 0.0 , 0.0 ) , axis_ratio = 1.0 , phi = 0.0 ) : from autolens . model . profiles . light_profiles import EllipticalGaussian gaussian = EllipticalGaussian ( centre = centre , axis_ratio = axis_ratio , phi = phi , intensity = 1.0 , sigma = sigma ) grid_1d = grid_util . regular_grid_1d_masked_from_mask_pixel_scales_and_origin ( mask = np . full ( shape , False ) , pixel_scales = ( pixel_scale , pixel_scale ) ) gaussian_1d = gaussian . intensities_from_grid ( grid = grid_1d ) gaussian_2d = mapping_util . map_unmasked_1d_array_to_2d_array_from_array_1d_and_shape ( array_1d = gaussian_1d , shape = shape ) return PSF ( array = gaussian_2d , pixel_scale = pixel_scale , renormalize = True )
4287	def get_thumb ( settings , filename ) : path , filen = os . path . split ( filename ) name , ext = os . path . splitext ( filen ) if ext . lower ( ) in settings [ 'video_extensions' ] : ext = '.jpg' return join ( path , settings [ 'thumb_dir' ] , settings [ 'thumb_prefix' ] + name + settings [ 'thumb_suffix' ] + ext )
10141	def parse_arguments ( args , clone_list ) : returned_string = "" host_number = args . host if args . show_list : print ( generate_host_string ( clone_list , "Available hosts: " ) ) exit ( ) if args . decrypt : for i in args . files : print ( decrypt_files ( i ) ) exit ( ) if args . files : for i in args . files : if args . limit_size : if args . host == host_number and host_number is not None : if not check_max_filesize ( i , clone_list [ host_number ] [ 3 ] ) : host_number = None for n , host in enumerate ( clone_list ) : if not check_max_filesize ( i , host [ 3 ] ) : clone_list [ n ] = None if not clone_list : print ( 'None of the clones is able to support so big file.' ) if args . no_cloudflare : if args . host == host_number and host_number is not None and not clone_list [ host_number ] [ 4 ] : print ( "This host uses Cloudflare, please choose different host." ) exit ( 1 ) else : for n , host in enumerate ( clone_list ) : if not host [ 4 ] : clone_list [ n ] = None clone_list = list ( filter ( None , clone_list ) ) if host_number is None or args . host != host_number : host_number = random . randrange ( 0 , len ( clone_list ) ) while True : try : if args . encrypt : returned_string = encrypt_files ( clone_list [ host_number ] , args . only_link , i ) else : returned_string = upload_files ( open ( i , 'rb' ) , clone_list [ host_number ] , args . only_link , i ) if args . only_link : print ( returned_string [ 0 ] ) else : print ( returned_string ) except IndexError : host_number = random . randrange ( 0 , len ( clone_list ) ) continue except IsADirectoryError : print ( 'limf does not support directory upload, if you want to upload ' 'every file in directory use limf {}/*.' . format ( i . replace ( '/' , '' ) ) ) if args . log : with open ( os . path . expanduser ( args . logfile ) , "a+" ) as logfile : if args . only_link : logfile . write ( returned_string [ 1 ] ) else : logfile . write ( returned_string ) logfile . write ( "\n" ) break else : print ( "limf: try 'limf -h' for more information" )
4683	def getAccounts ( self ) : pubkeys = self . getPublicKeys ( ) accounts = [ ] for pubkey in pubkeys : if pubkey [ : len ( self . prefix ) ] == self . prefix : accounts . extend ( self . getAccountsFromPublicKey ( pubkey ) ) return accounts
11673	def make_stacked ( self ) : "If unstacked, convert to stacked. If stacked, do nothing." if self . stacked : return self . _boundaries = bounds = np . r_ [ 0 , np . cumsum ( self . n_pts ) ] self . stacked_features = stacked = np . vstack ( self . features ) self . features = np . array ( [ stacked [ bounds [ i - 1 ] : bounds [ i ] ] for i in xrange ( 1 , len ( bounds ) ) ] , dtype = object ) self . stacked = True
12590	def get_reliabledictionary_list ( client , application_name , service_name ) : cluster = Cluster . from_sfclient ( client ) service = cluster . get_application ( application_name ) . get_service ( service_name ) for dictionary in service . get_dictionaries ( ) : print ( dictionary . name )
8051	def _darkest ( self ) : rgb , n = ( 1.0 , 1.0 , 1.0 ) , 3.0 for r , g , b in self : if r + g + b < n : rgb , n = ( r , g , b ) , r + g + b return rgb
12238	def rosenbrock ( theta ) : x , y = theta obj = ( 1 - x ) ** 2 + 100 * ( y - x ** 2 ) ** 2 grad = np . zeros ( 2 ) grad [ 0 ] = 2 * x - 400 * ( x * y - x ** 3 ) - 2 grad [ 1 ] = 200 * ( y - x ** 2 ) return obj , grad
1344	def _get_output ( self , a , image ) : sd = np . square ( self . _input_images - image ) mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) index = np . argmin ( mses ) if mses [ index ] > 0 : raise ValueError ( 'No precomputed output image for this image' ) return self . _output_images [ index ]
7049	def massradius ( age , planetdist , coremass , mass = 'massjupiter' , radius = 'radiusjupiter' ) : MR = { 0.3 : MASSESRADII_0_3GYR , 1.0 : MASSESRADII_1_0GYR , 4.5 : MASSESRADII_4_5GYR } if age not in MR : print ( 'given age not in Fortney 2007, returning...' ) return massradius = MR [ age ] if ( planetdist in massradius ) and ( coremass in massradius [ planetdist ] ) : print ( 'getting % Gyr M-R for planet dist %s AU, ' 'core mass %s Mearth...' % ( age , planetdist , coremass ) ) massradrelation = massradius [ planetdist ] [ coremass ] outdict = { 'mass' : array ( massradrelation [ mass ] ) , 'radius' : array ( massradrelation [ radius ] ) } return outdict
3950	def _read ( self , mux , gain , data_rate , mode ) : config = ADS1x15_CONFIG_OS_SINGLE config |= ( mux & 0x07 ) << ADS1x15_CONFIG_MUX_OFFSET if gain not in ADS1x15_CONFIG_GAIN : raise ValueError ( 'Gain must be one of: 2/3, 1, 2, 4, 8, 16' ) config |= ADS1x15_CONFIG_GAIN [ gain ] config |= mode if data_rate is None : data_rate = self . _data_rate_default ( ) config |= self . _data_rate_config ( data_rate ) config |= ADS1x15_CONFIG_COMP_QUE_DISABLE self . _device . writeList ( ADS1x15_POINTER_CONFIG , [ ( config >> 8 ) & 0xFF , config & 0xFF ] ) time . sleep ( 1.0 / data_rate + 0.0001 ) result = self . _device . readList ( ADS1x15_POINTER_CONVERSION , 2 ) return self . _conversion_value ( result [ 1 ] , result [ 0 ] )
159	def Grayscale ( alpha = 0 , from_colorspace = "RGB" , name = None , deterministic = False , random_state = None ) : if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )
5864	def remove_organization_course ( organization , course_key ) : _validate_organization_data ( organization ) _validate_course_key ( course_key ) return data . delete_organization_course ( course_key = course_key , organization = organization )
931	def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )
6242	def load_shader ( self , shader_type : str , path : str ) : if path : resolved_path = self . find_program ( path ) if not resolved_path : raise ValueError ( "Cannot find {} shader '{}'" . format ( shader_type , path ) ) print ( "Loading:" , path ) with open ( resolved_path , 'r' ) as fd : return fd . read ( )
3945	def _decode_field ( message , field , value ) : if field . type == FieldDescriptor . TYPE_MESSAGE : decode ( getattr ( message , field . name ) , value ) else : try : if field . type == FieldDescriptor . TYPE_BYTES : value = base64 . b64decode ( value ) setattr ( message , field . name , value ) except ( ValueError , TypeError ) as e : logger . warning ( 'Message %r ignoring field %s: %s' , message . __class__ . __name__ , field . name , e )
2773	def create ( self , * args , ** kwargs ) : rules_dict = [ rule . __dict__ for rule in self . forwarding_rules ] params = { 'name' : self . name , 'region' : self . region , 'forwarding_rules' : rules_dict , 'redirect_http_to_https' : self . redirect_http_to_https } if self . droplet_ids and self . tag : raise ValueError ( 'droplet_ids and tag are mutually exclusive args' ) elif self . tag : params [ 'tag' ] = self . tag else : params [ 'droplet_ids' ] = self . droplet_ids if self . algorithm : params [ 'algorithm' ] = self . algorithm if self . health_check : params [ 'health_check' ] = self . health_check . __dict__ if self . sticky_sessions : params [ 'sticky_sessions' ] = self . sticky_sessions . __dict__ data = self . get_data ( 'load_balancers/' , type = POST , params = params ) if data : self . id = data [ 'load_balancer' ] [ 'id' ] self . ip = data [ 'load_balancer' ] [ 'ip' ] self . algorithm = data [ 'load_balancer' ] [ 'algorithm' ] self . health_check = HealthCheck ( ** data [ 'load_balancer' ] [ 'health_check' ] ) self . sticky_sessions = StickySesions ( ** data [ 'load_balancer' ] [ 'sticky_sessions' ] ) self . droplet_ids = data [ 'load_balancer' ] [ 'droplet_ids' ] self . status = data [ 'load_balancer' ] [ 'status' ] self . created_at = data [ 'load_balancer' ] [ 'created_at' ] return self
13390	def format_pathname ( pathname , max_length ) : if max_length <= 3 : raise ValueError ( "max length must be larger than 3" ) if len ( pathname ) > max_length : pathname = "...{}" . format ( pathname [ - ( max_length - 3 ) : ] ) return pathname
1427	def create_parser ( subparsers ) : parser = subparsers . add_parser ( 'update' , help = 'Update a topology' , usage = "%(prog)s [options] cluster/[role]/[env] <topology-name> " + "[--component-parallelism <name:value>] " + "[--container-number value] " + "[--runtime-config [component:]<name:value>]" , add_help = True ) args . add_titles ( parser ) args . add_cluster_role_env ( parser ) args . add_topology ( parser ) args . add_config ( parser ) args . add_dry_run ( parser ) args . add_service_url ( parser ) args . add_verbose ( parser ) def parallelism_type ( value ) : pattern = re . compile ( r"^[\w\.-]+:[\d]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for component parallelism (<component_name:value>): %s" % value ) return value parser . add_argument ( '--component-parallelism' , action = 'append' , type = parallelism_type , required = False , help = 'Component name and the new parallelism value ' + 'colon-delimited: <component_name>:<parallelism>' ) def runtime_config_type ( value ) : pattern = re . compile ( r"^([\w\.-]+:){1,2}[\w\.-]+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for runtime config ([component:]<name:value>): %s" % value ) return value parser . add_argument ( '--runtime-config' , action = 'append' , type = runtime_config_type , required = False , help = 'Runtime configurations for topology and components ' + 'colon-delimited: [component:]<name>:<value>' ) def container_number_type ( value ) : pattern = re . compile ( r"^\d+$" ) if not pattern . match ( value ) : raise argparse . ArgumentTypeError ( "Invalid syntax for container number (value): %s" % value ) return value parser . add_argument ( '--container-number' , action = 'append' , type = container_number_type , required = False , help = 'Number of containers <value>' ) parser . set_defaults ( subcommand = 'update' ) return parser
9557	def _apply_record_predicates ( self , i , r , summarize = False , report_unexpected_exceptions = True , context = None ) : for predicate , code , message , modulus in self . _record_predicates : if i % modulus == 0 : rdict = self . _as_dict ( r ) try : valid = predicate ( rdict ) if not valid : p = { 'code' : code } if not summarize : p [ 'message' ] = message p [ 'row' ] = i + 1 p [ 'record' ] = r if context is not None : p [ 'context' ] = context yield p except Exception as e : if report_unexpected_exceptions : p = { 'code' : UNEXPECTED_EXCEPTION } if not summarize : p [ 'message' ] = MESSAGES [ UNEXPECTED_EXCEPTION ] % ( e . __class__ . __name__ , e ) p [ 'row' ] = i + 1 p [ 'record' ] = r p [ 'exception' ] = e p [ 'function' ] = '%s: %s' % ( predicate . __name__ , predicate . __doc__ ) if context is not None : p [ 'context' ] = context yield p
2928	def write_to_package_zip ( self , filename , data ) : self . manifest [ filename ] = md5hash ( data ) self . package_zip . writestr ( filename , data )
4268	def generate_thumbnail ( source , outname , box , fit = True , options = None , thumb_fit_centering = ( 0.5 , 0.5 ) ) : logger = logging . getLogger ( __name__ ) img = _read_image ( source ) original_format = img . format if fit : img = ImageOps . fit ( img , box , PILImage . ANTIALIAS , centering = thumb_fit_centering ) else : img . thumbnail ( box , PILImage . ANTIALIAS ) outformat = img . format or original_format or 'JPEG' logger . debug ( 'Save thumnail image: %s (%s)' , outname , outformat ) save_image ( img , outname , outformat , options = options , autoconvert = True )
3457	def find_bump ( target , tag ) : tmp = tag . split ( "." ) existing = [ intify ( basename ( f ) ) for f in glob ( join ( target , "[0-9]*.md" ) ) ] latest = max ( existing ) if int ( tmp [ 0 ] ) > latest [ 0 ] : return "major" elif int ( tmp [ 1 ] ) > latest [ 1 ] : return "minor" else : return "patch"
4978	def get_course_or_program_context ( self , enterprise_customer , course_id = None , program_uuid = None ) : context_data = { } if course_id : context_data . update ( { 'course_id' : course_id , 'course_specific' : True } ) if not self . preview_mode : try : catalog_api_client = CourseCatalogApiServiceClient ( enterprise_customer . site ) except ImproperlyConfigured : raise Http404 course_run_details = catalog_api_client . get_course_run ( course_id ) course_start_date = '' if course_run_details [ 'start' ] : course_start_date = parse ( course_run_details [ 'start' ] ) . strftime ( '%B %d, %Y' ) context_data . update ( { 'course_title' : course_run_details [ 'title' ] , 'course_start_date' : course_start_date , } ) else : context_data . update ( { 'course_title' : 'Demo Course' , 'course_start_date' : datetime . datetime . now ( ) . strftime ( '%B %d, %Y' ) , } ) else : context_data . update ( { 'program_uuid' : program_uuid , 'program_specific' : True , } ) return context_data
610	def _generateFileFromTemplates ( templateFileNames , outputFilePath , replacementDict ) : installPath = os . path . dirname ( __file__ ) outputFile = open ( outputFilePath , "w" ) outputLines = [ ] inputLines = [ ] firstFile = True for templateFileName in templateFileNames : if not firstFile : inputLines . extend ( [ os . linesep ] * 2 ) firstFile = False inputFilePath = os . path . join ( installPath , templateFileName ) inputFile = open ( inputFilePath ) inputLines . extend ( inputFile . readlines ( ) ) inputFile . close ( ) print "Writing " , len ( inputLines ) , "lines..." for line in inputLines : tempLine = line for k , v in replacementDict . iteritems ( ) : if v is None : v = "None" tempLine = re . sub ( k , v , tempLine ) outputFile . write ( tempLine ) outputFile . close ( )
13341	def expand_dims ( a , axis ) : if hasattr ( a , 'expand_dims' ) and hasattr ( type ( a ) , '__array_interface__' ) : return a . expand_dims ( axis ) else : return np . expand_dims ( a , axis )
1338	def crossentropy ( label , logits ) : assert logits . ndim == 1 logits = logits - np . max ( logits ) e = np . exp ( logits ) s = np . sum ( e ) ce = np . log ( s ) - logits [ label ] return ce
7508	def _finalize_stats ( self , ipyclient ) : print ( FINALTREES . format ( opr ( self . trees . tree ) ) ) if self . params . nboots : self . _compute_tree_stats ( ipyclient ) print ( BOOTTREES . format ( opr ( self . trees . cons ) , opr ( self . trees . boots ) ) ) if len ( self . samples ) < 20 : if self . params . nboots : wctre = ete3 . Tree ( self . trees . cons , format = 0 ) wctre . ladderize ( ) print ( wctre . get_ascii ( show_internal = True , attributes = [ "dist" , "name" ] ) ) print ( "" ) else : qtre = ete3 . Tree ( self . trees . tree , format = 0 ) qtre . ladderize ( ) print ( qtre . get_ascii ( ) ) print ( "" ) docslink = "https://toytree.readthedocs.io/" citelink = "https://ipyrad.readthedocs.io/tetrad.html" print ( LINKS . format ( docslink , citelink ) )
2618	def write_state_file ( self ) : fh = open ( 'awsproviderstate.json' , 'w' ) state = { } state [ 'vpcID' ] = self . vpc_id state [ 'sgID' ] = self . sg_id state [ 'snIDs' ] = self . sn_ids state [ 'instances' ] = self . instances state [ "instanceState" ] = self . instance_states fh . write ( json . dumps ( state , indent = 4 ) )
10405	def bond_canonical_statistics ( microcanonical_statistics , convolution_factors , ** kwargs ) : spanning_cluster = ( 'has_spanning_cluster' in microcanonical_statistics . dtype . names ) ret = np . empty ( 1 , dtype = canonical_statistics_dtype ( spanning_cluster ) ) if spanning_cluster : ret [ 'percolation_probability' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'has_spanning_cluster' ] ) ret [ 'max_cluster_size' ] = np . sum ( convolution_factors * microcanonical_statistics [ 'max_cluster_size' ] ) ret [ 'moments' ] = np . sum ( convolution_factors [ : , np . newaxis ] * microcanonical_statistics [ 'moments' ] , axis = 0 , ) return ret
4917	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer_catalog = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = True if course_run_ids : contains_content_items = enterprise_customer_catalog . contains_courses ( course_run_ids ) if program_uuids : contains_content_items = ( contains_content_items and enterprise_customer_catalog . contains_programs ( program_uuids ) ) return Response ( { 'contains_content_items' : contains_content_items } )
9483	def to_bytes_36 ( self , previous : bytes ) : bc = b"" it_bc = util . generate_bytecode_from_obb ( self . iterator , previous ) bc += it_bc bc += util . ensure_instruction ( tokens . GET_ITER )
13138	def http_get_provider ( provider , request_url , params , token_secret , token_cookie = None ) : if not validate_provider ( provider ) : raise InvalidUsage ( 'Provider not supported' ) klass = getattr ( socialauth . providers , provider . capitalize ( ) ) provider = klass ( request_url , params , token_secret , token_cookie ) if provider . status == 302 : ret = dict ( status = 302 , redirect = provider . redirect ) tc = getattr ( provider , 'set_token_cookie' , None ) if tc is not None : ret [ 'set_token_cookie' ] = tc return ret if provider . status == 200 and provider . user_id is not None : ret = dict ( status = 200 , provider_user_id = provider . user_id ) if provider . user_name is not None : ret [ 'provider_user_name' ] = provider . user_name return ret raise InvalidUsage ( 'Invalid request' )
11821	def create ( self , name , value ) : if value is None : raise ValueError ( 'Setting value cannot be `None`.' ) model = Setting . get_model_for_value ( value ) obj = super ( SettingQuerySet , model . objects . all ( ) ) . create ( name = name , value = value ) return obj
4450	def info ( self ) : res = self . redis . execute_command ( 'FT.INFO' , self . index_name ) it = six . moves . map ( to_string , res ) return dict ( six . moves . zip ( it , it ) )
7593	def get_constants ( self , ** params : keys ) : url = self . api . CONSTANTS return self . _get_model ( url , ** params )
8582	def get_attached_volumes ( self , datacenter_id , server_id , depth = 1 ) : response = self . _perform_request ( '/datacenters/%s/servers/%s/volumes?depth=%s' % ( datacenter_id , server_id , str ( depth ) ) ) return response
5023	def get_integrated_channels ( self , options ) : channel_classes = self . get_channel_classes ( options . get ( 'channel' ) ) filter_kwargs = { 'active' : True , 'enterprise_customer__active' : True , } enterprise_customer = self . get_enterprise_customer ( options . get ( 'enterprise_customer' ) ) if enterprise_customer : filter_kwargs [ 'enterprise_customer' ] = enterprise_customer for channel_class in channel_classes : for integrated_channel in channel_class . objects . filter ( ** filter_kwargs ) : yield integrated_channel
704	def _okToExit ( self ) : print >> sys . stderr , "reporter:status:In hypersearchV2: _okToExit" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( "Ready to end hyperseach, but not all models have " "matured yet. Sleeping a bit to wait for all models " "to mature." ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( "Stopping model %d because the search has ended" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True
4442	def add_suggestions ( self , * suggestions , ** kwargs ) : pipe = self . redis . pipeline ( ) for sug in suggestions : args = [ AutoCompleter . SUGADD_COMMAND , self . key , sug . string , sug . score ] if kwargs . get ( 'increment' ) : args . append ( AutoCompleter . INCR ) if sug . payload : args . append ( 'PAYLOAD' ) args . append ( sug . payload ) pipe . execute_command ( * args ) return pipe . execute ( ) [ - 1 ]
13774	def format ( self , record ) : record_fields = record . __dict__ . copy ( ) self . _set_exc_info ( record_fields ) event_name = 'default' if record_fields . get ( 'event_name' ) : event_name = record_fields . pop ( 'event_name' ) log_level = 'INFO' if record_fields . get ( 'log_level' ) : log_level = record_fields . pop ( 'log_level' ) [ record_fields . pop ( k ) for k in record_fields . keys ( ) if k not in self . fields ] defaults = self . defaults . copy ( ) fields = self . fields . copy ( ) fields . update ( record_fields ) filtered_fields = { } for k , v in fields . iteritems ( ) : if v is not None : filtered_fields [ k ] = v defaults . update ( { 'event_timestamp' : self . _get_now ( ) , 'event_name' : event_name , 'log_level' : log_level , 'fields' : filtered_fields } ) return json . dumps ( defaults , default = self . json_default )
4133	def codestr2rst ( codestr , lang = 'python' ) : code_directive = "\n.. code-block:: {0}\n\n" . format ( lang ) indented_block = indent ( codestr , ' ' * 4 ) return code_directive + indented_block
1171	def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return ", " . join ( opts )
1148	def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : memo [ id ( memo ) ] = [ x ]
12035	def sweepYfiltered ( self ) : assert self . kernel is not None return swhlab . common . convolve ( self . sweepY , self . kernel )
9235	def parse ( data ) : sections = re . compile ( "^## .+$" , re . MULTILINE ) . split ( data ) headings = re . findall ( "^## .+?$" , data , re . MULTILINE ) sections . pop ( 0 ) parsed = [ ] def func ( h , s ) : p = parse_heading ( h ) p [ "content" ] = s parsed . append ( p ) list ( map ( func , headings , sections ) ) return parsed
5958	def break_array ( a , threshold = numpy . pi , other = None ) : assert len ( a . shape ) == 1 , "Only 1D arrays supported" if other is not None and a . shape != other . shape : raise ValueError ( "arrays must be of identical shape" ) breaks = numpy . where ( numpy . abs ( numpy . diff ( a ) ) >= threshold ) [ 0 ] breaks += 1 m = len ( breaks ) b = numpy . empty ( ( len ( a ) + m ) ) b_breaks = breaks + numpy . arange ( m ) mask = numpy . zeros_like ( b , dtype = numpy . bool ) mask [ b_breaks ] = True b [ ~ mask ] = a b [ mask ] = numpy . NAN if other is not None : c = numpy . empty_like ( b ) c [ ~ mask ] = other c [ mask ] = numpy . NAN ma_c = numpy . ma . array ( c , mask = mask ) else : ma_c = None return numpy . ma . array ( b , mask = mask ) , ma_c
4763	def soft_fail ( msg = '' ) : global _soft_ctx if _soft_ctx : global _soft_err _soft_err . append ( 'Fail: %s!' % msg if msg else 'Fail!' ) return fail ( msg )
8500	def _map_arg ( arg ) : if isinstance ( arg , _ast . Str ) : return repr ( arg . s ) elif isinstance ( arg , _ast . Num ) : return arg . n elif isinstance ( arg , _ast . Name ) : name = arg . id if name == 'True' : return True elif name == 'False' : return False elif name == 'None' : return None return name else : return Unparseable ( )
12525	def condor_call ( cmd , shell = True ) : log . info ( cmd ) ret = condor_submit ( cmd ) if ret != 0 : subprocess . call ( cmd , shell = shell )
8512	def _create_kernel ( self ) : kernels = self . kernel_params if not isinstance ( kernels , list ) : raise RuntimeError ( 'Must provide enumeration of kernels' ) for kernel in kernels : if sorted ( list ( kernel . keys ( ) ) ) != [ 'name' , 'options' , 'params' ] : raise RuntimeError ( 'strategy/params/kernels must contain keys: "name", "options", "params"' ) kernels = [ ] for kern in self . kernel_params : params = kern [ 'params' ] options = kern [ 'options' ] name = kern [ 'name' ] kernel_ep = load_entry_point ( name , 'strategy/params/kernels' ) if issubclass ( kernel_ep , KERNEL_BASE_CLASS ) : if options [ 'independent' ] : kernel = np . sum ( [ kernel_ep ( 1 , active_dims = [ i ] , ** params ) for i in range ( self . n_dims ) ] ) else : kernel = kernel_ep ( self . n_dims , ** params ) if not isinstance ( kernel , KERNEL_BASE_CLASS ) : raise RuntimeError ( 'strategy/params/kernel must load a' 'GPy derived Kernel' ) kernels . append ( kernel ) self . kernel = np . sum ( kernels )
10993	def _check_for_inception ( self , root_dict ) : for key in root_dict : if isinstance ( root_dict [ key ] , dict ) : root_dict [ key ] = ResponseObject ( root_dict [ key ] ) return root_dict
12859	def to_date ( self ) : y , m , d = self . to_ymd ( ) return date ( y , m , d )
3994	def _load_ssh_auth_pre_yosemite ( ) : for process in psutil . process_iter ( ) : if process . name ( ) == 'ssh-agent' : ssh_auth_sock = subprocess . check_output ( [ 'launchctl' , 'bsexec' , str ( process . pid ) , 'launchctl' , 'getenv' , 'SSH_AUTH_SOCK' ] ) . rstrip ( ) if ssh_auth_sock : _set_ssh_auth_sock ( ssh_auth_sock ) break else : daemon_warnings . warn ( 'ssh' , 'No running ssh-agent found linked to SSH_AUTH_SOCK' )
5865	def course_key_is_valid ( course_key ) : if course_key is None : return False try : CourseKey . from_string ( text_type ( course_key ) ) except ( InvalidKeyError , UnicodeDecodeError ) : return False return True
12934	def _parse_frequencies ( self ) : frequencies = OrderedDict ( [ ( 'EXAC' , 'Unknown' ) , ( 'ESP' , 'Unknown' ) , ( 'TGP' , 'Unknown' ) ] ) pref_freq = 'Unknown' for source in frequencies . keys ( ) : freq_key = 'AF_' + source if freq_key in self . info : frequencies [ source ] = self . info [ freq_key ] if pref_freq == 'Unknown' : pref_freq = frequencies [ source ] return pref_freq , frequencies
3043	def _expires_in ( self ) : if self . token_expiry : now = _UTCNOW ( ) if self . token_expiry > now : time_delta = self . token_expiry - now return time_delta . days * 86400 + time_delta . seconds else : return 0
6863	def tic_single_object_crossmatch ( ra , dec , radius ) : for val in ra , dec , radius : if not isinstance ( val , float ) : raise AssertionError ( 'plz input ra,dec,radius in decimal degrees' ) crossmatchInput = { "fields" : [ { "name" : "ra" , "type" : "float" } , { "name" : "dec" , "type" : "float" } ] , "data" : [ { "ra" : ra , "dec" : dec } ] } request = { "service" : "Mast.Tic.Crossmatch" , "data" : crossmatchInput , "params" : { "raColumn" : "ra" , "decColumn" : "dec" , "radius" : radius } , "format" : "json" , 'removecache' : True } headers , out_string = _mast_query ( request ) out_data = json . loads ( out_string ) return out_data
13681	def get_json ( self , prettyprint = False , translate = True ) : j = [ ] if translate : d = self . get_translated_data ( ) else : d = self . data for k in d : j . append ( d [ k ] ) if prettyprint : j = json . dumps ( j , indent = 2 , separators = ( ',' , ': ' ) ) else : j = json . dumps ( j ) return j
4912	def contains_content_items ( self , request , pk , course_run_ids , program_uuids ) : enterprise_customer = self . get_object ( ) course_run_ids = [ unquote ( quote_plus ( course_run_id ) ) for course_run_id in course_run_ids ] contains_content_items = False for catalog in enterprise_customer . enterprise_customer_catalogs . all ( ) : contains_course_runs = not course_run_ids or catalog . contains_courses ( course_run_ids ) contains_program_uuids = not program_uuids or catalog . contains_programs ( program_uuids ) if contains_course_runs and contains_program_uuids : contains_content_items = True break return Response ( { 'contains_content_items' : contains_content_items } )
13635	def _splitHeaders ( headers ) : return [ cgi . parse_header ( value ) for value in chain . from_iterable ( s . split ( ',' ) for s in headers if s ) ]
462	def exit_tensorflow ( sess = None , port = 6006 ) : text = "[TL] Close tensorboard and nvidia-process if available" text2 = "[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on " if sess is not None : sess . close ( ) if _platform == "linux" or _platform == "linux2" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) os . system ( "nvidia-smi | grep python |awk '{print $3}'|xargs kill" ) _exit ( ) elif _platform == "darwin" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( "lsof -i tcp:" + str ( port ) + " | grep -v PID | awk '{print $2}' | xargs kill" , shell = True ) elif _platform == "win32" : raise NotImplementedError ( "this function is not supported on the Windows platform" ) else : tl . logging . info ( text2 + _platform )
6493	def get_mappings ( cls , index_name , doc_type ) : return cache . get ( cls . get_cache_item_name ( index_name , doc_type ) , { } )
9186	def get_moderation ( request ) : with db_connect ( ) as db_conn : with db_conn . cursor ( ) as cursor : cursor . execute ( ) moderations = [ x [ 0 ] for x in cursor . fetchall ( ) ] return moderations
13338	def redirect_resolver ( resolver , path ) : if not os . path . exists ( path ) : raise ResolveError if os . path . isfile ( path ) : path = os . path . dirname ( path ) for root , _ , _ in walk_up ( path ) : if is_redirecting ( root ) : env_paths = redirect_to_env_paths ( unipath ( root , '.cpenv' ) ) r = Resolver ( * env_paths ) return r . resolve ( ) raise ResolveError
2108	def login ( username , password , scope , client_id , client_secret , verbose ) : if not supports_oauth ( ) : raise exc . TowerCLIError ( 'This version of Tower does not support OAuth2.0. Set credentials using tower-cli config.' ) req = collections . namedtuple ( 'req' , 'headers' ) ( { } ) if client_id and client_secret : HTTPBasicAuth ( client_id , client_secret ) ( req ) req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "scope" : scope } , headers = req . headers ) elif client_id : req . headers [ 'Content-Type' ] = 'application/x-www-form-urlencoded' r = client . post ( '/o/token/' , data = { "grant_type" : "password" , "username" : username , "password" : password , "client_id" : client_id , "scope" : scope } , headers = req . headers ) else : HTTPBasicAuth ( username , password ) ( req ) r = client . post ( '/users/{}/personal_tokens/' . format ( username ) , data = { "description" : "Tower CLI" , "application" : None , "scope" : scope } , headers = req . headers ) if r . ok : result = r . json ( ) result . pop ( 'summary_fields' , None ) result . pop ( 'related' , None ) if client_id : token = result . pop ( 'access_token' , None ) else : token = result . pop ( 'token' , None ) if settings . verbose : result [ 'token' ] = token secho ( json . dumps ( result , indent = 1 ) , fg = 'blue' , bold = True ) config . main ( [ 'oauth_token' , token , '--scope=user' ] )
1360	def get_argument_starttime ( self ) : try : starttime = self . get_argument ( constants . PARAM_STARTTIME ) return starttime except tornado . web . MissingArgumentError as e : raise Exception ( e . log_message )
13816	def Parse ( text , message ) : if not isinstance ( text , six . text_type ) : text = text . decode ( 'utf-8' ) try : if sys . version_info < ( 2 , 7 ) : js = json . loads ( text ) else : js = json . loads ( text , object_pairs_hook = _DuplicateChecker ) except ValueError as e : raise ParseError ( 'Failed to load JSON: {0}.' . format ( str ( e ) ) ) _ConvertMessage ( js , message ) return message
1702	def outer_right_join ( self , join_streamlet , window_config , join_function ) : from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , join_function , self , join_streamlet ) self . _add_child ( join_streamlet_result ) join_streamlet . _add_child ( join_streamlet_result ) return join_streamlet_result
3128	def delete ( self , template_id ) : self . template_id = template_id return self . _mc_client . _delete ( url = self . _build_path ( template_id ) )
7498	def shuffle_cols ( seqarr , newarr , cols ) : for idx in xrange ( cols . shape [ 0 ] ) : newarr [ : , idx ] = seqarr [ : , cols [ idx ] ] return newarr
9912	def send ( self ) : context = { "verification_url" : app_settings . EMAIL_VERIFICATION_URL . format ( key = self . key ) } email_utils . send_email ( context = context , from_email = settings . DEFAULT_FROM_EMAIL , recipient_list = [ self . email . email ] , subject = _ ( "Please Verify Your Email Address" ) , template_name = "rest_email_auth/emails/verify-email" , ) logger . info ( "Sent confirmation email to %s for user #%d" , self . email . email , self . email . user . id , )
7939	def _resolve_srv ( self ) : resolver = self . settings [ "dns_resolver" ] self . _set_state ( "resolving-srv" ) self . event ( ResolvingSRVEvent ( self . _dst_name , self . _dst_service ) ) resolver . resolve_srv ( self . _dst_name , self . _dst_service , "tcp" , callback = self . _got_srv )
13540	def chisq_red ( self ) : if self . _chisq_red is None : self . _chisq_red = chisquare ( self . y_unweighted . transpose ( ) , _np . dot ( self . X_unweighted , self . beta ) , self . y_error , ddof = 3 , verbose = False ) return self . _chisq_red
13269	def deparagraph ( element , doc ) : if isinstance ( element , Para ) : if element . next is not None : return element elif element . prev is not None : return element return Plain ( * element . content )
4238	def get_traffic_meter ( self ) : _LOGGER . info ( "Get traffic meter" ) def parse_text ( text ) : def tofloats ( lst ) : return ( float ( t ) for t in lst ) try : if "/" in text : return tuple ( tofloats ( text . split ( '/' ) ) ) elif ":" in text : hour , mins = tofloats ( text . split ( ':' ) ) return timedelta ( hours = hour , minutes = mins ) else : return float ( text ) except ValueError : return None success , response = self . _make_request ( SERVICE_DEVICE_CONFIG , "GetTrafficMeterStatistics" ) if not success : return None success , node = _find_node ( response . text , ".//GetTrafficMeterStatisticsResponse" ) if not success : return None return { t . tag : parse_text ( t . text ) for t in node }
9144	def drop ( connection , skip ) : for idx , name , manager in _iterate_managers ( connection , skip ) : click . secho ( f'dropping {name}' , fg = 'cyan' , bold = True ) manager . drop_all ( )
6518	def is_excluded ( self , path ) : relpath = path . relative_to ( self . base_path ) . as_posix ( ) return matches_masks ( relpath , self . excludes )
2230	def register ( self , hash_types ) : if not isinstance ( hash_types , ( list , tuple ) ) : hash_types = [ hash_types ] def _decor_closure ( hash_func ) : for hash_type in hash_types : key = ( hash_type . __module__ , hash_type . __name__ ) self . keyed_extensions [ key ] = ( hash_type , hash_func ) return hash_func return _decor_closure
2253	def unique ( items , key = None ) : seen = set ( ) if key is None : for item in items : if item not in seen : seen . add ( item ) yield item else : for item in items : norm = key ( item ) if norm not in seen : seen . add ( norm ) yield item
499	def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )
373	def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x
7354	def predict_peptides ( self , peptides ) : from mhcflurry . encodable_sequences import EncodableSequences binding_predictions = [ ] encodable_sequences = EncodableSequences . create ( peptides ) for allele in self . alleles : predictions_df = self . predictor . predict_to_dataframe ( encodable_sequences , allele = allele ) for ( _ , row ) in predictions_df . iterrows ( ) : binding_prediction = BindingPrediction ( allele = allele , peptide = row . peptide , affinity = row . prediction , percentile_rank = ( row . prediction_percentile if 'prediction_percentile' in row else nan ) , prediction_method_name = "mhcflurry" ) binding_predictions . append ( binding_prediction ) return BindingPredictionCollection ( binding_predictions )
6427	def dist ( self , src , tar ) : if src == tar : return 0.0 src = src . encode ( 'utf-8' ) tar = tar . encode ( 'utf-8' ) src_comp = bz2 . compress ( src , self . _level ) [ 10 : ] tar_comp = bz2 . compress ( tar , self . _level ) [ 10 : ] concat_comp = bz2 . compress ( src + tar , self . _level ) [ 10 : ] concat_comp2 = bz2 . compress ( tar + src , self . _level ) [ 10 : ] return ( min ( len ( concat_comp ) , len ( concat_comp2 ) ) - min ( len ( src_comp ) , len ( tar_comp ) ) ) / max ( len ( src_comp ) , len ( tar_comp ) )
826	def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names
12054	def inspectABF ( abf = exampleABF , saveToo = False , justPlot = False ) : pylab . close ( 'all' ) print ( " ~~ inspectABF()" ) if type ( abf ) is str : abf = swhlab . ABF ( abf ) swhlab . plot . new ( abf , forceNewFigure = True ) if abf . sweepInterval * abf . sweeps < 60 * 5 : pylab . subplot ( 211 ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . sweep ( abf , 'all' ) pylab . subplot ( 212 ) swhlab . plot . sweep ( abf , 'all' , continuous = True ) swhlab . plot . comments ( abf ) else : print ( " -- plotting as long recording" ) swhlab . plot . sweep ( abf , 'all' , continuous = True , minutes = True ) swhlab . plot . comments ( abf , minutes = True ) pylab . title ( "%s [%s]" % ( abf . ID , abf . protoComment ) ) swhlab . plot . annotate ( abf ) if justPlot : return if saveToo : path = os . path . split ( abf . fname ) [ 0 ] basename = os . path . basename ( abf . fname ) pylab . savefig ( os . path . join ( path , "_" + basename . replace ( ".abf" , ".png" ) ) ) pylab . show ( ) return
9511	def replace_bases ( self , old , new ) : self . seq = self . seq . replace ( old , new )
9449	def schedule_hangup ( self , call_params ) : path = '/' + self . api_version + '/ScheduleHangup/' method = 'POST' return self . request ( path , method , call_params )
881	def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )
9015	def _row ( self , values ) : row_id = self . _to_id ( values [ ID ] ) row = self . _spec . new_row ( row_id , values , self ) if SAME_AS in values : self . _delay_inheritance ( row , self . _to_id ( values [ SAME_AS ] ) ) self . _delay_instructions ( row ) self . _id_cache [ row_id ] = row return row
6877	def _gunzip_sqlitecurve ( sqlitecurve ) : cmd = 'gunzip -k %s' % sqlitecurve try : subprocess . check_output ( cmd , shell = True ) return sqlitecurve . replace ( '.gz' , '' ) except subprocess . CalledProcessError : return None
8312	def draw_list ( markup , x , y , w , padding = 5 , callback = None ) : try : from web import _ctx except : pass i = 1 for chunk in markup . split ( "\n" ) : if callback != None : callback ( chunk , i ) m = re . search ( "^([0-9]{1,3}\. )" , chunk . lstrip ( ) ) if m : indent = re . search ( "[0-9]" , chunk ) . start ( ) * padding * 2 bullet = m . group ( 1 ) dx = textwidth ( "000." ) chunk = chunk . lstrip ( m . group ( 1 ) + "\t" ) if chunk . lstrip ( ) . startswith ( "*" ) : indent = chunk . find ( "*" ) * padding * 2 bullet = u"•" dx = textwidth ( "*" ) chunk = chunk . lstrip ( "* \t" ) _ctx . text ( bullet , x + indent , y ) dx += padding + indent _ctx . text ( chunk , x + dx , y , width = w - dx ) y += _ctx . textheight ( chunk , width = w - dx ) y += _ctx . textheight ( " " ) * 0.25 i += 1
6562	def iter_complete_graphs ( start , stop , factory = None ) : _ , nodes = start nodes = list ( nodes ) if factory is None : factory = count ( ) while len ( nodes ) < stop : G = nx . complete_graph ( nodes ) yield G v = next ( factory ) while v in G : v = next ( factory ) nodes . append ( v )
5770	def rsa_pkcs1v15_verify ( certificate_or_public_key , signature , data , hash_algorithm ) : if certificate_or_public_key . algorithm != 'rsa' : raise ValueError ( 'The key specified is not an RSA public key' ) return _verify ( certificate_or_public_key , signature , data , hash_algorithm )
10165	def get_arrays ( self , lines , personalities = [ ] ) : ret = { } i = 0 while i < len ( lines ) : try : md_device = self . get_md_device_name ( lines [ i ] ) except IndexError : pass else : if md_device is not None : ret [ md_device ] = self . get_md_device ( lines [ i ] , personalities ) i += 1 ret [ md_device ] . update ( self . get_md_status ( lines [ i ] ) ) i += 1 return ret
5575	def available_input_formats ( ) : input_formats = [ ] for v in pkg_resources . iter_entry_points ( DRIVERS_ENTRY_POINT ) : logger . debug ( "driver found: %s" , v ) driver_ = v . load ( ) if hasattr ( driver_ , "METADATA" ) and ( driver_ . METADATA [ "mode" ] in [ "r" , "rw" ] ) : input_formats . append ( driver_ . METADATA [ "driver_name" ] ) return input_formats
2423	def set_doc_version ( self , doc , value ) : if not self . doc_version_set : self . doc_version_set = True m = self . VERS_STR_REGEX . match ( value ) if m is None : raise SPDXValueError ( 'Document::Version' ) else : doc . version = version . Version ( major = int ( m . group ( 1 ) ) , minor = int ( m . group ( 2 ) ) ) return True else : raise CardinalityError ( 'Document::Version' )
3487	def _check ( value , message ) : if value is None : LOGGER . error ( 'Error: LibSBML returned a null value trying ' 'to <' + message + '>.' ) elif type ( value ) is int : if value == libsbml . LIBSBML_OPERATION_SUCCESS : return else : LOGGER . error ( 'Error encountered trying to <' + message + '>.' ) LOGGER . error ( 'LibSBML error code {}: {}' . format ( str ( value ) , libsbml . OperationReturnValue_toString ( value ) . strip ( ) ) ) else : return
3539	def hubspot ( parser , token ) : bits = token . split_contents ( ) if len ( bits ) > 1 : raise TemplateSyntaxError ( "'%s' takes no arguments" % bits [ 0 ] ) return HubSpotNode ( )
10214	def summarize_subgraph_node_overlap ( graph : BELGraph , node_predicates = None , annotation : str = 'Subgraph' ) : r1 = group_nodes_by_annotation_filtered ( graph , node_predicates = node_predicates , annotation = annotation ) return calculate_tanimoto_set_distances ( r1 )
12074	def _update_state ( self , vals ) : self . _steps_complete += 1 if self . _steps_complete == self . max_steps : self . _termination_info = ( False , self . _best_val , self . _arg ) return StopIteration arg_inc , arg_dec = vals best_val = min ( arg_inc , arg_dec , self . _best_val ) if best_val == self . _best_val : self . _termination_info = ( True , best_val , self . _arg ) return StopIteration self . _arg += self . stepsize if ( arg_dec > arg_inc ) else - self . stepsize self . _best_val = best_val return [ { self . key : self . _arg + self . stepsize } , { self . key : self . _arg - self . stepsize } ]
9450	def cancel_scheduled_hangup ( self , call_params ) : path = '/' + self . api_version + '/CancelScheduledHangup/' method = 'POST' return self . request ( path , method , call_params )
13754	def read_from_file ( file_path , encoding = "utf-8" ) : with codecs . open ( file_path , "r" , encoding ) as f : return f . read ( )
9545	def add_value_predicate ( self , field_name , value_predicate , code = VALUE_PREDICATE_FALSE , message = MESSAGES [ VALUE_PREDICATE_FALSE ] , modulus = 1 ) : assert field_name in self . _field_names , 'unexpected field name: %s' % field_name assert callable ( value_predicate ) , 'value predicate must be a callable function' t = field_name , value_predicate , code , message , modulus self . _value_predicates . append ( t )
1979	def wait ( self , readfds , writefds , timeout ) : logger . info ( "WAIT:" ) logger . info ( "\tProcess %d is going to wait for [ %r %r %r ]" , self . _current , readfds , writefds , timeout ) logger . info ( "\tProcess: %r" , self . procs ) logger . info ( "\tRunning: %r" , self . running ) logger . info ( "\tRWait: %r" , self . rwait ) logger . info ( "\tTWait: %r" , self . twait ) logger . info ( "\tTimers: %r" , self . timers ) for fd in readfds : self . rwait [ fd ] . add ( self . _current ) for fd in writefds : self . twait [ fd ] . add ( self . _current ) if timeout is not None : self . timers [ self . _current ] = self . clocks + timeout else : self . timers [ self . _current ] = None procid = self . _current next_index = ( self . running . index ( procid ) + 1 ) % len ( self . running ) self . _current = self . running [ next_index ] logger . info ( "\tTransfer control from process %d to %d" , procid , self . _current ) logger . info ( "\tREMOVING %r from %r. Current: %r" , procid , self . running , self . _current ) self . running . remove ( procid ) if self . _current not in self . running : logger . info ( "\tCurrent not running. Checking for timers..." ) self . _current = None if all ( [ x is None for x in self . timers ] ) : raise Deadlock ( ) self . check_timers ( )
4714	def trun_to_file ( trun , fpath = None ) : if fpath is None : fpath = yml_fpath ( trun [ "conf" ] [ "OUTPUT" ] ) with open ( fpath , 'w' ) as yml_file : data = yaml . dump ( trun , explicit_start = True , default_flow_style = False ) yml_file . write ( data )
11539	def set_pin_type ( self , pin , ptype ) : if type ( pin ) is list : for p in pin : self . set_pin_type ( p , ptype ) return pin_id = self . _pin_mapping . get ( pin , None ) if type ( ptype ) is not ahio . PortType : raise KeyError ( 'ptype must be of type ahio.PortType' ) elif pin_id : self . _set_pin_type ( pin_id , ptype ) else : raise KeyError ( 'Requested pin is not mapped: %s' % pin )
8077	def ellipsemode ( self , mode = None ) : if mode in ( self . CORNER , self . CENTER , self . CORNERS ) : self . ellipsemode = mode return self . ellipsemode elif mode is None : return self . ellipsemode else : raise ShoebotError ( _ ( "ellipsemode: invalid input" ) )
9257	def exclude_issues_by_labels ( self , issues ) : if not self . options . exclude_labels : return copy . deepcopy ( issues ) remove_issues = set ( ) exclude_labels = self . options . exclude_labels include_issues = [ ] for issue in issues : for label in issue [ "labels" ] : if label [ "name" ] in exclude_labels : remove_issues . add ( issue [ "number" ] ) break for issue in issues : if issue [ "number" ] not in remove_issues : include_issues . append ( issue ) return include_issues
4812	def tokenize ( text , custom_dict = None ) : global TOKENIZER if not TOKENIZER : TOKENIZER = DeepcutTokenizer ( ) return TOKENIZER . tokenize ( text , custom_dict = custom_dict )
8324	def isString ( s ) : try : return isinstance ( s , unicode ) or isinstance ( s , basestring ) except NameError : return isinstance ( s , str )
7559	def set_mkl_thread_limit ( cores ) : if "linux" in sys . platform : mkl_rt = ctypes . CDLL ( 'libmkl_rt.so' ) else : mkl_rt = ctypes . CDLL ( 'libmkl_rt.dylib' ) oldlimit = mkl_rt . mkl_get_max_threads ( ) mkl_rt . mkl_set_num_threads ( ctypes . byref ( ctypes . c_int ( cores ) ) ) return oldlimit
7948	def send_stream_head ( self , stanza_namespace , stream_from , stream_to , stream_id = None , version = u'1.0' , language = None ) : with self . lock : self . _serializer = XMPPSerializer ( stanza_namespace , self . settings [ "extra_ns_prefixes" ] ) head = self . _serializer . emit_head ( stream_from , stream_to , stream_id , version , language ) self . _write ( head . encode ( "utf-8" ) )
562	def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )
10648	def remove_component ( self , name ) : component_to_remove = None for c in self . components : if c . name == name : component_to_remove = c if component_to_remove is not None : self . components . remove ( component_to_remove )
1339	def batch_crossentropy ( label , logits ) : assert logits . ndim == 2 logits = logits - np . max ( logits , axis = 1 , keepdims = True ) e = np . exp ( logits ) s = np . sum ( e , axis = 1 ) ces = np . log ( s ) - logits [ : , label ] return ces
8456	def up_to_date ( version = None ) : temple . check . in_git_repo ( ) temple . check . is_temple_project ( ) temple_config = temple . utils . read_temple_config ( ) old_template_version = temple_config [ '_version' ] new_template_version = version or _get_latest_template_version ( temple_config [ '_template' ] ) return new_template_version == old_template_version
9271	def filter_since_tag ( self , all_tags ) : tag = self . detect_since_tag ( ) if not tag or tag == REPO_CREATED_TAG_NAME : return copy . deepcopy ( all_tags ) filtered_tags = [ ] tag_names = [ t [ "name" ] for t in all_tags ] try : idx = tag_names . index ( tag ) except ValueError : self . warn_if_tag_not_found ( tag , "since-tag" ) return copy . deepcopy ( all_tags ) since_tag = all_tags [ idx ] since_date = self . get_time_of_tag ( since_tag ) for t in all_tags : tag_date = self . get_time_of_tag ( t ) if since_date <= tag_date : filtered_tags . append ( t ) return filtered_tags
7570	def fastq_touchup_for_vsearch_merge ( read , outfile , reverse = False ) : counts = 0 with open ( outfile , 'w' ) as out : if read . endswith ( ".gz" ) : fr1 = gzip . open ( read , 'rb' ) else : fr1 = open ( read , 'rb' ) quarts = itertools . izip ( * [ iter ( fr1 ) ] * 4 ) writing = [ ] while 1 : try : lines = quarts . next ( ) except StopIteration : break if reverse : seq = lines [ 1 ] . strip ( ) [ : : - 1 ] else : seq = lines [ 1 ] . strip ( ) writing . append ( "" . join ( [ lines [ 0 ] , seq + "\n" , lines [ 2 ] , "B" * len ( seq ) ] ) ) counts += 1 if not counts % 1000 : out . write ( "\n" . join ( writing ) + "\n" ) writing = [ ] if writing : out . write ( "\n" . join ( writing ) ) out . close ( ) fr1 . close ( )
5458	def from_yaml ( cls , yaml_string ) : try : job = yaml . full_load ( yaml_string ) except AttributeError : job = yaml . load ( yaml_string ) dsub_version = job . get ( 'dsub-version' ) if not dsub_version : return cls . _from_yaml_v0 ( job ) job_metadata = { } for key in [ 'job-id' , 'job-name' , 'task-ids' , 'user-id' , 'dsub-version' , 'user-project' , 'script-name' ] : if job . get ( key ) is not None : job_metadata [ key ] = job . get ( key ) job_metadata [ 'create-time' ] = dsub_util . replace_timezone ( job . get ( 'create-time' ) , pytz . utc ) job_resources = Resources ( logging = job . get ( 'logging' ) ) job_params = { } job_params [ 'labels' ] = cls . _label_params_from_dict ( job . get ( 'labels' , { } ) ) job_params [ 'envs' ] = cls . _env_params_from_dict ( job . get ( 'envs' , { } ) ) job_params [ 'inputs' ] = cls . _input_file_params_from_dict ( job . get ( 'inputs' , { } ) , False ) job_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( job . get ( 'input-recursives' , { } ) , True ) job_params [ 'outputs' ] = cls . _output_file_params_from_dict ( job . get ( 'outputs' , { } ) , False ) job_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( job . get ( 'output-recursives' , { } ) , True ) job_params [ 'mounts' ] = cls . _mount_params_from_dict ( job . get ( 'mounts' , { } ) ) task_descriptors = [ ] for task in job . get ( 'tasks' , [ ] ) : task_metadata = { 'task-id' : task . get ( 'task-id' ) } create_time = task . get ( 'create-time' ) if create_time : task_metadata [ 'create-time' ] = dsub_util . replace_timezone ( create_time , pytz . utc ) if task . get ( 'task-attempt' ) is not None : task_metadata [ 'task-attempt' ] = task . get ( 'task-attempt' ) task_params = { } task_params [ 'labels' ] = cls . _label_params_from_dict ( task . get ( 'labels' , { } ) ) task_params [ 'envs' ] = cls . _env_params_from_dict ( task . get ( 'envs' , { } ) ) task_params [ 'inputs' ] = cls . _input_file_params_from_dict ( task . get ( 'inputs' , { } ) , False ) task_params [ 'input-recursives' ] = cls . _input_file_params_from_dict ( task . get ( 'input-recursives' , { } ) , True ) task_params [ 'outputs' ] = cls . _output_file_params_from_dict ( task . get ( 'outputs' , { } ) , False ) task_params [ 'output-recursives' ] = cls . _output_file_params_from_dict ( task . get ( 'output-recursives' , { } ) , True ) task_resources = Resources ( logging_path = task . get ( 'logging-path' ) ) task_descriptors . append ( TaskDescriptor ( task_metadata , task_params , task_resources ) ) return JobDescriptor ( job_metadata , job_params , job_resources , task_descriptors )
2059	def disassemble_instruction ( self , code , pc ) : return next ( self . disasm . disasm ( code , pc ) )
11779	def SyntheticRestaurant ( n = 20 ) : "Generate a DataSet with n examples." def gen ( ) : example = map ( random . choice , restaurant . values ) example [ restaurant . target ] = Fig [ 18 , 2 ] ( example ) return example return RestaurantDataSet ( [ gen ( ) for i in range ( n ) ] )
6499	def search ( self , query_string = None , field_dictionary = None , filter_dictionary = None , exclude_dictionary = None , facet_terms = None , exclude_ids = None , use_field_match = False , ** kwargs ) : log . debug ( "searching index with %s" , query_string ) elastic_queries = [ ] elastic_filters = [ ] if query_string : if six . PY2 : query_string = query_string . encode ( 'utf-8' ) . translate ( None , RESERVED_CHARACTERS ) else : query_string = query_string . translate ( query_string . maketrans ( '' , '' , RESERVED_CHARACTERS ) ) elastic_queries . append ( { "query_string" : { "fields" : [ "content.*" ] , "query" : query_string } } ) if field_dictionary : if use_field_match : elastic_queries . extend ( _process_field_queries ( field_dictionary ) ) else : elastic_filters . extend ( _process_field_filters ( field_dictionary ) ) if filter_dictionary : elastic_filters . extend ( _process_filters ( filter_dictionary ) ) if exclude_ids : if not exclude_dictionary : exclude_dictionary = { } if "_id" not in exclude_dictionary : exclude_dictionary [ "_id" ] = [ ] exclude_dictionary [ "_id" ] . extend ( exclude_ids ) if exclude_dictionary : elastic_filters . append ( _process_exclude_dictionary ( exclude_dictionary ) ) query_segment = { "match_all" : { } } if elastic_queries : query_segment = { "bool" : { "must" : elastic_queries } } query = query_segment if elastic_filters : filter_segment = { "bool" : { "must" : elastic_filters } } query = { "filtered" : { "query" : query_segment , "filter" : filter_segment , } } body = { "query" : query } if facet_terms : facet_query = _process_facet_terms ( facet_terms ) if facet_query : body [ "facets" ] = facet_query try : es_response = self . _es . search ( index = self . index_name , body = body , ** kwargs ) except exceptions . ElasticsearchException as ex : message = six . text_type ( ex ) if 'QueryParsingException' in message : log . exception ( "Malformed search query: %s" , message ) raise QueryParseError ( 'Malformed search query.' ) else : log . exception ( "error while searching index - %s" , str ( message ) ) raise return _translate_hits ( es_response )
1734	def remove_objects ( code , count = 1 ) : replacements = { } br = bracket_split ( code , [ '{}' , '[]' ] ) res = '' last = '' for e in br : if e [ 0 ] == '{' : n , temp_rep , cand_count = remove_objects ( e [ 1 : - 1 ] , count ) if is_object ( n , last ) : res += ' ' + OBJECT_LVAL % count replacements [ OBJECT_LVAL % count ] = e count += 1 else : res += '{%s}' % n count = cand_count replacements . update ( temp_rep ) elif e [ 0 ] == '[' : if is_array ( last ) : res += e else : n , rep , count = remove_objects ( e [ 1 : - 1 ] , count ) res += '[%s]' % n replacements . update ( rep ) else : res += e last = e return res , replacements , count
10520	def oneup ( self , window_name , object_name , iterations ) : if not self . verifyscrollbarvertical ( window_name , object_name ) : raise LdtpServerException ( 'Object not vertical scrollbar' ) object_handle = self . _get_object_handle ( window_name , object_name ) i = 0 minValue = 1.0 / 8 flag = False while i < iterations : if object_handle . AXValue <= 0 : raise LdtpServerException ( 'Minimum limit reached' ) object_handle . AXValue -= minValue time . sleep ( 1.0 / 100 ) flag = True i += 1 if flag : return 1 else : raise LdtpServerException ( 'Unable to decrease scrollbar' )
3736	def Stockmayer ( Tm = None , Tb = None , Tc = None , Zc = None , omega = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if CASRN in MagalhaesLJ_data . index : methods . append ( MAGALHAES ) if Tc and omega : methods . append ( TEEGOTOSTEWARD2 ) if Tc : methods . append ( FLYNN ) methods . append ( BSLC ) methods . append ( TEEGOTOSTEWARD1 ) if Tb : methods . append ( BSLB ) if Tm : methods . append ( BSLM ) if Tc and Zc : methods . append ( STIELTHODOS ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == FLYNN : epsilon = epsilon_Flynn ( Tc ) elif Method == BSLC : epsilon = epsilon_Bird_Stewart_Lightfoot_critical ( Tc ) elif Method == BSLB : epsilon = epsilon_Bird_Stewart_Lightfoot_boiling ( Tb ) elif Method == BSLM : epsilon = epsilon_Bird_Stewart_Lightfoot_melting ( Tm ) elif Method == STIELTHODOS : epsilon = epsilon_Stiel_Thodos ( Tc , Zc ) elif Method == TEEGOTOSTEWARD1 : epsilon = epsilon_Tee_Gotoh_Steward_1 ( Tc ) elif Method == TEEGOTOSTEWARD2 : epsilon = epsilon_Tee_Gotoh_Steward_2 ( Tc , omega ) elif Method == MAGALHAES : epsilon = float ( MagalhaesLJ_data . at [ CASRN , "epsilon" ] ) elif Method == NONE : epsilon = None else : raise Exception ( 'Failure in in function' ) return epsilon
6119	def circular_anti_annular ( cls , shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre = ( 0. , 0. ) , invert = False ) : mask = mask_util . mask_circular_anti_annular_from_shape_pixel_scale_and_radii ( shape , pixel_scale , inner_radius_arcsec , outer_radius_arcsec , outer_radius_2_arcsec , centre ) if invert : mask = np . invert ( mask ) return cls ( array = mask . astype ( 'bool' ) , pixel_scale = pixel_scale )
2554	def setdocument ( self , doc ) : if self . document != doc : self . document = doc for i in self . children : if not isinstance ( i , dom_tag ) : return i . setdocument ( doc )
7713	def handle_roster_push ( self , stanza ) : if self . server is None and stanza . from_jid : logger . debug ( u"Server address not known, cannot verify roster push" " from {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) if self . server and stanza . from_jid and stanza . from_jid != self . server : logger . debug ( u"Roster push from invalid source: {0}" . format ( stanza . from_jid ) ) return stanza . make_error_response ( u"service-unavailable" ) payload = stanza . get_payload ( RosterPayload ) if len ( payload ) != 1 : logger . warning ( "Bad roster push received ({0} items)" . format ( len ( payload ) ) ) return stanza . make_error_response ( u"bad-request" ) if self . roster is None : logger . debug ( "Dropping roster push - no roster here" ) return True item = payload [ 0 ] item . verify_roster_push ( True ) old_item = self . roster . get ( item . jid ) if item . subscription == "remove" : if old_item : self . roster . remove_item ( item . jid ) else : self . roster . add_item ( item , replace = True ) self . _event_queue . put ( RosterUpdatedEvent ( self , old_item , item ) ) return stanza . make_result_response ( )
2545	def add_review_comment ( self , doc , comment ) : if len ( doc . reviews ) != 0 : if not self . review_comment_set : self . review_comment_set = True doc . reviews [ - 1 ] . comment = comment return True else : raise CardinalityError ( 'ReviewComment' ) else : raise OrderError ( 'ReviewComment' )
556	def getCompletedSwarms ( self ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'status' ] == 'completed' : swarmIds . append ( swarmId ) return swarmIds
9642	def set_trace ( context ) : try : import ipdb as pdb except ImportError : import pdb print ( "For best results, pip install ipdb." ) print ( "Variables that are available in the current context:" ) render = lambda s : template . Template ( s ) . render ( context ) availables = get_variables ( context ) pprint ( availables ) print ( 'Type `availables` to show this list.' ) print ( 'Type <variable_name> to access one.' ) print ( 'Use render("template string") to test template rendering' ) for var in availables : locals ( ) [ var ] = context [ var ] pdb . set_trace ( ) return ''
13184	def row_to_dict ( cls , row ) : comment_code = row [ 3 ] if comment_code . lower ( ) == 'na' : comment_code = '' comp1 = row [ 4 ] if comp1 . lower ( ) == 'na' : comp1 = '' comp2 = row [ 5 ] if comp2 . lower ( ) == 'na' : comp2 = '' chart = row [ 6 ] if chart . lower ( ) == 'na' : chart = '' notes = row [ 7 ] if notes . lower ( ) == 'na' : notes = '' return { 'name' : row [ 0 ] , 'date' : row [ 1 ] , 'magnitude' : row [ 2 ] , 'comment_code' : comment_code , 'comp1' : comp1 , 'comp2' : comp2 , 'chart' : chart , 'notes' : notes , }
13364	def setup_engines ( client = None ) : if not client : try : client = ipyparallel . Client ( ) except : raise DistobClusterError ( u ) eids = client . ids if not eids : raise DistobClusterError ( u'No ipyparallel compute engines are available' ) nengines = len ( eids ) dv = client [ eids ] dv . use_dill ( ) with dv . sync_imports ( quiet = True ) : import distob ars = [ ] for i in eids : dv . targets = i ars . append ( dv . apply_async ( _remote_setup_engine , i , nengines ) ) dv . wait ( ars ) for ar in ars : if not ar . successful ( ) : raise ar . r if distob . engine is None : distob . engine = ObjectHub ( - 1 , client )
10472	def _addKeyToQueue ( self , keychr , modFlags = 0 , globally = False ) : if not keychr : return if not hasattr ( self , 'keyboard' ) : self . keyboard = AXKeyboard . loadKeyboard ( ) if keychr in self . keyboard [ 'upperSymbols' ] and not modFlags : self . _sendKeyWithModifiers ( keychr , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr . isupper ( ) and not modFlags : self . _sendKeyWithModifiers ( keychr . lower ( ) , [ AXKeyCodeConstants . SHIFT ] , globally ) return if keychr not in self . keyboard : self . _clearEventQueue ( ) raise ValueError ( 'Key %s not found in keyboard layout' % keychr ) keyDown = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , True ) keyUp = Quartz . CGEventCreateKeyboardEvent ( None , self . keyboard [ keychr ] , False ) Quartz . CGEventSetFlags ( keyDown , modFlags ) Quartz . CGEventSetFlags ( keyUp , modFlags ) if not globally : macVer , _ , _ = platform . mac_ver ( ) macVer = int ( macVer . split ( '.' ) [ 1 ] ) if macVer > 10 : appPid = self . _getPid ( ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPid , ( appPid , keyUp ) ) else : appPsn = self . _getPsnForPid ( self . _getPid ( ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyDown ) ) self . _queueEvent ( Quartz . CGEventPostToPSN , ( appPsn , keyUp ) ) else : self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyDown ) ) self . _queueEvent ( Quartz . CGEventPost , ( 0 , keyUp ) )
4729	def __run ( self , shell = True , echo = True ) : if env ( ) : return 1 cij . emph ( "cij.dmesg.start: shell: %r, cmd: %r" % ( shell , self . __prefix + self . __suffix ) ) return cij . ssh . command ( self . __prefix , shell , echo , self . __suffix )
5995	def plot_mask ( mask , units , kpc_per_arcsec , pointsize , zoom_offset_pixels ) : if mask is not None : plt . gca ( ) edge_pixels = mask . masked_grid_index_to_pixel [ mask . edge_pixels ] + 0.5 if zoom_offset_pixels is not None : edge_pixels -= zoom_offset_pixels edge_arcsec = mask . grid_pixels_to_grid_arcsec ( grid_pixels = edge_pixels ) edge_units = convert_grid_units ( array = mask , grid_arcsec = edge_arcsec , units = units , kpc_per_arcsec = kpc_per_arcsec ) plt . scatter ( y = edge_units [ : , 0 ] , x = edge_units [ : , 1 ] , s = pointsize , c = 'k' )
7199	def get_chip ( self , coordinates , catid , chip_type = 'PAN' , chip_format = 'TIF' , filename = 'chip.tif' ) : def t2s1 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ',' , '' ) def t2s2 ( t ) : return str ( t ) . strip ( '(,)' ) . replace ( ' ' , '' ) if len ( coordinates ) != 4 : print ( 'Wrong coordinate entry' ) return False W , S , E , N = coordinates box = ( ( W , S ) , ( W , N ) , ( E , N ) , ( E , S ) , ( W , S ) ) box_wkt = 'POLYGON ((' + ',' . join ( [ t2s1 ( corner ) for corner in box ] ) + '))' results = self . get_images_by_catid_and_aoi ( catid = catid , aoi_wkt = box_wkt ) description = self . describe_images ( results ) pan_id , ms_id , num_bands = None , None , 0 for catid , images in description . items ( ) : for partnum , part in images [ 'parts' ] . items ( ) : if 'PAN' in part . keys ( ) : pan_id = part [ 'PAN' ] [ 'id' ] bucket = part [ 'PAN' ] [ 'bucket' ] if 'WORLDVIEW_8_BAND' in part . keys ( ) : ms_id = part [ 'WORLDVIEW_8_BAND' ] [ 'id' ] num_bands = 8 bucket = part [ 'WORLDVIEW_8_BAND' ] [ 'bucket' ] elif 'RGBN' in part . keys ( ) : ms_id = part [ 'RGBN' ] [ 'id' ] num_bands = 4 bucket = part [ 'RGBN' ] [ 'bucket' ] band_str = '' if chip_type == 'PAN' : band_str = pan_id + '?bands=0' elif chip_type == 'MS' : band_str = ms_id + '?' elif chip_type == 'PS' : if num_bands == 8 : band_str = ms_id + '?bands=4,2,1&panId=' + pan_id elif num_bands == 4 : band_str = ms_id + '?bands=0,1,2&panId=' + pan_id location_str = '&upperLeft={}&lowerRight={}' . format ( t2s2 ( ( W , N ) ) , t2s2 ( ( E , S ) ) ) service_url = 'https://idaho.geobigdata.io/v1/chip/bbox/' + bucket + '/' url = service_url + band_str + location_str url += '&format=' + chip_format + '&token=' + self . gbdx_connection . access_token r = requests . get ( url ) if r . status_code == 200 : with open ( filename , 'wb' ) as f : f . write ( r . content ) return True else : print ( 'Cannot download chip' ) return False
13069	def r_collection ( self , objectId , lang = None ) : collection = self . resolver . getMetadata ( objectId ) return { "template" : "main::collection.html" , "collections" : { "current" : { "label" : str ( collection . get_label ( lang ) ) , "id" : collection . id , "model" : str ( collection . model ) , "type" : str ( collection . type ) , } , "members" : self . make_members ( collection , lang = lang ) , "parents" : self . make_parents ( collection , lang = lang ) } , }
3028	def _get_application_default_credential_from_file ( filename ) : with open ( filename ) as file_obj : client_credentials = json . load ( file_obj ) credentials_type = client_credentials . get ( 'type' ) if credentials_type == AUTHORIZED_USER : required_fields = set ( [ 'client_id' , 'client_secret' , 'refresh_token' ] ) elif credentials_type == SERVICE_ACCOUNT : required_fields = set ( [ 'client_id' , 'client_email' , 'private_key_id' , 'private_key' ] ) else : raise ApplicationDefaultCredentialsError ( "'type' field should be defined (and have one of the '" + AUTHORIZED_USER + "' or '" + SERVICE_ACCOUNT + "' values)" ) missing_fields = required_fields . difference ( client_credentials . keys ( ) ) if missing_fields : _raise_exception_for_missing_fields ( missing_fields ) if client_credentials [ 'type' ] == AUTHORIZED_USER : return GoogleCredentials ( access_token = None , client_id = client_credentials [ 'client_id' ] , client_secret = client_credentials [ 'client_secret' ] , refresh_token = client_credentials [ 'refresh_token' ] , token_expiry = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , user_agent = 'Python client library' ) else : from oauth2client import service_account return service_account . _JWTAccessCredentials . from_json_keyfile_dict ( client_credentials )
2946	def get_ready_user_tasks ( self ) : return [ t for t in self . get_tasks ( Task . READY ) if not self . _is_engine_task ( t . task_spec ) ]
5872	def serialize_organization ( organization ) : return { 'id' : organization . id , 'name' : organization . name , 'short_name' : organization . short_name , 'description' : organization . description , 'logo' : organization . logo }
1414	def get_pplan ( self , topologyName , callback = None ) : isWatching = False ret = { "result" : None } if callback : isWatching = True else : def callback ( data ) : ret [ "result" ] = data self . _get_pplan_with_watch ( topologyName , callback , isWatching ) return ret [ "result" ]
6788	def push ( self , components = None , yes = 0 ) : from burlap import notifier service = self . get_satchel ( 'service' ) self . lock ( ) try : yes = int ( yes ) if not yes : if self . genv . host_string == self . genv . hosts [ 0 ] : execute ( partial ( self . preview , components = components , ask = 1 ) ) notifier . notify_pre_deployment ( ) component_order , plan_funcs = self . get_component_funcs ( components = components ) service . pre_deploy ( ) for func_name , plan_func in plan_funcs : print ( 'Executing %s...' % func_name ) plan_func ( ) self . fake ( components = components ) service . post_deploy ( ) notifier . notify_post_deployment ( ) finally : self . unlock ( )
12538	def get_unique_field_values ( dcm_file_list , field_name ) : field_values = set ( ) for dcm in dcm_file_list : field_values . add ( str ( DicomFile ( dcm ) . get_attributes ( field_name ) ) ) return field_values
8452	def _get_current_branch ( ) : result = temple . utils . shell ( 'git rev-parse --abbrev-ref HEAD' , stdout = subprocess . PIPE ) return result . stdout . decode ( 'utf8' ) . strip ( )
5887	def extract ( self , url = None , raw_html = None ) : crawl_candidate = CrawlCandidate ( self . config , url , raw_html ) return self . __crawl ( crawl_candidate )
8826	def populate_subtasks ( self , context , sg , parent_job_id ) : db_sg = db_api . security_group_find ( context , id = sg , scope = db_api . ONE ) if not db_sg : return None ports = db_api . sg_gather_associated_ports ( context , db_sg ) if len ( ports ) == 0 : return { "ports" : 0 } for port in ports : job_body = dict ( action = "update port %s" % port [ 'id' ] , tenant_id = db_sg [ 'tenant_id' ] , resource_id = port [ 'id' ] , parent_id = parent_job_id ) job_body = dict ( job = job_body ) job = job_api . create_job ( context . elevated ( ) , job_body ) rpc_consumer = QuarkSGAsyncConsumerClient ( ) try : rpc_consumer . update_port ( context , port [ 'id' ] , job [ 'id' ] ) except om_exc . MessagingTimeout : LOG . error ( "Failed to update port. Rabbit running?" ) return None
11277	def run_program ( prog_list , debug , shell ) : try : if not shell : process = Popen ( prog_list , stdout = PIPE , stderr = PIPE ) stdout , stderr = process . communicate ( ) retcode = process . returncode if debug >= 1 : print ( "Program : " , " " . join ( prog_list ) ) print ( "Return Code: " , retcode ) print ( "Stdout: " , stdout ) print ( "Stderr: " , stderr ) return bool ( retcode ) else : command = " " . join ( prog_list ) os . system ( command ) return True except : return False
12360	def format_request_url ( self , resource , * args ) : return '/' . join ( ( self . api_url , self . api_version , resource ) + tuple ( str ( x ) for x in args ) )
1604	def run_metrics ( command , parser , cl_args , unknown_args ) : cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] topology = cl_args [ 'topology-name' ] try : result = tracker_access . get_topology_info ( cluster , env , topology , role ) spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) components = spouts + bolts cname = cl_args [ 'component' ] if cname : if cname in components : components = [ cname ] else : Log . error ( 'Unknown component: \'%s\'' % cname ) raise except Exception : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False cresult = [ ] for comp in components : try : metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) except : Log . error ( "Fail to connect to tracker: \'%s\'" , cl_args [ "tracker_url" ] ) return False stat , header = to_table ( metrics ) cresult . append ( ( comp , stat , header ) ) for i , ( comp , stat , header ) in enumerate ( cresult ) : if i != 0 : print ( '' ) print ( '\'%s\' metrics:' % comp ) print ( tabulate ( stat , headers = header ) ) return True
9740	def get_2d_markers_linearized ( self , component_info = None , data = None , component_position = None , index = None ) : return self . _get_2d_markers ( data , component_info , component_position , index = index )
10572	def walk_depth ( path , max_depth = float ( 'inf' ) ) : start_level = os . path . abspath ( path ) . count ( os . path . sep ) for dir_entry in os . walk ( path ) : root , dirs , _ = dir_entry level = root . count ( os . path . sep ) - start_level yield dir_entry if level >= max_depth : dirs [ : ] = [ ]
6801	def load_db_set ( self , name , r = None ) : r = r or self db_set = r . genv . db_sets . get ( name , { } ) r . genv . update ( db_set )
4749	def get_parm ( self , key ) : if key in self . __parm . keys ( ) : return self . __parm [ key ] return None
1293	def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )
3277	def handle_move ( self , dest_path ) : if "/by_tag/" not in self . path : raise DAVError ( HTTP_FORBIDDEN ) if "/by_tag/" not in dest_path : raise DAVError ( HTTP_FORBIDDEN ) catType , tag , _rest = util . save_split ( self . path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" assert tag in self . data [ "tags" ] self . data [ "tags" ] . remove ( tag ) catType , tag , _rest = util . save_split ( dest_path . strip ( "/" ) , "/" , 2 ) assert catType == "by_tag" if tag not in self . data [ "tags" ] : self . data [ "tags" ] . append ( tag ) return True
10037	def execute ( helper , config , args ) : out ( "Available solution stacks" ) for stack in helper . list_available_solution_stacks ( ) : out ( " " + str ( stack ) ) return 0
6613	def receive_finished ( self ) : if not self . isopen : logger = logging . getLogger ( __name__ ) logger . warning ( 'the drop box is not open' ) return return self . dropbox . poll ( )
8489	def get_watcher ( self ) : if not self . watching : raise StopIteration ( ) return self . client . eternal_watch ( self . prefix , recursive = True )
8432	def manual_pal ( values ) : max_n = len ( values ) def _manual_pal ( n ) : if n > max_n : msg = ( "Palette can return a maximum of {} values. " "{} were requested from it." ) warnings . warn ( msg . format ( max_n , n ) ) return values [ : n ] return _manual_pal
12264	def _send_file_internal ( self , * args , ** kwargs ) : super ( Key , self ) . _send_file_internal ( * args , ** kwargs ) mimicdb . backend . sadd ( tpl . bucket % self . bucket . name , self . name ) mimicdb . backend . hmset ( tpl . key % ( self . bucket . name , self . name ) , dict ( size = self . size , md5 = self . md5 ) )
5351	def __autorefresh_studies ( self , cfg ) : if 'studies' not in self . conf [ self . backend_section ] or 'enrich_areas_of_code:git' not in self . conf [ self . backend_section ] [ 'studies' ] : logger . debug ( "Not doing autorefresh for studies, Areas of Code study is not active." ) return aoc_index = self . conf [ 'enrich_areas_of_code:git' ] . get ( 'out_index' , GitEnrich . GIT_AOC_ENRICHED ) if not aoc_index : aoc_index = GitEnrich . GIT_AOC_ENRICHED logger . debug ( "Autorefresh for Areas of Code study index: %s" , aoc_index ) es = Elasticsearch ( [ self . conf [ 'es_enrichment' ] [ 'url' ] ] , timeout = 100 , verify_certs = self . _get_enrich_backend ( ) . elastic . requests . verify ) if not es . indices . exists ( index = aoc_index ) : logger . debug ( "Not doing autorefresh, index doesn't exist for Areas of Code study" ) return logger . debug ( "Doing autorefresh for Areas of Code study" ) aoc_backend = GitEnrich ( self . db_sh , None , cfg [ 'projects' ] [ 'projects_file' ] , self . db_user , self . db_password , self . db_host ) aoc_backend . mapping = None aoc_backend . roles = [ 'author' ] elastic_enrich = get_elastic ( self . conf [ 'es_enrichment' ] [ 'url' ] , aoc_index , clean = False , backend = aoc_backend ) aoc_backend . set_elastic ( elastic_enrich ) self . __autorefresh ( aoc_backend , studies = True )
2636	def update_parent ( self , fut ) : self . parent = fut try : fut . add_done_callback ( self . parent_callback ) except Exception as e : logger . error ( "add_done_callback got an exception {} which will be ignored" . format ( e ) )
11169	def _add_positional_argument ( self , posarg ) : if self . positional_args : if self . positional_args [ - 1 ] . recurring : raise ValueError ( "recurring positional arguments must be last" ) if self . positional_args [ - 1 ] . optional and not posarg . optional : raise ValueError ( "required positional arguments must precede optional ones" ) self . positional_args . append ( posarg )
6827	def add_remote ( self , path , name , remote_url , use_sudo = False , user = None , fetch = True ) : if path is None : raise ValueError ( "Path to the working copy is needed to add a remote" ) if fetch : cmd = 'git remote add -f %s %s' % ( name , remote_url ) else : cmd = 'git remote add %s %s' % ( name , remote_url ) with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
9501	def _disassemble ( self , lineno_width = 3 , mark_as_current = False ) : fields = [ ] if lineno_width : if self . starts_line is not None : lineno_fmt = "%%%dd" % lineno_width fields . append ( lineno_fmt % self . starts_line ) else : fields . append ( ' ' * lineno_width ) if mark_as_current : fields . append ( ' ) else : fields . append ( ' ' ) if self . is_jump_target : fields . append ( '>>' ) else : fields . append ( ' ' ) fields . append ( repr ( self . offset ) . rjust ( 4 ) ) fields . append ( self . opname . ljust ( 20 ) ) if self . arg is not None : fields . append ( repr ( self . arg ) . rjust ( 5 ) ) if self . argrepr : fields . append ( '(' + self . argrepr + ')' ) return ' ' . join ( fields ) . rstrip ( )
3482	def _get_doc_from_filename ( filename ) : if isinstance ( filename , string_types ) : if ( "win" in platform ) and ( len ( filename ) < 260 ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) elif ( "win" not in platform ) and os . path . exists ( filename ) : doc = libsbml . readSBMLFromFile ( filename ) else : if "<sbml" not in filename : raise IOError ( "The file with 'filename' does not exist, " "or is not an SBML string. Provide the path to " "an existing SBML file or a valid SBML string " "representation: \n%s" , filename ) doc = libsbml . readSBMLFromString ( filename ) elif hasattr ( filename , "read" ) : doc = libsbml . readSBMLFromString ( filename . read ( ) ) else : raise CobraSBMLError ( "Input type '%s' for 'filename' is not supported." " Provide a path, SBML str, " "or file handle." , type ( filename ) ) return doc
9572	async def message_handler ( self , data ) : message = self . build_message ( data ) if not message : logger . error ( '[%s] Unable to build Message with data, data=%s, error' , self . engine_name , data ) return logger . info ( '[%s] New message from %s: %s' , self . engine_name , message . user , message . text ) response = await self . get_response ( message ) if response : await self . send_response ( response )
5220	def save_intraday ( data : pd . DataFrame , ticker : str , dt , typ = 'TRADE' ) : cur_dt = pd . Timestamp ( dt ) . strftime ( '%Y-%m-%d' ) logger = logs . get_logger ( save_intraday , level = 'debug' ) info = f'{ticker} / {cur_dt} / {typ}' data_file = hist_file ( ticker = ticker , dt = dt , typ = typ ) if not data_file : return if data . empty : logger . warning ( f'data is empty for {info} ...' ) return exch = const . exch_info ( ticker = ticker ) if exch . empty : return end_time = pd . Timestamp ( const . market_timing ( ticker = ticker , dt = dt , timing = 'FINISHED' ) ) . tz_localize ( exch . tz ) now = pd . Timestamp ( 'now' , tz = exch . tz ) - pd . Timedelta ( '1H' ) if end_time > now : logger . debug ( f'skip saving cause market close ({end_time}) < now - 1H ({now}) ...' ) return logger . info ( f'saving data to {data_file} ...' ) files . create_folder ( data_file , is_file = True ) data . to_parquet ( data_file )
1257	def create_atomic_observe_operations ( self , states , actions , internals , terminal , reward , index ) : num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : states = util . map_tensors ( fn = tf . stop_gradient , tensors = states ) internals = util . map_tensors ( fn = tf . stop_gradient , tensors = internals ) actions = util . map_tensors ( fn = tf . stop_gradient , tensors = actions ) terminal = tf . stop_gradient ( input = terminal ) reward = tf . stop_gradient ( input = reward ) observation = self . fn_observe_timestep ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( observation , ) ) : self . unbuffered_episode_output = self . global_episode + 0
126	def Negative ( other_param , mode = "invert" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = False , mode = mode , reroll_count_max = reroll_count_max )
12228	def register_admin_models ( admin_site ) : global __MODELS_REGISTRY prefs = get_prefs ( ) for app_label , prefs_items in prefs . items ( ) : model_class = get_pref_model_class ( app_label , prefs_items , get_app_prefs ) if model_class is not None : __MODELS_REGISTRY [ app_label ] = model_class admin_site . register ( model_class , get_pref_model_admin_class ( prefs_items ) )
10492	def doubleClickDragMouseButtonLeft ( self , coord , dest_coord , interval = 0.5 ) : modFlags = 0 self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , dest_coord = dest_coord , clickCount = 2 ) self . _postQueuedEvents ( interval = interval )
3110	def locked_delete ( self ) : query = { self . key_name : self . key_value } self . model_class . objects . filter ( ** query ) . delete ( )
9510	def subseq ( self , start , end ) : return Fasta ( self . id , self . seq [ start : end ] )
13548	def update ( dst , src ) : stack = [ ( dst , src ) ] def isdict ( o ) : return hasattr ( o , 'keys' ) while stack : current_dst , current_src = stack . pop ( ) for key in current_src : if key not in current_dst : current_dst [ key ] = current_src [ key ] else : if isdict ( current_src [ key ] ) and isdict ( current_dst [ key ] ) : stack . append ( ( current_dst [ key ] , current_src [ key ] ) ) else : current_dst [ key ] = current_src [ key ] return dst
5358	def es_version ( self , url ) : try : res = self . grimoire_con . get ( url ) res . raise_for_status ( ) major = res . json ( ) [ 'version' ] [ 'number' ] . split ( "." ) [ 0 ] except Exception : logger . error ( "Error retrieving Elasticsearch version: " + url ) raise return major
13717	def request ( self , batch , attempt = 0 ) : try : q = self . api . new_queue ( ) for msg in batch : q . add ( msg [ 'event' ] , msg [ 'value' ] , source = msg [ 'source' ] ) q . submit ( ) except : if attempt > self . retries : raise self . request ( batch , attempt + 1 )
6664	def list_expiration_dates ( self , base = 'roles/all/ssl' ) : max_fn_len = 0 max_date_len = 0 data = [ ] for fn in os . listdir ( base ) : fqfn = os . path . join ( base , fn ) if not os . path . isfile ( fqfn ) : continue if not fn . endswith ( '.crt' ) : continue expiration_date = self . get_expiration_date ( fqfn ) max_fn_len = max ( max_fn_len , len ( fn ) ) max_date_len = max ( max_date_len , len ( str ( expiration_date ) ) ) data . append ( ( fn , expiration_date ) ) print ( '%s %s %s' % ( 'Filename' . ljust ( max_fn_len ) , 'Expiration Date' . ljust ( max_date_len ) , 'Expired' ) ) now = datetime . now ( ) . replace ( tzinfo = pytz . UTC ) for fn , dt in sorted ( data ) : if dt is None : expired = '?' elif dt < now : expired = 'YES' else : expired = 'NO' print ( '%s %s %s' % ( fn . ljust ( max_fn_len ) , str ( dt ) . ljust ( max_date_len ) , expired ) )
8460	def write_temple_config ( temple_config , template , version ) : with open ( temple . constants . TEMPLE_CONFIG_FILE , 'w' ) as temple_config_file : versioned_config = { ** temple_config , ** { '_version' : version , '_template' : template } , } yaml . dump ( versioned_config , temple_config_file , Dumper = yaml . SafeDumper )
12260	def columns ( x , rho , proxop ) : xnext = np . zeros_like ( x ) for ix in range ( x . shape [ 1 ] ) : xnext [ : , ix ] = proxop ( x [ : , ix ] , rho ) return xnext
10089	def files ( self ) : files_ = super ( Deposit , self ) . files if files_ : sort_by_ = files_ . sort_by def sort_by ( * args , ** kwargs ) : if 'draft' != self . status : raise PIDInvalidAction ( ) return sort_by_ ( * args , ** kwargs ) files_ . sort_by = sort_by return files_
4553	def set_colors ( self , colors , pos ) : self . _colors = colors self . _pos = pos end = self . _pos + self . numLEDs if end > len ( self . _colors ) : raise ValueError ( 'Needed %d colors but found %d' % ( end , len ( self . _colors ) ) )
2286	def graph_evaluation ( data , adj_matrix , gpu = None , gpu_id = 0 , ** kwargs ) : gpu = SETTINGS . get_default ( gpu = gpu ) device = 'cuda:{}' . format ( gpu_id ) if gpu else 'cpu' obs = th . FloatTensor ( data ) . to ( device ) cgnn = CGNN_model ( adj_matrix , data . shape [ 0 ] , gpu_id = gpu_id , ** kwargs ) . to ( device ) cgnn . reset_parameters ( ) return cgnn . run ( obs , ** kwargs )
4806	def _fmt_args_kwargs ( self , * some_args , ** some_kwargs ) : if some_args : out_args = str ( some_args ) . lstrip ( '(' ) . rstrip ( ',)' ) if some_kwargs : out_kwargs = ', ' . join ( [ str ( i ) . lstrip ( '(' ) . rstrip ( ')' ) . replace ( ', ' , ': ' ) for i in [ ( k , some_kwargs [ k ] ) for k in sorted ( some_kwargs . keys ( ) ) ] ] ) if some_args and some_kwargs : return out_args + ', ' + out_kwargs elif some_args : return out_args elif some_kwargs : return out_kwargs else : return ''
11098	def select_by_size ( self , min_size = 0 , max_size = 1 << 40 , recursive = True ) : def filters ( p ) : return min_size <= p . size <= max_size return self . select_file ( filters , recursive )
12340	def compress_blocking ( image , delete_tif = False , folder = None , force = False ) : debug ( 'compressing {}' . format ( image ) ) try : new_filename , extension = os . path . splitext ( image ) new_filename = new_filename . rsplit ( '.ome' , 1 ) [ 0 ] if folder : basename = os . path . basename ( new_filename ) new_filename = os . path . join ( folder , basename + '.png' ) else : new_filename = new_filename + '.png' if os . path . isfile ( new_filename ) and not force : compressed_images . append ( new_filename ) msg = "Aborting compress, PNG already" " exists: {}" . format ( new_filename ) raise AssertionError ( msg ) if extension != '.tif' : msg = "Aborting compress, not a TIFF: {}" . format ( image ) raise AssertionError ( msg ) img = Image . open ( image ) fptr = img . fp img . load ( ) tags = img . tag . as_dict ( ) with open ( new_filename [ : - 4 ] + '.json' , 'w' ) as f : if img . mode == 'P' : tags [ 'palette' ] = img . getpalette ( ) json . dump ( tags , f ) if img . mode == 'P' : debug ( 'palette-mode switched to luminance' ) img . mode = 'L' if img . mode == 'I;16' : img = img . convert ( mode = 'I' ) debug ( 'saving to {}' . format ( new_filename ) ) img . save ( new_filename ) fptr . close ( ) if delete_tif : os . remove ( image ) except ( IOError , AssertionError ) as e : print ( 'leicaexperiment {}' . format ( e ) ) return '' return new_filename
3437	def repair ( self , rebuild_index = True , rebuild_relationships = True ) : if rebuild_index : self . reactions . _generate_index ( ) self . metabolites . _generate_index ( ) self . genes . _generate_index ( ) self . groups . _generate_index ( ) if rebuild_relationships : for met in self . metabolites : met . _reaction . clear ( ) for gene in self . genes : gene . _reaction . clear ( ) for rxn in self . reactions : for met in rxn . _metabolites : met . _reaction . add ( rxn ) for gene in rxn . _genes : gene . _reaction . add ( rxn ) for l in ( self . reactions , self . genes , self . metabolites , self . groups ) : for e in l : e . _model = self
13755	def write_to_file ( file_path , contents , encoding = "utf-8" ) : with codecs . open ( file_path , "w" , encoding ) as f : f . write ( contents )
4725	def get_chunk_meta ( self , meta_file ) : chunks = self . envs [ "CHUNKS" ] if cij . nvme . get_meta ( 0 , chunks * self . envs [ "CHUNK_META_SIZEOF" ] , meta_file ) : raise RuntimeError ( "cij.liblight.get_chunk_meta: fail" ) chunk_meta = cij . bin . Buffer ( types = self . envs [ "CHUNK_META_STRUCT" ] , length = chunks ) chunk_meta . read ( meta_file ) return chunk_meta
12117	def ndist ( data , Xs ) : sigma = np . sqrt ( np . var ( data ) ) center = np . average ( data ) curve = mlab . normpdf ( Xs , center , sigma ) curve *= len ( data ) * HIST_RESOLUTION return curve
13354	def _pipepager ( text , cmd , color ) : import subprocess env = dict ( os . environ ) cmd_detail = cmd . rsplit ( '/' , 1 ) [ - 1 ] . split ( ) if color is None and cmd_detail [ 0 ] == 'less' : less_flags = os . environ . get ( 'LESS' , '' ) + ' ' . join ( cmd_detail [ 1 : ] ) if not less_flags : env [ 'LESS' ] = '-R' color = True elif 'r' in less_flags or 'R' in less_flags : color = True if not color : text = strip_ansi ( text ) c = subprocess . Popen ( cmd , shell = True , stdin = subprocess . PIPE , env = env ) encoding = get_best_encoding ( c . stdin ) try : c . stdin . write ( text . encode ( encoding , 'replace' ) ) c . stdin . close ( ) except ( IOError , KeyboardInterrupt ) : pass while True : try : c . wait ( ) except KeyboardInterrupt : pass else : break
11913	def git_tag ( tag ) : print ( 'Tagging "{}"' . format ( tag ) ) msg = '"Released version {}"' . format ( tag ) Popen ( [ 'git' , 'tag' , '-s' , '-m' , msg , tag ] ) . wait ( )
10871	def f_theta ( cos_theta , zint , z , n2n1 = 0.95 , sph6_ab = None , ** kwargs ) : wvfront = ( np . outer ( np . ones_like ( z ) * zint , cos_theta ) - np . outer ( zint + z , csqrt ( n2n1 ** 2 - 1 + cos_theta ** 2 ) ) ) if ( sph6_ab is not None ) and ( not np . isnan ( sph6_ab ) ) : sec2_theta = 1.0 / ( cos_theta * cos_theta ) wvfront += sph6_ab * ( sec2_theta - 1 ) * ( sec2_theta - 2 ) * cos_theta if wvfront . dtype == np . dtype ( 'complex128' ) : wvfront . imag = - np . abs ( wvfront . imag ) return wvfront
12530	def open_volume_file ( filepath ) : if not op . exists ( filepath ) : raise IOError ( 'Could not find file {}.' . format ( filepath ) ) def open_nifti_file ( filepath ) : return NiftiImage ( filepath ) def open_mhd_file ( filepath ) : return MedicalImage ( filepath ) vol_data , hdr_data = load_raw_data_with_mhd ( filepath ) return vol_data , hdr_data def open_mha_file ( filepath ) : raise NotImplementedError ( 'This function has not been implemented yet.' ) def _load_file ( filepath , loader ) : return loader ( filepath ) filext_loader = { 'nii' : open_nifti_file , 'mhd' : open_mhd_file , 'mha' : open_mha_file , } ext = get_extension ( filepath ) loader = None for e in filext_loader : if ext in e : loader = filext_loader [ e ] if loader is None : raise ValueError ( 'Could not find a loader for file {}.' . format ( filepath ) ) return _load_file ( filepath , loader )
2356	def is_element_present ( self , strategy , locator ) : return self . driver_adapter . is_element_present ( strategy , locator , root = self . root )
4206	def levup ( acur , knxt , ecur = None ) : if acur [ 0 ] != 1 : raise ValueError ( 'At least one of the reflection coefficients is equal to one.' ) acur = acur [ 1 : ] anxt = numpy . concatenate ( ( acur , [ 0 ] ) ) + knxt * numpy . concatenate ( ( numpy . conj ( acur [ - 1 : : - 1 ] ) , [ 1 ] ) ) enxt = None if ecur is not None : enxt = ( 1. - numpy . dot ( numpy . conj ( knxt ) , knxt ) ) * ecur anxt = numpy . insert ( anxt , 0 , 1 ) return anxt , enxt
13674	def add_directory ( self , * args , ** kwargs ) : exc = kwargs . get ( 'exclusions' , None ) for path in args : self . files . append ( DirectoryPath ( path , self , exclusions = exc ) )
8773	def get_lswitch_ids_for_network ( self , context , network_id ) : lswitches = self . _lswitches_for_network ( context , network_id ) . results ( ) return [ s [ 'uuid' ] for s in lswitches [ "results" ] ]
10423	def pair_is_consistent ( graph : BELGraph , u : BaseEntity , v : BaseEntity ) -> Optional [ str ] : relations = { data [ RELATION ] for data in graph [ u ] [ v ] . values ( ) } if 1 != len ( relations ) : return return list ( relations ) [ 0 ]
4110	def ac2poly ( data ) : a , e , _c = LEVINSON ( data ) a = numpy . insert ( a , 0 , 1 ) return a , e
13492	def custom_prompt ( msg , options , default ) : formatted_options = [ x . upper ( ) if x == default else x . lower ( ) for x in options ] sure = input ( "{0} [{1}]: " . format ( msg , "/" . join ( formatted_options ) ) ) if len ( sure ) == 0 : return default for option in options : if sure . upper ( ) == option . upper ( ) : return option return default
1688	def CheckEnd ( self , filename , clean_lines , linenum , error ) : line = clean_lines . raw_lines [ linenum ] if ( linenum - self . starting_linenum < 10 and not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace\b' , line ) ) : return if self . name : if not Match ( ( r'^\s*};*\s*(//|/\*).*\bnamespace\s+' + re . escape ( self . name ) + r'[\*/\.\\\s]*$' ) , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Namespace should be terminated with "// namespace %s"' % self . name ) else : if not Match ( r'^\s*};*\s*(//|/\*).*\bnamespace[\*/\.\\\s]*$' , line ) : if Match ( r'^\s*}.*\b(namespace anonymous|anonymous namespace)\b' , line ) : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' ' or "// anonymous namespace"' ) else : error ( filename , linenum , 'readability/namespace' , 5 , 'Anonymous namespace should be terminated with "// namespace"' )
1809	def SETE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) )
12984	def keywords ( func ) : @ wraps ( func ) def decorator ( * args , ** kwargs ) : idx = 0 if inspect . ismethod ( func ) else 1 if len ( args ) > idx : if isinstance ( args [ idx ] , ( dict , composite ) ) : for key in args [ idx ] : kwargs [ key ] = args [ idx ] [ key ] args = args [ : idx ] return func ( * args , ** kwargs ) return decorator
10695	def yiq_to_rgb ( yiq ) : y , i , q = yiq r = y + ( 0.956 * i ) + ( 0.621 * q ) g = y - ( 0.272 * i ) - ( 0.647 * q ) b = y - ( 1.108 * i ) + ( 1.705 * q ) r = 1 if r > 1 else max ( 0 , r ) g = 1 if g > 1 else max ( 0 , g ) b = 1 if b > 1 else max ( 0 , b ) return round ( r * 255 , 3 ) , round ( g * 255 , 3 ) , round ( b * 255 , 3 )
4113	def rc2is ( k ) : assert numpy . isrealobj ( k ) , 'Inverse sine parameters not defined for complex reflection coefficients.' if max ( numpy . abs ( k ) ) >= 1 : raise ValueError ( 'All reflection coefficients should have magnitude less than unity.' ) return ( 2 / numpy . pi ) * numpy . arcsin ( k )
7042	def stetson_jindex ( ftimes , fmags , ferrs , weightbytimediff = False ) : ndet = len ( fmags ) if ndet > 9 : medmag = npmedian ( fmags ) delta_prefactor = ( ndet / ( ndet - 1 ) ) sigma_i = delta_prefactor * ( fmags - medmag ) / ferrs sigma_j = nproll ( sigma_i , 1 ) if weightbytimediff : difft = npdiff ( ftimes ) deltat = npmedian ( difft ) weights_i = npexp ( - difft / deltat ) products = ( weights_i * sigma_i [ 1 : ] * sigma_j [ 1 : ] ) else : products = ( sigma_i * sigma_j ) [ 1 : ] stetsonj = ( npsum ( npsign ( products ) * npsqrt ( npabs ( products ) ) ) ) / ndet return stetsonj else : LOGERROR ( 'not enough detections in this magseries ' 'to calculate stetson J index' ) return npnan
3338	def is_child_uri ( parentUri , childUri ) : return ( parentUri and childUri and childUri . rstrip ( "/" ) . startswith ( parentUri . rstrip ( "/" ) + "/" ) )
8549	def create_firewall_rule ( self , datacenter_id , server_id , nic_id , firewall_rule ) : properties = { "name" : firewall_rule . name } if firewall_rule . protocol : properties [ 'protocol' ] = firewall_rule . protocol if firewall_rule . source_mac : properties [ 'sourceMac' ] = firewall_rule . source_mac if firewall_rule . source_ip : properties [ 'sourceIp' ] = firewall_rule . source_ip if firewall_rule . target_ip : properties [ 'targetIp' ] = firewall_rule . target_ip if firewall_rule . port_range_start : properties [ 'portRangeStart' ] = firewall_rule . port_range_start if firewall_rule . port_range_end : properties [ 'portRangeEnd' ] = firewall_rule . port_range_end if firewall_rule . icmp_type : properties [ 'icmpType' ] = firewall_rule . icmp_type if firewall_rule . icmp_code : properties [ 'icmpCode' ] = firewall_rule . icmp_code data = { "properties" : properties } response = self . _perform_request ( url = '/datacenters/%s/servers/%s/nics/%s/firewallrules' % ( datacenter_id , server_id , nic_id ) , method = 'POST' , data = json . dumps ( data ) ) return response
2370	def dump ( self ) : for table in self . tables : print ( "*** %s ***" % table . name ) table . dump ( )
6447	def stem ( self , word ) : word = normalize ( 'NFC' , text_type ( word . lower ( ) ) ) for suffix_len in range ( 11 , 0 , - 1 ) : ending = word [ - suffix_len : ] if ( ending in self . _suffix and len ( word ) - suffix_len >= 2 and ( self . _suffix [ ending ] is None or self . _suffix [ ending ] ( word , suffix_len ) ) ) : word = word [ : - suffix_len ] break if word [ - 2 : ] in { 'bb' , 'dd' , 'gg' , 'll' , 'mm' , 'nn' , 'pp' , 'rr' , 'ss' , 'tt' , } : word = word [ : - 1 ] for ending , replacement in self . _recode : if word . endswith ( ending ) : if callable ( replacement ) : word = replacement ( word ) else : word = word [ : - len ( ending ) ] + replacement return word
1367	def start_connect ( self ) : Log . debug ( "In start_connect() of %s" % self . _get_classname ( ) ) self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) self . _connecting = True self . connect ( self . endpoint )
5718	def pull_datapackage ( descriptor , name , backend , ** backend_options ) : warnings . warn ( 'Functions "push/pull_datapackage" are deprecated. ' 'Please use "Package" class' , UserWarning ) datapackage_name = name plugin = import_module ( 'jsontableschema.plugins.%s' % backend ) storage = plugin . Storage ( ** backend_options ) resources = [ ] for table in storage . buckets : schema = storage . describe ( table ) base = os . path . dirname ( descriptor ) path , name = _restore_path ( table ) fullpath = os . path . join ( base , path ) helpers . ensure_dir ( fullpath ) with io . open ( fullpath , 'wb' ) as file : model = Schema ( deepcopy ( schema ) ) data = storage . iter ( table ) writer = csv . writer ( file , encoding = 'utf-8' ) writer . writerow ( model . headers ) for row in data : writer . writerow ( row ) resource = { 'schema' : schema , 'path' : path } if name is not None : resource [ 'name' ] = name resources . append ( resource ) mode = 'w' encoding = 'utf-8' if six . PY2 : mode = 'wb' encoding = None resources = _restore_resources ( resources ) helpers . ensure_dir ( descriptor ) with io . open ( descriptor , mode = mode , encoding = encoding ) as file : descriptor = { 'name' : datapackage_name , 'resources' : resources , } json . dump ( descriptor , file , indent = 4 ) return storage
4352	def leave ( self , room ) : self . socket . rooms . remove ( self . _get_room_name ( room ) )
13600	def increment ( cls , name ) : with transaction . atomic ( ) : counter = Counter . objects . select_for_update ( ) . get ( name = name ) counter . value += 1 counter . save ( ) return counter . value
5485	def _eval_arg_type ( arg_type , T = Any , arg = None , sig = None ) : try : T = eval ( arg_type ) except Exception as e : raise ValueError ( 'The type of {0} could not be evaluated in {1} for {2}: {3}' . format ( arg_type , arg , sig , text_type ( e ) ) ) else : if type ( T ) not in ( type , Type ) : raise TypeError ( '{0} is not a valid type in {1} for {2}' . format ( repr ( T ) , arg , sig ) ) return T
10573	def get_local_songs ( filepaths , include_filters = None , exclude_filters = None , all_includes = False , all_excludes = False , exclude_patterns = None , max_depth = float ( 'inf' ) ) : logger . info ( "Loading local songs..." ) supported_filepaths = get_supported_filepaths ( filepaths , SUPPORTED_SONG_FORMATS , max_depth = max_depth ) included_songs , excluded_songs = exclude_filepaths ( supported_filepaths , exclude_patterns = exclude_patterns ) matched_songs , filtered_songs = filter_local_songs ( included_songs , include_filters = include_filters , exclude_filters = exclude_filters , all_includes = all_includes , all_excludes = all_excludes ) logger . info ( "Excluded {0} local songs" . format ( len ( excluded_songs ) ) ) logger . info ( "Filtered {0} local songs" . format ( len ( filtered_songs ) ) ) logger . info ( "Loaded {0} local songs" . format ( len ( matched_songs ) ) ) return matched_songs , filtered_songs , excluded_songs
3176	def update ( self , campaign_id , feedback_id , data ) : self . campaign_id = campaign_id self . feedback_id = feedback_id if 'message' not in data : raise KeyError ( 'The campaign feedback must have a message' ) return self . _mc_client . _patch ( url = self . _build_path ( campaign_id , 'feedback' , feedback_id ) , data = data )
200	def draw_on_image ( self , image , alpha = 0.75 , resize = "segmentation_map" , background_threshold = 0.01 , background_class_id = None , colors = None , draw_background = False ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ "segmentation_map" , "image" ] ) if resize == "image" : image = ia . imresize_single_image ( image , self . arr . shape [ 0 : 2 ] , interpolation = "cubic" ) segmap_drawn , foreground_mask = self . draw ( background_threshold = background_threshold , background_class_id = background_class_id , size = image . shape [ 0 : 2 ] if resize == "segmentation_map" else None , colors = colors , return_foreground_mask = True ) if draw_background : mix = np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) else : foreground_mask = foreground_mask [ ... , np . newaxis ] mix = np . zeros_like ( image ) mix += ( ~ foreground_mask ) . astype ( np . uint8 ) * image mix += foreground_mask . astype ( np . uint8 ) * np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) return mix
453	def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( "please input a name" ) logging . info ( " [*] geting variables with %s" % name ) if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( " got {:3}: {:15} {}" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars
12233	def pref_group ( title , prefs , help_text = '' , static = True , readonly = False ) : bind_proxy ( prefs , title , help_text = help_text , static = static , readonly = readonly ) for proxy in prefs : if isinstance ( proxy , PrefProxy ) : proxy . category = title
1318	def SetWindowText ( self , text : str ) -> bool : handle = self . NativeWindowHandle if handle : return SetWindowText ( handle , text ) return False
10919	def do_levmarq ( s , param_names , damping = 0.1 , decrease_damp_factor = 10. , run_length = 6 , eig_update = True , collect_stats = False , rz_order = 0 , run_type = 2 , ** kwargs ) : if rz_order > 0 : aug = AugmentedState ( s , param_names , rz_order = rz_order ) lm = LMAugmentedState ( aug , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) else : lm = LMGlobals ( s , param_names , damping = damping , run_length = run_length , decrease_damp_factor = decrease_damp_factor , eig_update = eig_update , ** kwargs ) if run_type == 2 : lm . do_run_2 ( ) elif run_type == 1 : lm . do_run_1 ( ) else : raise ValueError ( 'run_type=1,2 only' ) if collect_stats : return lm . get_termination_stats ( )
10456	def verifycheck ( self , window_name , object_name ) : try : object_handle = self . _get_object_handle ( window_name , object_name , wait_for_object = False ) if object_handle . AXValue == 1 : return 1 except LdtpServerException : pass return 0
5071	def get_configuration_value_for_site ( site , key , default = None ) : if hasattr ( site , 'configuration' ) : return site . configuration . get_value ( key , default ) return default
3600	def http_connection ( timeout ) : def wrapper ( f ) : def wrapped ( * args , ** kwargs ) : if not ( 'connection' in kwargs ) or not kwargs [ 'connection' ] : connection = requests . Session ( ) kwargs [ 'connection' ] = connection else : connection = kwargs [ 'connection' ] if not getattr ( connection , 'timeout' , False ) : connection . timeout = timeout connection . headers . update ( { 'Content-type' : 'application/json' } ) return f ( * args , ** kwargs ) return wraps ( f ) ( wrapped ) return wrapper
13034	def write_triples ( filename , triples , delimiter = DEFAULT_DELIMITER , triple_order = "hrt" ) : with open ( filename , 'w' ) as f : for t in triples : line = t . serialize ( delimiter , triple_order ) f . write ( line + "\n" )
2192	def renew ( self , cfgstr = None , product = None ) : products = self . _rectify_products ( product ) certificate = { 'timestamp' : util_time . timestamp ( ) , 'product' : products , } if products is not None : if not all ( map ( os . path . exists , products ) ) : raise IOError ( 'The stamped product must exist: {}' . format ( products ) ) certificate [ 'product_file_hash' ] = self . _product_file_hash ( products ) self . cacher . save ( certificate , cfgstr = cfgstr ) return certificate
2033	def MSTORE ( self , address , value ) : if istainted ( self . pc ) : for taint in get_taints ( self . pc ) : value = taint_with ( value , taint ) self . _allocate ( address , 32 ) self . _store ( address , value , 32 )
1149	def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )
8207	def reflect ( self , x0 , y0 , x , y ) : rx = x0 - ( x - x0 ) ry = y0 - ( y - y0 ) return rx , ry
362	def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( "Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported" ) return filepath
481	def main_restore_embedding_layer ( ) : vocabulary_size = 50000 embedding_size = 128 model_file_name = "model_word2vec_50k_128" batch_size = None print ( "Load existing embedding matrix and dictionaries" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )
419	def save_training_log ( self , ** kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . TrainLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( "[Database] train log: " + _log )
11951	def _import_config ( config_file ) : jocker_lgr . debug ( 'config file is: {0}' . format ( config_file ) ) try : jocker_lgr . debug ( 'importing config...' ) with open ( config_file , 'r' ) as c : return yaml . safe_load ( c . read ( ) ) except IOError as ex : jocker_lgr . error ( str ( ex ) ) raise RuntimeError ( 'cannot access config file' ) except yaml . parser . ParserError as ex : jocker_lgr . error ( 'invalid yaml file: {0}' . format ( ex ) ) raise RuntimeError ( 'invalid yaml file' )
11114	def remove_repository ( self , path = None , relatedFiles = False , relatedFolders = False , verbose = True ) : if path is not None : realPath = os . path . realpath ( os . path . expanduser ( path ) ) else : realPath = self . __path if realPath is None : if verbose : warnings . warn ( 'path is None and current Repository is not initialized!' ) return if not self . is_repository ( realPath ) : if verbose : warnings . warn ( "No repository found in '%s'!" % realPath ) return if realPath == os . path . realpath ( '/..' ) : if verbose : warnings . warn ( 'You are about to wipe out your system !!! action aboarded' ) return if path is not None : repo = Repository ( ) repo . load_repository ( realPath ) else : repo = self if relatedFiles : for relativePath in repo . walk_files_relative_path ( ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isfile ( realPath ) : continue if not os . path . exists ( realPath ) : continue os . remove ( realPath ) if relatedFolders : for relativePath in reversed ( list ( repo . walk_directories_relative_path ( ) ) ) : realPath = os . path . join ( repo . path , relativePath ) if not os . path . isdir ( realPath ) : continue if not os . path . exists ( realPath ) : continue if not len ( os . listdir ( realPath ) ) : os . rmdir ( realPath ) os . remove ( os . path . join ( repo . path , ".pyrepinfo" ) ) for fname in ( ".pyrepstate" , ".pyreplock" ) : p = os . path . join ( repo . path , fname ) if os . path . exists ( p ) : os . remove ( p ) if os . path . isdir ( repo . path ) : if not len ( os . listdir ( repo . path ) ) : os . rmdir ( repo . path ) repo . __reset_repository ( )
13503	def create ( self , server ) : return server . post ( 'challenge_admin' , self . as_payload ( ) , replacements = { 'slug' : self . slug } )
6940	def _gaussian ( x , amp , loc , std ) : return amp * np . exp ( - ( ( x - loc ) * ( x - loc ) ) / ( 2.0 * std * std ) )
5379	def build_pipeline ( cls , project , zones , min_cores , min_ram , disk_size , boot_disk_size , preemptible , accelerator_type , accelerator_count , image , script_name , envs , inputs , outputs , pipeline_name ) : if min_cores is None : min_cores = job_model . DEFAULT_MIN_CORES if min_ram is None : min_ram = job_model . DEFAULT_MIN_RAM if disk_size is None : disk_size = job_model . DEFAULT_DISK_SIZE if boot_disk_size is None : boot_disk_size = job_model . DEFAULT_BOOT_DISK_SIZE if preemptible is None : preemptible = job_model . DEFAULT_PREEMPTIBLE docker_command = cls . _build_pipeline_docker_command ( script_name , inputs , outputs , envs ) input_envs = [ { 'name' : SCRIPT_VARNAME } ] + [ { 'name' : env . name } for env in envs if env . value ] input_files = [ cls . _build_pipeline_input_file_param ( var . name , var . docker_path ) for var in inputs if not var . recursive and var . value ] output_files = [ cls . _build_pipeline_file_param ( var . name , var . docker_path ) for var in outputs if not var . recursive and var . value ] return { 'ephemeralPipeline' : { 'projectId' : project , 'name' : pipeline_name , 'resources' : { 'minimumCpuCores' : min_cores , 'minimumRamGb' : min_ram , 'bootDiskSizeGb' : boot_disk_size , 'preemptible' : preemptible , 'zones' : google_base . get_zones ( zones ) , 'acceleratorType' : accelerator_type , 'acceleratorCount' : accelerator_count , 'disks' : [ { 'name' : 'datadisk' , 'autoDelete' : True , 'sizeGb' : disk_size , 'mountPoint' : providers_util . DATA_MOUNT_POINT , } ] , } , 'inputParameters' : input_envs + input_files , 'outputParameters' : output_files , 'docker' : { 'imageName' : image , 'cmd' : docker_command , } } }
8942	def upload ( self , docs_base , release ) : return getattr ( self , '_to_' + self . target ) ( docs_base , release )
11509	def delete_item ( self , token , item_id ) : parameters = dict ( ) parameters [ 'token' ] = token parameters [ 'id' ] = item_id response = self . request ( 'midas.item.delete' , parameters ) return response
243	def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily
11484	def _upload_folder_as_item ( local_folder , parent_folder_id , reuse_existing = False ) : item_id = _create_or_reuse_item ( local_folder , parent_folder_id , reuse_existing ) subdir_contents = sorted ( os . listdir ( local_folder ) ) filecount = len ( subdir_contents ) for ( ind , current_file ) in enumerate ( subdir_contents ) : file_path = os . path . join ( local_folder , current_file ) log_ind = '({0} of {1})' . format ( ind + 1 , filecount ) _create_bitstream ( file_path , current_file , item_id , log_ind ) for callback in session . item_upload_callbacks : callback ( session . communicator , session . token , item_id )
13682	def get_json_tuples ( self , prettyprint = False , translate = True ) : j = self . get_json ( prettyprint , translate ) if len ( j ) > 2 : if prettyprint : j = j [ 1 : - 2 ] + ",\n" else : j = j [ 1 : - 1 ] + "," else : j = "" return j
8474	def getConfig ( self , section = None ) : data = { } if section is None : for s in self . config . sections ( ) : if '/' in s : parent , _s = s . split ( '/' ) data [ parent ] [ _s ] = dict ( self . config . items ( s ) ) else : data [ s ] = dict ( self . config . items ( s ) ) else : data = dict ( self . config . items ( section ) ) return data
805	def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( ** self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )
12144	def figureStimulus ( abf , sweeps = [ 0 ] ) : stimuli = [ 2.31250 , 2.35270 ] for sweep in sweeps : abf . setsweep ( sweep ) for stimulus in stimuli : S1 = int ( abf . pointsPerSec * stimulus ) S2 = int ( abf . pointsPerSec * ( stimulus + 0.001 ) ) abf . sweepY [ S1 : S2 ] = np . nan I1 = int ( abf . pointsPerSec * 2.2 ) I2 = int ( abf . pointsPerSec * 2.6 ) baseline = np . average ( abf . sweepY [ int ( abf . pointsPerSec * 2.0 ) : int ( abf . pointsPerSec * 2.2 ) ] ) Ys = lowPassFilter ( abf . sweepY [ I1 : I2 ] ) - baseline Xs = abf . sweepX2 [ I1 : I1 + len ( Ys ) ] . flatten ( ) plt . plot ( Xs , Ys , alpha = .5 , lw = 2 ) return
4828	def is_enrolled ( self , username , course_run_id ) : enrollment = self . get_course_enrollment ( username , course_run_id ) return enrollment is not None and enrollment . get ( 'is_active' , False )
5662	def return_segments ( shape , break_points ) : segs = [ ] bp = 0 bp2 = 0 for i in range ( len ( break_points ) - 1 ) : bp = break_points [ i ] if break_points [ i ] is not None else bp2 bp2 = break_points [ i + 1 ] if break_points [ i + 1 ] is not None else bp segs . append ( shape [ bp : bp2 + 1 ] ) segs . append ( [ ] ) return segs
10196	def aggregate_events ( aggregations , start_date = None , end_date = None , update_bookmark = True ) : start_date = dateutil_parse ( start_date ) if start_date else None end_date = dateutil_parse ( end_date ) if end_date else None results = [ ] for a in aggregations : aggr_cfg = current_stats . aggregations [ a ] aggregator = aggr_cfg . aggregator_class ( name = aggr_cfg . name , ** aggr_cfg . aggregator_config ) results . append ( aggregator . run ( start_date , end_date , update_bookmark ) ) return results
11138	def __clean_before_after ( self , stateBefore , stateAfter , keepNoneEmptyDirectory = True ) : errors = [ ] afterDict = { } [ afterDict . setdefault ( list ( aitem ) [ 0 ] , [ ] ) . append ( aitem ) for aitem in stateAfter ] for bitem in reversed ( stateBefore ) : relaPath = list ( bitem ) [ 0 ] basename = os . path . basename ( relaPath ) btype = bitem [ relaPath ] [ 'type' ] alist = afterDict . get ( relaPath , [ ] ) aitem = [ a for a in alist if a [ relaPath ] [ 'type' ] == btype ] if len ( aitem ) > 1 : errors . append ( "Multiple '%s' of type '%s' where found in '%s', this should never had happened. Please report issue" % ( basename , btype , relaPath ) ) continue if not len ( aitem ) : removeDirs = [ ] removeFiles = [ ] if btype == 'dir' : if not len ( relaPath ) : errors . append ( "Removing main repository directory is not allowed" ) continue removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirInfo ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __dirLock ) ) elif btype == 'file' : removeFiles . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileLock % basename ) ) else : removeDirs . append ( os . path . join ( self . __path , relaPath ) ) removeFiles . append ( os . path . join ( self . __path , relaPath , self . __fileInfo % basename ) ) for fpath in removeFiles : if os . path . isfile ( fpath ) : try : os . remove ( fpath ) except Exception as err : errors . append ( "Unable to clean file '%s' (%s)" % ( fpath , str ( err ) ) ) for dpath in removeDirs : if os . path . isdir ( dpath ) : if keepNoneEmptyDirectory or not len ( os . listdir ( dpath ) ) : try : shutil . rmtree ( dpath ) except Exception as err : errors . append ( "Unable to clean directory '%s' (%s)" % ( fpath , str ( err ) ) ) return len ( errors ) == 0 , errors
7078	def tic_conesearch ( ra , decl , radius_arcmin = 5.0 , apiversion = 'v0' , forcefetch = False , cachedir = '~/.astrobase/mast-cache' , verbose = True , timeout = 10.0 , refresh = 5.0 , maxtimeout = 90.0 , maxtries = 3 , jitter = 5.0 , raiseonfail = False ) : params = { 'ra' : ra , 'dec' : decl , 'radius' : radius_arcmin / 60.0 } service = 'Mast.Catalogs.Tic.Cone' return mast_query ( service , params , jitter = jitter , apiversion = apiversion , forcefetch = forcefetch , cachedir = cachedir , verbose = verbose , timeout = timeout , refresh = refresh , maxtimeout = maxtimeout , maxtries = maxtries , raiseonfail = raiseonfail )
9008	def get_index_in_row ( self ) : expected_index = self . _cached_index_in_row instructions = self . _row . instructions if expected_index is not None and 0 <= expected_index < len ( instructions ) and instructions [ expected_index ] is self : return expected_index for index , instruction_in_row in enumerate ( instructions ) : if instruction_in_row is self : self . _cached_index_in_row = index return index return None
13343	def _broadcast_shape ( * args ) : shapes = [ a . shape if hasattr ( type ( a ) , '__array_interface__' ) else ( ) for a in args ] ndim = max ( len ( sh ) for sh in shapes ) for i , sh in enumerate ( shapes ) : if len ( sh ) < ndim : shapes [ i ] = ( 1 , ) * ( ndim - len ( sh ) ) + sh return tuple ( max ( sh [ ax ] for sh in shapes ) for ax in range ( ndim ) )
13096	def watch ( self ) : wm = pyinotify . WatchManager ( ) self . notifier = pyinotify . Notifier ( wm , default_proc_fun = self . callback ) wm . add_watch ( self . directory , pyinotify . ALL_EVENTS ) try : self . notifier . loop ( ) except ( KeyboardInterrupt , AttributeError ) : print_notification ( "Stopping" ) finally : self . notifier . stop ( ) self . terminate_processes ( )
10334	def build_expand_node_neighborhood_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , BELGraph , str ] , None ] : @ uni_in_place_transformation def expand_node_neighborhood_by_hash ( universe : BELGraph , graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) return expand_node_neighborhood ( universe , graph , node ) return expand_node_neighborhood_by_hash
13339	def transpose ( a , axes = None ) : if isinstance ( a , np . ndarray ) : return np . transpose ( a , axes ) elif isinstance ( a , RemoteArray ) : return a . transpose ( * axes ) elif isinstance ( a , Remote ) : return _remote_to_array ( a ) . transpose ( * axes ) elif isinstance ( a , DistArray ) : if axes is None : axes = range ( a . ndim - 1 , - 1 , - 1 ) axes = list ( axes ) if len ( set ( axes ) ) < len ( axes ) : raise ValueError ( "repeated axis in transpose" ) if sorted ( axes ) != list ( range ( a . ndim ) ) : raise ValueError ( "axes don't match array" ) distaxis = a . _distaxis new_distaxis = axes . index ( distaxis ) new_subarrays = [ ra . transpose ( * axes ) for ra in a . _subarrays ] return DistArray ( new_subarrays , new_distaxis ) else : return np . transpose ( a , axes )
186	def copy ( self , coords = None , label = None ) : return LineString ( coords = self . coords if coords is None else coords , label = self . label if label is None else label )
11568	def open ( self , verbose ) : if verbose : print ( '\nOpening Arduino Serial port %s ' % self . port_id ) try : self . arduino . close ( ) time . sleep ( 1 ) self . arduino . open ( ) time . sleep ( 1 ) return self . arduino except Exception : raise
8624	def add_user_jobs ( session , job_ids ) : jobs_data = { 'jobs[]' : job_ids } response = make_post_request ( session , 'self/jobs' , json_data = jobs_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : raise UserJobsNotAddedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
6828	def fetch ( self , path , use_sudo = False , user = None , remote = None ) : if path is None : raise ValueError ( "Path to the working copy is needed to fetch from a remote repository." ) if remote is not None : cmd = 'git fetch %s' % remote else : cmd = 'git fetch' with cd ( path ) : if use_sudo and user is None : run_as_root ( cmd ) elif use_sudo : sudo ( cmd , user = user ) else : run ( cmd )
10805	def validate ( cls , policy ) : return policy in [ cls . OPEN , cls . APPROVAL , cls . CLOSED ]
9316	def _to_json ( resp ) : try : return resp . json ( ) except ValueError as e : six . raise_from ( InvalidJSONError ( "Invalid JSON was received from " + resp . request . url ) , e )
10413	def node_inclusion_filter_builder ( nodes : Iterable [ BaseEntity ] ) -> NodePredicate : node_set = set ( nodes ) def inclusion_filter ( _ : BELGraph , node : BaseEntity ) -> bool : return node in node_set return inclusion_filter
13227	def decorator ( decorator_func ) : assert callable ( decorator_func ) , type ( decorator_func ) def _decorator ( func = None , ** kwargs ) : assert func is None or callable ( func ) , type ( func ) if func : return decorator_func ( func , ** kwargs ) else : def _decorator_helper ( func ) : return decorator_func ( func , ** kwargs ) return _decorator_helper return _decorator
3085	def service_account_email ( self ) : if self . _service_account_email is None : self . _service_account_email = ( app_identity . get_service_account_name ( ) ) return self . _service_account_email
9506	def contains ( self , i ) : return self . start <= i . start and i . end <= self . end
9959	def restore_python ( self ) : orig = self . orig_settings sys . setrecursionlimit ( orig [ "sys.recursionlimit" ] ) if "sys.tracebacklimit" in orig : sys . tracebacklimit = orig [ "sys.tracebacklimit" ] else : if hasattr ( sys , "tracebacklimit" ) : del sys . tracebacklimit if "showwarning" in orig : warnings . showwarning = orig [ "showwarning" ] orig . clear ( ) threading . stack_size ( )
1509	def stop_cluster ( cl_args ) : Log . info ( "Terminating cluster..." ) roles = read_and_parse_roles ( cl_args ) masters = roles [ Role . MASTERS ] slaves = roles [ Role . SLAVES ] dist_nodes = masters . union ( slaves ) if masters : try : single_master = list ( masters ) [ 0 ] jobs = get_jobs ( cl_args , single_master ) for job in jobs : job_id = job [ "ID" ] Log . info ( "Terminating job %s" % job_id ) delete_job ( cl_args , job_id , single_master ) except : Log . debug ( "Error stopping jobs" ) Log . debug ( sys . exc_info ( ) [ 0 ] ) for node in dist_nodes : Log . info ( "Terminating processes on %s" % node ) if not is_self ( node ) : cmd = "ps aux | grep heron-nomad | awk '{print \$2}' " "| xargs kill" cmd = ssh_remote_execute ( cmd , node , cl_args ) else : cmd = "ps aux | grep heron-nomad | awk '{print $2}' " "| xargs kill" Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) ) Log . info ( "Cleaning up directories on %s" % node ) cmd = "rm -rf /tmp/slave ; rm -rf /tmp/master" if not is_self ( node ) : cmd = ssh_remote_execute ( cmd , node , cl_args ) Log . debug ( cmd ) pid = subprocess . Popen ( cmd , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) return_code = pid . wait ( ) output = pid . communicate ( ) Log . debug ( "return code: %s output: %s" % ( return_code , output ) )
7385	def group_theta ( self , group ) : for i , g in enumerate ( self . nodes . keys ( ) ) : if g == group : break return i * self . major_angle
7795	def unregister_fetcher ( self , object_class ) : self . _lock . acquire ( ) try : cache = self . _caches . get ( object_class ) if not cache : return cache . set_fetcher ( None ) finally : self . _lock . release ( )
6536	def output_error ( msg ) : click . echo ( click . style ( msg , fg = 'red' ) , err = True )
13174	def next ( self , name = None ) : if self . parent is None or self . index is None : return None for idx in xrange ( self . index + 1 , len ( self . parent ) ) : if name is None or self . parent [ idx ] . tagname == name : return self . parent [ idx ]
4577	def get_server ( self , key , ** kwds ) : kwds = dict ( self . kwds , ** kwds ) server = self . servers . get ( key ) if server : server . check_keywords ( self . constructor , kwds ) else : server = _CachedServer ( self . constructor , key , kwds ) self . servers [ key ] = server return server
2731	def create ( self ) : data = { "name" : self . name , "ip_address" : self . ip_address , } domain = self . get_data ( "domains" , type = POST , params = data ) return domain
2539	def set_pkg_excl_file ( self , doc , filename ) : self . assert_package_exists ( ) doc . package . add_exc_file ( filename )
10676	def Cp ( compound_string , T , mass = 1.0 ) : formula , phase = _split_compound_string_ ( compound_string ) TK = T + 273.15 compound = compounds [ formula ] result = compound . Cp ( phase , TK ) return _finalise_result_ ( compound , result , mass )
9737	def get_3d_markers_no_label ( self , component_info = None , data = None , component_position = None ) : return self . _get_3d_markers ( RT3DMarkerPositionNoLabel , component_info , data , component_position )
11163	def mtime ( self ) : try : return self . _stat . st_mtime except : self . _stat = self . stat ( ) return self . mtime
12345	def stitch ( self , folder = None ) : debug ( 'stitching ' + self . __str__ ( ) ) if not folder : folder = self . path macros = [ ] files = [ ] for well in self . wells : f , m = stitch_macro ( well , folder ) macros . extend ( m ) files . extend ( f ) chopped_arguments = zip ( chop ( macros , _pools ) , chop ( files , _pools ) ) chopped_filenames = Parallel ( n_jobs = _pools ) ( delayed ( fijibin . macro . run ) ( macro = arg [ 0 ] , output_files = arg [ 1 ] ) for arg in chopped_arguments ) return [ f for list_ in chopped_filenames for f in list_ ]
30	def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables )
11405	def record_drop_duplicate_fields ( record ) : out = { } position = 0 tags = sorted ( record . keys ( ) ) for tag in tags : fields = record [ tag ] out [ tag ] = [ ] current_fields = set ( ) for full_field in fields : field = ( tuple ( full_field [ 0 ] ) , ) + full_field [ 1 : 4 ] if field not in current_fields : current_fields . add ( field ) position += 1 out [ tag ] . append ( full_field [ : 4 ] + ( position , ) ) return out
2393	def gen_preds ( clf , arr ) : if ( hasattr ( clf , "predict_proba" ) ) : ret = clf . predict ( arr ) else : ret = clf . predict ( arr ) return ret
6237	def draw_buffers ( self , near , far ) : self . ctx . disable ( moderngl . DEPTH_TEST ) helper . draw ( self . gbuffer . color_attachments [ 0 ] , pos = ( 0.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . gbuffer . color_attachments [ 1 ] , pos = ( 0.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw_depth ( self . gbuffer . depth_attachment , near , far , pos = ( 1.0 , 0.0 ) , scale = ( 0.25 , 0.25 ) ) helper . draw ( self . lightbuffer . color_attachments [ 0 ] , pos = ( 1.5 , 0.0 ) , scale = ( 0.25 , 0.25 ) )
5468	def get_event_of_type ( op , event_type ) : events = get_events ( op ) if not events : return None return [ e for e in events if e . get ( 'details' , { } ) . get ( '@type' ) == event_type ]
3395	def remove_genes ( cobra_model , gene_list , remove_reactions = True ) : gene_set = { cobra_model . genes . get_by_id ( str ( i ) ) for i in gene_list } gene_id_set = { i . id for i in gene_set } remover = _GeneRemover ( gene_id_set ) ast_rules = get_compiled_gene_reaction_rules ( cobra_model ) target_reactions = [ ] for reaction , rule in iteritems ( ast_rules ) : if reaction . gene_reaction_rule is None or len ( reaction . gene_reaction_rule ) == 0 : continue if remove_reactions and not eval_gpr ( rule , gene_id_set ) : target_reactions . append ( reaction ) else : remover . visit ( rule ) new_rule = ast2str ( rule ) if new_rule != reaction . gene_reaction_rule : reaction . gene_reaction_rule = new_rule for gene in gene_set : cobra_model . genes . remove ( gene ) associated_groups = cobra_model . get_associated_groups ( gene ) for group in associated_groups : group . remove_members ( gene ) cobra_model . remove_reactions ( target_reactions )
6836	def base_boxes ( self ) : return sorted ( list ( set ( [ name for name , provider in self . _box_list ( ) ] ) ) )
8661	def from_config ( cls , cfg , default_fg = DEFAULT_FG_16 , default_bg = DEFAULT_BG_16 , default_fg_hi = DEFAULT_FG_256 , default_bg_hi = DEFAULT_BG_256 , max_colors = 2 ** 24 ) : e = PaletteEntry ( mono = default_fg , foreground = default_fg , background = default_bg , foreground_high = default_fg_hi , background_high = default_bg_hi ) if isinstance ( cfg , str ) : e . foreground_high = cfg if e . allowed ( cfg , 16 ) : e . foreground = cfg else : rgb = AttrSpec ( fg = cfg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( cfg , dict ) : bg = cfg . get ( "bg" , None ) if isinstance ( bg , str ) : e . background_high = bg if e . allowed ( bg , 16 ) : e . background = bg else : rgb = AttrSpec ( fg = bg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) elif isinstance ( bg , dict ) : e . background_high = bg . get ( "hi" , default_bg_hi ) if "lo" in bg : if e . allowed ( bg [ "lo" ] , 16 ) : e . background = bg [ "lo" ] else : rgb = AttrSpec ( fg = bg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . background = nearest_basic_color ( rgb ) fg = cfg . get ( "fg" , cfg ) if isinstance ( fg , str ) : e . foreground_high = fg if e . allowed ( fg , 16 ) : e . foreground = fg else : rgb = AttrSpec ( fg = fg , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) elif isinstance ( fg , dict ) : e . foreground_high = fg . get ( "hi" , default_fg_hi ) if "lo" in fg : if e . allowed ( fg [ "lo" ] , 16 ) : e . foreground = fg [ "lo" ] else : rgb = AttrSpec ( fg = fg [ "lo" ] , bg = "" , colors = max_colors ) . get_rgb_values ( ) [ 0 : 3 ] e . foreground = nearest_basic_color ( rgb ) return e
8313	def draw_table ( table , x , y , w , padding = 5 ) : try : from web import _ctx except : pass f = _ctx . fill ( ) _ctx . stroke ( f ) h = _ctx . textheight ( " " ) + padding * 2 row_y = y if table . title != "" : _ctx . fill ( f ) _ctx . rect ( x , row_y , w , h ) _ctx . fill ( 1 ) _ctx . text ( table . title , x + padding , row_y + _ctx . fontsize ( ) + padding ) row_y += h rowspans = [ 1 for i in range ( 10 ) ] previous_cell_w = 0 for row in table : cell_x = x cell_w = 1.0 * w cell_w -= previous_cell_w * len ( [ n for n in rowspans if n > 1 ] ) cell_w /= len ( row ) cell_h = 0 for cell in row : this_h = _ctx . textheight ( cell , width = cell_w - padding * 2 ) + padding * 2 cell_h = max ( cell_h , this_h ) i = 0 for cell in row : if rowspans [ i ] > 1 : rowspans [ i ] -= 1 cell_x += previous_cell_w i += 1 m = re . search ( "rowspan=\"(.*?)\"" , cell . properties ) if m : rowspan = int ( m . group ( 1 ) ) rowspans [ i ] = rowspan else : rowspan = 1 _ctx . fill ( f ) _ctx . text ( cell , cell_x + padding , row_y + _ctx . fontsize ( ) + padding , cell_w - padding * 2 ) _ctx . line ( cell_x , row_y , cell_x + cell_w , row_y ) if cell_x > x : _ctx . nofill ( ) _ctx . line ( cell_x , row_y , cell_x , row_y + cell_h ) cell_x += cell_w i += 1 row_y += cell_h previous_cell_w = cell_w _ctx . nofill ( ) _ctx . rect ( x , y , w , row_y - y )
4070	def upload ( self ) : result = { "success" : [ ] , "failure" : [ ] , "unchanged" : [ ] } self . _create_prelim ( ) for item in self . payload : if "key" not in item : result [ "failure" ] . append ( item ) continue attach = str ( self . basedir . joinpath ( item [ "filename" ] ) ) authdata = self . _get_auth ( attach , item [ "key" ] , md5 = item . get ( "md5" , None ) ) if authdata . get ( "exists" ) : result [ "unchanged" ] . append ( item ) continue self . _upload_file ( authdata , attach , item [ "key" ] ) result [ "success" ] . append ( item ) return result
9369	def legal_ogrn ( ) : ogrn = "" . join ( map ( str , [ random . randint ( 1 , 9 ) for _ in range ( 12 ) ] ) ) ogrn += str ( ( int ( ogrn ) % 11 % 10 ) ) return ogrn
10127	def draw ( self ) : if self . enabled : self . _vertex_list . colors = self . _gl_colors self . _vertex_list . vertices = self . _gl_vertices self . _vertex_list . draw ( pyglet . gl . GL_TRIANGLES )
501	def _recomputeRecordFromKNN ( self , record ) : inputs = { "categoryIn" : [ None ] , "bottomUpIn" : self . _getStateAnomalyVector ( record ) , } outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , "bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , "categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None
13761	def _handle_response ( self , response ) : if not str ( response . status_code ) . startswith ( '2' ) : raise get_api_error ( response ) return response
7775	def rfc2425encode ( name , value , parameters = None , charset = "utf-8" ) : if not parameters : parameters = { } if type ( value ) is unicode : value = value . replace ( u"\r\n" , u"\\n" ) value = value . replace ( u"\n" , u"\\n" ) value = value . replace ( u"\r" , u"\\n" ) value = value . encode ( charset , "replace" ) elif type ( value ) is not str : raise TypeError ( "Bad type for rfc2425 value" ) elif not valid_string_re . match ( value ) : parameters [ "encoding" ] = "b" value = binascii . b2a_base64 ( value ) ret = str ( name ) . lower ( ) for k , v in parameters . items ( ) : ret += ";%s=%s" % ( str ( k ) , str ( v ) ) ret += ":" while ( len ( value ) > 70 ) : ret += value [ : 70 ] + "\r\n " value = value [ 70 : ] ret += value + "\r\n" return ret
9957	def setup_ipython ( self ) : if self . is_ipysetup : return from ipykernel . kernelapp import IPKernelApp self . shell = IPKernelApp . instance ( ) . shell if not self . shell and is_ipython ( ) : self . shell = get_ipython ( ) if self . shell : shell_class = type ( self . shell ) shell_class . default_showtraceback = shell_class . showtraceback shell_class . showtraceback = custom_showtraceback self . is_ipysetup = True else : raise RuntimeError ( "IPython shell not found." )
3985	def _dusty_hosts_config ( hosts_specs ) : rules = '' . join ( [ '{} {}\n' . format ( spec [ 'forwarded_ip' ] , spec [ 'host_address' ] ) for spec in hosts_specs ] ) return config_file . create_config_section ( rules )
3572	def peripheral_didDiscoverServices_ ( self , peripheral , services ) : logger . debug ( 'peripheral_didDiscoverServices called' ) for service in peripheral . services ( ) : if service_list ( ) . get ( service ) is None : service_list ( ) . add ( service , CoreBluetoothGattService ( service ) ) peripheral . discoverCharacteristics_forService_ ( None , service )
6929	def trapezoid_transit_func ( transitparams , times , mags , errs , get_ntransitpoints = False ) : ( transitperiod , transitepoch , transitdepth , transitduration , ingressduration ) = transitparams iphase = ( times - transitepoch ) / transitperiod iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] zerolevel = np . median ( pmags ) modelmags = np . full_like ( phase , zerolevel ) halftransitduration = transitduration / 2.0 bottomlevel = zerolevel - transitdepth slope = transitdepth / ingressduration firstcontact = 1.0 - halftransitduration secondcontact = firstcontact + ingressduration thirdcontact = halftransitduration - ingressduration fourthcontact = halftransitduration ingressind = ( phase > firstcontact ) & ( phase < secondcontact ) bottomind = ( phase > secondcontact ) | ( phase < thirdcontact ) egressind = ( phase > thirdcontact ) & ( phase < fourthcontact ) in_transit_points = ingressind | bottomind | egressind n_transit_points = np . sum ( in_transit_points ) modelmags [ ingressind ] = zerolevel - slope * ( phase [ ingressind ] - firstcontact ) modelmags [ bottomind ] = bottomlevel modelmags [ egressind ] = bottomlevel + slope * ( phase [ egressind ] - thirdcontact ) if get_ntransitpoints : return modelmags , phase , ptimes , pmags , perrs , n_transit_points else : return modelmags , phase , ptimes , pmags , perrs
12443	def require_http_allowed_method ( cls , request ) : allowed = cls . meta . http_allowed_methods if request . method not in allowed : raise http . exceptions . MethodNotAllowed ( allowed )
12333	def wait ( self , cmd , raise_on_error = True ) : _ , stdout , stderr = self . exec_command ( cmd ) stdout . channel . recv_exit_status ( ) output = stdout . read ( ) if self . interactive : print ( output ) errors = stderr . read ( ) if self . interactive : print ( errors ) if errors and raise_on_error : raise ValueError ( errors ) return output
9858	def create_url ( self , path , params = { } , opts = { } ) : if opts : warnings . warn ( '`opts` has been deprecated. Use `params` instead.' , DeprecationWarning , stacklevel = 2 ) params = params or opts if self . _shard_strategy == SHARD_STRATEGY_CRC : crc = zlib . crc32 ( path . encode ( 'utf-8' ) ) & 0xffffffff index = crc % len ( self . _domains ) domain = self . _domains [ index ] elif self . _shard_strategy == SHARD_STRATEGY_CYCLE : domain = self . _domains [ self . _shard_next_index ] self . _shard_next_index = ( self . _shard_next_index + 1 ) % len ( self . _domains ) else : domain = self . _domains [ 0 ] scheme = "https" if self . _use_https else "http" url_obj = UrlHelper ( domain , path , scheme , sign_key = self . _sign_key , include_library_param = self . _include_library_param , params = params ) return str ( url_obj )
5403	def _get_localization_env ( self , inputs , user_project ) : non_empty_inputs = [ var for var in inputs if var . value ] env = { 'INPUT_COUNT' : str ( len ( non_empty_inputs ) ) } for idx , var in enumerate ( non_empty_inputs ) : env [ 'INPUT_{}' . format ( idx ) ] = var . name env [ 'INPUT_RECURSIVE_{}' . format ( idx ) ] = str ( int ( var . recursive ) ) env [ 'INPUT_SRC_{}' . format ( idx ) ] = var . value dst = os . path . join ( providers_util . DATA_MOUNT_POINT , var . docker_path ) path , filename = os . path . split ( dst ) if '*' in filename : dst = '{}/' . format ( path ) env [ 'INPUT_DST_{}' . format ( idx ) ] = dst env [ 'USER_PROJECT' ] = user_project return env
4036	def ss_wrap ( func ) : def wrapper ( self , * args , ** kwargs ) : if not self . savedsearch : self . savedsearch = SavedSearch ( self ) return func ( self , * args , ** kwargs ) return wrapper
11336	def get_records ( self , url ) : page = urllib2 . urlopen ( url ) pages = [ BeautifulSoup ( page ) ] numpag = pages [ 0 ] . body . findAll ( 'span' , attrs = { 'class' : 'number-of-pages' } ) if len ( numpag ) > 0 : if re . search ( '^\d+$' , numpag [ 0 ] . string ) : for i in range ( int ( numpag [ 0 ] . string ) - 1 ) : page = urllib2 . urlopen ( '%s/page/%i' % ( url , i + 2 ) ) pages . append ( BeautifulSoup ( page ) ) else : print ( "number of pages %s not an integer" % ( numpag [ 0 ] . string ) ) impl = getDOMImplementation ( ) doc = impl . createDocument ( None , "collection" , None ) links = [ ] for page in pages : links += page . body . findAll ( 'p' , attrs = { 'class' : 'title' } ) links += page . body . findAll ( 'h3' , attrs = { 'class' : 'title' } ) for link in links : record = self . _get_record ( link ) doc . firstChild . appendChild ( record ) return doc . toprettyxml ( )
4392	def adsSyncWriteControlReqEx ( port , address , ads_state , device_state , data , plc_data_type ) : sync_write_control_request = _adsDLL . AdsSyncWriteControlReqEx ams_address_pointer = ctypes . pointer ( address . amsAddrStruct ( ) ) ads_state_c = ctypes . c_ulong ( ads_state ) device_state_c = ctypes . c_ulong ( device_state ) if plc_data_type == PLCTYPE_STRING : data = ctypes . c_char_p ( data . encode ( "utf-8" ) ) data_pointer = data data_length = len ( data_pointer . value ) + 1 else : data = plc_data_type ( data ) data_pointer = ctypes . pointer ( data ) data_length = ctypes . sizeof ( data ) error_code = sync_write_control_request ( port , ams_address_pointer , ads_state_c , device_state_c , data_length , data_pointer , ) if error_code : raise ADSError ( error_code )
8268	def color ( self , clr = None , d = 0.035 ) : if clr != None and not isinstance ( clr , Color ) : clr = color ( clr ) if clr != None and not self . grayscale : if clr . is_black : return self . black . color ( clr , d ) if clr . is_white : return self . white . color ( clr , d ) if clr . is_grey : return choice ( ( self . black . color ( clr , d ) , self . white . color ( clr , d ) ) ) h , s , b , a = self . h , self . s , self . b , self . a if clr != None : h , a = clr . h + d * ( random ( ) * 2 - 1 ) , clr . a hsba = [ ] for v in [ h , s , b , a ] : if isinstance ( v , _list ) : min , max = choice ( v ) elif isinstance ( v , tuple ) : min , max = v else : min , max = v , v hsba . append ( min + ( max - min ) * random ( ) ) h , s , b , a = hsba return color ( h , s , b , a , mode = "hsb" )
10883	def patch_docs ( subclass , superclass ) : funcs0 = inspect . getmembers ( subclass , predicate = inspect . ismethod ) funcs1 = inspect . getmembers ( superclass , predicate = inspect . ismethod ) funcs1 = [ f [ 0 ] for f in funcs1 ] for name , func in funcs0 : if name . startswith ( '_' ) : continue if name not in funcs1 : continue if func . __doc__ is None : func = getattr ( subclass , name ) func . __func__ . __doc__ = getattr ( superclass , name ) . __func__ . __doc__
13722	def log ( self , url = None , credentials = None , do_verify_certificate = True ) : if url is None : url = self . url if re . match ( "file://" , url ) : self . log_file ( url ) elif re . match ( "https://" , url ) or re . match ( "http://" , url ) : self . log_post ( url , credentials , do_verify_certificate ) else : self . log_stdout ( )
3439	def _escape_str_id ( id_str ) : for c in ( "'" , '"' ) : if id_str . startswith ( c ) and id_str . endswith ( c ) and id_str . count ( c ) == 2 : id_str = id_str . strip ( c ) for char , escaped_char in _renames : id_str = id_str . replace ( char , escaped_char ) return id_str
4567	def _write ( self , filename , frames , fps , loop = 0 , palette = 256 ) : from PIL import Image images = [ ] for f in frames : data = open ( f , 'rb' ) . read ( ) images . append ( Image . open ( io . BytesIO ( data ) ) ) duration = round ( 1 / fps , 2 ) im = images . pop ( 0 ) im . save ( filename , save_all = True , append_images = images , duration = duration , loop = loop , palette = palette )
7062	def sqs_put_item ( queue_url , item , delay_seconds = 0 , client = None , raiseonfail = False ) : if not client : client = boto3 . client ( 'sqs' ) try : json_msg = json . dumps ( item ) resp = client . send_message ( QueueUrl = queue_url , MessageBody = json_msg , DelaySeconds = delay_seconds , ) if not resp : LOGERROR ( 'could not send item to queue: %s' % queue_url ) return None else : return resp except Exception as e : LOGEXCEPTION ( 'could not send item to queue: %s' % queue_url ) if raiseonfail : raise return None
4139	def save_thumbnail ( image_path , base_image_name , gallery_conf ) : first_image_file = image_path . format ( 1 ) thumb_dir = os . path . join ( os . path . dirname ( first_image_file ) , 'thumb' ) if not os . path . exists ( thumb_dir ) : os . makedirs ( thumb_dir ) thumb_file = os . path . join ( thumb_dir , 'sphx_glr_%s_thumb.png' % base_image_name ) if os . path . exists ( first_image_file ) : scale_image ( first_image_file , thumb_file , 400 , 280 ) elif not os . path . exists ( thumb_file ) : default_thumb_file = os . path . join ( glr_path_static ( ) , 'no_image.png' ) default_thumb_file = gallery_conf . get ( "default_thumb_file" , default_thumb_file ) scale_image ( default_thumb_file , thumb_file , 200 , 140 )
7476	def inserted_indels ( indels , ocatg ) : newcatg = np . zeros ( ocatg . shape , dtype = np . uint32 ) for iloc in xrange ( ocatg . shape [ 0 ] ) : indidx = np . where ( indels [ iloc , : ] ) [ 0 ] if np . any ( indidx ) : allrows = np . arange ( ocatg . shape [ 1 ] ) mask = np . ones ( allrows . shape [ 0 ] , dtype = np . bool_ ) for idx in indidx : mask [ idx ] = False not_idx = allrows [ mask == 1 ] newcatg [ iloc ] [ not_idx ] = ocatg [ iloc , : not_idx . shape [ 0 ] ] else : newcatg [ iloc ] = ocatg [ iloc ] return newcatg
11316	def update_title_to_proceeding ( self ) : titles = record_get_field_instances ( self . record , tag = "245" ) for title in titles : subs = field_get_subfields ( title ) new_subs = [ ] if "a" in subs : new_subs . append ( ( "a" , subs [ 'a' ] [ 0 ] ) ) if "b" in subs : new_subs . append ( ( "c" , subs [ 'b' ] [ 0 ] ) ) record_add_field ( self . record , tag = "111" , subfields = new_subs ) record_delete_fields ( self . record , tag = "245" ) record_delete_fields ( self . record , tag = "246" )
955	def title ( s = None , additional = '' , stream = sys . stdout ) : if s is None : callable_name , file_name , class_name = getCallerInfo ( 2 ) s = callable_name if class_name is not None : s = class_name + '.' + callable_name lines = ( s + additional ) . split ( '\n' ) length = max ( len ( line ) for line in lines ) print >> stream , '-' * length print >> stream , s + additional print >> stream , '-' * length
10452	def waittillguiexist ( self , window_name , object_name = '' , guiTimeOut = 30 , state = '' ) : timeout = 0 while timeout < guiTimeOut : if self . guiexist ( window_name , object_name ) : return 1 time . sleep ( 1 ) timeout += 1 return 0
1813	def SETNBE ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) )
12816	def _send_to_consumer ( self , block ) : self . _consumer . write ( block ) self . _sent += len ( block ) if self . _callback : self . _callback ( self . _sent , self . length )
259	def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )
104	def pad_to_aspect_ratio ( arr , aspect_ratio , mode = "constant" , cval = 0 , return_pad_amounts = False ) : pad_top , pad_right , pad_bottom , pad_left = compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) arr_padded = pad ( arr , top = pad_top , right = pad_right , bottom = pad_bottom , left = pad_left , mode = mode , cval = cval ) if return_pad_amounts : return arr_padded , ( pad_top , pad_right , pad_bottom , pad_left ) else : return arr_padded
1393	def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : topologies = list ( filter ( lambda t : t . name == topologyName and t . cluster == cluster and ( not role or t . execution_state . role == role ) and t . environ == environ , self . topologies ) ) if not topologies or len ( topologies ) > 1 : if role is not None : raise Exception ( "Topology not found for {0}, {1}, {2}, {3}" . format ( cluster , role , environ , topologyName ) ) else : raise Exception ( "Topology not found for {0}, {1}, {2}" . format ( cluster , environ , topologyName ) ) return topologies [ 0 ]
257	def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )
1881	def constrain ( self , constraint ) : constraint = self . migrate_expression ( constraint ) self . _constraints . add ( constraint )
4264	def build ( source , destination , debug , verbose , force , config , theme , title , ncpu ) : level = ( ( debug and logging . DEBUG ) or ( verbose and logging . INFO ) or logging . WARNING ) init_logging ( __name__ , level = level ) logger = logging . getLogger ( __name__ ) if not os . path . isfile ( config ) : logger . error ( "Settings file not found: %s" , config ) sys . exit ( 1 ) start_time = time . time ( ) settings = read_settings ( config ) for key in ( 'source' , 'destination' , 'theme' ) : arg = locals ( ) [ key ] if arg is not None : settings [ key ] = os . path . abspath ( arg ) logger . info ( "%12s : %s" , key . capitalize ( ) , settings [ key ] ) if not settings [ 'source' ] or not os . path . isdir ( settings [ 'source' ] ) : logger . error ( "Input directory not found: %s" , settings [ 'source' ] ) sys . exit ( 1 ) relative_check = True try : relative_check = os . path . relpath ( settings [ 'destination' ] , settings [ 'source' ] ) . startswith ( '..' ) except ValueError : pass if not relative_check : logger . error ( "Output directory should be outside of the input " "directory." ) sys . exit ( 1 ) if title : settings [ 'title' ] = title locale . setlocale ( locale . LC_ALL , settings [ 'locale' ] ) init_plugins ( settings ) gal = Gallery ( settings , ncpu = ncpu ) gal . build ( force = force ) for src , dst in settings [ 'files_to_copy' ] : src = os . path . join ( settings [ 'source' ] , src ) dst = os . path . join ( settings [ 'destination' ] , dst ) logger . debug ( 'Copy %s to %s' , src , dst ) copy ( src , dst , symlink = settings [ 'orig_link' ] , rellink = settings [ 'rel_link' ] ) stats = gal . stats def format_stats ( _type ) : opt = [ "{} {}" . format ( stats [ _type + '_' + subtype ] , subtype ) for subtype in ( 'skipped' , 'failed' ) if stats [ _type + '_' + subtype ] > 0 ] opt = ' ({})' . format ( ', ' . join ( opt ) ) if opt else '' return '{} {}s{}' . format ( stats [ _type ] , _type , opt ) print ( 'Done.\nProcessed {} and {} in {:.2f} seconds.' . format ( format_stats ( 'image' ) , format_stats ( 'video' ) , time . time ( ) - start_time ) )
7782	def update_state ( self ) : self . _lock . acquire ( ) try : now = datetime . utcnow ( ) if self . state == 'new' : self . state = 'fresh' if self . state == 'fresh' : if now > self . freshness_time : self . state = 'old' if self . state == 'old' : if now > self . expire_time : self . state = 'stale' if self . state == 'stale' : if now > self . purge_time : self . state = 'purged' self . state_value = _state_values [ self . state ] return self . state finally : self . _lock . release ( )
6391	def encode ( self , word , max_length = 8 ) : word = '' . join ( char for char in word . lower ( ) if char in self . _initial_phones ) if not word : word = '÷' values = [ self . _initial_phones [ word [ 0 ] ] ] values += [ self . _trailing_phones [ char ] for char in word [ 1 : ] ] shifted_values = [ _ >> 1 for _ in values ] condensed_values = [ values [ 0 ] ] for n in range ( 1 , len ( shifted_values ) ) : if shifted_values [ n ] != shifted_values [ n - 1 ] : condensed_values . append ( values [ n ] ) values = ( [ condensed_values [ 0 ] ] + [ 0 ] * max ( 0 , max_length - len ( condensed_values ) ) + condensed_values [ 1 : max_length ] ) hash_value = 0 for val in values : hash_value = ( hash_value << 8 ) | val return hash_value
1422	def loads ( string ) : f = StringIO . StringIO ( string ) marshaller = JavaObjectUnmarshaller ( f ) marshaller . add_transformer ( DefaultObjectTransformer ( ) ) return marshaller . readObject ( )
11845	def list_things_at ( self , location , tclass = Thing ) : "Return all things exactly at a given location." return [ thing for thing in self . things if thing . location == location and isinstance ( thing , tclass ) ]
6127	def contained_in ( filename , directory ) : filename = os . path . normcase ( os . path . abspath ( filename ) ) directory = os . path . normcase ( os . path . abspath ( directory ) ) return os . path . commonprefix ( [ filename , directory ] ) == directory
7222	def get ( self , recipe_id ) : self . logger . debug ( 'Retrieving recipe by id: ' + recipe_id ) url = '%(base_url)s/recipe/%(recipe_id)s' % { 'base_url' : self . base_url , 'recipe_id' : recipe_id } r = self . gbdx_connection . get ( url ) r . raise_for_status ( ) return r . json ( )
5728	def _get_responses_unix ( self , timeout_sec ) : timeout_time_sec = time . time ( ) + timeout_sec responses = [ ] while True : select_timeout = timeout_time_sec - time . time ( ) if select_timeout <= 0 : select_timeout = 0 events , _ , _ = select . select ( self . read_list , [ ] , [ ] , select_timeout ) responses_list = None try : for fileno in events : if fileno == self . stdout_fileno : self . gdb_process . stdout . flush ( ) raw_output = self . gdb_process . stdout . read ( ) stream = "stdout" elif fileno == self . stderr_fileno : self . gdb_process . stderr . flush ( ) raw_output = self . gdb_process . stderr . read ( ) stream = "stderr" else : raise ValueError ( "Developer error. Got unexpected file number %d" % fileno ) responses_list = self . _get_responses_list ( raw_output , stream ) responses += responses_list except IOError : pass if timeout_sec == 0 : break elif responses_list and self . _allow_overwrite_timeout_times : timeout_time_sec = min ( time . time ( ) + self . time_to_check_for_additional_output_sec , timeout_time_sec , ) elif time . time ( ) > timeout_time_sec : break return responses
5558	def _flatten_tree ( tree , old_path = None ) : flat_tree = [ ] for key , value in tree . items ( ) : new_path = "/" . join ( [ old_path , key ] ) if old_path else key if isinstance ( value , dict ) and "format" not in value : flat_tree . extend ( _flatten_tree ( value , old_path = new_path ) ) else : flat_tree . append ( ( new_path , value ) ) return flat_tree
11748	def _bundle_exists ( self , path ) : for attached_bundle in self . _attached_bundles : if path == attached_bundle . path : return True return False
5396	def _localize_inputs_command ( self , task_dir , inputs , user_project ) : commands = [ ] for i in inputs : if i . recursive or not i . value : continue source_file_path = i . uri local_file_path = task_dir + '/' + _DATA_SUBDIR + '/' + i . docker_path dest_file_path = self . _get_input_target_path ( local_file_path ) commands . append ( 'mkdir -p "%s"' % os . path . dirname ( local_file_path ) ) if i . file_provider in [ job_model . P_LOCAL , job_model . P_GCS ] : if user_project : command = 'gsutil -u %s -mq cp "%s" "%s"' % ( user_project , source_file_path , dest_file_path ) else : command = 'gsutil -mq cp "%s" "%s"' % ( source_file_path , dest_file_path ) commands . append ( command ) return '\n' . join ( commands )
5619	def execute ( mp , td_resampling = "nearest" , td_matching_method = "gdal" , td_matching_max_zoom = None , td_matching_precision = 8 , td_fallback_to_higher_zoom = False , clip_pixelbuffer = 0 , ** kwargs ) : if "clip" in mp . params [ "input" ] : clip_geom = mp . open ( "clip" ) . read ( ) if not clip_geom : logger . debug ( "no clip data over tile" ) return "empty" else : clip_geom = [ ] with mp . open ( "raster" , matching_method = td_matching_method , matching_max_zoom = td_matching_max_zoom , matching_precision = td_matching_precision , fallback_to_higher_zoom = td_fallback_to_higher_zoom , resampling = td_resampling ) as raster : raster_data = raster . read ( ) if raster . is_empty ( ) or raster_data [ 0 ] . mask . all ( ) : logger . debug ( "raster empty" ) return "empty" if clip_geom : clipped = mp . clip ( np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data ) , clip_geom , clip_buffer = clip_pixelbuffer , inverted = True ) return np . where ( clipped . mask , clipped , mp . params [ "output" ] . nodata ) else : return np . where ( raster_data [ 0 ] . mask , mp . params [ "output" ] . nodata , raster_data )
1789	def DIV ( cpu , src ) : size = src . size reg_name_h = { 8 : 'DL' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ size ] reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] dividend = Operators . CONCAT ( size * 2 , cpu . read_register ( reg_name_h ) , cpu . read_register ( reg_name_l ) ) divisor = Operators . ZEXTEND ( src . read ( ) , size * 2 ) if isinstance ( divisor , int ) and divisor == 0 : raise DivideByZeroError ( ) quotient = Operators . UDIV ( dividend , divisor ) MASK = ( 1 << size ) - 1 if isinstance ( quotient , int ) and quotient > MASK : raise DivideByZeroError ( ) remainder = Operators . UREM ( dividend , divisor ) cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , size ) ) cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , size ) )
7505	def _run_qmc ( self , boot ) : self . _tmp = os . path . join ( self . dirs , ".tmpwtre" ) cmd = [ ip . bins . qmc , "qrtt=" + self . files . qdump , "otre=" + self . _tmp ] proc = subprocess . Popen ( cmd , stderr = subprocess . STDOUT , stdout = subprocess . PIPE ) res = proc . communicate ( ) if proc . returncode : LOGGER . error ( res ) raise IPyradWarningExit ( res [ 1 ] ) with open ( self . _tmp ) as intree : tmp = ete3 . Tree ( intree . read ( ) . strip ( ) ) tmpwtre = self . _renamer ( tmp ) if boot : self . trees . boots = os . path . join ( self . dirs , self . name + ".boots" ) with open ( self . trees . boots , 'a' ) as outboot : outboot . write ( tmpwtre + "\n" ) else : self . trees . tree = os . path . join ( self . dirs , self . name + ".tree" ) with open ( self . trees . tree , 'w' ) as outtree : outtree . write ( tmpwtre ) self . _save ( )
8852	def on_new ( self ) : interpreter , pyserver , args = self . _get_backend_parameters ( ) self . setup_editor ( self . tabWidget . create_new_document ( extension = '.py' , interpreter = interpreter , server_script = pyserver , args = args ) ) self . actionRun . setDisabled ( True ) self . actionConfigure_run . setDisabled ( True )
2494	def package_verif_node ( self , package ) : verif_node = BNode ( ) type_triple = ( verif_node , RDF . type , self . spdx_namespace . PackageVerificationCode ) self . graph . add ( type_triple ) value_triple = ( verif_node , self . spdx_namespace . packageVerificationCodeValue , Literal ( package . verif_code ) ) self . graph . add ( value_triple ) excl_file_nodes = map ( lambda excl : Literal ( excl ) , package . verif_exc_files ) excl_predicate = self . spdx_namespace . packageVerificationCodeExcludedFile excl_file_triples = [ ( verif_node , excl_predicate , xcl_file ) for xcl_file in excl_file_nodes ] for trp in excl_file_triples : self . graph . add ( trp ) return verif_node
1106	def get_matching_blocks ( self ) : if self . matching_blocks is not None : return self . matching_blocks la , lb = len ( self . a ) , len ( self . b ) queue = [ ( 0 , la , 0 , lb ) ] matching_blocks = [ ] while queue : alo , ahi , blo , bhi = queue . pop ( ) i , j , k = x = self . find_longest_match ( alo , ahi , blo , bhi ) if k : matching_blocks . append ( x ) if alo < i and blo < j : queue . append ( ( alo , i , blo , j ) ) if i + k < ahi and j + k < bhi : queue . append ( ( i + k , ahi , j + k , bhi ) ) matching_blocks . sort ( ) i1 = j1 = k1 = 0 non_adjacent = [ ] for i2 , j2 , k2 in matching_blocks : if i1 + k1 == i2 and j1 + k1 == j2 : k1 += k2 else : if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) i1 , j1 , k1 = i2 , j2 , k2 if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) non_adjacent . append ( ( la , lb , 0 ) ) self . matching_blocks = map ( Match . _make , non_adjacent ) return self . matching_blocks
3700	def solubility_parameter ( T = 298.15 , Hvapm = None , Vml = None , CASRN = '' , AvailableMethods = False , Method = None ) : r def list_methods ( ) : methods = [ ] if T and Hvapm and Vml : methods . append ( DEFINITION ) methods . append ( NONE ) return methods if AvailableMethods : return list_methods ( ) if not Method : Method = list_methods ( ) [ 0 ] if Method == DEFINITION : if ( not Hvapm ) or ( not T ) or ( not Vml ) : delta = None else : if Hvapm < R * T or Vml < 0 : delta = None else : delta = ( ( Hvapm - R * T ) / Vml ) ** 0.5 elif Method == NONE : delta = None else : raise Exception ( 'Failure in in function' ) return delta
897	def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence
5935	def irecarray_to_py ( a ) : pytypes = [ pyify ( typestr ) for name , typestr in a . dtype . descr ] def convert_record ( r ) : return tuple ( [ converter ( value ) for converter , value in zip ( pytypes , r ) ] ) return ( convert_record ( r ) for r in a )
1738	def unify_string_literals ( js_string ) : n = 0 res = '' limit = len ( js_string ) while n < limit : char = js_string [ n ] if char == '\\' : new , n = do_escape ( js_string , n ) res += new else : res += char n += 1 return res
8711	def __read_chunk ( self , buf ) : log . debug ( 'reading chunk' ) timeout_before = self . _port . timeout if SYSTEM != 'Windows' : if self . _port . timeout != MINIMAL_TIMEOUT : self . _port . timeout = MINIMAL_TIMEOUT end = time . time ( ) + timeout_before while len ( buf ) < 130 and time . time ( ) <= end : buf = buf + self . _port . read ( ) if buf [ 0 ] != BLOCK_START or len ( buf ) < 130 : log . debug ( 'buffer binary: %s ' , hexify ( buf ) ) raise Exception ( 'Bad blocksize or start byte' ) if SYSTEM != 'Windows' : self . _port . timeout = timeout_before chunk_size = ord ( buf [ 1 ] ) data = buf [ 2 : chunk_size + 2 ] buf = buf [ 130 : ] return ( data , buf )
9286	def sendall ( self , line ) : if isinstance ( line , APRSPacket ) : line = str ( line ) elif not isinstance ( line , string_type ) : raise TypeError ( "Expected line to be str or APRSPacket, got %s" , type ( line ) ) if not self . _connected : raise ConnectionError ( "not connected" ) if line == "" : return line = line . rstrip ( "\r\n" ) + "\r\n" try : self . sock . setblocking ( 1 ) self . sock . settimeout ( 5 ) self . _sendall ( line ) except socket . error as exp : self . close ( ) raise ConnectionError ( str ( exp ) )
1418	def create_execution_state ( self , topologyName , executionState ) : if not executionState or not executionState . IsInitialized ( ) : raise_ ( StateException ( "Execution State protobuf not init properly" , StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) path = self . get_execution_state_path ( topologyName ) LOG . info ( "Adding topology: {0} to path: {1}" . format ( topologyName , path ) ) executionStateString = executionState . SerializeToString ( ) try : self . client . create ( path , value = executionStateString , makepath = True ) return True except NoNodeError : raise_ ( StateException ( "NoNodeError while creating execution state" , StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) except NodeExistsError : raise_ ( StateException ( "NodeExistsError while creating execution state" , StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) except ZookeeperError : raise_ ( StateException ( "Zookeeper while creating execution state" , StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) except Exception : raise
3312	def do_MKCOL ( self , environ , start_response ) : path = environ [ "PATH_INFO" ] provider = self . _davProvider if util . get_content_length ( environ ) != 0 : self . _fail ( HTTP_MEDIATYPE_NOT_SUPPORTED , "The server does not handle any body content." , ) if environ . setdefault ( "HTTP_DEPTH" , "0" ) != "0" : self . _fail ( HTTP_BAD_REQUEST , "Depth must be '0'." ) if provider . exists ( path , environ ) : self . _fail ( HTTP_METHOD_NOT_ALLOWED , "MKCOL can only be executed on an unmapped URL." , ) parentRes = provider . get_resource_inst ( util . get_uri_parent ( path ) , environ ) if not parentRes or not parentRes . is_collection : self . _fail ( HTTP_CONFLICT , "Parent must be an existing collection." ) self . _check_write_permission ( parentRes , "0" , environ ) parentRes . create_collection ( util . get_uri_name ( path ) ) return util . send_status_response ( environ , start_response , HTTP_CREATED )
10602	def alpha ( self , ** state ) : return self . k ( ** state ) / self . rho ( ** state ) / self . Cp ( ** state )
10231	def list_abundance_expansion ( graph : BELGraph ) -> None : mapping = { node : flatten_list_abundance ( node ) for node in graph if isinstance ( node , ListAbundance ) } relabel_nodes ( graph , mapping , copy = False )
5612	def prepare_array ( data , masked = True , nodata = 0 , dtype = "int16" ) : if isinstance ( data , ( list , tuple ) ) : return _prepare_iterable ( data , masked , nodata , dtype ) elif isinstance ( data , np . ndarray ) and data . ndim == 2 : data = ma . expand_dims ( data , axis = 0 ) if isinstance ( data , ma . MaskedArray ) : return _prepare_masked ( data , masked , nodata , dtype ) elif isinstance ( data , np . ndarray ) : if masked : return ma . masked_values ( data . astype ( dtype , copy = False ) , nodata , copy = False ) else : return data . astype ( dtype , copy = False ) else : raise ValueError ( "data must be array, masked array or iterable containing arrays." )
9253	def get_string_for_issue ( self , issue ) : encapsulated_title = self . encapsulate_string ( issue [ 'title' ] ) try : title_with_number = u"{0} [\\#{1}]({2})" . format ( encapsulated_title , issue [ "number" ] , issue [ "html_url" ] ) except UnicodeEncodeError : title_with_number = "ERROR ERROR ERROR: #{0} {1}" . format ( issue [ "number" ] , issue [ 'title' ] ) print ( title_with_number , '\n' , issue [ "html_url" ] ) return self . issue_line_with_user ( title_with_number , issue )
2912	def _ready ( self ) : if self . _has_state ( self . COMPLETED ) or self . _has_state ( self . CANCELLED ) : return self . _set_state ( self . READY ) self . task_spec . _on_ready ( self )
2908	def _find_child_of ( self , parent_task_spec ) : if self . parent is None : return self if self . parent . task_spec == parent_task_spec : return self return self . parent . _find_child_of ( parent_task_spec )
3037	def new_from_json ( cls , json_data ) : json_data_as_unicode = _helpers . _from_bytes ( json_data ) data = json . loads ( json_data_as_unicode ) module_name = data [ '_module' ] try : module_obj = __import__ ( module_name ) except ImportError : module_name = module_name . replace ( '.googleapiclient' , '' ) module_obj = __import__ ( module_name ) module_obj = __import__ ( module_name , fromlist = module_name . split ( '.' ) [ : - 1 ] ) kls = getattr ( module_obj , data [ '_class' ] ) return kls . from_json ( json_data_as_unicode )
5624	def absolute_path ( path = None , base_dir = None ) : if path_is_remote ( path ) : return path else : if os . path . isabs ( path ) : return path else : if base_dir is None or not os . path . isabs ( base_dir ) : raise TypeError ( "base_dir must be an absolute path." ) return os . path . abspath ( os . path . join ( base_dir , path ) )
387	def obj_box_imresize ( im , coords = None , size = None , interp = 'bicubic' , mode = None , is_rescale = False ) : if coords is None : coords = [ ] if size is None : size = [ 100 , 100 ] imh , imw = im . shape [ 0 : 2 ] imh = imh * 1.0 imw = imw * 1.0 im = imresize ( im , size = size , interp = interp , mode = mode ) if is_rescale is False : coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( "coordinate should be 4 values : [x, y, w, h]" ) x = int ( coord [ 0 ] * ( size [ 1 ] / imw ) ) y = int ( coord [ 1 ] * ( size [ 0 ] / imh ) ) w = int ( coord [ 2 ] * ( size [ 1 ] / imw ) ) h = int ( coord [ 3 ] * ( size [ 0 ] / imh ) ) coords_new . append ( [ x , y , w , h ] ) return im , coords_new else : return im , coords
1558	def _get_stream_schema ( fields ) : stream_schema = topology_pb2 . StreamSchema ( ) for field in fields : key = stream_schema . keys . add ( ) key . key = field key . type = topology_pb2 . Type . Value ( "OBJECT" ) return stream_schema
3448	def minimal_medium ( model , min_objective_value = 0.1 , exports = False , minimize_components = False , open_exchanges = False ) : exchange_rxns = find_boundary_types ( model , "exchange" ) if isinstance ( open_exchanges , bool ) : open_bound = 1000 else : open_bound = open_exchanges with model as mod : if open_exchanges : LOGGER . debug ( "Opening exchanges for %d imports." , len ( exchange_rxns ) ) for rxn in exchange_rxns : rxn . bounds = ( - open_bound , open_bound ) LOGGER . debug ( "Applying objective value constraints." ) obj_const = mod . problem . Constraint ( mod . objective . expression , lb = min_objective_value , name = "medium_obj_constraint" ) mod . add_cons_vars ( [ obj_const ] ) mod . solver . update ( ) mod . objective = Zero LOGGER . debug ( "Adding new media objective." ) tol = mod . solver . configuration . tolerances . feasibility if minimize_components : add_mip_obj ( mod ) if isinstance ( minimize_components , bool ) : minimize_components = 1 seen = set ( ) best = num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None exclusion = mod . problem . Constraint ( Zero , ub = 0 ) mod . add_cons_vars ( [ exclusion ] ) mod . solver . update ( ) media = [ ] for i in range ( minimize_components ) : LOGGER . info ( "Finding alternative medium #%d." , ( i + 1 ) ) vars = [ mod . variables [ "ind_" + s ] for s in seen ] if len ( seen ) > 0 : exclusion . set_linear_coefficients ( dict . fromkeys ( vars , 1 ) ) exclusion . ub = best - 1 num_components = mod . slim_optimize ( ) if mod . solver . status != OPTIMAL or num_components > best : break medium = _as_medium ( exchange_rxns , tol , exports = exports ) media . append ( medium ) seen . update ( medium [ medium > 0 ] . index ) if len ( media ) > 1 : medium = pd . concat ( media , axis = 1 , sort = True ) . fillna ( 0.0 ) medium . sort_index ( axis = 1 , inplace = True ) else : medium = media [ 0 ] else : add_linear_obj ( mod ) mod . slim_optimize ( ) if mod . solver . status != OPTIMAL : LOGGER . warning ( "Minimization of medium was infeasible." ) return None medium = _as_medium ( exchange_rxns , tol , exports = exports ) return medium
10714	def _setRTSDTR ( port , RTS , DTR ) : port . setRTS ( RTS ) port . setDTR ( DTR )
11440	def _get_children_as_string ( node ) : out = [ ] if node : for child in node : if child . nodeType == child . TEXT_NODE : out . append ( child . data ) else : out . append ( _get_children_as_string ( child . childNodes ) ) return '' . join ( out )
8758	def get_subnet ( context , id , fields = None ) : LOG . info ( "get_subnet %s for tenant %s with fields %s" % ( id , context . tenant_id , fields ) ) subnet = db_api . subnet_find ( context = context , limit = None , page_reverse = False , sorts = [ 'id' ] , marker_obj = None , fields = None , id = id , join_dns = True , join_routes = True , scope = db_api . ONE ) if not subnet : raise n_exc . SubnetNotFound ( subnet_id = id ) cache = subnet . get ( "_allocation_pool_cache" ) if not cache : new_cache = subnet . allocation_pools db_api . subnet_update_set_alloc_pool_cache ( context , subnet , new_cache ) return v . _make_subnet_dict ( subnet )
8228	def show ( self , format = 'png' , as_data = False ) : from io import BytesIO b = BytesIO ( ) if format == 'png' : from IPython . display import Image surface = cairo . ImageSurface ( cairo . FORMAT_ARGB32 , self . WIDTH , self . HEIGHT ) self . snapshot ( surface ) surface . write_to_png ( b ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return Image ( data ) elif format == 'svg' : from IPython . display import SVG surface = cairo . SVGSurface ( b , self . WIDTH , self . HEIGHT ) surface . finish ( ) b . seek ( 0 ) data = b . read ( ) if as_data : return data else : return SVG ( data )
5843	def get_design_run_results ( self , data_view_id , run_uuid ) : url = routes . get_data_view_design_results ( data_view_id , run_uuid ) response = self . _get ( url ) . json ( ) result = response [ "data" ] return DesignResults ( best_materials = result . get ( "best_material_results" ) , next_experiments = result . get ( "next_experiment_results" ) )
11785	def attrnum ( self , attr ) : "Returns the number used for attr, which can be a name, or -n .. n-1." if attr < 0 : return len ( self . attrs ) + attr elif isinstance ( attr , str ) : return self . attrnames . index ( attr ) else : return attr
1931	def update ( self , name : str , value = None , default = None , description : str = None ) : if name in self . _vars : description = description or self . _vars [ name ] . description default = default or self . _vars [ name ] . default elif name == 'name' : raise ConfigError ( "'name' is a reserved name for a group." ) v = _Var ( name , description = description , default = default , defined = False ) v . value = value self . _vars [ name ] = v
3702	def Tm_depression_eutectic ( Tm , Hm , x = None , M = None , MW = None ) : r if x : dTm = R * Tm ** 2 * x / Hm elif M and MW : MW = MW / 1000. dTm = R * Tm ** 2 * MW * M / Hm else : raise Exception ( 'Either molality or mole fraction of the solute must be specified; MW of the solvent is required also if molality is provided' ) return dTm
10401	def get_final_score ( self ) -> float : if not self . done_chomping ( ) : raise ValueError ( 'algorithm has not yet completed' ) return self . graph . nodes [ self . target_node ] [ self . tag ]
3654	def start_image_acquisition ( self ) : if not self . _create_ds_at_connection : self . _setup_data_streams ( ) num_required_buffers = self . _num_buffers for data_stream in self . _data_streams : try : num_buffers = data_stream . buffer_announce_min if num_buffers < num_required_buffers : num_buffers = num_required_buffers except InvalidParameterException as e : num_buffers = num_required_buffers self . _logger . debug ( e , exc_info = True ) if data_stream . defines_payload_size ( ) : buffer_size = data_stream . payload_size else : buffer_size = self . device . node_map . PayloadSize . value raw_buffers = self . _create_raw_buffers ( num_buffers , buffer_size ) buffer_tokens = self . _create_buffer_tokens ( raw_buffers ) self . _announced_buffers = self . _announce_buffers ( data_stream = data_stream , _buffer_tokens = buffer_tokens ) self . _queue_announced_buffers ( data_stream = data_stream , buffers = self . _announced_buffers ) try : acq_mode = self . device . node_map . AcquisitionMode . value if acq_mode == 'Continuous' : num_images_to_acquire = - 1 elif acq_mode == 'SingleFrame' : num_images_to_acquire = 1 elif acq_mode == 'MultiFrame' : num_images_to_acquire = self . device . node_map . AcquisitionFrameCount . value else : num_images_to_acquire = - 1 except LogicalErrorException as e : num_images_to_acquire = - 1 self . _logger . debug ( e , exc_info = True ) self . _num_images_to_acquire = num_images_to_acquire try : self . device . node_map . TLParamsLocked . value = 1 except LogicalErrorException : pass self . _is_acquiring_images = True for data_stream in self . _data_streams : data_stream . start_acquisition ( ACQ_START_FLAGS_LIST . ACQ_START_FLAGS_DEFAULT , self . _num_images_to_acquire ) if self . thread_image_acquisition : self . thread_image_acquisition . start ( ) self . device . node_map . AcquisitionStart . execute ( ) self . _logger . info ( '{0} started image acquisition.' . format ( self . _device . id_ ) ) if self . _profiler : self . _profiler . print_diff ( )
10809	def update ( self , name = None , description = None , privacy_policy = None , subscription_policy = None , is_managed = None ) : with db . session . begin_nested ( ) : if name is not None : self . name = name if description is not None : self . description = description if ( privacy_policy is not None and PrivacyPolicy . validate ( privacy_policy ) ) : self . privacy_policy = privacy_policy if ( subscription_policy is not None and SubscriptionPolicy . validate ( subscription_policy ) ) : self . subscription_policy = subscription_policy if is_managed is not None : self . is_managed = is_managed db . session . merge ( self ) return self
7389	def node_theta ( self , node ) : group = self . find_node_group_membership ( node ) return self . group_theta ( group )
11215	def compare_token ( expected : Union [ str , bytes ] , actual : Union [ str , bytes ] ) -> bool : expected = util . to_bytes ( expected ) actual = util . to_bytes ( actual ) _ , expected_sig_seg = expected . rsplit ( b'.' , 1 ) _ , actual_sig_seg = actual . rsplit ( b'.' , 1 ) expected_sig = util . b64_decode ( expected_sig_seg ) actual_sig = util . b64_decode ( actual_sig_seg ) return compare_signature ( expected_sig , actual_sig )
7390	def add_edges ( self ) : for group , edgelist in self . edges . items ( ) : for ( u , v , d ) in edgelist : self . draw_edge ( u , v , d , group )
12881	def next ( self ) : self . index += 1 t = self . peek ( ) if not self . depth : self . _cut ( ) return t
2288	def forward ( self ) : self . noise . data . normal_ ( ) if not self . confounding : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) else : for i in self . topological_order : self . generated [ i ] = self . blocks [ i ] ( th . cat ( [ v for c in [ [ self . generated [ j ] for j in np . nonzero ( self . adjacency_matrix [ : , i ] ) [ 0 ] ] , [ self . corr_noise [ min ( i , j ) , max ( i , j ) ] for j in np . nonzero ( self . i_adj_matrix [ : , i ] ) [ 0 ] ] [ self . noise [ : , [ i ] ] ] ] for v in c ] , 1 ) ) return th . cat ( self . generated , 1 )
10727	def _handle_array ( toks ) : if len ( toks ) == 5 and toks [ 1 ] == '{' and toks [ 4 ] == '}' : subtree = toks [ 2 : 4 ] signature = '' . join ( s for ( _ , s ) in subtree ) [ key_func , value_func ] = [ f for ( f , _ ) in subtree ] def the_dict_func ( a_dict , variant = 0 ) : elements = [ ( key_func ( x ) , value_func ( y ) ) for ( x , y ) in a_dict . items ( ) ] level = 0 if elements == [ ] else max ( max ( x , y ) for ( ( _ , x ) , ( _ , y ) ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Dictionary ( ( ( x , y ) for ( ( x , _ ) , ( y , _ ) ) in elements ) , signature = signature , variant_level = obj_level ) , func_level ) return ( the_dict_func , 'a{' + signature + '}' ) if len ( toks ) == 2 : ( func , sig ) = toks [ 1 ] def the_array_func ( a_list , variant = 0 ) : if isinstance ( a_list , dict ) : raise IntoDPValueError ( a_list , "a_list" , "is a dict, must be an array" ) elements = [ func ( x ) for x in a_list ] level = 0 if elements == [ ] else max ( x for ( _ , x ) in elements ) ( obj_level , func_level ) = _ToDbusXformer . _variant_levels ( level , variant ) return ( dbus . types . Array ( ( x for ( x , _ ) in elements ) , signature = sig , variant_level = obj_level ) , func_level ) return ( the_array_func , 'a' + sig ) raise IntoDPValueError ( toks , "toks" , "unexpected tokens" )
10336	def bel_to_spia_matrices ( graph : BELGraph ) -> Mapping [ str , pd . DataFrame ] : index_nodes = get_matrix_index ( graph ) spia_matrices = build_spia_matrices ( index_nodes ) for u , v , edge_data in graph . edges ( data = True ) : if isinstance ( u , CentralDogma ) and isinstance ( v , CentralDogma ) : update_spia_matrices ( spia_matrices , u , v , edge_data ) elif isinstance ( u , CentralDogma ) and isinstance ( v , ListAbundance ) : for node in v . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , u , node , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , CentralDogma ) : for node in u . members : if not isinstance ( node , CentralDogma ) : continue update_spia_matrices ( spia_matrices , node , v , edge_data ) elif isinstance ( u , ListAbundance ) and isinstance ( v , ListAbundance ) : for sub_member , obj_member in product ( u . members , v . members ) : if isinstance ( sub_member , CentralDogma ) and isinstance ( obj_member , CentralDogma ) : update_spia_matrices ( spia_matrices , sub_member , obj_member , edge_data ) return spia_matrices
7510	def _insert_to_array ( self , start , results ) : qrts , wgts , qsts = results with h5py . File ( self . database . output , 'r+' ) as out : chunk = self . _chunksize out [ 'quartets' ] [ start : start + chunk ] = qrts if self . checkpoint . boots : key = "qboots/b{}" . format ( self . checkpoint . boots - 1 ) out [ key ] [ start : start + chunk ] = qsts else : out [ "qstats" ] [ start : start + chunk ] = qsts
2465	def set_file_chksum ( self , doc , chksum ) : if self . has_package ( doc ) and self . has_file ( doc ) : if not self . file_chksum_set : self . file_chksum_set = True self . file ( doc ) . chk_sum = checksum_from_sha1 ( chksum ) return True else : raise CardinalityError ( 'File::CheckSum' ) else : raise OrderError ( 'File::CheckSum' )
4327	def downsample ( self , factor = 2 ) : if not isinstance ( factor , int ) or factor < 1 : raise ValueError ( 'factor must be a positive integer.' ) effect_args = [ 'downsample' , '{}' . format ( factor ) ] self . effects . extend ( effect_args ) self . effects_log . append ( 'downsample' ) return self
11208	def get_config ( jid ) : acls = getattr ( settings , 'XMPP_HTTP_UPLOAD_ACCESS' , ( ( '.*' , False ) , ) ) for regex , config in acls : if isinstance ( regex , six . string_types ) : regex = [ regex ] for subex in regex : if re . search ( subex , jid ) : return config return False
9118	def _add_admin ( self , app , ** kwargs ) : from flask_admin import Admin from flask_admin . contrib . sqla import ModelView admin = Admin ( app , ** kwargs ) for flask_admin_model in self . flask_admin_models : if isinstance ( flask_admin_model , tuple ) : if len ( flask_admin_model ) != 2 : raise TypeError model , view = flask_admin_model admin . add_view ( view ( model , self . session ) ) else : admin . add_view ( ModelView ( flask_admin_model , self . session ) ) return admin
10795	def perfect_platonic_per_pixel ( N , R , scale = 11 , pos = None , zscale = 1.0 , returnpix = None ) : if scale % 2 != 1 : scale += 1 if pos is None : pos = np . array ( [ ( N - 1 ) / 2.0 ] * 3 ) s = 1.0 / scale f = zscale ** 2 i = pos . astype ( 'int' ) p = i + s * ( ( pos - i ) / s ) . astype ( 'int' ) pos = p + 1e-10 image = np . zeros ( ( N , ) * 3 ) x , y , z = np . meshgrid ( * ( xrange ( N ) , ) * 3 , indexing = 'ij' ) for x0 , y0 , z0 in zip ( x . flatten ( ) , y . flatten ( ) , z . flatten ( ) ) : ddd = np . sqrt ( f * ( x0 - pos [ 0 ] ) ** 2 + ( y0 - pos [ 1 ] ) ** 2 + ( z0 - pos [ 2 ] ) ** 2 ) if ddd > R + 4 : image [ x0 , y0 , z0 ] = 0.0 continue xp , yp , zp = np . meshgrid ( * ( np . linspace ( i - 0.5 + s / 2 , i + 0.5 - s / 2 , scale , endpoint = True ) for i in ( x0 , y0 , z0 ) ) , indexing = 'ij' ) ddd = np . sqrt ( f * ( xp - pos [ 0 ] ) ** 2 + ( yp - pos [ 1 ] ) ** 2 + ( zp - pos [ 2 ] ) ** 2 ) if returnpix is not None and returnpix == [ x0 , y0 , z0 ] : outpix = 1.0 * ( ddd < R ) vol = ( 1.0 * ( ddd < R ) + 0.0 * ( ddd == R ) ) . sum ( ) image [ x0 , y0 , z0 ] = vol / float ( scale ** 3 ) if returnpix : return image , pos , outpix return image , pos
408	def _tf_repeat ( self , a , repeats ) : if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( "This is not a 1D Tensor" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a
9788	def bookmark ( ctx , username ) : ctx . obj = ctx . obj or { } ctx . obj [ 'username' ] = username
6097	def luminosity_within_circle_in_units ( self , radius : dim . Length , unit_luminosity = 'eps' , kpc_per_arcsec = None , exposure_time = None ) : if not isinstance ( radius , dim . Length ) : radius = dim . Length ( value = radius , unit_length = 'arcsec' ) profile = self . new_profile_with_units_converted ( unit_length = radius . unit_length , unit_luminosity = unit_luminosity , kpc_per_arcsec = kpc_per_arcsec , exposure_time = exposure_time ) luminosity = quad ( profile . luminosity_integral , a = 0.0 , b = radius , args = ( 1.0 , ) ) [ 0 ] return dim . Luminosity ( luminosity , unit_luminosity )
4395	def adsSyncReadByNameEx ( port , address , data_name , data_type , return_ctypes = False ) : handle = adsSyncReadWriteReqEx2 ( port , address , ADSIGRP_SYM_HNDBYNAME , 0x0 , PLCTYPE_UDINT , data_name , PLCTYPE_STRING , ) value = adsSyncReadReqEx2 ( port , address , ADSIGRP_SYM_VALBYHND , handle , data_type , return_ctypes ) adsSyncWriteReqEx ( port , address , ADSIGRP_SYM_RELEASEHND , 0 , handle , PLCTYPE_UDINT ) return value
11563	def set_digital_latch ( self , pin , threshold_type , cb = None ) : if 0 <= threshold_type <= 1 : self . _command_handler . set_digital_latch ( pin , threshold_type , cb ) return True else : return False
3229	def gce_list ( service = None , ** kwargs ) : resp_list = [ ] req = service . list ( ** kwargs ) while req is not None : resp = req . execute ( ) for item in resp . get ( 'items' , [ ] ) : resp_list . append ( item ) req = service . list_next ( previous_request = req , previous_response = resp ) return resp_list
1599	def pipe ( prev_proc , to_cmd ) : stdin = None if prev_proc is None else prev_proc . stdout process = subprocess . Popen ( to_cmd , stdout = subprocess . PIPE , stdin = stdin ) if prev_proc is not None : prev_proc . stdout . close ( ) return process
11877	def format ( self , record ) : try : record . message = record . getMessage ( ) except TypeError : if record . args : if isinstance ( record . args , collections . Mapping ) : record . message = record . msg . format ( ** record . args ) else : record . message = record . msg . format ( record . args ) self . _fmt = self . getfmt ( record . levelname ) if self . usesTime ( ) : record . asctime = self . formatTime ( record , self . datefmt ) s = self . _fmt . format ( ** record . __dict__ ) if record . exc_info : if not record . exc_text : record . exc_text = self . formatException ( record . exc_info ) if record . exc_text : if s [ - 1 : ] != '\n' : s += '\n' try : s = s + record . exc_text except UnicodeError : s = s + record . exc_text . decode ( sys . getfilesystemencoding ( ) , 'replace' ) return s
8804	def build_payload ( ipaddress , event_type , event_time = None , start_time = None , end_time = None ) : payload = { 'event_type' : unicode ( event_type ) , 'tenant_id' : unicode ( ipaddress . used_by_tenant_id ) , 'ip_address' : unicode ( ipaddress . address_readable ) , 'ip_version' : int ( ipaddress . version ) , 'ip_type' : unicode ( ipaddress . address_type ) , 'id' : unicode ( ipaddress . id ) } if event_type == IP_EXISTS : if start_time is None or end_time is None : raise ValueError ( 'IP_BILL: {} start_time/end_time cannot be empty' . format ( event_type ) ) payload . update ( { 'startTime' : unicode ( convert_timestamp ( start_time ) ) , 'endTime' : unicode ( convert_timestamp ( end_time ) ) } ) elif event_type in [ IP_ADD , IP_DEL , IP_ASSOC , IP_DISASSOC ] : if event_time is None : raise ValueError ( 'IP_BILL: {}: event_time cannot be NULL' . format ( event_type ) ) payload . update ( { 'eventTime' : unicode ( convert_timestamp ( event_time ) ) , 'subnet_id' : unicode ( ipaddress . subnet_id ) , 'network_id' : unicode ( ipaddress . network_id ) , 'public' : True if ipaddress . network_id == PUBLIC_NETWORK_ID else False , } ) else : raise ValueError ( 'IP_BILL: bad event_type: {}' . format ( event_type ) ) return payload
157	def noise2d ( self , x , y ) : stretch_offset = ( x + y ) * STRETCH_CONSTANT_2D xs = x + stretch_offset ys = y + stretch_offset xsb = floor ( xs ) ysb = floor ( ys ) squish_offset = ( xsb + ysb ) * SQUISH_CONSTANT_2D xb = xsb + squish_offset yb = ysb + squish_offset xins = xs - xsb yins = ys - ysb in_sum = xins + yins dx0 = x - xb dy0 = y - yb value = 0 dx1 = dx0 - 1 - SQUISH_CONSTANT_2D dy1 = dy0 - 0 - SQUISH_CONSTANT_2D attn1 = 2 - dx1 * dx1 - dy1 * dy1 extrapolate = self . _extrapolate2d if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , dx1 , dy1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_2D dy2 = dy0 - 1 - SQUISH_CONSTANT_2D attn2 = 2 - dx2 * dx2 - dy2 * dy2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , dx2 , dy2 ) if in_sum <= 1 : zins = 1 - in_sum if zins > xins or zins > yins : if xins > yins : xsv_ext = xsb + 1 ysv_ext = ysb - 1 dx_ext = dx0 - 1 dy_ext = dy0 + 1 else : xsv_ext = xsb - 1 ysv_ext = ysb + 1 dx_ext = dx0 + 1 dy_ext = dy0 - 1 else : xsv_ext = xsb + 1 ysv_ext = ysb + 1 dx_ext = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 1 - 2 * SQUISH_CONSTANT_2D else : zins = 2 - in_sum if zins < xins or zins < yins : if xins > yins : xsv_ext = xsb + 2 ysv_ext = ysb + 0 dx_ext = dx0 - 2 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 + 0 - 2 * SQUISH_CONSTANT_2D else : xsv_ext = xsb + 0 ysv_ext = ysb + 2 dx_ext = dx0 + 0 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 2 - 2 * SQUISH_CONSTANT_2D else : dx_ext = dx0 dy_ext = dy0 xsv_ext = xsb ysv_ext = ysb xsb += 1 ysb += 1 dx0 = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy0 = dy0 - 1 - 2 * SQUISH_CONSTANT_2D attn0 = 2 - dx0 * dx0 - dy0 * dy0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb , ysb , dx0 , dy0 ) attn_ext = 2 - dx_ext * dx_ext - dy_ext * dy_ext if attn_ext > 0 : attn_ext *= attn_ext value += attn_ext * attn_ext * extrapolate ( xsv_ext , ysv_ext , dx_ext , dy_ext ) return value / NORM_CONSTANT_2D
9038	def bounding_box ( self ) : min_x , min_y , max_x , max_y = zip ( * list ( self . walk_rows ( lambda row : row . bounding_box ) ) ) return min ( min_x ) , min ( min_y ) , max ( max_x ) , max ( max_y )
1770	def backup_emulate ( self , insn ) : if not hasattr ( self , 'backup_emu' ) : self . backup_emu = UnicornEmulator ( self ) try : self . backup_emu . emulate ( insn ) except unicorn . UcError as e : if e . errno == unicorn . UC_ERR_INSN_INVALID : text_bytes = ' ' . join ( '%02x' % x for x in insn . bytes ) logger . error ( "Unimplemented instruction: 0x%016x:\t%s\t%s\t%s" , insn . address , text_bytes , insn . mnemonic , insn . op_str ) raise InstructionEmulationError ( str ( e ) ) finally : del self . backup_emu
248	def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df
8425	def grey_pal ( start = 0.2 , end = 0.8 ) : gamma = 2.2 ends = ( ( 0.0 , start , start ) , ( 1.0 , end , end ) ) cdict = { 'red' : ends , 'green' : ends , 'blue' : ends } grey_cmap = mcolors . LinearSegmentedColormap ( 'grey' , cdict ) def continuous_grey_palette ( n ) : colors = [ ] for x in np . linspace ( start ** gamma , end ** gamma , n ) : x = ( x ** ( 1. / gamma ) - start ) / ( end - start ) colors . append ( mcolors . rgb2hex ( grey_cmap ( x ) ) ) return colors return continuous_grey_palette
9630	def render_to_message ( self , extra_context = None , ** kwargs ) : if extra_context is None : extra_context = { } kwargs . setdefault ( 'headers' , { } ) . update ( self . headers ) context = self . get_context_data ( ** extra_context ) return self . message_class ( subject = self . render_subject ( context ) , body = self . render_body ( context ) , ** kwargs )
9326	def refresh ( self ) : response = self . __raw = self . _conn . get ( self . url ) self . _populate_fields ( ** response ) self . _loaded = True
10504	def removecallback ( window_name ) : if window_name in _pollEvents . _callback : del _pollEvents . _callback [ window_name ] return _remote_removecallback ( window_name )
5060	def get_enterprise_customer ( uuid ) : EnterpriseCustomer = apps . get_model ( 'enterprise' , 'EnterpriseCustomer' ) try : return EnterpriseCustomer . objects . get ( uuid = uuid ) except EnterpriseCustomer . DoesNotExist : return None
9274	def filter_excluded_tags ( self , all_tags ) : filtered_tags = copy . deepcopy ( all_tags ) if self . options . exclude_tags : filtered_tags = self . apply_exclude_tags ( filtered_tags ) if self . options . exclude_tags_regex : filtered_tags = self . apply_exclude_tags_regex ( filtered_tags ) return filtered_tags
9886	def _call_multi_fortran_z ( self , names , data_types , rec_nums , dim_sizes , input_type_code , func , epoch = False , data_offset = None , epoch16 = False ) : idx , = np . where ( data_types == input_type_code ) if len ( idx ) > 0 : max_rec = rec_nums [ idx ] . max ( ) sub_names = np . array ( names ) [ idx ] sub_sizes = dim_sizes [ idx ] status , data = func ( self . fname , sub_names . tolist ( ) , sub_sizes , sub_sizes . sum ( ) , max_rec , len ( sub_names ) ) if status == 0 : if data_offset is not None : data = data . astype ( int ) idx , idy , = np . where ( data < 0 ) data [ idx , idy ] += data_offset if epoch : data -= 62167219200000 data = data . astype ( '<M8[ms]' ) if epoch16 : data [ 0 : : 2 , : ] -= 62167219200 data = data [ 0 : : 2 , : ] * 1E9 + data [ 1 : : 2 , : ] / 1.E3 data = data . astype ( 'datetime64[ns]' ) sub_sizes /= 2 self . _process_return_multi_z ( data , sub_names , sub_sizes ) else : raise IOError ( fortran_cdf . statusreporter ( status ) )
5259	def _adjust_delay ( self , slot , response ) : if response . status in self . retry_http_codes : new_delay = max ( slot . delay , 1 ) * 4 new_delay = max ( new_delay , self . mindelay ) new_delay = min ( new_delay , self . maxdelay ) slot . delay = new_delay self . stats . inc_value ( 'delay_count' ) elif response . status == 200 : new_delay = max ( slot . delay / 2 , self . mindelay ) if new_delay < 0.01 : new_delay = 0 slot . delay = new_delay
5801	def extract_from_system ( cert_callback = None , callback_only_on_failure = False ) : all_purposes = '2.5.29.37.0' ca_path = system_path ( ) output = [ ] with open ( ca_path , 'rb' ) as f : for armor_type , _ , cert_bytes in unarmor ( f . read ( ) , multiple = True ) : if armor_type == 'CERTIFICATE' : if cert_callback : cert_callback ( Certificate . load ( cert_bytes ) , None ) output . append ( ( cert_bytes , set ( ) , set ( ) ) ) elif armor_type == 'TRUSTED CERTIFICATE' : cert , aux = TrustedCertificate . load ( cert_bytes ) reject_all = False trust_oids = set ( ) reject_oids = set ( ) for purpose in aux [ 'trust' ] : if purpose . dotted == all_purposes : trust_oids = set ( [ purpose . dotted ] ) break trust_oids . add ( purpose . dotted ) for purpose in aux [ 'reject' ] : if purpose . dotted == all_purposes : reject_all = True break reject_oids . add ( purpose . dotted ) if reject_all : if cert_callback : cert_callback ( cert , 'explicitly distrusted' ) continue if cert_callback and not callback_only_on_failure : cert_callback ( cert , None ) output . append ( ( cert . dump ( ) , trust_oids , reject_oids ) ) return output
8641	def highlight_project_bid ( session , bid_id ) : headers = { 'Content-Type' : 'application/x-www-form-urlencoded' } bid_data = { 'action' : 'highlight' } endpoint = 'bids/{}' . format ( bid_id ) response = make_put_request ( session , endpoint , headers = headers , params_data = bid_data ) json_data = response . json ( ) if response . status_code == 200 : return json_data [ 'status' ] else : json_data = response . json ( ) raise BidNotHighlightedException ( message = json_data [ 'message' ] , error_code = json_data [ 'error_code' ] , request_id = json_data [ 'request_id' ] )
11412	def record_get_field ( rec , tag , field_position_global = None , field_position_local = None ) : if field_position_global is None and field_position_local is None : raise InvenioBibRecordFieldError ( "A field position is required to " "complete this operation." ) elif field_position_global is not None and field_position_local is not None : raise InvenioBibRecordFieldError ( "Only one field position is required " "to complete this operation." ) elif field_position_global : if tag not in rec : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) for field in rec [ tag ] : if field [ 4 ] == field_position_global : return field raise InvenioBibRecordFieldError ( "No field has the tag '%s' and the " "global field position '%d'." % ( tag , field_position_global ) ) else : try : return rec [ tag ] [ field_position_local ] except KeyError : raise InvenioBibRecordFieldError ( "No tag '%s' in record." % tag ) except IndexError : raise InvenioBibRecordFieldError ( "No field has the tag '%s' and " "the local field position '%d'." % ( tag , field_position_local ) )
8052	def parse_theme ( self , xml ) : kt = KulerTheme ( ) kt . author = xml . getElementsByTagName ( "author" ) [ 0 ] kt . author = kt . author . childNodes [ 1 ] . childNodes [ 0 ] . nodeValue kt . id = int ( self . parse_tag ( xml , "id" ) ) kt . label = self . parse_tag ( xml , "label" ) mode = self . parse_tag ( xml , "mode" ) for swatch in xml . getElementsByTagName ( "swatch" ) : c1 = float ( self . parse_tag ( swatch , "c1" ) ) c2 = float ( self . parse_tag ( swatch , "c2" ) ) c3 = float ( self . parse_tag ( swatch , "c3" ) ) c4 = float ( self . parse_tag ( swatch , "c4" ) ) if mode == "rgb" : kt . append ( ( c1 , c2 , c3 ) ) if mode == "cmyk" : kt . append ( cmyk_to_rgb ( c1 , c2 , c3 , c4 ) ) if mode == "hsv" : kt . append ( colorsys . hsv_to_rgb ( c1 , c2 , c3 ) ) if mode == "hex" : kt . append ( hex_to_rgb ( c1 ) ) if mode == "lab" : kt . append ( lab_to_rgb ( c1 , c2 , c3 ) ) if self . _cache . exists ( self . id_string + str ( kt . id ) ) : xml = self . _cache . read ( self . id_string + str ( kt . id ) ) xml = minidom . parseString ( xml ) for tags in xml . getElementsByTagName ( "tag" ) : tags = self . parse_tag ( tags , "label" ) tags = tags . split ( " " ) kt . tags . extend ( tags ) return kt
12543	def merge_images ( images , axis = 't' ) : if not images : return None axis_dim = { 'x' : 0 , 'y' : 1 , 'z' : 2 , 't' : 3 , } if axis not in axis_dim : raise ValueError ( 'Expected `axis` to be one of ({}), got {}.' . format ( set ( axis_dim . keys ( ) ) , axis ) ) img1 = images [ 0 ] for img in images : check_img_compatibility ( img1 , img ) image_data = [ ] for img in images : image_data . append ( check_img ( img ) . get_data ( ) ) work_axis = axis_dim [ axis ] ndim = image_data [ 0 ] . ndim if ndim - 1 < work_axis : image_data = [ np . expand_dims ( img , axis = work_axis ) for img in image_data ] return np . concatenate ( image_data , axis = work_axis )
2143	def ordered_dump ( data , Dumper = yaml . Dumper , ** kws ) : class OrderedDumper ( Dumper ) : pass def _dict_representer ( dumper , data ) : return dumper . represent_mapping ( yaml . resolver . BaseResolver . DEFAULT_MAPPING_TAG , data . items ( ) ) OrderedDumper . add_representer ( OrderedDict , _dict_representer ) return yaml . dump ( data , None , OrderedDumper , ** kws )
2760	def get_load_balancer ( self , id ) : return LoadBalancer . get_object ( api_token = self . token , id = id )
9233	def fetch_commit ( self , event ) : gh = self . github user = self . options . user repo = self . options . project rc , data = gh . repos [ user ] [ repo ] . git . commits [ event [ "commit_id" ] ] . get ( ) if rc == 200 : return data self . raise_GitHubError ( rc , data , gh . getheaders ( ) )
2851	def _mpsse_sync ( self , max_retries = 10 ) : self . _write ( '\xAB' ) tries = 0 sync = False while not sync : data = self . _poll_read ( 2 ) if data == '\xFA\xAB' : sync = True tries += 1 if tries >= max_retries : raise RuntimeError ( 'Could not synchronize with FT232H!' )
7453	def collate_files ( data , sname , tmp1s , tmp2s ) : out1 = os . path . join ( data . dirs . fastqs , "{}_R1_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out1 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp1s : cmd1 += [ tmpfile ] proc = sps . Popen ( [ 'which' , 'pigz' ] , stderr = sps . PIPE , stdout = sps . PIPE ) . communicate ( ) if proc [ 0 ] . strip ( ) : compress = [ "pigz" ] else : compress = [ "gzip" ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R1 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp1s : os . remove ( tmpfile ) if 'pair' in data . paramsdict [ "datatype" ] : out2 = os . path . join ( data . dirs . fastqs , "{}_R2_.fastq.gz" . format ( sname ) ) out = io . BufferedWriter ( gzip . open ( out2 , 'w' ) ) cmd1 = [ 'cat' ] for tmpfile in tmp2s : cmd1 += [ tmpfile ] proc1 = sps . Popen ( cmd1 , stderr = sps . PIPE , stdout = sps . PIPE ) proc2 = sps . Popen ( compress , stdin = proc1 . stdout , stderr = sps . PIPE , stdout = out ) err = proc2 . communicate ( ) if proc2 . returncode : raise IPyradWarningExit ( "error in collate_files R2 %s" , err ) proc1 . stdout . close ( ) out . close ( ) for tmpfile in tmp2s : os . remove ( tmpfile )
4572	def hsv2rgb_raw ( hsv ) : HSV_SECTION_3 = 0x40 h , s , v = hsv invsat = 255 - s brightness_floor = ( v * invsat ) // 256 color_amplitude = v - brightness_floor section = h // HSV_SECTION_3 offset = h % HSV_SECTION_3 rampup = offset rampdown = ( HSV_SECTION_3 - 1 ) - offset rampup_amp_adj = ( rampup * color_amplitude ) // ( 256 // 4 ) rampdown_amp_adj = ( rampdown * color_amplitude ) // ( 256 // 4 ) rampup_adj_with_floor = rampup_amp_adj + brightness_floor rampdown_adj_with_floor = rampdown_amp_adj + brightness_floor r , g , b = ( 0 , 0 , 0 ) if section : if section == 1 : r = brightness_floor g = rampdown_adj_with_floor b = rampup_adj_with_floor else : r = rampup_adj_with_floor g = brightness_floor b = rampdown_adj_with_floor else : r = rampdown_adj_with_floor g = rampup_adj_with_floor b = brightness_floor return ( r , g , b )
739	def cPrint ( self , level , message , * args , ** kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( "Invalid keywords for cPrint: %s" % str ( kw . keys ( ) ) ) newline = kw . get ( "newline" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( "Invalid keyword for cPrint: %s" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,
786	def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )
8554	def reserve_ipblock ( self , ipblock ) : properties = { "name" : ipblock . name } if ipblock . location : properties [ 'location' ] = ipblock . location if ipblock . size : properties [ 'size' ] = str ( ipblock . size ) raw = { "properties" : properties , } response = self . _perform_request ( url = '/ipblocks' , method = 'POST' , data = json . dumps ( raw ) ) return response
1615	def ReplaceAll ( pattern , rep , s ) : if pattern not in _regexp_compile_cache : _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) return _regexp_compile_cache [ pattern ] . sub ( rep , s )
12945	def hasSameValues ( self , other , cascadeObject = True ) : if self . FIELDS != other . FIELDS : return False oga = object . __getattribute__ for field in self . FIELDS : thisVal = oga ( self , field ) otherVal = oga ( other , field ) if thisVal != otherVal : return False if cascadeObject is True and issubclass ( field . __class__ , IRForeignLinkFieldBase ) : if thisVal and thisVal . isFetched ( ) : if otherVal and otherVal . isFetched ( ) : theseForeign = thisVal . getObjs ( ) othersForeign = otherVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if not theseForeign [ i ] . hasSameValues ( othersForeign [ i ] ) : return False else : theseForeign = thisVal . getObjs ( ) for i in range ( len ( theseForeign ) ) : if theseForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False else : if otherVal and otherVal . isFetched ( ) : othersForeign = otherVal . getObjs ( ) for i in range ( len ( othersForeign ) ) : if othersForeign [ i ] . hasUnsavedChanges ( cascadeObjects = True ) : return False return True
5349	def compose_title ( projects , data ) : for project in data : projects [ project ] = { 'meta' : { 'title' : data [ project ] [ 'title' ] } } return projects
7516	def init_arrays ( data ) : co5 = h5py . File ( data . clust_database , 'r' ) io5 = h5py . File ( data . database , 'w' ) maxlen = data . _hackersonly [ "max_fragment_length" ] + 20 chunks = co5 [ "seqs" ] . attrs [ "chunksize" ] [ 0 ] nloci = co5 [ "seqs" ] . shape [ 0 ] snps = io5 . create_dataset ( "snps" , ( nloci , maxlen , 2 ) , dtype = np . bool , chunks = ( chunks , maxlen , 2 ) , compression = 'gzip' ) snps . attrs [ "chunksize" ] = chunks snps . attrs [ "names" ] = [ "-" , "*" ] filters = io5 . create_dataset ( "filters" , ( nloci , 6 ) , dtype = np . bool ) filters . attrs [ "filters" ] = [ "duplicates" , "max_indels" , "max_snps" , "max_shared_hets" , "min_samps" , "max_alleles" ] edges = io5 . create_dataset ( "edges" , ( nloci , 5 ) , dtype = np . uint16 , chunks = ( chunks , 5 ) , compression = "gzip" ) edges . attrs [ "chunksize" ] = chunks edges . attrs [ "names" ] = [ "R1_L" , "R1_R" , "R2_L" , "R2_R" , "sep" ] edges [ : , 4 ] = co5 [ "splits" ] [ : ] filters [ : , 0 ] = co5 [ "duplicates" ] [ : ] io5 . close ( ) co5 . close ( )
1065	def getfirstmatchingheader ( self , name ) : name = name . lower ( ) + ':' n = len ( name ) lst = [ ] hit = 0 for line in self . headers : if hit : if not line [ : 1 ] . isspace ( ) : break elif line [ : n ] . lower ( ) == name : hit = 1 if hit : lst . append ( line ) return lst
8528	def get_ip_packet ( data , client_port , server_port , is_loopback = False ) : header = _loopback if is_loopback else _ethernet try : header . unpack ( data ) except Exception as ex : raise ValueError ( 'Bad header: %s' % ex ) tcp_p = getattr ( header . data , 'data' , None ) if type ( tcp_p ) != dpkt . tcp . TCP : raise ValueError ( 'Not a TCP packet' ) if tcp_p . dport == server_port : if client_port != 0 and tcp_p . sport != client_port : raise ValueError ( 'Request from different client' ) elif tcp_p . sport == server_port : if client_port != 0 and tcp_p . dport != client_port : raise ValueError ( 'Reply for different client' ) else : raise ValueError ( 'Packet not for/from client/server' ) return header . data
10506	def main ( port = 4118 , parentpid = None ) : if "LDTP_DEBUG" in os . environ : _ldtp_debug = True else : _ldtp_debug = False _ldtp_debug_file = os . environ . get ( 'LDTP_DEBUG_FILE' , None ) if _ldtp_debug : print ( "Parent PID: {}" . format ( int ( parentpid ) ) ) if _ldtp_debug_file : with open ( unicode ( _ldtp_debug_file ) , "a" ) as fp : fp . write ( "Parent PID: {}" . format ( int ( parentpid ) ) ) server = LDTPServer ( ( '' , port ) , allow_none = True , logRequests = _ldtp_debug , requestHandler = RequestHandler ) server . register_introspection_functions ( ) server . register_multicall_functions ( ) ldtp_inst = core . Core ( ) server . register_instance ( ldtp_inst ) if parentpid : thread . start_new_thread ( notifyclient , ( parentpid , ) ) try : server . serve_forever ( ) except KeyboardInterrupt : pass except : if _ldtp_debug : print ( traceback . format_exc ( ) ) if _ldtp_debug_file : with open ( _ldtp_debug_file , "a" ) as fp : fp . write ( traceback . format_exc ( ) )
3935	def get ( self ) : logger . info ( 'Loading refresh_token from %s' , repr ( self . _filename ) ) try : with open ( self . _filename ) as f : return f . read ( ) except IOError as e : logger . info ( 'Failed to load refresh_token: %s' , e )
7456	def splitfiles ( data , raws , ipyclient ) : tmpdir = os . path . join ( data . paramsdict [ "project_dir" ] , "tmp-chunks-" + data . name ) if os . path . exists ( tmpdir ) : shutil . rmtree ( tmpdir ) os . makedirs ( tmpdir ) totalreads = estimate_optim ( data , raws [ 0 ] [ 0 ] , ipyclient ) optim = int ( 8e6 ) njobs = int ( totalreads / ( optim / 4. ) ) * len ( raws ) nosplit = 0 if ( len ( raws ) > len ( ipyclient ) ) or ( totalreads < optim ) : nosplit = 1 start = time . time ( ) chunkfiles = { } for fidx , tups in enumerate ( raws ) : handle = os . path . splitext ( os . path . basename ( tups [ 0 ] ) ) [ 0 ] if nosplit : chunkfiles [ handle ] = [ tups ] else : chunklist = zcat_make_temps ( data , tups , fidx , tmpdir , optim , njobs , start ) chunkfiles [ handle ] = chunklist if not nosplit : print ( "" ) return chunkfiles
5579	def write_output_metadata ( output_params ) : if "path" in output_params : metadata_path = os . path . join ( output_params [ "path" ] , "metadata.json" ) logger . debug ( "check for output %s" , metadata_path ) try : existing_params = read_output_metadata ( metadata_path ) logger . debug ( "%s exists" , metadata_path ) logger . debug ( "existing output parameters: %s" , pformat ( existing_params ) ) existing_tp = existing_params [ "pyramid" ] current_params = params_to_dump ( output_params ) logger . debug ( "current output parameters: %s" , pformat ( current_params ) ) current_tp = BufferedTilePyramid ( ** current_params [ "pyramid" ] ) if existing_tp != current_tp : raise MapcheteConfigError ( "pyramid definitions between existing and new output do not match: " "%s != %s" % ( existing_tp , current_tp ) ) existing_format = existing_params [ "driver" ] [ "format" ] current_format = current_params [ "driver" ] [ "format" ] if existing_format != current_format : raise MapcheteConfigError ( "existing output format does not match new output format: " "%s != %s" % ( ( existing_format , current_format ) ) ) except FileNotFoundError : logger . debug ( "%s does not exist" , metadata_path ) dump_params = params_to_dump ( output_params ) write_json ( metadata_path , dump_params ) else : logger . debug ( "no path parameter found" )
1340	def binarize ( x , values , threshold = None , included_in = 'upper' ) : lower , upper = values if threshold is None : threshold = ( lower + upper ) / 2. x = x . copy ( ) if included_in == 'lower' : x [ x <= threshold ] = lower x [ x > threshold ] = upper elif included_in == 'upper' : x [ x < threshold ] = lower x [ x >= threshold ] = upper else : raise ValueError ( 'included_in must be "lower" or "upper"' ) return x
13298	def find_repos ( self , depth = 10 ) : repos = [ ] for root , subdirs , files in walk_dn ( self . root , depth = depth ) : if 'modules' in root : continue if '.git' in subdirs : repos . append ( root ) return repos
13216	def restore ( self , name , filename ) : if not self . exists ( name ) : self . create ( name ) else : log . warn ( 'overwriting contents of database %s' % name ) log . info ( 'restoring %s from %s' % ( name , filename ) ) self . _run_cmd ( 'pg_restore' , '--verbose' , '--dbname=%s' % name , filename )
669	def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , "UniformLink" , "" , srcOutput = "dataOut" , destInput = "bottomUpIn" )
12132	def linspace ( self , start , stop , n ) : if n == 1 : return [ start ] L = [ 0.0 ] * n nm1 = n - 1 nm1inv = 1.0 / nm1 for i in range ( n ) : L [ i ] = nm1inv * ( start * ( nm1 - i ) + stop * i ) return L
11963	def _dec_to_dot ( ip ) : first = int ( ( ip >> 24 ) & 255 ) second = int ( ( ip >> 16 ) & 255 ) third = int ( ( ip >> 8 ) & 255 ) fourth = int ( ip & 255 ) return '%d.%d.%d.%d' % ( first , second , third , fourth )
11311	def get_record ( self ) : self . recid = self . get_recid ( ) self . remove_controlfields ( ) self . update_system_numbers ( ) self . add_systemnumber ( "Inspire" , recid = self . recid ) self . add_control_number ( "003" , "SzGeCERN" ) self . update_collections ( ) self . update_languages ( ) self . update_reportnumbers ( ) self . update_authors ( ) self . update_journals ( ) self . update_subject_categories ( "INSPIRE" , "SzGeCERN" , "categories_cds" ) self . update_pagenumber ( ) self . update_notes ( ) self . update_experiments ( ) self . update_isbn ( ) self . update_dois ( ) self . update_links_and_ffts ( ) self . update_date ( ) self . update_date_year ( ) self . update_hidden_notes ( ) self . update_oai_info ( ) self . update_cnum ( ) self . update_conference_info ( ) self . fields_list = [ "909" , "541" , "961" , "970" , "690" , "695" , "981" , ] self . strip_fields ( ) if "ANNOUNCEMENT" in self . collections : self . update_conference_111 ( ) self . update_conference_links ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if "THESIS" in self . collections : self . update_thesis_information ( ) self . update_thesis_supervisors ( ) if "PROCEEDINGS" in self . collections : self . update_title_to_proceeding ( ) self . update_author_to_proceeding ( ) record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CONFERENCE" ) ] ) if self . tag_as_cern : record_add_field ( self . record , "690" , ind1 = "C" , subfields = [ ( "a" , "CERN" ) ] ) return self . record
8860	def goto_assignments ( request_data ) : code = request_data [ 'code' ] line = request_data [ 'line' ] + 1 column = request_data [ 'column' ] path = request_data [ 'path' ] encoding = 'utf-8' script = jedi . Script ( code , line , column , path , encoding ) try : definitions = script . goto_assignments ( ) except jedi . NotFoundError : pass else : ret_val = [ ( d . module_path , d . line - 1 if d . line else None , d . column , d . full_name ) for d in definitions ] return ret_val
4363	def _spawn_heartbeat ( self ) : self . spawn ( self . _heartbeat ) self . spawn ( self . _heartbeat_timeout )
428	def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test
13094	def start_processes ( self ) : self . relay = subprocess . Popen ( [ 'ntlmrelayx.py' , '-6' , '-tf' , self . targets_file , '-w' , '-l' , self . directory , '-of' , self . output_file ] , cwd = self . directory ) self . responder = subprocess . Popen ( [ 'responder' , '-I' , self . interface_name ] )
431	def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )
6248	def get_texture ( self , label : str ) -> Union [ moderngl . Texture , moderngl . TextureArray , moderngl . Texture3D , moderngl . TextureCube ] : return self . _project . get_texture ( label )
13895	def FindFiles ( dir_ , in_filters = None , out_filters = None , recursive = True , include_root_dir = True , standard_paths = False ) : if in_filters is None : in_filters = [ '*' ] if out_filters is None : out_filters = [ ] result = [ ] for dir_root , directories , filenames in os . walk ( dir_ ) : for i_directory in directories [ : ] : if MatchMasks ( i_directory , out_filters ) : directories . remove ( i_directory ) for filename in directories + filenames : if MatchMasks ( filename , in_filters ) and not MatchMasks ( filename , out_filters ) : result . append ( os . path . join ( dir_root , filename ) ) if not recursive : break if not include_root_dir : dir_prefix = len ( dir_ ) + 1 result = [ file [ dir_prefix : ] for file in result ] if standard_paths : result = map ( StandardizePath , result ) return result
13303	def nmse ( a , b ) : return np . square ( a - b ) . mean ( ) / ( a . mean ( ) * b . mean ( ) )
2661	def hold_worker ( self , worker_id ) : c = self . command_client . run ( "HOLD_WORKER;{}" . format ( worker_id ) ) logger . debug ( "Sent hold request to worker: {}" . format ( worker_id ) ) return c
6155	def cruise_control ( wn , zeta , T , vcruise , vmax , tf_mode = 'H' ) : tau = T / 2. * vmax / vcruise g = 9.8 g *= 3 * 60 ** 2 / 5280. Kp = T * ( 2 * zeta * wn - 1 / tau ) / vmax Ki = T * wn ** 2. / vmax K = Kp * vmax / T print ( 'wn = ' , np . sqrt ( K / ( Kp / Ki ) ) ) print ( 'zeta = ' , ( K + 1 / tau ) / ( 2 * wn ) ) a = np . array ( [ 1 , 2 * zeta * wn , wn ** 2 ] ) if tf_mode == 'H' : b = np . array ( [ K , wn ** 2 ] ) elif tf_mode == 'HE' : b = np . array ( [ 1 , 2 * zeta * wn - K , 0. ] ) elif tf_mode == 'HVW' : b = np . array ( [ 1 , wn ** 2 / K + 1 / tau , wn ** 2 / ( K * tau ) ] ) b *= Kp elif tf_mode == 'HED' : b = np . array ( [ g , 0 ] ) else : raise ValueError ( 'tf_mode must be: H, HE, HVU, or HED' ) return b , a
6075	def einstein_radius_in_units ( self , unit_length = 'arcsec' , kpc_per_arcsec = None ) : if self . has_mass_profile : return sum ( map ( lambda p : p . einstein_radius_in_units ( unit_length = unit_length , kpc_per_arcsec = kpc_per_arcsec ) , self . mass_profiles ) ) else : return None
363	def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]
1869	def MOVZX ( cpu , op0 , op1 ) : op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) )
1777	def TEST ( cpu , src1 , src2 ) : temp = src1 . read ( ) & src2 . read ( ) cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 cpu . ZF = temp == 0 cpu . PF = cpu . _calculate_parity_flag ( temp ) cpu . CF = False cpu . OF = False
6341	def sim ( self , src , tar ) : if src == tar : return 1.0 if not src or not tar : return 0.0 min_word , max_word = ( src , tar ) if len ( src ) < len ( tar ) else ( tar , src ) min_len = len ( min_word ) for i in range ( min_len , 0 , - 1 ) : if min_word [ : i ] == max_word [ : i ] : return i / min_len return 0.0
1	def nature_cnn ( unscaled_images , ** conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )
2897	def cancel ( self , success = False ) : self . success = success cancel = [ ] mask = Task . NOT_FINISHED_MASK for task in Task . Iterator ( self . task_tree , mask ) : cancel . append ( task ) for task in cancel : task . cancel ( )
1439	def update_sent_packet ( self , sent_pkt_size_bytes ) : self . update_count ( self . SENT_PKT_COUNT ) self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes )
2323	def forward ( self , pred , target ) : loss = th . FloatTensor ( [ 0 ] ) for i in range ( 1 , self . moments ) : mk_pred = th . mean ( th . pow ( pred , i ) , 0 ) mk_tar = th . mean ( th . pow ( target , i ) , 0 ) loss . add_ ( th . mean ( ( mk_pred - mk_tar ) ** 2 ) ) return loss
394	def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )
9714	async def connect ( host , port = 22223 , version = "1.19" , on_event = None , on_disconnect = None , timeout = 5 , loop = None , ) -> QRTConnection : loop = loop or asyncio . get_event_loop ( ) try : _ , protocol = await loop . create_connection ( lambda : QTMProtocol ( loop = loop , on_event = on_event , on_disconnect = on_disconnect ) , host , port , ) except ( ConnectionRefusedError , TimeoutError , OSError ) as exception : LOG . error ( exception ) return None try : await protocol . set_version ( version ) except QRTCommandException as exception : LOG . error ( Exception ) return None except TypeError as exception : LOG . error ( exception ) return None return QRTConnection ( protocol , timeout = timeout )
12184	def _add_parsley_ns ( cls , namespace_dict ) : namespace_dict . update ( { 'parslepy' : cls . LOCAL_NAMESPACE , 'parsley' : cls . LOCAL_NAMESPACE , } ) return namespace_dict
5419	def _google_v2_parse_arguments ( args ) : if ( args . zones and args . regions ) or ( not args . zones and not args . regions ) : raise ValueError ( 'Exactly one of --regions and --zones must be specified' ) if args . machine_type and ( args . min_cores or args . min_ram ) : raise ValueError ( '--machine-type not supported together with --min-cores or --min-ram.' )
8090	def textheight ( self , txt , width = None ) : w = width return self . textmetrics ( txt , width = w ) [ 1 ]
4193	def plot_frequencies ( self , mindB = None , maxdB = None , norm = True ) : from pylab import plot , title , xlim , grid , ylim , xlabel , ylabel self . compute_response ( norm = norm ) plot ( self . frequencies , self . response ) title ( "ENBW=%2.1f" % ( self . enbw ) ) ylabel ( 'Frequency response (dB)' ) xlabel ( 'Fraction of sampling frequency' ) xlim ( - 0.5 , 0.5 ) y0 , y1 = ylim ( ) if mindB : y0 = mindB if maxdB is not None : y1 = maxdB else : y1 = max ( self . response ) ylim ( y0 , y1 ) grid ( True )
5587	def extract_subset ( self , input_data_tiles = None , out_tile = None ) : if self . METADATA [ "data_type" ] == "raster" : mosaic = create_mosaic ( input_data_tiles ) return extract_from_array ( in_raster = prepare_array ( mosaic . data , nodata = self . nodata , dtype = self . output_params [ "dtype" ] ) , in_affine = mosaic . affine , out_tile = out_tile ) elif self . METADATA [ "data_type" ] == "vector" : return [ feature for feature in list ( chain . from_iterable ( [ features for _ , features in input_data_tiles ] ) ) if shape ( feature [ "geometry" ] ) . intersects ( out_tile . bbox ) ]
6566	def halfadder_gate ( variables , vartype = dimod . BINARY , name = 'HALF_ADDER' ) : variables = tuple ( variables ) if vartype is dimod . BINARY : configs = frozenset ( [ ( 0 , 0 , 0 , 0 ) , ( 0 , 1 , 1 , 0 ) , ( 1 , 0 , 1 , 0 ) , ( 1 , 1 , 0 , 1 ) ] ) else : configs = frozenset ( [ ( - 1 , - 1 , - 1 , - 1 ) , ( - 1 , + 1 , + 1 , - 1 ) , ( + 1 , - 1 , + 1 , - 1 ) , ( + 1 , + 1 , - 1 , + 1 ) ] ) def func ( augend , addend , sum_ , carry ) : total = ( augend > 0 ) + ( addend > 0 ) if total == 0 : return ( sum_ <= 0 ) and ( carry <= 0 ) elif total == 1 : return ( sum_ > 0 ) and ( carry <= 0 ) elif total == 2 : return ( sum_ <= 0 ) and ( carry > 0 ) else : raise ValueError ( "func recieved unexpected values" ) return Constraint ( func , configs , variables , vartype = vartype , name = name )
9080	def get_providers ( self , ** kwargs ) : if 'ids' in kwargs : ids = [ self . concept_scheme_uri_map . get ( id , id ) for id in kwargs [ 'ids' ] ] providers = [ self . providers [ k ] for k in self . providers . keys ( ) if k in ids ] else : providers = list ( self . providers . values ( ) ) if 'subject' in kwargs : providers = [ p for p in providers if kwargs [ 'subject' ] in p . metadata [ 'subject' ] ] return providers
3406	def ast2str ( expr , level = 0 , names = None ) : if isinstance ( expr , Expression ) : return ast2str ( expr . body , 0 , names ) if hasattr ( expr , "body" ) else "" elif isinstance ( expr , Name ) : return names . get ( expr . id , expr . id ) if names else expr . id elif isinstance ( expr , BoolOp ) : op = expr . op if isinstance ( op , Or ) : str_exp = " or " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) elif isinstance ( op , And ) : str_exp = " and " . join ( ast2str ( i , level + 1 , names ) for i in expr . values ) else : raise TypeError ( "unsupported operation " + op . __class__ . __name ) return "(" + str_exp + ")" if level else str_exp elif expr is None : return "" else : raise TypeError ( "unsupported operation " + repr ( expr ) )
11354	def record_add_field ( rec , tag , ind1 = '' , ind2 = '' , subfields = [ ] , controlfield_value = '' ) : if controlfield_value : doc = etree . Element ( "controlfield" , attrib = { "tag" : tag , } ) doc . text = unicode ( controlfield_value ) else : doc = etree . Element ( "datafield" , attrib = { "tag" : tag , "ind1" : ind1 , "ind2" : ind2 , } ) for code , value in subfields : field = etree . SubElement ( doc , "subfield" , attrib = { "code" : code } ) field . text = value rec . append ( doc ) return rec
11868	def strip_minidom_whitespace ( node ) : for child in node . childNodes : if child . nodeType == Node . TEXT_NODE : if child . nodeValue : child . nodeValue = child . nodeValue . strip ( ) elif child . nodeType == Node . ELEMENT_NODE : strip_minidom_whitespace ( child )
60	def union ( self , other ) : return BoundingBox ( x1 = min ( self . x1 , other . x1 ) , y1 = min ( self . y1 , other . y1 ) , x2 = max ( self . x2 , other . x2 ) , y2 = max ( self . y2 , other . y2 ) , )
891	def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )
10230	def flatten_list_abundance ( node : ListAbundance ) -> ListAbundance : return node . __class__ ( list ( chain . from_iterable ( ( flatten_list_abundance ( member ) . members if isinstance ( member , ListAbundance ) else [ member ] ) for member in node . members ) ) )
6343	def raw ( self ) : r doc_list = [ ] for doc in self . corpus : sent_list = [ ] for sent in doc : sent_list . append ( ' ' . join ( sent ) ) doc_list . append ( self . sent_split . join ( sent_list ) ) del sent_list return self . doc_split . join ( doc_list )
8427	def brewer_pal ( type = 'seq' , palette = 1 ) : def full_type_name ( text ) : abbrevs = { 'seq' : 'Sequential' , 'qual' : 'Qualitative' , 'div' : 'Diverging' } text = abbrevs . get ( text , text ) return text . title ( ) def number_to_palette_name ( ctype , n ) : n -= 1 palettes = sorted ( colorbrewer . COLOR_MAPS [ ctype ] . keys ( ) ) if n < len ( palettes ) : return palettes [ n ] raise ValueError ( "There are only '{}' palettes of type {}. " "You requested palette no. {}" . format ( len ( palettes ) , ctype , n + 1 ) ) def max_palette_colors ( type , palette_name ) : if type == 'Sequential' : return 9 elif type == 'Diverging' : return 11 else : qlimit = { 'Accent' : 8 , 'Dark2' : 8 , 'Paired' : 12 , 'Pastel1' : 9 , 'Pastel2' : 8 , 'Set1' : 9 , 'Set2' : 8 , 'Set3' : 12 } return qlimit [ palette_name ] type = full_type_name ( type ) if isinstance ( palette , int ) : palette_name = number_to_palette_name ( type , palette ) else : palette_name = palette nmax = max_palette_colors ( type , palette_name ) def _brewer_pal ( n ) : _n = n if n <= nmax else nmax try : bmap = colorbrewer . get_map ( palette_name , type , _n ) except ValueError as err : if 0 <= _n < 3 : bmap = colorbrewer . get_map ( palette_name , type , 3 ) else : raise err hex_colors = bmap . hex_colors [ : n ] if n > nmax : msg = ( "Warning message:" "Brewer palette {} has a maximum of {} colors" "Returning the palette you asked for with" "that many colors" . format ( palette_name , nmax ) ) warnings . warn ( msg ) hex_colors = hex_colors + [ None ] * ( n - nmax ) return hex_colors return _brewer_pal
12551	def dump_raw_data ( filename , data ) : if data . ndim == 3 : data = data . reshape ( [ data . shape [ 0 ] , data . shape [ 1 ] * data . shape [ 2 ] ] ) a = array . array ( 'f' ) for o in data : a . fromlist ( list ( o . flatten ( ) ) ) with open ( filename , 'wb' ) as rawf : a . tofile ( rawf )
4491	def clone ( args ) : osf = _setup_osf ( args ) project = osf . project ( args . project ) output_dir = args . project if args . output is not None : output_dir = args . output with tqdm ( unit = 'files' ) as pbar : for store in project . storages : prefix = os . path . join ( output_dir , store . name ) for file_ in store . files : path = file_ . path if path . startswith ( '/' ) : path = path [ 1 : ] path = os . path . join ( prefix , path ) if os . path . exists ( path ) and args . update : if checksum ( path ) == file_ . hashes . get ( 'md5' ) : continue directory , _ = os . path . split ( path ) makedirs ( directory , exist_ok = True ) with open ( path , "wb" ) as f : file_ . write_to ( f ) pbar . update ( )
6358	def lcsstr ( self , src , tar ) : lengths = np_zeros ( ( len ( src ) + 1 , len ( tar ) + 1 ) , dtype = np_int ) longest , i_longest = 0 , 0 for i in range ( 1 , len ( src ) + 1 ) : for j in range ( 1 , len ( tar ) + 1 ) : if src [ i - 1 ] == tar [ j - 1 ] : lengths [ i , j ] = lengths [ i - 1 , j - 1 ] + 1 if lengths [ i , j ] > longest : longest = lengths [ i , j ] i_longest = i else : lengths [ i , j ] = 0 return src [ i_longest - longest : i_longest ]
1780	def AAD ( cpu , imm = None ) : if imm is None : imm = 10 else : imm = imm . read ( ) cpu . AL += cpu . AH * imm cpu . AH = 0 cpu . _calculate_logic_flags ( 8 , cpu . AL )
8097	def path ( s , graph , path ) : def end ( n ) : r = n . r * 0.35 s . _ctx . oval ( n . x - r , n . y - r , r * 2 , r * 2 ) if path and len ( path ) > 1 and s . stroke : s . _ctx . nofill ( ) s . _ctx . stroke ( s . stroke . r , s . stroke . g , s . stroke . b , s . stroke . a ) if s . name != DEFAULT : s . _ctx . strokewidth ( s . strokewidth ) else : s . _ctx . strokewidth ( s . strokewidth * 2 ) first = True for id in path : n = graph [ id ] if first : first = False s . _ctx . beginpath ( n . x , n . y ) end ( n ) else : s . _ctx . lineto ( n . x , n . y ) s . _ctx . endpath ( ) end ( n )
6230	def draw_bbox ( self , projection_matrix = None , camera_matrix = None , all = True ) : projection_matrix = projection_matrix . astype ( 'f4' ) . tobytes ( ) camera_matrix = camera_matrix . astype ( 'f4' ) . tobytes ( ) self . bbox_program [ "m_proj" ] . write ( projection_matrix ) self . bbox_program [ "m_view" ] . write ( self . _view_matrix . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "m_cam" ] . write ( camera_matrix ) self . bbox_program [ "bb_min" ] . write ( self . bbox_min . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "bb_max" ] . write ( self . bbox_max . astype ( 'f4' ) . tobytes ( ) ) self . bbox_program [ "color" ] . value = ( 1.0 , 0.0 , 0.0 ) self . bbox_vao . render ( self . bbox_program ) if not all : return for node in self . root_nodes : node . draw_bbox ( projection_matrix , camera_matrix , self . bbox_program , self . bbox_vao )
1534	def create_socket_options ( ) : sys_config = system_config . get_sys_config ( ) opt_list = [ const . INSTANCE_NETWORK_WRITE_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_WRITE_BATCH_TIME_MS , const . INSTANCE_NETWORK_READ_BATCH_SIZE_BYTES , const . INSTANCE_NETWORK_READ_BATCH_TIME_MS , const . INSTANCE_NETWORK_OPTIONS_SOCKET_RECEIVED_BUFFER_SIZE_BYTES , const . INSTANCE_NETWORK_OPTIONS_SOCKET_SEND_BUFFER_SIZE_BYTES ] Log . debug ( "In create_socket_options()" ) try : value_lst = [ int ( sys_config [ opt ] ) for opt in opt_list ] sock_opt = SocketOptions ( * value_lst ) return sock_opt except ValueError as e : raise ValueError ( "Invalid value in sys_config: %s" % str ( e ) ) except KeyError as e : raise KeyError ( "Incomplete sys_config: %s" % str ( e ) )
7681	def piano_roll ( annotation , ** kwargs ) : times , midi = annotation . to_interval_values ( ) return mir_eval . display . piano_roll ( times , midi = midi , ** kwargs )
12915	def filelist ( self ) : if len ( self . _filelist ) == 0 : for item in self . _data : if isinstance ( self . _data [ item ] , filetree ) : self . _filelist . extend ( self . _data [ item ] . filelist ( ) ) else : self . _filelist . append ( self . _data [ item ] ) return self . _filelist
8599	def get_share ( self , group_id , resource_id , depth = 1 ) : response = self . _perform_request ( '/um/groups/%s/shares/%s?depth=%s' % ( group_id , resource_id , str ( depth ) ) ) return response
2017	def DIV ( self , a , b ) : try : result = Operators . UDIV ( a , b ) except ZeroDivisionError : result = 0 return Operators . ITEBV ( 256 , b == 0 , 0 , result )
1624	def CloseExpression ( clean_lines , linenum , pos ) : line = clean_lines . elided [ linenum ] if ( line [ pos ] not in '({[<' ) or Match ( r'<[<=]' , line [ pos : ] ) : return ( line , clean_lines . NumLines ( ) , - 1 ) ( end_pos , stack ) = FindEndOfExpressionInLine ( line , pos , [ ] ) if end_pos > - 1 : return ( line , linenum , end_pos ) while stack and linenum < clean_lines . NumLines ( ) - 1 : linenum += 1 line = clean_lines . elided [ linenum ] ( end_pos , stack ) = FindEndOfExpressionInLine ( line , 0 , stack ) if end_pos > - 1 : return ( line , linenum , end_pos ) return ( line , clean_lines . NumLines ( ) , - 1 )
5778	def _bcrypt_encrypt ( certificate_or_public_key , data , rsa_oaep_padding = False ) : flags = BcryptConst . BCRYPT_PAD_PKCS1 if rsa_oaep_padding is True : flags = BcryptConst . BCRYPT_PAD_OAEP padding_info_struct_pointer = struct ( bcrypt , 'BCRYPT_OAEP_PADDING_INFO' ) padding_info_struct = unwrap ( padding_info_struct_pointer ) hash_buffer = buffer_from_unicode ( BcryptConst . BCRYPT_SHA1_ALGORITHM ) padding_info_struct . pszAlgId = cast ( bcrypt , 'wchar_t *' , hash_buffer ) padding_info_struct . pbLabel = null ( ) padding_info_struct . cbLabel = 0 padding_info = cast ( bcrypt , 'void *' , padding_info_struct_pointer ) else : padding_info = null ( ) out_len = new ( bcrypt , 'ULONG *' ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , null ( ) , 0 , out_len , flags ) handle_error ( res ) buffer_len = deref ( out_len ) buffer = buffer_from_bytes ( buffer_len ) res = bcrypt . BCryptEncrypt ( certificate_or_public_key . key_handle , data , len ( data ) , padding_info , null ( ) , 0 , buffer , buffer_len , out_len , flags ) handle_error ( res ) return bytes_from_buffer ( buffer , deref ( out_len ) )
9455	def schedule_play ( self , call_params ) : path = '/' + self . api_version + '/SchedulePlay/' method = 'POST' return self . request ( path , method , call_params )
4228	def _config_root_Linux ( ) : _check_old_config_root ( ) fallback = os . path . expanduser ( '~/.local/share' ) key = 'XDG_CONFIG_HOME' root = os . environ . get ( key , None ) or fallback return os . path . join ( root , 'python_keyring' )
13830	def _url ( self ) : if self . _api_arg : mypart = str ( self . _api_arg ) else : mypart = self . _name if self . _parent : return '/' . join ( filter ( None , [ self . _parent . _url , mypart ] ) ) else : return mypart
322	def get_top_drawdowns ( returns , top = 10 ) : returns = returns . copy ( ) df_cum = ep . cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 drawdowns = [ ] for t in range ( top ) : peak , valley , recovery = get_max_drawdown_underwater ( underwater ) if not pd . isnull ( recovery ) : underwater . drop ( underwater [ peak : recovery ] . index [ 1 : - 1 ] , inplace = True ) else : underwater = underwater . loc [ : peak ] drawdowns . append ( ( peak , valley , recovery ) ) if ( len ( returns ) == 0 ) or ( len ( underwater ) == 0 ) : break return drawdowns
84	def CoarsePepper ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : mask = iap . handle_probability_param ( p , "p" , tuple_to_uniform = True , list_to_choice = True ) if size_px is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_px = size_px , min_size = min_size ) elif size_percent is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_percent = size_percent , min_size = min_size ) else : raise Exception ( "Either size_px or size_percent must be set." ) replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = "invert" ) + 0.5 replacement = replacement01 * 255 if name is None : name = "Unnamed%s" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = mask_low , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )
13782	def _ConvertEnumDescriptor ( self , enum_proto , package = None , file_desc = None , containing_type = None , scope = None ) : if package : enum_name = '.' . join ( ( package , enum_proto . name ) ) else : enum_name = enum_proto . name if file_desc is None : file_name = None else : file_name = file_desc . name values = [ self . _MakeEnumValueDescriptor ( value , index ) for index , value in enumerate ( enum_proto . value ) ] desc = descriptor . EnumDescriptor ( name = enum_proto . name , full_name = enum_name , filename = file_name , file = file_desc , values = values , containing_type = containing_type , options = enum_proto . options ) scope [ '.%s' % enum_name ] = desc self . _enum_descriptors [ enum_name ] = desc return desc
10447	def activatewindow ( self , window_name ) : window_handle = self . _get_window_handle ( window_name ) self . _grabfocus ( window_handle ) return 1
9732	def get_6d ( self , component_info = None , data = None , component_position = None ) : components = [ ] append_components = components . append for _ in range ( component_info . body_count ) : component_position , position = QRTPacket . _get_exact ( RT6DBodyPosition , data , component_position ) component_position , matrix = QRTPacket . _get_tuple ( RT6DBodyRotation , data , component_position ) append_components ( ( position , matrix ) ) return components
9471	def _xml ( self , root ) : element = root . createElement ( self . name ) keys = self . attrs . keys ( ) keys . sort ( ) for a in keys : element . setAttribute ( a , self . attrs [ a ] ) if self . body : text = root . createTextNode ( self . body ) element . appendChild ( text ) for c in self . elements : element . appendChild ( c . _xml ( root ) ) return element
5321	def get_data ( self , reset_device = False ) : try : if reset_device : self . _device . reset ( ) for interface in [ 0 , 1 ] : if self . _device . is_kernel_driver_active ( interface ) : LOGGER . debug ( 'Detaching kernel driver for interface %d ' 'of %r on ports %r' , interface , self . _device , self . _ports ) self . _device . detach_kernel_driver ( interface ) self . _device . set_configuration ( ) usb . util . claim_interface ( self . _device , INTERFACE ) self . _control_transfer ( COMMANDS [ 'temp' ] ) self . _interrupt_read ( ) self . _control_transfer ( COMMANDS [ 'temp' ] ) temp_data = self . _interrupt_read ( ) if self . _device . product == 'TEMPer1F_H1_V1.4' : humidity_data = temp_data else : humidity_data = None data = { 'temp_data' : temp_data , 'humidity_data' : humidity_data } usb . util . dispose_resources ( self . _device ) return data except usb . USBError as err : if not reset_device : LOGGER . warning ( "Encountered %s, resetting %r and trying again." , err , self . _device ) return self . get_data ( True ) if "not permitted" in str ( err ) : raise Exception ( "Permission problem accessing USB. " "Maybe I need to run as root?" ) else : LOGGER . error ( err ) raise
10949	def get_shares ( self ) : self . shares = self . api . get ( url = PATHS [ 'GET_SHARES' ] % self . url ) [ 'shares' ] return self . shares
5114	def clear_data ( self , queues = None , edge = None , edge_type = None ) : queues = _get_queues ( self . g , queues , edge , edge_type ) for k in queues : self . edge2queue [ k ] . data = { }
3271	def resolution_millis ( self ) : if self . resolution is None or not isinstance ( self . resolution , basestring ) : return self . resolution val , mult = self . resolution . split ( ' ' ) return int ( float ( val ) * self . _multipier ( mult ) * 1000 )
9037	def walk_connections ( self , mapping = identity ) : for start in self . walk_instructions ( ) : for stop_instruction in start . instruction . consuming_instructions : if stop_instruction is None : continue stop = self . _walk . instruction_in_grid ( stop_instruction ) connection = Connection ( start , stop ) if connection . is_visible ( ) : yield mapping ( connection )
9155	def scaling ( self , x , y ) : self . drawer . append ( pgmagick . DrawableScaling ( float ( x ) , float ( y ) ) )
7082	def fourier_sinusoidal_func ( fourierparams , times , mags , errs ) : period , epoch , famps , fphases = fourierparams forder = len ( famps ) iphase = ( times - epoch ) / period iphase = iphase - np . floor ( iphase ) phasesortind = np . argsort ( iphase ) phase = iphase [ phasesortind ] ptimes = times [ phasesortind ] pmags = mags [ phasesortind ] perrs = errs [ phasesortind ] fseries = [ famps [ x ] * np . cos ( 2.0 * np . pi * x * phase + fphases [ x ] ) for x in range ( forder ) ] modelmags = np . median ( mags ) for fo in fseries : modelmags += fo return modelmags , phase , ptimes , pmags , perrs
10072	def record_schema ( self ) : schema_path = current_jsonschemas . url_to_path ( self [ '$schema' ] ) schema_prefix = current_app . config [ 'DEPOSIT_JSONSCHEMAS_PREFIX' ] if schema_path and schema_path . startswith ( schema_prefix ) : return current_jsonschemas . path_to_url ( schema_path [ len ( schema_prefix ) : ] )
8609	def list_resources ( self , resource_type = None , depth = 1 ) : if resource_type is not None : response = self . _perform_request ( '/um/resources/%s?depth=%s' % ( resource_type , str ( depth ) ) ) else : response = self . _perform_request ( '/um/resources?depth=' + str ( depth ) ) return response
11897	def _create_index_files ( root_dir , force_no_processing = False ) : created_files = [ ] for here , dirs , files in os . walk ( root_dir ) : print ( 'Processing %s' % here ) dirs = sorted ( dirs ) image_files = [ f for f in files if re . match ( IMAGE_FILE_REGEX , f ) ] image_files = sorted ( image_files ) created_files . append ( _create_index_file ( root_dir , here , image_files , dirs , force_no_processing ) ) return created_files
1761	def read_string ( self , where , max_length = None , force = False ) : s = io . BytesIO ( ) while True : c = self . read_int ( where , 8 , force ) if issymbolic ( c ) or c == 0 : break if max_length is not None : if max_length == 0 : break max_length = max_length - 1 s . write ( Operators . CHR ( c ) ) where += 1 return s . getvalue ( ) . decode ( )
1371	def get_heron_dir ( ) : go_above_dirs = 9 path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) return normalized_class_path ( path )
12798	def _fetch ( self , method , url = None , post_data = None , parse_data = True , key = None , parameters = None , listener = None , full_return = False ) : headers = self . get_headers ( ) headers [ "Content-Type" ] = "application/json" handlers = [ ] debuglevel = int ( self . _settings [ "debug" ] ) handlers . append ( urllib2 . HTTPHandler ( debuglevel = debuglevel ) ) if hasattr ( httplib , "HTTPS" ) : handlers . append ( urllib2 . HTTPSHandler ( debuglevel = debuglevel ) ) handlers . append ( urllib2 . HTTPCookieProcessor ( cookielib . CookieJar ( ) ) ) password_url = self . _get_password_url ( ) if password_url and "Authorization" not in headers : pwd_manager = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) pwd_manager . add_password ( None , password_url , self . _settings [ "user" ] , self . _settings [ "password" ] ) handlers . append ( HTTPBasicAuthHandler ( pwd_manager ) ) opener = urllib2 . build_opener ( * handlers ) if post_data is not None : post_data = json . dumps ( post_data ) uri = self . _url ( url , parameters ) request = RESTRequest ( uri , method = method , headers = headers ) if post_data is not None : request . add_data ( post_data ) response = None try : response = opener . open ( request ) body = response . read ( ) if password_url and password_url not in self . _settings [ "authorizations" ] and request . has_header ( "Authorization" ) : self . _settings [ "authorizations" ] [ password_url ] = request . get_header ( "Authorization" ) except urllib2 . HTTPError as e : if e . code == 401 : raise AuthenticationError ( "Access denied while trying to access %s" % uri ) elif e . code == 404 : raise ConnectionError ( "URL not found: %s" % uri ) else : raise except urllib2 . URLError as e : raise ConnectionError ( "Error while fetching from %s: %s" % ( uri , e ) ) finally : if response : response . close ( ) opener . close ( ) data = None if parse_data : if not key : key = string . split ( url , "/" ) [ 0 ] data = self . parse ( body , key ) if full_return : info = response . info ( ) if response else None status = int ( string . split ( info [ "status" ] ) [ 0 ] ) if ( info and "status" in info ) else None return { "success" : ( status >= 200 and status < 300 ) , "data" : data , "info" : info , "body" : body } return data
5239	def market_open ( self , session , mins ) -> Session : if session not in self . exch : return SessNA start_time = self . exch [ session ] [ 0 ] return Session ( start_time , shift_time ( start_time , int ( mins ) ) )
2999	def sectorPerformanceDF ( token = '' , version = '' ) : df = pd . DataFrame ( sectorPerformance ( token , version ) ) _toDatetime ( df ) _reindex ( df , 'name' ) return df
5057	def build_notification_message ( template_context , template_configuration = None ) : if ( template_configuration is not None and template_configuration . html_template and template_configuration . plaintext_template ) : plain_msg , html_msg = template_configuration . render_all_templates ( template_context ) else : plain_msg = render_to_string ( 'enterprise/emails/user_notification.txt' , template_context ) html_msg = render_to_string ( 'enterprise/emails/user_notification.html' , template_context ) return plain_msg , html_msg
5204	def parse_markdown ( ) : readme_file = f'{PACKAGE_ROOT}/README.md' if path . exists ( readme_file ) : with open ( readme_file , 'r' , encoding = 'utf-8' ) as f : long_description = f . read ( ) return long_description
5316	def setup ( self , colormode = None , colorpalette = None , extend_colors = False ) : if colormode : self . colormode = colormode if colorpalette : if extend_colors : self . update_palette ( colorpalette ) else : self . colorpalette = colorpalette
3032	def credentials_from_code ( client_id , client_secret , scope , code , redirect_uri = 'postmessage' , http = None , user_agent = None , token_uri = oauth2client . GOOGLE_TOKEN_URI , auth_uri = oauth2client . GOOGLE_AUTH_URI , revoke_uri = oauth2client . GOOGLE_REVOKE_URI , device_uri = oauth2client . GOOGLE_DEVICE_URI , token_info_uri = oauth2client . GOOGLE_TOKEN_INFO_URI , pkce = False , code_verifier = None ) : flow = OAuth2WebServerFlow ( client_id , client_secret , scope , redirect_uri = redirect_uri , user_agent = user_agent , auth_uri = auth_uri , token_uri = token_uri , revoke_uri = revoke_uri , device_uri = device_uri , token_info_uri = token_info_uri , pkce = pkce , code_verifier = code_verifier ) credentials = flow . step2_exchange ( code , http = http ) return credentials
1461	def import_and_get_class ( path_to_pex , python_class_name ) : abs_path_to_pex = os . path . abspath ( path_to_pex ) Log . debug ( "Add a pex to the path: %s" % abs_path_to_pex ) Log . debug ( "In import_and_get_class with cls_name: %s" % python_class_name ) split = python_class_name . split ( '.' ) from_path = '.' . join ( split [ : - 1 ] ) import_name = python_class_name . split ( '.' ) [ - 1 ] Log . debug ( "From path: %s, import name: %s" % ( from_path , import_name ) ) if python_class_name . startswith ( "heron." ) : try : mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) return getattr ( mod , import_name ) except : Log . error ( "Could not resolve class %s with special handling" % python_class_name ) mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) Log . debug ( "Imported module: %s" % str ( mod ) ) return getattr ( mod , import_name )
7081	def send_email ( sender , subject , content , email_recipient_list , email_address_list , email_user = None , email_pass = None , email_server = None ) : if not email_user : email_user = EMAIL_USER if not email_pass : email_pass = EMAIL_PASSWORD if not email_server : email_server = EMAIL_SERVER if not email_server and email_user and email_pass : raise ValueError ( "no email server address and " "credentials available, can't continue" ) msg_text = EMAIL_TEMPLATE . format ( sender = sender , hostname = socket . gethostname ( ) , activity_time = '%sZ' % datetime . utcnow ( ) . isoformat ( ) , activity_report = content ) email_sender = '%s <%s>' % ( sender , EMAIL_USER ) email_recipients = [ ( '%s <%s>' % ( x , y ) ) for ( x , y ) in zip ( email_recipient_list , email_address_list ) ] email_msg = MIMEText ( msg_text ) email_msg [ 'From' ] = email_sender email_msg [ 'To' ] = ', ' . join ( email_recipients ) email_msg [ 'Message-Id' ] = make_msgid ( ) email_msg [ 'Subject' ] = '[%s on %s] %s' % ( sender , socket . gethostname ( ) , subject ) email_msg [ 'Date' ] = formatdate ( time . time ( ) ) try : server = smtplib . SMTP ( EMAIL_SERVER , 587 ) server_ehlo_response = server . ehlo ( ) if server . has_extn ( 'STARTTLS' ) : try : tls_start_response = server . starttls ( ) tls_ehlo_response = server . ehlo ( ) login_response = server . login ( EMAIL_USER , EMAIL_PASSWORD ) send_response = ( server . sendmail ( email_sender , email_address_list , email_msg . as_string ( ) ) ) except Exception as e : print ( 'script email sending failed with error: %s' % e ) send_response = None if send_response is not None : print ( 'script email sent successfully' ) quit_response = server . quit ( ) return True else : quit_response = server . quit ( ) return False else : print ( 'email server does not support STARTTLS,' ' bailing out...' ) quit_response = server . quit ( ) return False except Exception as e : print ( 'sending email failed with error: %s' % e ) returnval = False quit_response = server . quit ( ) return returnval
2029	def CALLDATACOPY ( self , mem_offset , data_offset , size ) : if issymbolic ( size ) : if solver . can_be_true ( self . _constraints , size <= len ( self . data ) + 32 ) : self . constraints . add ( size <= len ( self . data ) + 32 ) raise ConcretizeArgument ( 3 , policy = 'SAMPLED' ) if issymbolic ( data_offset ) : if solver . can_be_true ( self . _constraints , data_offset == self . _used_calldata_size ) : self . constraints . add ( data_offset == self . _used_calldata_size ) raise ConcretizeArgument ( 2 , policy = 'SAMPLED' ) self . _use_calldata ( data_offset , size ) self . _allocate ( mem_offset , size ) for i in range ( size ) : try : c = Operators . ITEBV ( 8 , data_offset + i < len ( self . data ) , Operators . ORD ( self . data [ data_offset + i ] ) , 0 ) except IndexError : c = 0 self . _store ( mem_offset + i , c )
65	def clip_out_of_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] ia . do_assert ( height > 0 ) ia . do_assert ( width > 0 ) eps = np . finfo ( np . float32 ) . eps x1 = np . clip ( self . x1 , 0 , width - eps ) x2 = np . clip ( self . x2 , 0 , width - eps ) y1 = np . clip ( self . y1 , 0 , height - eps ) y2 = np . clip ( self . y2 , 0 , height - eps ) return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = self . label )
5228	def load_info ( cat ) : res = _load_yaml_ ( f'{PKG_PATH}/markets/{cat}.yml' ) root = os . environ . get ( 'BBG_ROOT' , '' ) . replace ( '\\' , '/' ) if not root : return res for cat , ovrd in _load_yaml_ ( f'{root}/markets/{cat}.yml' ) . items ( ) : if isinstance ( ovrd , dict ) : if cat in res : res [ cat ] . update ( ovrd ) else : res [ cat ] = ovrd if isinstance ( ovrd , list ) and isinstance ( res [ cat ] , list ) : res [ cat ] += ovrd return res
6961	def _time_independent_equals ( a , b ) : if len ( a ) != len ( b ) : return False result = 0 if isinstance ( a [ 0 ] , int ) : for x , y in zip ( a , b ) : result |= x ^ y else : for x , y in zip ( a , b ) : result |= ord ( x ) ^ ord ( y ) return result == 0
3618	def set_settings ( self ) : if not self . settings : return try : self . __index . set_settings ( self . settings ) logger . info ( 'APPLY SETTINGS ON %s' , self . index_name ) except AlgoliaException as e : if DEBUG : raise e else : logger . warning ( 'SETTINGS NOT APPLIED ON %s: %s' , self . model , e )
5591	def tiles_from_bounds ( self , bounds , zoom ) : for tile in self . tiles_from_bbox ( box ( * bounds ) , zoom ) : yield self . tile ( * tile . id )
9503	def remove_contained_in_list ( l ) : i = 0 l . sort ( ) while i < len ( l ) - 1 : if l [ i + 1 ] . contains ( l [ i ] ) : l . pop ( i ) elif l [ i ] . contains ( l [ i + 1 ] ) : l . pop ( i + 1 ) else : i += 1
5846	def load_file_as_yaml ( path ) : with open ( path , "r" ) as f : raw_yaml = f . read ( ) parsed_dict = yaml . load ( raw_yaml ) return parsed_dict
2255	def unique_flags ( items , key = None ) : len_ = len ( items ) if key is None : item_to_index = dict ( zip ( reversed ( items ) , reversed ( range ( len_ ) ) ) ) indices = item_to_index . values ( ) else : indices = argunique ( items , key = key ) flags = boolmask ( indices , len_ ) return flags
9245	def encapsulate_string ( raw_string ) : raw_string . replace ( '\\' , '\\\\' ) enc_string = re . sub ( "([<>*_()\[\]#])" , r"\\\1" , raw_string ) return enc_string
10775	def _should_use_transaction_isolation ( self , test , settings ) : if not getattr ( test . context , 'use_transaction_isolation' , True ) : return False if getattr ( settings , 'DISABLE_TRANSACTION_MANAGEMENT' , False ) : return False if hasattr ( settings , 'DATABASE_SUPPORTS_TRANSACTIONS' ) : if not settings . DATABASE_SUPPORTS_TRANSACTIONS : return False return True
3193	def update ( self , list_id , subscriber_hash , data ) : subscriber_hash = check_subscriber_hash ( subscriber_hash ) self . list_id = list_id self . subscriber_hash = subscriber_hash return self . _mc_client . _patch ( url = self . _build_path ( list_id , 'members' , subscriber_hash ) , data = data )
9400	def _feval ( self , func_name , func_args = ( ) , dname = '' , nout = 0 , timeout = None , stream_handler = None , store_as = '' , plot_dir = None ) : engine = self . _engine if engine is None : raise Oct2PyError ( 'Session is closed' ) out_file = osp . join ( self . temp_dir , 'writer.mat' ) out_file = out_file . replace ( osp . sep , '/' ) in_file = osp . join ( self . temp_dir , 'reader.mat' ) in_file = in_file . replace ( osp . sep , '/' ) func_args = list ( func_args ) ref_indices = [ ] for ( i , value ) in enumerate ( func_args ) : if isinstance ( value , OctavePtr ) : ref_indices . append ( i + 1 ) func_args [ i ] = value . address ref_indices = np . array ( ref_indices ) req = dict ( func_name = func_name , func_args = tuple ( func_args ) , dname = dname or '' , nout = nout , store_as = store_as or '' , ref_indices = ref_indices ) write_file ( req , out_file , oned_as = self . _oned_as , convert_to_float = self . convert_to_float ) engine . stream_handler = stream_handler or self . logger . info if timeout is None : timeout = self . timeout try : engine . eval ( '_pyeval("%s", "%s");' % ( out_file , in_file ) , timeout = timeout ) except KeyboardInterrupt as e : stream_handler ( engine . repl . interrupt ( ) ) raise except TIMEOUT : stream_handler ( engine . repl . interrupt ( ) ) raise Oct2PyError ( 'Timed out, interrupting' ) except EOF : stream_handler ( engine . repl . child . before ) self . restart ( ) raise Oct2PyError ( 'Session died, restarting' ) resp = read_file ( in_file , self ) if resp [ 'err' ] : msg = self . _parse_error ( resp [ 'err' ] ) raise Oct2PyError ( msg ) result = resp [ 'result' ] . ravel ( ) . tolist ( ) if isinstance ( result , list ) and len ( result ) == 1 : result = result [ 0 ] if ( isinstance ( result , Cell ) and result . size == 1 and isinstance ( result [ 0 ] , string_types ) and result [ 0 ] == '__no_value__' ) : result = None if plot_dir : self . _engine . make_figures ( plot_dir ) return result
2986	def get_cors_options ( appInstance , * dicts ) : options = DEFAULT_OPTIONS . copy ( ) options . update ( get_app_kwarg_dict ( appInstance ) ) if dicts : for d in dicts : options . update ( d ) return serialize_options ( options )
12689	def send ( * args , ** kwargs ) : queue_flag = kwargs . pop ( "queue" , False ) now_flag = kwargs . pop ( "now" , False ) assert not ( queue_flag and now_flag ) , "'queue' and 'now' cannot both be True." if queue_flag : return queue ( * args , ** kwargs ) elif now_flag : return send_now ( * args , ** kwargs ) else : if QUEUE_ALL : return queue ( * args , ** kwargs ) else : return send_now ( * args , ** kwargs )
4351	def join ( self , room ) : self . socket . rooms . add ( self . _get_room_name ( room ) )
2182	def rebuild_auth ( self , prepared_request , response ) : if "Authorization" in prepared_request . headers : prepared_request . headers . pop ( "Authorization" , True ) prepared_request . prepare_auth ( self . auth ) return
246	def apply_slippage_penalty ( returns , txn_daily , simulate_starting_capital , backtest_starting_capital , impact = 0.1 ) : mult = simulate_starting_capital / backtest_starting_capital simulate_traded_shares = abs ( mult * txn_daily . amount ) simulate_traded_dollars = txn_daily . price * simulate_traded_shares simulate_pct_volume_used = simulate_traded_shares / txn_daily . volume penalties = simulate_pct_volume_used ** 2 * impact * simulate_traded_dollars daily_penalty = penalties . resample ( 'D' ) . sum ( ) daily_penalty = daily_penalty . reindex ( returns . index ) . fillna ( 0 ) portfolio_value = ep . cum_returns ( returns , starting_value = backtest_starting_capital ) * mult adj_returns = returns - ( daily_penalty / portfolio_value ) return adj_returns
7887	def filter_mechanism_list ( mechanisms , properties , allow_insecure = False , server_side = False ) : result = [ ] for mechanism in mechanisms : try : if server_side : klass = SERVER_MECHANISMS_D [ mechanism ] else : klass = CLIENT_MECHANISMS_D [ mechanism ] except KeyError : logger . debug ( " skipping {0} - not supported" . format ( mechanism ) ) continue secure = properties . get ( "security-layer" ) if not allow_insecure and not klass . _pyxmpp_sasl_secure and not secure : logger . debug ( " skipping {0}, as it is not secure" . format ( mechanism ) ) continue if not klass . are_properties_sufficient ( properties ) : logger . debug ( " skipping {0}, as the properties are not sufficient" . format ( mechanism ) ) continue result . append ( mechanism ) return result
13450	def field_value ( self , admin_model , instance , field_name ) : _ , _ , value = lookup_field ( field_name , instance , admin_model ) return value
11330	def get_record ( self , record ) : self . document = record rec = create_record ( ) language = self . _get_language ( ) if language and language != 'en' : record_add_field ( rec , '041' , subfields = [ ( 'a' , language ) ] ) publisher = self . _get_publisher ( ) date = self . _get_date ( ) if publisher and date : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) , ( 'c' , date ) ] ) elif publisher : record_add_field ( rec , '260' , subfields = [ ( 'b' , publisher ) ] ) elif date : record_add_field ( rec , '260' , subfields = [ ( 'c' , date ) ] ) title = self . _get_title ( ) if title : record_add_field ( rec , '245' , subfields = [ ( 'a' , title ) ] ) record_copyright = self . _get_copyright ( ) if record_copyright : record_add_field ( rec , '540' , subfields = [ ( 'a' , record_copyright ) ] ) subject = self . _get_subject ( ) if subject : record_add_field ( rec , '650' , ind1 = '1' , ind2 = '7' , subfields = [ ( 'a' , subject ) , ( '2' , 'PoS' ) ] ) authors = self . _get_authors ( ) first_author = True for author in authors : subfields = [ ( 'a' , author [ 0 ] ) ] for affiliation in author [ 1 ] : subfields . append ( ( 'v' , affiliation ) ) if first_author : record_add_field ( rec , '100' , subfields = subfields ) first_author = False else : record_add_field ( rec , '700' , subfields = subfields ) identifier = self . get_identifier ( ) conference = identifier . split ( ':' ) [ 2 ] conference = conference . split ( '/' ) [ 0 ] contribution = identifier . split ( ':' ) [ 2 ] contribution = contribution . split ( '/' ) [ 1 ] record_add_field ( rec , '773' , subfields = [ ( 'p' , 'PoS' ) , ( 'v' , conference . replace ( ' ' , '' ) ) , ( 'c' , contribution ) , ( 'y' , date [ : 4 ] ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'ConferencePaper' ) ] ) record_add_field ( rec , '980' , subfields = [ ( 'a' , 'HEP' ) ] ) return rec
12118	def abfinfo ( self , printToo = False , returnDict = False ) : info = "\n### ABF INFO ###\n" d = { } for thingName in sorted ( dir ( self ) ) : if thingName in [ 'cm' , 'evIs' , 'colormap' , 'dataX' , 'dataY' , 'protoX' , 'protoY' ] : continue if "_" in thingName : continue thing = getattr ( self , thingName ) if type ( thing ) is list and len ( thing ) > 5 : continue thingType = str ( type ( thing ) ) . split ( "'" ) [ 1 ] if "method" in thingType or "neo." in thingType : continue if thingName in [ "header" , "MT" ] : continue info += "%s <%s> %s\n" % ( thingName , thingType , thing ) d [ thingName ] = thing if printToo : print ( ) for line in info . split ( "\n" ) : if len ( line ) < 3 : continue print ( " " , line ) print ( ) if returnDict : return d return info
4629	def from_pubkey ( cls , pubkey , compressed = True , version = 56 , prefix = None ) : pubkey = PublicKey ( pubkey , prefix = prefix or Prefix . prefix ) if compressed : pubkey_plain = pubkey . compressed ( ) else : pubkey_plain = pubkey . uncompressed ( ) sha = hashlib . sha256 ( unhexlify ( pubkey_plain ) ) . hexdigest ( ) rep = hexlify ( ripemd160 ( sha ) ) . decode ( "ascii" ) s = ( "%.2x" % version ) + rep result = s + hexlify ( doublesha256 ( s ) [ : 4 ] ) . decode ( "ascii" ) result = hexlify ( ripemd160 ( result ) ) . decode ( "ascii" ) return cls ( result , prefix = pubkey . prefix )
5003	def handle ( self , * args , ** options ) : LOGGER . info ( 'Starting assigning enterprise roles to users!' ) role = options [ 'role' ] if role == ENTERPRISE_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_admin_users_batch , options ) elif role == ENTERPRISE_OPERATOR_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_operator_users_batch , options ) elif role == ENTERPRISE_LEARNER_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_customer_users_batch , options ) elif role == ENTERPRISE_ENROLLMENT_API_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_enrollment_api_admin_users_batch , options , True ) elif role == ENTERPRISE_CATALOG_ADMIN_ROLE : self . _assign_enterprise_role_to_users ( self . _get_enterprise_catalog_admin_users_batch , options , True ) else : raise CommandError ( 'Please provide a valid role name. Supported roles are {admin} and {learner}' . format ( admin = ENTERPRISE_ADMIN_ROLE , learner = ENTERPRISE_LEARNER_ROLE ) ) LOGGER . info ( 'Successfully finished assigning enterprise roles to users!' )
9082	def get_all ( self , ** kwargs ) : kwarguments = { } if 'language' in kwargs : kwarguments [ 'language' ] = kwargs [ 'language' ] return [ { 'id' : p . get_vocabulary_id ( ) , 'concepts' : p . get_all ( ** kwarguments ) } for p in self . providers . values ( ) ]
7946	def _write ( self , data ) : OUT_LOGGER . debug ( "OUT: %r" , data ) if self . _hup or not self . _socket : raise PyXMPPIOError ( u"Connection closed." ) try : while data : try : sent = self . _socket . send ( data ) except ssl . SSLError , err : if err . args [ 0 ] == ssl . SSL_ERROR_WANT_WRITE : continue else : raise except socket . error , err : if err . args [ 0 ] == errno . EINTR : continue if err . args [ 0 ] in BLOCKING_ERRORS : wait_for_write ( self . _socket ) continue raise data = data [ sent : ] except ( IOError , OSError , socket . error ) , err : raise PyXMPPIOError ( u"IO Error: {0}" . format ( err ) )
5334	def get_params_parser ( ) : parser = argparse . ArgumentParser ( add_help = False ) parser . add_argument ( '-g' , '--debug' , dest = 'debug' , action = 'store_true' , help = argparse . SUPPRESS ) parser . add_argument ( "--arthur" , action = 'store_true' , dest = 'arthur' , help = "Enable arthur to collect raw data" ) parser . add_argument ( "--raw" , action = 'store_true' , dest = 'raw' , help = "Activate raw task" ) parser . add_argument ( "--enrich" , action = 'store_true' , dest = 'enrich' , help = "Activate enrich task" ) parser . add_argument ( "--identities" , action = 'store_true' , dest = 'identities' , help = "Activate merge identities task" ) parser . add_argument ( "--panels" , action = 'store_true' , dest = 'panels' , help = "Activate panels task" ) parser . add_argument ( "--cfg" , dest = 'cfg_path' , help = "Configuration file path" ) parser . add_argument ( "--backends" , dest = 'backend_sections' , default = [ ] , nargs = '*' , help = "Backend sections to execute" ) if len ( sys . argv ) == 1 : parser . print_help ( ) sys . exit ( 1 ) return parser
9170	def declare_api_routes ( config ) : add_route = config . add_route add_route ( 'get-content' , '/contents/{ident_hash}' ) add_route ( 'get-resource' , '/resources/{hash}' ) add_route ( 'license-request' , '/contents/{uuid}/licensors' ) add_route ( 'roles-request' , '/contents/{uuid}/roles' ) add_route ( 'acl-request' , '/contents/{uuid}/permissions' ) add_route ( 'publications' , '/publications' ) add_route ( 'get-publication' , '/publications/{id}' ) add_route ( 'publication-license-acceptance' , '/publications/{id}/license-acceptances/{uid}' ) add_route ( 'publication-role-acceptance' , '/publications/{id}/role-acceptances/{uid}' ) add_route ( 'collate-content' , '/contents/{ident_hash}/collate-content' ) add_route ( 'bake-content' , '/contents/{ident_hash}/baked' ) add_route ( 'moderation' , '/moderations' ) add_route ( 'moderate' , '/moderations/{id}' ) add_route ( 'moderation-rss' , '/feeds/moderations.rss' ) add_route ( 'api-keys' , '/api-keys' ) add_route ( 'api-key' , '/api-keys/{id}' )
8858	def on_goto_out_of_doc ( self , assignment ) : editor = self . open_file ( assignment . module_path ) if editor : TextHelper ( editor ) . goto_line ( assignment . line , assignment . column )
5870	def fetch_organization_courses ( organization ) : organization_obj = serializers . deserialize_organization ( organization ) queryset = internal . OrganizationCourse . objects . filter ( organization = organization_obj , active = True ) . select_related ( 'organization' ) return [ serializers . serialize_organization_with_course ( organization ) for organization in queryset ]
26	def flatten_grads ( var_list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var_list , grads ) ] , 0 )
11061	def send_message ( self , channel , text , thread = None , reply_broadcast = None ) : if isinstance ( channel , SlackRoomIMBase ) : channel = channel . id self . log . debug ( "Trying to send to %s: %s" , channel , text ) self . sc . rtm_send_message ( channel , text , thread = thread , reply_broadcast = reply_broadcast )
4007	def _compile_docker_commands ( app_name , assembled_specs , port_spec ) : app_spec = assembled_specs [ 'apps' ] [ app_name ] commands = [ 'set -e' ] commands += _lib_install_commands_for_app ( app_name , assembled_specs ) if app_spec [ 'mount' ] : commands . append ( "cd {}" . format ( container_code_path ( app_spec ) ) ) commands . append ( "export PATH=$PATH:{}" . format ( container_code_path ( app_spec ) ) ) commands += _copy_assets_commands_for_app ( app_spec , assembled_specs ) commands += _get_once_commands ( app_spec , port_spec ) commands += _get_always_commands ( app_spec ) return commands
8741	def create_floatingip ( context , content ) : LOG . info ( 'create_floatingip %s for tenant %s and body %s' % ( id , context . tenant_id , content ) ) network_id = content . get ( 'floating_network_id' ) if not network_id : raise n_exc . BadRequest ( resource = 'floating_ip' , msg = 'floating_network_id is required.' ) fixed_ip_address = content . get ( 'fixed_ip_address' ) ip_address = content . get ( 'floating_ip_address' ) port_id = content . get ( 'port_id' ) port = None port_fixed_ip = { } network = _get_network ( context , network_id ) if port_id : port = _get_port ( context , port_id ) fixed_ip = _get_fixed_ip ( context , fixed_ip_address , port ) port_fixed_ip = { port . id : { 'port' : port , 'fixed_ip' : fixed_ip } } flip = _allocate_ip ( context , network , port , ip_address , ip_types . FLOATING ) _create_flip ( context , flip , port_fixed_ip ) return v . _make_floating_ip_dict ( flip , port_id )
5916	def _translate_residue ( self , selection , default_atomname = 'CA' ) : m = self . RESIDUE . match ( selection ) if not m : errmsg = "Selection {selection!r} is not valid." . format ( ** vars ( ) ) logger . error ( errmsg ) raise ValueError ( errmsg ) gmx_resid = self . gmx_resid ( int ( m . group ( 'resid' ) ) ) residue = m . group ( 'aa' ) if len ( residue ) == 1 : gmx_resname = utilities . convert_aa_code ( residue ) else : gmx_resname = residue gmx_atomname = m . group ( 'atom' ) if gmx_atomname is None : gmx_atomname = default_atomname return { 'resname' : gmx_resname , 'resid' : gmx_resid , 'atomname' : gmx_atomname }
4965	def clean_notify ( self ) : return self . cleaned_data . get ( self . Fields . NOTIFY , self . NotificationTypes . DEFAULT )
12286	def lookup ( username , reponame ) : mgr = plugins_get_mgr ( ) repomgr = mgr . get ( what = 'repomanager' , name = 'git' ) repo = repomgr . lookup ( username = username , reponame = reponame ) return repo
8143	def rotate ( self , angle ) : from math import sqrt , pow , sin , cos , degrees , radians , asin w0 , h0 = self . img . size d = sqrt ( pow ( w0 , 2 ) + pow ( h0 , 2 ) ) d_angle = degrees ( asin ( ( w0 * 0.5 ) / ( d * 0.5 ) ) ) angle = angle % 360 if angle > 90 and angle <= 270 : d_angle += 180 w = sin ( radians ( d_angle + angle ) ) * d w = max ( w , sin ( radians ( d_angle - angle ) ) * d ) w = int ( abs ( w ) ) h = cos ( radians ( d_angle + angle ) ) * d h = max ( h , cos ( radians ( d_angle - angle ) ) * d ) h = int ( abs ( h ) ) dx = int ( ( w - w0 ) / 2 ) dy = int ( ( h - h0 ) / 2 ) d = int ( d ) bg = ImageStat . Stat ( self . img ) . mean bg = ( int ( bg [ 0 ] ) , int ( bg [ 1 ] ) , int ( bg [ 2 ] ) , 0 ) box = Image . new ( "RGBA" , ( d , d ) , bg ) box . paste ( self . img , ( ( d - w0 ) / 2 , ( d - h0 ) / 2 ) ) box = box . rotate ( angle , INTERPOLATION ) box = box . crop ( ( ( d - w ) / 2 + 2 , ( d - h ) / 2 , d - ( d - w ) / 2 , d - ( d - h ) / 2 ) ) self . img = box self . x += ( self . w - w ) / 2 self . y += ( self . h - h ) / 2 self . w = w self . h = h
9255	def generate_log_for_tag ( self , pull_requests , issues , newer_tag , older_tag_name ) : newer_tag_link , newer_tag_name , newer_tag_time = self . detect_link_tag_time ( newer_tag ) github_site = "https://github.com" or self . options . github_endpoint project_url = "{0}/{1}/{2}" . format ( github_site , self . options . user , self . options . project ) log = self . generate_header ( newer_tag_name , newer_tag_link , newer_tag_time , older_tag_name , project_url ) if self . options . issues : log += self . issues_to_log ( issues , pull_requests ) if self . options . include_pull_request : log += self . generate_sub_section ( pull_requests , self . options . merge_prefix ) return log
12602	def col_values ( df , col_name ) : _check_cols ( df , [ col_name ] ) if 'O' in df [ col_name ] or pd . np . issubdtype ( df [ col_name ] . dtype , str ) : return [ nom . lower ( ) for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ] else : return [ nom for nom in df [ pd . notnull ( df ) ] [ col_name ] if not pd . isnull ( nom ) ]
7160	def next_question ( self ) : for key , questions in self . questions . items ( ) : if key in self . answers : continue for question in questions : if self . check_condition ( question . _condition ) : return question return None
10643	def Nu ( L : float , h : float , k : float ) -> float : return h * L / k
2614	def _write_submit_script ( self , template , script_filename , job_name , configs ) : try : submit_script = Template ( template ) . substitute ( jobname = job_name , ** configs ) with open ( script_filename , 'w' ) as f : f . write ( submit_script ) except KeyError as e : logger . error ( "Missing keys for submit script : %s" , e ) raise ( SchedulerMissingArgs ( e . args , self . sitename ) ) except IOError as e : logger . error ( "Failed writing to submit script: %s" , script_filename ) raise ( ScriptPathError ( script_filename , e ) ) except Exception as e : print ( "Template : " , template ) print ( "Args : " , job_name ) print ( "Kwargs : " , configs ) logger . error ( "Uncategorized error: %s" , e ) raise ( e ) return True
13351	def add_files ( self , filelist , ** kwargs ) : if not isinstance ( filelist , list ) : raise TypeError ( "request the list type." ) for file in filelist : self . add_file ( file )
6109	def xticks ( self ) : return np . linspace ( np . amin ( self . grid_stack . regular [ : , 1 ] ) , np . amax ( self . grid_stack . regular [ : , 1 ] ) , 4 )
4338	def phaser ( self , gain_in = 0.8 , gain_out = 0.74 , delay = 3 , decay = 0.4 , speed = 0.5 , modulation_shape = 'sinusoidal' ) : if not is_number ( gain_in ) or gain_in <= 0 or gain_in > 1 : raise ValueError ( "gain_in must be a number between 0 and 1." ) if not is_number ( gain_out ) or gain_out <= 0 or gain_out > 1 : raise ValueError ( "gain_out must be a number between 0 and 1." ) if not is_number ( delay ) or delay <= 0 or delay > 5 : raise ValueError ( "delay must be a positive number." ) if not is_number ( decay ) or decay < 0.1 or decay > 0.5 : raise ValueError ( "decay must be a number between 0.1 and 0.5." ) if not is_number ( speed ) or speed < 0.1 or speed > 2 : raise ValueError ( "speed must be a positive number." ) if modulation_shape not in [ 'sinusoidal' , 'triangular' ] : raise ValueError ( "modulation_shape must be one of 'sinusoidal', 'triangular'." ) effect_args = [ 'phaser' , '{:f}' . format ( gain_in ) , '{:f}' . format ( gain_out ) , '{:f}' . format ( delay ) , '{:f}' . format ( decay ) , '{:f}' . format ( speed ) ] if modulation_shape == 'sinusoidal' : effect_args . append ( '-s' ) elif modulation_shape == 'triangular' : effect_args . append ( '-t' ) self . effects . extend ( effect_args ) self . effects_log . append ( 'phaser' ) return self
8303	def find_example_dir ( ) : code_stub = textwrap . dedent ( ) code = code_stub % 'share/shoebot/examples' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) output , errors = p . communicate ( ) if errors : print ( 'Shoebot experienced errors searching for install and examples.' ) print ( 'Errors:\n{0}' . format ( errors . decode ( 'utf-8' ) ) ) return None else : examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir code = code_stub % 'examples/' cmd = [ "python" , "-c" , code ] p = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) output , errors = p . communicate ( ) examples_dir = output . decode ( 'utf-8' ) . strip ( ) if os . path . isdir ( examples_dir ) : return examples_dir if examples_dir : print ( 'Shoebot could not find examples at: {0}' . format ( examples_dir ) ) else : print ( 'Shoebot could not find install dir and examples.' )
11179	def authorize_url ( self ) : auth_url = OAUTH_ROOT + '/authorize' params = { 'client_id' : self . client_id , 'redirect_uri' : self . redirect_uri , } return "{}?{}" . format ( auth_url , urlencode ( params ) )
11081	def webhook ( * args , ** kwargs ) : def wrapper ( func ) : func . is_webhook = True func . route = args [ 0 ] func . form_params = kwargs . get ( 'form_params' , [ ] ) func . method = kwargs . get ( 'method' , 'POST' ) return func return wrapper
1612	def ProcessGlobalSuppresions ( lines ) : for line in lines : if _SEARCH_C_FILE . search ( line ) : for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True if _SEARCH_KERNEL_FILE . search ( line ) : for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : _global_error_suppressions [ category ] = True
10553	def create_helpingmaterial ( project_id , info , media_url = None , file_path = None ) : try : helping = dict ( project_id = project_id , info = info , media_url = None , ) if file_path : files = { 'file' : open ( file_path , 'rb' ) } payload = { 'project_id' : project_id } res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = payload , files = files ) else : res = _pybossa_req ( 'post' , 'helpingmaterial' , payload = helping ) if res . get ( 'id' ) : return HelpingMaterial ( res ) else : return res except : raise
10245	def create_timeline ( year_counter : typing . Counter [ int ] ) -> List [ Tuple [ int , int ] ] : if not year_counter : return [ ] from_year = min ( year_counter ) - 1 until_year = datetime . now ( ) . year + 1 return [ ( year , year_counter . get ( year , 0 ) ) for year in range ( from_year , until_year ) ]
9319	def _validate_status ( self ) : if not self . id : msg = "No 'id' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if not self . status : msg = "No 'status' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . total_count is None : msg = "No 'total_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . success_count is None : msg = "No 'success_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . failure_count is None : msg = "No 'failure_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if self . pending_count is None : msg = "No 'pending_count' in Status for request '{}'" raise ValidationError ( msg . format ( self . url ) ) if len ( self . successes ) != self . success_count : msg = "Found successes={}, but success_count={} in status '{}'" raise ValidationError ( msg . format ( self . successes , self . success_count , self . id ) ) if len ( self . pendings ) != self . pending_count : msg = "Found pendings={}, but pending_count={} in status '{}'" raise ValidationError ( msg . format ( self . pendings , self . pending_count , self . id ) ) if len ( self . failures ) != self . failure_count : msg = "Found failures={}, but failure_count={} in status '{}'" raise ValidationError ( msg . format ( self . failures , self . failure_count , self . id ) ) if ( self . success_count + self . pending_count + self . failure_count != self . total_count ) : msg = ( "(success_count={} + pending_count={} + " "failure_count={}) != total_count={} in status '{}'" ) raise ValidationError ( msg . format ( self . success_count , self . pending_count , self . failure_count , self . total_count , self . id ) )
8019	async def websocket_send ( self , message , stream_name ) : text = message . get ( "text" ) json = await self . decode_json ( text ) data = { "stream" : stream_name , "payload" : json } await self . send_json ( data )
3553	def start_scan ( self , timeout_sec = TIMEOUT_SEC ) : get_provider ( ) . _central_manager . scanForPeripheralsWithServices_options_ ( None , None ) self . _is_scanning = True
4995	def handle_user_post_save ( sender , ** kwargs ) : created = kwargs . get ( "created" , False ) user_instance = kwargs . get ( "instance" , None ) if user_instance is None : return try : pending_ecu = PendingEnterpriseCustomerUser . objects . get ( user_email = user_instance . email ) except PendingEnterpriseCustomerUser . DoesNotExist : return if not created : try : existing_record = EnterpriseCustomerUser . objects . get ( user_id = user_instance . id ) message_template = "User {user} have changed email to match pending Enterprise Customer link, " "but was already linked to Enterprise Customer {enterprise_customer} - " "deleting pending link record" logger . info ( message_template . format ( user = user_instance , enterprise_customer = existing_record . enterprise_customer ) ) pending_ecu . delete ( ) return except EnterpriseCustomerUser . DoesNotExist : pass enterprise_customer_user = EnterpriseCustomerUser . objects . create ( enterprise_customer = pending_ecu . enterprise_customer , user_id = user_instance . id ) pending_enrollments = list ( pending_ecu . pendingenrollment_set . all ( ) ) if pending_enrollments : def _complete_user_enrollment ( ) : for enrollment in pending_enrollments : enterprise_customer_user . enroll ( enrollment . course_id , enrollment . course_mode , cohort = enrollment . cohort_name ) track_enrollment ( 'pending-admin-enrollment' , user_instance . id , enrollment . course_id ) pending_ecu . delete ( ) transaction . on_commit ( _complete_user_enrollment ) else : pending_ecu . delete ( )
6968	def smooth_magseries_savgol ( mags , windowsize , polyorder = 2 ) : smoothed = savgol_filter ( mags , windowsize , polyorder ) return smoothed
6019	def from_inverse_noise_map ( cls , pixel_scale , inverse_noise_map ) : noise_map = 1.0 / inverse_noise_map return NoiseMap ( array = noise_map , pixel_scale = pixel_scale )
10139	def encrypt_files ( selected_host , only_link , file_name ) : if ENCRYPTION_DISABLED : print ( 'For encryption please install gpg' ) exit ( ) passphrase = '%030x' % random . randrange ( 16 ** 30 ) source_filename = file_name cmd = 'gpg --batch --symmetric --cipher-algo AES256 --passphrase-fd 0 ' '--output - {}' . format ( source_filename ) encrypted_output = Popen ( shlex . split ( cmd ) , stdout = PIPE , stdin = PIPE , stderr = PIPE ) encrypted_data = encrypted_output . communicate ( passphrase . encode ( ) ) [ 0 ] return upload_files ( encrypted_data , selected_host , only_link , file_name ) + '#' + passphrase
10106	def _process_tabs ( self , tabs , current_tab , group_current_tab ) : for t in tabs : t . current_tab = current_tab t . group_current_tab = group_current_tab tabs = list ( filter ( lambda t : t . tab_visible , tabs ) ) tabs . sort ( key = lambda t : t . weight ) return tabs
8001	def fix_in_stanza ( self , stanza ) : StreamBase . fix_in_stanza ( self , stanza ) if not self . initiator : if stanza . from_jid != self . peer : stanza . set_from ( self . peer )
13786	def require ( name , field , data_type ) : if not isinstance ( field , data_type ) : msg = '{0} must have {1}, got: {2}' . format ( name , data_type , field ) raise AssertionError ( msg )
9339	def loadtxt2 ( fname , dtype = None , delimiter = ' ' , newline = '\n' , comment_character = '#' , skiplines = 0 ) : dtypert = [ None , None , None ] def preparedtype ( dtype ) : dtypert [ 0 ] = dtype flatten = flatten_dtype ( dtype ) dtypert [ 1 ] = flatten dtypert [ 2 ] = numpy . dtype ( [ ( 'a' , ( numpy . int8 , flatten . itemsize ) ) ] ) buf = numpy . empty ( ( ) , dtype = dtypert [ 1 ] ) converters = [ _default_conv [ flatten [ name ] . char ] for name in flatten . names ] return buf , converters , flatten . names def fileiter ( fh ) : converters = [ ] buf = None if dtype is not None : buf , converters , names = preparedtype ( dtype ) yield None for lineno , line in enumerate ( fh ) : if lineno < skiplines : continue if line [ 0 ] in comment_character : if buf is None and line [ 1 ] == '?' : ddtype = pickle . loads ( base64 . b64decode ( line [ 2 : ] ) ) buf , converters , names = preparedtype ( ddtype ) yield None continue for word , c , name in zip ( line . split ( ) , converters , names ) : buf [ name ] = c ( word ) buf2 = buf . copy ( ) . view ( dtype = dtypert [ 2 ] ) yield buf2 if isinstance ( fname , basestring ) : fh = file ( fh , 'r' ) cleanup = lambda : fh . close ( ) else : fh = iter ( fname ) cleanup = lambda : None try : i = fileiter ( fh ) i . next ( ) return numpy . fromiter ( i , dtype = dtypert [ 2 ] ) . view ( dtype = dtypert [ 0 ] ) finally : cleanup ( )
10388	def calculate_average_scores_on_graph ( graph : BELGraph , key : Optional [ str ] = None , tag : Optional [ str ] = None , default_score : Optional [ float ] = None , runs : Optional [ int ] = None , use_tqdm : bool = False , ) : subgraphs = generate_bioprocess_mechanisms ( graph , key = key ) scores = calculate_average_scores_on_subgraphs ( subgraphs , key = key , tag = tag , default_score = default_score , runs = runs , use_tqdm = use_tqdm ) return scores
11867	def normalize ( self ) : "Return my probabilities; must be down to one variable." assert len ( self . vars ) == 1 return ProbDist ( self . vars [ 0 ] , dict ( ( k , v ) for ( ( k , ) , v ) in self . cpt . items ( ) ) )
333	def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax
8694	def terminal ( port = default_port ( ) , baud = '9600' ) : testargs = [ 'nodemcu-uploader' , port , baud ] sys . argv = testargs miniterm . main ( )
13444	def _rindex ( mylist : Sequence [ T ] , x : T ) -> int : return len ( mylist ) - mylist [ : : - 1 ] . index ( x ) - 1
10476	def _queueMouseButton ( self , coord , mouseButton , modFlags , clickCount = 1 , dest_coord = None ) : mouseButtons = { Quartz . kCGMouseButtonLeft : 'LeftMouse' , Quartz . kCGMouseButtonRight : 'RightMouse' , } if mouseButton not in mouseButtons : raise ValueError ( 'Mouse button given not recognized' ) eventButtonDown = getattr ( Quartz , 'kCGEvent%sDown' % mouseButtons [ mouseButton ] ) eventButtonUp = getattr ( Quartz , 'kCGEvent%sUp' % mouseButtons [ mouseButton ] ) eventButtonDragged = getattr ( Quartz , 'kCGEvent%sDragged' % mouseButtons [ mouseButton ] ) buttonDown = Quartz . CGEventCreateMouseEvent ( None , eventButtonDown , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDown , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonDown , Quartz . kCGMouseEventClickState , int ( clickCount ) ) if dest_coord : buttonDragged = Quartz . CGEventCreateMouseEvent ( None , eventButtonDragged , dest_coord , mouseButton ) Quartz . CGEventSetFlags ( buttonDragged , modFlags ) buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , dest_coord , mouseButton ) else : buttonUp = Quartz . CGEventCreateMouseEvent ( None , eventButtonUp , coord , mouseButton ) Quartz . CGEventSetFlags ( buttonUp , modFlags ) Quartz . CGEventSetIntegerValueField ( buttonUp , Quartz . kCGMouseEventClickState , int ( clickCount ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonDown ) ) if dest_coord : self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGHIDEventTap , buttonDragged ) ) self . _queueEvent ( Quartz . CGEventPost , ( Quartz . kCGSessionEventTap , buttonUp ) )
12445	def options ( self , request , response ) : response [ 'Allowed' ] = ', ' . join ( self . meta . http_allowed_methods ) response . status = http . client . OK
13226	async def process_ltd_doc ( session , github_api_token , ltd_product_url , mongo_collection = None ) : logger = logging . getLogger ( __name__ ) ltd_product_data = await get_ltd_product ( session , url = ltd_product_url ) product_name = ltd_product_data [ 'slug' ] doc_handle_match = DOCUMENT_HANDLE_PATTERN . match ( product_name ) if doc_handle_match is None : logger . debug ( '%s is not a document repo' , product_name ) return try : return await process_sphinx_technote ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotSphinxTechnoteError : logger . debug ( '%s is not a Sphinx-based technote.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return try : return await process_lander_page ( session , github_api_token , ltd_product_data , mongo_collection = mongo_collection ) except NotLanderPageError : logger . debug ( '%s is not a Lander page with a metadata.jsonld file.' , product_name ) except Exception : logger . exception ( 'Unexpected error trying to process %s' , product_name ) return
6333	def dist_abs ( self , src , tar ) : return self . _lev . dist_abs ( src , tar , mode = 'lev' , cost = ( 1 , 1 , 9999 , 9999 ) )
7149	def encode ( cls , hex ) : out = [ ] for i in range ( len ( hex ) // 8 ) : word = endian_swap ( hex [ 8 * i : 8 * i + 8 ] ) x = int ( word , 16 ) w1 = x % cls . n w2 = ( x // cls . n + w1 ) % cls . n w3 = ( x // cls . n // cls . n + w2 ) % cls . n out += [ cls . word_list [ w1 ] , cls . word_list [ w2 ] , cls . word_list [ w3 ] ] checksum = cls . get_checksum ( " " . join ( out ) ) out . append ( checksum ) return " " . join ( out )
5326	def __create_arthur_json ( self , repo , backend_args ) : backend_args = self . _compose_arthur_params ( self . backend_section , repo ) if self . backend_section == 'git' : backend_args [ 'gitpath' ] = os . path . join ( self . REPOSITORY_DIR , repo ) backend_args [ 'tag' ] = self . backend_tag ( repo ) ajson = { "tasks" : [ { } ] } ajson [ "tasks" ] [ 0 ] [ 'task_id' ] = self . backend_tag ( repo ) ajson [ "tasks" ] [ 0 ] [ 'backend' ] = self . backend_section . split ( ":" ) [ 0 ] ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] = backend_args ajson [ "tasks" ] [ 0 ] [ 'category' ] = backend_args [ 'category' ] ajson [ "tasks" ] [ 0 ] [ 'archive' ] = { } ajson [ "tasks" ] [ 0 ] [ 'scheduler' ] = { "delay" : self . ARTHUR_TASK_DELAY } es_col_url = self . _get_collection_url ( ) es_index = self . conf [ self . backend_section ] [ 'raw_index' ] es = ElasticSearch ( es_col_url , es_index ) connector = get_connector_from_name ( self . backend_section ) klass = connector [ 0 ] signature = inspect . signature ( klass . fetch ) last_activity = None filter_ = { "name" : "tag" , "value" : backend_args [ 'tag' ] } if 'from_date' in signature . parameters : last_activity = es . get_last_item_field ( 'metadata__updated_on' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'from_date' ] = last_activity . isoformat ( ) elif 'offset' in signature . parameters : last_activity = es . get_last_item_field ( 'offset' , [ filter_ ] ) if last_activity : ajson [ "tasks" ] [ 0 ] [ 'backend_args' ] [ 'offset' ] = last_activity if last_activity : logging . info ( "Getting raw item with arthur since %s" , last_activity ) return ( ajson )
13560	def decorate ( msg = "" , waitmsg = "Please wait" ) : def decorator ( func ) : @ functools . wraps ( func ) def wrapper ( * args , ** kwargs ) : spin = Spinner ( msg = msg , waitmsg = waitmsg ) spin . start ( ) a = None try : a = func ( * args , ** kwargs ) except Exception as e : spin . msg = "Something went wrong: " spin . stop_spinning ( ) spin . join ( ) raise e spin . stop_spinning ( ) spin . join ( ) return a return wrapper return decorator
11008	def get_project_slug ( self , bet ) : if bet . get ( 'form_params' ) : params = json . loads ( bet [ 'form_params' ] ) return params . get ( 'project' ) return None
6775	def force_stop_and_purge ( self ) : r = self . local_renderer self . stop ( ) with settings ( warn_only = True ) : r . sudo ( 'killall rabbitmq-server' ) with settings ( warn_only = True ) : r . sudo ( 'killall beam.smp' ) r . sudo ( 'rm -Rf /var/lib/rabbitmq/mnesia/*' )
9172	def includeme ( config ) : config . include ( 'pyramid_jinja2' ) config . add_jinja2_renderer ( '.html' ) config . add_jinja2_renderer ( '.rss' ) config . add_static_view ( name = '/a/static' , path = "cnxpublishing:static/" ) config . commit ( ) from cnxdb . ident_hash import join_ident_hash for ext in ( '.html' , '.rss' , ) : jinja2_env = config . get_jinja2_environment ( ext ) jinja2_env . globals . update ( join_ident_hash = join_ident_hash , ) declare_api_routes ( config ) declare_browsable_routes ( config )
10542	def find_tasks ( project_id , ** kwargs ) : try : kwargs [ 'project_id' ] = project_id res = _pybossa_req ( 'get' , 'task' , params = kwargs ) if type ( res ) . __name__ == 'list' : return [ Task ( task ) for task in res ] else : return res except : raise
13102	def get_template_uuid ( self ) : response = requests . get ( self . url + 'editor/scan/templates' , headers = self . headers , verify = False ) templates = json . loads ( response . text ) for template in templates [ 'templates' ] : if template [ 'name' ] == self . template_name : return template [ 'uuid' ]
6917	def simple_flare_find ( times , mags , errs , smoothbinsize = 97 , flare_minsigma = 4.0 , flare_maxcadencediff = 1 , flare_mincadencepoints = 3 , magsarefluxes = False , savgol_polyorder = 2 , ** savgol_kwargs ) : if errs is None : errs = 0.001 * mags finiteind = np . isfinite ( times ) & np . isfinite ( mags ) & np . isfinite ( errs ) ftimes = times [ finiteind ] fmags = mags [ finiteind ] ferrs = errs [ finiteind ] smoothed = savgol_filter ( fmags , smoothbinsize , savgol_polyorder , ** savgol_kwargs ) subtracted = fmags - smoothed series_mad = np . median ( np . abs ( subtracted ) ) series_stdev = 1.483 * series_mad if magsarefluxes : extind = np . where ( subtracted > ( flare_minsigma * series_stdev ) ) else : extind = np . where ( subtracted < ( - flare_minsigma * series_stdev ) ) if extind and extind [ 0 ] : extrema_indices = extind [ 0 ] flaregroups = [ ] for ind , extrema_index in enumerate ( extrema_indices ) : pass
2846	def close ( self ) : if self . _ctx is not None : ftdi . free ( self . _ctx ) self . _ctx = None
8509	def fit ( self , X , y = None ) : from pylearn2 . config import yaml_parse from pylearn2 . train import Train params = self . get_params ( ) yaml_string = Template ( self . yaml_string ) . substitute ( params ) self . trainer = yaml_parse . load ( yaml_string ) assert isinstance ( self . trainer , Train ) if self . trainer . dataset is not None : raise ValueError ( 'Train YAML database must evaluate to None.' ) self . trainer . dataset = self . _get_dataset ( X , y ) if ( hasattr ( self . trainer . algorithm , 'monitoring_dataset' ) and self . trainer . algorithm . monitoring_dataset is not None ) : monitoring_dataset = self . trainer . algorithm . monitoring_dataset if len ( monitoring_dataset ) == 1 and '' in monitoring_dataset : monitoring_dataset [ '' ] = self . trainer . dataset else : monitoring_dataset [ 'train' ] = self . trainer . dataset self . trainer . algorithm . _set_monitoring_dataset ( monitoring_dataset ) else : self . trainer . algorithm . _set_monitoring_dataset ( self . trainer . dataset ) self . trainer . main_loop ( )
100	def compute_line_intersection_point ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 ) : def _make_line ( p1 , p2 ) : A = ( p1 [ 1 ] - p2 [ 1 ] ) B = ( p2 [ 0 ] - p1 [ 0 ] ) C = ( p1 [ 0 ] * p2 [ 1 ] - p2 [ 0 ] * p1 [ 1 ] ) return A , B , - C L1 = _make_line ( ( x1 , y1 ) , ( x2 , y2 ) ) L2 = _make_line ( ( x3 , y3 ) , ( x4 , y4 ) ) D = L1 [ 0 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 0 ] Dx = L1 [ 2 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 2 ] Dy = L1 [ 0 ] * L2 [ 2 ] - L1 [ 2 ] * L2 [ 0 ] if D != 0 : x = Dx / D y = Dy / D return x , y else : return False
11225	def dump_OrderedDict ( self , obj , class_name = "collections.OrderedDict" ) : return { "$" + class_name : [ ( key , self . _json_convert ( value ) ) for key , value in iteritems ( obj ) ] }
5938	def help ( self , long = False ) : print ( "\ncommand: {0!s}\n\n" . format ( self . command_name ) ) print ( self . __doc__ ) if long : print ( "\ncall method: command():\n" ) print ( self . __call__ . __doc__ )
8387	def check_main ( argv ) : if len ( argv ) != 1 : print ( "Please provide the name of a file to check." ) return 1 filename = argv [ 0 ] if os . path . exists ( filename ) : print ( u"Checking existing copy of %s" % filename ) tef = TamperEvidentFile ( filename ) if tef . validate ( ) : print ( u"Your copy of %s is good" % filename ) else : print ( u"Your copy of %s seems to have been edited" % filename ) else : print ( u"You don't have a copy of %s" % filename ) return 0
4831	def get_course_certificate ( self , course_id , username ) : return self . client . certificates ( username ) . courses ( course_id ) . get ( )
4480	def checksum ( file_path , hash_type = 'md5' , block_size = 65536 ) : if hash_type == 'md5' : hash_ = hashlib . md5 ( ) elif hash_type == 'sha256' : hash_ = hashlib . sha256 ( ) else : raise ValueError ( "{} is an invalid hash_type. Expected 'md5' or 'sha256'." . format ( hash_type ) ) with open ( file_path , 'rb' ) as f : for block in iter ( lambda : f . read ( block_size ) , b'' ) : hash_ . update ( block ) return hash_ . hexdigest ( )
11577	def send_sysex ( self , sysex_command , sysex_data = None ) : if not sysex_data : sysex_data = [ ] sysex_message = chr ( self . START_SYSEX ) sysex_message += chr ( sysex_command ) if len ( sysex_data ) : for d in sysex_data : sysex_message += chr ( d ) sysex_message += chr ( self . END_SYSEX ) for data in sysex_message : self . pymata . transport . write ( data )
742	def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )
3637	def clubStaff ( self ) : method = 'GET' url = 'club/stats/staff' rc = self . __request__ ( method , url ) return rc
506	def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( "Invalid supplied range for 'getLabels'." , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results
2740	def add_tags ( self , tags ) : return self . get_data ( "firewalls/%s/tags" % self . id , type = POST , params = { "tags" : tags } )
810	def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None
862	def isTemporal ( inferenceType ) : if InferenceType . __temporalInferenceTypes is None : InferenceType . __temporalInferenceTypes = set ( [ InferenceType . TemporalNextStep , InferenceType . TemporalClassification , InferenceType . TemporalAnomaly , InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep ] ) return inferenceType in InferenceType . __temporalInferenceTypes
2241	def modpath_to_modname ( modpath , hide_init = True , hide_main = False , check = True , relativeto = None ) : if check and relativeto is None : if not exists ( modpath ) : raise ValueError ( 'modpath={} does not exist' . format ( modpath ) ) modpath_ = abspath ( expanduser ( modpath ) ) modpath_ = normalize_modpath ( modpath_ , hide_init = hide_init , hide_main = hide_main ) if relativeto : dpath = dirname ( abspath ( expanduser ( relativeto ) ) ) rel_modpath = relpath ( modpath_ , dpath ) else : dpath , rel_modpath = split_modpath ( modpath_ , check = check ) modname = splitext ( rel_modpath ) [ 0 ] if '.' in modname : modname , abi_tag = modname . split ( '.' ) modname = modname . replace ( '/' , '.' ) modname = modname . replace ( '\\' , '.' ) return modname
9347	def argsort ( data , out = None , chunksize = None , baseargsort = None , argmerge = None , np = None ) : if baseargsort is None : baseargsort = lambda x : x . argsort ( ) if argmerge is None : argmerge = default_argmerge if chunksize is None : chunksize = 1024 * 1024 * 16 if out is None : arg1 = numpy . empty ( len ( data ) , dtype = 'intp' ) out = arg1 else : assert out . dtype == numpy . dtype ( 'intp' ) assert len ( out ) == len ( data ) arg1 = out if np is None : np = sharedmem . cpu_count ( ) if np <= 1 or len ( data ) < chunksize : out [ : ] = baseargsort ( data ) return out CHK = [ slice ( i , i + chunksize ) for i in range ( 0 , len ( data ) , chunksize ) ] DUMMY = slice ( len ( data ) , len ( data ) ) if len ( CHK ) % 2 : CHK . append ( DUMMY ) with sharedmem . TPool ( ) as pool : def work ( i ) : C = CHK [ i ] start , stop , step = C . indices ( len ( data ) ) arg1 [ C ] = baseargsort ( data [ C ] ) arg1 [ C ] += start pool . map ( work , range ( len ( CHK ) ) ) arg2 = numpy . empty_like ( arg1 ) flip = 0 while len ( CHK ) > 1 : with sharedmem . TPool ( ) as pool : def work ( i ) : C1 = CHK [ i ] C2 = CHK [ i + 1 ] start1 , stop1 , step1 = C1 . indices ( len ( data ) ) start2 , stop2 , step2 = C2 . indices ( len ( data ) ) assert start2 == stop1 argmerge ( data , arg1 [ C1 ] , arg1 [ C2 ] , arg2 [ start1 : stop2 ] ) return slice ( start1 , stop2 ) CHK = pool . map ( work , range ( 0 , len ( CHK ) , 2 ) ) arg1 , arg2 = arg2 , arg1 flip = flip + 1 if len ( CHK ) == 1 : break if len ( CHK ) % 2 : CHK . append ( DUMMY ) if flip % 2 != 0 : out [ : ] = arg1 return out
530	def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ]
288	def plot_returns ( returns , live_start_date = None , ax = None ) : if ax is None : ax = plt . gca ( ) ax . set_label ( '' ) ax . set_ylabel ( 'Returns' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index >= live_start_date ] is_returns . plot ( ax = ax , color = 'g' ) oos_returns . plot ( ax = ax , color = 'r' ) else : returns . plot ( ax = ax , color = 'g' ) return ax
5896	def render_toolbar ( context , config ) : quill_config = getattr ( quill_app , config ) t = template . loader . get_template ( quill_config [ 'toolbar_template' ] ) return t . render ( context )
3373	def add_cons_vars_to_problem ( model , what , ** kwargs ) : context = get_context ( model ) model . solver . add ( what , ** kwargs ) if context : context ( partial ( model . solver . remove , what ) )
3348	def remove_members ( self , to_remove ) : if isinstance ( to_remove , string_types ) or hasattr ( to_remove , "id" ) : warn ( "need to pass in a list" ) to_remove = [ to_remove ] self . _members . difference_update ( to_remove )
10350	def lint_directory ( source , target ) : for path in os . listdir ( source ) : if not path . endswith ( '.bel' ) : continue log . info ( 'linting: %s' , path ) with open ( os . path . join ( source , path ) ) as i , open ( os . path . join ( target , path ) , 'w' ) as o : lint_file ( i , o )
3443	def save_json_model ( model , filename , sort = False , pretty = False , ** kwargs ) : obj = model_to_dict ( model , sort = sort ) obj [ u"version" ] = JSON_SPEC if pretty : dump_opts = { "indent" : 4 , "separators" : ( "," , ": " ) , "sort_keys" : True , "allow_nan" : False } else : dump_opts = { "indent" : 0 , "separators" : ( "," , ":" ) , "sort_keys" : False , "allow_nan" : False } dump_opts . update ( ** kwargs ) if isinstance ( filename , string_types ) : with open ( filename , "w" ) as file_handle : json . dump ( obj , file_handle , ** dump_opts ) else : json . dump ( obj , filename , ** dump_opts )
13139	def to_json ( self ) : if self . subreference is not None : return { "source" : self . objectId , "selector" : { "type" : "FragmentSelector" , "conformsTo" : "http://ontology-dts.org/terms/subreference" , "value" : self . subreference } } else : return { "source" : self . objectId }
921	def log ( self , level , msg , * args , ** kwargs ) : self . _baseLogger . log ( self , level , self . getExtendedMsg ( msg ) , * args , ** kwargs )
11576	def sonar_data ( self , data ) : val = int ( ( data [ self . MSB ] << 7 ) + data [ self . LSB ] ) pin_number = data [ 0 ] with self . pymata . data_lock : sonar_pin_entry = self . active_sonar_map [ pin_number ] self . digital_response_table [ data [ self . RESPONSE_TABLE_MODE ] ] [ self . RESPONSE_TABLE_PIN_DATA_VALUE ] = val if sonar_pin_entry [ 0 ] is not None : if sonar_pin_entry [ 1 ] != val : self . active_sonar_map [ pin_number ] [ 0 ] ( [ self . pymata . SONAR , pin_number , val ] ) sonar_pin_entry [ 1 ] = val self . active_sonar_map [ pin_number ] = sonar_pin_entry
12633	def calculate_file_distances ( dicom_files , field_weights = None , dist_method_cls = None , ** kwargs ) : if dist_method_cls is None : dist_method = LevenshteinDicomFileDistance ( field_weights ) else : try : dist_method = dist_method_cls ( field_weights = field_weights , ** kwargs ) except : log . exception ( 'Could not instantiate {} object with field_weights ' 'and {}' . format ( dist_method_cls , kwargs ) ) dist_dtype = np . float16 n_files = len ( dicom_files ) try : file_dists = np . zeros ( ( n_files , n_files ) , dtype = dist_dtype ) except MemoryError as mee : import scipy . sparse file_dists = scipy . sparse . lil_matrix ( ( n_files , n_files ) , dtype = dist_dtype ) for idxi in range ( n_files ) : dist_method . set_dicom_file1 ( dicom_files [ idxi ] ) for idxj in range ( idxi + 1 , n_files ) : dist_method . set_dicom_file2 ( dicom_files [ idxj ] ) if idxi != idxj : file_dists [ idxi , idxj ] = dist_method . transform ( ) return file_dists
145	def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )
9323	def refresh_information ( self , accept = MEDIA_TYPE_TAXII_V20 ) : response = self . __raw = self . _conn . get ( self . url , headers = { "Accept" : accept } ) self . _populate_fields ( ** response ) self . _loaded_information = True
8510	def _predict ( self , X , method = 'fprop' ) : import theano X_sym = self . trainer . model . get_input_space ( ) . make_theano_batch ( ) y_sym = getattr ( self . trainer . model , method ) ( X_sym ) f = theano . function ( [ X_sym ] , y_sym , allow_input_downcast = True ) return f ( X )
13657	def _forObject ( self , obj ) : router = type ( self ) ( ) router . _routes = list ( self . _routes ) router . _self = obj return router
12269	def evaluate ( self , repo , spec , args ) : status = [ ] with cd ( repo . rootdir ) : files = spec . get ( 'files' , [ '*' ] ) resource_files = repo . find_matching_files ( files ) files = glob2 . glob ( "**/*" ) disk_files = [ f for f in files if os . path . isfile ( f ) and f != "datapackage.json" ] allfiles = list ( set ( resource_files + disk_files ) ) allfiles . sort ( ) for f in allfiles : if f in resource_files and f in disk_files : r = repo . get_resource ( f ) coded_sha256 = r [ 'sha256' ] computed_sha256 = compute_sha256 ( f ) if computed_sha256 != coded_sha256 : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "Mismatch in checksum on disk and in datapackage.json" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'OK' , 'message' : "" } ) elif f in resource_files : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In datapackage.json but not in repo" } ) else : status . append ( { 'target' : f , 'rules' : "" , 'validator' : self . name , 'description' : self . description , 'status' : 'ERROR' , 'message' : "In repo but not in datapackage.json" } ) return status
9741	async def await_event ( self , event = None , timeout = None ) : if self . event_future is not None : raise Exception ( "Can't wait on multiple events!" ) result = await asyncio . wait_for ( self . _wait_loop ( event ) , timeout ) return result
3200	def create ( self , data ) : if 'recipients' not in data : raise KeyError ( 'The campaign must have recipients' ) if 'list_id' not in data [ 'recipients' ] : raise KeyError ( 'The campaign recipients must have a list_id' ) if 'settings' not in data : raise KeyError ( 'The campaign must have settings' ) if 'subject_line' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a subject_line' ) if 'from_name' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a from_name' ) if 'reply_to' not in data [ 'settings' ] : raise KeyError ( 'The campaign settings must have a reply_to' ) check_email ( data [ 'settings' ] [ 'reply_to' ] ) if 'type' not in data : raise KeyError ( 'The campaign must have a type' ) if not data [ 'type' ] in [ 'regular' , 'plaintext' , 'rss' , 'variate' , 'abspilt' ] : raise ValueError ( 'The campaign type must be one of "regular", "plaintext", "rss", or "variate"' ) if data [ 'type' ] == 'variate' : if 'variate_settings' not in data : raise KeyError ( 'The variate campaign must have variate_settings' ) if 'winner_criteria' not in data [ 'variate_settings' ] : raise KeyError ( 'The campaign variate_settings must have a winner_criteria' ) if data [ 'variate_settings' ] [ 'winner_criteria' ] not in [ 'opens' , 'clicks' , 'total_revenue' , 'manual' ] : raise ValueError ( 'The campaign variate_settings ' 'winner_criteria must be one of "opens", "clicks", "total_revenue", or "manual"' ) if data [ 'type' ] == 'rss' : if 'rss_opts' not in data : raise KeyError ( 'The rss campaign must have rss_opts' ) if 'feed_url' not in data [ 'rss_opts' ] : raise KeyError ( 'The campaign rss_opts must have a feed_url' ) if not data [ 'rss_opts' ] [ 'frequency' ] in [ 'daily' , 'weekly' , 'monthly' ] : raise ValueError ( 'The rss_opts frequency must be one of "daily", "weekly", or "monthly"' ) response = self . _mc_client . _post ( url = self . _build_path ( ) , data = data ) if response is not None : self . campaign_id = response [ 'id' ] else : self . campaign_id = None return response
2666	def write ( self , buf , ** kwargs ) : self . i2c . writeto ( self . device_address , buf , ** kwargs ) if self . _debug : print ( "i2c_device.write:" , [ hex ( i ) for i in buf ] )
4552	def fill_triangle ( setter , x0 , y0 , x1 , y1 , x2 , y2 , color = None , aa = False ) : a = b = y = last = 0 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y1 > y2 : y2 , y1 = y1 , y2 x2 , x1 = x1 , x2 if y0 > y1 : y0 , y1 = y1 , y0 x0 , x1 = x1 , x0 if y0 == y2 : a = b = x0 if x1 < a : a = x1 elif x1 > b : b = x1 if x2 < a : a = x2 elif x2 > b : b = x2 _draw_fast_hline ( setter , a , y0 , b - a + 1 , color , aa ) dx01 = x1 - x0 dy01 = y1 - y0 dx02 = x2 - x0 dy02 = y2 - y0 dx12 = x2 - x1 dy12 = y2 - y1 sa = 0 sb = 0 if y1 == y2 : last = y1 else : last = y1 - 1 for y in range ( y , last + 1 ) : a = x0 + sa / dy01 b = x0 + sb / dy02 sa += dx01 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa ) sa = dx12 * ( y - y1 ) sb = dx02 * ( y - y0 ) for y in range ( y , y2 + 1 ) : a = x1 + sa / dy12 b = x0 + sb / dy02 sa += dx12 sb += dx02 if a > b : a , b = b , a _draw_fast_hline ( setter , a , y , b - a + 1 , color , aa )
12994	def table ( cluster ) : teffs = teff ( cluster ) lums = luminosity ( cluster ) arr = cluster . to_array ( ) i = 0 for row in arr : row [ 'lum' ] [ 0 ] = np . array ( [ lums [ i ] ] , dtype = 'f' ) row [ 'temp' ] [ 0 ] = np . array ( [ teffs [ i ] ] , dtype = 'f' ) i += 1 arr = round_arr_teff_luminosity ( arr ) return arr
5315	def colorpalette ( self , colorpalette ) : if isinstance ( colorpalette , str ) : colorpalette = colors . parse_colors ( colorpalette ) self . _colorpalette = colors . sanitize_color_palette ( colorpalette )
3075	def callback_view ( self ) : if 'error' in request . args : reason = request . args . get ( 'error_description' , request . args . get ( 'error' , '' ) ) reason = markupsafe . escape ( reason ) return ( 'Authorization failed: {0}' . format ( reason ) , httplib . BAD_REQUEST ) try : encoded_state = request . args [ 'state' ] server_csrf = session [ _CSRF_KEY ] code = request . args [ 'code' ] except KeyError : return 'Invalid request' , httplib . BAD_REQUEST try : state = json . loads ( encoded_state ) client_csrf = state [ 'csrf_token' ] return_url = state [ 'return_url' ] except ( ValueError , KeyError ) : return 'Invalid request state' , httplib . BAD_REQUEST if client_csrf != server_csrf : return 'Invalid request state' , httplib . BAD_REQUEST flow = _get_flow_for_token ( server_csrf ) if flow is None : return 'Invalid request state' , httplib . BAD_REQUEST try : credentials = flow . step2_exchange ( code ) except client . FlowExchangeError as exchange_error : current_app . logger . exception ( exchange_error ) content = 'An error occurred: {0}' . format ( exchange_error ) return content , httplib . BAD_REQUEST self . storage . put ( credentials ) if self . authorize_callback : self . authorize_callback ( credentials ) return redirect ( return_url )
1887	def emulate ( self , instruction ) : while True : self . reset ( ) for base in self . _should_be_mapped : size , perms = self . _should_be_mapped [ base ] self . _emu . mem_map ( base , size , perms ) for address , values in self . _should_be_written . items ( ) : for offset , byte in enumerate ( values , start = address ) : if issymbolic ( byte ) : from . . native . cpu . abstractcpu import ConcretizeMemory raise ConcretizeMemory ( self . _cpu . memory , offset , 8 , "Concretizing for emulation" ) self . _emu . mem_write ( address , b'' . join ( values ) ) self . _should_try_again = False self . _step ( instruction ) if not self . _should_try_again : break
8117	def circle_line_intersection ( cx , cy , radius , x1 , y1 , x2 , y2 , infinite = False ) : dx = x2 - x1 dy = y2 - y1 A = dx * dx + dy * dy B = 2 * ( dx * ( x1 - cx ) + dy * ( y1 - cy ) ) C = pow ( x1 - cx , 2 ) + pow ( y1 - cy , 2 ) - radius * radius det = B * B - 4 * A * C if A <= 0.0000001 or det < 0 : return [ ] elif det == 0 : t = - B / ( 2 * A ) return [ ( x1 + t * dx , y1 + t * dy ) ] else : points = [ ] det2 = sqrt ( det ) t1 = ( - B + det2 ) / ( 2 * A ) t2 = ( - B - det2 ) / ( 2 * A ) if infinite or 0 <= t1 <= 1 : points . append ( ( x1 + t1 * dx , y1 + t1 * dy ) ) if infinite or 0 <= t2 <= 1 : points . append ( ( x1 + t2 * dx , y1 + t2 * dy ) ) return points
11261	def resplit ( prev , pattern , * args , ** kw ) : maxsplit = 0 if 'maxsplit' not in kw else kw . pop ( 'maxsplit' ) pattern_obj = re . compile ( pattern , * args , ** kw ) for s in prev : yield pattern_obj . split ( s , maxsplit = maxsplit )
13861	def to_datetime ( when ) : if when is None or is_datetime ( when ) : return when if is_time ( when ) : return datetime . combine ( epoch . date ( ) , when ) if is_date ( when ) : return datetime . combine ( when , time ( 0 ) ) raise TypeError ( "unable to convert {} to datetime" . format ( when . __class__ . __name__ ) )
8885	def fit ( self , x , y = None ) : if self . _dtype is not None : iter2array ( x , dtype = self . _dtype ) else : iter2array ( x ) return self
12154	def timeit ( timer = None ) : if timer is None : return time . time ( ) else : took = time . time ( ) - timer if took < 1 : return "%.02f ms" % ( took * 1000.0 ) elif took < 60 : return "%.02f s" % ( took ) else : return "%.02f min" % ( took / 60.0 )
10345	def get_merged_namespace_names ( locations , check_keywords = True ) : resources = { location : get_bel_resource ( location ) for location in locations } if check_keywords : resource_keywords = set ( config [ 'Namespace' ] [ 'Keyword' ] for config in resources . values ( ) ) if 1 != len ( resource_keywords ) : raise ValueError ( 'Tried merging namespaces with different keywords: {}' . format ( resource_keywords ) ) result = { } for resource in resources : result . update ( resource [ 'Values' ] ) return result
11124	def move_directory ( self , relativePath , relativeDestination , replace = False , verbose = True ) : relativePath = os . path . normpath ( relativePath ) relativeDestination = os . path . normpath ( relativeDestination ) filesInfo = list ( self . walk_files_info ( relativePath = relativePath ) ) dirsPath = list ( self . walk_directories_relative_path ( relativePath = relativePath ) ) dirInfoDict , errorMessage = self . get_directory_info ( relativePath ) assert dirInfoDict is not None , errorMessage self . remove_directory ( relativePath = relativePath , removeFromSystem = False ) self . add_directory ( relativeDestination ) for RP , info in filesInfo : source = os . path . join ( self . __path , relativePath , RP ) destination = os . path . join ( self . __path , relativeDestination , RP ) newDirRP , fileName = os . path . split ( os . path . join ( relativeDestination , RP ) ) dirInfoDict = self . add_directory ( newDirRP ) if os . path . isfile ( destination ) : if replace : os . remove ( destination ) if verbose : warnings . warn ( "file '%s' is copied replacing existing one in destination '%s'." % ( fileName , newDirRP ) ) else : if verbose : warnings . warn ( "file '%s' is not copied because the same file exists in destination '%s'." % ( fileName , destination ) ) continue os . rename ( source , destination ) dict . __getitem__ ( dirInfoDict , "files" ) [ fileName ] = info self . save ( )
1302	def keybd_event ( bVk : int , bScan : int , dwFlags : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . keybd_event ( bVk , bScan , dwFlags , dwExtraInfo )
9353	def job_title ( ) : result = random . choice ( get_dictionary ( 'job_titles' ) ) . strip ( ) result = result . replace ( '#{N}' , job_title_suffix ( ) ) return result
1818	def SETNZ ( cpu , dest ) : dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) )
1239	def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )
12018	def disassemble ( self ) : ser_pb = open ( self . input_file , 'rb' ) . read ( ) fd = FileDescriptorProto ( ) fd . ParseFromString ( ser_pb ) self . name = fd . name self . _print ( '// Reversed by pbd (https://github.com/rsc-dev/pbd)' ) self . _print ( 'syntax = "proto2";' ) self . _print ( '' ) if len ( fd . package ) > 0 : self . _print ( 'package {};' . format ( fd . package ) ) self . package = fd . package else : self . _print ( '// Package not defined' ) self . _walk ( fd )
10867	def rmatrix ( self ) : t = self . param_dict [ self . lbl_theta ] r0 = np . array ( [ [ np . cos ( t ) , - np . sin ( t ) , 0 ] , [ np . sin ( t ) , np . cos ( t ) , 0 ] , [ 0 , 0 , 1 ] ] ) p = self . param_dict [ self . lbl_phi ] r1 = np . array ( [ [ np . cos ( p ) , 0 , np . sin ( p ) ] , [ 0 , 1 , 0 ] , [ - np . sin ( p ) , 0 , np . cos ( p ) ] ] ) return np . dot ( r1 , r0 )
3760	def mass_fractions ( self ) : r things = dict ( ) for zi , atoms in zip ( self . zs , self . atomss ) : for atom , count in atoms . iteritems ( ) : if atom in things : things [ atom ] += zi * count else : things [ atom ] = zi * count return mass_fractions ( things )
9093	def _update_namespace ( self , namespace : Namespace ) -> None : old_entry_identifiers = self . _get_old_entry_identifiers ( namespace ) new_count = 0 skip_count = 0 for model in self . _iterate_namespace_models ( ) : if self . _get_identifier ( model ) in old_entry_identifiers : continue entry = self . _create_namespace_entry_from_model ( model , namespace = namespace ) if entry is None or entry . name is None : skip_count += 1 continue new_count += 1 self . session . add ( entry ) t = time . time ( ) log . info ( 'got %d new entries. skipped %d entries missing names. committing models' , new_count , skip_count ) self . session . commit ( ) log . info ( 'committed models in %.2f seconds' , time . time ( ) - t )
5092	def _login ( self , email , password ) : response = requests . post ( urljoin ( self . ENDPOINT , 'sessions' ) , json = { 'email' : email , 'password' : password , 'platform' : 'ios' , 'token' : binascii . hexlify ( os . urandom ( 64 ) ) . decode ( 'utf8' ) } , headers = self . _headers ) response . raise_for_status ( ) access_token = response . json ( ) [ 'access_token' ] self . _headers [ 'Authorization' ] = 'Token token=%s' % access_token
3507	def create_stoichiometric_matrix ( model , array_type = 'dense' , dtype = None ) : if array_type not in ( 'DataFrame' , 'dense' ) and not dok_matrix : raise ValueError ( 'Sparse matrices require scipy' ) if dtype is None : dtype = np . float64 array_constructor = { 'dense' : np . zeros , 'dok' : dok_matrix , 'lil' : lil_matrix , 'DataFrame' : np . zeros , } n_metabolites = len ( model . metabolites ) n_reactions = len ( model . reactions ) array = array_constructor [ array_type ] ( ( n_metabolites , n_reactions ) , dtype = dtype ) m_ind = model . metabolites . index r_ind = model . reactions . index for reaction in model . reactions : for metabolite , stoich in iteritems ( reaction . metabolites ) : array [ m_ind ( metabolite ) , r_ind ( reaction ) ] = stoich if array_type == 'DataFrame' : metabolite_ids = [ met . id for met in model . metabolites ] reaction_ids = [ rxn . id for rxn in model . reactions ] return pd . DataFrame ( array , index = metabolite_ids , columns = reaction_ids ) else : return array
273	def to_utc ( df ) : try : df . index = df . index . tz_localize ( 'UTC' ) except TypeError : df . index = df . index . tz_convert ( 'UTC' ) return df
8913	def save_service ( self , service , overwrite = True ) : name = namesgenerator . get_sane_name ( service . name ) if not name : name = namesgenerator . get_random_name ( ) if name in self . name_index : name = namesgenerator . get_random_name ( retry = True ) if name in self . name_index : if overwrite : self . _delete ( name = name ) else : raise Exception ( "service name already registered." ) self . _insert ( Service ( name = name , url = baseurl ( service . url ) , type = service . type , purl = service . purl , public = service . public , auth = service . auth , verify = service . verify ) ) return self . fetch_by_name ( name = name )
10263	def _collapse_edge_by_namespace ( graph : BELGraph , victim_namespaces : Strings , survivor_namespaces : str , relations : Strings ) -> None : relation_filter = build_relation_predicate ( relations ) source_namespace_filter = build_source_namespace_filter ( victim_namespaces ) target_namespace_filter = build_target_namespace_filter ( survivor_namespaces ) edge_predicates = [ relation_filter , source_namespace_filter , target_namespace_filter ] _collapse_edge_passing_predicates ( graph , edge_predicates = edge_predicates )
8015	async def dispatch_downstream ( self , message , steam_name ) : handler = getattr ( self , get_handler_name ( message ) , None ) if handler : await handler ( message , stream_name = steam_name ) else : await self . base_send ( message )
6172	def _select_manager ( backend_name ) : if backend_name == 'RedisBackend' : lock_manager = _LockManagerRedis elif backend_name == 'DatabaseBackend' : lock_manager = _LockManagerDB else : raise NotImplementedError return lock_manager
11902	def serve_dir ( dir_path ) : print ( 'Performing first pass index file generation' ) created_files = _create_index_files ( dir_path , True ) if ( PIL_ENABLED ) : print ( 'Performing PIL-enchanced optimised index file generation in background' ) background_indexer = BackgroundIndexFileGenerator ( dir_path ) background_indexer . run ( ) _run_server ( ) _clean_up ( created_files )
1150	def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass
68	def copy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return BoundingBox ( x1 = self . x1 if x1 is None else x1 , x2 = self . x2 if x2 is None else x2 , y1 = self . y1 if y1 is None else y1 , y2 = self . y2 if y2 is None else y2 , label = self . label if label is None else label )
12369	def rename ( self , id , name ) : return super ( DomainRecords , self ) . update ( id , name = name ) [ self . singular ]
13509	def sloccount ( ) : setup = options . get ( 'setup' ) packages = options . get ( 'packages' ) if setup else None if packages : dirs = [ x for x in packages if '.' not in x ] else : dirs = [ '.' ] ls = [ ] for d in dirs : ls += list ( path ( d ) . walkfiles ( ) ) files = ' ' . join ( ls ) param = options . paved . pycheck . sloccount . param sh ( 'sloccount {param} {files} | tee sloccount.sc' . format ( param = param , files = files ) )
9275	def apply_exclude_tags_regex ( self , all_tags ) : filtered = [ ] for tag in all_tags : if not re . match ( self . options . exclude_tags_regex , tag [ "name" ] ) : filtered . append ( tag ) if len ( all_tags ) == len ( filtered ) : self . warn_if_nonmatching_regex ( ) return filtered
12852	def scan ( xml ) : if xml . tag is et . Comment : yield { 'type' : COMMENT , 'text' : xml . text } return if xml . tag is et . PI : if xml . text : yield { 'type' : PI , 'target' : xml . target , 'text' : xml . text } else : yield { 'type' : PI , 'target' : xml . target } return obj = _elt2obj ( xml ) obj [ 'type' ] = ENTER yield obj assert type ( xml . tag ) is str , xml if xml . text : yield { 'type' : TEXT , 'text' : xml . text } for c in xml : for x in scan ( c ) : yield x if c . tail : yield { 'type' : TEXT , 'text' : c . tail } yield { 'type' : EXIT }
2997	def marketOhlcDF ( token = '' , version = '' ) : x = marketOhlc ( token , version ) data = [ ] for key in x : data . append ( x [ key ] ) data [ - 1 ] [ 'symbol' ] = key df = pd . io . json . json_normalize ( data ) _toDatetime ( df ) _reindex ( df , 'symbol' ) return df
4564	def fill ( strip , item , start = 0 , stop = None , step = 1 ) : if stop is None : stop = len ( strip ) for i in range ( start , stop , step ) : strip [ i ] = item
4373	def create ( ) : name = request . form . get ( "name" ) if name : room , created = get_or_create ( ChatRoom , name = name ) return redirect ( url_for ( 'room' , slug = room . slug ) ) return redirect ( url_for ( 'rooms' ) )
12563	def get_rois_centers_of_mass ( vol ) : from scipy . ndimage . measurements import center_of_mass roisvals = np . unique ( vol ) roisvals = roisvals [ roisvals != 0 ] rois_centers = OrderedDict ( ) for r in roisvals : rois_centers [ r ] = center_of_mass ( vol , vol , r ) return rois_centers
3159	def get_metadata ( self ) : try : r = requests . get ( 'https://login.mailchimp.com/oauth2/metadata' , auth = self ) except requests . exceptions . RequestException as e : raise e else : r . raise_for_status ( ) output = r . json ( ) if 'error' in output : raise requests . exceptions . RequestException ( output [ 'error' ] ) return output
4354	def _save_ack_callback ( self , msgid , callback ) : if msgid in self . ack_callbacks : return False self . ack_callbacks [ msgid ] = callback
1618	def IsCppString ( line ) : line = line . replace ( r'\\' , 'XX' ) return ( ( line . count ( '"' ) - line . count ( r'\"' ) - line . count ( "'\"'" ) ) & 1 ) == 1
9185	def _reassemble_binder ( id , tree , metadata ) : binder = cnxepub . Binder ( id , metadata = metadata ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) return binder
10498	def tripleClickMouse ( self , coord ) : modFlags = 0 for i in range ( 2 ) : self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags ) self . _queueMouseButton ( coord , Quartz . kCGMouseButtonLeft , modFlags , clickCount = 3 ) self . _postQueuedEvents ( )
6181	def load_PSFLab_file ( fname ) : if os . path . exists ( fname ) or os . path . exists ( fname + '.mat' ) : return loadmat ( fname ) [ 'data' ] else : raise IOError ( "Can't find PSF file '%s'" % fname )
6627	def _raiseUnavailableFor401 ( message ) : def __raiseUnavailableFor401 ( fn ) : def wrapped ( * args , ** kwargs ) : try : return fn ( * args , ** kwargs ) except requests . exceptions . HTTPError as e : if e . response . status_code == requests . codes . unauthorized : raise access_common . Unavailable ( message ) else : raise return wrapped return __raiseUnavailableFor401
7554	def store_all ( self ) : with h5py . File ( self . database . input , 'a' ) as io5 : fillsets = io5 [ "quartets" ] qiter = itertools . combinations ( xrange ( len ( self . samples ) ) , 4 ) i = 0 while i < self . params . nquartets : dat = np . array ( list ( itertools . islice ( qiter , self . _chunksize ) ) ) end = min ( self . params . nquartets , dat . shape [ 0 ] + i ) fillsets [ i : end ] = dat [ : end - i ] i += self . _chunksize print ( min ( i , self . params . nquartets ) )
4012	def configure_nfs_server ( ) : repos_for_export = get_all_repos ( active_only = True , include_specs_repo = False ) current_exports = _get_current_exports ( ) needed_exports = _get_exports_for_repos ( repos_for_export ) _ensure_managed_repos_dir_exists ( ) if not needed_exports . difference ( current_exports ) : if not _server_is_running ( ) : _restart_server ( ) return _write_exports_config ( needed_exports ) _restart_server ( )
1482	def get_commands_to_run ( self ) : if len ( self . packing_plan . container_plans ) == 0 : return { } if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : retval = { } retval [ 'heron-shell' ] = Command ( [ '%s' % self . heron_shell_binary , '--port=%s' % self . shell_port , '--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , '--secret=%s' % self . topology_id ] , self . shell_env ) return retval if self . shard == 0 : commands = self . _get_tmaster_processes ( ) else : self . _untar_if_needed ( ) commands = self . _get_streaming_processes ( ) commands . update ( self . _get_heron_support_processes ( ) ) return commands
4665	def id ( self ) : sigs = self . data [ "signatures" ] self . data . pop ( "signatures" , None ) h = hashlib . sha256 ( bytes ( self ) ) . digest ( ) self . data [ "signatures" ] = sigs return hexlify ( h [ : 20 ] ) . decode ( "ascii" )
1731	def call ( self , this , args = ( ) ) : if self . is_native : _args = SpaceTuple ( args ) _args . space = self . space return self . code ( this , _args ) else : return self . space . exe . _call ( self , this , args )
11850	def move_to ( self , thing , destination ) : "Move a thing to a new location." thing . bump = self . some_things_at ( destination , Obstacle ) if not thing . bump : thing . location = destination for o in self . observers : o . thing_moved ( thing )
3451	def find_blocked_reactions ( model , reaction_list = None , zero_cutoff = None , open_exchanges = False , processes = None ) : zero_cutoff = normalize_cutoff ( model , zero_cutoff ) with model : if open_exchanges : for reaction in model . exchanges : reaction . bounds = ( min ( reaction . lower_bound , - 1000 ) , max ( reaction . upper_bound , 1000 ) ) if reaction_list is None : reaction_list = model . reactions model . slim_optimize ( ) solution = get_solution ( model , reactions = reaction_list ) reaction_list = solution . fluxes [ solution . fluxes . abs ( ) < zero_cutoff ] . index . tolist ( ) flux_span = flux_variability_analysis ( model , fraction_of_optimum = 0. , reaction_list = reaction_list , processes = processes ) return flux_span [ flux_span . abs ( ) . max ( axis = 1 ) < zero_cutoff ] . index . tolist ( )
7968	def _run_io_threads ( self , handler ) : reader = ReadingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) writter = WrittingThread ( self . settings , handler , daemon = self . daemon , exc_queue = self . exc_queue ) self . io_threads += [ reader , writter ] reader . start ( ) writter . start ( )
6990	def parallel_varfeatures_lcdir ( lcdir , outdir , fileglob = None , maxobjects = None , timecols = None , magcols = None , errcols = None , recursive = True , mindet = 1000 , lcformat = 'hat-sql' , lcformatdir = None , nworkers = NCPUS ) : try : formatinfo = get_lcformat ( lcformat , use_lcformat_dir = lcformatdir ) if formatinfo : ( dfileglob , readerfunc , dtimecols , dmagcols , derrcols , magsarefluxes , normfunc ) = formatinfo else : LOGERROR ( "can't figure out the light curve format" ) return None except Exception as e : LOGEXCEPTION ( "can't figure out the light curve format" ) return None if not fileglob : fileglob = dfileglob LOGINFO ( 'searching for %s light curves in %s ...' % ( lcformat , lcdir ) ) if recursive is False : matching = glob . glob ( os . path . join ( lcdir , fileglob ) ) else : if sys . version_info [ : 2 ] > ( 3 , 4 ) : matching = glob . glob ( os . path . join ( lcdir , '**' , fileglob ) , recursive = True ) else : walker = os . walk ( lcdir ) matching = [ ] for root , dirs , _files in walker : for sdir in dirs : searchpath = os . path . join ( root , sdir , fileglob ) foundfiles = glob . glob ( searchpath ) if foundfiles : matching . extend ( foundfiles ) if matching and len ( matching ) > 0 : LOGINFO ( 'found %s light curves, getting varfeatures...' % len ( matching ) ) return parallel_varfeatures ( matching , outdir , maxobjects = maxobjects , timecols = timecols , magcols = magcols , errcols = errcols , mindet = mindet , lcformat = lcformat , lcformatdir = lcformatdir , nworkers = nworkers ) else : LOGERROR ( 'no light curve files in %s format found in %s' % ( lcformat , lcdir ) ) return None
11310	def get_object ( self , url , month_format = '%b' , day_format = '%d' ) : params = self . get_params ( url ) try : year = params [ self . _meta . year_part ] month = params [ self . _meta . month_part ] day = params [ self . _meta . day_part ] except KeyError : try : year , month , day = params [ '_0' ] , params [ '_1' ] , params [ '_2' ] except KeyError : raise OEmbedException ( 'Error extracting date from url parameters' ) try : tt = time . strptime ( '%s-%s-%s' % ( year , month , day ) , '%s-%s-%s' % ( '%Y' , month_format , day_format ) ) date = datetime . date ( * tt [ : 3 ] ) except ValueError : raise OEmbedException ( 'Error parsing date from: %s' % url ) if isinstance ( self . _meta . model . _meta . get_field ( self . _meta . date_field ) , DateTimeField ) : min_date = datetime . datetime . combine ( date , datetime . time . min ) max_date = datetime . datetime . combine ( date , datetime . time . max ) query = { '%s__range' % self . _meta . date_field : ( min_date , max_date ) } else : query = { self . _meta . date_field : date } for key , value in self . _meta . fields_to_match . iteritems ( ) : try : query [ value ] = params [ key ] except KeyError : raise OEmbedException ( '%s was not found in the urlpattern parameters. Valid names are: %s' % ( key , ', ' . join ( params . keys ( ) ) ) ) try : obj = self . get_queryset ( ) . get ( ** query ) except self . _meta . model . DoesNotExist : raise OEmbedException ( 'Requested object not found' ) return obj
12596	def _openpyxl_read_xl ( xl_path : str ) : try : wb = load_workbook ( filename = xl_path , read_only = True ) except : raise else : return wb
5768	def _advapi32_interpret_dsa_key_blob ( bit_size , public_blob , private_blob ) : len1 = 20 len2 = bit_size // 8 q_offset = len2 g_offset = q_offset + len1 x_offset = g_offset + len2 y_offset = x_offset p = int_from_bytes ( private_blob [ 0 : q_offset ] [ : : - 1 ] ) q = int_from_bytes ( private_blob [ q_offset : g_offset ] [ : : - 1 ] ) g = int_from_bytes ( private_blob [ g_offset : x_offset ] [ : : - 1 ] ) x = int_from_bytes ( private_blob [ x_offset : x_offset + len1 ] [ : : - 1 ] ) y = int_from_bytes ( public_blob [ y_offset : y_offset + len2 ] [ : : - 1 ] ) public_key_info = keys . PublicKeyInfo ( { 'algorithm' : keys . PublicKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'public_key' : core . Integer ( y ) , } ) private_key_info = keys . PrivateKeyInfo ( { 'version' : 0 , 'private_key_algorithm' : keys . PrivateKeyAlgorithm ( { 'algorithm' : 'dsa' , 'parameters' : keys . DSAParams ( { 'p' : p , 'q' : q , 'g' : g , } ) } ) , 'private_key' : core . Integer ( x ) , } ) return ( public_key_info , private_key_info )
10248	def update_context ( universe : BELGraph , graph : BELGraph ) : for namespace in get_namespaces ( graph ) : if namespace in universe . namespace_url : graph . namespace_url [ namespace ] = universe . namespace_url [ namespace ] elif namespace in universe . namespace_pattern : graph . namespace_pattern [ namespace ] = universe . namespace_pattern [ namespace ] else : log . warning ( 'namespace: %s missing from universe' , namespace ) for annotation in get_annotations ( graph ) : if annotation in universe . annotation_url : graph . annotation_url [ annotation ] = universe . annotation_url [ annotation ] elif annotation in universe . annotation_pattern : graph . annotation_pattern [ annotation ] = universe . annotation_pattern [ annotation ] elif annotation in universe . annotation_list : graph . annotation_list [ annotation ] = universe . annotation_list [ annotation ] else : log . warning ( 'annotation: %s missing from universe' , annotation )
12178	def ensureDetection ( self ) : if self . APs == False : self . log . debug ( "analysis attempted before event detection..." ) self . detect ( )
13320	def get_modules ( ) : modules = set ( ) cwd = os . getcwd ( ) for d in os . listdir ( cwd ) : if d == 'module.yml' : modules . add ( Module ( cwd ) ) path = unipath ( cwd , d ) if utils . is_module ( path ) : modules . add ( Module ( cwd ) ) module_paths = get_module_paths ( ) for module_path in module_paths : for d in os . listdir ( module_path ) : path = unipath ( module_path , d ) if utils . is_module ( path ) : modules . add ( Module ( path ) ) return sorted ( list ( modules ) , key = lambda x : x . name )
687	def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings
1596	def format_mtime ( mtime ) : now = datetime . now ( ) dt = datetime . fromtimestamp ( mtime ) return '%s %2d %5s' % ( dt . strftime ( '%b' ) , dt . day , dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) )
6862	def drop_views ( self , name = None , site = None ) : r = self . database_renderer result = r . sudo ( "mysql --batch -v -h {db_host} " "-u {db_user} -p'{db_password}' " "--execute=\"SELECT GROUP_CONCAT(CONCAT(TABLE_SCHEMA,'.',table_name) SEPARATOR ', ') AS views " "FROM INFORMATION_SCHEMA.views WHERE TABLE_SCHEMA = '{db_name}' ORDER BY table_name DESC;\"" ) result = re . findall ( r'^views[\s\t\r\n]+(.*)' , result , flags = re . IGNORECASE | re . DOTALL | re . MULTILINE ) if not result : return r . env . db_view_list = result [ 0 ] r . sudo ( "mysql -v -h {db_host} -u {db_user} -p'{db_password}' " "--execute=\"DROP VIEW {db_view_list} CASCADE;\"" )
3956	def update_nginx_from_config ( nginx_config ) : logging . info ( 'Updating nginx with new Dusty config' ) temp_dir = tempfile . mkdtemp ( ) os . mkdir ( os . path . join ( temp_dir , 'html' ) ) _write_nginx_config ( constants . NGINX_BASE_CONFIG , os . path . join ( temp_dir , constants . NGINX_PRIMARY_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'http' ] , os . path . join ( temp_dir , constants . NGINX_HTTP_CONFIG_NAME ) ) _write_nginx_config ( nginx_config [ 'stream' ] , os . path . join ( temp_dir , constants . NGINX_STREAM_CONFIG_NAME ) ) _write_nginx_config ( constants . NGINX_502_PAGE_HTML , os . path . join ( temp_dir , 'html' , constants . NGINX_502_PAGE_NAME ) ) sync_local_path_to_vm ( temp_dir , constants . NGINX_CONFIG_DIR_IN_VM )
10897	def get_scale_from_raw ( raw , scaled ) : t0 , t1 = scaled . min ( ) , scaled . max ( ) r0 , r1 = float ( raw . min ( ) ) , float ( raw . max ( ) ) rmin = ( t1 * r0 - t0 * r1 ) / ( t1 - t0 ) rmax = ( r1 - r0 ) / ( t1 - t0 ) + rmin return ( rmin , rmax )
1715	def compile ( self , start_loc = 0 ) : self . label_locs = { } if self . label_locs is None else self . label_locs loc = start_loc while loc < len ( self . tape ) : if type ( self . tape [ loc ] ) == LABEL : self . label_locs [ self . tape [ loc ] . num ] = loc del self . tape [ loc ] continue loc += 1 self . compiled = True
5010	def _call_post_with_user_override ( self , sap_user_id , url , payload ) : SAPSuccessFactorsEnterpriseCustomerConfiguration = apps . get_model ( 'sap_success_factors' , 'SAPSuccessFactorsEnterpriseCustomerConfiguration' ) oauth_access_token , _ = SAPSuccessFactorsAPIClient . get_oauth_access_token ( self . enterprise_configuration . sapsf_base_url , self . enterprise_configuration . key , self . enterprise_configuration . secret , self . enterprise_configuration . sapsf_company_id , sap_user_id , SAPSuccessFactorsEnterpriseCustomerConfiguration . USER_TYPE_USER ) response = requests . post ( url , data = payload , headers = { 'Authorization' : 'Bearer {}' . format ( oauth_access_token ) , 'content-type' : 'application/json' } ) return response . status_code , response . text
11666	def _get_rhos ( X , indices , Ks , max_K , save_all_Ks , min_dist ) : "Gets within-bag distances for each bag." logger . info ( "Getting within-bag distances..." ) if max_K >= X . n_pts . min ( ) : msg = "asked for K = {}, but there's a bag with only {} points" raise ValueError ( msg . format ( max_K , X . n_pts . min ( ) ) ) which_Ks = slice ( 1 , None ) if save_all_Ks else Ks indices = plog ( indices , name = "within-bag distances" ) rhos = [ None ] * len ( X ) for i , ( idx , bag ) in enumerate ( zip ( indices , X ) ) : r = np . sqrt ( idx . nn_index ( bag , max_K + 1 ) [ 1 ] [ : , which_Ks ] ) np . maximum ( min_dist , r , out = r ) rhos [ i ] = r return rhos
2753	def get_domain ( self , domain_name ) : return Domain . get_object ( api_token = self . token , domain_name = domain_name )
11974	def convert_nm ( nm , notation = IP_DOT , inotation = IP_UNKNOWN , check = True ) : return _convert ( nm , notation , inotation , _check = check , _isnm = True )
5635	def doc2md ( docstr , title , min_level = 1 , more_info = False , toc = True , maxdepth = 0 ) : text = doctrim ( docstr ) lines = text . split ( '\n' ) sections = find_sections ( lines ) if sections : level = min ( n for n , t in sections ) - 1 else : level = 1 shiftlevel = 0 if level < min_level : shiftlevel = min_level - level level = min_level sections = [ ( lev + shiftlevel , tit ) for lev , tit in sections ] head = next ( ( i for i , l in enumerate ( lines ) if is_heading ( l ) ) , 0 ) md = [ make_heading ( level , title ) , "" , ] + lines [ : head ] if toc : md += make_toc ( sections , maxdepth ) md += [ '' ] md += _doc2md ( lines [ head : ] , shiftlevel ) if more_info : return ( md , sections ) else : return "\n" . join ( md )
9129	def store_populate_failed ( cls , resource : str , session : Optional [ Session ] = None ) -> 'Action' : action = cls . make_populate_failed ( resource ) _store_helper ( action , session = session ) return action
9402	def _exist ( self , name ) : cmd = 'exist("%s")' % name resp = self . _engine . eval ( cmd , silent = True ) . strip ( ) exist = int ( resp . split ( ) [ - 1 ] ) if exist == 0 : msg = 'Value "%s" does not exist in Octave workspace' raise Oct2PyError ( msg % name ) return exist
5192	def send_select_and_operate_command ( self , command , index , callback = asiodnp3 . PrintingCommandCallback . Get ( ) , config = opendnp3 . TaskConfig ( ) . Default ( ) ) : self . master . SelectAndOperate ( command , index , callback , config )
5038	def enroll_user ( cls , enterprise_customer , user , course_mode , * course_ids ) : enterprise_customer_user , __ = EnterpriseCustomerUser . objects . get_or_create ( enterprise_customer = enterprise_customer , user_id = user . id ) enrollment_client = EnrollmentApiClient ( ) succeeded = True for course_id in course_ids : try : enrollment_client . enroll_user_in_course ( user . username , course_id , course_mode ) except HttpClientError as exc : if cls . is_user_enrolled ( user , course_id , course_mode ) : succeeded = True else : succeeded = False default_message = 'No error message provided' try : error_message = json . loads ( exc . content . decode ( ) ) . get ( 'message' , default_message ) except ValueError : error_message = default_message logging . error ( 'Error while enrolling user %(user)s: %(message)s' , dict ( user = user . username , message = error_message ) ) if succeeded : __ , created = EnterpriseCourseEnrollment . objects . get_or_create ( enterprise_customer_user = enterprise_customer_user , course_id = course_id ) if created : track_enrollment ( 'admin-enrollment' , user . id , course_id ) return succeeded
1864	def SARX ( cpu , dest , src , count ) : OperandSize = dest . size count = count . read ( ) countMask = { 8 : 0x1f , 16 : 0x1f , 32 : 0x1f , 64 : 0x3f } [ OperandSize ] tempCount = count & countMask tempDest = value = src . read ( ) sign = value & ( 1 << ( OperandSize - 1 ) ) while tempCount != 0 : cpu . CF = ( value & 0x1 ) != 0 value = ( value >> 1 ) | sign tempCount = tempCount - 1 res = dest . write ( value )
8998	def string ( self , string ) : object_ = json . loads ( string ) return self . object ( object_ )
8329	def findNext ( self , name = None , attrs = { } , text = None , ** kwargs ) : return self . _findOne ( self . findAllNext , name , attrs , text , ** kwargs )
2232	def _register_numpy_extensions ( self ) : import numpy as np numpy_floating_types = ( np . float16 , np . float32 , np . float64 ) if hasattr ( np , 'float128' ) : numpy_floating_types = numpy_floating_types + ( np . float128 , ) @ self . add_iterable_check def is_object_ndarray ( data ) : return isinstance ( data , np . ndarray ) and data . dtype . kind == 'O' @ self . register ( np . ndarray ) def hash_numpy_array ( data ) : if data . dtype . kind == 'O' : msg = 'directly hashing ndarrays with dtype=object is unstable' raise TypeError ( msg ) else : header = b'' . join ( _hashable_sequence ( ( len ( data . shape ) , data . shape ) ) ) dtype = b'' . join ( _hashable_sequence ( data . dtype . descr ) ) hashable = header + dtype + data . tobytes ( ) prefix = b'NDARR' return prefix , hashable @ self . register ( ( np . int64 , np . int32 , np . int16 , np . int8 ) + ( np . uint64 , np . uint32 , np . uint16 , np . uint8 ) ) def _hash_numpy_int ( data ) : return _convert_to_hashable ( int ( data ) ) @ self . register ( numpy_floating_types ) def _hash_numpy_float ( data ) : return _convert_to_hashable ( float ( data ) ) @ self . register ( np . random . RandomState ) def _hash_numpy_random_state ( data ) : hashable = b'' . join ( _hashable_sequence ( data . get_state ( ) ) ) prefix = b'RNG' return prefix , hashable
9839	def __gridconnections ( self ) : try : tok = self . __consume ( ) except DXParserNoTokens : return if tok . equals ( 'counts' ) : shape = [ ] try : while True : self . __peek ( ) . value ( 'INTEGER' ) tok = self . __consume ( ) shape . append ( tok . value ( 'INTEGER' ) ) except ( DXParserNoTokens , ValueError ) : pass if len ( shape ) == 0 : raise DXParseError ( 'gridconnections: no shape parameters' ) self . currentobject [ 'shape' ] = shape else : raise DXParseError ( 'gridconnections: ' + str ( tok ) + ' not recognized.' )
3801	def calculate ( self , T , method ) : r if method == SHEFFY_JOHNSON : kl = Sheffy_Johnson ( T , self . MW , self . Tm ) elif method == SATO_RIEDEL : kl = Sato_Riedel ( T , self . MW , self . Tb , self . Tc ) elif method == GHARAGHEIZI_L : kl = Gharagheizi_liquid ( T , self . MW , self . Tb , self . Pc , self . omega ) elif method == NICOLA : kl = Nicola ( T , self . MW , self . Tc , self . Pc , self . omega ) elif method == NICOLA_ORIGINAL : kl = Nicola_original ( T , self . MW , self . Tc , self . omega , self . Hfus ) elif method == LAKSHMI_PRASAD : kl = Lakshmi_Prasad ( T , self . MW ) elif method == BAHADORI_L : kl = Bahadori_liquid ( T , self . MW ) elif method == DIPPR_PERRY_8E : kl = EQ100 ( T , * self . Perrys2_315_coeffs ) elif method == VDI_PPDS : kl = horner ( self . VDI_PPDS_coeffs , T ) elif method == COOLPROP : kl = CoolProp_T_dependent_property ( T , self . CASRN , 'L' , 'l' ) elif method in self . tabular_data : kl = self . interpolate ( T , method ) return kl
11204	def valuestodict ( key ) : dout = { } size = winreg . QueryInfoKey ( key ) [ 1 ] tz_res = None for i in range ( size ) : key_name , value , dtype = winreg . EnumValue ( key , i ) if dtype == winreg . REG_DWORD or dtype == winreg . REG_DWORD_LITTLE_ENDIAN : if value & ( 1 << 31 ) : value = value - ( 1 << 32 ) elif dtype == winreg . REG_SZ : if value . startswith ( '@tzres' ) : tz_res = tz_res or tzres ( ) value = tz_res . name_from_string ( value ) value = value . rstrip ( '\x00' ) dout [ key_name ] = value return dout
2866	def write8 ( self , register , value ) : value = value & 0xFF self . _bus . write_byte_data ( self . _address , register , value ) self . _logger . debug ( "Wrote 0x%02X to register 0x%02X" , value , register )
12755	def enable_motors ( self , max_force ) : for joint in self . joints : amotor = getattr ( joint , 'amotor' , joint ) amotor . max_forces = max_force if max_force > 0 : amotor . enable_feedback ( ) else : amotor . disable_feedback ( )
3279	def add_provider ( self , share , provider , readonly = False ) : share = "/" + share . strip ( "/" ) assert share not in self . provider_map if compat . is_basestring ( provider ) : provider = FilesystemProvider ( provider , readonly ) elif type ( provider ) in ( dict , ) : if "provider" in provider : prov_class = dynamic_import_class ( provider [ "provider" ] ) provider = prov_class ( * provider . get ( "args" , [ ] ) , ** provider . get ( "kwargs" , { } ) ) else : provider = FilesystemProvider ( provider [ "root" ] , bool ( provider . get ( "readonly" , False ) ) ) elif type ( provider ) in ( list , tuple ) : raise ValueError ( "Provider {}: tuple/list syntax is no longer supported" . format ( provider ) ) if not isinstance ( provider , DAVProvider ) : raise ValueError ( "Invalid provider {}" . format ( provider ) ) provider . set_share_path ( share ) if self . mount_path : provider . set_mount_path ( self . mount_path ) provider . set_lock_manager ( self . lock_manager ) provider . set_prop_manager ( self . prop_manager ) self . provider_map [ share ] = provider self . sorted_share_list = [ s . lower ( ) for s in self . provider_map . keys ( ) ] self . sorted_share_list = sorted ( self . sorted_share_list , key = len , reverse = True ) return provider
7194	def histogram_stretch ( self , use_bands , ** kwargs ) : data = self . _read ( self [ use_bands , ... ] , ** kwargs ) data = np . rollaxis ( data . astype ( np . float32 ) , 0 , 3 ) return self . _histogram_stretch ( data , ** kwargs )
13292	def json_attributes ( self , vfuncs = None ) : vfuncs = vfuncs or [ ] js = { 'global' : { } } for k in self . ncattrs ( ) : js [ 'global' ] [ k ] = self . getncattr ( k ) for varname , var in self . variables . items ( ) : js [ varname ] = { } for k in var . ncattrs ( ) : z = var . getncattr ( k ) try : assert not np . isnan ( z ) . all ( ) js [ varname ] [ k ] = z except AssertionError : js [ varname ] [ k ] = None except TypeError : js [ varname ] [ k ] = z for vf in vfuncs : try : js [ varname ] . update ( vfuncs ( var ) ) except BaseException : logger . exception ( "Could not apply custom variable attribue function" ) return json . loads ( json . dumps ( js , cls = BasicNumpyEncoder ) )
9627	def url ( self ) : return reverse ( '%s:detail' % URL_NAMESPACE , kwargs = { 'module' : self . module , 'preview' : type ( self ) . __name__ , } )
3885	def get_user ( self , user_id ) : try : return self . _user_dict [ user_id ] except KeyError : logger . warning ( 'UserList returning unknown User for UserID %s' , user_id ) return User ( user_id , None , None , None , [ ] , False )
3268	def md_dynamic_default_values_info ( name , node ) : configurations = node . find ( "configurations" ) if configurations is not None : configurations = [ ] for n in node . findall ( "configuration" ) : dimension = n . find ( "dimension" ) dimension = dimension . text if dimension is not None else None policy = n . find ( "policy" ) policy = policy . text if policy is not None else None defaultValueExpression = n . find ( "defaultValueExpression" ) defaultValueExpression = defaultValueExpression . text if defaultValueExpression is not None else None configurations . append ( DynamicDefaultValuesConfiguration ( dimension , policy , defaultValueExpression ) ) return DynamicDefaultValues ( name , configurations )
6658	def _core_computation ( X_train , X_test , inbag , pred_centered , n_trees , memory_constrained = False , memory_limit = None , test_mode = False ) : if not memory_constrained : return np . sum ( ( np . dot ( inbag - 1 , pred_centered . T ) / n_trees ) ** 2 , 0 ) if not memory_limit : raise ValueError ( 'If memory_constrained=True, must provide' , 'memory_limit.' ) chunk_size = int ( ( memory_limit * 1e6 ) / ( 8.0 * X_train . shape [ 0 ] ) ) if chunk_size == 0 : min_limit = 8.0 * X_train . shape [ 0 ] / 1e6 raise ValueError ( 'memory_limit provided is too small.' + 'For these dimensions, memory_limit must ' + 'be greater than or equal to %.3e' % min_limit ) chunk_edges = np . arange ( 0 , X_test . shape [ 0 ] + chunk_size , chunk_size ) inds = range ( X_test . shape [ 0 ] ) chunks = [ inds [ chunk_edges [ i ] : chunk_edges [ i + 1 ] ] for i in range ( len ( chunk_edges ) - 1 ) ] if test_mode : print ( 'Number of chunks: %d' % ( len ( chunks ) , ) ) V_IJ = np . concatenate ( [ np . sum ( ( np . dot ( inbag - 1 , pred_centered [ chunk ] . T ) / n_trees ) ** 2 , 0 ) for chunk in chunks ] ) return V_IJ
6432	def encode ( self , word ) : word = word . upper ( ) word = self . _delete_consecutive_repeats ( word ) i = 0 while i < len ( word ) : for match_len in range ( 4 , 1 , - 1 ) : if word [ i : i + match_len ] in self . _rules [ match_len ] : repl = self . _rules [ match_len ] [ word [ i : i + match_len ] ] word = word [ : i ] + repl + word [ i + match_len : ] i += len ( repl ) break else : i += 1 word = word [ : 1 ] + word [ 1 : ] . translate ( self . _del_trans ) return word
10335	def build_delete_node_by_hash ( manager : Manager ) -> Callable [ [ BELGraph , str ] , None ] : @ in_place_transformation def delete_node_by_hash ( graph : BELGraph , node_hash : str ) -> None : node = manager . get_dsl_by_hash ( node_hash ) graph . remove_node ( node ) return delete_node_by_hash
761	def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal
5906	def create_portable_topology ( topol , struct , ** kwargs ) : _topoldir , _topol = os . path . split ( topol ) processed = kwargs . pop ( 'processed' , os . path . join ( _topoldir , 'pp_' + _topol ) ) grompp_kwargs , mdp_kwargs = filter_grompp_options ( ** kwargs ) mdp_kwargs = add_mdp_includes ( topol , mdp_kwargs ) with tempfile . NamedTemporaryFile ( suffix = '.mdp' ) as mdp : mdp . write ( '; empty mdp file\ninclude = {include!s}\n' . format ( ** mdp_kwargs ) ) mdp . flush ( ) grompp_kwargs [ 'p' ] = topol grompp_kwargs [ 'pp' ] = processed grompp_kwargs [ 'f' ] = mdp . name grompp_kwargs [ 'c' ] = struct grompp_kwargs [ 'v' ] = False try : gromacs . grompp ( ** grompp_kwargs ) finally : utilities . unlink_gmx ( 'topol.tpr' , 'mdout.mdp' ) return utilities . realpath ( processed )
10598	def clear ( self ) : self . solid_density = 1.0 self . H2O_mass = 0.0 self . size_class_masses = self . size_class_masses * 0.0
5672	def get_transfer_stop_pairs ( self ) : transfer_stop_pairs = [ ] previous_arrival_stop = None current_trip_id = None for leg in self . legs : if leg . trip_id is not None and leg . trip_id != current_trip_id and previous_arrival_stop is not None : transfer_stop_pair = ( previous_arrival_stop , leg . departure_stop ) transfer_stop_pairs . append ( transfer_stop_pair ) previous_arrival_stop = leg . arrival_stop current_trip_id = leg . trip_id return transfer_stop_pairs
10478	def _waitFor ( self , timeout , notification , ** kwargs ) : callback = self . _matchOther retelem = None callbackArgs = None callbackKwargs = None if 'callback' in kwargs : callback = kwargs [ 'callback' ] del kwargs [ 'callback' ] if 'args' in kwargs : if not isinstance ( kwargs [ 'args' ] , tuple ) : errStr = 'Notification callback args not given as a tuple' raise TypeError ( errStr ) callbackArgs = kwargs [ 'args' ] del kwargs [ 'args' ] if 'kwargs' in kwargs : if not isinstance ( kwargs [ 'kwargs' ] , dict ) : errStr = 'Notification callback kwargs not given as a dict' raise TypeError ( errStr ) callbackKwargs = kwargs [ 'kwargs' ] del kwargs [ 'kwargs' ] if kwargs : if callbackKwargs : callbackKwargs . update ( kwargs ) else : callbackKwargs = kwargs else : callbackArgs = ( retelem , ) callbackKwargs = kwargs return self . _setNotification ( timeout , notification , callback , callbackArgs , callbackKwargs )
11702	def cosine ( vec1 , vec2 ) : if norm ( vec1 ) > 0 and norm ( vec2 ) > 0 : return dot ( vec1 , vec2 ) / ( norm ( vec1 ) * norm ( vec2 ) ) else : return 0.0
3504	def _add_cycle_free ( model , fluxes ) : model . objective = model . solver . interface . Objective ( Zero , direction = "min" , sloppy = True ) objective_vars = [ ] for rxn in model . reactions : flux = fluxes [ rxn . id ] if rxn . boundary : rxn . bounds = ( flux , flux ) continue if flux >= 0 : rxn . bounds = max ( 0 , rxn . lower_bound ) , max ( flux , rxn . upper_bound ) objective_vars . append ( rxn . forward_variable ) else : rxn . bounds = min ( flux , rxn . lower_bound ) , min ( 0 , rxn . upper_bound ) objective_vars . append ( rxn . reverse_variable ) model . objective . set_linear_coefficients ( { v : 1.0 for v in objective_vars } )
12598	def read_xl ( xl_path : str ) : xl_path , choice = _check_xl_path ( xl_path ) reader = XL_READERS [ choice ] return reader ( xl_path )
12368	def records ( self , name ) : if self . get ( name ) : return DomainRecords ( self . api , name )
12355	def delete ( self , wait = True ) : resp = self . parent . delete ( self . id ) if wait : self . wait ( ) return resp
6843	def set_permissions ( self ) : r = self . local_renderer for path in r . env . paths_owned : r . env . path_owned = path r . sudo ( 'chown {celery_daemon_user}:{celery_daemon_user} {celery_path_owned}' )
9184	def _node_to_model ( tree_or_item , metadata = None , parent = None , lucent_id = cnxepub . TRANSLUCENT_BINDER_ID ) : if 'contents' in tree_or_item : tree = tree_or_item binder = cnxepub . TranslucentBinder ( metadata = tree ) for item in tree [ 'contents' ] : node = _node_to_model ( item , parent = binder , lucent_id = lucent_id ) if node . metadata [ 'title' ] != item [ 'title' ] : binder . set_title_for_node ( node , item [ 'title' ] ) result = binder else : item = tree_or_item result = cnxepub . DocumentPointer ( item [ 'id' ] , metadata = item ) if parent is not None : parent . append ( result ) return result
9761	def logs ( ctx , job , past , follow , hide_time ) : def get_experiment_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment . logs ( user , project_name , _experiment , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) def get_experiment_job_logs ( ) : if past : try : response = PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , stream = False ) get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time , stream = False ) ( response . content . decode ( ) . split ( '\n' ) ) print ( ) if not follow : return except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : if not follow : Printer . print_error ( 'Could not get logs for experiment `{}`.' . format ( _experiment ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) try : PolyaxonClient ( ) . experiment_job . logs ( user , project_name , _experiment , _job , message_handler = get_logs_handler ( handle_job_info = True , show_timestamp = not hide_time ) ) except ( PolyaxonHTTPError , PolyaxonShouldExitError , PolyaxonClientException ) as e : Printer . print_error ( 'Could not get logs for job `{}`.' . format ( _job ) ) Printer . print_error ( 'Error message `{}`.' . format ( e ) ) sys . exit ( 1 ) user , project_name , _experiment = get_project_experiment_or_local ( ctx . obj . get ( 'project' ) , ctx . obj . get ( 'experiment' ) ) if job : _job = get_experiment_job_or_local ( job ) get_experiment_job_logs ( ) else : get_experiment_logs ( )
10395	def iter_leaves ( self ) -> Iterable [ BaseEntity ] : for node in self . graph : if self . tag in self . graph . nodes [ node ] : continue if not any ( self . tag not in self . graph . nodes [ p ] for p in self . graph . predecessors ( node ) ) : yield node
11958	def is_dec ( ip ) : try : dec = int ( str ( ip ) ) except ValueError : return False if dec > 4294967295 or dec < 0 : return False return True
5188	def connect ( host = 'localhost' , port = 8080 , ssl_verify = False , ssl_key = None , ssl_cert = None , timeout = 10 , protocol = None , url_path = '/' , username = None , password = None , token = None ) : return BaseAPI ( host = host , port = port , timeout = timeout , ssl_verify = ssl_verify , ssl_key = ssl_key , ssl_cert = ssl_cert , protocol = protocol , url_path = url_path , username = username , password = password , token = token )
13633	def _negotiateHandler ( self , request ) : accept = _parseAccept ( request . requestHeaders . getRawHeaders ( 'Accept' ) ) for contentType in accept . keys ( ) : handler = self . _acceptHandlers . get ( contentType . lower ( ) ) if handler is not None : return handler , handler . contentType if self . _fallback : handler = self . _handlers [ 0 ] return handler , handler . contentType return NotAcceptable ( ) , None
11611	def run ( self , model , tol = 0.001 , max_iters = 999 , verbose = True ) : orig_err_states = np . seterr ( all = 'raise' ) np . seterr ( under = 'ignore' ) if verbose : print print "Iter No Time (hh:mm:ss) Total change (TPM) " print "------- --------------- ----------------------" num_iters = 0 err_sum = 1000000.0 time0 = time . time ( ) target_err = 1000000.0 * tol while err_sum > target_err and num_iters < max_iters : prev_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) prev_isoform_expression *= ( 1000000.0 / prev_isoform_expression . sum ( ) ) self . update_allelic_expression ( model = model ) curr_isoform_expression = self . get_allelic_expression ( ) . sum ( axis = 0 ) curr_isoform_expression *= ( 1000000.0 / curr_isoform_expression . sum ( ) ) err = np . abs ( curr_isoform_expression - prev_isoform_expression ) err_sum = err . sum ( ) num_iters += 1 if verbose : time1 = time . time ( ) delmin , s = divmod ( int ( time1 - time0 ) , 60 ) h , m = divmod ( delmin , 60 ) print " %5d %4d:%02d:%02d %9.1f / 1000000" % ( num_iters , h , m , s , err_sum )
429	def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )
5510	def release ( self ) : if self . value is not None : self . value += 1 if self . value > self . maximum_value : raise ValueError ( "Too many releases" )
4601	def main ( ) : if not _curses : if os . name == 'nt' : raise ValueError ( 'curses is not supported under Windows' ) raise ValueError ( 'Your platform does not support curses.' ) try : driver = next ( iter ( Curses . DRIVERS ) ) except : raise ValueError ( 'No Curses driver in project' ) _curses . wrapper ( driver . run_in_curses )
9860	async def rt_connect ( self , loop ) : if self . sub_manager is not None : return self . sub_manager = SubscriptionManager ( loop , "token={}" . format ( self . _access_token ) , SUB_ENDPOINT ) self . sub_manager . start ( )
12588	def insert_volumes_in_one_dataset ( file_path , h5path , file_list , newshape = None , concat_axis = 0 , dtype = None , append = True ) : def isalambda ( v ) : return isinstance ( v , type ( lambda : None ) ) and v . __name__ == '<lambda>' mode = 'w' if os . path . exists ( file_path ) : if append : mode = 'a' imgs = [ nib . load ( vol ) for vol in file_list ] shapes = [ np . array ( img . get_shape ( ) ) for img in imgs ] if newshape is not None : if isalambda ( newshape ) : nushapes = np . array ( [ newshape ( shape ) for shape in shapes ] ) else : nushapes = np . array ( [ shape for shape in shapes ] ) for nushape in nushapes : assert ( len ( nushape ) - 1 < concat_axis ) n_dims = nushapes . shape [ 1 ] ds_shape = np . zeros ( n_dims , dtype = np . int ) for a in list ( range ( n_dims ) ) : if a == concat_axis : ds_shape [ a ] = np . sum ( nushapes [ : , concat_axis ] ) else : ds_shape [ a ] = np . max ( nushapes [ : , a ] ) if dtype is None : dtype = imgs [ 0 ] . get_data_dtype ( ) with h5py . File ( file_path , mode ) as f : try : ic = 0 h5grp = f . create_group ( os . path . dirname ( h5path ) ) h5ds = h5grp . create_dataset ( os . path . basename ( h5path ) , ds_shape , dtype ) for img in imgs : nushape = nushapes [ ic , : ] def append_to_dataset ( h5ds , idx , data , concat_axis ) : shape = data . shape ndims = len ( shape ) if ndims == 1 : if concat_axis == 0 : h5ds [ idx ] = data elif ndims == 2 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif ndims == 3 : if concat_axis == 0 : h5ds [ idx ] = data elif concat_axis == 1 : h5ds [ idx ] = data elif concat_axis == 2 : h5ds [ idx ] = data append_to_dataset ( h5ds , ic , np . reshape ( img . get_data ( ) , tuple ( nushape ) ) , concat_axis ) ic += 1 except ValueError as ve : raise Exception ( 'Error creating group {} in hdf file {}' . format ( h5path , file_path ) ) from ve
12478	def get_sys_path ( rcpath , app_name , section_name = None ) : if op . exists ( rcpath ) : return op . realpath ( op . expanduser ( rcpath ) ) try : settings = rcfile ( app_name , section_name ) except : raise try : sys_path = op . expanduser ( settings [ rcpath ] ) except KeyError : raise IOError ( 'Could not find an existing variable with name {0} in' ' section {1} of {2}rc config setup. Maybe it is a ' ' folder that could not be found.' . format ( rcpath , section_name , app_name ) ) else : if not op . exists ( sys_path ) : raise IOError ( 'Could not find the path {3} indicated by the ' 'variable {0} in section {1} of {2}rc config ' 'setup.' . format ( rcpath , section_name , app_name , sys_path ) ) return op . realpath ( op . expanduser ( sys_path ) )
7016	def concat_write_pklc ( lcbasedir , objectid , aperture = 'TF1' , postfix = '.gz' , sortby = 'rjd' , normalize = True , outdir = None , recursive = True ) : concatlcd = concatenate_textlcs_for_objectid ( lcbasedir , objectid , aperture = aperture , sortby = sortby , normalize = normalize , recursive = recursive ) if not outdir : outdir = 'pklcs' if not os . path . exists ( outdir ) : os . mkdir ( outdir ) outfpath = os . path . join ( outdir , '%s-%s-pklc.pkl' % ( concatlcd [ 'objectid' ] , aperture ) ) pklc = lcdict_to_pickle ( concatlcd , outfile = outfpath ) return pklc
5076	def is_course_run_upgradeable ( course_run ) : now = datetime . datetime . now ( pytz . UTC ) for seat in course_run . get ( 'seats' , [ ] ) : if seat . get ( 'type' ) == 'verified' : upgrade_deadline = parse_datetime_handle_invalid ( seat . get ( 'upgrade_deadline' ) ) return not upgrade_deadline or upgrade_deadline > now return False
7822	def _make_response ( self , nonce , salt , iteration_count ) : self . _salted_password = self . Hi ( self . Normalize ( self . password ) , salt , iteration_count ) self . password = None if self . channel_binding : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header + self . _cb_data ) else : channel_binding = b"c=" + standard_b64encode ( self . _gs2_header ) client_final_message_without_proof = ( channel_binding + b",r=" + nonce ) client_key = self . HMAC ( self . _salted_password , b"Client Key" ) stored_key = self . H ( client_key ) auth_message = ( self . _client_first_message_bare + b"," + self . _server_first_message + b"," + client_final_message_without_proof ) self . _auth_message = auth_message client_signature = self . HMAC ( stored_key , auth_message ) client_proof = self . XOR ( client_key , client_signature ) proof = b"p=" + standard_b64encode ( client_proof ) client_final_message = ( client_final_message_without_proof + b"," + proof ) return Response ( client_final_message )
10099	def snippets ( self , timeout = None ) : return self . _api_request ( self . SNIPPETS_ENDPOINT , self . HTTP_GET , timeout = timeout )
4809	def prepare_feature ( best_processed_path , option = 'train' ) : n_pad = 21 n_pad_2 = int ( ( n_pad - 1 ) / 2 ) pad = [ { 'char' : ' ' , 'type' : 'p' , 'target' : True } ] df_pad = pd . DataFrame ( pad * n_pad_2 ) df = [ ] for article_type in article_types : df . append ( pd . read_csv ( os . path . join ( best_processed_path , option , 'df_best_{}_{}.csv' . format ( article_type , option ) ) ) ) df = pd . concat ( df ) df = pd . concat ( ( df_pad , df , df_pad ) ) df [ 'char' ] = df [ 'char' ] . map ( lambda x : CHARS_MAP . get ( x , 80 ) ) df [ 'type' ] = df [ 'type' ] . map ( lambda x : CHAR_TYPES_MAP . get ( x , 4 ) ) df_pad = create_n_gram_df ( df , n_pad = n_pad ) char_row = [ 'char' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'char' ] type_row = [ 'type' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type-' + str ( i + 1 ) for i in range ( n_pad_2 ) ] + [ 'type' ] x_char = df_pad [ char_row ] . as_matrix ( ) x_type = df_pad [ type_row ] . as_matrix ( ) y = df_pad [ 'target' ] . astype ( int ) . as_matrix ( ) return x_char , x_type , y
8125	def draw_cornu_bezier ( x0 , y0 , t0 , t1 , s0 , c0 , flip , cs , ss , cmd , scale , rot ) : s = None for j in range ( 0 , 5 ) : t = j * .2 t2 = t + .2 curvetime = t0 + t * ( t1 - t0 ) curvetime2 = t0 + t2 * ( t1 - t0 ) Dt = ( curvetime2 - curvetime ) * scale if not s : s , c = eval_cornu ( curvetime ) s *= flip s -= s0 c -= c0 dx1 = cos ( pow ( curvetime , 2 ) + ( flip * rot ) ) dy1 = flip * sin ( pow ( curvetime , 2 ) + ( flip * rot ) ) x = ( ( c * cs - s * ss ) + x0 ) y = ( ( s * cs + c * ss ) + y0 ) s2 , c2 = eval_cornu ( curvetime2 ) s2 *= flip s2 -= s0 c2 -= c0 dx2 = cos ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) dy2 = flip * sin ( pow ( curvetime2 , 2 ) + ( flip * rot ) ) x3 = ( ( c2 * cs - s2 * ss ) + x0 ) y3 = ( ( s2 * cs + c2 * ss ) + y0 ) x1 = ( x + ( ( Dt / 3.0 ) * dx1 ) ) y1 = ( y + ( ( Dt / 3.0 ) * dy1 ) ) x2 = ( x3 - ( ( Dt / 3.0 ) * dx2 ) ) y2 = ( y3 - ( ( Dt / 3.0 ) * dy2 ) ) if cmd == 'moveto' : print_pt ( x , y , cmd ) cmd = 'curveto' print_crv ( x1 , y1 , x2 , y2 , x3 , y3 ) dx1 , dy1 = dx2 , dy2 x , y = x3 , y3 return cmd
11664	def start ( self , total ) : self . logger . info ( json . dumps ( [ 'START' , self . name , total ] ) )
4199	def identify_names ( code ) : finder = NameFinder ( ) finder . visit ( ast . parse ( code ) ) example_code_obj = { } for name , full_name in finder . get_mapping ( ) : module , attribute = full_name . rsplit ( '.' , 1 ) module_short = get_short_module_name ( module , attribute ) cobj = { 'name' : attribute , 'module' : module , 'module_short' : module_short } example_code_obj [ name ] = cobj return example_code_obj
13728	def balance ( address ) : txhistory = Address . transactions ( address ) balance = 0 for i in txhistory : if i . recipientId == address : balance += i . amount if i . senderId == address : balance -= ( i . amount + i . fee ) delegates = Delegate . delegates ( ) for i in delegates : if address == i . address : forged_blocks = Delegate . blocks ( i . pubkey ) for block in forged_blocks : balance += ( block . reward + block . totalFee ) if balance < 0 : height = Node . height ( ) logger . fatal ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) raise NegativeBalanceError ( 'Negative balance for address {0}, Nodeheight: {1)' . format ( address , height ) ) return balance
6688	def groupinstall ( group , options = None ) : manager = MANAGER if options is None : options = [ ] elif isinstance ( options , str ) : options = [ options ] options = " " . join ( options ) run_as_root ( '%(manager)s %(options)s groupinstall "%(group)s"' % locals ( ) , pty = False )
2398	def encode_plus ( s ) : regex = r"\+" pat = re . compile ( regex ) return pat . sub ( "%2B" , s )
12072	def show ( self ) : copied = self . copy ( ) enumerated = [ el for el in enumerate ( copied ) ] for ( group_ind , specs ) in enumerated : if len ( enumerated ) > 1 : print ( "Group %d" % group_ind ) ordering = self . constant_keys + self . varying_keys spec_lines = [ ', ' . join ( [ '%s=%s' % ( k , s [ k ] ) for k in ordering ] ) for s in specs ] print ( '\n' . join ( [ '%d: %s' % ( i , l ) for ( i , l ) in enumerate ( spec_lines ) ] ) ) print ( 'Remaining arguments not available for %s' % self . __class__ . __name__ )
13653	def Text ( name , encoding = None ) : def _match ( request , value ) : return name , query . Text ( value , encoding = contentEncoding ( request . requestHeaders , encoding ) ) return _match
5760	def configure_ci_jobs ( config_url , rosdistro_name , ci_build_name , groovy_script = None , dry_run = False ) : config = get_config_index ( config_url ) build_files = get_ci_build_files ( config , rosdistro_name ) build_file = build_files [ ci_build_name ] index = get_index ( config . rosdistro_index_url ) targets = [ ] for os_name in build_file . targets . keys ( ) : for os_code_name in build_file . targets [ os_name ] . keys ( ) : for arch in build_file . targets [ os_name ] [ os_code_name ] : targets . append ( ( os_name , os_code_name , arch ) ) print ( 'The build file contains the following targets:' ) for os_name , os_code_name , arch in targets : print ( ' -' , os_name , os_code_name , arch ) dist_file = get_distribution_file ( index , rosdistro_name , build_file ) if not dist_file : print ( 'No distribution file matches the build file' ) return ci_view_name = get_ci_view_name ( rosdistro_name ) from ros_buildfarm . jenkins import connect jenkins = connect ( config . jenkins_url ) if groovy_script is None else False view_configs = { } views = { ci_view_name : configure_ci_view ( jenkins , ci_view_name , dry_run = dry_run ) } if not jenkins : view_configs . update ( views ) groovy_data = { 'dry_run' : dry_run , 'expected_num_views' : len ( view_configs ) , } ci_job_names = [ ] job_configs = OrderedDict ( ) is_disabled = False for os_name , os_code_name , arch in targets : try : job_name , job_config = configure_ci_job ( config_url , rosdistro_name , ci_build_name , os_name , os_code_name , arch , config = config , build_file = build_file , index = index , dist_file = dist_file , jenkins = jenkins , views = views , is_disabled = is_disabled , groovy_script = groovy_script , dry_run = dry_run , trigger_timer = build_file . jenkins_job_schedule ) ci_job_names . append ( job_name ) if groovy_script is not None : print ( "Configuration for job '%s'" % job_name ) job_configs [ job_name ] = job_config except JobValidationError as e : print ( e . message , file = sys . stderr ) groovy_data [ 'expected_num_jobs' ] = len ( job_configs ) groovy_data [ 'job_prefixes_and_names' ] = { } if groovy_script is not None : print ( "Writing groovy script '%s' to reconfigure %d jobs" % ( groovy_script , len ( job_configs ) ) ) content = expand_template ( 'snippet/reconfigure_jobs.groovy.em' , groovy_data ) write_groovy_script_and_configs ( groovy_script , content , job_configs , view_configs )
13489	def update ( self , server ) : for chunk in self . __cut_to_size ( ) : server . put ( 'tasks_admin' , chunk . as_payload ( ) , replacements = { 'slug' : chunk . challenge . slug } )
8161	def next_event ( block = False , timeout = None ) : try : return channel . listen ( block = block , timeout = timeout ) . next ( ) [ 'data' ] except StopIteration : return None
256	def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats
7131	def setup_paths ( source , destination , name , add_to_global , force ) : if source [ - 1 ] == "/" : source = source [ : - 1 ] if not name : name = os . path . split ( source ) [ - 1 ] elif name . endswith ( ".docset" ) : name = name . replace ( ".docset" , "" ) if add_to_global : destination = DEFAULT_DOCSET_PATH dest = os . path . join ( destination or "" , name + ".docset" ) dst_exists = os . path . lexists ( dest ) if dst_exists and force : shutil . rmtree ( dest ) elif dst_exists : log . error ( 'Destination path "{}" already exists.' . format ( click . format_filename ( dest ) ) ) raise SystemExit ( errno . EEXIST ) return source , dest , name
8482	def env_key ( key , default ) : env = key . upper ( ) . replace ( '.' , '_' ) return os . environ . get ( env , default )
9866	def price_unit ( self ) : currency = self . currency consumption_unit = self . consumption_unit if not currency or not consumption_unit : _LOGGER . error ( "Could not find price_unit." ) return " " return currency + "/" + consumption_unit
12686	def find ( self , * args ) : curr_node = self . __root return self . __traverse ( curr_node , 0 , * args )
3048	def _implicit_credentials_from_files ( ) : credentials_filename = _get_environment_variable_file ( ) if not credentials_filename : credentials_filename = _get_well_known_file ( ) if os . path . isfile ( credentials_filename ) : extra_help = ( ' (produced automatically when running' ' "gcloud auth login" command)' ) else : credentials_filename = None else : extra_help = ( ' (pointed to by ' + GOOGLE_APPLICATION_CREDENTIALS + ' environment variable)' ) if not credentials_filename : return SETTINGS . env_name = DEFAULT_ENV_NAME try : return _get_application_default_credential_from_file ( credentials_filename ) except ( ApplicationDefaultCredentialsError , ValueError ) as error : _raise_exception_for_reading_json ( credentials_filename , extra_help , error )
478	def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id
8192	def nodes_by_category ( self , category ) : return [ n for n in self . nodes if n . category == category ]
5144	def _load ( self , config ) : if isinstance ( config , six . string_types ) : try : config = json . loads ( config ) except ValueError : pass if not isinstance ( config , dict ) : raise TypeError ( 'config block must be an istance ' 'of dict or a valid NetJSON string' ) return config
13437	def _cut_range ( self , line , start , current_position ) : result = [ ] try : for j in range ( start , len ( line ) ) : index = _setup_index ( j ) try : result . append ( line [ index ] ) except IndexError : result . append ( self . invalid_pos ) finally : result . append ( self . separator ) result . append ( line [ - 1 ] ) except IndexError : pass try : int ( self . positions [ current_position + 1 ] ) result . append ( self . separator ) except ( ValueError , IndexError ) : pass return result
822	def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage
6010	def load_poisson_noise_map ( poisson_noise_map_path , poisson_noise_map_hdu , pixel_scale , convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image , image , exposure_time_map , convert_from_electrons , gain , convert_from_adus ) : poisson_noise_map_options = sum ( [ convert_poisson_noise_map_from_weight_map , convert_poisson_noise_map_from_inverse_noise_map , poisson_noise_map_from_image ] ) if poisson_noise_map_options == 0 and poisson_noise_map_path is not None : return PoissonNoiseMap . from_fits_with_pixel_scale ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu , pixel_scale = pixel_scale ) elif poisson_noise_map_from_image : if not ( convert_from_electrons or convert_from_adus ) and exposure_time_map is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if an ' 'exposure-time (or exposure time map) is not supplied to convert to adus' ) if convert_from_adus and gain is None : raise exc . DataException ( 'Cannot compute the Poisson noise-map from the image if a' 'gain is not supplied to convert from adus' ) return PoissonNoiseMap . from_image_and_exposure_time_map ( pixel_scale = pixel_scale , image = image , exposure_time_map = exposure_time_map , convert_from_electrons = convert_from_electrons , gain = gain , convert_from_adus = convert_from_adus ) elif convert_poisson_noise_map_from_weight_map and poisson_noise_map_path is not None : weight_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_weight_map ( weight_map = weight_map , pixel_scale = pixel_scale ) elif convert_poisson_noise_map_from_inverse_noise_map and poisson_noise_map_path is not None : inverse_noise_map = Array . from_fits ( file_path = poisson_noise_map_path , hdu = poisson_noise_map_hdu ) return PoissonNoiseMap . from_inverse_noise_map ( inverse_noise_map = inverse_noise_map , pixel_scale = pixel_scale ) else : return None
13347	def cmd ( ) : if platform == 'win' : return [ 'cmd.exe' , '/K' ] elif platform == 'linux' : ppid = os . getppid ( ) ppid_cmdline_file = '/proc/{0}/cmdline' . format ( ppid ) try : with open ( ppid_cmdline_file ) as f : cmd = f . read ( ) if cmd . endswith ( '\x00' ) : cmd = cmd [ : - 1 ] cmd = cmd . split ( '\x00' ) return cmd + [ binpath ( 'subshell.sh' ) ] except : cmd = 'bash' else : cmd = 'bash' return [ cmd , binpath ( 'subshell.sh' ) ]
1543	def get_clusters ( ) : instance = tornado . ioloop . IOLoop . instance ( ) try : return instance . run_sync ( lambda : API . get_clusters ( ) ) except Exception : Log . debug ( traceback . format_exc ( ) ) raise
7401	def down ( self ) : self . swap ( self . get_ordering_queryset ( ) . filter ( order__gt = self . order ) )
4924	def get_required_query_params ( self , request ) : email = get_request_value ( request , self . REQUIRED_PARAM_EMAIL , '' ) enterprise_name = get_request_value ( request , self . REQUIRED_PARAM_ENTERPRISE_NAME , '' ) number_of_codes = get_request_value ( request , self . OPTIONAL_PARAM_NUMBER_OF_CODES , '' ) if not ( email and enterprise_name ) : raise CodesAPIRequestError ( self . get_missing_params_message ( [ ( self . REQUIRED_PARAM_EMAIL , bool ( email ) ) , ( self . REQUIRED_PARAM_ENTERPRISE_NAME , bool ( enterprise_name ) ) , ] ) ) return email , enterprise_name , number_of_codes
1154	def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )
13300	def upgrade ( self , package ) : logger . debug ( 'Upgrading ' + package ) shell . run ( self . pip_path , 'install' , '--upgrade' , '--no-deps' , package ) shell . run ( self . pip_path , 'install' , package )
9205	def before_constant ( self , constant , key ) : newlines_split = split_on_newlines ( constant ) for c in newlines_split : if is_newline ( c ) : self . current . advance_line ( ) if self . current . line > self . target . line : return self . STOP else : advance_by = len ( c ) if self . is_on_targetted_node ( advance_by ) : self . found_path = deepcopy ( self . current_path ) return self . STOP self . current . advance_columns ( advance_by )
8567	def add_loadbalanced_nics ( self , datacenter_id , loadbalancer_id , nic_id ) : data = '{ "id": "' + nic_id + '" }' response = self . _perform_request ( url = '/datacenters/%s/loadbalancers/%s/balancednics' % ( datacenter_id , loadbalancer_id ) , method = 'POST' , data = data ) return response
